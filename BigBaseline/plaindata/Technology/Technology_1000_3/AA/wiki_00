{"id": "52481827", "url": "https://en.wikipedia.org/wiki?curid=52481827", "title": "50 Things That Made the Modern Economy", "text": "50 Things That Made the Modern Economy\n\n50 Things That Made the Modern Economy began as a weekly economic history documentary radio series on the BBC World Service presented by economist and journalist Tim Harford. The first episode was broadcast on Saturday 5 November 2016, and all episodes are available as podcasts.\n\nHarford explained his motivation \"to paint a picture of economic change by telling the stories of the ideas, people, and tools that had far-reaching consequences\". He was fascinated by the many unexpected outcomes, such as \"the impact of the fridge on global politics, or of the gramophone on income inequality.\"\n\nThe series was subsequently released by Harford as a book titled Fifty Things That Made The Modern Economy in the UK published by Little, Brown Books, and as Fifty Inventions That Shaped The Modern Economy in the US published by Riverhead Books. The hardcover and eBook editions were released on 6 July 2017 in the UK and the US edition was released on 29 August 2017. A paperback edition has been announced for May 2018.\n\nA public call was made for suggestions for a 51st thing in August 2017. Harford chose his six favourite submissions for an online vote from late September to 6 October 2017. The winning item was announced as the Credit Card in an episode released on 28 October 2017.\n\nEach of the nine-minute long programmes introduces the story of fifty products or inventions that have revolutionised the modern world.\n\nEach episode was originally broadcast on BBC World Service, with a subsequent broadcast on BBC Radio 4 and distribution as a BBC podcast.\n\nThe shortlist selected by Tim Harford for public vote as the 51st thing was:\n\nThe item selected by public vote was the Credit Card, announced by Tim Harford in the episode entitled “Number 51”\n\nEach of the short chapters describes fifty products or inventions that have revolutionised the modern world.\n\nThe book title uses the word Fifty whereas the radio programme uses the number 50. The chapter order is different to the radio broadcast and podcast order, some book chapters have modified titles, and the chapters are grouped into sections in the book.\n\n\n"}
{"id": "27581577", "url": "https://en.wikipedia.org/wiki?curid=27581577", "title": "ATM Adaptation Layer 1", "text": "ATM Adaptation Layer 1\n\nAn ATM Adaptation layer 1 or AAL1 is used for transmitting Class A network traffic, that is, real-time, constant bit rate, connection oriented traffic (example- uncompressed audio and video). Bits are fed in by the application at constant rate and must be delivered to other end with minimum delay, jitter or overhead. The input is stream of bits without message boundaries. For this traffic, error detection protocols cannot be used since timeouts and retransmission causes delay but the missing cells are reported to the application, that must take its own action to recover from them.\n"}
{"id": "20441402", "url": "https://en.wikipedia.org/wiki?curid=20441402", "title": "Alliance Ground Surveillance", "text": "Alliance Ground Surveillance\n\nAlliance Ground Surveillance (AGS) is a NATO programme to acquire an airborne ground surveillance capability (Multi-Platform Radar Technology Insertion Program on the Northrop Grumman RQ-4 Global Hawk).\n\nIn a similar fashion as with Strategic Airlift Capability the program is run by 15 NATO member states: Bulgaria, Czech Republic, Denmark, Estonia, Germany, Italy, Latvia, Lithuania, Luxembourg, Norway, Poland, Romania, Slovakia, Slovenia and the United States. AGS is scheduled to reach initial operational capability by the end of 2017. The Main Operating Base will be located at Sigonella Air Base, Italy.\n\nFirst Global Hawk Block 40 UAVs destined for the NATO AGS program rolled off Northrop Grumman production line in Palmdale Ca , 4 June 2015 1 arrived at Edwards on Dec. 19, 2015 completing its first flight. the rest stayed in plant 42 located in Palmdale CA\n\nIn July 2017, the USAF assigned the Mission Designation Series (MDS) of RQ-4D to the NATO AGS air vehicle.\n\n\n"}
{"id": "53531390", "url": "https://en.wikipedia.org/wiki?curid=53531390", "title": "Amoriguard", "text": "Amoriguard\n\nAmoriguard is a water-based paint whose fillers are based on recycled industrial materials. These fillers replace mineral constituents in traditional paint that are commonly mined with detrimental effects on the environment. In order to maximize the use of recycled materials, the paint has an effective mass of solids of 70℅ that occupy a volume of at least 55℅ excluding water. It was invented by Mulalo Doyoyo and co-developed by Ryan Purchase. The amount of all harmful and polluting substances such as volatile organic compounds, ammonia, formaldehyde, lead, alkyl phenol ethoxylate (APEO) and glycol in the paint are low or absent. It is manufactured below critical pigment volume concentration (CPVC) which means that most voids between pigment particles in the dried film are filled with solid particles as opposed to air in those products above CPVC. The paint is hydrophobic and chemical-resistant. \n"}
{"id": "4867808", "url": "https://en.wikipedia.org/wiki?curid=4867808", "title": "Andante ticket", "text": "Andante ticket\n\nAndante is a public transport ticketing system used in and around Porto, Portugal. \nIt started operation in November 2002 at Metro do Porto stations and is now a cross-network ticket used on the Porto Metro, selected bus and train routes and the Funicular dos Guindais cable railway.\n\nTwo types of card are currently in use:\n\nOccasional tickets can be bought at the terminals in stations. Ticket machines can recharge both kinds of ticket, although Gold tickets can only be purchased in \"Lojas Andante\" (Andante Shops). Tickets can also be recharged at MultiBanco ATMs. When purchasing tickets, passengers must select how many zones the card will allow travel within. The minimum Z2 (2 zone) ticket allows travel for 1 hour after validation, with allowed travel time increasing for each valid zone purchased.\n\nUnlike others ticket zoning systems, Andante zones are not concentric. This makes the system slightly fairer, but also slightly more complicated. To travel within the same zone or up to one neighbouring zone you need Z2 ticket, the more zones you need to cross, the higher the Z ticket you need. Although with Andante Occasional you can use your tickets in any zones, with Andante Gold you must choose in advance which zones you will be travelling in.\n\nThe system uses ISO/IEC 14443 type B for communication between card readers (check-in points, automatic vending machines, vending stores and controller handsets) and the card itself. The system is entirely contactless, with validation activated by holding the ticket a short distance in front of the reader for about a second. Teams of ticket inspectors make random checks across the network with hand-held ticket readers.\n\n"}
{"id": "5324835", "url": "https://en.wikipedia.org/wiki?curid=5324835", "title": "Button copy", "text": "Button copy\n\nButton copy is a deprecated physical design of road signs in the United States. Round plastic retroreflective buttons made of transparent plastic are placed in rows following the contours of sign legend elements, usually painted white, such as letters, numbers, arrows, and borders. In daylight, the buttons visually blend with the white sign legend elements and so are minimally conspicuous. At night, light from each approaching vehicle's headlamps strikes the retroreflective buttons and is reflected back towards the eyes of the vehicle's driver. Thus the sign is made sufficiently conspicuous and legible for adequately fast and accurate recognition and interpretation by drivers.\nButton copy is no longer manufactured; technologically it has been supplanted by retroreflective sheeting made by various manufacturers in numerous colors and grades. As state departments of transportation increasingly stopped specifying button copy signs in favor of signs made with sheeting, it became uneconomic to maintain production of the materials and supplies to make button copy signs. The last state to specify button copy signs was Arizona, which switched to sheeting in 2000.\n\n"}
{"id": "229714", "url": "https://en.wikipedia.org/wiki?curid=229714", "title": "Carriage", "text": "Carriage\n\nA carriage is a wheeled vehicle for people, usually horse-drawn; litters (palanquins) and sedan chairs are excluded, since they are wheelless vehicles. The carriage is especially designed for private passenger use, though some are also used to transport goods. A public passenger vehicle would not usually be called a carriage – terms for such include stagecoach, charabanc and omnibus. It may be light, smart and fast or heavy, large and comfortable or luxurious. Carriages normally have suspension using leaf springs, elliptical springs (in the 19th century) or leather strapping. Working vehicles such as the (four-wheeled) wagon and (two-wheeled) cart share important parts of the history of the carriage, as does too the fast (two-wheeled) chariot.\n\nThe word \"carriage\" (abbreviated \"carr\" or \"cge\") is from Old Northern French \"cariage\", to carry in a vehicle. The word \"car\", then meaning a kind of two-wheeled cart for goods, also came from Old Northern French about the beginning of the 14th century (probably derived from the Late Latin \"carro\", a car); it was also used for railway carriages, and was extended to cover \"automobile\" around the end of the nineteenth century, when early models were called \"horseless carriages\".\n\nA carriage is sometimes called a \"team\", as in \"horse and team\". A carriage with its horse is a \"rig\". An elegant horse-drawn carriage with its retinue of servants is an \"equipage\". A carriage together with the horses, harness and attendants is a \"turnout\" or \"setout\". A procession of carriages is a \"cavalcade\".\n\nSome horsecarts found in Celtic graves show hints that their platforms were suspended elastically. Four-wheeled wagons were used in prehistoric Europe, and their form known from excavations suggests that the basic construction techniques of wheel and undercarriage (that survived until the age of the motor car) were established then.\n\nThe earliest recorded sort of carriage was the chariot, reaching Mesopotamia as early as 1900 BC. Used typically for warfare by Egyptians, the near Easterners and Europeans, it was essentially a two-wheeled light basin carrying one or two passengers, drawn by one to two horses. The chariot was revolutionary and effective because it delivered fresh warriors to crucial areas of battle with swiftness.\n\nFirst century BC Romans used sprung wagons for overland journeys. It is likely that Roman carriages employed some form of suspension on chains or leather straps, as indicated by carriage parts found in excavations.\n\nIn the kingdom of the Zhou Dynasty the Warring States were also known to have used carriages as transportation. With the decline of these civilizations these techniques almost disappeared.\n\nThe medieval carriage was typically a four-wheeled wagon type, with a rounded top ('tilt') similar in appearance to the Conestoga Wagon familiar from the United States. Sharing the traditional form of wheels and undercarriage known since the Bronze Age, it very likely also employed the pivoting fore-axle in continuity from the ancient world. Suspension (on chains) is recorded in visual images and written accounts from the 14th century ('chars branlant' or rocking carriages), and was in widespread use by the 15th century. Carriages were largely used by royalty, aristocrats (and especially by women), and could be elaborately decorated and gilded. These carriages were on four wheels often and were pulled by two to four horses depending on how they were decorated (elaborate decoration with gold lining made the carriage heavier). Wood and iron were the primary requirements needed to build a carriage and carriages that were used by non-royalty were covered by plain leather.\n\nAnother form of carriage was the pageant wagon of the 14th century. Historians debate on the structure and size of pageant wagons; however, they are generally miniature house-like structures that rest on four to six wheels depending on the size of the wagon. The pageant wagon is significant because up until the 14th century most carriages were on two or 3 wheels; the chariot, rocking carriage, and baby carriage are two examples of carriages which pre-date the pageant wagon. Historians also debate whether or not pageant wagons were built with pivotal axle systems, which allowed the wheels to turn. Whether it was a four- or six-wheel pageant wagon, most historians maintain that pivotal axle systems were implemented on pageant wagons because many roads were often winding with some sharp turns. Six wheel pageant wagons also represent another innovation in carriages; they were one of the first carriages to use multiple pivotal axles. Pivotal axles were used on the front set of wheels and the middle set of wheels. This allowed the horse to move freely and steer the carriage in accordance with the road or path.\n\nOne of the great innovations of the carriage was the invention of the suspended carriage or the \"chariot branlant\" (though whether this was a Roman or medieval innovation remains uncertain). The 'chariot branlant' of medieval illustrations was suspended by chains rather than leather straps as had been believed. Chains provided a smoother ride in the chariot branlant because the compartment no longer rested on the turning axles. In the 15th century, carriages were made lighter and needed only one horse to haul the carriage. This carriage was designed and innovated in Hungary. Both innovations appeared around the same time and historians believe that people began comparing the chariot branlant and the Hungarian light coach. However, the earliest illustrations of the Hungarian 'Kochi-wagon' do not indicate any suspension, and often the use of three horses in harness.\n\nUnder King Mathias Corvinus (1458–90), who enjoyed fast travel, the Hungarians developed fast road transport, and the town of Kocs between Budapest and Vienna became an important post-town, and gave its name to the new vehicle type. The Hungarian coach was highly praised because it was capable of holding 8 men, used light wheels, could be towed by only one horse (it may have been suspended by leather straps, but this is a topic of debate). Ultimately it was the Hungarian coach that generated a greater buzz of conversation than the chariot branlant of France because it was a much smoother ride. Henceforth, the Hungarian coach spread across Europe rather quickly, in part due to Ippolito d'Este of Ferrara (1479–1529), nephew of Mathias' queen Beatrix of Aragon, who as a very junior Archbishopric of Esztergom developed a liking of Hungarian riding and took his carriage and driver back to Italy. Around 1550 the 'coach' made its appearance throughout the major cities of Europe, and the new word entered the vocabulary of all their languages. However, the new 'coach' seems to have been a concept (fast road travel for men) as much as any particular type of vehicle, and there is no obvious change that accompanied the innovation. As it moved throughout Europe in the late 16th century, the coach’s body structure was ultimately changed, from a round-top to the 'four-poster' carriages that became standard by c.1600.\n\nThe coach had doors in the side, with an iron step protected by leather that became the \"boot\" in which servants might ride. The driver sat on a seat at the front, and the most important occupant sat in the back facing forwards. The earliest coaches can be seen at Veste Coburg, Lisbon, and the Moscow Kremlin, and they become a commonplace in European art. It was not until the 17th century that further innovations with steel springs and glazing took place, and only in the 18th century, with better road surfaces, was there a major innovation with the introduction of the steel C-spring.\n\nIt was not until the 18th century that steering systems were truly improved. Erasmus Darwin was a young English doctor who was driving a carriage about 10,000 miles a year to visit patients all over England. Darwin found two essential problems or shortcomings of the commonly used light carriage or Hungarian carriage. First, the front wheels were turned by a pivoting front axle, which had been used for years, but these wheels were often quite small and hence the rider, carriage and horse felt the brunt of every bump on the road. Secondly, he recognized the danger of overturning.\n\nA pivoting front axle changes a carriage’s base from a rectangle to a triangle because the wheel on the inside of the turn is able to turn more sharply than the outside front wheel. Darwin proposed to fix these insufficiencies by proposing a principle in which the two front wheels turn about a centre that lies on the extended line of the back axle. This idea was later patented as Ackerman Steering. Darwin argued that carriages would then be easier to pull and less likely to overturn.\n\nCarriage use in North America came with the establishment of European settlers. Early colonial horse tracks quickly grew into roads especially as the colonists extended their territories southwest. Colonists began using carts as these roads and trading increased between the north and south. Eventually, carriages or coaches were sought to transport goods as well as people. As in Europe, chariots, coaches and/or carriages were a mark of status. The tobacco planters of the South were some of the first Americans to use the carriage as a form of human transportation. As the tobacco farming industry grew in the southern colonies so did the frequency of carriages, coaches and wagons. Upon the turn of the 18th century wheeled vehicle use in the colonies was at an all-time high. Carriages, coaches and wagons were being taxed based on the number of wheels they had. These taxes were implemented in the South primarily as the South had superior numbers of horses and wheeled vehicles when compared to the North. Europe, however, still used carriage transportation far more often and on a much larger scale than anywhere else in the world.\nCarriages and coaches began to disappear as use of steam propulsion began to generate more and more interest and research. Steam power quickly won the battle against animal power as is evident by a newspaper article written in England in 1895 entitled \"Horseflesh vs. Steam\". The article highlights the death of the carriage as the means of transportation.\n\nNowadays, carriages are still used for day-to-day transport in the United States by some minority groups such as the Amish. They are also still used in the tourism as vehicles for sightseeing in cities such as Bruges, Vienna, New Orleans, and Little Rock, Arkansas.\n\nThe most complete working collection of carriages can be seen at the Royal Mews in London where a large selection of vehicles is in regular use. These are supported by a staff of liveried coachmen, footmen and postillions. The horses earn their keep by supporting the work of the Royal Household, particularly during ceremonial events. Horses pulling a large carriage known as a \"covered brake\" collect the Yeoman of the Guard in their distinctive red uniforms from St James's Palace for Investitures at Buckingham Palace; High Commissioners or Ambassadors are driven to their audiences with The Queen in landaus; visiting heads of state are transported to and from official arrival ceremonies and members of the Royal Family are driven in Royal Mews coaches during Trooping the Colour, the Order of the Garter service at Windsor Castle and carriage processions at the beginning of each day of Royal Ascot.\n\nCarriages may be enclosed or open, depending on the type. The top cover for the body of a carriage, called the \"head\" or \"hood\", is often flexible and designed to be folded back when desired. Such a folding top is called a \"bellows top\" or \"calash\". A \"hoopstick\" forms a light framing member for this kind of hood. The top, roof or second-story compartment of a closed carriage, especially a diligence, was called an \"imperial\". A closed carriage may have side windows called \"quarter lights\" (British) as well as windows in the doors, hence a \"glass coach\". On the forepart of an open carriage, a screen of wood or leather called a \"dashboard\" intercepts water, mud or snow thrown up by the heels of the horses. The dashboard or carriage top sometimes has a projecting sidepiece called a \"wing\" (British). A \"foot iron\" or \"footplate\" may serve as a carriage step.\n\nA carriage driver sits on a \"box\" or \"perch\", usually elevated and small. When at the front it is known as a \"dickey box\", a term also used for a seat at the back for servants. A footman might use a small platform at the rear called a \"footboard\" or a seat called a \"rumble\" behind the body. Some carriages have a moveable seat called a \"jump seat\". Some seats had an attached backrest called a \"lazyback\".\n\nThe shafts of a carriage were called \"limbers\" in English dialect. \"Lancewood\", a tough elastic wood of various trees, was often used especially for carriage shafts. A \"holdback\", consisting of an iron catch on the shaft with a looped strap, enables a horse to back or hold back the vehicle. The end of the tongue of a carriage is suspended from the collars of the harness by a bar called the \"yoke\". At the end of a trace, a loop called a \"cockeye\" attaches to the carriage.\n\nIn some carriage types the body is suspended from several leather straps called \"braces\" or \"thoroughbraces\", attached to or serving as springs.\n\nBeneath the carriage body is the \"undergear\" or \"undercarriage\" (or simply \"carriage\"), consisting of the running gear and chassis. The wheels and axles, in distinction from the body, are the \"running gear\". The wheels revolve upon bearings or a spindle at the ends of a bar or beam called an \"axle\" or \"axletree\". Most carriages have either one or two axles. On a four-wheeled vehicle, the forward part of the running gear, or \"forecarriage\", is arranged to permit the front axle to turn independently of the fixed rear axle. In some carriages a 'dropped axle', bent twice at a right angle near the ends, allows a low body with large wheels. A guard called a \"dirtboard\" keeps dirt from the axle arm.\n\nSeveral structural members form parts of the chassis supporting the carriage body. The fore axletree and the splinter bar above it (supporting the springs) are united by a piece of wood or metal called a \"futchel\", which forms a socket for the pole that extends from the front axle. For strength and support, a rod called the \"backstay\" may extend from either end of the rear axle to the reach, the pole or rod joining the hind axle to the forward bolster above the front axle.\n\nA skid called a \"drag\", \"dragshoe\", \"shoe\" or \"skidpan\" retards the motion of the wheels. A London patent of 1841 describes one such apparatus: An iron-shod beam, slightly longer than the radius of the wheel, is hinged under the axle so that when it is released to strike the ground the forward momentum of the vehicle wedges it against the axle. The original feature of this modification was that, instead of the usual practice of having to stop the carriage to retract the beam and so lose useful momentum, the chain holding it in place is released (from the driver's position) so that it is allowed to rotate further in its backwards direction, releasing the axle. A system of \"pendant-levers\" and straps then allows the beam to return to its first position and be ready for further use.\n\nA catch or block called a \"trigger\" may be used to hold a wheel on a declivity.\n\nA horizontal wheel or segment of a wheel called a \"fifth wheel\" sometimes forms an extended support to prevent the carriage from tipping; it consists of two parts rotating on each other about the kingbolt above the fore axle and beneath the body. A block of wood called a \"headblock\" might be placed between the fifth wheel and the forward spring.\n\nMany of these fittings were carried over to horseless carriages and evolved into the modern elements of automobiles. During the Brass Era they were often the same parts on either type of carriage (i.e., horse-drawn or horseless). \n\nA trap, pony trap or horse trap is a light, often sporty, two-wheeled or sometimes four-wheeled horse-drawn carriage, accommodating usually two to four persons in various seating arrangements, such as face-to-face or back-to-back.\n\nA tanga (Hindi: टाँगा, Urdu: ٹانگہ, Bengali: টাঙ্গা) or Tonga is a light horse-drawn carriage used for transportation in India, Pakistan, and Bangladesh. Tangas are a popular mode of transportation because they are fun to ride in, and are usually cheaper to hire than a taxi or rickshaw. However, in many cities, tangas are not allowed to use highways because of their slow pace. In Pakistan, tangas are mainly found in the older parts of cities and towns, and are becoming less popular for utilitarian travel and more popular for pleasure. Tangas have become a traditional feature of weddings and other social functions in Pakistan, as well as in other nations. They are usually pulled by two horses, though some require only one. Others are designed for farm work. The room under the seats is sometimes used by the coachman (locally called \"coach-waan\") to keep his horse's food and sometimes to keep luggage if required.\n\nTangas are used for economic activity, mainly to carry heavy goods within the city limits.\n\nTangas were the most common means of transport in urban India and Pakistan until the early 1980s. Although autorickshaws have overtaken them in popularity, tangas are still common today in many cities and villages.\n\nA volante is a two-wheeled, one- or two-passenger Spanish carriage formerly much used in Cuba. The axle was behind an open, hooded body. The carriage was driven by a rider on the horse.\n\nAn araba (from Arabic: عربة, \"araba\" or ) (also arba or aroba) is a carriage (such as a cabriolet or coach), wagon or cart drawn by horses or oxen, used in Turkey and neighboring Middle Eastern countries. It is usually heavy and without springs, and often covered.\n\nThe names of many of these have now passed into obscurity but some have been adopted to describe automotive car body styles: \"coupé,\" \"victoria,\" \"brougham,\" \"landau\" and \"landaulet\", \"cabriolet\" (giving us our \"cab\"), \"phaeton,\" and \"limousine\" – all these once denoted particular types of carriages.\n\nA person whose business was to drive a carriage was a \"coachman\". A servant in livery called a \"footman\" or \"piquer\" formerly served in attendance upon a rider or was required to run before his master's carriage to clear the way. An attendant on horseback called an \"outrider\" often rode ahead of or next to a carriage. A \"carriage starter\" directed the flow of vehicles taking on passengers at the curbside. A \"hackneyman\" hired out horses and carriages. When hawking wares, a \"hawker\" was often assisted by a carriage.\n\nUpper-class people of wealth and social position, those wealthy enough to keep carriages, were referred to as \"carriage folk\" or \"carriage trade\".\n\nCarriage passengers often used a \"lap robe\" as a blanket or similar covering for their legs, lap and feet. A \"buffalo robe\", made from the hide of an American bison dressed with the hair on, was sometimes used as a carriage robe; it was commonly trimmed to rectangular shape and lined on the skin side with fabric. A \"carriage boot\", fur-trimmed for winter wear, was made usually of fabric with a fur or felt lining. A \"knee boot\" protected the knees from rain or splatter.\n\nA horse especially bred for carriage use by appearance and stylish action is called a \"carriage horse\"; one for use on a road is a \"road horse\". One such breed is the \"Cleveland Bay\", uniformly bay in color, of good conformation and strong constitution. Horses were broken in using a bodiless carriage frame called a \"break\" or \"brake\".\n\nA \"carriage dog\" or \"coach dog\" is bred for running beside a carriage.\n\nA roofed structure that extends from the entrance of a building over an adjacent driveway and that shelters callers as they get in or out of their vehicles is known as a \"carriage porch\" or \"porte cochere\". An outbuilding for a carriage is a \"coach house\", which was often combined with accommodation for a groom or other servants.\n\nA \"livery stable\" kept horses and usually carriages for hire. A range of stables, usually with \"carriage houses\" (\"remises\") and living quarters built around a yard, court or street, is called a \"mews\".\n\nA kind of dynamometer called a \"peirameter\" indicates the power necessary to haul a carriage over a road or track.\n\nIn most European and English-speaking countries, driving is a competitive equestrian sport. Many horse shows host driving competitions for a particular style of driving, breed of horse, or type of vehicle. Show vehicles are usually carriages, carts, or buggies and, occasionally, sulkies or wagons. Modern high-technology carriages are made purely for competition by companies such as Bennington Carriages. in England.\nTerminology varies: the simple, lightweight two- or four-wheeled show vehicle common in many nations is called a \"cart\" in the USA, but a \"carriage\" in Australia.\n\nInternationally, there is intense competition in the all-round test of driving: combined driving, also known as \"horse-driving trials\", an equestrian discipline regulated by the Fédération Équestre Internationale (International Equestrian Federation) with national organizations representing each member country. World championships are conducted in alternate years, including single-horse, horse pairs and four-in-hand championships. The World Equestrian Games, held at four-year intervals, also includes a four-in-hand competition.\n\nFor pony drivers, the World Combined Pony Championships are held every two years and include singles, pairs and four-in-hand events.\n\nAn almost bewildering variety of horse-drawn carriages existed. Arthur Ingram's \"Horse Drawn Vehicles since 1760 in Colour\" lists 325 types with a short description of each. By the early 19th century one's choice of carriage was only in part based on practicality and performance; it was also a status statement and subject to changing fashions. The types of carriage included the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10371312", "url": "https://en.wikipedia.org/wiki?curid=10371312", "title": "Clymer repair manual", "text": "Clymer repair manual\n\nClymer repair manuals are vehicle repair manuals that often focus on powersport vehicles such as motorcycles, all-terrain vehicles, personal water craft and snowmobiles. Clymer also has several books dedicated to small engines and \"outdoor power equipment\" such as leaf blowers, chainsaws and other lawn and garden power equipment.\n\nClymer repair manuals are named after their creator Floyd Clymer, who is described in the Motorcycle Hall of Fame as:\n\n\"a pioneer in the sport of motorcycling. He was a racer, a motorcycle dealer and distributor, a magazine publisher, a racing promoter, an author and a motorcycle manufacturer.\"\nClymer repair manuals are categorized as an aftermarket product or non-OEM. Unlike OEM manuals, Clymer repair manuals are written specifically for the do it yourself as well as the professional and experienced mechanic. OEM manuals are often designed for a professional technician, who often has at their disposal an array of specialized tools, equipment and knowledge.\n\nOne valuable step in the creation process of a Clymer repair manuals is the complete disassembly and reassembly of the machine. This helps the writers for Clymer provide easy-to-follow instructions that allow the manual user to safely and efficiently service and repair their machine. This high level of detail sets Clymer apart from its factory \"OEM\" counterparts.\n\nIn 2013, Haynes Publishing Group acquired Clymer repair manuals from Penton Media.\n\nClymer currently has over three hundred repair manuals that cover thousands of models. Some of the most popular models are the Honda TRX ATVs, International Harvester Farm Tractors, BMW K1200 Series, Harley-Davidson FLH & FLT Twin Cam 88 & 103 models and the MerCruiser Stern Drive Marine Engines.\n\nHere are some of the manufacturers covered in the Clymer library:\n\n\n\n"}
{"id": "8725303", "url": "https://en.wikipedia.org/wiki?curid=8725303", "title": "Comparison of high definition optical disc formats", "text": "Comparison of high definition optical disc formats\n\nThis article compares the technical specifications of multiple high definition formats, including HD DVD and Blu-ray Disc; two mutually incompatible, high definition optical disc formats that, beginning in 2006, attempted to improve upon and eventually replace the DVD standard. The two formats remained in a format war until February 19, 2008 when Toshiba, HD DVD's creator, announced plans to cease development, manufacturing and marketing of HD DVD players and recorders.\n\nOther high-definition optical disc formats were attempted, including the multi-layered red-laser Versatile Multilayer Disc and a Chinese made format called EVD. Both appear to have been abandoned by their respective developers.\n\na These maximum storage capacities apply to currently released media as of January 2012. First two layers of Blu-ray have a 25 GB capacity, but the triple layer disc adds a further 50 GB making 100 GB total. The fourth layer adds a further 28 GB. <br>\nb All HD DVD players are required to decode the two primary channels (left and right) of any Dolby TrueHD track; however, every Toshiba made stand-alone HD DVD player released thus far decodes 5.1 channels of TrueHD.<br>\nc On November 1, 2007 Secondary video and audio decoder became mandatory for new Blu-ray Disc players when the Bonus View requirement came into effect. However, players introduced to the market before this date can continue to be sold without Bonus View.<br>\nd There are some differences in the implementation of Dolby Digital Plus (DD+) on the two formats. On Blu-ray Disc, DD+ can only be used to extend a primary Dolby Digital (DD) 5.1 audiotrack. In this method 640 kbit/s is allocated to the primary DD 5.1 audiotrack (which is independently playable on players that do not support DD+), and up to 1 Mbit/s is allocated for the DD+ extension. The DD+ extension is used to replace the rear channels of the DD track with higher fidelity versions, along with adding additional channels for 6.1/7.1 audiotracks. On HD DVD, DD+ is used to encode all channels (up to 7.1), and no legacy DD track is required since all HD DVD players are required to decode DD+.<br>\ne On PAL DVDs, 24 frame per second content is stored as 50 interlaced frames per second and gets replayed 4% faster. This process can be reversed to retrieve the original 24 frame per second content. On NTSC DVDs, 24 frame per second content is stored as 60 interlaced frames per second using a process called 3:2 pulldown, which if done properly can also be reversed.<br>\nf As of July 2008, about 66.7% of Blu-ray discs are region free and 33.3% use region codes.<br>\ng DVD supports any valid MPEG-2 refresh rate as long as it is packaged with metadata converting it to 576i50 or 480i60, This metadata takes the form of REPEAT_FIRST_FIELD instructions embedded in the MPEG-2 stream itself, and is a part of the MPEG-2 standard. HD DVD is the only high-def disc format that can decode 1080p25 while Blu-ray and HD DVD can both decode 1080p24 and 1080p30. 1080p25 content can only be presented on Blu-ray as 1080i50.<br>\nh Linear PCM is the only lossless audio codec that is mandatory for both HD DVD and Blu-ray disc players, only HD DVD players are required to decode two lossless sound formats and those are Linear PCM and Dolby TrueHD. Dolby TrueHD and DTS-HD Master Audio have become sound format of choice for many studios on their Blu-ray titles but ever since Blu-ray won the format war, it has not become clear if they are now Mandatory for all new Blu-ray disc players since the end of the format war.\n\nBlu-ray Disc has a higher maximum disc capacity than HD DVD (50 GB vs. 30 GB for a double layered disc). In September 2007 the DVD Forum approved a preliminary specification for the triple-layer 51GB HD DVD (ROM only) disc though Toshiba never stated whether it was compatible with existing HD DVD players. In September 2006 TDK announced a prototype Blu-ray Disc with a capacity of 200GB. TDK was also the first to develop a Blu-ray prototype with a capacity of 100GB in May 2005. In October 2007 Hitachi developed a Blu-ray prototype with a capacity of 100GB. Hitachi has stated that current Blu-ray drives would only require a few firmware updates in order to play the disc.\n\nThe first 50 GB dual-layer Blu-ray Disc release was the movie \"Click\", which was released on October 10, 2006. As of July 2008, over 95% of Blu-ray movies/games are published on 50 GB dual layer discs with the remainder on 25 GB discs. 85% of HD DVD movies are published on 30 GB dual layer discs, with the remainder on 15 GB discs.\n\nThe choice of video compression technology (codec) complicates any comparison of the formats. Blu-ray Disc and HD DVD both support the same three video compression standards: MPEG-2, VC-1 and AVC, each of which exhibits different bitrate/noise-ratio curves, visual impairments/artifacts, and encoder maturity. Initial Blu-ray Disc titles often used MPEG-2 video, which requires the highest average bitrate and thus the most space, to match the picture quality of the other two video codecs. As of July 2008 over 70% of Blu-ray Disc titles have been authored with the newer compression standards: AVC and VC-1. HD DVD titles have used VC-1 and AVC almost exclusively since the format's introduction. Warner Bros., which used to release movies in both formats prior to June 1, 2007, often used the same encode (with VC-1 codec) for both Blu-ray Disc and HD DVD, with identical results. In contrast, Paramount used different encodings: initially MPEG-2 for early Blu-ray Disc releases, VC-1 for early HD DVD releases, and eventually AVC for both formats.\n\nWhilst the two formats support similar audio codecs, their usage varies. Most titles released on the Blu-ray format include Dolby Digital tracks for each language in the region, a DTS-HD Master Audio track for all 20th Century Fox and Sony Pictures and many upcoming Universal titles, Dolby TrueHD for Disney and Sony Pictures and some Paramount and Warner titles, and for many Blu-ray titles a Linear PCM track for the primary language. On the other hand, most titles released on the HD DVD format include Dolby Digital Plus tracks for each language in the region, and some also include a Dolby TrueHD track for the primary language.\n\nBoth Blu-ray Disc and HD DVD have two main options for interactivity (on-screen menus, bonus features, etc.).\n\nHD DVD's Standard Content is a minor change from standard DVD's subpicture technology, while Blu-ray's BDMV is completely new. This makes transitioning from standard DVD to Standard Content HD DVD relatively simple —for example, Apple's DVD Studio Pro has supported authoring Standard Content since version 4.0.3. For more advanced interactivity Blu-ray disc supports BD-J while HD DVD supports Advanced Content.\n\nBlu-ray Discs contain their data relatively close to the surface (less than 0.1 mm) which combined with the smaller spot size presents a problem when the surface is scratched as data would be destroyed. To overcome this, TDK, Sony, and Panasonic each have developed a proprietary scratch resistant surface coating. TDK trademarked theirs as Durabis, which has withstood direct abrasion by steel wool and marring with markers in tests.\n\nHD DVD uses traditional material and has the same scratch and surface characteristics of a regular DVD. The data is at the same depth (0.6 mm) as DVD as to minimize damage from scratching. As with DVD the construction of the HD DVD allows for a second side of either HD DVD or DVD.\n\nA study performed by Home Media Magazine (August 5, 2007) concluded that HD DVDs and Blu-ray discs are essentially equal in production cost. Quotes from several disc manufacturers for 25,000 units of HD DVDs and Blu-rays revealed a price differential of only 5-10 cents. (Lowest price: 90 cents versus 100 cents. Highest price: $1.45 versus $1.50.) Another study performed by Wesley Tech (February 9, 2007) arrived at a similar conclusion. Quotes for 10,000 discs show that a 15 gigabyte HD DVD costs $11,500 total, and 25 gigabyte Blu-ray or a 30 gigabyte HD DVD costs $13,000 total. For larger quantities of 100,000 units, the 30 gigabyte HD DVD was more expensive than the 25 gigabyte Blu-ray ($1.55 versus $1.49).\n\nAt the Consumer Electronics Show, on , Warner Bros. introduced a hybrid technology, Total HD, which would reportedly support both formats on a single disc. The new discs were to overlay the Blu-ray and HD DVD layers, placing them respectively and beneath the surface. The Blu-ray top layer would act as a two-way mirror, reflecting just enough light for a Blu-ray reader to read and an HD DVD player to ignore.\n\nLater that year, however, in September 2007, Warner President Ron Sanders said that the technology was on hold due to Warner being the only company who would publish on it.\n\nOne year after the original announcement, on 4 January 2008, Warner Bros. stated that it would support the Blu-ray format exclusively beginning on 1 June 2008, which, along with the demise of HD DVD the following month, ended development of hybrid discs permanently.\n\nThe primary copy protection system used on both formats is the Advanced Access Content System (AACS). Other copy protection systems include:\n\nThe Blu-ray specification and all currently available players support region coding. As of July 2008 about 66.7% of Blu-ray Disc titles are region-free and 33.3% use region codes.\n\nThe HD DVD specification had no region coding, so a HD DVD from anywhere in the world will work in any player. The DVD Forum's steering committee discussed a request from Disney to add it, but many of the 20 companies on the committee actively opposed it.\n\nSome film titles that were exclusive to Blu-ray in the United States such as Sony's \"xXx\", Fox's \"\" and \"The Prestige\", were released on HD DVD in other countries due to different distribution agreements; for example, \"The Prestige\" was released outside the U.S. by once format-neutral studio Warner Bros. Pictures. Since HD DVDs had no region coding, there are no restrictions playing foreign-bought HD DVDs in an HD DVD player.\n"}
{"id": "25815945", "url": "https://en.wikipedia.org/wiki?curid=25815945", "title": "Concatenative synthesis", "text": "Concatenative synthesis\n\nConcatenative synthesis is a technique for synthesising sounds by concatenating short samples of recorded sound (called \"units\"). The duration of the units is not strictly defined and may vary according to the implementation, roughly in the range of 10 milliseconds up to 1 second. It is used in speech synthesis and music sound synthesis to generate user-specified sequences of sound from a database built from recordings of other sequences.\n\nIn contrast to granular synthesis, concatenative synthesis is driven by an analysis of the source sound, in order to identify the units that\nbest match the specified criterion.\n\nConcatenative synthesis for music started to develop in the 2000s in particular through the work of Schwarz \nand Pachet (so-called musaicing). \nThe basic techniques are similar to those for speech, although with differences due to the differing nature of speech and music: for example, the segmentation is not into phonetic units but often into subunits of musical notes or events.\n\n"}
{"id": "2606880", "url": "https://en.wikipedia.org/wiki?curid=2606880", "title": "Consumer Electronics Hall of Fame", "text": "Consumer Electronics Hall of Fame\n\nThe Consumer Electronics Hall of Fame, founded by the Consumer Electronics Association (CEA), honors leaders whose creativity, persistence, determination and personal charisma helped to shape the industry and made the consumer electronics marketplace what it is today. According to the CEA, the Consumer Electronics Hall of Fame inductees have made a significant contribution to the world, and without these people, our lives would not be the same.\n\nThe CEA announced the first 50 inductees into the Hall of Fame at the 2000 International Consumer Electronics Show. The first class of inductees was in 2000. Each year another group of inventors, engineers, business leaders, retailers and journalists are inducted into the Consumer Electronics Hall of Fame.\n\n"}
{"id": "9627438", "url": "https://en.wikipedia.org/wiki?curid=9627438", "title": "Daedeok Science Town", "text": "Daedeok Science Town\n\nDaedeok Innopolis, formerly known as Daedeok Science Town, is the research and development district in the Yuseong-gu district in Daejeon, South Korea. Daedeok Innopolis grew out of the research cluster established by President Park Chunghee in 1973 with the opening of the KAIST. Over 20 major research institutes and over 40 corporate research centers make up this science cluster. Over the last few years, a number of IT venture companies have sprung up in this region, which has a high concentration of Ph.Ds in the applied sciences. There are 232 research and educational institutions to be found in Daejeon, many in the Daedeok region, among them the Electronics and Telecommunications Research Institute and the Korea Aerospace Research Institute. The \"town\" will provide a core for the International Science and Business Belt.\n\nThe Daedeok Innopolis logo was created by the industrial design company INNO Design in Palo Alto, USA. \n\n\n\n\n"}
{"id": "172734", "url": "https://en.wikipedia.org/wiki?curid=172734", "title": "De facto standard", "text": "De facto standard\n\nA standard is a custom or convention that has achieved a dominant position by public acceptance or market forces (for example, by early entrance to the market). is a Latin phrase that means \"in fact\" (literally by or from fact) in the sense of \"in practice but not necessarily ordained by law\" or \"in practice or actuality, but not officially established\", as opposed to .\n\nThe term standard is used in contrast with obligatory standards (also known as \" standards\"); or to express the dominant voluntary standard, when there is more than one standard available for the same use.\n\nIn social sciences, a voluntary standard that is also a standard is a typical solution to a coordination problem. The choice of a \"de facto\" standard tends to be stable in situations in which all parties can realize mutual gains, but only by making mutually consistent decisions. In contrast, an enforced \" standard\" is a solution to the prisoner's problem.\n\nA selection of well-known and illustrative examples of and standards are:\n\n\nExamples of long-time but never standards (for computer file formats):\n\nOther examples:\n\nVarious connectors and interconnect standards - despite being formalized and standardized, almost no product is required by law or other legal standard to use them. Examples:\n\nMaterials and units of packaging:\n\nThere are many examples of consolidation (of a standard) by market forces and competition, in a two-sided market, after a dispute. Examples:\n\n\nExamples of standards that are \"in dispute\" for turns :\n\n"}
{"id": "42249611", "url": "https://en.wikipedia.org/wiki?curid=42249611", "title": "Digital electronic computer", "text": "Digital electronic computer\n\nIn computer science, a digital electronic computer is a computer machine which is both an electronic computer and a digital computer. Examples of a digital electronic computers include the IBM PC, the Apple Macintosh as well as modern smartphones. When computers that were both digital and electronic appeared, they displaced almost all other kinds of computers, but computation has historically been performed in various non-digital and non-electronic ways: the Lehmer sieve is an example of a digital non-electronic computer, while analog computers are examples of non-digital computers which can be electronic (with analog electronics), and mechanical computers are examples of non-electronic computers (which may be digital or not). An example of a computer which is both non-digital and non-electronic is the ancient Antikythera mechanism found in Greece. All kinds of computers, whether they are digital or analog, and electronic or non-electronic, can be Turing complete if they have sufficient memory. A digital electronic computer is not necessarily a programmable computer, a stored program computer, or a general purpose computer, since in essence a digital electronic computer can be built for one specific application and be non-reprogrammable. As of 2014, most personal computers and smartphones in people's homes that use multicore central processing units (such as AMD FX, Intel Core i7, or the multicore varieties of ARM-based chips) are also parallel computers using the MIMD (multiple instructions - multiple data) paradigm, a technology previously only used in digital electronic supercomputers. As of 2014, most digital electronic supercomputers are also cluster computers, a technology that can be used at home in the form of small Beowulf clusters. Parallel computation is also possible with non-digital or non-electronic computers. An example of a parallel computation system using the abacus would be a group of human computers using a number of abacus machines for computation and communicating using natural language.\n\nA digital computer can perform its operations in the decimal system, in binary, in ternary or in other numeral systems. As of 2014, all digital electronic computers commonly used, whether personal computers or supercomputers, are working in the binary number system and also use binary logic. A few ternary computers using ternary logic were built mainly in the Soviet Union as research projects.\n\nA digital electronic computer is not necessarily a transistorized computer: before the advent of the transistor, computers used vacuum tubes. The transistor enabled electronic computers to become much more powerful, and recent and future developments in digital electronics may enable humanity to build even more powerful electronic computers. One such possible development is the memristor.\n\nPeople living in the beginning of the 21st century use digital electronic computers for storing data, such as photos, music, documents, and for performing complex mathematical computations or for communication, commonly over a worldwide computer network called the internet which connects many of the world's computers. All these activities made possible by digital electronic computers could, in essence, be performed with non-digital or non-electronic computers if they were sufficiently powerful, but it was only the combination of electronics technology with digital computation in binary that enabled humanity to reach the computation power necessary for today's computing. Advances in quantum computing, DNA computing, optical computing or other technologies could lead to the development of more powerful computers in the future.\n\nDigital computers are inherently best described by discrete mathematics, while analog computers are most commonly associated with continuous mathematics.\n\nThe philosophy of digital physics views the universe as being digital. Konrad Zuse wrote a book known as \"Rechnender Raum\" in which he described the whole universe as one all-encompassing computer.\n\n"}
{"id": "564695", "url": "https://en.wikipedia.org/wiki?curid=564695", "title": "Discrete system", "text": "Discrete system\n\nA discrete system is a system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models.\n\nA computer is a finite state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals.\n\n\n"}
{"id": "15399117", "url": "https://en.wikipedia.org/wiki?curid=15399117", "title": "Electro-mechanical modeling", "text": "Electro-mechanical modeling\n\nThe purpose of electro-mechanical modeling is to model and simulate an electro-mechanical system, such that its physical parameters can be examined before the actual system is built. Parameter estimation and physical realization of the overall system is the major design objective of Electro-Mechanical modeling. Theory driven mathematical model can be used or applied to other system to judge the performance of the joint system as a whole. This is a well known & proven technique for designing large control system for industrial as well as academic multi-disciplinary complex system. This technique is also being employed in MEMS technology recently.\n\nThe modeling of purely mechanical systems is mainly based on the Lagrangian which is a function of the generalized coordinates and the associated velocities. If all forces are derivable from a potential, then the time behavior of the dynamical systems is completely determined. For simple mechanical systems, the Lagrangian is defined as the difference of the kinetic energy and the potential energy.\n\nThere exists a similar approach for electrical system. By means of the electrical co-energy and well defined power quantities, the equations of motions are uniquely defined. The currents of the inductors and the voltage drops across the capacitors play the role of the generalized coordinates. All constraints, for instance caused by the Kirchhoff laws, are eliminated from the considerations. After that, a suitable transfer function is to be derived from the system parameters which eventually governs the behavior of the system.\n\nIn consequence, we have quantities (kinetic and potential energy, generalized forces) which determine the mechanical part and quantities (co-energy, powers) for the description of the electrical part. This offers a combination of the mechanical and electrical parts by means of an energy approach. As a result, an extended Lagrangian format is produced.\n\n\n"}
{"id": "46804622", "url": "https://en.wikipedia.org/wiki?curid=46804622", "title": "Factum Arte", "text": "Factum Arte\n\nFactum Arte is a company based in Madrid, Milan, and London that seeks to construct a bridge between new technologies and craft skills in the conservation of cultural heritage and in contemporary art. By using various forms of high-definition 3D scanners, Factum Arte has been able to record, in digital form using non-contact equipment, a number of endangered sites/objects of cultural importance. This is done in conjunction with the Factum Foundation for Digital Technology in Conservation, which seeks to promote the use of non-contact 3D scanners to record museum collections and historic monuments, especially in areas where they are at risk.\n\nIn addition to recording objects, Factum Arte is able to use the digital data to create an exact facsimile of the object on a scale of 1:1. In 2014, Factum Arte completed the installation of an exact facsimile of the tomb of Tutankhamun in the Valley of the Kings in Luxor, near Howard Carter’s house. The facsimile, and its proximity to the original tomb, is intended to provoke a debate about preservation; as Factum Arte’s Director, Adam Lowe, was said: \"The tomb of Tutankhamun was built to last for eternity, but it wasn’t built to be visited\".\n\nOver the years, Factum Arte has worked with institutions such as the British Museum in London, the Musée du Louvre in Paris, the Pergamon Museum in Berlin, the Museo del Prado in Madrid, and the Supreme Council of Antiquities in Egypt. In addition to its work in the field of cultural heritage, Factum Arte also assists a wide range of contemporary artist in creating technically difficult and innovative works of art.\n\nFactum Arte was founded in 2001 by the artists Adam Lowe, Manuel Franquelo, with Fernando Garcia-Guereta to facilitate the recording of the Tomb of Seti I and works with a number of artists including Marc Quinn and Anish Kapoor. The Seti project involved the design and construction of 3D laser scanners, software, and photographic equipment to record the walls of the tomb at high-resolution.\n\nFactum Arte was founded in 2001 in order to facilitate the development of technology needed specifically for the recording of the Tomb of Seti I.\n\nSeti’s tomb is regarded by many as the most visually impressive, and historically important tomb in the Valley of the Kings. Discovered by Giovanni Battista Belzoni in October 1817, the tomb of Pharaoh Seti I is the longest and one of the most decorated tombs in the Valley of the Kings. Despite being in excellent condition on its discovery, the tomb is currently closed to visitors to the Valley due to its deteriorating condition over the years. In addition to the removal of wall panels, and the loss of paint due to 19th century plaster casts, the tomb has suffered from collapses and cracks due to expeditions searching for hidden chambers in the 1950s and 60s that caused changes in the moisture levels of the surrounding rock.\n\nFactum Arte was commissioned by United Exhibits Group to make a 1:1 facsimile of the Tomb of Thutmose III in 2002. The facsimile was toured at exhibitions in various museums in the United States between November 2002 and December 2007. In 2005 a second facsimile of the tomb was exhibited in Madrid, Edinburgh, and Basel titled Immortal Pharaoh: The Tomb of Thutmose III (Edinburgh) and The Tomb of Thutmose III: The Dark Hours of the Sun (Madrid and Basel).\n\nThe Tomb of Thutmose III is the oldest complete version of the narrative of the Egyptian Amduat, the journey the Sun God takes through the hours of the night.\n\nThe facsimile of the tomb was installed briefly outside the Conrad Hotel in Cairo for the EU-Egypt Business and Tourism Summit and was unveiled by Catherine Ashton the European High Representative of the Union for Foreign Affairs in November 2012 as a gift to the people of Egypt, coinciding with the 90th anniversary of the tomb’s discovery.\nIn 2014 Factum Arte installed the facsimile in the Valley of the Kings, beside Howard Carter’s house, the consultant of the construction site where the facsimile was installed was \"Tarek Waly center architecture and heritage\". It was unveiled by the Minister of Antiquities Mohamed Ibrahim, the Minister for Tourism Hisham Zazou, the Governor of Luxor Tarek Saad el Din and EU Ambassador James Moran. The aim of the facsimile is to inform visitors to the valley about the importance of preservation and to promote awareness about the degrading state of the tombs since their opening to tourists.\n\nIn August 2015, Egyptologist Nicholas Reeves published a paper in which he hypothesised the presence of the tomb of Nefertiti as being behind one of the walls of Tutankhamun's tomb. Reeves' based his theory on markings he observed on the wall in the data recorded by Factum Arte. In September 2015, Egyptian newspaper \"Ahram\" reported that initial examinations had confirmed the existence of the wall markings observed by Reeves in Factum Arte's data. The same article reported that the results of further examinations would be published on 4 November 2015; the same day that the tomb was discovered in 1922.\n\nIn 2004, during the Second Gulf War, Factum Arte and Danish company United Exhibits Group (UEG) embarked on a project to record, and create a facsimile of the throne room of Ashurnasirpal II in the ancient city of Nimrud. Fragments of the throne room exist in the collections of various museums in Europe and the United States. Factum Arte was given permission to record these fragments in the British Museum, the Pergamon in Berlin, Dresden, Harvard, and Princeton.\n\nUnfortunately, at the time, Iraq was considered too dangerous to send a team out to record the remaining fragments in Nimrud. In 2015 the Islamic State militants in Northern Iraq destroyed much of the remaining artwork in the ruins of the palace of Nimrud. While recording these fragments in 2005 would not have prevented their destruction by ISIS, it would have kept avenues open to further in-depth study through the high-resolution 3D data, and presented the possibility of reuniting the fragments in the form of facsimile.\n\nIn November 2007, Factum Arte’s facsimile of \"The Wedding Feast at Cana\" (1563), by Paolo Veronese, was presented by the Cini Foundation in the original location of the painting, the Andrea Palladio's refectory for the Monastery of San Giorgio Maggiore, Venice. The original painting, commissioned in 1562, was plndered by the French Revolutionary Army of Napoleon in 1797 and sent to the Louvre Museum, where it hangs opposite the Mona Lisa. The facsimile was commissioned in 2006 by the Fondazione Giorgio Cini and, following an agreement with the Louvre, Factum Arte’s technicians were allowed to scan the painting at night. \"Corriere della Sera\" called the facsimile a \"turning point in art\".\n\nIn 2010 the Cini Foundation commissioned the visualisation and manufacture of objects designed by the 18th century artist and antiquarian Giambattista Piranesi. The project was conceived by Adam Lowe, Michele de Lucchi, and John Wilton-Ely and was exhibited in the Cini Foundation on the island of San Giorgio Maggiore for the Venice Biannale. The objects were later toured for exhibitions in Madrid, Barcelona, and San Diego\n\nIn March–May 2014, Factum Arte exhibited the series at the Sir John Soane Museum in London. Diverse Manieri: Piranesi, Fantasy and Excess aimed to explore the relationship between Sir John Soane and Piranesi. The objects were shown in the context of prints, drawings and books in Soane’s library.\n\nThe objects were visualised in digital form from Piranesi’s designs and then rematerialized in three dimensions in the materials specified in the design. The manufacture of the objects involved a variety of methods including stereo-lithography, milling, fused deposition modelling, electro forming and electro plating, in addition to a host of moulding and casting technologies\n\nThe 16 panels of the Polittico Griffoni once formed the altarpiece of the Basilica of San Petronio in Bologna. It was considered one of the greatest altarpieces of the 15th Century Bolognese School. The panels were originally painted by Francesco del Cossa and Ercole De Roberti. The panels, removed in 1725, are now scattered in various museums in Italy, the United Kingdom, the United States, France, the Netherlands, and the Vatican City.\n\nUsing the Lucida 3D scanner, designed by Manuel Franquelo, Factum Arte and the Factum Foundation for Digital Technology in Conservation collaborated with San Petronio Basilica to record, reproduce and reunite the panels as a facsimile in their original location.\n\nOther projects in the realm of cultural conservation include:\n\nFactum Arte has developed a number technologies in order to better facilitate the recording and production of objects.\n\n\nFactum Arte collaborates with a large number of companies and individuals from the tech industry and the art world.\n\nFactum Arte has undertaken projects with, among others, the following artists:\n\n\nIn 2013, when referring to the facsimile of the Tomb of Tutankhamun and the facsimile of the caves at Lascaux, historian Tom Holland voiced criticism of the idea of creating \"fakes\" as a means to protect the originals:\nIn our society, there is a huge premium set on authenticity. Clearly, were there not a difference between the copy and the original, it wouldn’t matter – you could make a replica and trash the original. Tutankhamun and Lascaux were created by people who believed in the world of the spirits, the dead, and the supernatural. You don’t have to believe in a god or gods to feel a place is consecrated and has a particular quality that cannot be reproduced.\n\n"}
{"id": "52972673", "url": "https://en.wikipedia.org/wiki?curid=52972673", "title": "Feel Train", "text": "Feel Train\n\nFeel Train is a technology collaborative co-founded by Courtney Stanton and Darius Kazemi and based in Portland, Oregon.\n\nFeel Train is a worker-owned cooperative. Stanton and Kazemi are its first two worker-owners, and the organization is chartered to allow a maximum of eight employees, each with equal salary, equal share in the company and equal firing power over others, including the founders. \n\nFeel Train projects have included the Stay Woke Bot, a Twitter bot developed in collaboration with activists DeRay Mckesson and Samuel Sinyangwe, and Shortcut, an app developed with radio program \"This American Life\" to facilitate sharing audio clips across social media, similar to the way gifs allow video clips to be shared. Feel Train is also developing a Twitter bot based on the Obama Social Media Archive called Relive 44, which beginning in May 2017 will repost, eight years later, every tweet from President Barack Obama (whose first tweet came in May 2009.)\n\nFeel Train website\n"}
{"id": "20514835", "url": "https://en.wikipedia.org/wiki?curid=20514835", "title": "Flash comparison", "text": "Flash comparison\n\nA list of flash guns, for easy comparison of strobes, from different manufactures. The list is intended to supplement the list of photographic equipment makers.\n\n"}
{"id": "48697908", "url": "https://en.wikipedia.org/wiki?curid=48697908", "title": "GS1 EDI", "text": "GS1 EDI\n\nGS1 EDI is a set of global electronic messaging standards for business documents used in Electronic Data Interchange (EDI). The standards are developed and maintained by GS1. GS1 EDI is part of the overall GS1 system, fully integrated with other GS1 standards, increasing the speed and accuracy of the supply chain.\nExamples of GS1 EDI standards include messages such as: Order, Despatch Advice (Shipping Notice), Invoice, Transport Instruction, etc. \nThe development and maintenance of all GS1 standards is based on a rigorous process called the Global Standard Management Process (GSMP). GS1 develops its global supply chain standards in partnership with the industries using them. Any organization can submit a request to modify the standard. Maintenance releases of GS1 EDI standards are typically published every two years, while code lists can be updated up to 4 times a year.\n\nGS1 developed the following sets of complementary EDI standards:\n\n\nThese groups of standards are being implemented in parallel by various users, GS1 supports and maintains all of them.\nGS1 EDI standards are designed to work together with other GS1 standards for the identification and labeling of goods, locations, parties and packages. This means that the information and product flows can be combined to provide business with tool enabling traceability, visibility and safety.\nIn EDI, it is essential to unambiguously identify products, services and parties involved in the transaction. In GS1 EDI standard messages, each product, party and location is identified by a unique GS1 identification key, e.g.:\n\n\nUsing the GS1 ID Keys enables master data alignment between trading partners before any trading transaction takes place. This ensures data quality, eliminates errors and removes the need to send redundant information in electronic messages (such as product specifications, party addresses, etc.).\n\nGS1 EDI standards are developed based on other global standards, such as:\n\n\nUser companies are involved in the development of GS1 standards, either directly or via industry associations, such as The Consumer Goods Forum.\n\nGS1 EDI standards are globally used by companies and organizations from different sectors and applied in various processes like Retail Up- and Downstream, Transport and Warehouse Management, Healthcare, Defense, Finance, Packaging (collaborative artwork development), Cash Handling, public administration and much more.\n\n\n"}
{"id": "22579001", "url": "https://en.wikipedia.org/wiki?curid=22579001", "title": "Gender differences in social network service use", "text": "Gender differences in social network service use\n\nMen and women use social network services (SNSs) differently and with different frequencies. In general, several researchers have found that women tend to use SNSs more than men and for different and more social purposes.\n\nTechnologies, including communications technologies, have a long history of shaping and being shaped by the gender of their users. Although technologies used to perform housework have an apparent historical connection to gender in many cultures, a more ready connection to SNSs may be drawn with telephones as a communications technology readily and widely available in the home. Telephone use has long had gendered connections ranging from the widespread assumption that women simply talk more than men, and the employment of women as telephone operators. In particular, young women have been closely associated with extensive and trivial use of the telephone for purely social purposes. Similarly, women's use of and influence on the development of computers has been trivialized while significant developments in computers have been masculinized. Thus the idea that there may be both real and perceived differences in how men and women use SNSs – and that those uses may shape the SNSs – is neither new nor surprising and has historical analogues.\n\nThere is historical and contemporary evidence that current fears about young girls' online safety have historical antecedents such as telegraphs and telephones. Further, in many cases those historical reactions resulted in restrictions of girls' use of technology to protect them from predators, molesters, and other criminals threatening their innocence. Like current fears focused on computer use, particularly SNSs and other communication media, these fears are most intense when the medium enters the home. These fears have the potential to – at least temporarily – overwhelm the positive and empowering uses of these technologies. These historical fears are echoed in contemporary media accounts of youths' use of SNSs.\n\nFinally, the histories of some SNSs themselves have ties with gender. For example, gay men were one of the earliest groups to join and use the early SNS Friendster.\n\nMany studies have found that women are more likely to use either specific SNSs such as Facebook or MySpace or SNSs in general. In 2015, 73% of online men and 80% of online women used social networking sites. The gap in gender differences has become less apparent in LinkedIn. In 2015 about 26 percent of online men and 25% of online women used the business-and employee-oriented networking site.\n\nResearchers who have examined the gender of users of multiple SNSs have found contradictory results. Hargittai's groundbreaking 2007 study examining race, gender, and other differences between undergraduate college student users of SNSs found that women were not only more likely to have used SNSes than men but that they were also more likely to have used many different services, including Facebook, MySpace, and Friendster; these differences persisted in several models and analyses. Although she only surveyed students at one institution – the University of Illinois at Chicago – Hargittai selected that institution intentionally as \"an ideal location for studies of how different kinds of people use online sites and services.\" In contrast, data collected by the Pew Internet & American Life Project found that men were more likely to have multiple SNS profiles. Although the sample sizes of the two surveys are comparable – 1,650 Internet users in the Pew survey compared with 1,060 in Hargittai's survey – the data from the Pew survey are newer and arguably more representative of the entire adult United States population.\n\nIn general, women seem to use SNSs more to explicitly foster social connections. A study conducted by Pew research centers found that women were more avid users of social media. In November 2010, the gap between men and women was as high as 15%. Female participants in a multi-stage study conducted in 2007 to discover the motivations of Facebook users scored higher on scales for social connection and posting of photographs. Studies have also been conducted on the differences between females and males with regards to blogging. The Pew Research Center found that younger females are more likely blog than males their own age, even males that are older than them. Similarly, in a study of blogs maintained in MySpace, women were found to be more likely to not only write blogs but also write about family, romantic relationships, friendships, and health in those blogs. A study of Swedish SNS users found that women were more likely to have expressions of friendship, specifically in the areas of (a) publishing photos of their friends, (b) specifically naming their best friends, and (c) writing poems to and about their friends. Women were also more likely to have expressions related to family relationships and romantic relationships. One of the key findings of this research is that those men who do have expressions of romantic relationships in their profile had expressions just as strong as the women. However, the researcher speculated that this may be in part due to a desire to publicly express heterosexual behaviors and mannerisms instead of merely expressing romantic feelings.\n\nA large-scale study of gender differences in MySpace found that both men and women tended to have a majority of female Friends, and both men and women tended to have a majority of female \"Top\" Friends in the site.\nA later study found women to author disproportionately many (public) comments in MySpace,\nbut an investigation into the role of emotion in public MySpace comments found that women both give and receive stronger positive emotion.\nIt was hypothesised that women are simply more effective at using social networking sites because they are better able to harness positive emotion.\n\nA study focused on the influence of gender and personality on individuals’ use of online social networking websites such as Facebook, reported that men use social networking sites with the intention of forming new relationships, whereas, women use them more for relationship maintenance. (Muscanell and Guadagno, 2012)\nIn addition to this, women are more likely to use Facebook or MySpace to compare themselves to others and also to search for information. Men, however, are more likely to look at other people's profiles with in the intention to find friends. (Haferkamp et al., 2012) \n\nPrivacy has been the primary topic of many studies of SNS users, and many of these studies have found differences between male and female SNS users, although some studies have found results contradictory to those found in other studies.\n\nSome researchers have found that women are more protective of their personal information and more likely to have private profiles. Other researchers have found that women are less likely to post some types of information. Acquisti and Gross found that women in their sample were less likely to reveal their sexual orientation, personal address, or cell phone number. This is similar to Pew Internet & American Life research of children users of SNSs that found that boys and girls presented different views of privacy and behaviors, with girls being more concerned about and restrictive of information such as city, town, last name, and cell phone number that could be used to locate them. At least one group of researchers has found that women are less likely to share information that \"identifies them directly – last name, cell phone number, and address or home phone number,\" linking that resistance to women's greater concerns about \"cyberstalking\", \"cyberbullying\", and security problems.\n\nDespite these concerns about privacy, researchers have found that women are more likely to maintain up-to-date photos of themselves. Further, Kolek and Saunders found in their sample of college student Facebook users that women were more likely to not only post a photograph of themselves in their profile but that they were more likely to have a publicly viewable Facebook account (a contradictory finding compared to many other studies), post photos, and post photo albums.\n\nWomen were more likely to have: (a) a publicly viewable Facebook account, (b) more photo albums, (c) more photos, (d) a photo of themselves as their profile picture, (e) positive references to alcohol, partying, or drugs, and (f) more positive references to or about the institution or institution-related activities. In general, women were more likely to disclose information about themselves in their Facebook profile, with the primary exception of sharing their telephone number. Similarly, female respondents to Strano's study were more likely to keep their profile photo recent and choose a photo that made them appear attractive, happy, and fun-loving. Citing several examples, Strano opined that there may also be a difference in how men and women Facebook users display and interpret profile photos depicting relationships.\n\nPrivacy has also been a concern for the SnapChat app, which allows you to send messages either text or photo or video which then disappear. One study has shown that security is not a major concern for the majority of users and that most do not use Snapchat to send sensitive content (although up to 25% may do so experimentally). As part of their research almost no statistically significant gender differences were found.\n\nCyberbullying\n\nPast research carried out to investigate if there are any gender differences in cyber-bulling has found that boys commit more cyber verbal bullying, cyber forgery and more violence based on hidden identity or presenting themselves as other person. (Sincek 2014).\n\nAlthough men and women users of SNSs exhibit different behavior and motivations, they share some similarities. For example, one study that examined the veracity of information shared on SNSs by college students found that men and women were as likely to \"provide accurate and complete information about their birthday, schedule of classes, partner's name, AIM, or political views.\"\n\nIn contradiction to several of the studies described above that found that women are more likely to be SNS users, at least one very reputable study has found that men and women are equally likely to be SNS users. Data gathered in December 2008 by the Pew Internet & American Life Project showed that the SNS users in their sample were equally divided among men and women. As mentioned above, the data from the Pew survey are newer and arguably more representative of the entire adult United States population than the data in much of the previously described research.\n\nSome studies have found that traditional gender roles are present in SNSs, with men in this study conforming to traditional views of masculinity and the women to traditional views of femininity. Qualitative work with college student SNS users by Martínez Alemán and Wartman and Managgo \"et al.\" have found similar results for both Facebook and MySpace users. Moreover, the work by Managgo \"et al.\" discovered not only traditional gender roles and images but sexualisation of women users of MySpace. Similarly, research into the impact of comments in the profile of a Facebook users on that user's perceived attractiveness revealed a \"sexual double standard\", wherein negative statements resulted in male profile owners being judged more attractive and female profile owners less attractive. Finally, at least one study has found that men and women SNS users both left textual clues about their gender.\n\nIn February 2014, Facebook announced the vast expansion of options for choosing gender identities to list on profiles, ranging to up to 56 gender identity choices. In August 2014, Facebook followed up with allowing gender-neutral relationship identities for identifying family members.\n\n"}
{"id": "7064233", "url": "https://en.wikipedia.org/wiki?curid=7064233", "title": "History of nanotechnology", "text": "History of nanotechnology\n\nThe history of nanotechnology traces the development of the concepts and experimental work falling under the broad category of nanotechnology. Although nanotechnology is a relatively recent development in scientific research, the development of its central concepts happened over a longer period of time. The emergence of nanotechnology in the 1980s was caused by the convergence of experimental advances such as the invention of the scanning tunneling microscope in 1981 and the discovery of fullerenes in 1985, with the elucidation and popularization of a conceptual framework for the goals of nanotechnology beginning with the 1986 publication of the book \"Engines of Creation\". The field was subject to growing public awareness and controversy in the early 2000s, with prominent debates about both its potential implications as well as the feasibility of the applications envisioned by advocates of molecular nanotechnology, and with governments moving to promote and fund research into nanotechnology. The early 2000s also saw the beginnings of commercial applications of nanotechnology, although these were limited to bulk applications of nanomaterials rather than the transformative applications envisioned by the field. .\n\nThe earliest evidence of the use and applications of nanotechnology can be traced back to carbon nanotubes, cementite nanowires found in the microstructure of wootz steel manufactured in ancient India from the time period of 600 BC and exported globally.\n\nAlthough nanoparticles are associated with modern science, they were used by artisans as far back as the ninth century in Mesopotamia for creating a glittering effect on the surface of pots.\n\nIn modern times, pottery from the Middle Ages and Renaissance often retains a distinct gold- or copper-colored metallic glitter. This luster is caused by a metallic film that was applied to the transparent surface of a glazing, which contains silver and copper nanoparticles dispersed homogeneously in the glassy matrix of the ceramic glaze. These nanoparticles are created by the artisans by adding copper and silver salts and oxides together with vinegar, ochre, and clay on the surface of previously-glazed pottery. The technique originated in the Muslim world. As Muslims were not allowed to use gold in artistic representations, they sought a way to create a similar effect without using real gold. The solution they found was using luster.\n\nThe American physicist Richard Feynman lectured, \"There's Plenty of Room at the Bottom,\" at an American Physical Society meeting at Caltech on December 29, 1959, which is often held to have provided inspiration for the field of nanotechnology. Feynman had described a process by which the ability to manipulate individual atoms and molecules might be developed, using one set of precise tools to build and operate another proportionally smaller set, so on down to the needed scale. In the course of this, he noted, scaling issues would arise from the changing magnitude of various physical phenomena: gravity would become less important, surface tension and Van der Waals attraction would become more important.\n\nAfter Feynman's death, scholars studying the historical development of nanotechnology have concluded that his actual role in catalyzing nanotechnology research was limited, based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, found that the published versions of Feynman’s talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s. This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by K. Eric Drexler in his 1986 book, \"Engines of Creation: The Coming Era of Nanotechnology\", which took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves via computer control instead of control by a human operator; and in a cover article headlined \"Nanotechnology\", published later that year in a mass-circulation science-oriented magazine, \"Omni\". Toumey’s analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.\n\nThese and other developments hint that the retroactive rediscovery of Feynman’s “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman. Feynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past.\n\nThe Japanese scientist called Norio Taniguchi of Tokyo University of Science was first to use the term \"nano-technology\" in a 1974 conference, to describe semiconductor processes such as thin film deposition and ion beam milling exhibiting characteristic control on the order of a nanometer. His definition was, \"'Nano-technology' mainly consists of the processing of, separation, consolidation, and deformation of materials by one atom or one molecule.\" However, the term was not used again until 1981 when Eric Drexler, who was unaware of Taniguchi's prior use of the term, published his first paper on nanotechnology in 1981.\n\nIn the 1980s the idea of nanotechnology as a deterministic, rather than stochastic, handling of individual atoms and molecules was conceptually explored in depth by K. Eric Drexler, who promoted the technological significance of nano-scale phenomena and devices through speeches and two influential books.\nIn 1980, Drexler encountered Feynman's provocative 1959 talk \"There's Plenty of Room at the Bottom\" while preparing his initial scientific paper on the subject, “Molecular Engineering: An approach to the development of general capabilities for molecular manipulation,” published in the \"Proceedings of the National Academy of Sciences\" in 1981. The term \"nanotechnology\" (which paralleled Taniguchi's \"nano-technology\") was independently applied by Drexler in his 1986 book \"Engines of Creation: The Coming Era of Nanotechnology\", which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity. He also first published the term \"grey goo\" to describe what might happen if a hypothetical self-replicating machine, capable of independent operation, were constructed and released. Drexler's vision of nanotechnology is often called \"Molecular Nanotechnology\" (MNT) or \"molecular manufacturing.\"\n\nHis 1991 Ph.D. work at the MIT Media Lab was the first doctoral degree on the topic of molecular nanotechnology and (after some editing) his thesis, \"Molecular Machinery and Manufacturing with Applications to Computation,\" was published as \"Nanosystems: Molecular Machinery, Manufacturing, and Computation,\" which received the Association of American Publishers award for Best Computer Science Book of 1992. Drexler founded the Foresight Institute in 1986 with the mission of \"Preparing for nanotechnology.” Drexler is no longer a member of the Foresight Institute.\n\nNanotechnology and nanoscience got a boost in the early 1980s with two major developments: the birth of cluster science and the invention of the scanning tunneling microscope (STM). These developments led to the discovery of fullerenes in 1985 and the structural assignment of carbon nanotubes a few years later\n\nThe scanning tunneling microscope, an instrument for imaging surfaces at the atomic level, was developed in 1981 by Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory, for which they were awarded the Nobel Prize in Physics in 1986. Binnig, Calvin Quate and Christoph Gerber invented the first atomic force microscope in 1986. The first commercially available atomic force microscope was introduced in 1989.\n\nIBM researcher Don Eigler was the first to manipulate atoms using a scanning tunneling microscope in 1989. He used 35 Xenon atoms to spell out the IBM logo. He shared the 2010 Kavli Prize in Nanoscience for this work.\n\nInterface and colloid science had existed for nearly a century before they became associated with nanotechnology. The first observations and size measurements of nanoparticles had been made during the first decade of the 20th century by Richard Adolf Zsigmondy, winner of the 1925 Nobel Prize in Chemistry, who made a detailed study of gold sols and other nanomaterials with sizes down to 10 nm using an ultramicroscope which was capable of visualizing particles much smaller than the light wavelength. Zsigmondy was also the first to use the term \"nanometer\" explicitly for characterizing particle size. In the 1920s, Irving Langmuir, winner of the 1932 Nobel Prize in Chemistry, and Katharine B. Blodgett introduced the concept of a monolayer, a layer of material one molecule thick. In the early 1950s, Derjaguin and Abrikosova conducted the first measurement of surface forces.\n\nIn 1974 the process of atomic layer deposition for depositing uniform thin films one atomic layer at a time was developed and patented by Tuomo Suntola and co-workers in Finland.\n\nIn another development, the synthesis and properties of semiconductor nanocrystals were studied. This led to a fast increasing number of semiconductor nanoparticles of quantum dots.\n\nFullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry. Smalley's research in physical chemistry investigated formation of inorganic and semiconductor clusters using pulsed molecular beams and time of flight mass spectrometry. As a consequence of this expertise, Curl introduced him to Kroto in order to investigate a question about the constituents of astronomical dust. These are carbon rich grains expelled by old stars such as R Corona Borealis. The result of this collaboration was the discovery of C and the fullerenes as the third allotropic form of carbon. Subsequent discoveries included the endohedral fullerenes, and the larger family of fullerenes the following year.\n\nThe discovery of carbon nanotubes is largely attributed to Sumio Iijima of NEC in 1991, although carbon nanotubes have been produced and observed under a variety of conditions prior to 1991. Iijima's discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods in 1991 and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, then they would exhibit remarkable conducting properties helped create the initial buzz that is now associated with carbon nanotubes. Nanotube research accelerated greatly following the independent discoveries by Bethune at IBM and Iijima at NEC of \"single-walled\" carbon nanotubes and methods to specifically produce them by adding transition-metal catalysts to the carbon in an arc discharge.\n\nIn the early 1990s Huffman and Kraetschmer, of the University of Arizona, discovered how to synthesize and purify large quantities of fullerenes. This opened the door to their characterization and functionalization by hundreds of investigators in government and industrial laboratories. Shortly after, rubidium doped C was found to be a mid temperature (Tc = 32 K) superconductor. At a meeting of the Materials Research Society in 1992, Dr. T. Ebbesen (NEC) described to a spellbound audience his discovery and characterization of carbon nanotubes. This event sent those in attendance and others downwind of his presentation into their laboratories to reproduce and push those discoveries forward. Using the same or similar tools as those used by Huffman and Kratschmer, hundreds of researchers further developed the field of nanotube-based nanotechnology.\n\nThe National Nanotechnology Initiative is a United States federal nanotechnology research and development program. “The NNI serves as the central point of communication, cooperation, and collaboration for all Federal agencies engaged in nanotechnology research, bringing together the expertise needed to advance this broad and complex field.\" Its goals are to advance a world-class nanotechnology research and development (R&D) program, foster the transfer of new technologies into products for commercial and public benefit, develop and sustain educational resources, a skilled workforce, and the supporting infrastructure and tools to advance nanotechnology, and support responsible development of nanotechnology. The initiative was spearheaded by Mihail Roco, who formally proposed the National Nanotechnology Initiative to the Office of Science and Technology Policy during the Clinton administration in 1999, and was a key architect in its development. He is currently the Senior Advisor for Nanotechnology at the National Science Foundation, as well as the founding chair of the National Science and Technology Council subcommittee on Nanoscale Science, Engineering and Technology.\n\nPresident Bill Clinton advocated nanotechnology development. In a 21 January 2000 speech at the California Institute of Technology, Clinton said, \"Some of our research goals may take twenty or more years to achieve, but that is precisely why there is an important role for the federal government.\" Feynman's stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, as mentioned in President Clinton's speech:\n\nPresident George W. Bush further increased funding for nanotechnology. On December 3, 2003 Bush signed into law the 21st Century Nanotechnology Research and Development Act, which authorizes expenditures for five of the participating agencies totaling US$3.63 billion over four years. The NNI budget supplement for Fiscal Year 2009 provides $1.5 billion to the NNI, reflecting steady growth in the nanotechnology investment.\n\n\"Why the future doesn't need us\" is an article written by Bill Joy, then Chief Scientist at Sun Microsystems, in the April 2000 issue of \"Wired\" magazine. In the article, he argues that \"Our most powerful 21st-century technologies — robotics, genetic engineering, and nanotech — are threatening to make humans an endangered species.\" Joy argues that developing technologies provide a much greater danger to humanity than any technology before it has ever presented. In particular, he focuses on genetics, nanotechnology and robotics. He argues that 20th-century technologies of destruction, such as the nuclear bomb, were limited to large governments, due to the complexity and cost of such devices, as well as the difficulty in acquiring the required materials. He also voices concern about increasing computer power. His worry is that computers will eventually become more intelligent than we are, leading to such dystopian scenarios as robot rebellion. He notably quotes the Unabomber on this topic. After the publication of the article, Bill Joy suggested assessing technologies to gauge their implicit dangers, as well as having scientists refuse to work on technologies that have the potential to cause harm.\n\nIn the AAAS Science and Technology Policy Yearbook 2001 article titled \"A Response to Bill Joy and the Doom-and-Gloom Technofuturists\", Bill Joy was criticized for having technological tunnel vision on his prediction, by failing to consider social factors. In Ray Kurzweil's \"The Singularity Is Near\", he questioned the regulation of potentially dangerous technology, asking \"Should we tell the millions of people afflicted with cancer and other devastating conditions that we are canceling the development of all bioengineered treatments because there is a risk that these same technologies may someday be used for malevolent purposes?\".\n\n\"Prey\" is a 2002 novel by Michael Crichton which features an artificial swarm of nanorobots which develop intelligence and threaten their human inventors. The novel generated concern within the nanotechnology community that the novel could negatively affect public perception of nanotechnology by creating fear of a similar scenario in real life.\n\nRichard Smalley, best known for co-discovering the soccer ball-shaped “buckyball” molecule and a leading advocate of nanotechnology and its many applications, was an outspoken critic of the idea of molecular assemblers, as advocated by Eric Drexler. In 2001 he introduced scientific objections to them attacking the notion of universal assemblers in a 2001 \"Scientific American\" article, leading to a rebuttal later that year from Drexler and colleagues, and eventually to an exchange of open letters in 2003.\n\nSmalley criticized Drexler's work on nanotechnology as naive, arguing that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Smalley believed that such assemblers were not physically possible and introduced scientific objections to them. His two principal technical objections, which he had termed the “fat fingers problem\" and the \"sticky fingers problem”, argued against the feasibility of molecular assemblers being able to precisely select and place individual atoms. He also believed that Drexler’s speculations about apocalyptic dangers of molecular assemblers threaten the public support for development of nanotechnology.\n\nSmalley first argued that \"fat fingers\" made MNT impossible. He later argued that nanomachines would have to resemble chemical enzymes more than Drexler's assemblers and could only work in water. He believed these would exclude the possibility of \"molecular assemblers\" that worked by precision picking and placing of individual atoms. Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible.\n\nSmalley also believed that Drexler's speculations about apocalyptic dangers of self-replicating machines that have been equated with \"molecular assemblers\" would threaten the public support for development of nanotechnology. To address the debate between Drexler and Smalley regarding molecular assemblers \"Chemical & Engineering News\" published a point-counterpoint consisting of an exchange of letters that addressed the issues.\n\nDrexler and coworkers responded to these two issues in a 2001 publication. Drexler and colleagues noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in \"Nanosystems\". Drexler maintained that both were straw man arguments, and in the case of enzymes, Prof. Klibanov wrote in 1994, \"...using an enzyme in organic solvents eliminates several obstacles...\" Drexler also addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. Drexler had difficulty in getting Smalley to respond, but in December 2003, \"Chemical & Engineering News\" carried a 4-part debate.\n\nRay Kurzweil spends four pages in his book 'The Singularity Is Near' to showing that Richard Smalley's arguments are not valid, and disputing them point by point. Kurzweil ends by stating that Drexler's visions are very practicable and even happening already.\n\nThe Royal Society and Royal Academy of Engineering's 2004 report on the implications of nanoscience and nanotechnologies was inspired by Prince Charles' concerns about nanotechnology, including molecular manufacturing. However, the report spent almost no time on molecular manufacturing. In fact, the word \"Drexler\" appears only once in the body of the report (in passing), and \"molecular manufacturing\" or \"molecular nanotechnology\" not at all. The report covers various risks of nanoscale technologies, such as nanoparticle toxicology. It also provides a useful overview of several nanoscale fields. The report contains an annex (appendix) on grey goo, which cites a weaker variation of Richard Smalley's contested argument against molecular manufacturing. It concludes that there is no evidence that autonomous, self replicating nanomachines will be developed in the foreseeable future, and suggests that regulators should be more concerned with issues of nanoparticle toxicology.\n\nThe early 2000s saw the beginnings of the use of nanotechnology in commercial products, although most applications are limited to the bulk use of passive nanomaterials. Examples include titanium dioxide and zinc oxide nanoparticles in sunscreen, cosmetics and some food products; silver nanoparticles in food packaging, clothing, disinfectants and household appliances such as Silver Nano; carbon nanotubes for stain-resistant textiles; and cerium oxide as a fuel catalyst. As of March 10, 2011, the Project on Emerging Nanotechnologies estimated that over 1300 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.\n\nThe National Science Foundation funded researcher David Berube to study the field of nanotechnology. His findings are published in the monograph Nano-Hype: The Truth Behind the Nanotechnology Buzz. This study concludes that much of what is sold as “nanotechnology” is in fact a recasting of straightforward materials science, which is leading to a “nanotech industry built solely on selling nanotubes, nanowires, and the like” which will “end up with a few suppliers selling low margin products in huge volumes.\" Further applications which require actual manipulation or arrangement of nanoscale components await further research. Though technologies branded with the term 'nano' are sometimes little related to and fall far short of the most ambitious and transformative technological goals of the sort in molecular manufacturing proposals, the term still connotes such ideas. According to Berube, there may be a danger that a \"nano bubble\" will form, or is forming already, from the use of the term by scientists and entrepreneurs to garner funding, regardless of interest in the transformative possibilities of more ambitious and far-sighted work.\n\n\n"}
{"id": "14285", "url": "https://en.wikipedia.org/wiki?curid=14285", "title": "History of science and technology", "text": "History of science and technology\n\nThe history of science and technology (HST) is a field of history which examines how humanity's understanding of the natural world (science) and ability to manipulate it (technology) have changed over the centuries. This academic discipline also studies the cultural, economic, and political impacts of scientific innovation.\n\nHistories of science were originally written by practicing and retired scientists, starting primarily with William Whewell, as a way to communicate the virtues of science to the public. In the early 1930s, after a famous paper given by the Soviet historian Boris Hessen, was focused into looking at the ways in which scientific practices were allied with the needs and motivations of their context. After World War II, extensive resources were put into teaching and researching the discipline, with the hopes that it would help the public better understand both Science and Technology as they came to play an exceedingly prominent role in the world. In the 1960s, especially in the wake of the work done by Thomas Kuhn, the discipline began to serve a very different function, and began to be used as a way to critically examine the scientific enterprise. At the present time it is often closely aligned with the field of science studies.\n\nModern engineering as it is understood today took form during the scientific revolution, though much of the mathematics and science was built on the work of the Greeks, Egyptians, Mesopotamians, Chinese, Indians. See the main articles History of science and History of technology for these respective topics.\n\n\n\n\n\n\n\n\nHistory of science and technology is a well developed field in India. At least three generations of scholars can be identified.\nThe first generation includes D.D.Kosambi, Dharmpal, Debiprasad Chattopadhyay and Rahman. The second generation mainly consists of Ashis Nandy, Deepak Kumar, Dhruv Raina, S. Irfan Habib, Shiv Visvanathan, Gyan Prakash, Stan Lourdswamy, V.V. Krishna, Itty Abraham, Richard Grove, Kavita Philip, Mira Nanda and Rob Anderson. There is an emergent third generation that includes scholars like Abha Sur and Jahnavi Phalkey.\n\nDepartments and Programmes\n\nThe National Institute of Science, Technolology and Development Studies had a research group active in the 1990s which consolidated social history of science as a field of research in India. \nCurrently there are several institutes and university departments offering HST programmes.\n\n\n\n\n\n\n\n\nAcademic study of the History of Science as an independent discipline was launched by George Sarton at Harvard with his book \"Introduction to the History of Science\" (1927) and the \"Isis\" journal (founded in 1912). Sarton exemplified the early 20th century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The History of Science was not a recognized subfield of American history in this period, and most of the work was carried out by interested Scientists and Physicians rather than professional Historians. With the work of I. Bernard Cohen at Harvard, the history of Science became an established subdiscipline of history after 1945.\n\n\n\n\n\nHistoriography of science\n\nHistory of science as a discipline\n\n"}
{"id": "18558517", "url": "https://en.wikipedia.org/wiki?curid=18558517", "title": "IPv6 deployment", "text": "IPv6 deployment\n\nDeployment of Internet Protocol Version 6 (IPv6), the next generation of the Internet Protocol, has been in progress since the mid-2000s.\n\nIPv6 was designed as a replacement for IPv4 which has been in use since 1982, and is in the final stages of exhausting its unallocated address space, but still carries most Internet traffic. Google's statistics show IPv6 availability of its users up to 25% depending on the day of the week (more use on weekends), with use over 20% any day of the week . Adoption is uneven across countries and Internet service providers.\nIn November 2016, 1491 (98.2%) of the 1519 top-level domains (TLDs) in the Internet supported IPv6 to access their domain name servers, and 1485 (97.8%) zones contained IPv6 glue records, and approximately 9.0 million domains (4.6%) had IPv6 address records in their zones. Of all networks in the global BGP routing table, 29.2% had IPv6 protocol support.\n\nBy 2011, all major operating systems in use on personal computers and server systems had production-quality IPv6 implementations. Cellular telephone systems present a large deployment field for Internet Protocol devices as mobile telephone service is making the transition from 3G to \"next-generation\" 4G technologies, in which voice is provisioned as a voice over IP (VoIP) service. This mandates the use of IPv6 for such networks. In 2009, the US cellular operator Verizon released technical specifications for devices to operate on its \"next-generation\" networks. The specification mandates IPv6 operation according to the \"3GPP Release 8 Specifications (March 2009)\", and deprecates IPv4 as an optional capability.\n\nGoogle publishes statistics on IPv6 adoption among Google users. A graph of IPv6 adoption since 2008 and a map of IPv6 deployment by country are available.\n\nAkamai publishes by-country and by-network statistics on IPv6 adoption for traffic it sees on its global Content Distribution Network (CDN). This set of data also shows graphs for each country and network over time.\n\nA global view into the history of the growing IPv6 routing tables can be obtained with the SixXS Ghost Route Hunter. This tool provided a list of all allocated IPv6 prefixes until 2014 and marks with colors the ones that were actually being announced into the Internet BGP tables. When a prefix was announced, it means that the ISP at least can receive IPv6 packets for their prefix.\n\nThe integration of IPv6 on existing network infrastructure may be monitored from other sources, for example:\n\nA few organizations are involved with international IPv6 test and evaluation, ranging from the United States Department of Defense to the University of New Hampshire.\n\n\nBy 2011, all major operating systems in use on personal computers and server systems had production-quality IPv6 implementations. Microsoft Windows has supported IPv6 since Windows 2000, and in production-ready state beginning with Windows XP. Windows Vista and later have improved IPv6 support. macOS since Panther (10.3), Linux 2.6, FreeBSD, and Solaris also have mature production implementations. Some implementations of the BitTorrent peer-to-peer file transfer protocol make use of IPv6 to avoid NAT issues common for IPv4 private networks.\n\nIn the early 2000s, governments increasingly required support for IPv6 in new equipment. The US government, for example, specified in 2005 that the network backbones of all federal agencies had to be upgraded to IPv6 by June 30, 2008; this was completed before the deadline. In addition, the US government in 2010 required federal agencies to provide native dual-stacked IPv4/IPv6 access to external/public services by 2012, and internal clients were to utilize IPv6 by 2014. Progress on the US government's external facing IPv6 services is tracked by NIST. The government of the People's Republic of China implemented a five-year plan for deployment of IPv6 called the \"China Next Generation Internet\" (see below).\n\nOn March 07, 2013, the Internet Engineering Task Force, created a working group for IPv4 sunset. However in May 2018 this working group was closed.\n\nAnwarNet (www.anwarnet.dz); AfriNIC has allocated range of IPv6 address space to AnwarNet. AnwarNet started IPV6 services in 2011.\n\n\n\n\nAs of 2017, Brazil has 20% IPv6 adoption, IPv6 being adopted by most universities, companies and made available for home users by larger ISPs.\n\nHas constructed a research center to study the possibilities of adopting IPv6 in the country. The center will operate alongside another facility, which is equipped with an IBM Blue Gene/P supercomputer.\n\nSince 2015 the ISP Blizoo enabled IPv6 for many home customers.\n\nAt the end of 2016, the ISP ComNet Bulgaria Holding Ltd. has provided complete IPv6 support for all customers and households within company network in Bulgaria.\n\nIPv6 deployment is slow but ongoing, with major Canadian ISPs (notably Bell Canada, Vidéotron, and Cogeco) lacking in support for its residential customers, and the majority of their business customers (including server packages). Canadian IPv6 usage jumped from 0.5% in July 2015 to 7% in Dec 2015 due to IPv6 deployment at Rogers and Telus.\n\nAccording to Google's statistics, Canada has reached an IPv6 adoption rate of 16% by December 2016.\n\n\nThe China Next Generation Internet (CNGI, 中国下一代互联网) project is a five-year plan initiated by the Chinese government with the purpose of gaining a significant position in the development of the Internet through the early adoption of IPv6. China showcased CNGI's IPv6 infrastructure during the 2008 Summer Olympic Games, being the first time a major world event has had a presence on the IPv6 Internet. At the time of the event, it was believed that the Olympics provided the largest showcase of IPv6 technology since the inception of IPv6. The deployment of IPv6 was widespread in all related applications, from data networking and camera transmissions for sporting events, to civil applications, such as security cameras and taxis. The events were streamed live over the Internet and networked cars were able to monitor traffic conditions readily, all network operations of the Games being conducted using IPv6.\n\nAlso, the CERNET (China Education and Research NETwork, 中国教育和科研计算机网, 教育网) set up native IPv6 (CERNET2), and since then many academic institutions in China joined CERNET2 for IPv6 connectivity. CERNET-2 is probably the widest deployment of IPv6 in China. It is managed and operated jointly by 25 universities. Students in Shanghai Jiao Tong University and Beijing University of Posts and Telecommunications, for example, get native IPv6.\n\nIn November 2017, the Communist party decreed a plan to get all its Internet users on IPv6 by 2025 with a quarter of them by the end of 2018.\n\nAs of December 2016, the country has only 2% IPv6 traffic (according to both Google and Apnic stats).\n\nA web page (in Danish) follows national IPv6 deployment.\n\nThe ISP Fullrate has begun offering IPv6 to its customers, on the condition that their router (provided by the ISP itself) is compatible. If the router is of a different version, the customer has to request a new router.\n\nEstonian Telekom is providing native IPv6 access on residential and business broadband connections since September 2014. According to Google's statistics, Estonia has reached an IPv6 adoption rate of 18% by end of June 2017.\n\nFICORA (Finnish Communications Regulatory Authority), the NIC for the .fi top level domain, has added IPv6 address to DNS servers, and allows entering IPv6 address when registering domains. The registration service domain.fi for new domains is also available over IPv6.\n\nA small Finnish ISP has offered IPv6 access since 2007.\n\nFICORA held national IPv6 day on June 9, 2015. At that time Elisa and DNA Oyj started providing IPv6 on mobile subscriptions, and Telia Company and DNA Oyj started providing IPv6 on fixed-line connections.\n\nAccording to Google's statistics, Finland has reached an IPv6 adoption rate of 15% by May 2017.\n\n\nAs of May 2017, France has 16% IPv6 traffic (according to Google and 20% Apnic stats).\n\nAccording to Google's statistics, Germany has reached an IPv6 adoption rate of 30% by end of May 2017.\n\n\nIn Hungary was the first ISP starting deploying IPv6 on its network in 2008 August. The service was commercially available since 2009 May.\n\nMagyar Telekom was running tests on its production environments since the beginning of 2009. Free customer trials started on November 2, 2009, for those on ADSL or Fiber Optic. Customers are given a /128 via DHCP-ND unless they register their DUID in which case they receive a /56 using a static configuration results in a single /64.\n\nAccording to information on telecompaper.com, UPC Hungary will start deploying IPv6 in mid-2013, finishing it in 2013.\n\nIn 2015, December RCS&RDS (Digi) has enabled native dual stack IPv6(customers receive dynamic /64 prefixes) for its FTTB/H customers. In November the same year UPC Hungary introduced DS Lite(with private IPv4 addresses) which can be enabled on a customer-to-customer basis if the customer asks for it.\n\nMagyar Telekom deployed dual stack IPv6 (using dynamic /56 prefixes on DSL and GPON and static /56 prefixes on DOCSIS) for all of its wired (and for all of its compatible mobile) customers in October 2016.\n\nAccording to the statistics of APNIC, IPv6 use in Hungary as of 2017 June has reached around 11%.\n\nAccording to Google's IPv6 statistics the adoption rate at 2017 June was 11%.\n\nAccording to Google's statistics, India has reached an IPv6 adoption rate of around 32% at the end of May 2018.\nAPNIC places India at more than 50% preferring IPv6.\n\n\n\nGrowth of IPv6 in Ireland as seen by Google.\n\n\nAccording to Google's statistics, Japan has reached an IPv6 adoption rate of 17% by May 2017.\n\n\nThe LITNET academic & research network has supported IPv6 since 2001. Most commercial ISPs have not publicly deployed IPv6 yet.\n\nAccording to Google's statistics, Luxembourg has reached an IPv6 adoption rate of 24% by May 2017.\n\n\n, surveys conducted by the New Zealand IPv6 Task Force indicated that awareness of IPv6 had reached a near-universal level among New Zealand's large public- and private-sector organisations, with adoption mostly occurring as part of normal network refresh cycles. Most of New Zealand's ISP and carrier community have a test environment for IPv6 and many have started bringing IPv6 products and services on-stream. An increasing number of New Zealand government websites are available over IPv6, including those of the Ministry of Defence (New Zealand), Ministry for Primary Industries (New Zealand) and the Department of Internal Affairs.\n\n\nThe government is in process of upgrading its facilities. Globe Telecom has already set in motion the transition of its core IP network to IPv6, noting that it is now fully prepared even as the Internet runs out of IPv4 addresses. Globe claims it is the first local telecommunication company to test IPv6 with Department of Science and Technology (Philippines). In some cases, like test networks or users, IPv6 or both maybe present.\n\n\n\n\nThe Sudanese IPv6 task Force SDv6TF was formed in 2010 to fellow the implementation of IPv6 migration plan (2011–2015).\n\nBy November 2012, all telecom operators are becoming IPv6 enabled, this was tested for the first time at the AFRINIC-17 meeting held in Khartoum.\n\nSudREN (Sudanese Research and Education Network) is the first ISP to provide native IPv6 connectivity of the member institution. By August 2014, SudREN.edu.sd is fully IPv6 Enabled.\nTwo certification received from IPv6 Forum, for WWW and ISP Enabled Logos.\n\n\nOperators offering native IPv6 access for business clients and collocation customers include Tele2 and Phonera.\n\n\nStarted deploying IPv6 in 2010. In 2011, ATI (Tunisian Internet Agency) obtained a new IPv6 bloc from Afrinic (2c0f:fab0::/28).\nIn 2013–2015, Gnet (Global Net), and CIMSP (Computing Departement of Health Ministry) received IPv6 prefixes from Afrinic.\nDeployment of an IPv6 tunnel between ATI and HE (Hurricane Electric).\nIn 2016, CCK (Centre de Calcul El Khawarizmi) obtains its own IPv6 (/32) bloc from Afrinic. In 2016, ISET Charguia (Higher Institute of Technologies in Tunisia) deployed its IPv6 network as end user.\n\n\nIn the United States the majority of smartphones use IPv6, but only a small percent of computers and tablets use IPv6.\n\nThe Internet Society promoted June 8, 2011, as \"World IPv6 Day\". The event was described as a \"test drive\" for full IPv6 rollouts.\n\nThe Internet Society declared June 6, 2012, to be the date for \"World IPv6 Launch\", with participating major websites enabling IPv6 permanently, participating ISPs offering IPv6 connectivity, and participating router manufacturers offering devices enabled for IPv6 by default.\n\n\n"}
{"id": "55052289", "url": "https://en.wikipedia.org/wiki?curid=55052289", "title": "ISO 12083", "text": "ISO 12083\n\nISO 12083 (informally known as \"AAP markup\") is an international SGML standard for document interchange between authors and publishers, featuring separate DTDs for books, serials, articles, and math.\n\nIn 1983, the Association of American Publishers (AAP), a coalition of book and journal publishers in North America, launched the Electronic Manuscript Project, the first effort ever to develop a commercial SGML application. The project sought to create a standard for document interchange that would allow the publishing industry to reap the benefits of descriptive markup, which was seen as “the most effective means of establishing a consistent method for preparing electronic manuscripts which can feed the publishing process.”. Key participants in the project included organizations such as the US Library of Congress, the American Society of Indexers, the IEEE, the American Chemical Society, the American Institute of Physics, and the American Mathematical Society.\n\nIn 1983, Aspen Systems Corporation was hired to conduct the work over a two-year period. Two preliminary works with restricted distribution were produced in 1985, the draft AAP DTD and author guidelines.\n\nIn 1986, the Electronic Publishing Special Interest Group (EPSIG), a consortium sponsored by the Online Computer Library Center (OCLC), recommended the DTDs developed by the Electronic Manuscript Project be used as an American Standard. With the support of the AAP and the Graphic Communications Association (GCA), the recommendations were accepted and, in 1988, the AAP DTD became the American National Standards Institute's \"Electronic Manuscript Preparation and Markup\" (ANSI/NISO Z39.59) standard. Being based on the ASCII character encoding standard, it includes a large set of entity definitions for special characters.\n\nThe AAP and EPSIG continued their collaboration and published a revised version of the specification in 1989, identifying three document types in the field of publishing: Book, Serial Publication, and Article, for each of which the revised specification offers a DTD.\n\nThe AAP and the European Physical Society (EPS) further collaborated on a standard method for marking up tables and mathematical notation in scientific documents. Building on this work, Eric van Herwijnen, then head of the text processing section at CERN, edited the specification for adoption by the International Organization for Standardization as ISO 12083, first published in 1993, revised in 1994 and last reconfirmed in 2016. ISO 12083 features four DTDs: Article, Book, Serial, and Math.\n\nIn practice, ISO 12083 is seldom used in its pure form, yet it is the basis of many DTDs in common use.\n\n"}
{"id": "42132949", "url": "https://en.wikipedia.org/wiki?curid=42132949", "title": "Industrial Revolution in Scotland", "text": "Industrial Revolution in Scotland\n\nThe Industrial Revolution in Scotland was the transition to new manufacturing processes and economic expansion between the mid-eighteenth century and the late nineteenth century. By the start of the eighteenth century, a political union between Scotland and England became politically and economically attractive, promising to open up the much larger markets of England, as well as those of the growing British Empire, resulting in the Treaty of Union of 1707. There was a conscious attempt among the gentry and nobility to improve agriculture in Scotland. New crops were introduced and enclosures began to displace the run rig system and free pasture. The economic benefits of union were very slow to appear, some progress was visible, such as the sales of linen and cattle to England, the cash flows from military service, and the tobacco trade that was dominated by Glasgow after 1740. Merchants who profited from the American trade began investing in leather, textiles, iron, coal, sugar, rope, sailcloth, glass-works, breweries, and soap-works, setting the foundations for the city's emergence as a leading industrial center after 1815.\n\nThe linen industry was Scotland's premier industry in the eighteenth century and formed the basis for the later cotton, jute, and woolen industries. Encouraged and subsidized by the Board of Trustees so it could compete with German products, merchant entrepreneurs became dominant in all stages of linen manufacturing and built up the market share of Scottish linens, especially in the American colonial market. Historians often emphasize that the flexibility and dynamism of the Scottish banking system contributed significantly to the rapid development of the economy in the nineteenth century. At first the leading industry, based in the west, was the spinning and weaving of cotton. After the cutting off of supplies of raw cotton from 1861 as a result of the American Civil War Scottish entrepreneurs and engineers, and its large stock of easily mined coal, the country diversified into engineering, shipbuilding, and locomotive construction, with steel replacing iron after 1870. As a result, Scotland became a center for engineering, shipbuilding and the production of locomotives.\n\nScotland was already one of the most urbanized societies in Europe by 1800. Glasgow became one of the largest cities in the world, and known as \"the Second City of the Empire\" after London. Dundee upgraded its harbor and established itself as an industrial and trading center. The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis. Owners to support government sponsored housing programs as well as self-help projects among the respectable working class. Even with the growth of industry there were insufficient good jobs, as a result, during the period 1841–1931, about two million Scots emigrated to North America and Australia, and another 750,000 Scots relocated to England. By the twenty-first century, there were about as many people who were Scottish Canadians and Scottish Americans as the five million remaining in Scotland.\n\nBy the start of the eighteenth century, a political union between Scotland and England became politically and economically attractive, promising to open up the much larger markets of England, as well as those of the growing British Empire. The Scottish parliament voted on 6 January 1707, by 110 to 69 to adopt the Treaty of Union. It was a full economic union. Most of its 25 articles dealt with economic arrangements for the new state known as \"Great Britain\". It added 45 Scots to the 513 members of the House of Commons of Great Britain and 16 Scots to the 190 members of the House of Lords, and ended the Scottish parliament. It also replaced the Scottish systems of currency, taxation and laws regulating trade with laws made in London. England had about five times the population of Scotland at the time, and about 36 times as much wealth.\n\nMajor factors that facilitated industrialisation in Scotland included cheap and abundant labour; natural resources that included coal, blackband ironstone and potential water power; the development of new technologies, among them the steam engine and markets that would buy Scottish products. Other factors that also contributed to the process included the improvement of transport links, which helped facilitated the movement of goods, an extensive banking system, and the widespread adoption of ideas about economic development with their origins in the Scottish Enlightenment.\n\nIn the eighteenth century, the Scottish Enlightenment brought the country to the front of intellectual achievement in Europe. The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific. Adam Smith developed and published \"The Wealth of Nations,\" the first work of modern economics. It had an immediate impact on British economic policy and still frames discussions on globalisation and tariffs. Key scientific work included the discoveries of William Cullen, physician and chemist, James Anderson, an agronomist, Joseph Black, physicist and chemist, and James Hutton, the first modern geologist. While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the eighteenth century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as James Hutton, James Watt, William Murdoch, James Clerk Maxwell and Lord Kelvin.\n\nAfter the union with England in 1707, there was a conscious attempt among the gentry and nobility to improve agriculture in Scotland. The Society of Improvers was founded in 1723, including in its 300 members dukes, earls, lairds and landlords. In the first half of the century these changes were limited to tenanted farms in East Lothian and the estates of a few enthusiasts, such as John Cockburn and Archibald Grant. Not all were successful, with Cockburn driving himself into bankruptcy, but the ethos of improvement spread among the landed classes. Haymaking was introduced along with the English plough and foreign grasses, the sowing of rye grass and clover. Turnips and cabbages were introduced, lands enclosed and marshes drained, lime was put down, roads built and woods planted. Drilling and sowing and crop rotation were introduced. The introduction of the potato to Scotland in 1739 greatly improved the diet of the peasantry. Enclosures began to displace the runrig system and free pasture. There was increasing specialisation, with the Lothians became a major centre of grain, Ayrshire of cattle breading and the borders of sheep. Although some estate holders improved the quality of life of their displaced workers, the Agricultural Revolution led directly to what is increasingly known as the Lowland Clearances, when hundreds of thousands of cottars and tenant farmers from central and southern Scotland were forcibly moved from the farms and small holdings their families had occupied for hundreds of years. Improvement continued in the nineteenth century. Innovations included the first working reaping machine, developed by Patrick Bell in 1828. His rival James Smith turned to improving sub-soil drainage and developed a method of ploughing that could break up the subsoil barrier without disturbing the topsoil. Previously unworkable low-lying carselands could now be brought into arable production and the result was the even Lowland landscape that still predominates. The development of Scottish agriculture meant that Scotland could support its increased population with food and it released labour that would take part in industrial production.\n\nThe first banks formed in Scotland were the Bank of Scotland (Edinburgh, 1695) and the Royal Bank of Scotland (Edinburgh, 1727). Glasgow would soon follow with branches of its own (notably, the first was to be Dunlop, Houston & Co. in 1749, known as \"the Ship Bank\" for the image of a ship printed on all their bills) and Scotland had a flourishing financial system by the end of the century. There were over 400 branches, amounting to one office per 7,000 people, double the level in England. The banks were more lightly regulated than those in England. Historians often emphasise that the flexibility and dynamism of the Scottish banking system contributed significantly to the rapid development of the economy in the nineteenth century. As a joint-stock company the British Linen Company had the right to raise funds through the issue of promissory notes or bonds. With its bonds functioning as bank notes, the company gradually moved into the business of lending and discounting to other linen manufacturers, and in the early 1770s banking became its main activity.\n\nThe extensive Scottish coastline meant that few parts of the country that were not within easy reach of sea transportation, particularly the central belt that would be the heartland of industrial development. Before the eighteenth century most roads were relatively poor dirt tracks. In the late eighteenth century there were improvements carried out by turnpike trusts and the creation of a series of military roads. Canal building also developed, with four major lowland canals: the Forth and Clyde, Union, Monkland and Crinan and further north the Paisley, Caledonian and Inverurie canals, carrying thousands of passengers and tons of goods by the early nineteenth century.\n\nWith tariffs with England abolished, the potential for trade for Scottish merchants was considerable, especially with Colonial America. However, the economic benefits of union were very slow to appear, primarily because Scotland was too poor to exploit the opportunities of the greatly expanded free market. Scotland in 1750 was still a poor rural, agricultural society with a population of 1.3 million. Furthermore, Scotland's economy had been ravaged by the Darien scheme: according to some estimates, half of all the circulating wealth in Scotland went into the scheme. Glasgow merchants had been particularly enthusiastic, and consequently had no ships of their own for twenty years following the disaster. Some progress was visible, such as the sales of linen and cattle to England, the cash flows from military service, and the tobacco trade that was dominated by Glasgow after 1740. The clippers belonging to the Glasgow Tobacco Lords were the fastest ships on the route to Virginia. The trade had started as smuggling during the 1600s, but with the Act of Union, it became legal and trade picked up. Merchants who profited from the American trade began investing in leather, textiles, iron, coal, sugar, rope, sailcloth, glassworks, breweries, and soapworks, setting the foundations for the city's emergence as a leading industrial centre after 1815. The tobacco trade collapsed during the American Revolution (1776–83), when its sources were cut off by the British blockade of American ports. However, trade with the West Indies began to make up for the loss of the tobacco business, reflecting the extensive growth of the cotton industry, the British demand for sugar and the demand in the West Indies for herring and linen goods. During 1750–1815, 78 Glasgow merchants not only specialised in the importation of sugar, cotton, and rum from the West Indies, but diversified their interests by purchasing West Indian plantations, Scottish estates, or cotton mills. They were not to be self-perpetuating due to the hazards of the trade, the incident of bankruptcy, and the changing complexity of Glasgow's economy. Other burghs also benefited. Greenock enlarged its port in 1710 and sent its first ship to the Americas in 1719, but was soon playing a major part in importing sugar and rum.\n\nLinen manufacture was Scotland's premier industry in the eighteenth century and formed the basis for the later cotton, jute, and woollen industries. Scottish industrial policy was made by the Board of Trustees for Fisheries and Manufactures in Scotland, which sought to build an economy complementary, not competitive, with England. Since England had woolens, this meant linen. The Scottish members of parliament managed to see off an attempt to impose an export duty on linen and from 1727 it received subsidies of £2,750 a year for six years, resulting in a considerable expansion of the trade. Paisley adopted Dutch methods and became a major centre of production. Glasgow manufactured for the export trade, which doubled between 1725 and 1738.\n\nEncouraged and subsidised by the Board of Trustees, so that they could compete with German products, merchant entrepreneurs became dominant in all stages of linen manufacturing and built up the market share of Scottish linens, especially in the American colonial market. The British Linen Company, established in 1746, was the largest firm in the Scottish linen industry in the eighteenth century, exporting linen to England and America. In 1728, 2.2 million yards of linen cloth had been produced and by 1730 it had already supplanted woollen cloth as the major manufacturing industry. By 1750 it reached 7.6 million and it peaked at 12.1 million yards in 1775. However, there were sharp slumps, particularly in the periods 1734–43 and 1763–72. It was a mainly rural industry, with most of the manufacture carried out in homes, rather than factories. It employed perhaps 100,000 people, four out of five of which were women who spun the flax, while men operated the looms.\n\nThe government promoted the use of linen from the late 17th century: a 1686 Act of Parliament stated that all Scots were to be buried in Scottish-made linen winding sheets, using Scottish flax. In 1748, an embargo on the import or use of French cambric provided a further boost to the linen industry. By 1770, Glasgow was the largest linen manufacturer in Britain, and in 1787, Calton, Glasgow was the site of Scotland's first industrial dispute when 7,000 weavers went on strike in protest against a 25% cut in their wages. The 39th Foot were sent in, and three people were killed.\n\nSheer linen, which had then come into vogue, was almost unobtainable in Scotland in the 1780s. In a bid to stay competitive, Glasgow manufacturers turned to fine cotton muslin, at which they succeeded so well that it became cheaper than imported Indian muslins. With the popularity of Indian muslins, from the 1760s onwards, had come a fashion for tambour lace, or sewed muslin, which briefly became a flourishing business in Ayrshire, thanks to the enterprising spirit of Mrs Jamieson.\n\nThe economy, long based on agriculture, began to industrialize after 1790. At first the leading industry, based in the west, was the production of cotton. After the cutting off of supplies of raw cotton from 1861 as a result of the American Civil War the country diversified into engineering, shipbuilding, and locomotive construction, with steel replacing iron after 1870.\n\nFrom about 1790 textiles became the most important industry in the west of Scotland, especially the spinning and weaving of cotton. The first cotton spinning mill was opened at Penicuik in 1778. By 1787 Scotland had 19 mills, 95 by 1795 and there were 192 by 1839. The rise of cotton was the result of a sudden fall in the price of the raw materials, mostly imported from the US, and the availability of a pool of cheap labour caused by population rise and migration. In 1775 137,000 lb of raw cotton were being imported into the Clyde and by 1812 it had increased eightfold to over 11 million lb. The capital invested in the industry increased sevenfold between 1790 and 1840. By 1800, cotton was the main industry in the Glasgow area: New Lanark mills were at the time the largest in the world. Early production was aided by the new technology of the spinning mule, water frame and water power. Steam powered machines were introduced into the industry from 1782. However, only about a third of workers were employed in factories and it continued to rely heavily on the hand loom weaver, working in his own home. In 1790 there were about 10,000 weavers involved in cotton manufacture and by 1800 it was 50,000. The cotton industry flourished until in 1861 the American Civil War cut off the supplies of raw cotton. The industry never recovered, but by that time Scotland had developed heavy industries based on its coal and iron resources.\n\nCoal mining became a major industry, and continued to grow into the twentieth century, producing the fuel to smelt iron, heat homes and factories and drive steam engines locomotives and steamships. Coal mining expanded rapidly in the eighteenth century, reaching 700,000 tons a year by 1750. Most coal was in five fields across the Central Belt. The first Newcomen Steam Engine was introduced into a Scottish colliery in 1719, but water remained the most important source of power for most of the century. With increased demand for household fuel from a growing urban population and the emerging demands of heavy industry, production grew from an estimated 1 million tons a year in 1775, to 3 million by 1830. Production almost doubled by the 1840s and peaked in 1914 at about 42 million tons a year.\n\nInitially increased production was made possible by the introduction of cheap labour, provided from the 1830s by large numbers of Irish immigrants. There were then changes in mining practices, which included the introduction of blasting powder in the 1850s and the use of mechanised methods of transferring the coal to the surface, along with the introduction of steam power in the 1870s. Landed proprietors were replaced by profit-seeking leasehold partnerships and joint-stock companies, whose members were often involved in the emerging iron industry. By 1914 there were a million coal miners in Scotland. The stereotype emerged early on of Scottish colliers as brutish, non-religious and socially isolated serfs; that was an exaggeration, for their life style resembled coal miners everywhere, with a strong emphasis on masculinity, egalitarianism, group solidarity, and support for radical labour movements.\n\nThe invention of James Beaumont Neilson's hot blast process for smelting iron in 1828 revolutionised the Scottish iron industry, allowing abundant native blackstone iron ore to be smelted with ordinary coal. In 1830 Scotland had 27 iron furnaces and by 1840 it was 70, 143 in 1850 and it peaked at 171 in 1860. Output was over 2,500,000 tons of iron ore in 1857, 6.5 per cent of UK output. Output of pig iron rose from 797,000 tons in 1854 to peak at 1,206,00 in 1869. As a result, Scotland became a centre for engineering, shipbuilding and the production of locomotives. In the 1871 census, the workforce in heavy industry overtook textiles in the Strathclyde region and in 1891 it became the majority employer in the country. Toward the end of the nineteenth century, steel production largely replaced iron production.\n\nBritain was the world leader in the construction of railways, and their use to expand trade and coal supplies. The first successful locomotive-powered line in Scotland, between Monkland and Kirkintilloch, opened in 1826. By the late 1830s there was a network of railways that included lines between Dundee and Arbroath, and connecting Glasgow, Paisley and Ayr. The line between Glasgow and Edinburgh, largely designed for passenger transport, opened in 1842 and proved highly successful. By the mid-1840s the mania for railways had begun. A good passenger service established by the late 1840s, and a network of freight lines reduced the cost of shipping coal, making products manufactured in Scotland competitive throughout Britain. The North British Railway was formed in 1844 to link Edinburgh and eastern Scotland with Newcastle and the next year the Caledonian Railway began connecting Glasgow and the west to Carlisle. The creation of a dense network in the Lowlands with connections to England would take until the 1870s to complete.\n\nA series of amalgamations meant that five main companies operated 98 per cent of the system by the 1860s. The capital invested in Scottish railways was £26.6 million in 1859 and by 1900 it had reached £166.1 million. The floatation of railway companies was a major factor in the formation of the Scottish stock exchanges and the rise of share holding in Scotland and after the early stages drew in large amounts of English investment. The travel time between Edinburgh or Glasgow and London was cut from 43 hours to 17 and by the 1880s it had been reduced to 8. Railways opened the London market to Scottish beef and milk. They enabled the Aberdeen Angus to become a cattle breed of worldwide reputation.\n\nShipbuilding on Clydeside (the river Clyde through Glasgow and other points) began when the first small yards were opened in 1712 at the Scott family's shipyard at Greenock. Major firms included Denny of Dumbarton, Scotts Shipbuilding and Engineering Company of Greenock, Lithgows of Port Glasgow, Simon and Lobnitz of Renfrew, Alexander Stephen and Sons of Linthouse, Fairfield of Govan, Inglis of Pointhouse, Barclay Curle of Whiteinch, Connell and Yarrow of Scotstoun. Equally important were the engineering firms that supplied the machinery to drive these vessels, the boilers and pumps and steering gear – Rankin & Blackmore, Hastie's and Kincaid's of Greenock, Rowan's of Finnieston, Weir's of Cathcart, Howden's of Tradeston and Babcock & Wilcox of Renfrew. The biggest customer was Sir William Mackinnon, who ran five shipping companies in the nineteenth century from his base in Glasgow. The Vulcan works, owned by Robert Napier and Sons were the first to start the production of large passenger iron ships in the 1840s.\n\nIn 1835 the Clyde produced only 5 per cent of the ship tonnage built in Britain. The transition from wooden to iron ships was an uneven one, with wooden ships still cheaper to build until around 1850 and the material continued to be used until the 1860s in ships with composite hulls, like the clipper the \"Cutty Sark\", which was launched from Dumbarton in 1869. Cost also delayed the transition from iron to steel shipbuilding and as late as 1879 only 18,000 tons of steel-built shipping was launched on the Clyde, 10 per cent of all tonnage. A similar process occurred in means of propulsion with shifts from sail to steam and back again between the 1840s and the introduction of the more efficient steam turbine engine that became dominant in the mid-1880s. Tonnage increased by more than a factor of six between 1880 and 1914. Production peaked during the First World War and the term \"Clyde-built\" became synonymous with industrial quality.\n\nThe nineteenth century saw some major engineering projects including Thomas Telford's (1757–1834) stone Dean Bridge (1829–31) and iron Craigellachie Bridge (1812–14). In the 1850s the possibilities of new wrought- and cast-iron construction were explored in the building of commercial warehouses in Glasgow. This adopted a round-arched Venetian style first used by Alexander Kirkland (1824–92) at the heavily ornamented 37–51 Miller Street (1854) and translated into iron in John Baird I's Gardner's Warehouse (1855–56), with an exposed iron frame and almost uninterrupted glazing. Most industrial buildings avoided this cast-iron aesthetic, like William Spence's (1806?–83) Elgin Engine Works built in 1856–58, using massive rubble blocks.\n\nThe most important engineering project was the Forth Bridge, a cantilever railway bridge over the Firth of Forth in the east of Scotland, 14 kilometres (9 mi) west of central Edinburgh. Construction of a suspension bridge designed by Thomas Bouch (1822–80), was stopped after the collapse of another of his works, the Tay Bridge in 1847. The project was taken over by John Fowler (1817–98) and Benjamin Baker (1840–1907), who designed a structure that was built by Glasgow-based company Sir William Arrol & Co. from 1883. It was opened on 4 March 1890, and spans a total length of . It was the first major structure in Britain to be constructed of steel; its contemporary, the Eiffel Tower was built of wrought iron.\n\nThe census conducted by the Reverend Alexander Webster in 1755 showed the inhabitants of Scotland as 1,265,380 persons. By the time of the first decadal census in 1801, the population was 1,608,420. It grew steadily in the nineteenth century, to 2,889,000 in 1851 and 4,472,000 in 1901.\n\nWhile population fell in some rural areas as a result of the agricultural revolution, it rose rapidly in the towns. Aberdeen, Dundee and Glasgow all grew by a third or more between 1755 and 1775 and the textile town of Paisley more than doubled its population. Scotland was already one of the most urbanised societies in Europe by 1800. In 1800, 17 per cent of people in Scotland lived in towns of more than 10,000 inhabitants. By 1850 it was 32 per cent and by 1900 it was 50 per cent. By 1900 one in three of the entire population were in the four cities of Glasgow, Edinburgh, Dundee and Aberdeen.\n\nGlasgow emerged as the largest city. Its population in 1780 was 43,000, reaching 147,000 by 1820; by 1901 it had grown to 762,000. This was due to a high birth rate and immigration from the countryside and particularly from Ireland; but from the 1870s there was a fall in the birth rate and lower rates of migration and much of the growth was due to longer life expectancy. Glasgow was now one of the largest cities in the world, and it became known as \"the Second City of the Empire\" after London.\nDundee upgraded its harbour and established itself as an industrial and trading centre. Dundee's industrial heritage was based on \"the three Js\": jute, jam and journalism. East-central Scotland became too heavily dependent on linens, hemp, and jute. Despite the cyclical nature of the trade which periodically ruined weaker companies, profits held up well in the nineteenth century. Typical firms were family affairs, even after the introduction of limited liability in the 1890s. The profits helped make the city an important source of overseas investment, especially in North America. However, the profits were seldom invested locally, apart from the linen trade. The reasons were that low wages limited local consumption, and because there were no important natural resources; thus the Dundee region offered little opportunity for profitable industrial diversification.\n\nThe industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis. Mortality rates were high compared with England and other European nations. Evidence suggests a national death rate of 30 per 1,000 in 1755, 24 in the 1790s and 22 in the early 1860s. Mortality tended to be much higher in urban than rural settlements. The first time these were measured, 1861–82, in the four major cities these were 28.1 per 1,000 and 17.9 in rural areas. Mortality probably peaked in Glasgow in the 1840s, when large inflows of population from the Highlands and Ireland combined population outgrowing sanitary provision and combining with outbreaks of epidemic disease. National rates began to fall in the 1870s, particularly in the cities, as environmental conditions improved. The companies attracted rural workers, as well as immigrants from Catholic Ireland, by inexpensive company housing that was a dramatic move upward from the inner-city slums. This paternalistic policy led many owners to support government sponsored housing programs as well as self-help projects among the respectable working class.\n\nOne of the consequences of industrialisation and urbanisation was the development of a distinct skilled working class. W. H. Fraser argues that the emergence of a class identity can be located to the period before the 1820s, when cotton workers in particular were involved in a series of political protests and events. This led to the Radical War of 1820, in which a declaration of a provisional government by three weavers coincided with a strike by Glasgow cotton workers. The climax of the five-day war was a march from Glasgow Green to Falkirk to take control of the Carron Iron Works. It ended in a cavalry charge by government forces at Bonnymuir. The result was a discouragement of direct political action by workers, although attempts at political reform continued in movements like Chartism and the short hours movement in the 1830s.\n\nFrom the 1830s the political influence of the working classes was expanded through the widening of the franchise, industrial action and the growth and organisation of trade unionism. There were less than 5,000 eligible voters in Scotland before the 1832 Reform Act saw the enlargement of the franchise to include middle class men of business. The 1868 Act brought in skilled artisans and that of 1884 admitted many farm workers, crofters, miners and unskilled men. These changes were supported by trade unions, which developed from the mid-century. Concerted industrial action was undertaken by spinners in the cotton industry in 1836-7 after a collapse of foreign markets led to wage cuts, but was ultimately defeated by the factory owners. The most sustained industrial action was in the mining industry, where the owners controlled employment as well as housing and retail trade through the truck system. In 1887 colliers in the west of Scotland won a major victory over both wages and rents. Scottish trade unionism in the nineteenth century differed from that in the rest of Britain in that unions were often small and highly localised and lacked higher industrial and national organisation. Trade councils were established in Edinburgh in 1853 and in Glasgow in 1858 in an attempt to organise on a regional basis, but were often ineffectual. The new unionism of the last two decades of the century saw the dockers and railwaymen organise a network of regional and national support, but this began to wain towards the end of the century and the situation of union parochialism would remain the dominant mode until union amalgamations got under way after 1914.\n\nThe industrialisation of Scotland had a major impact of the roles of women. Women and girls formed a much higher proportion of the workforce than elsewhere in Britain and were the majority of workers in some industries. The expansion of flax spinning and the rise of linen industry in the eighteenth century was almost totally dependent on female labour and the situation was similar in the sewed muslin industry in the West of Scotland towards the end of the century. When flax spinning became mechanised the proportion of men to women was 100:280, the highest proportion of women in the United Kingdom. In Dundee in the 1840s, while male employment increased by a factor of 1.6, female employment went up by 2.5, making it the only large town in Scotland with the majority of its female population in paid employment. In cotton in the nineteenth century women and girls accounted for 61 per cent of the workforce in Scottish mills, compared with 50 per cent in Lancashire. Although most women were employed in the textile industries, they were also a significant proportion of the workforce in other areas, making up 12 per cent of underground workers in mining, compared with 4 per cent in Britain overall. The expanded opportunities for women, and the extra income they and children brought into the household, probably did the most to help increase living standards for working-class families.\n\nThe role of women in workforce peaked in the 1830s. As heavy industry began to dominate there were less opportunities for women. From the mid-nineteenth century there were a series of laws that limited female roles in industry, beginning with the Mines Regulation Act of 1842, which prevented them from working underground. This put 2,500 women out of work in the east of Scotland, causing real hardship as their contribution to the family economy was vital. This was followed by a series of factory acts that placed restrictions on the employment of women. Many of these acts were brought in because of pressure from trade unions who were attempting to secure a living wage for their male members. Women had relatively little involvement in official trade unions for much of the period of industrialisation. However, they were frequently involved in unofficial disputes, the first being recorded in 1768 and of which there are known to have been 300 strikes that involved women between 1850 and 1914. Towards the end of the century there were increasing attempts to unionise women. The Scottish Women's Trade Council (SWTC) was formed in 1887. From this emerged the Women's Protective and Provident League (WPPL) and the Glasgow Council for Women's Trades (GCWT). In 1893, the National Federal Council of Scotland for Women's Trades (NFCSWT) was established and the Scottish Council for Women's Trades (SCWT) in 1900. By 1895 the NFCSWT alone had an affiliated membership of 100,000.\n\nThe growth of industry resulted in the arrival of large numbers of workers from Ireland who moved into the factories and mines in the 1830s and 1840s. Many were seasonal workers employed as navvies on the construction of docks, canals and then railways. An estimated 60–70 per cent of colliers in Lanarkshire were Irish by the 1840s. The arrivals intensified with the Potato famine of 1845. By the census of 1841, 126,321, or 4.6 per cent of Scotland's population, had been born in Ireland and many more were of Irish descent. Most were concentrated in the west of Scotland, and in Glasgow there were 44,000 people who were born in Ireland, 16 per cent of the city's population. Most Irish immigrants, about three quarters, were Catholic, leading to a major cultural and religious change in Scotland, but a quarter were Protestant, eventually bringing with them institutions like the Orange Order and intensifying a sectarian divide in the major cities.\n\nEven with the growth of industry there were insufficient good jobs, which with major changes in agriculture, meant that during the period 1841–1931, about two million Scots emigrated to North America and Australasia, and another 750,000 Scots relocated to England. Of those who migrated to non-European locations in the century before 1914, 44 per cent went to the US, 28 per cent to Canada, and 25 per cent to Australia and New Zealand. Other important locations included the Caribbean, India and South Africa. By the twenty-first century, there were about as many people who were Scottish Canadians and Scottish Americans as the five million remaining in Scotland. There was little support from the government and in the early stages many migrants agreed to indentures, particularly to the thirteen colonies, that paid for their passage and guaranteed accommodation and work for five or seven years. Later immigration was assisted by agents and societies, such as the Salvation Army, Barnados and the Aberdeen Ladies Union, who often focused on the young or female immigrants.\n"}
{"id": "49366750", "url": "https://en.wikipedia.org/wiki?curid=49366750", "title": "International Society for Technology in Education", "text": "International Society for Technology in Education\n\nThe International Society for Technology in Education (ISTE) is a nonprofit organization that serves educators interested in the use of technology in education. ISTE serves more than 100,000 education stakeholders throughout the world through individual and organizational membership and support services. ISTE provides educational technology resources to support professional learning for educators and education leaders, including the \"ISTE Conference & Expo\"—a worldwide comprehensive ed tech event, and the widely adopted \"ISTE Standards for learning, teaching and leading with technology\". ISTE also provides a suite of professional learning resources to members, including webinars, online courses, consulting services, books, and peer-reviewed journals and publications.\n\nISTE is probably best known for its annual conference (called the ISTE Conference & Expo). The annual conference serves as a forum for exploring and exchanging ideas about education technology with educators from around the world. The event attracts more than 24,000 educators and education leaders, and includes keynote speakers, hundreds of sessions, and a massive expo where vendors can show off the latest ed tech products and services. Recent conferences have been held in Chicago, IL (2018), San Antonio, TX (2017), Denver, CO (2016) Philadelphia, PA (2015) and Atlanta, GA (2014).\n\nThe 2019 conference will be held in Philadelphia, Pennsylvania, June 23–26, 2019. See ISTE Conference 2019 for more information.\n\nThe ISTE Standards (formerly \"National Educational Technology Standards\", NETS) are a framework for implementing digital strategies in education to positively impact learning, teaching and leading. Along with the standards themselves, ISTE offers information and resources to support understanding and implementation of the standards at a variety of levels. See ISTE Standards.\n\nISTE actively advocates for education technology at the local and national levels to advance the global transformation of education through the application of technology to education. We work with educators and policy makers at all levels to try to ensure that all learners have equal access to tools, connectivity and skills needed for success in using technology. See ISTE Advocacy.\n\nISTE membership is extended to individuals, affiliates (organizations, like school districts and state technology organizations), and corporate members interested in the use and application of technology in Education.\n\nIn addition to an individual membership of over 20,000, ISTE has several corporate members, including:\n\nAdobe, Apple, Best Buy, BrainPOP, Canon U.S.A., CDW-G, Cisco Systems, Dell, Google, LEGO Education, Microsoft, Pearson, PowerSchool, SMART Technologies, Summit Learning, the Verizon Foundation, and more\n\nThe International Council for Computers in Education (ICCE) was founded in 1979, with David Moursund as executive officer and editor-in-chief of the organization's organ \"The Computing Teacher\". In 1989 ICCE changed its name to the present name, International Society for Technology in Education (ISTE). Shortly after, in 1990, The \"Computing Teacher\" was retitled \"Learning and Leading with Technology\".\n\n"}
{"id": "27912738", "url": "https://en.wikipedia.org/wiki?curid=27912738", "title": "John Holdren", "text": "John Holdren\n\nJohn Paul Holdren (Sewickley, Pennsylvania, March 1, 1944) is an American scientist who served as the senior advisor to President Barack Obama on science and technology issues through his roles as Assistant to the President for Science and Technology, Director of the White House Office of Science and Technology Policy, and Co-Chair of the President’s Council of Advisors on Science and Technology (PCAST).\n\nHoldren was previously the Teresa and John Heinz Professor of Environmental Policy at the Kennedy School of Government at Harvard University, director of the Science, Technology, and Public Policy Program at the School's Belfer Center for Science and International Affairs, and Director of the Woods Hole Research Center.\n\nHoldren was born in Sewickley, Pennsylvania, and grew up in San Mateo, California. He trained in aeronautics, astronautics and plasma physics and earned a bachelor's degree from the Massachusetts Institute of Technology in 1965 and a Ph.D. from Stanford University in 1970 supervised by Oscar Buneman.\n\nHoldren taught at Harvard for 13 years and at the University of California, Berkeley for more than two decades. His work has focused on the causes and consequences of global environmental change, population control, energy technologies and policies, ways to reduce the dangers from nuclear weapons and materials, and science and technology policy. He has also taken measures to contextualize the United State's current energy challenge, noting the role that nuclear energy could play.\nHoldren was involved in the famous Simon–Ehrlich wager in 1980. He, along with two other scientists helped Paul R. Ehrlich establish the bet with Julian Simon, in which they bet that the price of five key metals would be higher in 1990. The bet was centered around a disagreement concerning the future scarcity of resources in an increasingly polluted and heavily populated world. Ehrlich and Holdren lost the bet, when the price of metals had decreased by 1990.\n\nHoldren was chair of the Executive Committee of the Pugwash Conferences on Science and World Affairs from 1987 until 1997 and delivered the Nobel Peace Prize acceptance lecture on behalf of Pugwash Conferences in December 1995. From 1993 until 2003, he was chair of the Committee on International Security and Arms Control of the National Academy of Sciences, and Co-Chairman of the bipartisan National Committee on Energy Policy from 2002 until 2007. Holdren was elected President of the American Association for the Advancement of Science (AAAS) (2006–2007), and served as board Chairman (2007–2008). He was the founding chair of the advisory board for \"Innovations\", a quarterly journal about entrepreneurial solutions to global challenges published by MIT Press, and has written and lectured extensively on the topic of global warming.\n\nHoldren served as one of President Bill Clinton's science advisors (PCAST) from 1994 to 2001. Eight years later, President Barack Obama nominated Holdren for the position of science advisor and Director of the Office of Science and Technology Policy in December 2008, and he was confirmed on March 19, 2009, by a unanimous vote in the Senate. He testified to the nomination committee that he does not believe that government should have a role in determining optimal population size and that he never endorsed forced sterilization.\n\nOverpopulation was an early concern and interest. In a 1969 article, Holdren and co-author Paul R. Ehrlich argued, \"if the population control measures are not initiated immediately, and effectively, all the technology man can bring to bear will not fend off the misery to come.\" In 1973, Holdren encouraged a decline in fertility to well below replacement in the United States, because \"210 million now is too many and 280 million in 2040 is likely to be much too many.\" In 1977, Paul R. Ehrlich, Anne H. Ehrlich, and Holdren co-authored the textbook \"\".\nOther early publications include \"Energy\" (1971), \"Human Ecology\" (1973), \"Energy in Transition\" (1980), \"Earth and the Human Future\" (1986), \"Strategic Defenses and the Future of the Arms Race\" (1987), \"Building Global Security Through Cooperation (1990)\", and \"Conversion of Military R&D\" (1998).\n\nHoldren also authored over 200 articles and papers and has co-authored and co-edited some 20 books and book-length reports including:\n\nHoldren lives in Falmouth, Massachusetts, with his wife, biologist Cheryl E. Holdren (formerly Cheryl Lea Edgar), with whom he has two children and five grandchildren.\n\n\n"}
{"id": "54496", "url": "https://en.wikipedia.org/wiki?curid=54496", "title": "Lawrence Lessig", "text": "Lawrence Lessig\n\nLester Lawrence Lessig III (born June 3, 1961) is an American academic, attorney, and political activist. He is the Roy L. Furman Professor of Law at Harvard Law School and the former director of the Edmond J. Safra Center for Ethics at Harvard University. Lessig was a candidate for the Democratic Party's nomination for President of the United States in the 2016 U.S. presidential election, but withdrew before the primaries.\n\nLessig is a proponent of reduced legal restrictions on copyright, trademark, and radio frequency spectrum, particularly in technology applications. In 2001, he founded Creative Commons, a non-profit organization devoted to expanding the range of creative works available for others to build upon and to share legally. Prior to his most recent appointment at Harvard, he was a professor of law at Stanford Law School, where he founded the Center for Internet and Society, and at the University of Chicago. He is a former board member of the Free Software Foundation and Software Freedom Law Center; the Washington, D.C. lobbying groups Public Knowledge and Free Press; and the Electronic Frontier Foundation.\n\nAs a political activist, Lessig has called for state-based activism to promote substantive reform of government with a Second Constitutional Convention. In May 2014, he launched a crowd-funded political action committee which he termed Mayday PAC with the purpose of electing candidates to Congress who would pass campaign finance reform. Lessig is also the co-founder of Rootstrikers, and is on the boards of MapLight and Represent.Us. He serves on the advisory boards of the \"Democracy Café\" and the Sunlight Foundation.\n\nIn August 2015, Lessig announced that he was exploring a possible candidacy for President of the United States, promising to run if his exploratory committee raised $1 million by Labor Day. After accomplishing this, on September 6, 2015, Lessig announced that he was entering the race to become a candidate for the 2016 Democratic Party's presidential nomination. Lessig has described his candidacy as a referendum on campaign finance reform and electoral reform legislation. He stated that, if elected, he would serve a full term as president with his proposed reforms as his legislative priorities. He ended his campaign in November 2015, citing rule changes from the Democratic Party that precluded him from appearing in the televised debates.\n\nLessig earned a B.A. degree in economics and a B.S. degree in management (Wharton School) from the University of Pennsylvania, an M.A. degree in philosophy from the University of Cambridge (Trinity) in England, and a J.D. degree from Yale Law School in 1989. After graduating from law school, he clerked for a year for Judge Richard Posner, at the 7th Circuit Court of Appeals in Chicago, Illinois, and another year for Justice Antonin Scalia at the Supreme Court.\n\nLessig started his academic career at the University of Chicago Law School, where he was professor from 1991 to 1997. As co-director of the Center for the Study of Constitutionalism in Eastern Europe there, he helped the newly-independent Republic of Georgia draft a constitution. From 1997 to 2000, he was at Harvard Law School, holding for a year the chair of Berkman Professor of Law, affiliated with the Berkman Klein Center for Internet & Society. He subsequently joined Stanford Law School, where he established the school's Center for Internet and Society.\n\nLessig returned to Harvard in July 2009 as professor and director of the Edmond J. Safra Center for Ethics. In 2013, Lessig was appointed as the Roy L. Furman Professor of Law and Leadership; his chair lecture was titled \"Aaron's Laws: Law and Justice in a Digital Age.\"\n\nLessig was portrayed by Christopher Lloyd in \"The Wake Up Call\", during season 6 of \"The West Wing\".\n\nLessig has been politically liberal since studying philosophy at Cambridge in the mid-1980s. By the late 1980s, two influential conservative judges, Judge Richard Posner and Justice Antonin Scalia, selected him to serve as a law clerk, choosing him for his supposed \"brilliance\" rather than for his ideology and effectively making him the \"token liberal\" on their staffs. Posner would later call him \"the most distinguished law professor of his generation.\"\n\nLessig has emphasized in interviews that his philosophy experience at Cambridge radically changed his values and career path. Previously, he had held strong conservative or libertarian political views, desired a career in business, was a highly active member of Teenage Republicans, served as the Youth Governor for Pennsylvania through the YMCA Youth and Government program in 1978, and almost pursued a Republican political career.\n\nWhat was intended to be a year abroad at Cambridge convinced him instead to stay another two years to complete an undergraduate degree in philosophy and develop his changed political values. During this time, he also traveled in the Eastern Bloc, where he acquired a lifelong interest in Eastern European law and politics.\n\nLessig remains skeptical of government intervention but favors some regulation, calling himself \"a constitutionalist.\" On one occasion, Lessig also commended the John McCain campaign for discussing fair use rights in a letter to YouTube where it took issue with YouTube for indulging overreaching copyright claims leading to the removal of various campaign videos.\n\nIn computer science, \"code\" typically refers to the text of a computer program (the source code). In law, \"code\" can refer to the texts that constitute statutory law. In his 1999 book \"Code and Other Laws of Cyberspace\", Lessig explores the ways in which code in both senses can be instruments for social control, leading to his dictum that \"Code is law.\" Lessig later updated his work in order to keep up with the prevailing views of the time and released the book as \"\" in December 2006.\n\nLessig has been a proponent of the remix culture since the early 2000s. In his 2008 book \"Remix\" he presents this as a desirable cultural practice distinct from piracy. Lessig further articulates remix Culture as intrinsic to technology and the Internet. Remix culture is therefore an amalgam of practice, creativity, \"read/write\" culture and the hybrid economy.\n\nAccording to Lessig, the problem with the remix comes when it is at odds with stringent US copyright law. He has compared this to the failure of Prohibition, both in its ineffectiveness and in its tendency to normalize criminal behavior. Instead he proposes more lenient licensing, namely Creative Commons licenses, as a remedy to maintain \"rule of law\" while combating plagiarism.\n\nOn March 28, 2004 he was elected to the FSF's board of directors. He proposed the concept of \"free culture\". He also supports free and open-source software and open spectrum. At his free culture keynote at the O'Reilly Open Source Convention 2002, a few minutes of his speech was about software patents, which he views as a rising threat to free software, open source software and innovation.\n\nIn March 2006, Lessig joined the board of advisors of the Digital Universe project. A few months later, Lessig gave a talk on the ethics of the Free Culture Movement at the 2006 Wikimania conference. In December 2006, his lecture \" On Free, and the Differences between Culture and Code\" was one of the highlights at 23C3 \"Who can you trust?\".\n\nLessig claimed in 2009 that, because 70% of young people obtain digital information from illegal sources, the law should be changed.\n\nIn a foreword to the Freesouls book project, Lessig makes an argument in favor of amateur artists in the world of digital technologies: \"there is a different class of amateur creators that digital technologies have ... enabled, and a different kind of creativity has emerged as a consequence.\"\n\nLessig is also a well-known critic of copyright term extensions.\n\nLessig has long been known to be a supporter of net neutrality. In 2006, he testified before the US Senate that he believed Congress should ratify Michael Powell's four Internet freedoms and add a restriction to access-tiering, i.e. he does not believe content providers should be charged different amounts. The reason is that the Internet, under the neutral end-to-end design is an invaluable platform for innovation, and the economic benefit of innovation would be threatened if large corporations could purchase faster service to the detriment of newer companies with less capital. However, Lessig has supported the idea of allowing ISPs to give consumers the option of different tiers of service at different prices. He was reported on CBC News as saying that he has always been in favour of allowing internet providers to charge differently for consumer access at different speeds. He said, \"Now, no doubt, my position might be wrong. Some friends in the network neutrality movement as well as some scholars believe it is wrong—that it doesn't go far enough. But the suggestion that the position is 'recent' is baseless. If I'm wrong, I've always been wrong.\"\n\nDespite presenting an anti-regulatory standpoint in many fora, Lessig still sees the need for legislative enforcement of copyright. He has called for limiting copyright terms for creative professionals to five years, but believes that creative professionals' work, many of them independent, would become more easily and quickly available if bureaucratic procedure were introduced to renew trademarks for up to 75 years after this five-year term.\nLessig has repeatedly taken a stance that privatization through legislation like that seen in the 1980s in the UK with British Telecommunications is not the best way to help the Internet grow. He said, \"When government disappears, it's not as if paradise will take its place. When governments are gone, other interests will take their place,\" \"My claim is that we should focus on the values of liberty. If there is not government to insist on those values, then who?\" \"The single unifying force should be that we govern ourselves.\"\n\nFrom 1999 to 2002, Lessig represented a high-profile challenge the Sonny Bono Copyright Term Extension Act. Working with the Berkman Center for Internet and Society, Lessig led the team representing the plaintiff in \"Eldred v. Ashcroft\". The plaintiff in the case was joined by a group of publishers who frequently published work in the public domain and a large number of \"amici\" including the Free Software Foundation, the American Association of Law Libraries, the Bureau of National Affairs, and the College Art Association.\n\nIn March 2003, Lessig acknowledged severe disappointment with his Supreme Court defeat in the Eldred copyright-extension case, where he unsuccessfully tried to convince Chief Justice William Rehnquist, who had sympathies for de-regulation, to back his \"market-based\" approach to intellectual property regulation.\n\nIn August 2013, Lawrence Lessig brought suit against Liberation Music PTY Ltd., after Liberation issued a takedown notice of one of Lessig's lectures on YouTube which had used the song \"Lisztomania\" by the band Phoenix, whom Liberation Music represents. Lessig sought damages under section 512(f) of the Digital Millennium Copyright Act, which holds parties liable for misrepresentations of infringement or removal of material. Lessig was represented by the Electronic Frontier Foundation and Jones Day. In February 2014, the case ended with a settlement in which Liberation Music admitted wrongdoing in issuing the takedown notice, issued an apology, and paid a confidential sum in compensation.\n\nIn October 2014, \"Killswitch\", a film featuring Lawrence Lessig, as well as Aaron Swartz, Tim Wu, and Edward Snowden received its World Premiere at the Woodstock Film Festival, where it won the award for Best Editing. In the film, Lessig frames the story of two young hacktivists, Swartz and Snowden, who symbolize the disruptive and dynamic nature of the Internet. The film reveals the emotional bond between Lessig and Swartz, and how it was Swartz (the mentee) that challenged Lessig (the mentor) to engage in the political activism that has led to Lessig's crusade for campaign finance reform.\n\nIn February 2015, \"Killswitch\" was invited to screen at the Capitol Visitor's Center in Washington DC by Congressman Alan Grayson. The event was held on the eve of the Federal Communications Commission's historic decision on Net Neutrality. Lessig, Congressman Grayson, and Free Press (organization) CEO Craig Aaron spoke about the importance of protecting net neutrality and the free and open Internet.\n\nCongressman Grayson states that Killswitch is \"One of the most honest accounts of the battle to control the Internet -- and access to information itself.\". Richard von Busack of the Metro Silicon Valley, writes of \"Killswitch\", \"Some of the most lapidary use of found footage this side of The Atomic Café\". Fred Swegles of the Orange County Register, remarks, \"Anyone who values unfettered access to online information is apt to be captivated by \"Killswitch\", a gripping and fast-paced documentary.\" Kathy Gill of GeekWire asserts that \"\"Killswitch\" is much more than a dry recitation of technical history. Director Ali Akbarzadeh, producer Jeff Horn, and writer Chris Dollar created a human centered story. A large part of that connection comes from Lessig and his relationship with Swartz.\"\n\nIn December 2016 Lawrence Lessig and Laurence Tribe established The Electors Trust under the aegis of EqualCitizens.US to provide pro bono legal counsel as well as a secure communications platform for those of the 538 members of the United States Electoral College who are regarding a vote of conscience against Donald Trump in the presidential election \n\nAt the iCommons iSummit 07, Lessig announced that he would stop focusing his attention on copyright and related matters and work on political corruption instead, as the result of a transformative conversation with Aaron Swartz, a young internet prodigy whom Lessig met through his work with Creative Commons. This new work was partially facilitated through his wiki, Lessig Wiki, which he has encouraged the public to use to document cases of corruption. Lessig criticized the revolving door phenomenon in which legislators and staffers leave office to become lobbyists and have become beholden to special interests.\n\nIn February 2008, a Facebook group formed by law professor John Palfrey encouraged him to run for Congress from California's 12th congressional district, the seat vacated by the death of U.S. Representative Tom Lantos. Later that month, after forming an \"exploratory project\", he decided not to run for the vacant seat.\n\nDespite having decided to forgo running for Congress himself, Lessig remained interested in attempting to change Congress to reduce corruption. To this end, he worked with political consultant Joe Trippi to launch a web based project called \"Change Congress\". In a press conference on March 20, 2008, Lessig explained that he hoped the Change Congress website would help provide technological tools voters could use to hold their representatives accountable and reduce the influence of money on politics. He is a board member of MAPLight.org, a nonprofit research group illuminating the connection between money and politics.\n\nChange Congress later became \"Fix Congress First\", and was finally named Rootstrikers. In November 2011, Lessig announced that Rootstrikers would join forces with Dylan Ratigan's \"Get Money Out\" campaign, under the umbrella of the United Republic organization. Rootstrikers subsequently came under the aegis of Demand Progress, an organization co-founded by Aaron Swartz.\n\nIn 2010, Lessig began to organize for a national Article V convention. He co-founded \"Fix Congress First!\" with Joe Trippi. In a speech in 2011, Lessig revealed that he was disappointed with Obama's performance in office, criticizing it as a \"betrayal\", and he criticized the president for using \"the (Hillary) Clinton playbook\". Lessig has called for state governments to call for a national Article V convention, including by supporting Wolf PAC, a national organization attempting to call an Article V convention to address the problem. The convention Lessig supports would be populated by a \"random proportional selection of citizens\" which he suggested would work effectively. He said \"politics is a rare sport where the amateur is better than the professional.\" He promoted this idea at a September 24–25, 2011, conference he co-chaired with the Tea Party Patriots' national coordinator, in Lessig's October 5, 2011, book, \"Republic, Lost: How Money Corrupts Congress—and a Plan to Stop It,\" and at the Occupy protest in Washington, DC. Reporter Dan Froomkin said the book offers a manifesto for the Occupy Wall Street protestors, focusing on the core problem of corruption in both political parties and their elections. An Article V convention does not dictate a solution, but Lessig would support a constitutional amendment that would allow legislatures to limit political contributions from non-citizens, including corporations, anonymous organizations, and foreign nationals, and he also supports public campaign financing and electoral college reform to establish the one person, one vote principle.\n\nThe New Hampshire Rebellion is a walk to raise awareness about corruption in politics. The event began in 2014 with a 185-mile march in New Hampshire. In its second year the walk expanded to include other locations in New Hampshire.\n\nFrom January 11 to 24, 2014, Lessig and many others, like New York activist Jeff Kurzon, marched from Dixville Notch, New Hampshire to Nashua (a 185-mile march) to promote the idea of tackling \"the systemic corruption in Washington\". Lessig chose this language over the related term \"campaign finance reform,\" commenting that \"Saying we need campaign finance reform is like referring to an alcoholic as someone who has a liquid intake problem.\" The walk was to continue the work of NH Native Doris \"Granny D\" Haddock, and in honor of deceased activist Aaron Swartz. The New Hampshire Rebellion marched 16 miles from Hampton to New Castle on the New Hampshire Seacoast. The initial location was also chosen because of its important and visible role in the quadrennial \"New Hampshire primaries\", the traditional first primary of the presidential election.\n\nLessig announced the launch of his presidential campaign on September 6, 2015.\nOn August 11, 2015, Lessig announced that he had launched an exploratory campaign for the purpose of exploring his prospects of winning the Democratic Party's nomination for President of the United States in the 2016 election. Lessig pledged to seek the nomination if he raised $1 million by Labor Day 2015. The announcement was widely reported in national media outlets, and was timed to coincide with a media blitz by the Lessig 2016 Campaign. Lessig was interviewed in \"The New York Times\" and Bloomberg. Campaign messages and Lessig's electoral finance reform positions were circulated widely on social media. His campaign was focused on a single issue: The Citizen Equality Act, a proposal that couples campaign finance reform with other laws aimed at curbing gerrymandering and ensuring voting access. As an expression of his commitment to the proposal, Lessig initially promised to resign once the Citizen Equality Act became law and turn the presidency over to his vice president, who would then serve out the remainder of the term as a typical American president and act on a variety of issues. In October 2015, Lessig abandoned his automatic resignation plan and adopted a full policy platform for the presidency, though he did retain the passage of the Citizen Equality Act as his primary legislative objective.\n\nHe announced the end of his campaign on November 2, 2015.\n\nIn 2017, Lessig announced a movement to challenge the winner-take-all Electoral College vote allocation in the various states, called \"Equal Votes\". Lessig is counsel for plaintiffs in \"Nemanich v. Williams\", an electoral law case in Colorado.\n\nIn 2002, Lessig received the Award for the Advancement of Free Software from the Free Software Foundation (FSF). He also received the Scientific American 50 Award for having \"argued against interpretations of copyright that could stifle innovation and discourse online.\" Then, in 2006, Lessig was elected to the American Academy of Arts and Sciences.\n\nIn 2011, Lessig was named to the Fastcase 50, \"honoring the law's smartest, most courageous innovators, techies, visionaries, and leaders.\" Lessig was awarded honorary doctorates by the Faculty of Social Sciences at Lund University, Sweden in 2013 and by l'Université Catholique de Louvain in 2014. Lessig received the 2014 Webby Lifetime Achievement award for co-founding Creative Commons and defending net neutrality and the free and open software movement.\n\nLessig was born in Rapid City, South Dakota, the son of Patricia, who sold real estate, and Lester L. \"Jack\" Lessig, an engineer. He grew up in Williamsport, Pennsylvania.\n\nIn May 2005, it was revealed that Lessig had experienced sexual abuse by the director at the American Boychoir School, which he had attended as an adolescent. Lessig reached a settlement with the school in the past, under confidential terms. He revealed his experiences in the course of representing another student victim, John Hardwicke, in court. In August 2006, he succeeded in persuading the New Jersey Supreme Court to restrict the scope of immunity radically, which had protected nonprofits that failed to prevent sexual abuse from legal liability.\n\nLessig is married to Bettina Neuefeind, a German-born Harvard University colleague. The two married in 1999. He and Neuefeind have three children: Willem, Teo, and Tess.\n\n\n\n\n\n"}
{"id": "2498045", "url": "https://en.wikipedia.org/wiki?curid=2498045", "title": "Line shaft", "text": "Line shaft\n\nA line shaft is a power driven rotating shaft for power transmission that was used extensively from the Industrial Revolution until the early 20th century. Prior to the widespread use of electric motors small enough to be connected directly to each piece of machinery, line shafting was used to distribute power from a large central power source to machinery throughout a workshop or an industrial complex. The central power source could be a water wheel, turbine, windmill, animal power or a steam engine. Power was distributed from the shaft to the machinery by a system of belts, pulleys and gears known as \"millwork\".\n\nA typical line shaft would be suspended from the ceiling of one area and would run the length of that area. One pulley on the shaft would receive the power from a parent line shaft elsewhere in the building. The other pulleys would supply power to pulleys on each individual machine or to subsequent line shafts. In manufacturing where there were a large number of machines performing the same tasks, the design of the system was fairly regular and repeated. In other applications such as machine and wood shops where there was a variety of machines with different orientations and power requirements, the system would appear erratic and inconsistent with many different shafting directions and pulley sizes. Shafts were usually horizontal and overhead but occasionally were vertical and could be underground. Shafts were usually rigid steel, made up of several parts bolted together at flanges. The shafts were suspended by hangers with bearings at certain intervals of length. The distance depended on the weight of the shaft and the number of pulleys. The shafts had to be kept aligned or the stress would overheat the bearings and could break the shaft. The bearings were usually friction type and had to be kept lubricated. Pulley lubricator employees were required in order to ensure that the bearings did not freeze or malfunction.\n\nIn the earliest applications power was transmitted between pulleys using loops of rope on grooved pulleys. This method is extremely rare today, dating mostly from the 18th century. Flat belts on flat pulleys or drums were the most common method during the 19th and early 20th centuries. The belts were generally tanned leather or cotton duck impregnated with rubber. Leather belts were fastened in loops with rawhide or wire lacing, lap joints and glue, or one of several types of steel fasteners. Cotton duck belts usually used metal fasteners or were melted together with heat. The leather belts were run with the hair side against the pulleys for best traction. The belts needed periodic cleaning and conditioning to keep them in good condition. Belts were often twisted 180 degrees per leg and reversed on the receiving pulley to cause the second shaft to rotate in the opposite direction.\n\nPulleys were constructed of wood, iron, steel or a combination thereof. Varying sizes of pulleys were used in conjunction to change the speed of rotation. For example a 40\" pulley at 100 rpm would turn a 20\" pulley at 200 rpm. Pulleys solidly attached (\"fast\") to the shaft could be combined with adjacent pulleys that turned freely (\"loose\") on the shaft (idlers). In this configuration the belt could be maneuvered onto the idler to stop power transmission or onto the solid pulley to convey the power. This arrangement was often used near machines to provide a means of shutting the machine off when not in use. Usually at the last belt feeding power to a machine, a pair of stepped pulleys could be used to give a variety of speed settings for the machine.\n\nOccasionally gears were used between shafts to change speed rather than belts and different sized pulleys, but this seems to have been relatively uncommon.\n\nEarly versions of line shafts date back into the 18th century, but they were in widespread use in the late 19th century with industrialization. Line shafts were widely used in manufacturing, woodworking shops, machine shops, saw mills and grist mills.\n\nIn 1828 in Lowell, Massachusetts, Paul Moody substituted leather belting for metal gearing to transfer power from the main shaft running from a water wheel. This innovation quickly spread in the U.S.\n\nFlat belt drive systems became popular in the UK from the 1870s, with the firms of J & E Wood and W & J Galloway & Sons prominent in their introduction. Both of these firms manufactured stationary steam engines and the continuing demand for more power and reliability could be met not merely by improved engine technology but also improved methods of transferring power from the engines to the looms and similar machinery which they were intended to service. The use of flat belts was already common in the US but rare in Britain until this time. The advantages included less noise and less wasted energy in the friction losses inherent in the previously common drive shafts and their associated gearing. Also, maintenance was simpler and cheaper, and it was a more convenient method for the arrangement of power drives such that if one part were to fail then it would not cause loss of power to all sections of a factory or mill. These systems were in turn superseded in popularity by rope drive methods.\n\nNear the end of the 19th century some factories had a mile or more of line shafts in a single building.\n\nIn order to provide power for small shops and light industry, specially constructed \"power buildings\" were constructed. Power buildings used a central steam engine and distributed power through line shafts to all the leased rooms. Power buildings continued to be built in the early days of electrification, still using line shafts but driven by an electric motor.\n\nAs some factories grew too large and complex to be powered by a single steam engine, a system of \"sub divided\" power came into use. This was also important when a wide range of speed control was necessary for a sensitive operation such as wire drawing or hammering iron. Under sub divided power, steam was piped from a central boiler to smaller steam engines located where needed. However, small steam engines were much less efficient than large ones. The Baldwin Locomotive Works 63 acre site changed to sub divided power, then because of the inefficiency converted to group drive with several large steam engines driving the line shafts. Eventually Baldwin converted to electric drive, with a substantial saving in labor and building space.\n\nWith factory electrification in the early 1900s, many line shafts began converting to electric drive. In early factory electrification only large motors were available, so new factories installed a large motor to drive line shafting and millwork. After 1900 smaller industrial motors became available and most new installations used individual electric drives.\n\nSteam turbine powered line shafts were commonly used to drive paper machines for speed control reasons until economical methods for precision electric motor speed control became available in the 1980s; since then many have been replaced with sectional electric drives. Economical variable speed control using electric motors was made possible by silicon controlled rectifiers (SCRs) to produce direct current and variable frequency drives using inverters to change DC back to AC at the frequency required for the desired speed.\n\nMost systems were out of service by the mid-20th century and relatively few remain in the 21st century, even fewer in their original location and configuration.\n\nCompared to individual electric motor or \"unit\" drive, line shafts have the following disadvantages: \n\nFirms switching to electric power showed significantly less employee sick time, and, using the same equipment, showed significant increases in production.\n\n“We can scarcely step into a shop or factory of any description without encountering a mass of belts which seem at first to monopolize every nook in the building and leave little or no room for anything else”.\n\nTo overcome the distance and friction limitations of line shafts, wire rope systems were developed in the late 19th century. Wire rope operated at higher velocities than line shafts and were a practical means of transmitting mechanical power for a distance of a few miles or kilometers. They used widely spaced, large diameter wheels and had much lower friction loss than line shafts, and had one-tenth the initial cost.\n\nTo supply small scale power that was impractical for individual steam engines, central station hydraulic systems were developed. Hydraulic power was used to operate cranes and other machinery in British ports and elsewhere in Europe. The largest hydraulic system was in London. Hydraulic power was used extensively in Bessemer steel production\n\nThere were also some central stations providing pneumatic power in the late 19th century.\n\nIn an early example, Jedediah Strutt's water-powered cotton mill, North Mill in Belper, built in 1776, all the power to operate the machinery came from an water wheel.\n\n\n\n\n\n\n\n"}
{"id": "7822632", "url": "https://en.wikipedia.org/wiki?curid=7822632", "title": "MTV-1", "text": "MTV-1\n\nThe MTV-1 Micro TV was the second model of a near pocket-sized television. The first was the Panasonic IC model TR-001 introduced in 1970. The MTV-1 was developed by Clive Sinclair (Sinclair Radionics Ltd). It was shown to the public at trade shows in London and Chicago in January, 1977, and released for sale in 1978. Development spanned 10 years and included a cash infusion of (about ) from the UK government in 1976.\n\nThe MTV-1 used a German AEG Telefunken black-and-white, electrostatic cathode ray tube (CRT), the smallest CRT built into a commercially available product, and included a rechargeable 4-AA-cell NiCad battery pack. It measured and weighed . It was able to receive either PAL or NTSC transmissions on VHF or UHF. A Welsh company, Wolsey Electronics, manufactured it for Sinclair. Custom ICs made by Texas Instruments and Sinclair contributed to its small size and low power consumption.\n\nThe original (about ) price tag proved to be too high to sell many of them, and Sinclair lost over in 1978, eventually selling its remaining inventory to liquidators at greatly reduced prices.\n\nThe MTV-1B, released later in 1978 at the much lower price of , was able to receive only British and South African UHF PAL signals.\n\n\n"}
{"id": "7480035", "url": "https://en.wikipedia.org/wiki?curid=7480035", "title": "Machine Age", "text": "Machine Age\n\nThe Machine Age is an era that includes the early 20th century, sometimes also including the late 19th century. An approximate dating would be about 1880 to 1945. Considered to be at a peak in the time between the first and second world wars, it forms a late part of the Second Industrial Revolution. The 1940s saw the beginning of the Atomic Age, where modern physics saw new applications such as the atomic bomb, the first computers, and the transistor. The Digital Revolution ended the intellectual model of the machine age founded in the mechanical and heralding a new more complex model of high technology. The digital era has been called the Second Machine Age, with its increased focus on machines that do mental tasks.\n\nArtifacts of the Machine Age include:\n\n\n\n\nThe Machine Age is considered to have influenced:\n\n"}
{"id": "47972925", "url": "https://en.wikipedia.org/wiki?curid=47972925", "title": "Managed Mobility Services", "text": "Managed Mobility Services\n\nManaged Mobility Services (MMS) is a term used by analysts and businesses to describe the outsourcing and managing services that many businesses provide.\n\nMobility Managed Services includes the IT and process management service needed for a company to acquire, provision and support smartphones, tablets and other field force devices. These services are designed to support devices for corporations are liable and provide a level of control to companies that support them by accessing corporate resources and information.\n\nManaged Mobility Services has existed for some time, but organizations have increasingly shifted responsibility for logistics and management as the environments have become more diverse and updates more frequent. Android fragmentation is sometimes cited as a driver of this growth as is the consumerization of IT, including the adoption of mobile devices by IT departments. This accumulation of influences has been referred to as \"The 3 V's\": Volume (the number of devices and users involved); Variety (policy changes in order to fit changing standards) and Volatility (high rates of change that could threaten the business).\n\nGartner includes the following categories of services:\n\nGartner first officially produced research on MMS in 2011 with Critical Capabilities for Managed Mobility Services, 22 December 2011, G00225198 Analyst(s): Eric Goodness, Phillip Redman.\n\nSince then Gartner, Forrester Research, GigaOM and other analyst organizations have published research on these services, each with slight variations on what services are included.\n\n"}
{"id": "709958", "url": "https://en.wikipedia.org/wiki?curid=709958", "title": "Matrioshka brain", "text": "Matrioshka brain\n\nA matrioshka brain is a hypothetical megastructure proposed by Robert Bradbury, based on the Dyson sphere, of immense computational capacity. It is an example of a Class B stellar engine, employing the entire energy output of a star to drive computer systems. \nThis concept derives its name from the nesting Russian Matryoshka dolls.\nThe concept was deployed by its inventor, Robert Bradbury, in the anthology \"Year Million: Science at the Far Edge of Knowledge\", and attracted interest from reviewers in the \"Los Angeles Times\" and the \"Wall Street Journal\".\n\nThe concept of a matrioshka brain comes from the idea of using Dyson spheres to power an enormous, star-sized computer. The term \"matrioshka brain\" originates from matryoshka dolls, which are wooden Russian nesting dolls. Matrioshka brains are composed of several Dyson spheres nested inside one another, the same way that matryoshka dolls are composed of multiple nested doll components. The innermost Dyson sphere of the matrioshka brain would draw energy directly from the star it surrounds and give off large amounts of waste heat while computing at a high temperature. The next surrounding Dyson sphere would absorb this waste heat and use it for its computational purposes, all while giving off waste heat of its own. This heat would be absorbed by the next sphere, and so on, with each sphere giving off less heat than the one before it. For this reason, matrioshka brains with more nested Dyson spheres would tend to be more efficient, as they would waste less heat energy. The inner shells could run at nearly the same temperature as the star itself, while the outer ones would be close to the temperature of interstellar space. The engineering requirements and resources needed for this would be enormous.\n\nThe term \"matrioshka brain\" was invented by Robert Bradbury as an alternative to the \"Jupiter brain\"—a concept similar to the matrioshka brain, but on a smaller planetary scale and optimized for minimal signal propagation delay. A matrioshka brain design is concentrated on sheer capacity and the maximum amount of energy extracted from its source star, while a Jupiter brain is optimized for computational speed.\n\nSome possible uses of such an immense computational resource have been proposed. One idea suggested by Charles Stross, in his novel \"Accelerando\", would be to use it to run perfect simulations or uploads of human minds into virtual reality spaces supported by the Matrioshka brain. Stross even went so far as to suggest that a sufficiently powerful species utilizing enough raw processing power could launch attacks upon, and manipulate, the structure of the universe itself.\nIn \"Godplayers\" (2005), Damien Broderick surmises that a matrioshka brain would allow simulating entire alternate universes.\nThe futurist and transhumanist author Anders Sandberg wrote an essay speculating on implications of computing on the massive scale of machines such as the matrioshka brain, published by the Institute for Ethics and Emerging Technologies.\nMatrioshka brains and other megastructures are a common theme in the fictional Orion's Arm universe where they are used by superintelligences as processing nodes connected via artificial wormholes.\n\n\n"}
{"id": "2234982", "url": "https://en.wikipedia.org/wiki?curid=2234982", "title": "Message-waiting indicator", "text": "Message-waiting indicator\n\nA message-waiting indicator (MWI) in telephony, is a Telcordia Technologies (formerly Bellcore) term for an FSK-based telephone calling feature that illuminates an LED on selected telephones to notify a telephone user of waiting voicemail messages on most North American public telephone networks and PBXs.\n\nAs described in Telcordia Generic Requirements document GR-283-CORE, a Message_Waiting_Indicator (MWI) is a mechanism that informs the subscriber about the status of recorded messages. The subscriber may subscribe to a notification feature that makes use of the status of this MWI.\n\nThis feature is also frequently called (and abbreviated) as visual message waiting indicator (VMWI). A VMWI, as defined in Telcordia GR-1401-CORE, is a stored program controlled switching (SPCS) system feature that activates and deactivates a visual indicator on customer-premises equipment (CPE) to notify the customer that new messages are waiting. VMWI differs from existing features that use other message indicators, such as audible stuttered dial tone, in that it activates a visual indicator on the CPE. The visual indicator may be as simple as lighting or flashing a light-emitting diode (LED), or as advanced as displaying a special message on a liquid-crystal display (LCD).\n\nThis technology was invented by Jerome (Jerry) Schull and Wayne Howe at BellSouth's Advanced Technology R&D group in 1992 and was issued as US Patents #5,363,431 and #5,521,964. It was introduced in 1995, with the introduction of CLASS-based calling features and ADSI. It was at one time only compatible with ADSI-compliant telephones but is now compatible with any customer premises equipment (CPE) that simply responds visually to visual FSK.\n\nThis service is often erroneously associated with the abilities of most Caller ID standalone set-top boxes. Caller ID boxes manufactured after 1998 feature an LED that blinks green to notify that new calls have been recorded and red to indicate that a subscriber has new voicemail messages waiting. Some units also display the text \"MESSAGE WAITING\" (similar to ADSI-compliant telephones). These units do not use visual FSK to activate their red LEDs, but instead, they briefly \"pick-up\" the line at certain intervals (normally, within two minutes of a new call) to check for a \"stuttered\" dial tone. The presence of a stutter dial tone activates a red LED; while absence deactivates it.\n\nFor mobile phones, the message-waiting indicator is sent via a Short Message Service (SMS) message — the same system used for texting. (SMS was actually invented for utilitarian uses like this, not for user conversation.) It not only indicates that a message is waiting, but also how many unheard messages there are on the voicemail server for that telephone number. In the event that a phone is deactivated, out of range, or otherwise removed from the network, there is often no way to clear this indicator until another message is recorded to the system, causing the MWI message to be sent again once the phone reconnects. The customer service call center for the mobile network operator may also be able to provide simple technical support to reset this by resending the MWI message manually.\n"}
{"id": "10133876", "url": "https://en.wikipedia.org/wiki?curid=10133876", "title": "Mobile architecture", "text": "Mobile architecture\n\nIn the past computers needed to be disconnected from their internal network if they needed to be taken or moved anywhere. \"Mobile architecture\" allows maintaining this connection whilst during transit. Each day the number of mobile devices is increasing, mobile architecture is the pieces of technology needed to create a rich, connected user experience. \nCurrently there is a lack of uniform interoperability plans and implementation. There is a lack of common industry view on architectural framework. This increases costs and slows down 3rd party mobile development. An open approach is required across all industries to achieve same end results and services.\n\n\nThe basic and detail architecture of the Mobile device consists of Hardware and Software architecture. The main hardware components\nof the mobile phone is the application processor that controls all other components of the device such as display, keypad, power, audio , video etc. The radio signals are handled by base band\nprocessor which in turn communicates with other processors to use their functionality. Power and audio processor controls the functioning of speaker and microphone with the help of application\nprocessor. Subscriber Identification Module (SIM) contains the details about the subscriber.\n\n\nA consortium of companies are pushing for products and services to be based on open, global standards, protocols and interfaces and are not locked to proprietary technologies. The applications layer to be bearer agnostic (examples: GSM, GPRS, EDGE, CDMA, UMTS). The architecture framework and service enablers will be independent of operating systems. There will be support for inseparability of applications and platforms, seamless geographic and inter-generational roaming.\n\n\nAlso:\nMobile Architecture – architecture, a tipe of building or building components;(transform, move, adapt, interact)\n"}
{"id": "1544319", "url": "https://en.wikipedia.org/wiki?curid=1544319", "title": "Nanosocialism", "text": "Nanosocialism\n\nNanosocialism refers generally to a set of economic theories of social organization advocating state or collective ownership and administration of the research, development and use of nanotechnology.\n\nNanosocialism is a stance that favors participatory politics to guide state intervention in the effort to manage the transition to a society revolutionized by molecular nanotechnology.\n\n\"Nanosocialism\" is a term coined by David M. Berube, the associate director of Nanoscience and Technology Studies at the USC NanoCenter, who argues that nanotechnological projections need to be tempered by technorealism about the implications of nanotechnology in a technocapitalist society, but that its applications also offer enormous opportunities for economic abundance and social progress.\n\nIn the role-playing game \"Transhuman Space\", nanosocialism is described as a descendant of infosocialism, in which intellectual property is nationalized and freely distributed by the state. It is adopted by some developing nations to counter the hold corporations from wealthier nations have on copyrights and patents. This fictional version of nanosocialism was coined by David L. Pulver, the game's creator, who was unaware that the term had already been used by Berube.\n\n"}
{"id": "2165622", "url": "https://en.wikipedia.org/wiki?curid=2165622", "title": "Neophile", "text": "Neophile\n\nNeophile or Neophiliac, a term popularised by cult writer Robert Anton Wilson, is a personality type characterized by a strong affinity for novelty. The term was used earlier by Christopher Booker in his book The Neophiliacs (1969), and by J. D. Salinger in his short story Hapworth 16, 1924 (1965).\n\nNeophiles/Neophiliacs have the following basic characteristics:\n\n\nA neophile is distinct from a revolutionary in that anyone might become a revolutionary if pushed far enough by the reigning authorities or social norms, whereas neophiles are revolutionaries by nature. Their intellectual abhorrence of tradition and repetition usually bemoans a deeper emotional need for constant novelty and change. The meaning of neophile approaches and is not mutually exclusive to the term visionary, but differs in that a neophile actively seeks first-hand experience of novelty rather than merely pontificating about it.\n\nThe opposite of a neophile is a neophobe; a person with an aversion to novelty and change. Wilson observes that neophobes tend to regard neophiles, especially extreme ones, with fear and contempt, and to brand them with titles such as \"witch,\" \"satanist,\" \"heretic,\" etc. He also speculates in his Prometheus Rising series of books that the industrial revolution and related enlightenment represents one of the first periods of history in which neophiles were a dominant force in society. Neophiles accelerate change because they like it that way.\n\nOpen-source advocate and programmer Eric S. Raymond observes that this personality is especially prevalent in certain fields of expertise; in business, these are primarily computer science and other areas of high technology. Raymond speculates that the rapid progress of these fields (especially computers) is a result of this. A neophile's love of novelty is likely to lead them into subjects outside of the normal areas of human interest. Raymond observes a high concentration of neophiles in or around what he calls \"leading edge subcultures\" such as science fiction fandom, neo-paganism, transhumanism, etc. as well as in or around nontraditional areas of thought such as fringe philosophy or the occult. Raymond observes that most neophiles have roving interests and tend to be widely well-read.\n\nThere is more than one type of neophile. There are social neophiles (the extreme social butterfly), intellectual neophiles (the revolutionary philosopher and the technophile), and physical/kinetic neophiles (the extreme sports enthusiast). These tendencies are not mutually exclusive, and might exist simultaneously in the same individual.\n\nThe word \"neophilia\" has particular significance in Internet and hacker culture. \"The New Hacker's Dictionary\" gave the following definition to neophilia:\n\nThe trait of being excited and pleased by novelty. Common among most hackers, SF fans, and members of several other connected leading-edge subcultures, including the pro-technology 'Whole Earth' wing of the ecology movement, space activists, many members of Mensa, and the Discordian/neo-pagan underground (see geek). All these groups overlap heavily and (where evidence is available) seem to share characteristic hacker tropisms for science fiction, music.\n\nResearch has uncovered a possible link between certain predisposition to some kind of neophilia and increased levels of the enzyme monoamine oxidase A.\n\n\n"}
{"id": "46972538", "url": "https://en.wikipedia.org/wiki?curid=46972538", "title": "Online shaming", "text": "Online shaming\n\nOnline shaming is a form of Internet vigilantism in which targets are publicly humiliated using technology like social and new media. Proponents of shaming see it as a form of online participation that allows hacktivists and cyber-dissidents to right injustices. Critics see it as a tool that encourages online mobs to destroy the reputation and careers of people or organizations who made perceived slights.\n\nOnline shaming frequently involves the publication of private information on the Internet (called doxing), which can frequently lead to hate messages and death threats being used to intimidate that person. The ethics of public humiliation has been a source of debate over privacy and ethics.\n\nThe social networking tools of the Internet have been used as a tool to easily and widely publicize instances of perceived anti-social behavior.\n\nDavid Furlow, chairman of the Media, Privacy and Defamation Committee of the American Bar Association, has identified the potential privacy concerns raised by websites facilitating the distribution of information that is not part of the public record (documents filed with a government agency), and has said that such websites \"just [give] a forum to people whose statements may not reflect truth.\"\n\nAfter some controversial incidents of public shaming, the popular link-sharing and discussion website Reddit introduced a strict rule against the publication of non-public personally-identifying information via the site (colloquially known on Reddit and elsewhere as \"doxing\"). Those who break the rule are subject to a site-wide ban, and their posts and even entire communities may be removed for breaking the rule.\n\nIn 2015, online shaming was the subject of the book \"So You've Been Publicly Shamed\" by Jon Ronson. Ronson documented how people had become agoraphobic due to humiliation online for misinterpreted jokes, and says people should think twice before gleefully condemning someone for doing almost nothing wrong.\n\nDoxing involves researching and broadcasting personally identifiable information about an individual, often with the intention of harming that person. This can often lead to extortion, coercion, harassment and other forms of abuse. On February 1, 2017, Reddit, a social news website, has banned two alt-right communities, r/altright and r/alternativeright for doxing and violating Reddit community guidelines.\n\nNonconsensual pornography is a form of sexually explicit recording publicized on the Internet in order to humiliate a person, frequently distributed by computer hackers or ex-partners (called revenge porn). Images and video of sexual acts are often combined with doxing of a person's private details, such as their home addresses and workplaces. Victims' lives can be ruined as a result, the victims exposed to cyber-stalking and physical attack as well as facing difficulties in their workplace should their images become known as a result of routine checks by employers. Some have lost their jobs, while others have been unable to find work at all.\n\nProducts frequently attract negative reviews on Goodreads, Amazon and other online commerce websites.\n\nIn many cases, users of Yelp write reviews in order to lash out at corporate interests or businesses they dislike. During the Chick-fil-A same-sex marriage controversy, activists encouraged a consumer boycott of Chick-fil-A and left negative reviews of the site's locations on restaurant rating websites after the founder declared that corporate profits would be donated to political causes opposing same-sex marriage in the United States. In 2015 an Indiana pizzeria was swarmed with negative Yelp reviews after the owner said it wouldn't cater gay weddings. Similar reactions have frequently followed bakers refusing to make cakes for gay weddings. After Cecil the lion was shot by an American recreational big-game hunter, his business was flooded with negative reviews.\n\nVarious governments have used \"name and shame\" policies to punish tax evasion, environmental violations and minor crimes like littering.\n\nIn July 2015, a group hacked the user data of Ashley Madison, a commercial dating website marketed as helping people have extramarital affairs. In August 2015, over 30 million user account details, including names and email addresses were released publicly.\n\nA variety of security researchers and Internet privacy activists debated the ethics of the release.\n\nClinical psychologists argued that dealing with an affair in a particularly public way increases the hurt for spouses and children. Carolyn Gregoire argued \"[s]ocial media has created an aggressive culture of public shaming in which individuals take it upon themselves to inflict psychological damage\" and more often than not, \"the punishment goes beyond the scope of the crime.\" Charles J. Orlando, who had joined the site to conduct research concerning women who cheat, said he felt users of the site were anxious the release of sexually explicit messages would humiliate their spouses and children. He wrote it is alarming \"the mob that is the Internet is more than willing to serve as judge, jury, and executioner\" and members of the site \"don’t deserve a flogging in the virtual town square with millions of onlookers.\"\n\nIn December 2013, Justine Sacco, a woman with 170 Twitter followers, tweeted acerbic jokes during a plane trip from New York to Cape Town, such as \"Weird German dude get some deodorant\" and, in Heathrow; \"Going to Africa. Hope I don't get AIDS. Just Kidding. I'm white!\" Sacco, a South African herself, intended the tweet to mock American ignorance of South Africa, and in a later interview expressed that her intention was to \"mimic—and mock what an actual racist, ignorant person would say.\" Sacco slept during her 11-hour plane trip, and woke up to find out that she had lost her job and was the number one Twitter topic worldwide, with celebrities and new media bloggers all over the globe denouncing her and encouraging all their followers to do the same. Sacco's employer, New York internet firm IAC, declared that she had lost her job as Director of Corporate Communications. People began tweeting \"Has Justine landed yet?\", expressing schadenfreude at the loss of her career. Sam Biddle, the Gawker Media blogger who promoted the #HasJustineLandedYet hashtag, later apologised for his role, admitting that he did so for Internet traffic to his blog, and noting that \"it's easy and thrilling to hate a stranger online.”\n\nAccording to Ronson, the public does not understand that a vigilante campaign of public shaming, undertaken with the ostensible intention of defending the underdog, may create a mob mentality capable of destroying the lives and careers of the public figures singled out for shaming. Ronson argued that in the early days of Twitter, people used the platform to share intimate details of their lives, and not as a vehicle of shaming. Brooke Gladstone argued that the Sacco affair may deter people from expressing themselves online due to a fear of being misinterpreted. Kelly McBride argues that journalists play a key role in expanding the shame and humiliation of targets of the campaigns by relaying claims to a larger audience, while justifying their actions as simply documenting an event in an impartial manner. She writes: \"Because of the mob mentality that accompanies public shaming events, often there is very little information about the target, sometimes only a single tweet. Yet there is a presumption of guilt and swift move toward justice, with no process for ascertaining facts.\" McBride further notes \"If newspapers ran front-page photos of adulterers in the Middle East being stripped naked and whipped in order to further their shame, we would criticize them as part of a backward system of justice.\" Ben Adler compared the Sacco incident to a number of Twitter hoaxes, and argued that the media needs to be more careful to fact-check articles and evaluate context.\n\nIn March 2013, at a PyCon technology conference, a female participant named Adria Richards took offense at a private discussion between two male attendees seated nearby using the words \"dongle\" and \"forking\" in reference to the male presenter, which she perceived as a sexual joke. Richards photographed the attendees with their faces visible, then published the photograph on Twitter including a shaming statement in her tweet. The following day, the employer of one of the photographed individuals, a software developer, terminated his employment because of the joke.\n\nIn response to Richards' public shaming of the developers, Internet users who were uninvolved launched a DDoS Attack on her employer, SendGrid, and according to an article by Jon Ronson in \"The New York Times Magazine\" \"told the employer the attacks would stop if Richards was fired\". SendGrid subsequently terminated her employment later the same day citing Richards' dividing the very community she was hired to unite, and the male anatomy joke she had posted a few days earlier on the employer website. Following the incident, PyCon updated its attendee rules stating, \"Public shaming can be counter-productive to building a strong community. PyCon does not condone nor participate in such actions out of respect.\"\n\nIn a 2014 interview, Richards—still unemployed—speculated whether the developer was responsible for instigating the Internet backlash against her. The developer, who was offered a new job \"right away\", said he had not engaged with those who sent him messages of support, and had posted a short statement on Hacker News the same night after he was fired saying in part that Richards had \"every right to report me to staff, and I defend her position\".\n\nIn November 2012, an Australian man filmed several passengers on a Melbourne bus verbally abusing and threatening a woman who had begun singing a song in French. A video alerting viewers of their racist and sexist comments was uploaded to YouTube and quickly attracted national and international media attention. The two male perpetrators who were most prominent in the video were later jailed, with Magistrate Jennifer Goldsbrough describing their threats as \"offensive to the entire population\".\n\nStarting as a turn of phrase, manspreading is a critique of men who take up more than one seat with their legs widely spread. In New York, actor Tom Hanks was photographed on the subway, taking up two seats, and then criticized for it. He responded on a talk show, \"Hey Internet, you idiot! The train was half empty! It was scattered—there was plenty of room!\" The controversy surrounding manspreading have been described by libertarian feminist Cathy Young as \"pseudo feminism—preoccupied with male misbehavior, no matter how trivial\". The practice of posting pictures of manspreading taken on subways, buses, and other modes of transportation online has been described as a form of public shaming. The criticism and campaigns against manspreading have been counter-criticized for not addressing similar behavior by women, such as taking up adjacent seats with bags, or \"she-bagging\". Twitter campaigns with the hashtag #manspreading have also been accompanied by hashtags like #she-bagging.\n\nThe feminist philosophy journal became involved in a dispute in April 2017 that led to the online shaming of one of its authors. The journal published an article about transracialism by Rebecca Tuvel, an assistant professor of philosophy, comparing the situation of Caitlyn Jenner, a trans woman, to that of Rachel Dolezal, a white woman who identifies as black. The article was criticized on Facebook and Twitter as a source of \"epistemic violence\", and the author became the subject of personal attacks. Academics associated with \"Hypatia\" joined in the criticism. A member of the journal's editorial board became the point of contact for an open letter demanding that the article be retracted, and the journal's board of associate editors issued an unauthorized apology, saying the article should never have been published. Rogers Brubaker described the episode in the \"New York Times\" as an example of \"internet shaming\".\n\nIn February 2009, an incident occurred involving the posting on YouTube of a video clip in which a domestic cat, named Dusty, was beaten and tortured by a 14-year-old boy calling himself \"Timmy\". After about 30,000 viewings, this clip and the account were removed by YouTube as a violation of their terms of service. Members of the 4chan imageboard investigated the incident, and by extrapolating from the poster's YouTube user name and the background in the video, they identified the abuser. As a result of these complaints, the Comanche County Sheriff's Department investigated the incident, and two suspects were arrested. Dusty survived the abuse, and was placed in the care of a local veterinarian. Both the assailant and the cameraman were charged with animal cruelty; as both were juveniles, possible punishments included \"psychological counseling, court monitoring until they turn 18, community service to provide restitution for treatment of animals, and/or placement in court custody.\"\n\nIn 2006, Wang-Jue (), a Chinese nurse appearing in an Internet crush video stomping a helpless kitten with her stilettos, gave herself up to authorities after bloggers and some print media started a campaign to trace back the recording. In the beginning, she was labeled as the kitten killer of Hangzhou, because it was believed she was from there; but some internauts recognized an island in northern Heilongjiang province. Upon discovery of her identity, Wang Jue received death threats from many angry animal lovers.\n\nWang posted an apology on the Luobei city government official website. She said she was recently divorced and did not know what to do with her life. She and the cameraman, a provincial TV employee, lost their jobs when internauts discovered their identities.\n\nIn August 2010, a passer-by in Coventry, England, later identified as Mary Bale by 4chan's members, was caught on a private security camera stroking a cat, named Lola, then looking around and dumping her in a wheelie bin, where she was found by her owners 15 hours later. The owners posted the video on the Internet in a bid to identify the woman, who was later interviewed by the RSPCA about her conduct. Outrage was sparked among animal lovers, and a Facebook group called \"Death to Mary Bale\" was created, and later removed. Police said they were speaking to the 45-year-old about her personal safety.\n\nThe woman, who at first downplayed her actions (\"I thought it would be funny\", \"it's just a cat\" and \"didn't see what all the buzz was about\") eventually apologised \"profusely for the upset and distress\".\n\nBale was convicted under the Animal Welfare Act of 2006 with causing unnecessary suffering to a cat. An additional charge of failing to provide the cat with a suitable environment was dropped. She was fined £250 and ordered to pay costs, totaling £1,436.04.\n\nIn 2010, a case was publicized involving a young female from Sichuan, using the alias Huang siu siu (黄小小), torturing and crushing rabbits. The group that financially sponsored the making of these videos, later revealed to be called \"Crushfetish\", paid young girls to crush fish, insects, rabbits and other small animals. The girl was paid 100 yuan for each attempt, and she had been participating since 2007. Police said the group makes videos to sell overseas, and the company has allegedly made 279 animal abuse videos with a subscription fee. Because of the concurrent hosting of the 2010 Asian Games, the animal videos were limited to being hosted online for a few hours a day.\n\nIn October 2013, a delicately balanced hoodoo in Goblin Valley State Park was intentionally knocked over by Boy Scout leaders who had been camping in the area. David Benjamin Hall captured video and shouted encouragement while Glenn Tuck Taylor toppled the formation. They posted the video to Facebook, whereupon it was viewed by thousands and the two men began receiving death threats. Their claim that the hoodoo appeared unstable, and that they vandalised it out of concern for passersby, was rejected by Fred Hayes, director of the Utah Division of State Parks and Recreation. Hall and Taylor were charged with third-degree felonies and were expelled from Boy Scouts.\n\nOn August 26, 2012, Yang Dacai, chief of the Shanxi provincial work safety administration, was caught grinning widely amid the wreckage of a long-distance bus that killed 36 passengers when it collided with a tanker loaded with highly flammable methanol on a Chinese highway in Shanxi Province. Pictures of the accident began to circulate on Sina Weibo, the most popular micro-blogging site in China which led to a meme dubbing him as the \"Smiling Brother\". Searches on the human flesh search engine followed leading to pictures surfacing on Weibo, showing Yang wearing luxury watches such as a $10,000 Rolex initiating another meme calling him \"Watch Brother\". On September 21, Yang was relieved of his position and accused of serious discipline violations. He was subsequently jailed for 14 years after being found guilty of taking bribes.\n\nIn 2005 in South Korea, bloggers targeted a woman who refused to clean up when her dog defecated on the floor of a Seoul subway car, labeling her \"Dog Poop Girl\" (rough translation of into English). Another commuter had taken a photograph of the woman and her dog, and posted it on a popular South Korean website. Within days, she had been identified by Internet vigilantes, and much of her personal information was leaked onto the Internet in an attempt to punish her for the offense. The story received mainstream attention when it was widely reported in South Korean media. The public humiliation led the woman to drop out of her university, according to reports.\n\nThe reaction by the South Korean public to the incident prompted several newspapers in South Korea to run editorials voicing concern over Internet vigilantism. One paper quoted Daniel Solove as saying that the woman was the victim of a \"cyber-posse, tracking down norm violators and branding them with digital Scarlet Letters.\" Another called it an \"Internet witch-hunt,\" and went on to say that \"the Internet is turning the whole society into a kangaroo court.\"\n\nOther notable instances also include the case of Evan Guttman and his friend's stolen Sidekick II smartphone, and the case of Jesse McPherson and his stolen Xbox 360, PowerBook, and TV.\n\nIn 2008, a 5-year-old girl asked to use the bathroom at the Rocky Mountain Chocolate Factory at Bella Terra/Huntington Beach, but was disallowed from using it because the Factory's restrooms were for employees only. The girl's mother describes the incident this way: \"I explained she had diarrhea and couldn't hold it and told [the store owners] she was about to go on the floor. They refused again and never offered me any alternatives. I begged them to have a heart and that she was 5 but by that time she had lost it all over herself and me.\" The story then spread to sites like digg.com where contact information for the owner of the store was released in message boards.\n\nIn 2008, a girl called Zhang Ya () from Liaoning province, Northeast China, posted a 4-minute video of herself complaining about the amount of attention the Sichuan earthquake victims were receiving on television. An intense response from Internet vigilantes resulted in the girl's personal details (even including her blood type) being made available online, as well as dozens of abusive video responses on Chinese websites and blogs. The girl was taken into police custody for three days as protection from vigilante death threats.\n\nStephen Fowler, a British expatriate and venture capitalist businessman, gained notoriety after his performance on ABC's \"Wife Swap\" (originally aired Friday January 30, 2009) when his wife exchanged positions in his family with a woman from Missouri for a two-week period. In response to her rule changes (standard procedure for the second week in the show) he insulted his guest and, in doing so, groups including the lower classes, soldiers, and the overweight. Several websites were made in protest against his behaviour. After the show, and after watching the \"Wife Swap\" video, his wife, a professional life coach, reported that she had encouraged him to attend professional behaviour counselling. Businesses with only tangential connection to Fowler publicly disclaimed any association with him due to the negative publicity. He resigned positions on the boards of two environmental charities to avoid attracting negative press.\n\nIn 2008, video of Patrick Pogan, a rookie police officer, body-slamming Christopher Long, a cyclist, surfaced on the Internet. The altercation happened when members of Critical Mass conducted a bicycling advocacy event at Times Square. The officer claimed the cyclist had veered into him, and so the biker was charged with assault, disorderly conduct and resisting arrest.\n\nThe charges against the cyclist were later dropped and Pogan was convicted of lying about the confrontation with the cyclist.\n\nIn 2009, a Facebook group was started, accusing a single mother for the death of a 13-month-old child in her foster care. It was the mother's then common-law husband who pleaded guilty to manslaughter and the mother was not formally accused of any wrongdoing. However, the members of the group, such as the boy's biological mother, accuse her of knowing what was going on and doing nothing to stop it.\n\nThe food magazine \"Cooks Source\" printed an article by Monica Gaudio without her permission in their October 2010 issue. Learning of the copyright violation, Gaudio emailed Judith Griggs, managing editor of \"Cooks Source Magazine\", requesting that the magazine both apologize and also donate $130 to the Columbia School of Journalism as payment for using her work. Instead she received a very unapologetic letter stating that she (Griggs) herself should be thanked for making the piece better and that Gaudio should be glad that she didn't give someone else credit for writing the article. During the ensuing public outcry, online vigilantes took it upon themselves to avenge Gaudio. The \"Cooks Source\" Facebook page was flooded with thousands of contemptuous comments, forcing the magazine's staff to create new pages in an attempt to escape the protest and accuse 'hackers' of taking control of the original page. The magazine's website was stripped of all content by the staff and shut down a week later.\n\nIn June 2012, An elderly bus monitor, Karen Klein, was taunted, picked on, and threatened by four seventh-graders. The act was caught on video and uploaded to the Internet which in turn caused an act of kindness from complete strangers. $703,833 was raised for Klein in donations from concerned strangers who were outraged after viewing a video that captured her torment.\n\nIn 2015 a junior barrister Charlotte Proudman working in the UK tweeted a screenshot of her LinkedIn exchange with Alexander Carter-Silk, a senior City solicitor, rebuking him for complimenting her on her profile photograph. The social media backlash included Proudman finding herself condemned as a \"feminazi\".\n\n\n"}
{"id": "25767", "url": "https://en.wikipedia.org/wiki?curid=25767", "title": "Real-time computing", "text": "Real-time computing\n\nIn computer science, real-time computing (RTC), or reactive computing describes hardware and software systems subject to a \"real-time constraint\", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\". The correctness of these types of systems depends on their temporal aspects as well as their functional aspects. Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually \"guarantee\" a response within any timeframe, although \"typical\" or \"expected\" response times may be given.\n\nA real-time system has been described as one which \"controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time\". The term \"real-time\" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean \"without significant delay\".\n\nReal-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.\n\nSystems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes on a vehicle, which must produce maximum deceleration but intermittently stop braking to prevent skidding. Real-time processing \"fails\" if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.\n\nThe term \"real-time\" derives from its use in early simulation, in which a real-world process is simulated at a rate that matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. \n\nMinicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as DOG scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data and so operating systems such as Data General's RDOS (Real-Time Disk Operatings System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOG (and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.\n\nOnce when the MOS Technology 6502 (used in the Commodore 64 and Apple II), and later when the Motorola 68000 (used in the Macintosh, Atari ST, and Commodore Amiga) were popular, anybody could use their home computer as a real-time system. The possibility to deactivate other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus surpassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. The Motorola 68000 and subsequent family members (68010, 68020 etc.) also became popular with manufacturers of industrial control systems. This application area is one in which real-time control offers genuine advantages in terms of process performance and safety.\n\nA system is said to be \"real-time\" if the total correctness of an operation depends not only upon its logical correctness, but also upon the time in which it is performed. Real-time systems, as well as their deadlines, are classified by the consequence of missing a deadline:\n\nThus, the goal of a \"hard real-time system\" is to ensure that all deadlines are met, but for \"soft real-time systems\" the goal becomes meeting a certain subset of deadlines in order to optimize some application-specific criteria. The particular criteria optimized depend on the application, but some typical examples include maximizing the number of deadlines met, minimizing the lateness of tasks and maximizing the number of high priority tasks meeting their deadlines.\n\nHard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system). For example, a car engine control system is a hard real-time system because a delayed signal may cause engine failure or damage. Other examples of hard real-time embedded systems include medical systems such as heart pacemakers and industrial process controllers. Hard real-time systems are typically found interacting at a low level with physical hardware, in embedded systems. Early video game systems such as the Atari 2600 and Cinematronics vector graphics had hard real-time requirements because of the nature of the graphics and timing hardware.\n\nIn the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). Other scheduling algorithms include earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%. New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications.\n\nSoft real-time systems are typically used to solve issues of concurrent access and the need to keep a number of connected systems up-to-date through changing situations. An example can be software that maintains and updates the flight plans for commercial airliners: the flight plans must be kept reasonably current, but they can operate with the latency of a few seconds. Live audio-video systems are also usually soft real-time; violation of constraints results in degraded quality, but the system can continue to operate and also recover in the future using workload prediction and reconfiguration methodologies.\n\nIn a real-time digital signal processing (DSP) process, the analyzed (input) and generated (output) samples can be processed (or generated) continuously in the time it takes to input and output the same set of samples \"independent\" of the processing delay. It means that the processing delay must be bounded even if the processing continues for an unlimited time. That means that the mean processing time per sample, including overhead, is no greater than the sampling period, which is the reciprocal of the sampling rate. This is the criterion whether the samples are grouped together in large segments and processed as blocks or are processed individually and whether there are long, short, or non-existent input and output buffers.\n\nConsider an audio DSP example; if a process requires 2.01 seconds to analyze, synthesize, or process 2.00 seconds of sound, it is not real-time. However, if it takes 1.99 seconds, it is or can be made into a real-time DSP process.\n\nA common life analog is standing in a line or queue waiting for the checkout in a grocery store. If the line asymptotically grows longer and longer without bound, the checkout process is not real-time. If the length of the line is bounded, customers are being \"processed\" and output as rapidly, on average, as they are being inputted and that process \"is\" real-time. The grocer might go out of business or must at least lose business if they cannot make their checkout process real-time; thus, it is fundamentally important that this process is real-time.\n\nA signal processing algorithm that cannot keep up with the flow of input data with output falling farther and farther behind the input is not real-time. But if the delay of the output (relative to the input) is bounded regarding a process that operates over an unlimited time, then that signal processing algorithm is real-time, even if the throughput delay may be very long.\n\nReal-time signal processing is necessary, but not sufficient in and of itself, for live signal processing such as what is required in live event support. Live audio digital signal processing requires both real-time operation and a sufficient limit to throughput delay so as to be tolerable to performers using stage monitors or in-ear monitors and not noticeable as lip sync error by the audience also directly watching the performers. Tolerable limits to latency for live, real-time processing is a subject of investigation and debate but is estimated to be between 6 and 20 milliseconds.\n\nReal-time bidirectional telecommunications delays of less than 300 ms (\"round trip\" or twice the unidirectional delay) are considered \"acceptable\" to avoid undesired \"talk-over\" in conversation.\n\nReal-time computing is sometimes misunderstood to be high-performance computing, but this is not an accurate classification. For example, a massive supercomputer executing a scientific simulation may offer impressive performance, yet it is not executing a real-time computation. Conversely, once the hardware and software for an anti-lock braking system have been designed to meet its required deadlines, no further performance gains are obligatory or even useful. Furthermore, if a network server is highly loaded with network traffic, its response time may be slower but will (in most cases) still succeed before it times out (hits its deadline). Hence, such a network server would not be considered a real-time system: temporal failures (delays, time-outs, etc.) are typically small and compartmentalized (limited in effect) but are not catastrophic failures. In a real-time system, such as the FTSE 100 Index, a slow-down beyond limits would often be considered catastrophic in its application context. Therefore, the most important requirement of a real-time system is predictability and not performance.\n\nSome kinds of software, such as many chess-playing programs, can fall into either category. For instance, a chess program designed to play in a tournament with a clock will need to decide on a move before a certain deadline or lose the game, and is therefore a real-time computation, but a chess program that is allowed to run indefinitely before moving is not. In both of these cases, however, high performance is desirable: the more work a tournament chess program can do in the allotted time, the better its moves will be, and the faster an unconstrained chess program runs, the sooner it will be able to move. This example also illustrates the essential difference between real-time computations and other computations: if the tournament chess program does not make a decision about its next move in its allotted time it loses the game—i.e., it fails as a real-time computation—while in the other scenario, meeting the deadline is assumed not to be necessary. High-performance is indicative of the amount of processing that is performed in a given amount of time, whereas real-time is the ability to get done with the processing to yield a useful output in the available time.\n\nThe term \"near real-time\" or \"nearly real-time\" (NRT), in telecommunications and computing, refers to the time delay introduced, by automated data processing or network transmission, between the occurrence of an event and the use of the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.\n\nThe distinction between the terms \"near real time\" and \"real time\" is somewhat nebulous and must be defined for the situation at hand. The term implies that there are no significant delays. In many cases, processing described as \"real-time\" would be more accurately described as \"near real-time\".\n\nNear real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export/import to common flat files that the other database can import/export on a scheduled basis so that they can sync/share common data in \"near real-time\" with each other.\n\nThe distinction between \"near real-time\" and \"real-time\" varies, and the delay is dependent on the type and speed of the transmission. The delay in near real-time is typically of the order of several seconds to several minutes.\n\nSeveral methods exist to aid the design of real-time systems, an example of which is MASCOT, an old but very successful method which represents the concurrent structure of the system. Other examples are HOOD, Real-Time UML, AADL, the Ravenscar profile, and Real-Time Java.\n\n\n"}
{"id": "22845262", "url": "https://en.wikipedia.org/wiki?curid=22845262", "title": "Real-time locating system", "text": "Real-time locating system\n\nReal-time locating systems (RTLS) are used to automatically identify and track the location of objects or people in real time, usually within a building or other contained area. Wireless RTLS tags are attached to objects or worn by people, and in most RTLS, fixed reference points receive wireless signals from tags to determine their location. Examples of real-time locating systems include tracking automobiles through an assembly line, locating pallets of merchandise in a warehouse, or finding medical equipment in a hospital.\n\nThe physical layer of RTLS technology is usually some form of radio frequency (RF) communication, but some systems use optical (usually infrared) or acoustic (usually ultrasound) technology instead of or in addition to RF. Tags and fixed reference points can be transmitters, receivers, or both, resulting in numerous possible technology combinations.\n\nRTLS are a form of local positioning system, and do not usually refer to GPS or to mobile phone tracking. Location information usually does not include speed, direction, or spatial orientation.\n\nThe term RTLS was created (circa 1998) at the ID EXPO trade show by Tim Harrington (WhereNet), Jay Werb, (PinPoint), and Bert Moore, (Automatic Identification Manufacturers, Inc.(AIM)). It was created to describe and differentiate an emerging technology that not only provided the automatic identification capabilities of active RFID tags, but also added the ability to view the location on a computer screen. It was at this show that the first examples of a commercial radio based RTLS system were shown by PinPoint and WhereNet. Although this capability had been utilized previously by military and government agencies, the technology had been too expensive for commercial purposes. In the early 1990s, the first commercial RTLS were installed at three healthcare facilities in the United States, and were based on the transmission and decoding of infrared light signals from actively transmitting tags. Since then, new technology has emerged that also enables RTLS to be applied to passive tag applications.\n\nRTLS are generally used in indoor and/or confined areas, such as buildings, and do not provide global coverage like GPS. RTLS tags are affixed to mobile items to be tracked or managed. RTLS reference points, which can be either transmitters or receivers, are spaced throughout a building (or similar area of interest) to provide the desired tag coverage. In most cases, the more RTLS reference points that are installed, the better the location accuracy, until the technology limitations are reached.\n\nA number of disparate system designs are all referred to as \"real-time locating systems\", but there are two primary system design elements:\n\nThe simplest form of choke point locating is where short range ID signals from a moving tag are received by a single fixed reader in a sensory network, thus indicating the location coincidence of reader and tag. Alternately, a choke point identifier can be received by the moving tag, and then relayed, usually via a second wireless channel, to a location processor. Accuracy is usually defined by the sphere spanned with the reach of the choke point transmitter or receiver. The use of directional antennas, or technologies such as infrared or ultrasound that are blocked by room partitions, can support choke points of various geometries.\n\nID signals from a tag is received by a multiplicity of readers in a sensory network, and a position is estimated using one or more locating algorithms, such as trilateration, multilateration, or triangulation. Equivalently, ID signals from several RTLS reference points can be received by a tag, and relayed back to a location processor. Localization with multiple reference points requires that distances between reference points in the sensory network be known in order to precisely locate a tag, and the determination of distances is called ranging.\n\nAnother way to calculate relative location is if mobile tags communicate directly with each other, then relay this information to a location processor.\n\nRF trilateration uses estimated ranges from multiple receivers to estimate the location of a tag. RF triangulation uses the angles at which the RF signals arrive at multiple receivers to estimate the location of a tag. Many obstructions, such as walls or furniture, can distort the estimated range and angle readings leading to varied qualities of location estimate. Estimation-based locating is often measured in accuracy for a given distance, such as 90% accurate for 10 meter range.\n\nSystems that use locating technologies that do not go through walls, such as infrared or ultrasound, tend to be more accurate in an indoor environment because only tags and receivers that have line of sight (or near line of sight) can communicate.\n\nRTLS can be used numerous logistical or operational areas such as:\n\nRTLS may be seen as a threat to privacy when used to determine the location of people. The newly declared human right of informational self-determination gives the right to prevent one's identity and personal data from being disclosed to others, and also covers disclosure of locality, though this does not generally apply to the workplace.\n\nSeveral prominent labor unions have come out against the use of RTLS systems to track workers calling them \"the beginning of Big Brother\" and \"an invasion of privacy\". A common strategy to overcome opposition to these systems is often similar to the following. However, this loss of privacy may be outweighed by other benefits to staff. For example, Toronto General Hospital is looking at RTLS to reduce quarantine times after an infectious disease outbreak. After a recent SARS outbreak, 1% of all staff were quarantined, and more accurate data regarding who had been exposed to the virus could have reduced the need for quarantines.\n\nThere is a wide variety of systems concepts and designs to provide real-time locating.\n\nA general model for selection of the best solution for a locating problem has been constructed at the Radboud University of Nijmegen.\nMany of these references do not comply with the definitions given in international standardization with ISO/IEC 19762-5 and ISO/IEC 24730-1. However, some aspects of real-time performance are served and aspects of locating are addressed in context of absolute coordinates.\n\nDepending on the physical technology used, at least one and often some combination of ranging and/or angulating methods are used to determine location:\n\nReal-time locating is affected by a variety of errors. Many of the major reasons relate to the physics of the locating system, and may not be reduced by improving the technical equipment.\n\n\nMany RTLS systems require direct and clear line of sight visibility. For those systems, where there is no visibility from mobile tags to fixed nodes there will be no result or a non valid result from locating engine. This applies to satellite locating as well as other RTLS systems such as angle of arrival and time of arrival. Fingerprinting is a way to overcome the visibility issue: If the locations in the tracking area contain distinct measurement fingerprints, line of sight is not necessarily needed. For example, if each location contains a unique combination of signal strength readings from transmitters, the location system will function properly. This is true, for example, with some Wi-Fi based RTLS solutions. However, having distinct signal strength fingerprints in each location typically requires a fairly high saturation of transmitters.\n\n\nThe measured location may appear entirely faulty. This is a generally result of simple operational models to compensate for the plurality of error sources. It proves impossible to serve proper location after ignoring the errors.\n\n\n\"Real time\" is no registered branding and has no inherent quality. A variety of offers sails under this term. As motion causes location changes, inevitably the latency time to compute a new location may be dominant with regard to motion. Either an RTLS system that requires waiting for new results is not worth the money or the operational concept that asks for faster location updates does not comply with the chosen systems approach.\n\n\nLocation will never be reported \"exactly\", as the term \"real-time\" and the term \"precision\" directly contradict in aspects of measurement theory as well as the term \"precision\" and the term \"cost\" contradict in aspects of economy. That is no exclusion of precision, but the limitations with higher speed are inevitable.\n\n\nRecognizing a reported location steadily apart from physical presence generally indicates the problem of insufficient over-determination and missing of visibility along at least one link from resident anchors to mobile transponders. Such effect is caused also by insufficient concepts to compensate for calibration needs.\n\n\nNoise from various sources has an erratic influence on stability of results. The aim to provide a steady appearance increases the latency contradicting to real time requirements.\n\n\nAs objects containing mass have limitations to jump, such effects are mostly beyond physical reality. Jumps of reported location not visible with the object itself generally indicate improper modeling with the location engine. Such effect is caused by changing dominance of various secondary responses.\n\n\nLocation of residing objects gets reported moving, as soon as the measures taken are biased by secondary path reflections with increasing weight over time. Such effect is caused by simple averaging and the effect indicates insufficient discrimination of first echoes.\n\nThe basic issues of RTLS are standardized by the International Organization for Standardization and the International Electrotechnical Commission, under the ISO/IEC 24730 series. In this series of standards, the basic standard ISO/IEC 24730-1 identifies the terms describing a form of RTLS used by a set of vendors, but does not encompass the full scope of RTLS technology.\n\nCurrently several standards are published:\n\nThese standards do not stipulate any special method of computing locations, nor the method of measuring locations. This may be defined in specifications for trilateration, triangulation or any hybrid approaches to trigonometric computing for planar or spherical models of a terrestrial area.\n\n\nIn RTLS application in the Healthcare industry, various studies were issued discussing the limitations of the currently adopted RTLS. Currently used technologies RFID, Wi-fi, UWB, all RFID based are hazardous in the sense of interference with sensitive equipment. A study carried out by Dr Erik Jan van Lieshout of the Academic Medical Centre of the University of Amsterdam published in \"JAMA\" (\"Journal of the American Medical Equipment\") claimed \"RFID and UWB could shut down equipment patients rely on\" as \"RFID caused interference in 34 of the 123 tests they performed\". The first Bluetooth RTLS provider in the medical industry is supporting this in their article: \"The fact that RFID cannot be used near sensitive equipment should in itself be a red flag to the medical industry\". The RFID Journal responded to this study not negating it rather explaining real-case solution: \"The Purdue study showed no effect when ultrahigh-frequency (UHF) systems were kept at a reasonable distance from medical equipment. So placing readers in utility rooms, near elevators and above doors between hospital wings or departments to track assets is not a problem\". However the case of ”keeping at a reasonable distance” might be still an open question for the RTLS technology adopters and providers in medical facilities.\n\nIn many applications it is very difficult and at the same time important to make a proper choice among various communication technologies (e.g., RFID, WiFi, etc.) which RTLS may include. Wrong design decision made at early stages can lead to catastrophic results for the system and a significant loss of money for fixing and redesign. To solve this problem a special methodology for RTLS design space exploration was developed. It consists of such steps as modelling, requirements specification and verification into a single efficient process.\n\n\n"}
{"id": "43173625", "url": "https://en.wikipedia.org/wiki?curid=43173625", "title": "Routing in cellular networks", "text": "Routing in cellular networks\n\nNetwork routing in a cellular network deals with the challenges of traditional telephony such as switching and call setup. \n\nMost cellular network routing issues in different cells can be attributed to the multiple access methods used for transmission. The location of each mobile phone must be known to reuse a given band of frequencies in different cells and forms space-division multiple access (SDMA).\n\nFDMA is one of the multiple access methods used in cellular networks. 50 MHz blocks of communication channel are assigned, which lie in radio frequency range and contain an equal number of uplinks (terminal to base station) and downlinks (base station to terminal). One or more bidirectional channels are carried by 10-90 band pairs. The digital networks additionally make use of either CDMA or TDMA methods.\n\nA special service called mobility management provides this application. Terminals (handsets) can move from one place to another during the call and therefore requires the call to be handed over from one channel to another. Soft handover uses the same frequency channel. The same terminals can operate in the same area covered by different service providers, which is known as roaming.\n\n"}
{"id": "1227094", "url": "https://en.wikipedia.org/wiki?curid=1227094", "title": "Single-source publishing", "text": "Single-source publishing\n\nSingle-source publishing, also known as single-sourcing publishing, is a content management method which allows the same source content to be used across different forms of media and more than one time. The labor-intensive and expensive work of editing need only be carried out once, on only one document; that source document can then be stored in one place and reused. This reduces the potential for error, as corrections are only made one time in the source document.\n\nThe benefits of single-source publishing primarily relate to the editor rather than the user. The user benefits from the consistency that single-sourcing brings to terminology and information. This assumes the content manager has applied an organized conceptualization to the underlying content (A poor conceptualization can make single-source publishing less useful). Single-source publishing is sometimes used synonymously with multi-channel publishing though whether or not the two terms are synonymous is a matter of discussion.\n\nWhile there is a general definition of single-source publishing, there is no single official delineation between single-source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delineation. Single-source publishing is most often understood as the creation of one source document in an authoring tool and converting that document into different file formats or human languages (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single-source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.\n\nThe origins of single-source publishing lie, indirectly, with the release of Windows 3.0 in 1990. With the eclipsing of MS-DOS by graphical user interfaces, help files went from being unreadable text along the bottom of the screen to hypertext systems such as WinHelp. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of software documentation did not simply move from being writers of traditional bound books to writers of electronic publishing, but rather they became authors of central documents which could be reused multiple times across multiple formats.\n\nThe first single-source publishing project was started in 1993 by Cornelia Hofmann at Schneider Electric in Seligenstadt, using software based on Interleaf to automatically create paper documentation in multiple languages based on a single original source file.\n\nXML, developed during the mid- to late-1990s, was also significant to the development of single-source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.\n\nIn the mid-1990s, several firms began creating and using single-source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt & Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor. The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.\nFord, for example, was able to tag its single owner's manual files so that 12 model years could be generated via a resolution script running on the single completed file. Pratt & Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single-source files, calling out the desired version at publication time. World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.\n\nStarting from the early 2000s, single-source publishing was used with an increasing frequency in the field of technical translation. It is still regarded as the most efficient method of publishing the same material in different languages. Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method. Metadata could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.\n\nAlthough single-source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single-source publishing or render one's operations obsolete.\n\nEditors using single-source publishing have been criticized for below-standard work quality, leading some critics to describe single-source publishing as the \"conveyor belt assembly\" of content creation. \n\nWhile heavily used in technical translation, there are risks of error in regard to indexing. While two words might be synonyms in English, they may not be synonyms in another language. In a document produced via single-sourcing, the index will be translated automatically and the two words will be rendered as synonyms. This is because they are synonyms in the source language, while in the target language they are not.\n\n\n\n\n"}
{"id": "28743", "url": "https://en.wikipedia.org/wiki?curid=28743", "title": "Slide rule", "text": "Slide rule\n\nThe slide rule, also known colloquially in the United States as a slipstick, is a mechanical analog computer. The slide rule is used primarily for multiplication and division, and also for functions such as exponents, roots, logarithms and trigonometry, but typically not for addition or subtraction. Though similar in name and appearance to a standard ruler, the slide rule is not meant to be used for measuring length or drawing straight lines.\n\nSlide rules exist in a diverse range of styles and generally appear in a linear or circular form with a standardized set of markings (scales) essential to performing mathematical computations. Slide rules manufactured for specialized fields such as aviation or finance typically feature additional scales that aid in calculations common to those fields.\n\nAt its simplest, each number to be multiplied is represented by a length on a sliding ruler. As the rulers each have a logarithmic scale, it is possible to align them to read the sum of the logarithms, and hence calculate the product of the two numbers.\n\nThe Reverend William Oughtred and others developed the slide rule in the 17th century based on the emerging work on logarithms by John Napier. Before the advent of the electronic calculator, it was the most commonly used calculation tool in science and engineering. The use of slide rules continued to grow through the 1950s and 1960s even as computers were being gradually introduced; but around 1974 the handheld electronic scientific calculator made them largely obsolete and most suppliers left the business.\n\nIn its most basic form, the slide rule uses two logarithmic scales to allow rapid multiplication and division of numbers. These common operations can be time-consuming and error-prone when done on paper. More elaborate slide rules allow other calculations, such as square roots, exponentials, logarithms, and trigonometric functions.\n\nScales may be grouped in decades, which are numbers ranging from 1 to 10 (i.e. 10 to 10). Thus single decade scales C and D range from 1 to 10 across the entire width of the slide rule while double decade scales A and B range from 1 to 100 over the width of the slide rule.\n\nIn general, mathematical calculations are performed by aligning a mark on the sliding central strip with a mark on one of the fixed strips, and then observing the relative positions of other marks on the strips. Numbers aligned with the marks give the approximate value of the product, quotient, or other calculated result.\n\nThe user determines the location of the decimal point in the result, based on mental estimation. Scientific notation is used to track the decimal point in more formal calculations. Addition and subtraction steps in a calculation are generally done mentally or on paper, not on the slide rule.\n\nMost slide rules consist of three linear strips of the same length, aligned in parallel and interlocked so that the central strip can be moved lengthwise relative to the other two. The outer two strips are fixed so that their relative positions do not change.\n\nSome slide rules (\"duplex\" models) have scales on both sides of the rule and slide strip, others on one side of the outer strips and both sides of the slide strip (which can usually be pulled out, flipped over and reinserted for convenience), still others on one side only (\"simplex\" rules). A sliding with a vertical alignment line is used to find corresponding points on scales that are not adjacent to each other or, in duplex models, are on the other side of the rule. The cursor can also record an intermediate result on any of the scales.\n\nA logarithm transforms the operations of multiplication and division to addition and subtraction according to the rules formula_1 and formula_2.\nMoving the top scale to the right by a distance of formula_3, by matching the beginning of the top scale with the label formula_4 on the bottom, aligns each number formula_5, at position formula_6 on the top scale, with the number at position formula_7 on the bottom scale. Because formula_8, this position on the bottom scale gives formula_9, the product of formula_4 and formula_5. For example, to calculate 3×2, the 1 on the top scale is moved to the 2 on the bottom scale. The answer, 6, is read off the bottom scale where 3 is on the top scale. In general, the 1 on the top is moved to a factor on the bottom, and the answer is read off the bottom where the other factor is on the top. This works because the distances from the \"1\" are proportional to the logarithms of the marked values:\n\nOperations may go \"off the scale;\" for example, the diagram above shows that the slide rule has not positioned the 7 on the upper scale above any number on the lower scale, so it does not give any answer for 2×7. In such cases, the user may slide the upper scale to the left until its right index aligns with the 2, effectively dividing by 10 (by subtracting the full length of the C-scale) and then multiplying by 7, as in the illustration below:\n\nHere the user of the slide rule must remember to adjust the decimal point appropriately to correct the final answer. We wanted to find 2×7, but instead we calculated (2/10)×7=0.2×7=1.4. So the true answer is not 1.4 but 14. Resetting the slide is not the only way to handle multiplications that would result in off-scale results, such as 2×7; some other methods are:\n\nMethod 1 is easy to understand, but entails a loss of precision. Method 3 has the advantage that it only involves two scales.\n\nThe illustration below demonstrates the computation of 5.5/2. The 2 on the top scale is placed over the 5.5 on the bottom scale. The 1 on the top scale lies above the quotient, 2.75. There is more than one method for doing division, but the method presented here has the advantage that the final result cannot be off-scale, because one has a choice of using the 1 at either end.\n\nIn addition to the logarithmic scales, some slide rules have other mathematical functions encoded on other auxiliary scales. The most popular were trigonometric, usually sine and tangent, common logarithm (log) (for taking the log of a value on a multiplier scale), natural logarithm (ln) and exponential (\"e\") scales. Some rules include a Pythagorean scale, to figure sides of triangles, and a scale to figure circles. Others feature scales for calculating hyperbolic functions. On linear rules, the scales and their labeling are highly standardized, with variation usually occurring only in terms of which scales are included and in what order:\n\nThe Binary Slide Rule manufactured by Gilson in 1931 performed an addition and subtraction function limited to fractions.\n\nThere are single-decade (C and D), double-decade (A and B), and triple-decade (K) scales. To compute formula_12, for example, locate x on the D scale and read its square on the A scale. Inverting this process allows square roots to be found, and similarly for the powers 3, 1/3, 2/3, and 3/2. Care must be taken when the base, x, is found in more than one place on its scale. For instance, there are two nines on the A scale; to find the square root of nine, use the first one; the second one gives the square root of 90.\n\nFor formula_13 problems, use the LL scales. When several LL scales are present, use the one with \"x\" on it. First, align the leftmost 1 on the C scale with x on the LL scale. Then, find \"y\" on the C scale and go down to the LL scale with \"x\" on it. That scale will indicate the answer. If \"y\" is \"off the scale,\" locate formula_14 and square it using the A and B scales as described above. Alternatively, use the rightmost 1 on the C scale, and read the answer off the next higher LL scale. For example, aligning the rightmost 1 on the C scale with 2 on the LL2 scale, 3 on the C scale lines up with 8 on the LL3 scale.\n\nTo extract a cube root using a slide rule with only C/D and A/B scales, align 1 on the B cursor with the base number on the A scale (taking care as always to distinguish between the lower and upper halves of the A scale). Slide the cursor until the number on the D scale which is against 1 on the C cursor is the same as the number on the B cursor which is against the base number on the A scale. (Examples: A 8, B 2, C 1, D 2; A 27, B 3, C 1, D 3.)\n\nThe S, T, and ST scales are used for trig functions and multiples of trig functions, for angles in degrees.\n\nFor angles from around 5.7 up to 90 degrees, sines are found by comparing the S scale with C (or D) scale; though on many closed-body rules the S scale relates to the A scale instead, and what follows must be adjusted appropriately. The S scale has a second set of angles (sometimes in a different color), which run in the opposite direction, and are used for cosines. Tangents are found by comparing the T scale with the C (or D) scale for angles less than 45 degrees. For angles greater than 45 degrees the CI scale is used. Common forms such as formula_15 can be read directly from \"x\" on the S scale to the result on the D scale, when the C-scale index is set at \"k\". For angles below 5.7 degrees, sines, tangents, and radians are approximately equal, and are found on the ST or SRT (sines, radians, and tangents) scale, or simply divided by 57.3 degrees/radian. Inverse trigonometric functions are found by reversing the process.\n\nMany slide rules have S, T, and ST scales marked with degrees and minutes (e.g. some Keuffel and Esser models, late-model Teledyne-Post Mannheim-type rules). So-called \"decitrig\" models use decimal fractions of degrees instead.\n\nBase-10 logarithms and exponentials are found using the L scale, which is linear. Some slide rules have a Ln scale, which is for base e. Logarithms to any other base can be calculated by reversing the procedure for calculating powers of a number. For example, log2 values can be determined by lining up either leftmost or rightmost 1 on the C scale with 2 on the LL2 scale, finding the number whose logarithm is to be calculated on the corresponding LL scale, and reading the log2 value on the C scale.\n\nSlide rules are not typically used for addition and subtraction, but it is nevertheless possible to do so using two different techniques.\n\nThe first method to perform addition and subtraction on the C and D (or any comparable scales) requires converting the problem into one of division. For addition, the quotient of the two variables plus one times the divisor equals their sum:\n\nFor subtraction, the quotient of the two variables minus one times the divisor equals their difference:\n\nThis method is similar to the addition/subtraction technique used for high-speed electronic circuits with the logarithmic number system in specialized computer applications like the Gravity Pipe (GRAPE) supercomputer and hidden Markov models.\n\nThe second method utilizes a sliding linear L scale available on some models. Addition and subtraction are performed by sliding the cursor left (for subtraction) or right (for addition) then returning the slide to 0 to read the result.\n\nThe width of the slide rule is quoted in terms of the nominal width of the scales. Scales on the most common \"10-inch\" models are actually 25 cm, as they were made to metric standards, though some rules offer slightly extended scales to simplify manipulation when a result overflowed. Pocket rules are typically 5 inches. Models a couple of metres wide were sold to be hung in classrooms for teaching purposes.\n\nTypically the divisions mark a scale to a precision of two significant figures, and the user estimates the third figure. Some high-end slide rules have magnifier cursors that make the markings easier to see. Such cursors can effectively double the accuracy of readings, permitting a 10-inch slide rule to serve as well as a 20-inch.\n\nVarious other conveniences have been developed. Trigonometric scales are sometimes dual-labeled, in black and red, with complementary angles, the so-called \"Darmstadt\" style. Duplex slide rules often duplicate some of the scales on the back. Scales are often \"split\" to get higher accuracy.\n\nCircular slide rules come in two basic types, one with two cursors, and another with a free dish and one cursor. The dual cursor versions perform multiplication and division by holding a fast angle between the cursors as they are rotated around the dial. The onefold cursor version operates more like the standard slide rule through the appropriate alignment of the scales.\n\nThe basic advantage of a circular slide rule is that the widest dimension of the tool was reduced by a factor of about 3 (i.e. by π). For example, a 10 cm circular would have a maximum precision approximately equal to a 31.4 cm ordinary slide rule. Circular slide rules also eliminate \"off-scale\" calculations, because the scales were designed to \"wrap around\"; they never have to be reoriented when results are near 1.0—the rule is always on scale. However, for non-cyclical non-spiral scales such as S, T, and LL's, the scale width is narrowed to make room for end margins.\n\nCircular slide rules are mechanically more rugged and smoother-moving, but their scale alignment precision is sensitive to the centering of a central pivot; a minute 0.1 mm off-centre of the pivot can result in a 0.2 mm worst case alignment error. The pivot, however, does prevent scratching of the face and cursors. The highest accuracy scales are placed on the outer rings. Rather than \"split\" scales, high-end circular rules use spiral scales for more complex operations like log-of-log scales. One eight-inch premium circular rule had a 50-inch spiral log-log scale. Around 1970, an inexpensive model from B. C. Boykin (Model 510) featured 20 scales, including 50-inch C-D (multiplication) and log scales. The RotaRule featured a friction brake for the cursor.\n\nThe main disadvantages of circular slide rules are the difficulty in locating figures along a dish, and limited number of scales. Another drawback of circular slide rules is that less-important scales are closer to the center, and have lower precisions. Most students learned slide rule use on the linear slide rules, and did not find reason to switch.\n\nOne slide rule remaining in daily use around the world is the E6B. This is a circular slide rule first created in the 1930s for aircraft pilots to help with dead reckoning. With the aid of scales printed on the frame it also helps with such miscellaneous tasks as converting time, distance, speed, and temperature values, compass errors, and calculating fuel use. The so-called \"prayer wheel\" is still available in flight shops, and remains widely used. While GPS has reduced the use of dead reckoning for aerial navigation, and handheld calculators have taken over many of its functions, the E6B remains widely used as a primary or backup device and the majority of flight schools demand that their students have some degree of proficiency in its use.\n\nProportion wheels are simple circular slide rules used in graphic design to calculate aspect ratios. Lining up the original and desired size values on the inner and outer wheels will display their ratio as a percentage in a small window. They are not as common since the advent of computerized layout, but \n\nIn 1952, Swiss watch company Breitling introduced a pilot's wristwatch with an integrated circular slide rule specialized for flight calculations: the Breitling Navitimer. The Navitimer circular rule, referred to by Breitling as a \"navigation computer\", featured airspeed, rate/time of climb/descent, flight time, distance, and fuel consumption functions, as well as kilometer—nautical mile and gallon—liter fuel amount conversion functions.\nThere are two main types of cylindrical slide rules: those with helical scales such as the Fuller, the Otis King and the Bygrave slide rule, and those with bars, such as the Thacher and some Loga models. In either case, the advantage is a much longer scale, and hence potentially greater precision, than afforded by a straight or circular rule.\nTraditionally slide rules were made out of hard wood such as mahogany or boxwood with cursors of glass and metal. At least one high precision instrument was made of steel.\n\nIn 1895, a Japanese firm, Hemmi, started to make slide rules from bamboo, which had the advantages of being dimensionally stable, strong, and naturally self-lubricating. These bamboo slide rules were introduced in Sweden in September, 1933, and probably only a little earlier in Germany. Scales were made of celluloid, plastic, or painted aluminium. Later cursors were acrylics or polycarbonates sliding on Teflon bearings.\n\nAll premium slide rules had numbers and scales engraved, and then filled with paint or other resin. Painted or imprinted slide rules were viewed as inferior because the markings could wear off. Nevertheless, Pickett, probably America's most successful slide rule company, made all printed scales. Premium slide rules included clever catches so the rule would not fall apart by accident, and bumpers to protect the scales and cursor from rubbing on tabletops. The recommended cleaning method for engraved markings is to scrub lightly with steel-wool. For painted slide rules, use diluted commercial window-cleaning fluid and a soft cloth.\n\nThe slide rule was invented around 1620–1630, shortly after John Napier's publication of the concept of the logarithm. In 1620 Edmund Gunter of Oxford developed a calculating device with a single logarithmic scale; with additional measuring tools it could be used to multiply and divide. In c. 1622, William Oughtred of Cambridge combined two handheld Gunter rules to make a device that is recognizably the modern slide rule. Like his contemporary at Cambridge, Isaac Newton, Oughtred taught his ideas privately to his students. Also like Newton, he became involved in a vitriolic controversy over priority, with his one-time student Richard Delamain and the prior claims of Wingate. Oughtred's ideas were only made public in publications of his student William Forster in 1632 and 1653.\n\nIn 1677, Henry Coggeshall created a two-foot folding rule for timber measure, called the Coggeshall slide rule, expanding the slide rule's use beyond mathematical inquiry.\n\nIn 1722, Warner introduced the two- and three-decade scales, and in 1755 Everard included an inverted scale; a slide rule containing all of these scales is usually known as a \"polyphase\" rule.\n\nIn 1815, Peter Mark Roget invented the log log slide rule, which included a scale displaying the logarithm of the logarithm. This allowed the user to directly perform calculations involving roots and exponents. This was especially useful for fractional powers.\n\nIn 1821, Nathaniel Bowditch, described in the \"American Practical Navigator\" a \"sliding rule\" that contained scales trigonometric functions on the fixed part and a line of log-sines and log-tans on the slider used to solve navigation problems.\n\nIn 1845, Paul Cameron of Glasgow introduced a nautical slide rule capable of answering navigation questions, including right ascension and declination of the sun and principal stars.\n\nA more modern form of slide rule was created in 1859 by French artillery lieutenant Amédée Mannheim, \"who was fortunate in having his rule made by a firm of national reputation and in having it adopted by the French Artillery.\" It was around this time that engineering became a recognized profession, resulting in widespread slide rule use in Europe–but not in the United States. There, Edwin Thacher's cylindrical rule took hold after 1881. The duplex rule was invented by William Cox in 1891, and was produced by Keuffel and Esser Co. of New York.\n\nAstronomical work also required precise computations, and, in 19th-century Germany, a steel slide rule about two meters long was used at one observatory. It had a microscope attached, giving it accuracy to six decimal places..\n\nThroughout the 1950s and 1960s, the slide rule was the symbol of the engineer's profession in the same way the stethoscope is that of the medical profession.\n\nGerman rocket scientist Wernher von Braun bought two \"Nestler\" slide rules in the 1930s. Ten years later he brought them with him when he moved to the U.S. after World War II to work on the American space effort. Throughout his life he never used any other slide rule. He used his two Nestlers while heading the NASA program that landed a man on the moon in July 1969.\n\nAluminium Pickett-brand slide rules were carried on Project Apollo space missions. The model N600-ES owned by Buzz Aldrin that flew with him to the moon on Apollo 11 was sold at auction in 2007. The model N600-ES taken along on Apollo 13 in 1970 is owned by the National Air and Space Museum.\n\nSome engineering students and engineers carried ten-inch slide rules in belt holsters, a common sight on campuses even into the mid-1970s. Until the advent of the pocket digital calculator, students also might keep a ten- or twenty-inch rule for precision work at home or the office while carrying a five-inch pocket slide rule around with them.\n\nIn 2004, education researchers David B. Sher and Dean C. Nataro conceived a new type of slide rule based on \"prosthaphaeresis\", an algorithm for rapidly computing products that predates logarithms. However, there has been little practical interest in constructing one beyond the initial prototype.\n\nSlide rules have often been specialized to varying degrees for their field of use, such as excise, proof calculation, engineering, navigation, etc., but some slide rules are extremely specialized for very narrow applications. For example, the John Rabone & Sons 1892 catalog lists a \"Measuring Tape and Cattle Gauge\", a device to estimate the weight of a cow from its measurements.\n\nThere were many specialized slide rules for photographic applications; for example, the actinograph of Hurter and Driffield was a two-slide boxwood, brass, and cardboard device for estimating exposure from time of day, time of year, and latitude.\n\nSpecialized slide rules were invented for various forms of engineering, business and banking. These often had common calculations directly expressed as special scales, for example loan calculations, optimal purchase quantities, or particular engineering equations. For example, the Fisher Controls company distributed a customized slide rule adapted to solving the equations used for selecting the proper size of industrial flow control valves.\n\nPilot balloon slide rules were used by meteorologists in weather services to determine the upper wind velocities from an ascending hydrogen or helium filled pilot balloon.\n\nIn World War II, bombardiers and navigators who required quick calculations often used specialized slide rules. One office of the U.S. Navy actually designed a generic slide rule \"chassis\" with an aluminium body and plastic cursor into which celluloid cards (printed on both sides) could be placed for special calculations. The process was invented to calculate range, fuel use and altitude for aircraft, and then adapted to many other purposes.\n\nThe E6-B is a circular slide rule used by pilots and navigators.\n\nCircular slide rules to estimate ovulation dates and fertility are known as \"wheel calculators\".\n\nThe importance of the slide rule began to diminish as electronic computers, a new but rare resource in the 1950s, became more widely available to technical workers during the 1960s. (See History of computing hardware (1960s–present).)\n\nAnother step away from slide rules was the introduction of relatively inexpensive electronic desktop scientific calculators. The first included the Wang Laboratories LOCI-2, introduced in 1965, which used logarithms for multiplication and division; and the Hewlett-Packard HP 9100A, introduced in 1968. Both of these were programmable and provided exponential and logarithmic functions; the HP had trigonometric functions (sine, cosine, and tangent) and hyperbolic trigonometric functions as well. The HP used the CORDIC (coordinate rotation digital computer) algorithm, which allows for calculation of trigonometric functions using only shift and add operations. This method facilitated the development of ever smaller scientific calculators.\n\nAs with mainframe computing, the availability of these machines did not significantly affect the ubiquitous use of the slide rule until cheap hand held scientific electronic calculators became available in the mid-1970s, at which point, it rapidly declined. \nThe pocket-sized Hewlett-Packard HP-35 scientific calculator was the first handheld device of its type, but it cost US$395 in 1972. This was justifiable for some engineering professionals but too expensive for most students. \nBy 1975, basic four-function electronic calculators could be purchased for less than $50, and, by 1976, the TI-30 scientific calculator was sold for less than $25.\n\nMost people find slide rules difficult to learn and use. Even during their heyday, they never caught on with the general public. Addition and subtraction are not well-supported operations on slide rules and doing a calculation on a slide rule tends to be slower than on a calculator. This led engineers to use mathematical equations that favored operations that were easy on a slide rule over more accurate but complex functions, these approximations could lead to inaccuracies and mistakes. On the other hand, the spatial, manual operation of slide rules cultivates in the user an intuition for numerical relationships and scale that people who have used only digital calculators often lack. A slide rule will also display all the terms of a calculation along with the result, thus eliminating uncertainty about what calculation was actually performed.\n\nA slide rule requires the user to separately compute the order of magnitude of the answer in order to position the decimal point in the results. For example, 1.5 × 30 (which equals 45) will show the same result as 1,500,000 × 0.03 (which equals 45,000). This separate calculation is less likely to lead to extreme calculation errors, but forces the user to keep track of magnitude in short-term memory (which is error-prone), keep notes (which is cumbersome) or reason about it in every step (which distracts from the other calculation requirements).\n\nThe typical arithmetic precision of a slide rule is about three significant digits, compared to many digits on digital calculators. As order of magnitude gets the greatest prominence when using a slide rule, users are less likely to make errors of false precision.\n\nWhen performing a sequence of multiplications or divisions by the same number, the answer can often be determined by merely glancing at the slide rule without any manipulation. This can be especially useful when calculating percentages (e.g. for test scores) or when comparing prices (e.g. in dollars per kilogram). Multiple speed-time-distance calculations can be performed hands-free at a glance with a slide rule. Other useful linear conversions such as pounds to kilograms can be easily marked on the rule and used directly in calculations.\n\nBeing entirely mechanical, a slide rule does not depend on grid electricity or batteries. However, mechanical imprecision in slide rules that were poorly constructed or warped by heat or use will lead to errors.\n\nMany sailors keep slide rules as backups for navigation in case of electric failure or battery depletion on long route segments. Slide rules are still commonly used in aviation, particularly for smaller planes. They are being replaced only by integrated, special purpose and expensive flight computers, and not general-purpose calculators. The E6B circular slide rule used by pilots has been in continuous production and remains available in a variety of models. Some wrist watches designed for aviation use still feature slide rule scales to permit quick calculations. The Citizen Skyhawk AT is a notable example.\n\nEven today, some people prefer a slide rule over an electronic calculator as a practical computing device. Others keep their old slide rules out of a sense of nostalgia, or collect them as a hobby.\n\nA popular collectible model is the Keuffel & Esser \"Deci-Lon\", a premium scientific and engineering slide rule available both in a ten-inch (25 cm) \"regular\" (\"Deci-Lon 10\") and a five-inch \"pocket\" (\"Deci-Lon 5\") variant. Another prized American model is the eight-inch (20 cm) Scientific Instruments circular rule. Of European rules, Faber-Castell's high-end models are the most popular among collectors.\n\nAlthough a great many slide rules are circulating on the market, specimens in good condition tend to be expensive. Many rules found for sale on are damaged or have missing parts, and the seller may not know enough to supply the relevant information. Replacement parts are scarce, expensive, and generally available only for separate purchase on individual collectors' web sites. The Keuffel and Esser rules from the period up to about 1950 are particularly problematic, because the end-pieces on the cursors, made of celluloid, tend to chemically break down over time.\n\nThere are still a handful of sources for brand new slide rules. The Concise Company of Tokyo, which began as a manufacturer of circular slide rules in July 1954, continues to make and sell them today. In September 2009, on-line retailer ThinkGeek introduced its own brand of straight slide rules, described as \"faithful replica[s]\" that are \"individually hand tooled\". These are no longer available in 2012. In addition, Faber-Castell has a number of slide rules still in inventory, available for international purchase through their web store. Proportion wheels are still used in graphic design.\n\nVarious slide rule simulator apps are available for Android and iOS-based smart phones and tablets.\n\nSpecialized slide rules such as the E6B used in aviation, and gunnery slide rules used in laying artillery are still used though no longer on a routine basis. These rules are used as part of the teaching and instruction process as in learning to use them the student also learns about the principles behind the calculations, it also allows the student to be able to use these instruments as a back up in the event that the modern electronics in general use fail.\n\n"}
{"id": "17017917", "url": "https://en.wikipedia.org/wiki?curid=17017917", "title": "Software studies", "text": "Software studies\n\nSoftware studies is an emerging interdisciplinary research field, which studies software systems and their social and cultural effects.\n\nThe implementation and use of software has been studied in recent fields such as cyberculture, Internet studies, new media studies, and digital culture, yet prior to software studies, software was rarely ever addressed as a distinct object of study.\n\nSoftware studies is an interdisciplinary field. To study software as an artifact, it draws upon methods and theory from the digital humanities and from computational perspectives on software. Methodologically, software studies usually differs from the approaches of computer science and software engineering, which concern themselves primarily with software in information theory and in practical application; however, these fields all share an emphasis on computer literacy, particularly in the areas of programming and source code. This emphasis on analyzing software sources and processes (rather than interfaces) often distinguishes software studies from new media studies, which is usually restricted to discussions of interfaces and observable effects.\n\nThe conceptual origins of software studies include Marshall McLuhan's focus on the role of media in themselves, rather than the content of media platforms, in shaping culture. Early references to the study of software as a cultural practice appear in Friedrich Kittler's essay, \"Es gibt keine Software,\" Lev Manovich's \"Language of New Media\", and Matthew Fuller's \"Behind the Blip: Essays on the culture of software\". Much of the impetus for the development of software studies has come from videogame studies, particularly platform studies, the study of videogames and other software artifacts in their hardware and software contexts. New media art, software art, motion graphics, and computer-aided design are also significant software-based cultural practices, as is the creation of new protocols and platforms.\n\nThe first conference events in the emerging field were Software Studies Workshop 2006 and SoftWhere 2008.\n\nIn 2008, MIT Press launched a \"Software Studies\" book series with an edited volume of essays (Matthew Fuller's \"Software Studies: a Lexicon\"), and the first academic program was launched, (Lev Manovich, Benjamin H. Bratton and Noah Wardrip-Fruin's \"Software Studies Initiative\" at U. California San Diego). \nIn 2011, a number of mainly British researchers established \"Computational Culture\", an open-access peer-reviewed journal. The journal provides a platform for \"inter-disciplinary enquiry into the nature of the culture of computational objects, practices, processes and structures.\"\n\nSoftware studies is closely related to a number of other emerging fields in the digital humanities that explore functional components of technology from a social and cultural perspective. Software studies' focus is at the level of the entire program, specifically the relationship between interface and code. Notably related are critical code studies, which is more closely attuned to the code rather than the program, and platform studies, which investigates the relationships between hardware and software.\n\n\n\n"}
{"id": "34066587", "url": "https://en.wikipedia.org/wiki?curid=34066587", "title": "Technological transitions", "text": "Technological transitions\n\nTechnological innovations have occurred throughout history and rapidly increased over the modern age. New technologies are developed and co-exist with the old before supplanting them. Transport offers several examples; from sailing to steam ships to automobiles replacing horse-based transportation. Technological transitions (TT) describe how these technological innovations occur and are incorporated into society. Alongside the technological developments TT considers wider societal changes such as “user practices, regulation, industrial networks (supply, production, distribution), infrastructure, and symbolic meaning or culture”. For a technology to have use, it must be linked to social structures human agency and organisations to fulfil a specific need. Hughes refers to the ‘seamless web’ where physical artefacts, organisations, scientific communities, and social practices combine. A technological system includes technical and non-technical aspects, and it a major shift in the socio-technical configurations (involving at least one new technology) is when a technological transition occurs.\n\nWork on technological transitions draws on a number of fields including history of science, technology studies, and evolutionary economics. The focus of evolutionary economics is on economic change, but as a driver of this technological change has been considered in the literature. Joseph Schumpeter, in his classic \"Theory of Economic Development\" placed the emphasis on non-economic forces as the driver for growth. The human actor, the entrepreneur is seen as the cause of economic development which occurs as a cyclical process. Schumpeter proposed that radical innovations were the catalyst for Kondratiev cycles.\n\nThe Russian economist Kondratiev proposed that economic growth operated in boom and bust cycles of approximately 50 year periods. These cycles were characterised by periods of expansion, stagnation and recession. The period of expansion is associated with the introduction of a new technology, e.g. steam power or the microprocessor. At the time of publication, Kondratiev had considered that two cycles had occurred in the nineteenth century and third was beginning at the turn of the twentieth. Modern writers, such as Freeman and Perez outlined five cycles in the modern age:\n\n\nFreeman and Perez proposed that each cycle consists of pervasive technologies, their production and economic structures that support them. Termed ‘techno-economic paradigms’, they suggest that the shift from one paradigm to another is the result of emergent new technologies. \n\nFollowing the recent economic crisis, authors such as Moody and Nogrady have suggested that a new cycle is emerging from the old, centred on the use of sustainable technologies in a resource depleted world.\n\nThomas Kuhn described how a paradigm shift is a wholesale shift in the basic understanding of a scientific theory. Examples in science include the change of thought from miasma to germ theory as a cause of disease. Building on this work, Giovanni Dosi developed the concept of ’technical paradigms’ and ‘technological trajectories’. In considering how engineers work, the technical paradigm is an outlook on the technological problem, a definition of what the problems and solutions are. It charts the idea of specific progress. By identifying the problems to be solved the paradigm exerts an influence on technological change. The pattern of problem solving activity and the direction of progress is the technological trajectory. In similar fashion, Nelson and Winter (,)defined the concept of the ‘technological regime’ which directs technological change through the beliefs of engineers of what problems to solve. The work of the actors and organisations is the result of organisational and cognitive routines which determines search behaviour. This places boundaries and also trajectories (direction) to those boundaries.\n\nIn analysing (historic) cases of technological transitions researchers from the systems in transition branch of transitions research have used a multi-level perspective (MLP) as a heuristic model to understand changes in socio-technical systems. () Innovation system approaches traditionally focus on the production side. A socio-technical approach combines the science and technology in devising a production, with the application of the technology in fulfilling a societal function. Linking the two domains are the distribution, infrastructure and markets of the product. This approach considers a transition to be multi-dimensional as technology is only one aspect. \n\nThe MLP proposes three analytical levels: the niche, regime and landscape. \n\nNiche (Micro-level)\nRadical innovations occur at the niche level. These act as ‘safe havens’ for fledgling technologies to develop, largely free from market pressures which occur at the regime level. The US Military has acted as niche for major twentieth century technologies such as the aircraft, radio and the internet. More recently, California’s Silicon Valley has provided an arena for ICT focused technologies to emerge. Some innovations will challenge the existing regime while others fail. \n\nRegime (Meso-level)\nThe socio-technical regime, as defined by Geels, includes a web of inter-linking actors across different social groups and communities following a set of rules. In effect, the established practices of a given system. Seven dimensions have been identified in the socio-technical regime: technology, user practices and application, the symbolic meaning of technology, infrastructure, policy and techno-scientific knowledge. Change does occur at the regime level but it is normally slow and incremental unlike the radical change at the niche level. The actors who constitute the existing regime are set to gain from perpetuating the incumbent technology at the expense of the new. This is known as ‘lock-in’.\n\nLandscape (Macro-level)\nExogenous to the previous levels is the socio-technical landscape. A broad range of factors are contained here, such as economic pressures, cultural values, social trends, wars and environmental issues. Change occurs at an even slower rate than at the regime level. \n\nA transition is said to happen when a regime shift has occurred. This is the result of the interplay between the three levels. Regimes are relatively inert and resistant to change being structured to incremental innovation following established trajectories. As such, transitions are difficult to achieve. The current regime is typically suffering internal issues. Pressure from the landscape level may cause ‘cracks’ or ‘windows of opportunity’ through which innovations at the niche level may initially co-exist with the established technology before achieving ascendency. Once the technology has fully embedded into society the transition is said to be completed.\n\nThe MLP has been used in describing a range of historic transitions in socio-technical regimes for mobility, sanitation, food, lighting and so on. While early research focused on historical transitions, a second strand of research was more focused on transitions to sustainable technologies in key sectors such as transport, energy and housing.\n\nGeels presented three historical transitions on system innovation relating to modes of transportation. The technological transition from sailing ships to steamships in the UK will be summarised and shown in the context of a wider system innovation. \n\nGreat Britain was the world’s leading naval power in the nineteenth century, and led the way in the transition from sail to steam. At first, the introduction of steam technology co-existed with the current regime. Steam tugs assisted sail ships into port and hybrid steam / sail ships appeared. Landscape developments create the necessity for improvements in the technology. A demand for trans-Atlantic emigration was prompted by the Irish potato famine, European political instability and the lure of gold in California. The requirement for such arduous journeys had prompted a wealth of innovations at the niche level in steamship-development. From the late 1880s, as steamship technology improved and costs dropped, the new technology was widely diffused and a new regime established. The changes go beyond a technological transition as it involved new ship management and fleet management practices, new supporting infrastructures and new functionalities.\n\nThe nature of transitions varies and the differing qualities result in multiple pathways occurring. Geels and Schot defined five transition paths:\n\n\nSix characteristics of technological transitions have been identified.,\n\n\"Transitions are co-evolutionary and multi-dimensional\"\nTechnological developments occur intertwined with societal needs, wants and uses. A technology is adopted and diffused based on this interplay between innovation and societal requirements. Co-evolution has different aspects. As well as the co-evolution of technology and society, aspects between science, technology, users and culture have been considered.\n\"Multi-actors are involved\"\nScientific and engineering communities are central to the development of a technology, but a wide range of actors are involved in a transition. This can include organisations, policy-makers, government, NGOs, special interest groups and others.\n\n\"Transitions occur at multiple levels\"\nAs shown in the MLP transitions occur through the interplay of processes at different levels. \n\n\"Transitions are a long-term process\"\nComplete system-change takes time and can be decades in the making. Case studies show them to be between 40 and 90 years.\n\n\"Transitions are radical\"\nFor a true transition to occur the technology has to be a radical innovation. \n\n\"Change is Non-linear\"\nThe rate of change will vary over time. For example, the pace of change may be slow at the gestation period (at the niche level) but much more rapid when a breakthrough is occurring.\n\nDiffusion of an innovation is the concept of how it is picked up by society, at what rate and why. Everett (1962).The diffusion of a technological innovation into society can be considered in distinct phases. Pre-development is the gestation period where the new technology has yet to make an impact. Take-off is when the process of a system shift is beginning. A breakthrough is occurring when fundamental changes are occurring in existing structures through the interplay of economic, social and cultural forces. Once the rate of change has decreased and a new balance is achieved, stabilization is said to have occurred. A full transition involves an overhaul of existing rules and change of beliefs which takes time, typically spanning at least a generation. This process can be speeded-up through seismic, unforeseen events such as war or economic strife. \n\nGeels proposed a similar four phased approach which draws on the multi-level perspective (MLP) developed by Dutch scholars. Phases one sees the emergence of a novelty, born from the existing regime. Development then occurs in the niche level at phase two. As before, breakthrough then occurs at phase three. In the parlance of the MLP the new technology, having been developed at the niche level, is in competition with the established regime. To breakthrough and achieve wide diffusion, external factors – ‘windows of opportunity’ are required.\n\nA number of possible circumstances can act as windows of opportunity for the diffusion of new technologies: \n\n\nAlongside external influences, internal drivers catalyse diffusion. These include economic factors such as the price performance ration. Socio-technical perspectives focus on the links between disparate social and technological elements. Following the breakthrough, the final phases see the new technology supersede the old.\n\nThe study of technological transitions has an impact beyond academic interest. The transitions referred to in the literature may relate to historic processes, such as the transportation transitions studied by Geels, but system changes are required to achieve a safe transition to a low carbon-economy. (). Current structural problems are apparent in a range of sectors. Dependency on oil is problematic in the energy sector due to availability, access and contribution to greenhouse gas (GHG) emissions. Transportation is a major user of energy causing significant emission of GHGs. Food production will need to keep pace with an ever-growing world population while overcoming challenges presented by global warming and transportation issues. Incremental change has provided some improvements but a more radical transition is required to achieve a more sustainable future. \n\nDeveloped from the work on technological transitions is the field of transition management. Within this is an attempt to shape the direction of change complex socio-technical systems to more sustainable patterns. Whereas work on technological transitions is largely based on historic processes, proponents of transition management seek to actively steer transitions in progress.\n\nGenus and Coles outlined a number of criticisms against the analysis of technological transitions, in particular when using the MLP. Empirical research on technological transitions occurring now has been limited, with the focus on historic transitions. Depending on the perspective on transition case studies they could be presented as having occurred on a different transition path to what was shown. For example, the bicycle could be considered an intermediate transport technology between the horse and the car. Judged from shorter different time-frame this could appear a transition in its own right. Determining the nature of a transition is problematic; when it started and ended, or whether one occurred in the sense of a radical innovation displacing an existing socio-technical regime. The perception of time casts doubt on whether a transition has occurred. If viewed over a long enough period even inert regimes may demonstrate radical change in the end. The MLP has also been criticised by scholars studying sustainability transitions using Social Practice Theories.\n\n"}
{"id": "30862857", "url": "https://en.wikipedia.org/wiki?curid=30862857", "title": "Technology and society", "text": "Technology and society\n\nTechnology society and life or technology and culture refers to cyclical co-dependence, co-influence, and co-production of technology and society upon the other (technology upon culture, and vice versa). This synergistic relationship occurred from the dawn of humankind, with the invention of simple tools and continues into modern technologies such as the printing press and computers. The academic discipline studying the impacts of science, technology, and society, and vice versa is called science and technology studies.\n\nThe importance of stone tools, circa 2.5 million years ago, is considered fundamental in the human development in the hunting hypothesis.\n\nPrimatologist, Richard Wrangham, theorizes that the control of fire by early humans and the associated development of cooking was the spark that radically changed human evolution. Texts such as \"Guns, Germs, and Steel\" suggest that early advances in plant agriculture and husbandry fundamentally shifted the way that collective groups of individuals, and eventually societies, developed.\n\nTechnology has become a huge part in society and day-to-day life. When societies know more about the development in a technology, they become able to take advantage of it. When an innovation achieves a certain point after it has been presented and promoted, this technology becomes part of the society.The use of technology in education provides students with technology literacy, information literacy, capacity for life-long learning and other skills necessary for the 21st century workplace. Digital technology has entered each process and activity made by the social system. In fact, it constructed another worldwide communication system in addition to its origin.\n\nA 1982 study by \"The New York Times\" described a technology assessment study by the Institute for the Future, \"peering into the future of an electronic world.\" The study focused on the emerging videotex industry, formed by the marriage of two older technologies, communications and computing. It estimated that 40 percent of American households will have two-way videotex service by the end of the century. By comparison, it took television 16 years to penetrate 90 percent of households from the time commercial service was begun.\n\nSince the creation of computers achieved an entire better approach to transmit and store data. Digital technology became commonly used for downloading music and watching movies at home either by DVDs or purchasing it online.\nDigital music records are not quite the same as traditional recording media. Obviously, because digital ones are reproducible, portable and free.\n\nSeveral states started to implement education technology in schools, universities and colleges. According to the statistics, in the early beginnings of 1990s the use of Internet in schools was ,on average, 2-3%. Continuously, by the end of 1990s the evolution of technology increases rapidly and reaches to 60%, and by the year of 2008 nearly 100% of schools use Internet on educational form. According to ISTE researchers, technological improvements can lead to numerous achievements in classrooms. E-learning system, collaboration of students on project based learning, and technological skills for future results in motivation of students. \n\nAlthough these previous examples only show a few of the positive aspects of technology in society, there are negative side effects as well. Within this virtual realm, social media platforms such as Instagram, Facebook, and Snapchat have altered the way Generation Y culture is understanding the world and thus how they view themselves. In recent years, there has been more research on the development of social media depression in users of sites like these. \"Facebook Depression\" is when users are so affected by their friends' posts and lives that their own jealousy depletes their sense of self-worth. They compare themselves to the posts made by their peers and feel unworthy or monotonous because they feel like their lives are not nearly as exciting as the lives of others.\n\nAnother instance of the negative effects of technology in society, is how quickly it is pushing younger generations into maturity. With the world at their fingertips, children can learn anything they wish to. But with the uncensored sources from the internet, without proper supervision, children can be exposed to explicit material at inappropriate ages. This comes in the forms of premature interests in experimenting with makeup or opening an email account or social media page—all of which can become a window for predators and other dangerous entities that threaten a child's innocence. Technology has a serious effect on youth's health. The overuse of technology is said to be associated with sleep deprivation which is linked to obesity and poor academic performance in the lives of adolescents.\n\nIn ancient history, economics began when spontaneous exchange of goods and services was replaced over time by deliberate trade structures. Makers of arrowheads, for example, might have realized they could do better by concentrating on making arrowheads and barter for other needs. Regardless of goods and services bartered, some amount of technology was involved—if no more than in the making of shell and bead jewelry. Even the shaman's potions and sacred objects can be said to have involved some technology. So, from the very beginnings, technology can be said to have spurred the development of more elaborate economies.Technology is seen as primary source in economic development.\n\nTechnology advancement and economic growth are related to each other.The level of technology is important to determine the economic growth.It is the technological process which keeps the economy moving.\n\nIn the modern world, superior technologies, resources, geography, and history give rise to robust economies; and in a well-functioning, robust economy, economic excess naturally flows into greater use of technology. Moreover, because technology is such an inseparable part of human society, especially in its economic aspects, funding sources for (new) technological endeavors are virtually illimitable. However, while in the beginning, technological investment involved little more than the time, efforts, and skills of one or a few men, today, such investment may involve the collective labor and skills of many millions.\n\nConsequently, the sources of funding for large technological efforts have dramatically narrowed, since few have ready access to the collective labor of a whole society, or even a large part. It is conventional to divide up funding sources into governmental (involving whole, or nearly whole, social enterprises) and private (involving more limited, but generally more sharply focused) business or individual enterprises.\n\nThe government is a major contributor to the development of new technology in many ways. In the United States alone, many government agencies specifically invest billions of dollars in new technology.\n\n[In 1980, the UK government invested just over six million pounds in a four-year program, later extended to six years, called the Microelectronics Education Programme (MEP), which was intended to give every school in Britain at least one computer, software, training materials, and extensive teacher training. Similar programs have been instituted by governments around the world.]\n\nTechnology has frequently been driven by the military, with many modern applications developed for the military before they were adapted for civilian use. However, this has always been a two-way flow, with industry often developing and adopting a technology only later adopted by the military.\n\nEntire government agencies are specifically dedicated to research, such as America's National Science Foundation, the United Kingdom's scientific research institutes, America's Small Business Innovative Research effort. Many other government agencies dedicate a major portion of their budget to research and development.\n\nResearch and development is one of the smallest areas of investments made by corporations toward new and innovative technology.\nMany foundations and other nonprofit organizations contribute to the development of technology. In the OECD, about two-thirds of research and development in scientific and technical fields is carried out by industry, and 98 percent and 10 percent, respectively, by universities and government. But in poorer countries such as Portugal and Mexico the industry contribution is significantly less. The U.S. government spends more than other countries on military research and development, although the proportion has fallen from about 30 percent in the 1980s to less than 10 percent.\n\nThe 2009 founding of Kickstarter allows individuals to receive funding via crowdsourcing for many technology related products including both new physical creations as well as documentaries, films, and webseries that focus on technology management. This circumvents the corporate or government oversight most inventors and artists struggle against but leaves the accountability of the project completely with the individual receiving the funds.\n\n\nThe implementation of technology influences the values of a society by changing expectations and realities. The implementation of technology is also influenced by values. There are (at least) three major, interrelated values that inform, and are informed by, technological innovations:\n\nTechnology often enables organizational and bureaucratic group structures that otherwise and heretofore were simply not possible. Examples of this might include:\n\nTechnology enables greater knowledge of international issues, values, and cultures. Due mostly to mass transportation and mass media, the world seems to be a much smaller place, due to the following:\n\nTechnology provides an understanding, and an appreciation for the world around us.\n\nMost modern technological processes produce unwanted by products in addition to the desired products, which is known as industrial waste and pollution. While most material waste is re-used in the industrial process, many forms are released into the environment, with negative environmental side effects, such as pollution and lack of sustainability. Different social and political systems establish different balances between the value they place on additional goods versus the disvalues of waste products and pollution. Some technologies are designed specifically with the environment in mind, but most are designed first for economic or ergonomic effects. Historically, the value of a clean environment and more efficient productive processes has been the result of an increase in the wealth of society, because once people are able to provide for their basic needs, they are able to focus on less tangible goods such as clean air and water.\n\nThe effects of technology on the environment are both obvious and subtle. The more obvious effects include the depletion of nonrenewable natural resources (such as petroleum, coal, ores), and the added pollution of air, water, and land. The more subtle effects include debates over long-term effects (e.g., global warming, deforestation, natural habitat destruction, coastal wetland loss.)\n\nEach wave of technology creates a set of waste previously unknown by humans: toxic waste, radioactive waste, electronic waste.\n\nOne of the main problems is the lack of an effective way to remove these pollutants on a large scale expediently. In nature, organisms \"recycle\" the wastes of other organisms, for example, plants produce oxygen as a by-product of photosynthesis, oxygen-breathing organisms use oxygen to metabolize food, producing carbon dioxide as a by-product, which plants use in a process to make sugar, with oxygen as a waste in the first place. No such mechanism exists for the removal of technological wastes.\n\nSociety also controls technology through the choices it makes. These choices not only include consumer demands; they also include:\n\nAccording to Williams and Edge, the construction and shaping of technology includes the concept of choice (and not necessarily conscious choice). Choice is inherent in both the design of individual artifacts and systems, and in the making of those artifacts and systems.\n\nThe idea here is that a single technology may not emerge from the unfolding of a predetermined logic or a single determinant, technology could be a garden of forking paths, with different paths potentially leading to different technological outcomes. This is a position that has been developed in detail by Judy Wajcman. Therefore, choices could have differing implications for society and for particular social groups.\n\nIn one line of thought, technology develops autonomously, in other words, technology seems to feed on itself, moving forward with a force irresistible by humans. To these individuals, technology is \"inherently dynamic and self-augmenting.\"\n\nJacques Ellul is one proponent of the irresistibleness of technology to humans. He espouses the idea that humanity cannot resist the temptation of expanding our knowledge and our technological abilities. However, he does not believe that this seeming autonomy of technology is inherent. But the perceived autonomy is because humans do not adequately consider the responsibility that is inherent in technological processes.\n\nLangdon Winner critiques the idea that technological evolution is essentially beyond the control of individuals or society in his book Autonomous Technology. He argues instead that the apparent autonomy of technology is a result of \"technological somnambulism,\" the tendency of people to uncritically and unreflectively embrace and utilize new technologies without regard for their broader social and political effects.\n\nIndividuals rely on governmental assistance to control the side effects and negative consequences of technology.\n\nRecently, the social shaping of technology has had new influence in the fields of e-science and e-social science in the United Kingdom, which has made centers focusing on the social shaping of science and technology a central part of their funding programs.\n\n\n"}
{"id": "168531", "url": "https://en.wikipedia.org/wiki?curid=168531", "title": "The Age of Plastic", "text": "The Age of Plastic\n\nThe Age of Plastic is the debut studio album by the British new wave duo The Buggles, composed of Trevor Horn and Geoff Downes. The name of the record was conceived from the group's intention of being a \"plastic group\". The album has lyrical themes of nostalgia and anxiety about the possible effects of modern technology. The titular song, \"Living in the Plastic Age\", views the experiences of watching media coverage of the Vietnam War, while \"Kid Dynamo\" follows a child overexposed to media and its resulting effects on him. Described by writers as the first technopop landmark, the record is an electropop new wave album that includes musical elements and influences of disco, punk, progressive rock and 1960s pop music. In a 1979 interview, Downes defined the album as \"science fiction music. It's like modern psychedelic music. It's very futuristic.\" \n\nHorn used pre-dated technology for the album to have sounds unlike what was typical in records that were released at the time. The songs were written by The Buggles between 1977 and 1979, with contributions on several tracks from Bruce Woolley. The backing tracks were recorded at Virgin's Town House in West London, while the vocals were recorded and mixed at Sarm East Studios. Mixing was completed before Christmas 1979.\n\n\"The Age of Plastic\" was released by Island Records in Australia, January 1980, and later in February in the UK. The album's release followed the success of the group's 1979 first single, \"Video Killed the Radio Star\", which reached number 1 on the UK Singles Chart. Most of the songs for the album were written during promotion of the song. Three subsequent singles, \"The Plastic Age\", \"Clean, Clean\" and \"Elstree\", all released in 1980, followed \"Video\", and also charted in the UK, reaching number 16, 38 and 55 respectively. In Europe, \"The Age of Plastic\" reached the Top 20 in Italy and the Top 40 in France, Norway, Sweden and the United Kingdom. In other continents, the album reached the top 40 most played in Japan and was number 83 on the Canadian \"RPM\" albums chart. It has been met with positive critical response, with some critics comparing it to other albums of its genre. There have been two reissues of the album, in 2000 and in 2010. A September 2010 performance of the album by the Buggles, a gig live at the \"Ladbroke Grove's Supperclub\" in Notting Hill, London, marked the first time the group performed it in its entirety.\n\nIn 2016, \"Paste\" magazine called \"The Age of Plastic\" the 45th best new wave album of all time.\n\nGeoff Downes formed The Buggles in 1977 in Wimbledon, South West London with Trevor Horn and Bruce Woolley. The trio had done rough demos of early compositions such as \"Video Killed the Radio Star\", \"Clean, Clean\", and \"On TV\", a track later included on their second album \"Adventures in Modern Recording\". Talking about the formation of the Buggles, Downes said about the demos: \nThe Buggles were signed to Island Records, who gave Horn and Downes recording and publishing contracts, and started recording their upcoming first studio album in the first half of 1979. Although Woolley was originally intended to be the band's lead vocalist, he left the group during the sessions to form his own band, The Camera Club, who also did versions of \"Clean, Clean\" and \"Video\" that appeared on their album \"English Garden\". When \"Video Killed the Radio Star\" was a huge commercial success, they had realized the problem that they had not finished an album's worth of material yet, so they wrote more during the promotion of the single, while in airport lounges, dressing rooms, rehearsal rooms and studios.\n\n\"The Age of Plastic\" had a budget of £60,000. Hugh Padgham recorded the backing tracks at Virgin's Town House in West London, due to Sarm East's very small size and Horn wanting to record real drums. The Buggles went to London’s Wardour Street to gain the attention of two females to appear on the album. The mixing and Horn's vocal recording were later done at Sarm East Studios, and mixing was finished before Christmas 1979 for a new year's release of the album. Sarm East mixer Gary Langan used a 40-input Trident TSM console to record and mix the album, which was housed inside the same control room as two Studer A80 24-track machines and outboard gear that included an EMT 140 echo plate, Eventide digital delay, Eventide phaser, Marshall Time Modulator, Kepex noise gates, Urei and Orban EQs, and Urei 1176, Dbx 160, and UA LA2 and LA3 compressors.\n\nThe vocals of the album were recorded at Sarm East to a click track using a Roland TR-808 drum machine, and other various machines and boxes synced up the tracks. As Langan recalls: \"In those days of relatively limited technology we again had to push what we had to the limit... If, for instance, something required an effect, whether it be tape delay or phasing or some big, delayed reverb, the art was to get that effect right and record it... It all had to be done and then, as I said, it would influence the next process.\" Langan has noted that balancing the backing vocals in the songs from the album was a major problem, due to no available terabytes of storage. He stated: \"We'd make it as clean as we possibly could, bounce that down to two tracks and then we'd erase.\"\n\nUnable to make the album sound like what was typical of other records released in the late 1970s, as well as finding it boring, Horn \"figured that if I couldn't get records to sound like Elton John, which I couldn't because I couldn't figure out how they did it, then whatever I could do, I'd better exaggerate it.\" He had also wanted to \"perverse things with sound, except that in 1978 and 1979, none of the equipment which would later allow me to do that was available. So I had to pre-date that technology by finding my own ways of achieving certain sounds.\" \"Audio\" noted the album's sound to be reminiscent of duo Godley & Creme's debut album \"Consequences\".\n\nWriters have labeled \"The Age of Plastic\" as the first landmark of another electropop era. In his book \"Are We Not New Wave?: Modern Pop at the Turn of the 1980s\", Theo Cateforis wrote that the album's title and the songs \"I Love You (Miss Robot)\" and \"Astroboy\" \"picture the arrival of the 1980s as a novelty era of playful futurism\". In a 1979 interview, Downes defined the album as \"science fiction music. It's like modern psychedelic music. It's very futuristic.\" Writers have described it as a mixture of synthpop and new wave music, with elements of disco, punk, progressive rock and pop music from the 1960s. The music on the album was also influenced by groups such as 10cc, ELO and Kraftwerk. \nJournalists noted the tracks' instrumentations of guitars, bass guitar, drums, vocoded, robotic and female vocals, and synthesizers used to emulate orchestral instruments, and well as compositional elements of a variety of complex builds. Downes said that he used five synthesizers in making \"The Age of Plastic\", which were used to \"fake up things and to provide effects we won’t use them in the manner that somebody like John Foxx does.\" According to Horn: \"We used about three different drummers including one <nowiki>[</nowiki>Richard James Burgess<nowiki>]</nowiki> from Landscape and Johnny Richardson from The Rubettes, who’s really good. We also used the occasional session guitarist to play various bits and there were three or four girl singers involved. Apart from that, we did everything ourselves.\" Downes claimed of using George Shearing's trick of doubling melody lines in block chords very heavily on some of the songs.\n\n\"The Age of Plastic\" is a tragicomic concept album with lyrical themes of intense nostalgia and anxiety about the possible effects of modern technology. The lyrics, which were written by Trevor Horn, were inspired by the works of J.G. Ballard. The Buggles claimed they were necessarily a \"plastic group\" to meet the needs of a plastic age, which was why their debut album was called \"The Age of Plastic\". Downes said that the lyrics were \"trying to make cynical comments on a number of issues.\" Eight tracks are included on \"The Age of Plastic\": \"Living in the Plastic Age\", \"Video Killed the Radio Star\", \"Kid Dynamo\", \"I Love You (Miss Robot)\", \"Clean, Clean\", \"Elstree\", \"Astroboy (And the Proles on Parade)\", and \"Johnny (on the Monorail)\". The album's lyrical concept was compared by \"Orange Coast\" magazine to that of the works of Canadian progressive rock band Klaatu.\n\n\"The Age of Plastic\" starts with the title track \"The Plastic Age,\" which, according to Downes, is about a person's view of plastic experiences from watching and reading news reports on the Vietnam War. \"Video Killed the Radio Star,\" the second track, refers to a period of technological change in the 1960s, the desire to remember the past and the disappointment that children of the current generation would not appreciate the past. The fast-paced third song, \"Kid Dynamo,\" is about the effects of media on a futuristic kid of the 1980s. \"I Love You (Miss Robot)\" is the album's fourth track, that Downes said it talks about \"being on the road and making love to someone you don't really like, while all the time you're wanting to phone someone who's a long way off.\" Wave Maker Magazine viewed the song as \"a darkly soothing, bass guitar-driven ballad which brings us back into cyberpunk country.\"\n\n\"Clean, Clean\" is the album's fifth track, and follows the story of a young boy who grows out of being a gangster, and, despite not willing to do so, he will at least try to keep the fighting clean. Wave Maker Magazine found \"Elstree,\" the album's sixth song, as lyrically similar to \"Video Killed the Radio Star,\" as it follows \"a failed actor taking up a more regular position behind the scenes and looking back at his life in regret.\" The slow-tempo ballad \"Astroboy (And the Proles on Parade)\", according to Wave Maker, \"once again revisits cyberpunk with a much lighter vibe, although the keyboards do occasionally border darker realms, expecially with the post-chorus hook,\" and the album closer \"Johnny on the Monorail\" has a \"pop atmosphere\" that \"better suits the flow of the rest of the album.\"\n\n\"The Age of Plastic\" was first released in Australia on 10 January 1980, and in the United Kingdom on 4 February the same year. In 2000, as part of the \"Island Remasters\" series, the album was reissued with three bonus tracks, \"Technopop\", \"Island\", and \"Johnny on the Monorail (A Very Different Version)\". The album was remastered and re-released again on 24 February 2010 in Japan. The new edition included 9 additional tracks, three of which were from the 2000 re-release album. Initially, songs from \"The Age of Plastic\" were played on English radio stations from 31 December 1979, and two advertisements of the album were also released. Domestically, \"The Age of Plastic\" appeared on the UK Album Charts for six weeks, reaching 27 on the chart. In Nordic countries, the record debuted at number 32 in Norway, eventually reaching number 23, and peaked at number 24 in Sweden. It also reached the number 15 spot on the French Albums Chart, and number 35 in Japan. The album's 2010 reissue briefly appeared at number 225 in Japan.\n\nFour singles were released in support of the record, with the second track \"Video Killed the Radio Star\" being released as the album's leading single. The LP version of the song was included on the 2010 reissue of the album, with the song \"Kid Dynamo\" as its B-side. The single was commercially successful, topping the record charts in 16 countries, including the UK Singles Chart. The album's title track \"Living in the Plastic Age\" was released as the second single. The single version was later included in the 2010 reissue. The fourth track \"Clean, Clean\" was released as the third single from the album, while \"Elstree\" was the album's fourth and final single. All of the singles had chart success in the UK, with \"The Plastic Age\" and \"Clean, Clean\" gaining chart success on international level. A single and special DJ version of \"Elstree\" also appeared on the 2010 reissue of the album, as well as a 12\" version of \"Clean, Clean\".\n\nOn 28 September 2010, The Buggles reunited to play their first full-length live concert. The event was billed as \"The Lost Gig\" and took place at \"Ladbroke Grove's Supperclub\" in Notting Hill, London. It was a fundraiser with all earnings going to the Royal Hospital for Neuro-disability. With the exception of \"Video Killed the Radio Star\" and \"The Plastic Age\", which the band had previously played together, \"The Lost Gig\" saw the first live performances of all of the remaining songs from \"The Age of Plastic\".\n\nMost reviews of \"The Age of Plastic\" have been approving, with some critics comparing it to other albums of its genre. \"Melody Maker\" noted that the album is \"all jerky twitchings and absurdly inflated post-punk melodrama\" and named it as \"essential\". \"The Canberra Times\"'s Keith Gosman found the production excellent and said the album sounded \"crisp as fresh dollar bills\". \"The Encyclopedia of Science Fiction\" described the LP as \"one of the best examples of the decade's characteristically disposable pop\", while \"Spin\" named it one of the \"8 Essentials of Post-Kraftwerk Pop\".\n\nIn a retrospective review, Jeri Montesano from AllMusic gave the album 4.5 out of 5 stars and described it as \"a fun record that doesn't need to be taken too seriously\". He further said that \"it would be difficult to find a record from this era that sounds half as good. Pop rarely reaches these heights.\" While reviewing the Buggles' second album, the same author stated that both albums \"still sound fresh\" compared to 1990s pop music. An Amazon.com editor named Grant Alden also compared the album to 1990s pop, and labeled the group as \"Part of the early-1980s great explosion of pop music [...] to have any real impact\". \"Trouser Press\" called both albums \"technically stunning, reasonably catchy and crashingly hollow.\"\n\n\"Smash Hits\" rated the album 8 out of 10 and called it \"quite human and therefore the most enjoyable of the lot\". In a review of the 1999 re-release of the album, Vincent L. of \"Krinein Magazine\" praised the entire record because it contains \"catchy tracks and joyful melodies\". Napster's Nicholas Baker liked the album's composition and concluded \"this LP is not so much a guilty pleasure as an essential point in electropop history.\" \"Metro Pulse\"<nowiki>'</nowiki>s Anthony Nownes found the tunes \"punchy, memorable\" and \"accessible\", concluding his review with \"If all rock records sounded like this—shiny and slick and highly processed—the world would be terrible. But a few Trevor Horns—people who use studio technology the way a curious and playful child uses a room full of fictile toys—are nice to have around.\"\n\nOn the mixed to negative side, Joseph Stannard, in his review for \"Adventures in Modern Recording\", opined \"The Age of Plastic\" \"sounds like unfinished business, a series of good ideas in need of elaboration\". Dave Marsh and John Swenson, writing for \"The Rolling Stone Record Guide\", opined that \"aside from the wonderful 'Video Killed the Radio Star' — perhaps the most successful recent example of a single where the production was catchier than the material\", the album was \"high-tech dreck\". Betty Page from \"Sounds\" commented that the group \"stretches uncomfortably out into the long playing medium like a skein of well-chewed bubblegum.\" In a review of the 1999 reissue of the album, Richard Wallace of \"Daily Mirror\" wrote that the record \"shows how [The Buggles] pioneered the synth-led nonsense which fused much of the decade's pop, but had little creative imagination. That bombastic electro-sound became [Trevor] Horn's trademark as a producer. Skip it.\" Alexis Petridis of \"The Guardian\" has been the harshest on the LP, calling it \"awful beyond measure\" and \"everything bad that people say about the music industry: it's wasteful, it's stupid, it has no interest in actual music.\"\n\nPopular French bands such as Justice, Daft Punk and Phoenix have been known to include influences of \"The Age of Plastic\". Justice said that they were \"totally fascinated by The Buggles’ first album [\"The Age of Plastic\"]. It’s full of stuff we like - there’s a bit of electro, a bit of pop, a bit of classical going on there… We like the way they operated too, as an autonomous duo…\"\n\nAll songs written, produced and performed by The Buggles, except \"Video Killed the Radio Star\" and \"Clean, Clean\" written by Horn/Downes/Bruce Woolley.\n\n\n\n\n\nCredits are adapted from the album's liner notes.\nThe Buggles – producers\nAdditional musicians\n\nTechnical personnel\n"}
{"id": "40302986", "url": "https://en.wikipedia.org/wiki?curid=40302986", "title": "Warp-field experiments", "text": "Warp-field experiments\n\nWarp-field experiments are a series of current and proposed experiments to create and detect instances of spacetime warping. The ultimate goal is to prove or disprove the possibility of spacetime metric engineering with reasonable amounts of energy.\n\nSpacetime metric engineering is a requirement for physically recreating solutions of general relativity such as Einstein–Rosen bridges or the Alcubierre drive. Current experiments focus on the Alcubierre metric and its modifications. Alcubierre's work from 1994 implies that even if the required exotic matter with negative energy densities can be created, the total mass–energy demand for his proposed warp drive would exceed anything that could be realistically attained by human technology. Other researchers aimed to improve the energy efficiency (see Alcubierre drive: Difficulties), however the propositions remain mostly speculative. Research groups at NASA's Johnsons Space Center and Dakota State University currently aim to experimentally evaluate several new approaches, especially a redesigned energy-density topology as well as an implication of brane cosmology theory. If space actually were to be embedded in higher dimensions, the energy requirements could be decreased dramatically and a comparatively small energy density could already lead to a spacetime curvature measurable using an interferometer. The theoretical framework for the experiments dates back to work by Harold G. White from 2003 as well as work by White and Eric W. Davis from 2006 that was published in the AIP, where they also consider how baryonic matter could, at least mathematically, adopt characteristics of dark energy (see section below). In the process, they described how a toroidal positive energy density may result in a spherical negative-pressure region, possibly eliminating the need for actual exotic matter.\n\nThe metric derived by Alcubierre was mathematically motivated by cosmological inflation. The original \"warp-drive\" spacetime metric can be written in (t,x,y,z) coordinates as:\n\nIt uses the curve (world line) formula_2 where formula_3 expresses the x-coordinate position of the moving spaceship frame.\n\nThe radius formula_4 is the euclidean distance from the curve. Furthermore, formula_5 is the speed of light and formula_6 is equivalent to formula_7, the velocity associated with the curve.\n\nThe shaping function formula_8 is any smooth function that satisfies formula_9 and decreases from the origin, vanishing at formula_10 for some point formula_11.\n\nThe driving phenomenon behind the apparently arbitrary velocity (including formula_12) could be (and was) postulated to be the York extrinsic time, formula_13, defined as:\nIt provides for the contraction of space in front, and expansion in the back of the warp bubble. The idea can be seen in some way as an applied extension of the hypothesis that the early universe also saw a rapid inflationary expansion that possibly exceeded the speed of light for some time. However, according to research, it appears that the York time behaviour is merely a side effect of another underlying mechanism. The problem that led to the assumption that York time is only one part in the larger picture is the unusual symmetry in the required energy density. Using the Einstein field equations, the stress energy tensor formula_15 can be derived from the Alcubierre metric, resulting in the necessary energy density:\n\nwhere formula_17 is the gravitational constant and formula_18.\n\nThis corresponds to a negative toroidal energy density symmetric to the x-axis. It is notable here, that during a sensitivity analysis for the DARPA-funded 100 Year Starship symposium, Harold White discovered that changing the energy-density distribution from a thin ring as originally proposed to more of a doughnut shape (effectively increasing the warp-bubble wall thickness) can decrease the required total negative energy by several orders of magnitude.\n\nThe symmetry in the energy distribution leads to the scenario in which the choice of positive x-axis is in fact arbitrary. The warp-drive mechanism would not know whether to go forward or backward along the x-axis. This paradox can be resolved by putting the Alcubierre metric into canonical form using Rindler's method and extracting the potential, formula_19. With the potential it is possible to derive the field equation for the spacetime expansion \"boost\", formula_20:\n\nThe boost can roughly be seen as a scalar multiplier acting on an initial velocity leading to the actual warp-velocity:\nThis boost is also an important analogy when considering higher-dimensional models (as seen below).\nThe expansion and contraction of space as measured by York time are now more of a secondary effect and can be considered equivalent to a pressure gradient of a moving sphere in a fluid. The scalar nature of the effect is an important clue when considering higher dimensions.\nConsidering the null geodesics for formula_23 inside the warp field, it can be seen that the world lines are space-like for external observers, but the moving frame never travels outside its local comoving light cone and thus does not violate special relativity.\n\nFrom there on, White and Davis showed similarities of the spacetime boost when considered in a higher-dimensional spacetime, such as in the Chung-Freese model. In this particular model our space exists on a brane and the space surrounding the brane is called the \"bulk\". The size of each extra dimension is considered to be at least finite and the latest research at CERN also constrains any large extra dimension theories. However, the actual size and total number of extra dimensions is not important when considering the implications on the boost field. The modified Robertson–Walker metric representing the model by Chung and Freese is:\nwhere the formula_25 term specifies our normal space and formula_26 the higher-dimensional bulk with our space located at the formula_27 plane.\n\nformula_28 is a typical cosmological expansion parameter (see accelerated expansion) and formula_29 is an arbitrary compactification factor for the extra dimensions. Considering the null geodesic solutions ( ds²=0 ) allows to develop the following relationship:\nFor zero formula_31 the expected speed of the photon is c, as expected. For large off-bulk coordinates formula_32 the speed formula_33 can be made arbitrarily large.\n\nThat means that light rays could have a spacelike appearance, an obvious parallel to the Alcubierre model. However, in the Alcubierre model the spacetime expansion boost is the driving phenomenon, whereas in the Chung–Freese model the off-brane bulk location U serves this purpose. It is thus theoretically possible that the boost of a 3+1 spacetime model is a scalar correction factor for higher-dimensional geometric effects on our brane, leading the following analogies to Alcubierre's model:\n\nIf a particle like an electron gains a high spacetime boost relative to an observer, it might actually leave the 3+1 brane (i.e. it gains non-zero U bulk coordinates) and its ability to interact electromagnetically diminishes. To illustrate this, White and Davis stated that in a 2D lab located at the x,y plane, a 2D electron that gets accelerated (obtains a high boost) gains a non-zero z coordinate. Thus, if a photon were to interact with it, it would need to be at the same (t,x,y,z) coordinate.\n\nConsidering the null geodesic equation again, one can see that if dU/dt=c, dX/dt=0 meaning that light comes to a standstill. This implies that a high hyperspace velocity reduces spacetime's \"stiffness\" or ability to resist being curved by energy, effectively reducing the energy requirements to warp it. This observation, together with the modified energy density distribution, first led researchers at NASA to begin thinking of testbeds to verify the new theoretical approach. Using the analogy between U and formula_19, it is obvious that a high velocity (dU/dt) with U=0 requires a field oscillation.\n\nBack in the Alcubierre model, it is notable that an outside observer would perceive a uniform potential (from the uniform boost within the sphere) that represents the warp-field region even though it originates from a toroidal energy density. It has similar characteristics as a Gaussian spherical surface held at constant electrostatic potential. To the outside observer, the warp-field sphere has a uniform energy density. By expanding the spherical region while maintaining the same relative boost value for the Gaussian surface, when considering the first law of thermodynamics, the following can be concluded (limited to the 3+1 brane):\nformula_39 can be replaced by formula_40, which is the total energy for the warp sphere with the same volume change formula_41 as on the right side of the equation.\n\nThus, the equation of state relating the pressure of the warp sphere formula_42 to its energy density formula_43 is\nwhich noticeably resembles the equation of state of cosmological vacuum energy (moreover, it is the equation of state of dark energy). If formula_43 of the original warp sphere is negative, formula_42 would be positive. However the last equation shows that the opposite is true, formula_43 is positive and formula_42 is negative. Considering that the spacetime expansion boost for the Alcubierre model can be made arbitrarily high depending on the choice of input variables, a high boost is thereby clearly not an exclusive feature common only to negative energy densities and could be obtained in the lab, provided powerful enough equipment.\n\nEinstein's field equations show that comparatively high amounts of energy are required for any significant curvature of spacetime under ordinary conditions. With the energy-requirement-decreasing concepts only being partially implemented yet, the available measurement methods are reaching the limits of what is technically possible. That is why current results remain mostly inconclusive until measurements can be further refined or the effect can be increased. New experimental setups have been proposed to boost sensitivity, and using the higher-dimensional yet still purely theoretical approach may increase any effect sufficiently to obtain significant results to prove or disprove the theory.\n\nThe only reported warp-field experiment currently being conducted is on a modified Michelson–Morley interferometer as proposed by Harold White and Eric Davis in 2003. This setup includes a ring-shaped energy device using high-voltage barium-titanate ceramic capacitors to attempt to warp space as shown in the diagram. White announced at a 2013 space conference that the first experimental results from this device were inconclusive.\n\nA time-of-flight experiment was proposed in 2013 using a modified Fabry–Pérot interferometer.\n"}
