{"id": "55782936", "url": "https://en.wikipedia.org/wiki?curid=55782936", "title": "Andy and Bill's law", "text": "Andy and Bill's law\n\nAndy and Bill's law is a statement regarding the relationship between hardware and software upgrades. The law originates from a humorous one-liner told in the 1990s during computing conferences: \"what Andy giveth, Bill taketh away.\" The phrase is a riff upon the business strategies of former Intel CEO Andy Grove, and former CEO of Microsoft, Bill Gates. Intel and Microsoft had entered into a lucrative partnership in the 1980s through to the 1990s, and the standard chipsets in Microsoft Windows were Intel brand. Despite the profit Intel gained from the deal, Grove felt that Gates wasn't making full use of the powerful capabilities of Intel chips, and that he was in fact refusing to upgrade his software to achieve optimum hardware performance. Grove's frustration with the dominance of Microsoft software over Intel hardware became public, which spawned the humorous catchphrase; and, later, Andy and Bill's law: that new software will always push ahead of hardware and get credit for the performance of the computer.\n"}
{"id": "39892656", "url": "https://en.wikipedia.org/wiki?curid=39892656", "title": "Barn cleaner", "text": "Barn cleaner\n\nA barn cleaner is a device used to aid in the cleaning of tie-stall and stanchion barns. It usually consists of a series of paddles chain linked together, that move manure and other waste through a gutter and into a manure spreader.\n"}
{"id": "9171756", "url": "https://en.wikipedia.org/wiki?curid=9171756", "title": "Cellular digital accessory", "text": "Cellular digital accessory\n\nA cellular digital accessory (CDA) number identifies the software version and customization of a Mobile phone.\n\nCustomization, among others, relates to operator-customized startup and shutdown videos, user greeting, web browser's home page and preinstalled bookmarks and also language packs that come preinstalled with the phone. CDA version depends on the country of purchase as well as on the network operator.\n\nThe Sony Ericsson Update Service will only allow software updates within one group of CDA numbers.\n\n"}
{"id": "39177819", "url": "https://en.wikipedia.org/wiki?curid=39177819", "title": "Cognitive computer", "text": "Cognitive computer\n\nA cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain.\n\nAn example of neural network implementations of cognitive convolution and deep learning is provided by the IBM company's Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain than the von Neumann architecture used in conventional computers. In 2017 Intel announced its own version of a cognitive chip in \"Loihi\", which will be available to university and research labs in 2018.\n\nIntel's self-learning neuromorphic chip, named Loihi, perhaps named after the Hawaiian seamount Loihi, offers substantial power efficiency designed after the human brain. Intel claims Loihi is about 1000 times more energy efficient than the general-purpose computing power needed to train the neural networks that rival Loihi’s performance. \nIn theory, this would support both machine learning training and inference on the same silicon independently of a cloud connection, and more efficient than using convolutional neural networks (CNNs) or deep learning neural networks. Intel points to a system for monitoring a person’s heartbeat, taking readings after events such as exercise or eating, and uses the cognitive computing chip to normalize the data and work out the ‘normal’ heartbeat. It can then spot abnormalities, but also deal with any new events or conditions.\n\nThe first iteration of the Loihi chip was made using Intel’s 14 nm fabrication process, and houses 128 clusters of 1,024 artificial neurons each for a total of 131,072 simulated neurons. This offers around 130 million synapses, which is still a rather long way from the human brain's 800 trillion synapses, and behind IBM’s TrueNorth, which has around 16 billion by using 64 by 4,096 cores.\n\nThe IBM cognitive computers implement learning using Hebbian theory. Instead of being programmable in a traditional sense within machine language or a higher level programming language such a device learns by inputting instances through an input device that are aggregated within a computational convolution or neural network architecture consisting of weights within a parallel memory system. An early instantiation of such a device has been developed in 2012 under the Darpa SyNAPSE program at IBM directed by Dharmendra Modha.\n\nIn 2017 this IBM 64-chip array will contain the processing equivalent of 64 million neurons and 16 billion synapses, yet each processor consumes just 10 watts of electricity.\nLike other neural networks, this system will be put to use in pattern recognition and sensory processing roles. The Air Force wants to combine the TrueNorth ability to convert multiple data feeds — whether it's audio, video or text — into machine readable symbols with a conventional supercomputer's ability to crunch data.\nThis isn't the first time that IBM's neural chip system has been integrated into cutting-edge technology. August, 2017 Samsung installed the chips in its Dynamic Vision Sensors enabling cameras to capture images at up to 2,000 fps while using just 300 milliwatts of power.\n\nThere are many approaches and definitions for a cognitive computer, and other approaches may be more fruitful than the others. \n\nSpecifically, there are critics who argue that a room-sized computer - like the case of Watson - is not a viable alternative to a three-pound human brain. Some also cite the difficulty for a single system to bring so many elements together such as the disparate sources of information as well as computing resources. During the 2018 World Economic Forum, there are experts who claim that cognitive systems could adopt the biases of their developers and this was demonstrated in the case of the Google image-recognition or computer vision algorithm, which identified African Americans unfavorably. \n\n\nhttp://www.foxnews.com/tech/2018/01/09/ces-2018-intel-gives-glimpse-into-mind-blowing-future-computing.html\n"}
{"id": "1290423", "url": "https://en.wikipedia.org/wiki?curid=1290423", "title": "Collingridge dilemma", "text": "Collingridge dilemma\n\nThe Collingridge dilemma is a methodological quandary in which efforts to influence or control the further development of technology face a double-bind problem:\nThe idea was coined by David Collingridge, The University of Aston, Technology Policy Unit, in his 1980 book \"The Social Control of Technology\". The dilemma is a basic point of reference in technology assessment debates.\n\nIn \"This Explains Everything,\" edited by John Brockman, technology critic Evgeny Morozov explains Collingridge's idea by quoting Collingridge himself: \"When change is easy, the need for it cannot be foreseen; when the need for change is apparent, change has become expensive, difficult, and time-consuming.\" \n\nIn \"The Pacing Problem, the Collingridge Dilemma & Technological Determinism\" by Adam Thierer, a senior research fellow at the Mercatus Center at George Mason University, the Collingridge dilemma is related to the \"pacing problem\" in technology regulation. The \"pacing problem\" refers to the notion that technological innovation is increasingly outpacing the ability of laws and regulations to keep up, first explained in Larry Downes' 2009 book \"The Laws of Disruption\", in which he famously states that \"technology changes exponentially, but social, economic, and legal systems change incrementally\". In Thierer's essay, he tries to correlate these two concepts by saying that \"the 'Collingridge dilemma' is simply a restatement of the pacing problem but with greater stress on the social drivers behind the pacing problem and an implicit solution to 'the problem' in the form of preemptive control of new technologies while they are still young and more manageable.\"\n\nA widely-adopted solution to Collingridge dilemma is the \"Precautionary Principle\", the belief that new innovations should not be embraced \"until their developers can prove that they will not cause any harm to individuals, groups, specific entities, cultural norms, or various existing laws, norms, or traditions\". If they fail to do so, this innovation should be \"prohibited, curtailed, modified, junked, or ignored\". This approach received criticisms from technology critics like Kevin Kelly who believe the contour of such principle is ill-defined and such principle is biased against anything new because it drastically elevates threshold for anything innovative. According to American philosopher Max More, the Precautionary Principle \"is very good for one thing—stopping technological progress...not because it leads in bad directions, but because it leads in no direction at all.\"\n"}
{"id": "27125801", "url": "https://en.wikipedia.org/wiki?curid=27125801", "title": "Comparison of train and tram tracks", "text": "Comparison of train and tram tracks\n\nA railway or railroad is a track where the vehicle travels over two parallel steel bars, called rails. The rails support and guide the wheels of the vehicles, which are traditionally either trains or trams. Modern light rail is a relatively new innovation which combines aspects of those two modes of transport. However fundamental differences in the track and wheel design are important, especially where trams or light railways and trains have to share a section of track, as sometimes happens in congested areas.\n\nBoth trams and trains have flanged iron wheels with a horizontal section transferring the vehicle weight to the rail and a vertical flange \"inboard\" to guide the vehicle along the rail using its inside edge.\n\nRail vehicle wheels are usually mounted on a solid axle, so they turn at the same speed. When a vehicle turns the outer wheel has to travel further than the inner wheel. On a road vehicle, this is usually achieved by allowing the wheels to move independently, and fixing the front wheels in an arrangement known as Ackermann steering geometry.\n\nTrains and trams can turn corners without wheel-slip because the outer horizontal part of the wheels has a slightly tapered rim. The guide flange (ridge) is on the inside to prevent the vehicle from slipping sideways off the rails. The horizontal (cone-shaped) rim makes contact with the slightly convex top of a steel rail in different (horizontal) places so that the outer wheel has a larger effective diameter than the inner wheel.\n\nWith both tram and train wheels, this happens naturally because the tires are cone shaped sloping surfaces: the inside diameter is a few millimeters larger than the outside. As the track starts to curve, the train tries to run straight. The wheel flange presses against the side of the curved rail so the \"contact point\" between rail and wheel moves a few millimeters outwards, making the effective diameter of the outer wheel temporarily larger, and equally opposite: the effective diameter of the inner wheel effectively becomes temporarily smaller. This technique works well on large-radius curves which are canted, but not as well on tight curves and railway switches (also known as \"points\"). This is because the geometry or cant of the track is more difficult to optimize for every possible combination of vehicle and direction of travel.\n\nCity trams often use tight curves - sometimes with a radius of much less than about , and canting may be impossible because the surface is shared with road vehicles or pedestrian zones or sidewalks, so the track often has to be flush with the road surface or pavement. In sharp curves, the rail grooves are sometimes made very shallow, which causes the outer wheel to temporarily ride up onto the edge of its flange. This increases the wheel diameter and the curve can be taken more easily. In extreme cases, the rail has a groove so that the rim of the flange can take most of the weight, the \"out-board\" tire (on the outer radius of the outer rail) acting as no more than a vertical plate. \n\nIn contrast, a train wheel is almost never designed to transfer weight through the flange rim, and some train wheels may be damaged if this should happen even once. \n\nThe point where two straight but intersecting railroads cross is called a frog. A groove through each rail allows the wheel flanges to pass through the intersecting rails. Without countermeasures each wheel would dip into the groove and strike the frog point gap causing unacceptable wear. The point where two tracks join and the vehicle can take one of two directions is called a railway switch. This works on the same principle, except that the inner rail is almost continuous and the outer rail has a gap for the flange to pass through.\n\nWith a train this problem is solved by using a wide tire. Train rails usually cross at a shallow angle. In the middle of the interchange there is a supporting frog. The tire is guided on each side by guide rails and some portion of the tire always maintains rail contact. This method is not feasible with trams and light railways.\n\nTram tires are generally narrower than train tires. Trams use bigger crossing angles and tighter curve radii are more likely than for train tracks. To cope with this difficulty the wheels of trams temporarily transfer the weight of the tram onto the flange to reduce wear on both the frog point and the horizontal surface of the tram wheels. Train wheels are not designed to bear such weight on their flanges.\n\nA tram wheel which runs on the flange rather than on the horizontal tyre has a larger effective diameter, so the distance travelled per revolution is greater. On the outside track of the curve this is an advantage. It may be necessary to compensate the inner wheel or allow for some slippage. Modern trams and trams tend to have thicker and wider tires which allow for a greater (horizontal) conical section and so greater effective diameter variation and turning ability.\n\nAt the junctions of train tracks, the gap in the frog or switch rail is wide. So trams can be accommodated.\n\nThe main problem with a train on tram rails is the relatively narrow width of frog and switch gaps and channels of the groove rails designed to accommodate the narrow flanges of tram wheels . The wider flanges of train wheels increase the risk of derailment at these points. On routes where train carriages are driven on tram tracks (as in the past in parts of The Hague), wider grooves are required as a compromise that is practical as wide grooved girder rail is available. A larger structure gauge would also be required This was also done in Los Angeles and in Vancouver as well as elsewhere in North America. The usually or normally limited structure gauge, and tight curves, on tram tracks will also prevent trains from using tram tracks.\n\nIn North America the groove would have to be a minimum of wide and by extension, the maximum distance between the inside faces of the guard flanges of the grooved rails can be no more than , see below. \n\nQuote: \n1. Lateral tolerance between wheels and rails \nTwo types of flanges are permitted on railroad wheels - narrow and wide. The maximum\nlateral movement T1 possible for a new wheel set centered on in-gage track is a function of\nthe flange type and is determined by the following formula: \nCAUTION: Use only English Units in Formulas in this Recommended Practice \n\nWhere: gt = standard track gage at a point “5/8” () below top of rail = 56.5\" ()\ngw = minimum gage of wheel set between backs of flanges = 53” ()\nfn = minimum thickness of new wheel flange = 1.15625” () for narrow flange or = 1.375\" () for wide flange\nQuoted from \n\nRural and suburban lines can be made compatible for use by several types of vehicles. For example, the narrow gauge railway used by Charleroi Metro in Belgium is ridden by trams, but the tracks are built to train track standards. Trams nonetheless run smoothly on the old NMVB tram net in Anderlues, where shallow groove rails are used. Between The Hague and Rotterdam, an old railway line was converted for RandstadRail into a route able to carry both the Rotterdam Metro, which uses vehicles built to train standards as well as The Hague trams which uses vehicles built to tram standards. The Electroliners which ran out of Chicago on the Chicago North Shore and Milwaukee Railroad, and afterwards on the Norristown High Speed Line, were another example.\n\n"}
{"id": "33232179", "url": "https://en.wikipedia.org/wiki?curid=33232179", "title": "Corporate Technology Directory", "text": "Corporate Technology Directory\n\nThe Corporate Technology Directory also known as the CorpTech directory of technology companies was a directory of technology companies published from 1986-2004 by CorpTech. It listed thousands of technology companies including software, services, and hardware as well as developers.\n\nThe directory was later made available in digital form as a cd and subsequently database subscription.\n\n"}
{"id": "21657889", "url": "https://en.wikipedia.org/wiki?curid=21657889", "title": "Cosmic Call", "text": "Cosmic Call\n\nCosmic Call was the name of two sets of interstellar radio messages that were sent from RT-70 in Yevpatoria, Crimea in 1999 (Cosmic Call 1) and 2003 (Cosmic Call 2) to various nearby stars. The messages were designed with noise-resistant format and characters.\n\nThe project was funded by Team Encounter, Charlie Chafer (CEO) a Texas-based startup, which went out of business in 2004.\n\nBoth transmissions were at ~150 kW, 5.01 GHz (FSK +/-24 kHz).\n\nEach Cosmic Call 1 session had the following structure. The Scientific Part (DDM, BM, AM, and ESM) was sent three times (at 100 bit/s), and the Public Part (PP) was sent once (at 2000 bit/s), according to the following arrangement:\n\nwhere DDM is the Dutil-Dumas Message, created by Canadian scientists Yvan Dutil and Stéphane Dumas, BM is the Braastad Message, AM is the Arecibo Message, and ESM is the Encounter 2001 Staff Message.\n\nEach Cosmic Call 2 session in 2003 had the following structure:\n\nwhere DDM2 is modernized DDM (aka Interstellar Rosetta Stone, ISR), BIG is Bilingual Image Glossary. All but the PP were transmitted at 400 bit/s\n\nThe ISR was 263,906 bits; BM, 88,687 bits, AM, 1,679 bits; BIG was 12 binary images 121,301 bits; ESM 24,899 bits. Total = 500,472 bits for 53 minutes. PP was 220 megabytes and sent at a rate of 100,000 bit/s for 11 hours total.\n\nThe messages were sent to the following stars:\n\n\n"}
{"id": "56449548", "url": "https://en.wikipedia.org/wiki?curid=56449548", "title": "Cosmos (standard)", "text": "Cosmos (standard)\n\nCOSMOS stands for \"COSMetic Organic and Natural Standard\", which sets certification requirements for organic and natural cosmetics products in the Europe. The standard is recognized globally by the cosmetic industry. By adhering to specific guidelines, cosmetics marketers can use COSMOS signatures, which are registered trademarks, on packaging to confirm the products meet minimum industry requirements to be considered organic or natural.\n\nIn 2002 five European organisations responsible for setting organic and natural cosmetics standards met at a trade show to share ideas for broader standards to be used globally. These five COSMOS members are:\n\nOver 1,600 manufacturers who sell over 25,000 products across over 45 countries follow the standard, according to Cosmos-standard.org. About 85% of the certified cosmetics industry uses COSMOS signatures on its products.\n\nAlthough the five members differed on certain standards separately, they were able to smooth out differences to create a harmonised international standard that was first published in 2010. At this time the five members formed a non-profit international association overseeing the standard. In June 2010 the COSMOS-standard AISBL was awarded Royal Assent from Belgian authorities. The documents published with the standard include:\n\n\nThere are four main certification signatures that comprise the COSMOS-standard, which are for ORGANIC, NATURAL, COSMOS CERTIFIED and COSMOS APPROVED products. Here are six steps to gaining approval of product labeling within the certification process:\n\n\nManufacturers and marketers are only allowed to use COSMOS terms and signatures for products authorized by the certification body. The certification body must be identified on product labels if it is not clearly mentioned anywhere else on the product. In cases in which the label size restricts product labeling, the certification body may allow flexibility as long as the product maintains the general principles of the Labeling Guide. The firm must at least mention the nature of the certification (such as organic or natural) and the identity of the certification body.\n\nMarketers are allowed to use the COSMOS terms and signatures on company letterhead and websites under certain conditions. All the products of a brand must be certified organic in order for the company to make the claim they are \"COSMOS ORGANIC certified.\" Otherwise, the firm must be clear that only specific products have been certified organic. In other words, the use of the terms and signatures must not be misleading to the consumer.\n\nEssentially, the labelling must clearly and accurately describe the product, which must comply with the standard. The marketer must avoid listing ingredients or naming the product in a way that implies it contains certain ingredients that are not present. Any use or branding of the term \"organic,\" for example, must comply with the organic standard and not be confusing to the consumer.\n\nThe labeling must not confuse the terms \"organic\" and \"natural,\" which have separate definitions and certifications based on the way the products and ingredients are processed. If a brand sells several organic products and a few natural products, they must make it clear in their labeling and marketing the differences. The firm must also be clear if some of its products have no certification at all. In other words, in order for a company to promote itself as \"COSMOS ORGANIC certified,\" its entire range of products must meet the organic standard and be certified.\n\nCompanies are not allowed to use logos or seals that may mislead customers into believing the products are COSMOS certified.\n\n"}
{"id": "3779965", "url": "https://en.wikipedia.org/wiki?curid=3779965", "title": "Critical technical practice", "text": "Critical technical practice\n\nCritical technical practice is critical theory based approach towards technological design proposed by Phil Agre where critical and cultural theories are brought to bear in the work of designers and engineers. One of the goals of critical technical practice is to increase awareness and critical reflection on the hidden assumptions, ideologies and values underlying technology design.\n\nA different fork of Critical Technical Practice from Agre’s root (Agre, 1997) was initiated at the former Centre for Cultural Studies between 2007-2017, Goldsmiths, University of London as a way to examine, the live techno-social aspects of contemporary digital culture. This stream of Critical Technical Practice (CTP) at its most broad use can be described as the formation of thought and action that incorporates art as a method of enquiry. This is a compacted intellectual form, that makes the space between the technical, theory and practice ambiguous. A typical digital culture class would make/explore things, attempting to explain the phenomena caught in the lens of a project or proposition. \nAn example of a class may clarify this approach. We might propose that the class create a simple (DOS) denial of service attack on a test remote server by learning to code computers for the first time. It is empowering for students to find out how quickly they can code. The class would learn how to do this from T.J. Connor’s book Violent Python. After the group had reached a self-satisfied tingle of radicalism the group would be asked to look up the author and would find that Connor is a top grade US Military expert. The group would then look at the books distribution and market penetration and be encouraged to question the affective logics, politics and culture of the book, reflect on why the workshop had been constructed the way it was, and their own learning in different registers of technicality, politics of information and personal critic and empowerment was achieved. \nCritical Technical Practice then is not necessarily a reduction of phenomena to literature or a system of logics, but can instead be thought of as knowledge incorporated into a thing that the class created, look at or pointed too, through revealing a certain type of gaze. A prerequisite of Critical Practice is that incorporates this form of gaze is thinking through the formation of oneself as a thinker, actor in the world. Enquiring into one's pre-existence helps understand the structuring of potential that has informed what one has become, what one could easily recognise, and what one could easily achieve. This is not a summation of limit but an acknowledgement of the hard work needed to escape a pre-existence, as it may relate to the four pillars of repression, class, gender, sexuality and race. The situated knowledge of family and friends, their relation to making things, to popular culture, to oral histories, to struggles with money or law, reading, writing and speaking, all inform this process. \nTo this end, CTP is partially related to a schizoanalysis of Foucault's question “What are we today?” (Foucault 1984: pp. 42ff.) The class is always encouraged to unfold what conditions, constrain, control, resist, govern, determine this moment and not another? What patterns of recognition are we privileging and why does it blind us to others? How does language restrict us at the very moment we are able to say something? How can this engagement be born anew in every instance? \n\nPeople whose work contributes to the critical technical practice agenda include \nPhil Agre,\nPaul Dourish,\nNatalie Jeremijenko,\nMichael Mateas,\nSimon Penny,\nWarren Sack,\nGarnet Hertz,\nYoHa, (Matsuko Yokokoji, Graham Harwood)\nand \nPhoebe Sengers.\n\n\n"}
{"id": "2742349", "url": "https://en.wikipedia.org/wiki?curid=2742349", "title": "Cyber-dissident", "text": "Cyber-dissident\n\nA cyber-dissident is a professional journalist, an activist or citizen journalist who posts news, information, or commentary on the internet that implies criticism of a government or regime.\n\nThe practice of cyber-dissidence may have been inaugurated by Dr. Daniel Mengara, a Gabonese scholar and activist living in political exile in New Jersey in the United States. In 1998, he created a website in French whose name \"Bongo Doit Partir\" (Bongo Must Go) was clearly indicative of its purpose: it encouraged a revolution against the then 29-year-old regime of Omar Bongo in Gabon. The original URL, http://www.globalwebco.net/bdp/, began to redirect to http://www.bdpgabon.org in the year 2000. Inaugurating what was to become common current-day practice in the politically involved blogosphere, this movement's attempt at rallying the Gabonese around revolutionary ideals and actions has ultimately been vindicated by the 2011 Tunisian and Egyptian revolutions, where the Internet has proved to be an effective tool for instigating successful critique, opposition and revolution against dictators.\n\nAt least two nonprofit organizations are currently working to raise awareness of the contributions of cyber-dissidents and to defend them against the human rights violations to which some of them are subjected: Global Voices Online and Reporters Without Borders. The latter has released a \"Handbook For Bloggers and Cyber-Dissidents\" and maintains a roster of currently imprisoned cyber-dissidents. The Committee to Protect Bloggers has been created.\n\nIn regions where print and broadcast media are tightly controlled, anonymous online postings by cyber-dissidents may be the only source of information about the experiences, feelings, and opinions of ordinary citizens. This advantage may be offset by the difficulty in assessing the good faith and accuracy of reports originating from anonymous sources.\n\nRecently, social-media tools have been widely credited with igniting pervasive social upheavals, some of which have even brought down governments.\n\nIn July 2003, Amnesty International reported the arrest of five Gabonese known to be members of the cyber-dissident group \"Bongo Doit Partir\". The five members were detained for three months (See: Gabon: Prisoners of Conscience and Gabon: Further information on Prisoners of conscience.)\n\nIn 2003, Cai Lujun was imprisoned for posting a series of articles online under the pen name \"盼民主\"(\"expecting for democracy\") criticizing the Chinese government.\n\nIn 2006, several bloggers in Egypt were arrested for allegedly defaming the president Hosni Mubarak and expressing critical views about Islam \n\nIn 2005, Mohamad Reza Nasab Abdolahi was imprisoned for publishing an open letter to Ayatollah Ali Khamenei; Mohamad's pregnant wife and other bloggers who commented on Mohamad's treatment were also imprisoned.\n\nWhen Russian president Vladimir Putin in 2006 called on his nation's women to have more children, journalist Vladimir Rakhmankov published a satiric article on the Internet calling Putin \"the nation's phallic symbol\". Rakhmankov was found guilty of offending Vladimir Putin, and fined by the court of the region he lived in to the sum equal of 680 USD. The overall story served as a good adversiting for Rakhmanov's article, that was republished by numerous Russian sources afterwards.\n\nThree Russian bloggers has supposed in 2003, that Russian state security service FSB, the main successor to the KGB, created special teams of people who appear on various blogs to harass and intimidate political bloggers and thus effectively prevent free discussion of undesirable subjects. They referred to such tactics are known as \"active measures\". A Russian critic of this theory has noted in 2003, that security services have more important tasks than flooding in forums.\n\nThe Digital Freedom Network has pointed out cases of imprisoning cyber-dissidents in Vietnam, such as the 2004 case of Pham Que Dong, a former People's Army colonel, military historian who had quit the Communist Party in 1999. For publicly discussing issues related to corruption in the official structures and encouraging democratic reforms, he was charged with \"abuse of democratic freedoms\" and imprisoned.\n\n"}
{"id": "10319663", "url": "https://en.wikipedia.org/wiki?curid=10319663", "title": "DigRF", "text": "DigRF\n\nThe DigRF working group was formed as a MIPI Alliance (MIPI) working group in April 2007. The group is focused on developing specifications for wireless mobile RFIC to base-band IC (BBIC) interfaces in mobile devices.\n\nThe group's current charter is split into short-term and long-term development efforts. The short-term development will focus on a specification targeted for completion by end of 2007 for LTE and WiMax air interface standards. The longer term development will focus on future air interface standards which promise further improvements in high speed, data optimized traffic. In addition, the future work will seek to harmonize efforts with the MIPI's PHY and UniPro working groups.\n\nThese specifications will describe the logical, electrical and timing characteristics of the digital RF-BB Interface with sufficient detail to allow physical implementation of the interface, and with sufficient rigor that implementations of the interface from different suppliers are fully compatible at the physical level.\n\nThere is DigRF version v1.12 for usage in GSM/EDGE handsets, which was specified in 2004. DigRF v3.09 from the year 2006 with its 312 Mbit/s can additionally handle UMTS. The present DigRF v4 draft offers Gbit/s bandwidth for LTE and WiMax.\n"}
{"id": "43348955", "url": "https://en.wikipedia.org/wiki?curid=43348955", "title": "Digital detox", "text": "Digital detox\n\nDigital detox refers to a period of time during which a person refrains from using electronic connecting devices such as smartphones and computers. It is regarded as an opportunity to reduce stress, focus more on social interaction and connection with nature in the physical world. Claimed benefits include increased mindfulness, lowered anxiety, and an overall better appreciation of one's environment. There have been many stories where a digital detox has lead to a more refreshed feeling along the people involved. They described there digital detox as a \"getaway\" in some aspect. When people get caught up in how much they need to do, the overuse of technology becomes prevalent. This even becomes the case in a social context; people want to make sure they're up to date on the latest news stories, Instagram posts, political tweets etc. and to do this, they need to stay plugged in. \n\nSmartphones, laptops, and tablets, combined with the increasing wireless Internet accessibility, enable technology users to constantly be connected to the digital world.Constant online connectivity may have a negative impact on the users’ experience with electronic connecting devices and result in a wish to temporarily refrain from communication technology usage.\n\nIn one study in Mind, 95% of those interviewed said their mood improved after putting down their phones to spend time outside, changing from depressed, stressed, and anxious to more calm and balanced. \n\nThe motivations behind digital detoxing vary. In some cases, the motivation is negative emotional responses to the technology usage, such as dissatisfaction or disappointment of the technology device and its functions. In other cases, users see the technology as a distracting factor that consumes time and energy and wants to take back control over their everyday lives. Some people have moral, ethical or political reasons to refrain from technology usage. Furthermore, a concern of developing addictive behavior in terms of tech addiction or Internet addiction disorder is one of the motivations for disconnecting for a period of time.This excessive technology usage can be considered an addiction. It's said that around 50% of smartphone users check their account 5 minutes prior to going to bed and within 5 minutes post waking up. \n\nConstant engagement with digital connecting devices at the workplace is claimed to lead to increased stress levels and reduce productivity. Certain characteristics of the technology make it more difficult to distinguish work from leisure. Moreover, being continually connected increases the number of interruptions at work. Allowing employees to disconnect for a part of the day in order to truly focus on their work without disturbance from colleagues is claimed to be beneficial to the productivity and work environment. \"To combat the effects of constant digital exposure, there are digital detox retreats held for select numbers of people. It's an average of 12 people per week getting detoxed for a little less than $600 dollars including flights.\"\n\nIn certain cultures, constant work is encouraged and taking breaks are sometimes even discouraged. This is due to the idea that a break equals a lack of productivity. In the Indian culture, this is extremely prevalent; there have even been studies to back these ideas up. It's said that more than half of Indians check their work and social emails constantly. Even while vacationing, about 30% admitted that they had an irresistible urge to check and post on social media. There is also a significant estimated 30% difference between people being willing to leave their laptops at home compared to people being willing to leave their smartphones at home. It's admitted that if work wasn't a factor a lot of Indians wouldn't constantly be on their devices and stay unplugged. \n\nThe connecting devices’ multitasking character has a serious impact on the learning ability. Multitasking implies operating on a surface level, which only involves the short-time memory. Using multiple connecting devices as learning platforms is therefore not beneficial. A reduction of information choices enables the brain to focus more on the quality of the information rather than the hastiness of it.\n\n"}
{"id": "18025626", "url": "https://en.wikipedia.org/wiki?curid=18025626", "title": "Document automation", "text": "Document automation\n\nDocument automation (also known as document assembly) is the design of systems and workflows that assist in the creation of electronic documents. These include logic-based systems that use segments of pre-existing text and/or data to assemble a new document. This process is increasingly used within certain industries to assemble legal documents, contracts and letters. Document automation systems can also be used to automate all conditional text, variable text, and data contained within a set of documents.\n\nAutomation systems allow companies to minimize data entry, reduce the time spent proof-reading, and reduce the risks associated with human error. Additional benefits include: time and financial savings due to decreased paper handling, document loading, storage, distribution, postage/shipping, faxes, telephone, labor and waste.\n\nThe basic functions are to replace the cumbersome manual filling in of repetitive documents with template-based systems where the user answers software-driven interview questions or data entry screen. The information collected then populates the document to form a good first draft'. Today's more advanced document automation systems allow users to create their own data and rules (logic) without the need for programming.\n\nWhile document automation software is used primarily in the legal, financial services, and risk management industries, it can be used in any industry that creates transaction-based documents. A good example of how document automation software can be used is with commercial mortgage documents. A typical commercial mortgage transaction can include several documents including:\nSome of these documents can contain as many as 80 to 100 pages, with hundreds of optional paragraphs and data elements. Document automation software has the ability to automatically fill in the correct document variables based on the transaction data. In addition, some document automation software has the ability to create a document suite where all related documents are encapsulated into one file, making updates and collaboration easy and fast. Established companies in this field includes the likes of Contract Express from Thomson Reuters\n\nSimpler software applications that are easier to learn can also be used to automate the preparation of documents, without undue complexity. For example, Pathagoras holds itself out as a 'plain text, no fields allowed' document assembly system. Clipboard managers allow the user to save frequently-used text fragments, organize them into logical groups, and then quickly access them to paste into final documents.\n\nThere are many documents used in logistics. They are called: invoices, packing lists/slips/sheets (manifests), content lists, pick tickets, arrival acknowledgement forms/reports of many types (e.g. MSDS, damaged goods, returned goods, detailed/summary, etc.), import/export, delivery, bill of lading (BOL), etc. These documents are usually the contracts between the consignee and the consignor, so they are very important for both parties and any intermediary, like a third party logistics company (3PL) and governments. Document handling within logistics, supply chain management and distribution centers is usually performed manual labor or semi-automatically using bar code scanners, software and tabletop laser printers. There are some manufacturers of high speed document automation systems that will automatically compare the laser printed document to the order and either insert or automatically apply an enclosed wallet/pouch to the shipping container (usually a flexible polybag or corrugated fiberboard/rigid container). See below for external website video links showing these document automation systems. Protection of Privacy and Identity Theft are major concerns, especially with the increase of e-Commerce, Internet/Online shopping and Shopping channel (other, past references are catalogue and mail order shopping) making it more important than ever to guarantee the correct document is married or associated to the correct order or shipment every time. Software that produce documents are: ERP, WMS, TMS, legacy middleware and most accounting packages.\n\nA number of research projects have looked into wider standardization and automation of documents in the freight industry.\n\nThe role of automation technology in the production of legal documents has been widely recognised. For example, Richard Susskind’s book ‘The End of Lawyers’ looks at the use of document automation software that enables clients to generate employment contracts and Wills with the use of an online interview or decision tree. Susskind regards Document Assembly as one of 10 'disruptive technologies' that are altering the face of the legal profession. In large law firms document assembly systems are increasingly being used to systemise work, such as complex term sheets and the first drafts of credit agreements.\n\nWith the liberalisation of the UK legal services market spearheaded by the Legal Services Act 2007 large institutions have broadened their services to include legal assistance for their customers. Most of these companies use some element of document automation technology to provide legal document services over the Web. This has been seen as heralding a trend towards commoditisation whereby technologies like document automation result in high volume, low margin legal services being ‘packaged’ and provided to a mass-market audience.\n\nIn United States, companies like LegalZoom and RocketLawyer offer automated document production for individuals and small businesses..\n\nCompanies like Contract Express by Thomson Reuters, HotDocs (acquired by AbacusNext) and HelpSelf Legal also provide document automation services that allow lawyers to customize their own interview workflows to merge into legal documents. These and other sites are incorporating artificial intelligence into their document automation software.\n\nInsurance policies and certificates, depending on the type, policy documents can also be hundreds of pages long and include specific information on the insured. Typically, in the past, these insurance document packets were created by a) typing out free-form letters, b) adding pre-printed brochures c) editing templates and d) customizing graphics with the required information, then manually sorting and inserting all the documents into one packet and mailing them to the insured. The various documents included in one packet could include the following kinds of documents:\n\nA lot of work can go into putting one packet together. In most policy admin systems, the system will generate some kind of policy statement as a starting point but might need to be customized and enhanced with other required materials.\n\n"}
{"id": "164430", "url": "https://en.wikipedia.org/wiki?curid=164430", "title": "Documentation", "text": "Documentation\n\nDocumentation is a set of documents provided on paper, or online, or on digital or analog media, such as audio tape or CDs. Examples are user guides, white papers, on-line help, quick-reference guides. It is becoming less common to see paper (hard-copy) documentation. Documentation is distributed via websites, software products, and other on-line applications.\n\nProfessionals educated in this field are termed documentalists. This field changed its name to information science in 1968, but some uses of the term documentation still exists and there have been efforts to reintroduce the term documentation as a field of study.\n\nWhile associated ISO standards are not easily available publicly, a guide from other sources for this topic may serve the purpose. . David Berger has provided several principles of document writing, regarding the terms used, procedure numbering and even lengths of sentences, etc.\n\nThe following is a list of guides dealing with each specific field and type:\n\nThe procedures of documentation vary from one sector, or one type, to another. In general, these may involve document drafting, formatting, submitting, reviewing, approving, distributing, reposting and tracking, etc., and are convened by associated SOPs in a regulatory industry. It could also involve creating content from scratch. Documentation should be easy to read and understand. If it's too long and too wordy, it may be misunderstood or ignored. Clear, Short, Familiar words should be used to a maximum of 15 words to a sentence. Only gender hyper neutral word should be used and cultural biases should be avoided. Procedures should be numbered when they are to be performed. .\n\nTechnical writers and corporate communicators are professionals whose field and work is documentation. Ideally, technical writers have a background in both the subject matter and also in writing and managing content (information architecture). Technical writers more commonly collaborate with subject matter experts (SMEs), such as engineers, technical experts, medical professionals, or other types of clients to define and then create content (documentation) that meets the user's needs. Corporate communications includes other types of written documentation that is required for most companies.\n\n\n\nThe following are typical software documentation types\n\nThe following are typical hardware and service documentation types\n\nDocumentation include such as feasibility report, technical documentation, operational documentation, log book, etc.\n\nThere are many types of software and applications used to create documentation.\n\nSOFTWARE DOCUMENTATION FOLDER (SDF)\n\nA common type of software document written by software engineers in the simulation industry is the SDF. When developing software for a simulator, which can range from embedded avionics devices to 3D terrain databases by way of full motion control systems, the engineer keeps a notebook detailing the development \"the build\" of the project or module. The document can be a wiki page, MS word document or other environment. They should contain a \"requirements\" section, an \"interface\" section to detail the communication interface of the software. Often a \"notes\" section is used to detail the proof of concept, and then track errors and enhancements. Finally, a \"testing\" section to document how the software was tested. This documents conformance to the client's requirements. The result is a detailed description of how the software is designed, how to build and install the software on the target device, and any known defects and work-arounds. This build document enables future developers and maintainers to come up to speed on the software in a timely manner, and also provides a roadmap to modifying code or searching for bugs.\n\nSOFTWARE FOR NETWORK INVENTORY AND CONFIGURATION (CMDB)\n\nThese software tools can automatically collect data of your network equipment. The data could be for inventory and for configuration information. The ITIL Library requests to create such a database as a basis for all information for the IT responsible. It's also the basis for IT documentation. Examples include XIA Configuration.\n\n\"Documentation\" is the preferred term for the process of populating criminal databases. Examples include the National Counter-terrorism Center's Terrorist Identities Datamart Environment (\"TIDE\"), sex offender registries, and gang databases.\n\nDocumentation, as it pertains to the Early Childhood Education field, is \"when we notice and value children's ideas, thinking, questions, and theories about the world and then collect traces of their work (drawings, photographs of the children in action, and transcripts of their words) to share with a wider community\" \n\nThusly, documentation is a process, used to link the educator's knowledge and learning of the child/children with the families, other collaborators, and even to the children themselves. \n\nDocumentation is an integral part of the cycle of inquiry - observing, reflecting, documenting, sharing and responding. \n\nPedagogical documentation, in terms of the teacher documentation, is the \"teacher's story of the movement in children's understanding\". According to Stephanie Cox Suarez in 'Documentation - Transforming our Perspectives', \"teachers are considered researchers, and documentation is a research tool to support knowledge building among children and adults\" \n\nDocumentation can take many different styles in the classroom. The following exemplifies ways in which documentation can make the 'research', or learning, visible:\n\n\nDocumentation is certainly a process in and of itself, and it is also a process within the educator. The following is the development of documentation as it progresses for and in the educator themselves:\n\n\n\n"}
{"id": "28937856", "url": "https://en.wikipedia.org/wiki?curid=28937856", "title": "Dyson–Harrop satellite", "text": "Dyson–Harrop satellite\n\nA Dyson–Harrop satellite is a hypothetical megastructure intended for power generation using the solar wind. It is inspired by the Dyson sphere, but much harder to detect from another star system.\n\nThe concept for the so-called Dyson–Harrop satellite begins with a long metal wire loop pointed at the sun. This wire is charged to generate a cylindrical magnetic field that snags the electrons that make up half the solar wind. These electrons get funnelled into a metal spherical receiver to produce a current, which generates the wire's magnetic field – making the system self-sustaining. Any current not needed for the magnetic field powers an infrared laser trained on satellite dishes back on Earth, designed to collect the energy. Earth's air does not absorb infra-red light, so the system would be highly efficient. Back on the satellite, the current has been drained of its electrical energy by the laser – the electrons fall onto a ring-shaped sail, where incoming sunlight can excite them enough to keep the satellite in orbit around the sun.\n\nA relatively small Dyson–Harrop satellite using a 1-centimetre-wide copper wire 300 metres long, a receiver 2 metres wide and a sail 10 metres in diameter, sitting at roughly the same distance from the sun as the Earth, could generate 1.7 megawatts of power – enough for about 1000 family homes in the US. Larger sizes could produce far greater amounts of power, even exceeding the current usage of Earth. Satellites could be placed anywhere in the solar system, and networks of satellites could combine to generate terawatts of power.\n"}
{"id": "41920098", "url": "https://en.wikipedia.org/wiki?curid=41920098", "title": "ENIAC Day", "text": "ENIAC Day\n\nENIAC Day or the World’s First Computer Day is celebrated on 15 February.\nOn February 10, 2011, the City of Philadelphia officially declared that February 15, 2011 - the 65th anniversary of the unveiling of the Electronic Numerical Integrator and Computer (ENIAC), the world's first general-purpose electronic computer, developed at the University of Pennsylvania's Moore School of Electrical Engineering - would that year and henceforth be known as ENIAC Day.\n"}
{"id": "15399117", "url": "https://en.wikipedia.org/wiki?curid=15399117", "title": "Electro-mechanical modeling", "text": "Electro-mechanical modeling\n\nThe purpose of electro-mechanical modeling is to model and simulate an electro-mechanical system, such that its physical parameters can be examined before the actual system is built. Parameter estimation and physical realization of the overall system is the major design objective of Electro-Mechanical modeling. Theory driven mathematical model can be used or applied to other system to judge the performance of the joint system as a whole. This is a well known & proven technique for designing large control system for industrial as well as academic multi-disciplinary complex system. This technique is also being employed in MEMS technology recently.\n\nThe modeling of purely mechanical systems is mainly based on the Lagrangian which is a function of the generalized coordinates and the associated velocities. If all forces are derivable from a potential, then the time behavior of the dynamical systems is completely determined. For simple mechanical systems, the Lagrangian is defined as the difference of the kinetic energy and the potential energy.\n\nThere exists a similar approach for electrical system. By means of the electrical co-energy and well defined power quantities, the equations of motions are uniquely defined. The currents of the inductors and the voltage drops across the capacitors play the role of the generalized coordinates. All constraints, for instance caused by the Kirchhoff laws, are eliminated from the considerations. After that, a suitable transfer function is to be derived from the system parameters which eventually governs the behavior of the system.\n\nIn consequence, we have quantities (kinetic and potential energy, generalized forces) which determine the mechanical part and quantities (co-energy, powers) for the description of the electrical part. This offers a combination of the mechanical and electrical parts by means of an energy approach. As a result, an extended Lagrangian format is produced.\n\n\n"}
{"id": "47231848", "url": "https://en.wikipedia.org/wiki?curid=47231848", "title": "Ente Nazionale Italiano di Unificazione", "text": "Ente Nazionale Italiano di Unificazione\n\nEnte Nazionale Italiano di Unificazione (Italian National Unification, acronym UNI) is a private non-profit association that performs regulatory activities in Italy across industrial, commercial, and service sectors, with the exception of electrical engineering and electronic competence of CEI .\n\nThe UNI is recognized by the Italian State and by the European Union, and represents Italian legislative activity at the International Standards Organization (ISO) and European Committee for Standardization (CEN).\n\nPiero Torretta is the president of the organization.\nThe UNI was formed in 1921 with the initials \"UNIM\" in the face of demands for standardization of mechanical engineering at the time. At the 1928 General Confederation of Italian Industry (Confindustria) it was promoted to include all sectors of industry, becoming the current UNI.\n\nThe main tasks of UNI are:\n\n\nThe UNI also avails of federated entities for specific fields of competence. Between them in the field of standardization in the field of computer science is relevant UNINFO, which is the UNI, the areas of competence, at the ISO, the ISO / IEC JTC1 (ISO / IEC Joint Technical Committee) and the CEN.\n\nOther federated entity UNI is the CTI, Italian Committee thermo energy and environment, which deals with the legislative activity in the areas of heating technology and the production and utilization of thermal energy, both nationally and internationally, where he, on behalf of UNI, the work of numerous groups CEN and ISO detaining, for some of these, the relative answering techniques.\n\nThe most important federated entity UNI is today the Italian Gas Committee (CIG) that deals with regulation of the sector gas, nationally and internationally and cooperates with many Italian and European institutions in the same sector. Currently this committee is working on several initiatives with strong impact on national energy issues, such as the introduction of biomethane in transport networks and distribution of natural gas, the European project on the quality of gas, the project of smart meters for natural gas.\n\nThe published standards can significantly affect the safety of workers, the public, or the environment, that the government refer to them recalling them in documents legislative and attributing to them some level of cogency.\n\nThe designation of a UNI standard shows its origin (# denotes a number):\n\n"}
{"id": "52972673", "url": "https://en.wikipedia.org/wiki?curid=52972673", "title": "Feel Train", "text": "Feel Train\n\nFeel Train is a technology collaborative co-founded by Courtney Stanton and Darius Kazemi and based in Portland, Oregon.\n\nFeel Train is a worker-owned cooperative. Stanton and Kazemi are its first two worker-owners, and the organization is chartered to allow a maximum of eight employees, each with equal salary, equal share in the company and equal firing power over others, including the founders. \n\nFeel Train projects have included the Stay Woke Bot, a Twitter bot developed in collaboration with activists DeRay Mckesson and Samuel Sinyangwe, and Shortcut, an app developed with radio program \"This American Life\" to facilitate sharing audio clips across social media, similar to the way gifs allow video clips to be shared. Feel Train is also developing a Twitter bot based on the Obama Social Media Archive called Relive 44, which beginning in May 2017 will repost, eight years later, every tweet from President Barack Obama (whose first tweet came in May 2009.)\n\nFeel Train website\n"}
{"id": "36535684", "url": "https://en.wikipedia.org/wiki?curid=36535684", "title": "Glossary of engineering", "text": "Glossary of engineering\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\n\nThis glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n\n \n\n \n\nwhere formula_1 is the infinitesimal amount of heat absorbed by the system from the reservoir and formula_2 is the temperature of the external reservoir (surroundings) at a particular instant in time. In the special case of a reversible process, the equality holds. The reversible case is used to introduce the entropy state function. This is because in a cyclic process the variation of a state function is zero. In words, the Clausius statement states that it is impossible to construct a device whose sole effect is the transfer of heat from a cool reservoir to a hot reservoir. Equivalently, heat spontaneously flows from a hot body to a cooler one, not the other way around. The generalized \"inequality of Clausius\" \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n"}
{"id": "14449116", "url": "https://en.wikipedia.org/wiki?curid=14449116", "title": "History of timekeeping devices", "text": "History of timekeeping devices\n\nFor thousands of years, devices have been used to measure and keep track of time. The current sexagesimal system of time measurement dates to approximately 2000  from the Sumerians.\n\nThe Egyptians divided the day into two 12-hour periods, and used large obelisks to track the movement of the sun. They also developed water clocks, which were probably first used in the Precinct of Amun-Re, and later outside Egypt as well; they were employed frequently by the Ancient Greeks, who called them \"clepsydrae\". The Zhou dynasty is believed to have used the outflow water clock around the same time, devices which were introduced from Mesopotamia as early as 2000.\n\nOther ancient timekeeping devices include the candle clock, used in ancient China, ancient Japan, England and Mesopotamia; the timestick, widely used in India and Tibet, as well as some parts of Europe; and the hourglass, which functioned similarly to a water clock. The sundial, another early clock, relies on shadows to provide a good estimate of the hour on a sunny day. It is not so useful in cloudy weather or at night and requires recalibration as the seasons change (if the gnomon was not aligned with the Earth's axis).\n\nThe earliest known clock with a water-powered escapement mechanism, which transferred rotational energy into intermittent motions, dates back to 3rd century in ancient Greece; Chinese engineers later invented clocks incorporating mercury-powered escapement mechanisms in the 10th century, followed by Iranian engineers inventing water clocks driven by gears and weights in the 11th century.\n\nThe first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656. The invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century.\n\nThe pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2. Although initially limited to laboratories, the development of microelectronics in the 1960s made quartz clocks both compact and cheap to produce, and by the 1980s they became the world's dominant timekeeping technology in both clocks and wristwatches.\n\nAtomic clocks are far more accurate than any previous timekeeping device, and are used to calibrate other clocks and to calculate the International Atomic Time; a standardized civil system, Coordinated Universal Time, is based on atomic time.\n\nMany ancient civilizations observed astronomical bodies, often the Sun and Moon, to determine times, dates, and seasons. The first calendars may have been created during the last glacial period, by hunter-gatherers who employed tools such as sticks and bones to track the phases of the moon or the seasons. Stone circles, such as England's Stonehenge, were built in various parts of the world, especially in Prehistoric Europe, and are thought to have been used to time and predict seasonal and annual events such as equinoxes or solstices. As those megalithic civilizations left no recorded history, little is known of their calendars or timekeeping methods. Methods of sexagesimal timekeeping, now common in both Western and Eastern societies, are first attested nearly 4,000 years ago in Mesopotamia and Egypt. Mesoamericans similarly modified their usual vigesimal counting system when dealing with calendars to produce a 360-day year.\n\nThe oldest known sundial is from Egypt; it dates back to around 1500 (19th Dynasty), and was discovered in the Valley of the Kings in 2013. Sundials have their origin in shadow clocks, which were the first devices used for measuring the parts of a day. Ancient Egyptian obelisks, constructed about 3500, are also among the earliest shadow clocks.\nEgyptian shadow clocks divided daytime into 12 parts with each part further divided into more precise parts. One type of shadow clock consisted of a long stem with five variable marks and an elevated crossbar which cast a shadow over those marks. It was positioned eastward in the morning, and was turned west at noon. Obelisks functioned in much the same manner: the shadow cast on the markers around it allowed the Egyptians to calculate the time. The obelisk also indicated whether it was morning or afternoon, as well as the summer and winter solstices. A third shadow clock, developed c. 1500, was similar in shape to a bent T-square. It measured the passage of time by the shadow cast by its crossbar on a non-linear rule. The \"T\" was oriented eastward in the mornings, and turned around at noon, so that it could cast its shadow in the opposite direction.\n\nAlthough accurate, shadow clocks relied on the sun, and so were useless at night and in cloudy weather. The Egyptians therefore developed a number of alternative timekeeping instruments, including water clocks, and a system for tracking star movements. The oldest description of a water clock is from the tomb inscription of the 16th-century Egyptian court official Amenemhet, identifying him as its inventor. There were several types of water clocks, some more elaborate than others. One type consisted of a bowl with small holes in its bottom, which was floated on water and allowed to fill at a near-constant rate; markings on the side of the bowl indicated elapsed time, as the surface of the water reached them. The oldest-known waterclock was found in the tomb of pharaoh Amenhotep I (1525–1504), suggesting that they were first used in ancient Egypt. Another Egyptian method of determining the time during the night was using plumb-lines called merkhets. In use since at least 600, two of these instruments were aligned with Polaris, the north pole star, to create a north–south meridian. The time was accurately measured by observing certain stars as they crossed the line created with the \"merkhets\".\n\nWater clocks, or clepsydrae, were commonly used in Ancient Greece following their introduction by Plato, who also invented a water-based alarm clock. One account of Plato's alarm clock describes it as depending on the nightly overflow of a vessel containing lead balls, which floated in a columnar vat. The vat held a steadily increasing amount of water, supplied by a cistern. By morning, the vessel would have floated high enough to tip over, causing the lead balls to cascade onto a copper platter. The resultant clangor would then awaken Plato's students at the Academy. Another possibility is that it comprised two jars, connected by a siphon. Water emptied until it reached the siphon, which transported the water to the other jar. There, the rising water would force air through a whistle, sounding an alarm. The Greeks and Chaldeans regularly maintained timekeeping records as an essential part of their astronomical observations.\n\nGreek astronomer, Andronicus of Cyrrhus, supervised the construction of the Tower of the Winds in Athens in the 1st century.\n\nIn Greek tradition, clepsydrae were used in court; later, the Romans adopted this practice, as well. There are several mentions of this in historical records and literature of the era; for example, in \"Theaetetus\", Plato says that \"Those men, on the other hand, always speak in haste, for the flowing water urges them on\". Another mention occurs in Lucius Apuleius' \"The Golden Ass\": \"The Clerk of the Court began bawling again, this time summoning the chief witness for the prosecution to appear. Up stepped an old man, whom I did not know. He was invited to speak for as long as there was water in the clock; this was a hollow globe into which water was poured through a funnel in the neck, and from which it gradually escaped through fine perforations at the base\". The clock in Apuleius's account was one of several types of water clock used. Another consisted of a bowl with a hole in its centre, which was floated on water. Time was kept by observing how long the bowl took to fill with water.\n\nAlthough clepsydrae were more useful than sundials—they could be used indoors, during the night, and also when the sky was cloudy—they were not as accurate; the Greeks, therefore, sought a way to improve their water clocks. Although still not as accurate as sundials, Greek water clocks became more accurate around 325, and they were adapted to have a face with an hour hand, making the reading of the clock more precise and convenient. One of the more common problems in most types of clepsydrae was caused by water pressure: when the container holding the water was full, the increased pressure caused the water to flow more rapidly. This problem was addressed by Greek and Roman horologists beginning in 100, and improvements continued to be made in the following centuries. To counteract the increased water flow, the clock's water containers—usually bowls or jugs—were given a conical shape; positioned with the wide end up, a greater amount of water had to flow out in order to drop the same distance as when the water was lower in the cone. Along with this improvement, clocks were constructed more elegantly in this period, with hours marked by gongs, doors opening to miniature figurines, bells, or moving mechanisms. There were some remaining problems, however, which were never solved, such as the effect of temperature. Water flows more slowly when cold, or may even freeze.\n\nBetween 270 and 500, Hellenistic (Ctesibius, Hero of Alexandria, Archimedes) and Roman horologists and astronomers began developing more elaborate mechanized water clocks. The added complexity was aimed at regulating the flow and at providing fancier displays of the passage of time. For example, some water clocks rang bells and gongs, while others opened doors and windows to show figurines of people, or moved pointers, and dials. Some even displayed astrological models of the universe.\n\nAlthough the Greeks and Romans did much to advance water clock technology, they still continued to use shadow clocks. The mathematician and astronomer Theodosius of Bithynia, for example, is said to have invented a universal sundial that was accurate anywhere on Earth, though little is known about it. Others wrote of the sundial in the mathematics and literature of the period. Marcus Vitruvius Pollio, the Roman author of \"De Architectura\", wrote on the mathematics of gnomons, or sundial blades. During the reign of Emperor Augustus, the Romans constructed the largest sundial ever built, the Solarium Augusti. Its gnomon was an obelisk from Heliopolis. Similarly, the obelisk from Campus Martius was used as the gnomon for Augustus's zodiacal sundial. Pliny the Elder records that the first sundial in Rome arrived in 264, looted from Catania, Sicily; according to him, it gave the incorrect time until the markings and angle appropriate for Rome's latitude were used—a century later.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Zeebad, dates back to 500. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" – the shortest, longest, and equal-length days and nights of the years. The water clocks used in Iran were one of the most practical ancient tools for timing the yearly calendar.\n\nWater clocks, or \"Fenjaan\", in Persia reached a level of accuracy comparable to today's standards of timekeeping. The fenjaan was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from a qanat or well for irrigation of the farms, until it was replaced by more accurate current clock. Persian water clocks were a practical and useful tool for the qanat's shareholders to calculate the length of time they could divert water to their farm. The qanat was the only water source for agriculture and irrigation so a just and fair water distribution was very important. Therefore, a very fair and clever old person was elected to be the manager of the water clock, and at least two full-time managers were needed to control and observe the number of fenjaans and announce the exact time during the days and nights.\n\nThe fenjaan was a big pot full of water and a bowl with small hole in the center. When the bowl become full of water, it would sink into the pot, and the manager would empty the bowl and again put it on the top of the water in the pot. He would record the number of times the bowl sank by putting small stones into a jar.\n\nThe place where the clock was situated, and its managers, were collectively known as \"khaneh fenjaan\". Usually this would be the top floor of a public-house, with west- and east-facing windows to show the time of sunset and sunrise. There was also another time-keeping tool named a \"staryab\" or astrolabe, but it was mostly used for superstitious beliefs and was not practical for use as a farmers' calendar. The Zeebad Gonabad water clock was in use until 1965 when it was substituted by modern clocks.\n\nJoseph Needham speculated that the introduction of the outflow clepsydra to China, perhaps from Mesopotamia, occurred as far back as the 2nd millennium, during the Shang Dynasty, and at the latest by the 1st millennium. By the beginning of the Han Dynasty, in 202, the outflow clepsydra was gradually replaced by the inflow clepsydra, which featured an indicator rod on a float. To compensate for the falling pressure head in the reservoir, which slowed timekeeping as the vessel filled, Zhang Heng added an extra tank between the reservoir and the inflow vessel. Around 550 AD, Yin Gui was the first in China to write of the overflow or constant-level tank added to the series, which was later described in detail by the inventor Shen Kuo. Around 610, this design was trumped by two Sui Dynasty inventors, Geng Xun and Yuwen Kai, who were the first to create the balance clepsydra, with standard positions for the steelyard balance. Joseph Needham states that:\nThe term 'clock' encompasses a wide spectrum of devices, ranging from wristwatches to the Clock of the Long Now. The English word \"clock\" is said to derive from the Middle English \"clokke\", Old North French \"cloque\", or Middle Dutch \"clocke\", all of which mean \"bell\", and are derived from the Medieval Latin \"clocca\", also meaning bell. Indeed, bells were used to mark the passage of time; they marked the passage of the hours at sea and in abbeys.\n\nThroughout history, clocks have had a variety of power sources, including gravity, springs, and electricity. Mechanical clocks became widespread in the 14th century, when they were used in medieval monasteries to keep the regulated schedule of prayers. The clock continued to be improved, with the first pendulum clock being designed and built in the 17th century.\n\nThe earliest mention of candle clocks comes from a Chinese poem, written in 520 by You Jianfu. According to the poem, the graduated candle was a means of determining time at night. Similar candles were used in Japan until the early 10th century.\n\nThe candle clock most commonly mentioned and written of is attributed to King Alfred the Great. It consisted of six candles made from 72 pennyweights of wax, each high, and of uniform thickness, marked every inch (2.54 cm). As these candles burned for about four hours, each mark represented 20 minutes. Once lit, the candles were placed in wooden framed glass boxes, to prevent the flame from extinguishing.\n\nThe most sophisticated candle clocks of their time were those of Al-Jazari in 1206. One of his candle clocks included a dial to display the time and, for the first time, employed a bayonet fitting, a fastening mechanism still used in modern times. Donald Routledge Hill described Al-Jazari's candle clocks as follows:\n\nA variation on this theme were oil-lamp clocks. These early timekeeping devices consisted of a graduated glass reservoir to hold oil — usually whale oil, which burned cleanly and evenly — supplying the fuel for a built-in lamp. As the level in the reservoir dropped, it provided a rough measure of the passage of time.\n\nIn addition to water, mechanical, and candle clocks, incense clocks were used in the Far East, and were fashioned in several different forms. Incense clocks were first used in China around the 6th century; in Japan, one still exists in the Shōsōin, although its characters are not Chinese, but Devanagari. Due to their frequent use of Devanagari characters, suggestive of their use in Buddhist ceremonies, Edward H. Schafer speculated that incense clocks were invented in India. Although similar to the candle clock, incense clocks burned evenly and without a flame; therefore, they were more accurate and safer for indoor use.\n\nSeveral types of incense clock have been found, the most common forms include the incense stick and incense seal. An incense stick clock was an incense stick with calibrations; most were elaborate, sometimes having threads, with weights attached, at even intervals. The weights would drop onto a platter or gong below, signifying that a certain amount of time had elapsed. Some incense clocks were held in elegant trays; open-bottomed trays were also used, to allow the weights to be used together with the decorative tray. Sticks of incense with different scents were also used, so that the hours were marked by a change in fragrance. The incense sticks could be straight or spiraled; the spiraled ones were longer, and were therefore intended for long periods of use, and often hung from the roofs of homes and temples. In Japan, a geisha was paid for the number of \"senkodokei\" (incense sticks) that had been consumed while she was present, a practice which continued until 1924.\n\nIncense seal clocks were used for similar occasions and events as the stick clock; while religious purposes were of primary importance, these clocks were also popular at social gatherings, and were used by Chinese scholars and intellectuals. The seal was a wooden or stone disk with one or more grooves etched in it into which incense was placed. These clocks were common in China, but were produced in fewer numbers in Japan. To signal the passage of a specific amount of time, small pieces of fragrant woods, resins, or different scented incenses could be placed on the incense powder trails. Different powdered incense clocks used different formulations of incense, depending on how the clock was laid out. The length of the trail of incense, directly related to the size of the seal, was the primary factor in determining how long the clock would last; all burned for long periods of time, ranging between 12 hours and a month.\n\nWhile early incense seals were made of wood or stone, the Chinese gradually introduced disks made of metal, most likely beginning during the Song dynasty. This allowed craftsmen to more easily create both large and small seals, as well as design and decorate them more aesthetically. Another advantage was the ability to vary the paths of the grooves, to allow for the changing length of the days in the year. As smaller seals became more readily available, the clocks grew in popularity among the Chinese, and were often given as gifts. Incense seal clocks are often sought by modern-day clock collectors; however, few remain that have not already been purchased or been placed on display at museums or temples.\n\nSundials had been used for timekeeping since Ancient Egypt. Ancient dials were nodus-based with straight hour-lines that indicated unequal hours—also called temporary hours—that varied with the seasons. Every day was divided into 12 equal segments regardless of the time of year; thus, hours were shorter in winter and longer in summer. The sundial was further developed by Muslim astronomers. The idea of using hours of equal length throughout the year was the innovation of Abu'l-Hasan Ibn al-Shatir in 1371, based on earlier developments in trigonometry by Muhammad ibn Jābir al-Harrānī al-Battānī (Albategni). Ibn al-Shatir was aware that \"using a gnomon that is parallel to the Earth's axis will produce sundials whose hour lines indicate equal hours on any day of the year\". His sundial is the oldest polar-axis sundial still in existence. The concept appeared in Western sundials starting in 1446.\n\nFollowing the acceptance of heliocentrism and equal hours, as well as advances in trigonometry, sundials appeared in their present form during the Renaissance, when they were built in large numbers. In 1524, the French astronomer Oronce Finé constructed an ivory sundial, which still exists; later, in 1570, the Italian astronomer Giovanni Padovani published a treatise including instructions for the manufacture and laying out of mural (vertical) and horizontal sundials. Similarly, Giuseppe Biancani's \"Constructio instrumenti ad horologia solaria\" (c. 1620) discusses how to construct sundials.\n\nSince the hourglass was one of the few reliable methods of measuring time at sea, it is speculated that it was used on board ships as far back as the 11th century, when it would have complemented the magnetic compass as an aid to navigation. However, the earliest unambiguous evidence of their use appears in the painting \"Allegory of Good Government\", by Ambrogio Lorenzetti, from 1338. From the 15th century onwards, hourglasses were used in a wide range of applications at sea, in churches, in industry, and in cooking; they were the first dependable, reusable, reasonably accurate, and easily constructed time-measurement devices. The hourglass also took on symbolic meanings, such as that of death, temperance, opportunity, and Father Time, usually represented as a bearded, old man. Though also used in China, the hourglass's history there is unknown. The Portuguese navigator Ferdinand Magellan used 18 hourglasses on each ship during his circumnavigation of the globe in 1522.\n\nThe earliest instance of a liquid-driven escapement was described by the Greek engineer Philo of Byzantium (fl. 3rd century) in his technical treatise \"Pneumatics\" (chapter 31) where he likens the escapement mechanism of a washstand automaton with those as employed in (water) clocks. Another early clock to use escapements was built during the 7th century in Chang'an, by Tantric monk and mathematician, Yi Xing, and government official Liang Lingzan. An astronomical instrument that served as a clock, it was discussed in a contemporary text as follows:\n[It] was made in the image of the round heavens and on it were shown the lunar mansions in their order, the equator and the degrees of the heavenly circumference. Water, flowing into scoops, turned a wheel automatically, rotating it one complete revolution in one day and night. Besides this, there were two rings fitted around the celestial sphere outside, having the sun and moon threaded on them, and these were made to move in circling orbit ... And they made a wooden casing the surface of which represented the horizon, since the instrument was half sunk in it. It permitted the exact determinations of the time of dawns and dusks, full and new moons, tarrying and hurrying. Moreover, there were two wooden jacks standing on the horizon surface, having one a bell and the other a drum in front of it, the bell being struck automatically to indicate the hours, and the drum being beaten automatically to indicate the quarters. All these motions were brought about by machinery within the casing, each depending on wheels and shafts, hooks, pins and interlocking rods, stopping devices and locks checking mutually.\n\nSince Yi Xing's clock was a water clock, it was affected by temperature variations. That problem was solved in 976 by Zhang Sixun by replacing the water with mercury, which remains liquid down to . Zhang implemented the changes into his clock tower, which was about tall, with escapements to keep the clock turning and bells to signal every quarter-hour. Another noteworthy clock, the elaborate Cosmic Engine, was built by Su Song, in 1088. It was about the size of Zhang's tower, but had an automatically rotating armillary sphere—also called a celestial globe—from which the positions of the stars could be observed. It also featured five panels with mannequins ringing gongs or bells, and tablets showing the time of day, or other special times. Furthermore, it featured the first known endless power-transmitting chain drive in horology. Originally built in the capital of Kaifeng, it was dismantled by the Jin army and sent to the capital of Yanjing (now Beijing), where they were unable to put it back together. As a result, Su Song's son Su Xie was ordered to build a replica.\nThe clock towers built by Zhang Sixun and Su Song, in the 10th and 11th centuries, respectively, also incorporated a striking clock mechanism, the use of clock jacks to sound the hours. A striking clock outside of China was the Jayrun Water Clock, at the Umayyad Mosque in Damascus, Syria, which struck once every hour. It was constructed by Muhammad al-Sa'ati in the 12th century, and later described by his son Ridwan ibn al-Sa'ati, in his \"On the Construction of Clocks and their Use\" (1203), when repairing the clock. In 1235, an early monumental water-powered alarm clock that \"announced the appointed hours of prayer and the time both by day and by night\" was completed in the entrance hall of the Mustansiriya Madrasah in Baghdad.\n\nThe first geared clock was invented in the 11th century by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia; it was a water clock that employed a complex gear train mechanism, including both segmental and epicyclic gearing, capable of transmitting high torque. The clock was unrivalled in its use of sophisticated complex gearing, until the mechanical clocks of the mid-14th century. Al-Muradi's clock also employed the use of mercury in its hydraulic linkages, which could function mechanical automata. Al-Muradi's work was known to scholars working under Alfonso X of Castile, hence the mechanism may have played a role in the development of the European mechanical clocks. Other monumental water clocks constructed by medieval Muslim engineers also employed complex gear trains and arrays of automata. Like the earlier Greeks and Chinese, Arab engineers at the time also developed a liquid-driven escapement mechanism which they employed in some of their water clocks. Heavy floats were used as weights and a constant-head system was used as an escapement mechanism, which was present in the hydraulic controls they used to make heavy floats descend at a slow and steady rate.\n\nA mercury clock, described in the \"Libros del saber de Astronomia\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. However, the device was actually a compartmented cylindrical water clock, which the Jewish author of the relevant section, Rabbi Isaac, constructed using principles described by a philosopher named \"Iran\", identified with Heron of Alexandria (fl. 1st century AD), on how heavy objects may be lifted.\n\nClock towers in Western Europe in the Middle Ages were also sometimes striking clocks. The most famous original still standing is possibly St Mark's Clock on the top of St Mark's Clocktower in St Mark's Square in Venice, assembled in 1493 by the clockmaker Gian Carlo Rainieri from Reggio Emilia. In 1497, Simone Campanato moulded the great bell on which every definite time-lapse is beaten by two mechanical bronze statues (h. 2,60 m.) called \"Due Mori\" (\"Two Moors\"), handling a hammer. Possibly earlier (1490) is the Prague Astronomical Clock by clockmaster Jan Růže (also called Hanuš) – according to another source this device was assembled as early as 1410 by clockmaker Mikuláš of Kadaň and mathematician Jan Šindel. The allegorical parade of animated sculptures rings on the hour every day.\n\nDuring the 11th century in the Song Dynasty, the Chinese astronomer, horologist and mechanical engineer Su Song created a water-driven astronomical clock for his clock tower of Kaifeng City. It incorporated an escapement mechanism as well as the earliest known endless power-transmitting chain drive, which drove the armillary sphere.\n\nContemporary Muslim astronomers also constructed a variety of highly accurate astronomical clocks for use in their mosques and observatories, such as the water-powered astronomical clock by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century. The most sophisticated timekeeping astrolabes were the geared astrolabe mechanisms designed by Abū Rayhān Bīrūnī in the 11th century and by Muhammad ibn Abi Bakr in the 13th century. These devices functioned as timekeeping devices and also as calendars.\nA sophisticated water-powered astronomical clock was built by Al-Jazari in 1206. This castle clock was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar paths, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing doors to open, each revealing a mannequin, every hour. It was possible to reset the length of day and night in order to account for the changing lengths of day and night throughout the year. This clock also featured a number of automata including falcons and musicians who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel.\n\nThe earliest medieval European clockmakers were Catholic monks. Medieval religious institutions required clocks because they regulated daily prayer- and work-schedules strictly, using various types of time-telling and recording devices, such as water clocks, sundials and marked candles, probably in combination. When mechanical clocks came into use, they were often wound at least twice a day to ensure accuracy. Monasteries broadcast important times and durations with bells, rung either by hand or by a mechanical device, such as by a falling weight or by rotating beater.\n\nAlthough the mortuary inscription of Pacificus, archdeacon of Verona, records that he constructed a night clock (\"horologium nocturnum\") as early as 850, his clock has been identified as being an observation tube used to locate stars with an accompanying book of astronomical observations, rather than a mechanical or water clock, an interpretation supported by illustrations from medieval manuscripts.\n\nThe religious necessities and technical skill of the medieval monks were crucial factors in the development of clocks, as the historian Thomas Woods writes:\nThe appearance of clocks in writings of the 11th century implies that they were well known in Europe in that period. In the early 14th-century, the Florentine poet Dante Alighieri referred to a clock in his \"Paradiso\"; the first known literary reference to a clock that struck the hours. Giovanni da Dondi, Professor of Astronomy at Padua, presented the earliest detailed description of clockwork in his 1364 treatise \"Il Tractatus Astrarii\". This has inspired several modern replicas, including some in London's Science Museum and the Smithsonian Institution. Other notable examples from this period were built in Milan (1335), Strasbourg (1354), Lund (1380), Rouen (1389), and Prague (1462).\n\nSalisbury cathedral clock, dating from about 1386, is one of the oldest working clocks in the world, and may be the oldest. It still has most of its original parts, although its original verge and foliot timekeeping mechanism is lost, having been converted to a pendulum, which was replaced by a replica verge in 1956. It has no dial, as its purpose was to strike a bell at precise times. The wheels and gears are mounted in an open, box-like iron frame, measuring about square. The framework is held together with metal dowels and pegs. Two large stones, hanging from pulleys, supply the power. As the weights fall, ropes unwind from the wooden barrels. One barrel drives the main wheel, which is regulated by the escapement, and the other drives the striking mechanism and the air brake.\n\nNote also Peter Lightfoot's Wells Cathedral clock, constructed c. 1390. The dial represents a geocentric view of the universe, with the Sun and Moon revolving around a central fixed Earth. It is unique in having its original medieval face, showing a philosophical model of the pre-Copernican universe. Above the clock is a set of figures, which hit the bells, and a set of jousting knights who revolve around a track every 15 minutes. The clock was converted to pendulum-and-anchor escapement in the 17th century, and was installed in London's Science Museum in 1884, where it continues to operate. Similar astronomical clocks, or \"horologes\", survive at Exeter, Ottery St Mary, and Wimborne Minster.\nOne clock that has not survived is that of the Abbey of St Albans, built by the 14th-century abbot Richard of Wallingford. It may have been destroyed during Henry VIII's Dissolution of the Monasteries, but the abbot's notes on its design have allowed a full-scale reconstruction. As well as keeping time, the astronomical clock could accurately predict lunar eclipses, and may have shown the Sun, Moon (age, phase, and node), stars and planets, as well as a wheel of fortune, and an indicator of the state of the tide at London Bridge. According to Thomas Woods, \"a clock that equaled it in technological sophistication did not appear for at least two centuries\". Giovanni de Dondi was another early mechanical clockmaker whose clock did not survive, but his work has been replicated based on the designs. De Dondi's clock was a seven-faced construction with 107 moving parts, showing the positions of the Sun, Moon, and five planets, as well as religious feast days. Around this period, mechanical clocks were introduced into abbeys and monasteries to mark important events and times, gradually replacing water clocks which had served the same purpose.\n\nDuring the Middle Ages, clocks primarily served religious purposes; the first employed for secular timekeeping emerged around the 15th century. In Dublin, the official measurement of time became a local custom, and by 1466 a public clock stood on top of the Tholsel (the city court and council chamber). It was the first of its kind to be clearly recorded in Ireland, and would only have had an hour hand. The increasing lavishness of castles led to the introduction of turret clocks. A 1435 example survives from Leeds castle; its face is decorated with the images of the Crucifixion of Jesus, Mary and St George.\n\nEarly clock dials showed hours: the display of minutes and seconds evolved later. A clock with a minutes dial is mentioned in a 1475 manuscript, and clocks indicating minutes and seconds existed in Germany in the 15th century. Timepieces which indicated minutes and seconds were occasionally made from this time on, but this was not common until the increase in accuracy made possible by the pendulum clock and, in watches, by the spiral balance spring. The 16th-century astronomer Tycho Brahe used clocks with minutes and seconds to observe stellar positions.\n\nThe Ottoman engineer Taqi al-Din described a weight-driven clock with a verge-and-foliot escapement, a striking train of gears, an alarm, and a representation of the moon's phases in his book \"The Brightest Stars for the Construction of Mechanical Clocks\" (\"Al-Kawākib al-durriyya fī wadh' al-bankāmat al-dawriyya\"), written around 1556.\n\nThe concept of the wristwatch goes back to the production of the very earliest watches in the 16th century. Elizabeth I of England received a wristwatch from Robert Dudley in 1571, described as an arm watch. From the beginning, wrist watches were almost exclusively worn by women, while men used pocket-watches up until the early 20th century. This was not just a matter of fashion or prejudice; watches of the time were notoriously prone to fouling from exposure to the elements, and could only reliably be kept safe from harm if carried securely in the pocket. When the waistcoat was introduced as a manly fashion at the court of Charles II in the 17th century, the pocket watch was tucked into its pocket. Prince Albert, the consort to Queen Victoria, introduced the 'Albert chain' accessory, designed to secure the pocket watch to the man's outergarment by way of a clip. By the mid nineteenth century, most watchmakers produced a range of wristwatches, often marketed as bracelets, for women.\n\nWristwatches were first worn by military men towards the end of the nineteenth century, when the importance of synchronizing manoeuvres during war without potentially revealing the plan to the enemy through signalling was increasingly recognized. It was clear that using pocket watches while in the heat of battle or while mounted on a horse was impractical, so officers began to strap the watches to their wrist. The Garstin Company of London patented a 'Watch Wristlet' design in 1893, although they were probably producing similar designs from the 1880s. Clearly, a market for men's wristwatches was coming into being at the time. Officers in the British Army began using wristwatches during colonial military campaigns in the 1880s, such as during the Anglo-Burma War of 1885.\n\nDuring the Boer War, the importance of coordinating troop movements and synchronizing attacks against the highly mobile Boer insurgents was paramount, and the use of wristwatches subsequently became widespread among the officer class. The company Mappin & Webb began production of their successful 'campaign watch' for soldiers during the campaign at the Sudan in 1898 and ramped up production for the Boer War a few years later.\nThese early models were essentially standard pocket-watches fitted to a leather strap, but by the early 20th century, manufacturers began producing purpose-built wristwatches. The Swiss company, Dimier Frères & Cie patented a wristwatch design with the now standard wire lugs in 1903. In 1904, Alberto Santos-Dumont, an early aviator, asked his friend, a French watchmaker called Louis Cartier, to design a watch that could be useful during his flights.\n\nThe impact of the First World War dramatically shifted public perceptions on the propriety of the man's wristwatch, and opened up a mass market in the post-war era. The creeping barrage artillery tactic, developed during the War, required precise synchronization between the artillery gunners and the infantry advancing behind the barrage. Service watches produced during the War were specially designed for the rigours of trench warfare, with luminous dials and unbreakable glass. Wristwatches were also found to be needed in the air as much as on the ground: military pilots found them more convenient than pocket watches for the same reasons as Santos-Dumont had. The British War Department began issuing wristwatches to combatants from 1917.\nThe company H. Williamson Ltd., based in Coventry, was one of the first to capitalize on this opportunity. During the company's 1916 AGM it was noted that \"...the public is buying the practical things of life. Nobody can truthfully contend that the watch is a luxury. It is said that one soldier in every four wears a wristlet watch, and the other three mean to get one as soon as they can.\" By the end of the War, almost all enlisted men wore a wristwatch, and after they were demobilized, the fashion soon caught on – the British \"Horological Journal\" wrote in 1917 that \"...the wristlet watch was little used by the sterner sex before the war, but now is seen on the wrist of nearly every man in uniform and of many men in civilian attire.\" Within a decade, sales of wristwatches had outstripped those of pocket watches.\n\nIn the late 17th and 18th Centuries, equation clocks were made, which allowed the user to see or calculate apparent solar time, as would be shown by a sundial. Before the invention of the pendulum clock, sundials were the only accurate timepieces. When good clocks became available, they appeared inaccurate to people who were used to trusting sundials. The annual variation of the equation of time made a clock up to about 15 minutes fast or slow, relative to a sundial, depending on the time of year. Equation clocks satisfied the demand for clocks that always agreed with sundials. Several types of equation clock mechanism were devised. which can be seen in surviving examples, mostly in museums.\n\nInnovations to the mechanical clock continued, with miniaturization leading to domestic clocks in the 15th century, and personal watches in the 16th. In the 1580s, the Italian polymath Galileo Galilei investigated the regular swing of the pendulum, and discovered that it could be used to regulate a clock. Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design. The first pendulum clock was designed and built by Dutch scientist Christiaan Huygens, in 1656. Early versions erred by less than one minute per day, and later ones only by 10 seconds, very accurate for their time.\n\nIn England, the manufacturing of pendulum clocks was soon taken up. The longcase clock (also known as the grandfather clock) was first created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671; this became feasible after Clement invented the anchor escapement mechanism in about 1670. Before then, pendulum clocks used the older verge escapement mechanism, which required very wide pendulum swings of about 100°. To avoid the need for a very large case, most clocks using the verge escapement had a short pendulum. The anchor mechanism, however, reduced the pendulum's necessary swing to between 4° to 6°, allowing clockmakers to use longer pendulums with consequently slower beats. These required less power to move, caused less friction and wear, and were more accurate than their shorter predecessors. Most longcase clocks use a pendulum about a metre (39 inches) long to the center of the bob, with each swing taking one second. This requirement for height, along with the need for a long drop space for the weights that power the clock, gave rise to the tall, narrow case.\n\nClement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clock-maker, and the Second Hand was introduced.\n\nThe Jesuits were another major contributor to the development of pendulum clocks in the 17th and 18th centuries, having had an \"unusually keen appreciation of the importance of precision\". In measuring an accurate one-second pendulum, for example, the Italian astronomer Father Giovanni Battista Riccioli persuaded nine fellow Jesuits \"to count nearly 87,000 oscillations in a single day\". They served a crucial role in spreading and testing the scientific ideas of the period, and collaborated with contemporary scientists, such as Huygens.\n\nThe invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century. Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring. Huygens was clearly the first to use a spiral balance spring, the form used in virtually all watches to the present day. The addition of the balance spring made the balance wheel a harmonic oscillator like the pendulum in a pendulum clock, which oscillated at a fixed resonant frequency and resisted oscillating at other rates. This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the watch face around 1680 in Britain and 1700 in France.\n\nLike the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry. The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. This resulted in a great advance in accuracy of pocket watches, from perhaps several hours per day to 10 minutes per day, similar to the effect of the pendulum upon mechanical clocks. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration.\n\nThe Rev. Edward Barlow invented the rack and snail striking mechanism for striking clocks, which was a great improvement over the previous mechanism. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nMarine chronometers are clocks used at sea as time standards, to determine longitude by celestial navigation.\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. The marine chronometer would have to keep the time of a fixed location—usually Greenwich Mean Time—allowing seafarers to determine longitude by comparing the local high noon to the clock. This clock could not contain a pendulum, which would be virtually useless on a rocking ship.\nAfter the Scilly naval disaster of 1707 where four ships ran aground due to navigational mistakes, the British government offered a large prize of £20,000, equivalent to millions of pounds today, for anyone who could determine longitude accurately. The reward was eventually claimed in 1761 by Yorkshire carpenter John Harrison, who dedicated his life to improving the accuracy of his clocks.\n\nIn 1735 Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat.\n\nThe chronometer was trialled in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nIn 1815, Sir Francis Ronalds (1788-1873) of London published the forerunner of the electric clock, the electrostatic clock. It was powered with dry piles, a high voltage battery with extremely long life but the disadvantage of its electrical properties varying with the weather. He trialled various means of regulating the electricity and these models proved to be reliable across a range of meteorological conditions.\n\nAlexander Bain, a Scottish clock and instrument maker, was the first to invent and patent the electric clock in 1840. On January 11, 1841, Alexander Bain along with John Barwise, a chronometer maker, took out another important patent describing a clock in which an electromagnetic pendulum and an electric current is employed to keep the clock going instead of springs or weights. Later patents expanded on his original ideas.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first quartz crystal oscillator was built by Walter G. Cady in 1921, and in 1927 the first quartz clock was built by Warren Marrison and J. W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. In 1932, a quartz clock able to measure small weekly variations in the rotation rate of the Earth was developed. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production has resulted in the subsequent proliferation of quartz clocks and watches.\n\nAtomic clocks are the most accurate timekeeping devices in practical use today. Accurate to within a few seconds over many thousands of years, they are used to calibrate other clocks and timekeeping instruments.\n\nThe idea of using atomic transitions to measure time was first suggested by Lord Kelvin in 1879, although it was only in the 1930s with the development of Magnetic resonance that there was a practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept.\nThe first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET).\n\nThe International System of Units standardized its unit of time, the second, on the properties of cesium in 1967. SI defines the second as 9,192,631,770 cycles of the radiation which corresponds to the transition between two electron spin energy levels of the ground state of the Cs atom. The cesium atomic clock, maintained by the National Institute of Standards and Technology, is accurate to 30 billionths of a second per year. Atomic clocks have employed other elements, such as hydrogen and rubidium vapor, offering greater stability—in the case of hydrogen clocks—and smaller size, lower power consumption, and thus lower cost (in the case of rubidium clocks).\n\nThe first professional clockmakers came from the guilds of locksmiths and jewellers. Clockmaking developed from a specialized craft into a mass production industry over many years.\n\nParis and Blois were the early centres of clockmaking in France. French clockmakers such as Julien Le Roy, clockmaker of Versailles, were leaders in case design and ornamental clocks. Le Roy belonged to the fifth generation of a family of clockmakers, and was described by his contemporaries as \"the most skillful clockmaker in France, possibly in Europe\". He invented a special repeating mechanism which improved the precision of clocks and watches, a face that could be opened to view the inside clockwork, and made or supervised over 3,500 watches. The competition and scientific rivalry resulting from his discoveries further encouraged researchers to seek new methods of measuring time more accurately.\n\nBetween 1794 and 1795, in the aftermath of the French Revolution, the French government briefly mandated decimal clocks, with a day divided into 10 hours of 100 minutes each. The astronomer and mathematician Pierre-Simon Laplace, among other individuals, modified the dial of his pocket watch to decimal time. A clock in the Palais des Tuileries kept decimal time as late as 1801, but the cost of replacing all the nation's clocks prevented decimal clocks from becoming widespread. Because decimalized clocks only helped astronomers rather than ordinary citizens, it was one of the most unpopular changes associated with the metric system, and it was abandoned.\n\nIn Germany, Nuremberg and Augsburg were the early clockmaking centers, and the Black Forest came to specialize in wooden cuckoo clocks.\nThe English became the predominant clockmakers of the 17th and 18th centuries. The main centres of the British industry were in the City of London, the West End of London, Soho where many skilled French Huguenots settled and later in Clerkenwell. The Worshipful Company of Clockmakers was established in 1631 as one of the Livery Companies of the City of London.\n\nThomas Tompion was the first English clockmaker with an international reputation and many of his pupils went on to become great horologists in their own right, such as George Graham who invented the deadbeat escapement, orrery and mercury pendulum, and his pupil Thomas Mudge who created the first lever escapement. Famous clockmakers of this period included Joseph Windmills, Simon de Charmes who established the De Charmes clockmaker firm and Christopher Pinchbeck who invented the alloy pinchbeck.\n\nLater famous horologists included John Arnold who made the first practical and accurate modern watch by refining Harrison's chronometer, Thomas Earnshaw who was the first to make these available to the public, Daniel Quare, who invented a repeating watch movement, a portable barometer and introduced the concentric minute hand.\n\nQuality control and standards were imposed on clockmakers by the Worshipful Company of Clockmakers, a guild which licensed clockmakers for doing business. By the rise of consumerism in the late 18th century, clocks, especially pocket watches, became regarded as fashion accessories and were made in increasingly decorative styles. By 1796, the industry reached a high point with almost 200,000 clocks being produced annually in London, however by the mid-19th century the industry had gone into steep decline from Swiss competition.\n\nSwitzerland established itself as a clockmaking center following the influx of Huguenot craftsmen, and in the 19th century, the Swiss industry \"gained worldwide supremacy in high-quality machine-made watches\". The leading firm of the day was Patek Philippe, founded by Antoni Patek of Warsaw and Adrien Philippe of Bern.\n\n\n\n\n"}
{"id": "47111027", "url": "https://en.wikipedia.org/wiki?curid=47111027", "title": "Implementation Rule", "text": "Implementation Rule\n\nThe Implementation Rules are regulations of the PRC, which set the framework of the valid product standards. For each product group there is a specific implementation rule, which is set by the Chinese authorities.\n\nThe Implementation Rules include 12 or 13 chapters, which determine the scope of the product certification. The following table provides an overview of the most common contents of the Implementation Rules:\n\nIn 2014, the Implementation Rules have been updated, so that from 2015 some changes have come into effect. For example, some product groups, which were previously grouped under one Implementation Rule, are now divided and subject to some different Implementation Rules. Furthermore, new products have been added, which are now subject to the mandatory certification. Further factory levels were introduced, so that the companies that carry out a product certification will be assigned a level (A-D) in future. Companies that receive particularly positive results on their products certification will receive level A.\n\n\n\n \n"}
{"id": "174521", "url": "https://en.wikipedia.org/wiki?curid=174521", "title": "Infrastructure", "text": "Infrastructure\n\nInfrastructure refers to the fundamental facilities and systems serving a country, city, or other area, including the services and facilities necessary for its economy to function. Infrastructure is composed of public and private physical improvements such as roads, bridges, tunnels, water supply, sewers, electrical grids, and telecommunications (including Internet connectivity and broadband speeds). In general, it has also been defined as \"the physical components of interrelated systems providing commodities and services essential to enable, sustain, or enhance societal living conditions\".\n\nThere are two general types of ways to view infrastructure, hard or soft. Hard infrastructure refers to the physical networks necessary for the functioning of a modern industry. This includes roads, bridges, railways, etc. Soft infrastructure refers to all the institutions that maintain the economic, health, social, and cultural standards of a country. This includes educational programs, parks and recreational facilities, law enforcement agencies, and emergency services.\n\nThe word infrastructure has been used in English since 1887 and in French since 1875, originally meaning \"The installations that form the basis for any operation or system\". The word was imported from French, where it means \"subgrade\", the native material underneath a constructed pavement or railway. The word is a combination of the Latin prefix \"infra\", meaning \"below\" and many of these constructions are underground, for example, tunnels, water and gas systems, and railways. The army use of the term achieved currency in the United States after the formation of NATO in the 1940s, and by 1970 was adopted by urban planners in its modern civilian sense.\n\nA 1987 US National Research Council panel adopted the term \"public works infrastructure\", referring to:\n\n\"... both specific functional modes – highways, streets, roads, and bridges; mass transit; airports and airways; water supply and water resources; wastewater management; solid-waste treatment and disposal; electric power generation and transmission; telecommunications; and hazardous waste management – and the combined system these modal elements comprise. A comprehension of infrastructure spans not only these public works facilities, but also the operating procedures, management practices, and development policies that interact together with societal demand and the physical world to facilitate the transport of people and goods, provision of water for drinking and a variety of other uses, safe disposal of society's waste products, provision of energy where it is needed, and transmission of information within and between communities.\"\nThe OECD also classifies communications as a part of infrastructure.\n\nThe American Society of Civil Engineers publish a \"Infrastructure Report Card\" which represents the organizations opinion on the condition of various infrastructure every 2–4 years. they grade 16 categories, namely Aviation, Bridges, Dams, Drinking Water, Energy, Hazardous Waste, Inland Waterways, Levees, Parks & Recreation, Ports, Rail, Roads, Schools, Solid Waste, Transit and Wastewater.\n\nA way to embody personal infrastructure is to think of it in term of human capital. Human capital is defined by the Encyclopedia Britannica as “intangible collective resources possessed by individuals and groups within a given population\". The goal of personal infrastructure is to determine the quality of the economic agents’ values. This results in three major tasks: the task of economic proxies’ in the economic process (teachers, unskilled and qualified labor, etc.); the importance of personal infrastructure for an individual (short and long-term consumption of education); and the social relevance of personal infrastructure.\n\nInstitutional infrastructure branches from the term \"economic constitution\". According to Gianpiero Torrisi, Institutional infrastructure is the object of economic and legal policy. It compromises the grown and sets norms. It refers to the degree of actual equal treatment of equal economic data and determines the framework within which economic agents may formulate their own economic plans and carry them out in co-operation with others.\n\nMaterial infrastructure is defined as “those immobile, non-circulating capital goods that essentially contribute to the production of infrastructure goods and services needed to satisfy basic physical and social requirements of economic agents\". There are two distinct qualities of material infrastructures: 1) Fulfillment of social needs and 2) Mass production. The first characteristic deals with the basic needs of human life. The second characteristic is the non-availability of infrastructure goods and services.\n\nAccording to the business dictionary, economic infrastructure can be defined as \"internal facilities of a country that make business activity possible, such as communication, transportation and distribution networks, financial institutions and markets, and energy supply systems\". Economic infrastructure support productive activities and events. This includes roads, highways, bridges, airports, water distribution networks, sewer systems, irrigation plants, etc.\n\nSocial infrastructure can be broadly defined as the construction and maintenance of facilities that support social services. Social infrastructures are created to increase social comfort and act on economic activity. These being schools, parks and playgrounds, structures for public safety, waste disposal plants, hospitals, sports area, etc.\n\nCore assets provide essential services and have monopolistic characteristics. Investors seeking core infrastructure look for five different characteristics: Income, Low volatility of returns, Diversification, Inflation Protection, and Long-term liability matching. Core Infrastructure incorporates all the main types of infrastructure. For instance; roads, highways, railways, public transportation, water and gas supply, etc.\n\nBasic infrastructure refers to main railways, roads, canals, harbors and docks, the electromagnetic telegraph, drainage, dikes, and land reclamation. It consist of the more well-known features of infrastructure. The things in the world we come across everyday (buildings, roads, docks, etc).\n\nComplementary infrastructure refers to things like light railways, tramways, gas/electricity/water supply, etc. To complement something, means to bring to perfection or complete it. So, complementary infrastructure deals with the little parts of the engineering world the brings more life. The lights on the sidewalks, the landscaping around buildings, the benches for pedestrians to rest, etc.\n\nThe term \"infrastructure\" may be confused with the following overlapping or related concepts.\n\nLand improvement and land development are general terms that in some contexts may include infrastructure, but in the context of a discussion of infrastructure would refer only to smaller scale systems or works that are not included in infrastructure, because they are typically limited to a single parcel of land, and are owned and operated by the land owner. For example, an irrigation canal that serves a region or district would be included with infrastructure, but the private irrigation systems on individual land parcels would be considered land improvements, not infrastructure. Service connections to municipal service and public utility networks would also be considered land improvements, not infrastructure.\n\nThe term public works includes government-owned and operated infrastructure as well as public buildings, such as schools and court houses. Public works generally refers to physical assets needed to deliver public services. Public services include both infrastructure and services generally provided by government.\n\nInfrastructure may be owned and managed by governments or by private companies, such as sole public utility or railway companies. Generally, most roads, major airports and other ports, water distribution systems, and sewage networks are publicly owned, whereas most energy and telecommunications networks are privately owned. Publicly owned infrastructure may be paid for from taxes, tolls, or metered user fees, whereas private infrastructure is generally paid for by metered user fees. Major investment projects are generally financed by the issuance of long-term bonds.\n\nGovernment-owned and operated infrastructure may be developed and operated in the private sector or in public-private partnerships, in addition to in the public sector. in the United States for example, public spending on infrastructure has varied between 2.3% and 3.6% of GDP since 1950. Many financial institutions invest in infrastructure.\n\nEngineers generally limit the term \"infrastructure\" to describe fixed assets that are in the form of a large network; in other words, hard infrastructure. Efforts to devise more generic definitions of infrastructures have typically referred to the network aspects of most of the structures, and to the accumulated value of investments in the networks as assets. One such definition from 1998 defined infrastructure as the network of assets \"where the system as a whole is intended to be maintained indefinitely at a specified standard of service by the continuing replacement and refurbishment of its components\".\n\nCivil defense planners and developmental economists generally refer to both hard and soft infrastructure, including public services such as schools and hospitals, emergency services such as police and fire fighting, and basic financial services. The notion of infrastructure-based development combining long-term infrastructure investments by government agencies at central and regional levels with public private partnerships has proven popular among economists in Asia (notably Singapore and China), mainland Europe, and Latin America.\n\nMilitary infrastructure is the buildings and permanent installations necessary for the support of military forces, whether they are stationed in bases, being deployed or engaged in operations. For example, barracks, headquarters, airfields, communications facilities, stores of military equipment, port installations, and maintenance stations.\n\nGreen infrastructure (or blue-green infrastructure) highlights the importance of the natural environment in decisions about land use planning. In particular there is an emphasis on the \"life support\" functions provided by a network of natural ecosystems, with an emphasis on interconnectivity to support long-term sustainability. Examples include clean water and healthy soils, as well as the more anthropocentric functions such as recreation and providing shade and shelter in and around towns and cities. The concept can be extended to apply to the management of stormwater runoff at the local level through the use of natural systems, or engineered systems that mimic natural systems, to treat polluted runoff.\n\nIn Marxism, the term \"infrastructure\" is sometimes used as a synonym for \"base\" in the dialectic synthetic pair \"base and superstructure\". However, the Marxist notion of \"base\" is broader than the non-Marxist use of the term \"infrastructure\", and some soft infrastructure, such as laws, governance, regulations, and standards, would be considered by Marxists to be part of the superstructure, not the base.\n\nCommunications infrastructure is the informal and formal channels of communication, political and social networks, or beliefs held by members of particular groups, as well as information technology, software development tools. Still underlying these more conceptual uses is the idea that infrastructure provides organizing structure and support for the system or organization it serves, whether it is a city, a nation, a corporation, or a collection of people with common interests. Examples include IT infrastructure, research infrastructure, terrorist infrastructure, employment infrastructure and tourism infrastructure.\n\nAccording to researchers at the Overseas Development Institute, the lack of infrastructure in many developing countries represents one of the most significant limitations to economic growth and achievement of the Millennium Development Goals (MDGs). Infrastructure investments and maintenance can be very expensive, especially in such areas as landlocked, rural and sparsely populated countries in Africa. It has been argued that infrastructure investments contributed to more than half of Africa's improved growth performance between 1990 and 2005, and increased investment is necessary to maintain growth and tackle poverty. The returns to investment in infrastructure are very significant, with on average thirty to forty percent returns for telecommunications (ICT) investments, over forty percent for electricity generation, and eighty percent for roads.\n\nThe demand for infrastructure, both by consumers and by companies is much higher than the amount invested. There are severe constraints on the supply side of the provision of infrastructure in Asia. The infrastructure financing gap between what is invested in Asia-Pacific (around US$48 billion) and what is needed (US$228 billion) is around US$180 billion every year.\n\nIn Latin America, three percent of GDP (around US$71 billion) would need to be invested in infrastructure in order to satisfy demand, yet in 2005, for example, only around two percent was invested leaving a financing gap of approximately US$24 billion.\n\nIn Africa, in order to reach the seven percent annual growth calculated to be required to meet the MDGs by 2015 would require infrastructure investments of about fifteen percent of GDP, or around US$93 billion a year. In fragile states, over thirty-seven percent of GDP would be required.\n\nThe source of financing varies significantly across sectors. Some sectors are dominated by government spending, others by overseas development aid (ODA), and yet others by private investors. In California, infrastructure financing districts are established by local governments to pay for physical facilities and services within a specified area by using property tax increases. In order to facilitate investment of the private sector in developing countries' infrastructure markets, it is necessary to design risk-allocation mechanisms more carefully, given the higher risks of their markets.\n\nThe spending money that comes from the government is less than it used to be. Compared to the global GDP percentages, The United States is tied for second-to-last place, with an average percentage of 2.4%. This means that the government spends less money on repairing old infrastructure and or on infrastructure as a whole. \n\nIn Sub-Saharan Africa, governments spend around US$9.4 billion out of a total of US$24.9 billion. In irrigation, governments represent almost all spending. In transport and energy a majority of investment is government spending. In ICT and water supply and sanitation, the private sector represents the majority of capital expenditure. Overall, between them aid, the private sector, and non-OECD financiers exceed government spending. The private sector spending alone equals state capital expenditure, though the majority is focused on ICT infrastructure investments. External financing increased in the 2000s (decade) and in Africa alone external infrastructure investments increased from US$7 billion in 2002 to US$27 billion in 2009. China, in particular, has emerged as an important investor.\n\n\n"}
{"id": "827154", "url": "https://en.wikipedia.org/wiki?curid=827154", "title": "Internet activism", "text": "Internet activism\n\nInternet activism (also known as web activism, online activism, digital campaigning, digital activism\", \"online organizing, electronic advocacy, cyberactivism, e-campaigning, and e-activism) is the use of electronic communication technologies such as social media, e-mail, and podcasts for various forms of activism to enable faster and more effective communication by citizen movements, the delivery of particular information to large and specific audiences as well as coordination. Internet technologies are used for cause-related fundraising, community building, lobbying, and organizing. A digital activism campaign is \"an\norganized public effort, making collective claims on a target authority, in which civic initiators or supporters use digital media.\" Research has started to address specifically how activist/advocacy groups in the U.S. and Canada are using social media to achieve digital activism objectives.\n\nSandor Vegh divides online activism into three main categories: Awareness/advocacy, organization/mobilization, and action/reaction. There are other ways of classifying types of online activism, such as by the degree of reliance on the Internet. Thus, Internet sleuthing or hacking could be viewed as purely online forms of activism, whereas the Occupy Wall Street movement was only partially online.\n\nThe Internet is a key resource for independent activists, or E-activists, particularly those whose message may run counter to the mainstream. \"Especially when a serious violation of human rights occurs, the Internet is essential in reporting the atrocity to the outside world.\" Listservs like BurmaNet and Freedom News Group help distribute news that would otherwise be inaccessible in these countries.\n\nInternet activists also pass on E-petitions to be sent to the government and public and private organizations to protest against and urge for positive policy change in areas from the arms trade to animal testing. Many non-profits and charities use these methods, emailing petitions to those on their email list and asking people to pass them on. The Internet also enables organizations such as NGOs to communicate with individuals in an inexpensive and timely manner. Gatherings and protests can be organized with the input of the organizers and the participants. Lobbying is also made easier via the Internet, thanks to mass e-mail and its ability to broadcast a message widely at little cost. Vegh's concept of organization/mobilization, for example, can refer to activities taking place solely online, solely offline but organized online, or a combination of online and offline. Mainstream social-networking sites, most noticeably Facebook.com, are also making e-activist tools available to their users. An active participatory culture is enabled by the communities on social networking sites because they permit communication between groups that are otherwise unable to communicate. In the article \"Why We Argue about Virtual Community: A Case Study of the Phish.net Fan Community,\" Nessim Watson stresses the necessity of communication in online communities. He even goes as far as to say that \"Without ongoing communication among its participants, a community dissolves\". The constant ability to communicate with members of the community enriches online community experiences and redefines the word community.\n\nIn addition, denial-of-Service attacks, the taking over and vandalizing of a website, uploading Trojan horses, and sending out e-mail bombs (mass e-mailings) are also examples of Internet activism. For more examples of these types of subversive action, see hacktivism.\n\nHashtag activism is the use of hashtags for fighting or supporting a cause through the usage of social media outlets. Its use has been associated with the 2014 Chibok kidnapping, with hopes that it would help keep the story in the news and raise international attention. The hashtag itself has received 2 million retweets.\n\nOne example of the powerful rise of hashtag activism can be seen in the black feminist movement's use of hashtags to convey their cause. The famous hashtag \"IamJada\" was an internet backlash to the mocking \"#Jadapose\" that went viral, ensuing after a sixteen-year-old girl Jada Smart was photographed following her gang rape In this instance, a hashtag was employed to convey a powerful anti-rape message.\n\nExploring the dynamics of online activism for expressing resistance to a powerful organization, a study published in Information and Organization developed a critical mass approach to online activism. The results were integrated in a four-year longitudinal process model that explains how online activism started, generated societal outcomes, and changed over time. The model suggests that online activism helped organize collective actions and amplify the conditions for revolutionary movements to form. Yet, it provoked elites’ reactions such as Internet filtering and surveillance, which do not only promote self-censorship and generate digital divide, but contribute to the ultimate decline of activism over time. The process model suggests a complex interplay among stakeholders’ interests, opportunities for activism, costs and outcomes that are neither foreseen nor entirely predictable. The authors challenge universal access to the Internet as a convenient and cost-free forum for practicing social activism by organizational stakeholders (customers, employees, outside parties). In fact, the technology enablers of social activism also enable its filtering and repression and thus more extreme states of information asymmetry may result in which powerful elites preserve their status and impose a greater digital divide.\n\nIn one study, a discussion of a developmental model of political mobilization is discussed. By citizens joining groups and creating discussion, they are beginning their first stage of involvement. Progressively, it is hoped that they will begin signing petitions online and graduating to offline contact as long as the organization provides the citizen with escalating steps of involvement (Vitak et al., 2011).\n\nThe issue of the mass media's centrality has been highly contested, with some people arguing that it promoted the voices of marginalized groups while others believe it sends forth the messages of the majority alone, leaving minority groups to have their voices robbed.\n\nOne of the earliest known uses of the Internet as a medium for activism was that around Lotus MarketPlace. On April 10, 1990, Lotus announced a direct-mail marketing database product that was to contain name, address, and spending habit information on 120 million individual U.S. citizens. While much of the same data was already available, privacy advocates worried about the availability of this data within one database. Furthermore, the data would be on CD-ROM, and so would remain fixed until a new CD-ROM was issued.\n\nIn response, a mass e-mail and E-bulletin-board campaign was started, which included information on contacting Lotus and form letters. Larry Seiler, a New England-based computer professional, posted a message that was widely reposted on newsgroups and via e-mail: \"It will contain a LOT of personal information about YOU, which anyone in the country can access by just buying the discs. It seems to me (and to a lot of other people, too) that this will be a little too much like big brother, and it seems like a good idea to get out while there is still time.\"Over 30,000 people contacted Lotus and asked for their names to be removed from the database. On January 23, 1991, Lotus announced that it had cancelled MarketPlace.\n\nIn 1993, a survey article about online activism around the world, from Croatia to the United States, appeared in \"The Nation\" magazine, with several activists being quoted about their projects and views.\n\nThe earliest example of mass emailing as a rudimentary form of DDoS occurred on Guy Fawkes Day 1994, when the Intervasion of the UK began email-bombing John Major's cabinet and UK parliamentary servers in protest against the Criminal Justice Bill, which outlawed outdoor rave festivals and \"music with a repetitive beat\"\n\nIn 1995–1998, Z magazine offered courses online through Left Online University, with lessons on \"Using the Internet for Electronic Activism.\"\n\nThe practice of cyber-dissidence and activism per se, that is, in its modern-day form, may have been inaugurated by Dr. Daniel Mengara, a Gabonese scholar and activist living in political exile in New Jersey in the United States. In 1998, he created a Website in French whose name \"Bongo Doit Partir\" (Bongo Must Go) was clearly indicative of its purpose: it encouraged a revolution against the then 29-year-old regime of Omar Bongo in Gabon. The original URL, http://www.globalwebco.net/bdp/, began to redirect to http://www.bdpgabon.org in the year 2000. Inaugurating what was to become common current-day practice in the politically involved blogosphere, this movement's attempt at rallying the Gabonese around revolutionary ideals and actions has ultimately been vindicated by the 2011 Tunisian and Egyption revolutions, where the Internet has proven to be an effective tool for instigating successful critique, opposition, and revolution against dictators. In July 2003, Amnesty International reported the arrest of five Gabonese known-to-be members of the cyber-dissident group \"Bongo Doit Partir\". The five members were detained for three months (See: Gabon: Prisoners of Conscience and Gabon: Further information on Prisoners of conscience).\n\nAnother well-known example of early Internet activism took place in 1998, when the Mexican rebel group EZLN used decentralized communications, such as cell phones, to network with developed world activists and help create the anti-globalization group Peoples Global Action (PGA) to protest the World Trade Organization (WTO) in Geneva. The PGA continued to call for \"global days of action\" and rally support of other anti-globalization groups in this way.\n\nLater, a worldwide network of Internet activist sites, under the umbrella name of Indymedia, was created \"for the purpose of providing grassroots coverage of the WTO protests in Seattle\" in 1999. Dorothy Kidd quotes Sheri Herndon in a July 2001 telephone interview about the role of the Internet in the anti-WTO protests: \"The timing was right, there was a space, the platform was created, the Internet was being used, we could bypass the corporate media, we were using open publishing, we were using multimedia platforms. So those hadn't been available, and then there was the beginning of the anti-globalization movement in the United States.\"\n\nIn the UK, in 1999, the Government introduced a new employment tax called IR35. One of the first online trade associations was created to campaign against it. Within weeks they had raised £100,000 off the Internet from individuals who had never even met. They became a fully formed trade association called the Professional Contractors Group, which two years later had 14,000 members all paying £100 each to join. They presented the first ever e-petition to Parliament and organized one of the first flash mobs when using their database, to their surprise and others, 1,000 came in their call to lobby Parliament. They later raised £500,000 from the Internet to fund an unsuccessful High Court challenge against the tax, though ultimately they secured some concessions. Their first external affairs director, Philip Ross, has written a history of the campaign.\n\nThe engagement in the practice of strategic voting was another development that came with Internet activism. People coordinated their vote pairing by entering their contact information into an online database, thereby reducing cost completely.\n\n\"Kony 2012\", a short film released on March 5, 2012. The film's purpose was to promote the charity's \"Stop Kony\" movement to make African cult and militia leader, indicted war criminal and the International Criminal Court fugitive Joseph Kony globally known in order to have him arrested by the end of 2012, when the campaign expired. The film spread virally. A poll suggested that more than half of young adult Americans heard about \"Kony 2012\" in the days following the video's release. It was included among the top international events of 2012 by PBS and called the most viral video ever by \"TIME\". The campaign resulted in a resolution by the United States Senate and contributed to the decision to send troops by the African Union.\n\n\nInternet activism has had the effect of causing increased collective action among people, as found by Postmes and Brunsting (2002), who discovered a tendency among internet users to rely on internalized group memberships and social identities in order to achieve social involvement online. \nThe Internet is \"tailor-made for a populist, insurgent movement,\" says Joe Trippi, who managed the Howard Dean campaign. In his campaign memoir, \"The Revolution Will Not Be Televised,\" Trippi notes that:\n\n[The Internet's] roots in the open-source ARPAnet, its hacker culture, and its decentralized, scattered architecture make it difficult for big, establishment candidates, companies and media to gain control of it. And the establishment loathes what it can't control. This independence is by design, and the Internet community values above almost anything the distance it has from the slow, homogeneous stream of American commerce and culture. Progressive candidates and companies with forward-looking vision have an advantage on the Internet, too. Television is, by its nature, a nostalgic medium. Look at Ronald Reagan's campaign ads in the 1980s – they were masterpieces of nostalgia promising a return to America's past glory and prosperity. The Internet, on the other hand, is a forward-thinking and forward-moving medium, embracing change and pushing the envelope of technology and communication.\n\nWhen discussing the 2004 U.S. presidential election candidates, Carol Darr, director of the Institute for Politics, Democracy & the Internet at George Washington University in Washington, D.C., said of the candidates which benefited from use of the Internet to attract supporters: \"They are all charismatic, outspoken mavericks and insurgents. Given that the Internet is interactive and requires an affirmative action on the part of the users, as opposed to a passive response from TV users, it is not surprising that the candidate has to be someone people want to touch and interact with.\"\n\nA more decentralized approach to campaigning arose, in contrast to a top-down, message-focused approach usually conducted in the mainstream. \"The mantra has always been, 'Keep your message consistent. Keep your message consistent,'\" said John Hlinko, who has participated in Internet campaigns for MoveOn.org and the electoral primary campaign of Wesley Clark. \"That was all well and good in the past. Now it's a recipe for disaster ... You can choose to have a Stalinist structure that's really doctrinaire and that's really opposed to grassroots. Or you can say, 'Go forth. Do what you're going to do.' As long as we're running in the same direction, it's much better to give some freedom.\"\n\nTwo-thirds of Internet users under the age of 30 have a SNS, and during the 2008 election, half of them used a SNS site for candidate information (Hirzalla, 2010).\n\nThe Internet has become the catalyst for protests such as Occupy Wall Street and the Arab Spring as those involved have increasingly relied on social media to organize and stay connected.\nIn Myanmar, online news paper Freedom News Group has leaked some government corruption and fuel to protests.\n\nIn 2017, the Sleeping Giants cyberactivist group, among others, launched a boycott campaign against controversial, conservative webpage Breitbart News, getting more than 2,000 organizations to remove it from ad buys.\n\nCorporations are also using Internet activist techniques to increase support for their causes. According to Christopher Palmeri with \"BusinessWeek\" Online, companies launch sites with the intent to positively influence their own public image, to provide negative pressure on competitors, to influence opinion within select groups, and to push for policy changes.\n\nThe clothing manufacturer, American Apparel is an example: The company hosts a website called Legalize LA that advocates immigration reform via blog, online advertising, links to news stories and educational materials. Protest groups have responded by posting YouTube videos and establishing a boycott website.\n\nCorporate methods of information dissemination is labelled \"astroturfing,\" as opposed to \"grassroots activism,\" due to the funding for such movements being largely private. More recent examples include the right-wing FreedomWorks.org which organized the \"Taxpayer March on Washington\" on September 12, 2009 and the Coalition to Protect Patients' Rights, which opposes universal health care in the U.S.\n\nCybersectarianism is a new organizational form which involves: \"highly dispersed small groups of practitioners that may remain largely anonymous within the larger social context and operate in relative secrecy, while still linked remotely to a larger network of believers who share a set of practices and texts, and often a common devotion to a particular leader. Overseas supporters provide funding and support; domestic practitioners distribute tracts, participate in acts of resistance, and share information on the internal situation with outsiders. Collectively, members and practitioners of such sects construct viable virtual communities of faith, exchanging personal testimonies and engaging in collective study via email, on-line chat rooms and web-based message boards.\"\n\nOne of the earliest books on activism was Don Rittner's \"Ecolinking - Everyone's Guide to Online Environmental Information,\" Published by Peachpit Press in 1992. Rittner, an environmental activist from upstate New York, spent more than 20 years researching and saving the Albany Pine Barrens. He was a beta tester for America Online and ran their Environmental Forum for the company from 1988 to when it launched in 1990. He took his early environmental knowledge and computer savvy and wrote what was called the bible of the online environmental community. It showed new Net users how to get online, find environmental information, connect to environmentalists around the world, and how to use those resources to save the planet.\n\nActivism against sexual assault can be led on the internet, where individuals may feel comfortable talking about controversial topics. One such movements is the #NotGuilty movement. This movement began in April 2015 with Ione Wells. She shared a \"letter to her attacker\" in her college paper. The letter described how she was sexually assaulted and how she chose to respond and build from that point in her life. At the end of the letter she urged readers to send a letter back describing their own sexual assault experience with the hashtag #notguilty. She received so many letters from locals that she decided to create a website, this caused global attention and inspired many to share their stories. The Me Too movement is a similar movement that started in Hollywood. Tarana Burke created the phrase to \"empower women through empathy\" and Alyssa Milano helped spread the use of the phrase. This phrase was first used to demonstrate the amount of sexual assault that happens to young actresses and actors in Hollywood. It soon spread to apply to all forms of sexual assault, especially in the work place. These movements were intended to create an outlet for men and women to share their experiences with those with similar views without blame or guilt. They brought widespread attention to sexual assault and caused much controversy about changes that should be made accordingly. Criticism around movements such as these centers on concerns about whether or not participants are being dishonest for their own gain or are misinterpreting acts of kindness.\n\nAccording to some observers, the Internet may have considerable potential to reach and engage opinion leaders who influence the thinking and behavior of others. According to the Institute for Politics, Democracy & the Internet, what they call \"Online Political Citizens\" (OPCs) are \"seven times more likely than average citizens to serve as opinion leaders among their friends, relatives and colleagues… Normally, 10% of Americans qualify as Influentials. Our study found that 69% of Online Political Citizens are Influentials.\"\n\nInformation communication technologies (ICTs) make communication and information readily available and efficient. There are millions of Facebook accounts, Twitter users and websites, and one can educate oneself on nearly any subject. While this is for the most part a positive thing, it can also be dangerous. For example, people can read up on the latest news events relatively easily and quickly; however, there is danger in the fact that apathy or fatigue can quickly arise when people are inundated with so many messages, or that the loudest voice on a subject can often be the most extreme one, distorting public perception on the issue.\n\nThese social networks which occupy ICTs are simply modern forms of political instruments which pre-date the technological era. People can now go to online forums or Twitter instead of town hall meetings. People can essentially mobilize worldwide through the Internet. Women can create transnational alliances and lobby for rights within their respective countries; they can give each other tips and share up-to-date information. This information becomes \"hyper textual\", available in downloadable formats with easy access for all. The UN organizations also use \"hyper textual\" formats. They can post information about upcoming summits, they can post newsletters on what occurred at these meetings, and links to videos can be shared; all of this information can be downloaded at the click of a button. The UN and many other actors are presenting this information in an attempt to get a certain message out in the cyber sphere and consequently steer public perception on an issue.\n\nWith all this information so readily available, there is a rising trend of \"slacktivism\" or \"clicktivism\". While it is positive that information can be distributed so quickly and efficiently all around the world, there is negativity in the fact that people often take this information for granted, or quickly forget about it once they have seen it flash across our computer screens. Viral campaigns are great for sparking initial interest and conversation, but they are not as effective in the long term—people begin to think that clicking \"like\" on something is enough of a contribution, or that posting information about a current hot topic on their Facebook page or Twitter feed means that they have made a difference.\n\nThe Internet has also made it easier for small donors to play a meaningful role in financing political campaigns. Previously, small-donor fundraising was prohibitively expensive, as costs of printing and postage ate up most of the money raised. Groups like MoveOn, however, have found that they can raise large amounts of money from small donors at minimal cost, with credit card transaction fees constituting their biggest expense. \"For the first time, you have a door into the political process that isn't marked 'big money,' \" says Darr. \"That changes everything.\n\nWith internet technology vastly changing existing and introducing new mechanisms by which to attain, share and employ information, internet activism raises ethical issues for consideration. Proponents contend internet activism serves as an outlet for social progress but only if personal and professional ethics are employed. Supporters of online activism claim new information and communications technologies help increase the political power of activist groups that would otherwise have less resources. Proponents along this line of thinking claim the most effective use of online activism is its use in conjunction with more traditional or historical activism activities. Conversely, critics worry about facts and beliefs becoming indistinct in online campaigns and about \"sectors of online activism [being] more self-interested than socially interested.\" These critics warn against the manipulation commonplace to online activism for private or personal interests such as exploiting charities for monetary gain, influencing voters in the political arena and inflating self-importance or effectiveness. In this sense, the ethical implication is that activism becomes descriptive rather than transformative of society. One of these reviewers suggests seven pitfalls to beware of in internet activism: \"self-promotion at the expense of the movement... unsolicited bulk email... Hacktivism... violating copyright... nagging... violating privacy... and being scary.\" Many of the ethical criticisms against the prevalence of online activism are further discussed in the criticisms section of this article.\n\nCritics argue that Internet activism faces the same challenges as other aspects of the digital divide, particularly the global digital divide. Some say it gives disproportionate representation to those with greater access or technological ability. Groups that may be disadvantaged by the move to activist activity online are those that have limited access to technologies, or lack the technological literacy to engage meaningfully online; these include ethnic and racial minorities, those of lower socioeconomic status, those with lower levels of education, and the elderly.\n\nA study looked at the impact of Social Networking Sites (SNS) on various demographics and their political activity. Not surprisingly college students used SNS for political activity the most but this was followed by a more unlikely group, those that had not completed high school. In addition the probability for non-White citizens to consume political information was shown to be higher than that of Whites. These two outcomes go in the face of normal predictors of political activity. Despite these surprising findings older generations, men and whites showed the highest levels of political mobilization. Acts of political mobilization, such as fundraising, volunteering, protesting require the most continued interest, resources and knowledge (Nam, 2010).\n\nThe experience of the echo chamber is easier to create with a computer than with many of the forms of political interaction that preceded it,\" Sunstein told the \"New York Times\". \"The discussion will be about strategy, or horse-race issues or how bad the other candidates are, and it will seem like debate. It's not like this should be censored, but it can increase acrimony, increase extremism and make mutual understanding more difficult.\n\nOn the other hand, Scott Duke Harris of the \"San Jose Mercury News\" noted that \"the Internet connects [all sides of issues, not just] an ideologically broad anti-war constituency, from the leftists of ANSWER to the pressed-for-time 'soccer moms' who might prefer MoveOn, and conservative activists as well.\"\n\nAnother concern, according to University of California professor Barbara Epstein, is that the Internet \"allows people who agree with each other to talk to each other and gives them the impression of being part of a much larger network than is necessarily the case.\" She warns that the impersonal nature of communication by computer may actually undermine the human contact that always has been crucial to social movements.\n\nAnother concern, expressed by author and law professor Cass Sunstein, is that online political discussions lead to \"cyberbalkanization\"—discussions that lead to fragmentation and polarization rather than consensus, because the same medium that lets people access a large number of news sources also enables them to pinpoint the ones they agree with and ignore the rest.\n\nFamed activist Ralph Nader has stated that \"the Internet doesn't do a very good job of motivating action\", citing that the United States Congress, corporations and the Pentagon do not necessarily \"fear the civic use of the Internet.\" Ethan Zuckerman talks about \"slacktivism\", claiming that the Internet has devalued certain currencies of activism. Citizens may \"like\" an activist group on Facebook, visit a website, or comment on a blog, but fail to engage in political activism beyond the Internet, such as volunteering or canvassing. This critique has been criticized as Western-centric, however, because it discounts the impact this can have in authoritarian or repressive contexts. Journalist Courtney C. Radsch argued that even this low level of engagement was an important form of activism for Arab youth because it is a form of free speech, and can spark mainstream media coverage.\n\nScholars are divided as to whether the Internet will increase or decrease political participation, including online activism. Those who suggest political participation will increase believe the Internet can be used to recruit and communicate with more users, and offers lower-costs modes of participation for those who lack the time or motivation to engage otherwise. Those concerned that the Internet will decrease activism argue that the Internet occupies free time that can no longer be spent getting involved in activist groups, or that Internet activism will replace more substantial, effortful forms of in-person activism.\n\nAnother criticism is clicktivism. According to techopedia, clicktivism is a controversial form of digital activism. Proponents believe that applying advertising principles such as A/B testing increases the impact of a message by leveraging the Internet to further its reach. Opponents believe that clicktivism reduces activism to a mere mouse-click, yielding numbers with little or no real engagement or commitment to the cause.\n\nMicah M. White argues, \"Political engagement becomes a matter of clicking a few links. In promoting the illusion that surfing the web can change the world, clicktivism is to activism as McDonalds is to a slow-cooked meal. It may look like food, but the life-giving nutrients are long gone.\" He argues that political engagement becomes a matter of clicking a few links and neglects the vital, immeasurable inner-events and personal epiphanies that great social ruptures are actually made of. It reduces activism to a mere mouse click. Micah M. White goes on to argue that \"... clicktivism reinforces the fear of standing out from the crowd and taking a strong position. It discourages calling for drastic action. And as such, clicktivism will never breed social revolution. To think that it will is a fallacy. One that is dawning on us\".\n\nIn \"Net Delusion\", author Evgeny Morozov argues against cyberutopianism. He describes how the Internet is successfully used against activists and for the sake of state repression.\n\n"}
{"id": "683128", "url": "https://en.wikipedia.org/wiki?curid=683128", "title": "Iron Ring", "text": "Iron Ring\n\nThe Iron Ring is a ring worn by many Canadian-trained engineers, as a symbol and reminder of the obligations and ethics associated with their profession. From a concept originated in 1922, the ring is presented to graduates in a closed ceremony known as The Ritual of the Calling of an Engineer, developed with the assistance of English poet Rudyard Kipling.\n\nThe ring symbolizes the pride which engineers have in their profession, while simultaneously reminding them of their humility. The ring serves as a reminder to the engineer and others of the engineer's obligation to live by a high standard of professional conduct. It is not a symbol of qualification as an engineer – this is determined by the provincial and territorial licensing bodies.\n\nThe Iron Ring is made from either iron or stainless steel. The first ceremony awarding the ring was held in 1925, under the supervision of Herbert E. T. Haultain, professor of mining engineering at the University of Toronto.\n\nThe rings are given in ceremonies held at individual universities, each assigned one of 26 camps of the Corporation of the Seven Wardens. Because iron deteriorates turning the finger black and making the ring fit more loosely, all camps except Toronto have stopped conferring rings made of iron and have switched to stainless steel rings. At the Toronto camp, the individual ceremonies held at the University of Toronto, University of Ottawa, Ryerson University, York University and the University of Ontario Institute of Technology, continue to provide recipients with a choice of rings made of iron or stainless steel.\n\nMany incorrectly believe that the rings are made from the steel of a beam from the first Quebec Bridge, which collapsed during construction in 1907. Seventy-five construction workers died in the collapse which was attributed to poor planning and design by the overseeing engineers. Rudyard Kipling, who wrote the ritual obligation, indicated that the Ring as an allegory in itself be rough, not smoothed, and hammered, and as a ring have no beginning nor end. There is no evidence that there is any particular history in the source of \"Cold Iron\" (from the Calling of the Engineer ceremony) for the Ring, nor any intention that there should have been. Remnants of the Quebec Bridge legend still exist in Canada.\n\nThe Iron Ring is worn on the little finger (\"pinky\") of the working (dominant) hand. There, the facets act as a sharp reminder of one's obligation while the engineer works, because it could drag on the writing surface while the engineer is drawing or writing. This is particularly true of recently obligated engineers, whose rings bear sharp, unworn, facets. Protocol dictates that the rings should be returned by retired engineers or by the families of deceased engineers. Some camps offer previously obligated or \"experienced\" rings, but they are now rare due to medical and practical complications.\n\nThe Ring itself is small and understated, designed as a constant reminder, rather than a piece of jewelry. The Rings were originally hammered manually with a rough outer surface. The modern machined ring design is unique, a reminder of the manual process. Twelve half-circle facets are carved into the top and bottom of the outer surface, with the two sets of facets offset rotationally by fifteen degrees.\n\nThe Ritual of the Calling of an Engineer is the ceremony where Iron Rings are given to graduating engineers who choose to obligate themselves to the highest professionalism and humility of their profession. It is a symbol that reflects the moral, ethical and professional commitment made by the engineer who wears the ring. The ceremonies are private affairs with no publicity. Invitations to attend are extended to local engineering alumni and professional engineers by those who are scheduled to participate. For some schools, the invitation to witness the ceremony is open to anyone in the engineering profession, and non-obligated engineers may not participate in the ritual. Some graduating engineers choose to receive a ring passed on from a relative or mentor, giving the ceremony a personal touch.\n\nBased upon the success of the Iron Ring in Canada, a similar program was created in the United States, where the Order of the Engineer was founded in 1970. The organization conducts similar ring ceremonies at a number of U.S. colleges, in which the recipient signs an \"Obligation of the Engineer\" and receives a stainless steel Engineer's Ring (which, unlike the Canadian Iron Ring, can be smooth and not faceted). The first such ceremony occurred on June 4, 1970, at the Cleveland State University under the supervision of Lloyd Chancy.\n\n\n\n"}
{"id": "27774511", "url": "https://en.wikipedia.org/wiki?curid=27774511", "title": "Lakes District Technocity", "text": "Lakes District Technocity\n\nThe Lakes District Technocity(established in 2004) is a science park located on the campus of Süleyman Demirel University. The technocity is a full member of International Association of Science Parks. The science park occupies 112000 squaremeters area.\n\nThe activities of the technocity include Energy and Renewable Energy, Internet Technology and Services / E-Business, Plasma Technology, and Environment Technologies. \n\nAs of 2010, 57% of firms on the tecnocity involved in the area of computer software industry.\n\nThe technocity made its first export(plasma sensors to Saudi Arabia) in 2008.\n\nThe following entities are shareholders of the park:\n\n"}
{"id": "54812230", "url": "https://en.wikipedia.org/wiki?curid=54812230", "title": "Laser 50", "text": "Laser 50\n\nThe Laser 50 is an educational portable computer that ran the BASIC programming language released in 1984.\n\nThe Laser 50 used a Zilog Z80 central processing unit running at 3.5 MHz, 2 kB to 18 kB of RAM, a 12 kB ROM, and a 80x7 dots LCD screen.\n"}
{"id": "9249236", "url": "https://en.wikipedia.org/wiki?curid=9249236", "title": "List of WiMAX networks", "text": "List of WiMAX networks\n\nA list of WiMAX networks worldwide.\n\n\n"}
{"id": "13706125", "url": "https://en.wikipedia.org/wiki?curid=13706125", "title": "List of emerging technologies", "text": "List of emerging technologies\n\nEmerging technologies are those technical innovations which represent progressive developments within a field for competitive advantage.\n\n\n"}
{"id": "23849259", "url": "https://en.wikipedia.org/wiki?curid=23849259", "title": "List of first-person shooter engines", "text": "List of first-person shooter engines\n\nThis is a sortable list of first-person shooter engines.\n\nSome features may be integrated into engines. For instance for trees and foliage a special \"engine\" is available, SpeedTree, that does just that (or could be integrated into general engines). The Euphoria character's 3D animating engine can be used independently but is integrated in the Rockstar Advanced Game Engine and the game \"Grand Theft Auto IV\".\n\n"}
{"id": "57698112", "url": "https://en.wikipedia.org/wiki?curid=57698112", "title": "List of obsolete technology", "text": "List of obsolete technology\n\nThis is a list of obsolete technology which includes newer technologies that replaced the older ones. Many technologies that have newer alternatives have not been completely replaced.\n\nOlder technologies substantially co-existing with newer technologies include:\n\n"}
{"id": "32727729", "url": "https://en.wikipedia.org/wiki?curid=32727729", "title": "List of online digital musical document libraries", "text": "List of online digital musical document libraries\n\nThis is a list of online digital musical document libraries. Each source listed below offers access to collections of digitized music documents (typically originating from printed or manuscript musical sources), and containing music notation of some kind, stored as an image file. \n\n"}
{"id": "40277431", "url": "https://en.wikipedia.org/wiki?curid=40277431", "title": "Ministry of Science and Technology (Bangladesh)", "text": "Ministry of Science and Technology (Bangladesh)\n\nThe Ministry of Science and Technology (; \"Bijñāna ō prayukti Montronaloya\") (abbreviated as MOST) is a ministry of the government of Bangladesh which coordinates science and technology activities in Bangladesh.\n\nThe principle agenda of the Ministry of Science and Technology is socio-economic development of Bangladesh through research, development, extension and successful utilization of Science and Technology.\n\n\n"}
{"id": "43099588", "url": "https://en.wikipedia.org/wiki?curid=43099588", "title": "Mobile workspace", "text": "Mobile workspace\n\nA mobile workspace is a user’s portable working environment that gives them access to the applications, files and services they need to do their job no matter where they are.\n\nMobile workspace technology describes a set of software and services that deliver corporate apps, files and services to a user on any device and over any network. This technology was designed for business users that require access to all of their content on both corporate and personally-owned devices, including PCs, smartphones and tablets. Mobile workspace technologies are formed by bringing together a set of software and services including desktop virtualization, application virtualization, enterprise mobility management, file sharing, virtual private networks and more (see the full list below).\n\nAccording to recent research, combined shipments of devices including PCs, tablets, ultramobiles and mobile phones are projected to reach 2.5 billion units in 2014, a 7.6 percent increase from 2013. This trend, which is expected to continue to grow, is being driven by users who utilize more than one device. In fact, today the average user has 3+ different devices that they use for work purposes on a daily basis. These mobile devices entering the enterprise has led to over 60% of information workers working outside of a traditional office. While the shift to mobility seems to be growing, it is causing problems for both the end user and IT department. End users don’t feel they are equipped to work outside of the office and IT is forced to manage the security risks presented by data and applications leaving the corporate network.\n\nTo address these challenges, organizations are looking to mobile workspace technology that can provide users access to their corporate applications, files and services while maintaining security for their IT department. Mark Bowker, senior analyst, Enterprise Strategy Group said, \"Mobile workspaces are playing a key role in addressing the new organizational imperative around secure mobility\".\n\nApplications \nMicrosoft (particularly Microsoft Office) have been the dominant office software suite for enterprise along with numerous other Windows applications. Newer to the market is the growth of Software-as-a-Service and web-based applications such as Salesforce.com, LinkedIn and Evernote. Additionally, mobile applications have seen growth of 200% from 2011 to 2012, a trend that is expected to continue into 2013. A mobile workspace includes access to all types of applications (Windows, mobile, web, SaaS, and HTML5-based) from the user device.\n\nData\nFiles access is important for business productivity as well as business collaboration. A user’s mobile workspace must give a user access to all of their corporate data and allow for this data to be accessible on multiple devices. This requirement for file sync and sharing has led to the growth of many online sharing options including Citrix Sharefile, Dropbox, Google Drive, Box, and iCloud.\n\nCollaboration \nDue to factors like globalization, telecommuting and employees working outside of traditional offices, collaboration tools are becoming increasingly important. Collaboration tools include electronic communication, online web-conferencing, file sharing and social collaboration software.\n\nNetwork access\nPeople now work from all locations – the office, a coffee shop, the airport, their homes – yet still require access to their content. When outside the office, people still need access to resources behind the firewall generally requiring VPN access. A mobile workspace allows secure access to apps, data and services no matter where the user is or what type of network they are using (LAN, WAN, 3G/4G, etc.).\n\nCloud\nAs cloud computing becomes more popular, IT organizations are looking to the cloud to host more services than ever before. IT departments now can choose where to host their apps, data, desktops, etc. in the most appropriate location whether that’s on-premises in a datacenter or a private, public or hybrid cloud. Users need access to these resources from their mobile workspace.\n\nAny device\nResearch suggests that mobile devices are set to outnumber the total number people on earth by 2014. As device types and manufacturers continue to grow, users still need to access their mobile workspace irrespective of device type, operating system or manufacturer.\n\n"}
{"id": "42159582", "url": "https://en.wikipedia.org/wiki?curid=42159582", "title": "Mobiles for development", "text": "Mobiles for development\n\nMobiles for Development (M4D), a more specific iteration of Information and Communication Technologies for Development (ICT4D), refers to the use of mobile technologies in global development strategies. Focusing on the fields of international and socioeconomic development and human rights, M4D relies on the theory that increased access to mobile devices acts as an integral cornerstone in the promotion of overall societal development.\n\nOnce viewed as an item of luxury and privilege, mobile phones and devices have become a near necessity throughout the developed and developing world alike. According to a 2007 United Nations study, over two thirds of the world’s mobile phones are owned and utilized within developing countries. With less-developed wired infrastructure and the high cost associated with its modernization and implementation, the adoption of cellular technologies can be attributed to a necessary leapfrogging of traditional telephony and communication technologies. In addition, the unsound and undependable electrical infrastructure of many developing countries does not cater well to mass hardwired ICT adoption. The portability, battery power, and flexibility of mobile technologies is well suited to the common pursuits and lifestyles of those residing in the developing world.\n\nThis mass adoption of ICTs and mobile phones as well the increased quality and expanse of signal coverage within many developing countries has led to increased academic, socioeconomic, and political attention as the various impacts of the M4D movement continue to expand. In addition to the predictable developmental outcomes of mobile adoption including increased economic agency, unforeseen progress has been experienced in the forms of individual empowerment, female agency, as well as familial and community growth.\n\nThe opportunities for effective mass mobilization and aggregation of information and data offered by developmental movements utilizing cellular telephones and other mobile devices such as tablets have been widely featured in the mass media and academia. Literature on this matter is being steadily produced as developing countries continue to adopt mobile technologies at a remarkable rate.\n\nRecent developments in mobile communication and computation technologies have led to the expansion of mobile phone, smartphone, tablet computer, and netbook ownership. Typically marketed to the developed world as supplementary to standard laptops and desktop computers, these electronic products often offer lower price points to the consumer. This lower price point caters well to developing countries and their rapidly evolving markets for ICT expansion and adoption. These mobile devices come equipped with basic mobile communications hard and software such as WiFi and 3G services which allow users to connect to the Internet via mobile and wireless networks without having to secure a landline or an expensive broadband connection via DSL, cable Internet or fiber optics. This leapfrogging movement towards the acceptance and implementation of mobile technologies made the Internet and modern digital telecommunications more accessible to people, particularly those in emerging markets and developing countries.\n\nAccording to International Telecommunication Union, mobile ICTs have emerged as the primary form of technology that will offer a bridge within the digital divide. Data collected by the ITU shows the trajectory of mobile technologies as their adoption out-paces and even replaces the adoption of desktop computers and standard laptops.\n\nThe International Telecommunication Union estimates that as of 2013, approximately 6.8 billion mobile-cellular subscriptions are held worldwide, 5.2 billion of which are held in developing countries. These numbers stand in stark contrast to the penetration rate of fixed-telephone subscriptions which stand at approximately 1.2 billion worldwide, a small margin over half of which belong to the developing world.\n\nThe following table displays key ICT indicators for developed and developing countries as well as world totals from 2010 to 2013:\n\nThe trends displayed in the table above are further supported by the successful sales reports of technology companies selling mobile technologies in emerging markets within developing countries. Some multinational computer manufacturers like Acer and Lenovo are marketing more affordable netbooks to emerging markets such as those in China, Indonesia and India.\n\nOriginally a concept used in reference to economic growth theories and industrialization, leapfrogging has more recently been used in the context of sustainable development for developing countries within world development theories. Technological leapfrogging refers to the acceleration of development through the skipping of low-grade, less efficient, and more costly technologies and industries in favour of the direct adoption of more effective and advanced technologies.\n\nIn the case of M4D, the trajectory of the rapid mass adoption of mobile technologies can be attributed to the \"mobile leapfrog effect\", whereby many developing countries have been seen to bypass traditional routes of wired telephony and broadband infrastructure and development, opting instead for the immediate appropriation of wireless cellular and broadband technology. This mobile leapfrogging can be attributed to the lengthy process and high cost of wired infrastructure implementation. Additionally, as seen in mobile adoption in the Arab States, wired infrastructure that predates mobile technology is often old, outdated, and incapable of data transmission, \"the basic requirement for implementing Digital Subscriber Line (DSL) services\".\n\nMobile ICT platforms and their wide adoption in developing countries serves as an ideal example of leapfrog technology and current practices of leapfrogging within the sustainable development of developing countries. By enabling developing countries to “leapfrog” over legacy technologies of the wired telephone and Internet service of the 20th century and embrace the mobile technologies of the 21st century, opportunities to bridge the Digital divide have been argued to become more prevalent.\n\nThe term ‘mobile hack’ refers to the use-based practices of mobile technology that goes beyond the use intended by the creators of the device. Often used in order to circumvent the high cost of mobile ownership and use, mobile hacks have become common practice in many developing countries. Mobile hacks have also proven to be widespread in the adoption of mobile technologies in developing countries. Device sharing, the ‘Missed Call’ technique, and the transferring of mobile credits has allowed for individuals who were previously isolated from digital communication the opportunity to economically engage in the networked community.\n\nDevice sharing, most commonly the sharing of mobile telephones, refers to the practice of sharing a mobile device between a number of individuals either within a family or a community.\n\nThe 'Missed call' technique refers to the establishment of a code that works around connection charges by utilizing a specified number of rings prior to cutting a call. This method avoids call charge and uses discreet codes to convey messages. This technique goes against the profit models of mobile service providers and allows for individuals to communicate messages in a cost-effective manner.\n\nThrough the transfer of mobile credits, mobile device owners are able to use credits of an amount specified by their service provider as a means of monetary transaction. By sending credits to another's mobile device, an exchange of cash can be made in situations that it is necessitated.\n\nAccording to the ITU's report \"Measuring the Information Society,\" mobile phones and other mobile ICT devices are seen to be replacing standard laptops and desktop computers as the main platform for Internet and ICT access and use. This increased access to and use of mobile phones offers individuals a handheld communication platform that can assist in expanding the level of citizen agency in the development of local and international social, economic, and political endeavours. It also offers individuals the opportunity to form social, economic, and political communities, regardless of geographic location, and provides an at-hand device that allows individuals to fight against human rights abuses from the ground. Organizations such as Digital Democracy (Dd) and the Democratic Voice of Burma offers users the ability to report, compile, and disseminate news and information about human rights violations in order to effectuate global attention and action.\n\nIn the establishment and fostering of mobile citizens within developing countries, a \"bottom of the pyramid\" corporate approach has been encouraged in order to \"convert poverty into a business opportunity that benefits everyone.\"\n\n\nAccording to a report by InfoDev, “[h]ealth conditions in rural areas are generally poorer, and access to information, services, and supplies is most limited.” With the rapid worldwide adoption of mobile technology, a range of health-related areas such as the improvement of public health information dissemination, the facilitation of remote consultation, diagnosis, and treatment, the sharing of a patient’s health information between health professional, and the monitoring and increased efficiency of public health systems have adopted and benefited from mobile based practices.\n\nStudies have suggested that the use of mobile capabilities such as text message reminders regarding dosage information and the increased communication between health professionals has allowed for increased effectiveness in treatment and control of disease. Cases of mobile technology being effectively piloted and utilized in the area of public health exhibits the promise of M4D programs and practices in the spheres of health prevention and medical care for the world’s developing nations.\n\nThe following lists a sampling of programs utilizing M4D strategies for the improvement of public health in developing countries around the world:\n\n\nThe ability to mobilize data aggregation to the mobile-carrying public offers NGOs a valuable resource for their efforts for social, political, economic, and environmental justice. According to a study published by the Vodafone Group Foundation and the UN Foundation Partnership, of a sample of over 500 NGOs, “eighty-six per cent used mobiles, with 99 per cent characterizing its utility positively, with one-quarter of those citing it as a ‘revolutionary’ technology and another one-third calling it indispensable for their work.”. The benefits perceived by M4D initiatives include their \"ability to mobilize and aggregate information and data more effectively and to a wider audience\" as well as their time-saving qualities in the fields of social justice, environmental conservation, global health and humanitarian assistance.\n\nThe following lists organizations engaging in M4D strategies and programming:\n\n\nThe potential for the expansion and replication of M4D projects has been recognized as vital to the overall success of this development practice. The sharing and exchange of information and technical advancements can allow for easier and less costly adoption in other developing countries however, many of the organizations creating and implementing M4D projects act within 'innovation silos'. This siloing of information threatens to create and solidify boundaries between organizations and mobile development projects.\n\nThe environmental implications of increased mobile usage can be seen in the form of the large electronic waste dumps found in many of the developing countries meant to benefit from M4D programs and policies.\n\nIn addition, it has been argued that the introduction of mobile information and communication technologies could result in the proliferation of the Matthew effect, whereby the \"rich get richer.\" In the case of mobile adoption, the inequalities of wealth considered encompass both economic and knowledge-based wealth. Despite the benefits attributed to the adoption and use of mobile ICTs for development purposes, the information and communication resources in question have been initially created and adopted within already developed countries.\n\nAlso problematic is the potential for mobile hardware and software development in developing countries to become a purely for-profit endeavour. As stated by a UN Foundation-Vodafone Group Foundation Partnership publication, \"the potential for scaling up 'mobile for good' initiatives may come with identifying commercial incentives.\" The free and open source software applications that have been developed and implemented by various NGOs in developing countries as well as the ad-hoc communal use of mobile devices could be threatened by the prospective monetization of the mass markets available in the developing world.\n\n"}
{"id": "19375335", "url": "https://en.wikipedia.org/wiki?curid=19375335", "title": "Near-field magnetic induction communication", "text": "Near-field magnetic induction communication\n\nA Near-Field Magnetic Induction communication system is a short range wireless physical layer that communicates by coupling a tight, low-power, non-propagating magnetic field between devices. The concept is for a transmitter coil in one device to modulate a magnetic field which is measured by means of a receiver coil in another device.\n\nNear-field magnetic induction (NFMI) communication systems differ from other wireless communications in that most conventional wireless RF systems use an antenna to generate, transmit, and propagate an electromagnetic wave. In these types of systems all of the transmission energy is designed to radiate into free space. This type of transmission is referred to as \"far-field.\"\n\nAccording to Maxwell's equation for a radiating wire, the power density of far-field transmissions attenuates or rolls off at a rate proportional to the inverse of the range to the second power (1/r) or −20 dB per decade. This slow attenuation over distance allows far-field transmissions to communicate effectively over a long range. The properties that make long range communication possible are a disadvantage for short range communication systems.\n\nThe NFMI system uses a short range (less than 2 meters).\n\nThe standard modulation schemes used in typical RF communications (amplitude modulation, phase modulation, and frequency modulation) can be used in near-field magnetic induction system\n\nNFMI systems are designed to contain transmission energy within the localized magnetic field. This magnetic field energy resonates around the communication system, but does not radiate into free space. This type of transmission is referred to as \"near-field.\" The power density of near-field transmissions is extremely restrictive and attenuates or rolls off at a rate proportional to the inverse of the range to the sixth power (1/r) or −60 dB per decade.\n\nIn current commercial implementations of near-field communications, the most commonly used carrier frequency is 13.56 MHz and has a wavelength (λ) of 22.1 meters. The crossover point between near-field and far-field occurs at approximately λ/2π. At this frequency the crossover occurs at 3.52 meters, at which point the propagating energy from the NFMI system conforms to the same propagation rules as any far-field system; rolling off at −20 dB per decade. At this distance the propagated energy levels are −40 dB to −60 dB (10,000 to 1,000,000 times) lower than an equivalent intentional far-field system.\n\nNear-field magnetic induction technology has been in use nearly exclusively by the company FreeLinc. Using NFMI to create a secure wireless communication between two-way radio accessories. This is done by creating a magnetic communication \"bubble\" around headsets, speaker-microphones and radios. This magnetic bubble has a radius of approximately 1.5 meters, is immune from radio frequency (RF) interference and virtually secure from eavesdropping. An eavesdropper would have to be standing next to the radio, within the magnetic bubble, to intercept wireless transmissions to and from a microphone or headset.\n\n\nhttp://www.freelinc.com/technology/\n\nhttps://www.nxp.com/products/wireless-connectivity/miglo:NFMI-RADIO-SOLUTIONS\n"}
{"id": "18134915", "url": "https://en.wikipedia.org/wiki?curid=18134915", "title": "Parameter Value Language", "text": "Parameter Value Language\n\nIn computer programming, Parameter Value Language (PVL) is a markup language similar to XML. It is commonly employed for entries in the Planetary Database System used by NASA to store mission data, among other uses.\n\nThere are at least two \"dialects\" – \"USGS Isis Cube Label\" and \"NASA PDS 3 Label\". \n\n"}
{"id": "1034453", "url": "https://en.wikipedia.org/wiki?curid=1034453", "title": "Podcast", "text": "Podcast\n\nA podcast or generically netcast, is an episodic series of digital audio or video files which a user can download in order to listen to. It is often available for subscription, so that new episodes are automatically downloaded via web syndication to the user's own local computer, mobile application, or portable media player.\n\nThe word was originally suggested by Ben Hammersley as a portmanteau of \"iPod\" (a brand of media player) and \"broadcast\".\n\nThe files distributed are in audio format, but may sometimes include other file formats such as PDF or EPUB. Videos which are shared following a podcast model are sometimes called video podcasts or vodcasts.\n\nThe generator of a podcast maintains a central list of the files on a server as a web feed that can be accessed through the Internet. The listener or viewer uses special client application software on a computer or media player, known as a podcatcher, which accesses this web feed, checks it for updates, and downloads any new files in the series. This process can be automated to download new files automatically, which may seem to users as though new episodes are broadcast or \"pushed\" to them. Files are stored locally on the user's device, ready for offline use. There are many different mobile applications available for people to use to subscribe and to listen to podcasts. Many of these applications allow users to download podcasts or to stream them on demand as an alternative to downloading. Many podcast players (apps as well as dedicated devices) allow listeners to skip around the podcast and control the playback speed.\n\nSome have labeled podcasting as a converged medium bringing together audio, the web, and portable media players, as well as a disruptive technology that has caused some individuals in the radio business to reconsider established practices and preconceptions about audiences, consumption, production, and distribution.\nPodcasts are usually free of charge to listeners and can often be created for little to no cost, which sets them apart from the traditional model of \"gate-kept\" media and production tools. Podcast creators can monetize their podcasts by allowing companies to purchase ad time, as well as via sites such as Patreon, which provides special extras and content to listeners for a fee. Podcasting is very much a horizontal media form – producers are consumers, consumers may become producers, and both can engage in conversations with each other.\n\n\"Podcast\" is a portmanteau word, formed by combining \"iPod\" and \"broadcast\". The term \"podcasting\" as a name for the nascent technology was first suggested by The Guardian columnist and BBC journalist Ben Hammersley, who invented it in early February 2004 while \"padding out\" an article for The Guardian newspaper. Despite the etymology, the content can be accessed using any computer or similar device that can play media files. Use of the term \"podcast\" predated Apple's addition of formal support for podcasting to the iPod, or its iTunes software.\n\nOther names for podcasting include \"net cast\", intended as a vendor-neutral term without the loose reference to the Apple iPod. This name is used by shows from the TWiT.tv network. Some sources have also suggested the backronym \"portable on demand\" or \"POD\", for similar reasons.\n\nFormer MTV video jockey Adam Curry, in collaboration with Dave Winer – co-author of the RSS specification – is credited with coming up with the idea to automate the delivery and syncing of textual content to portable audio players.\n\nPodcasting, once an obscure method of spreading information, has become a recognized medium for distributing audio content, whether for corporate or personal use. Podcasts are similar to radio programs, but they are audio files. Listeners can play them at their convenience, using devices that have become more common than portable broadcast receivers.\n\nThe first application to make this process feasible was iPodderX, developed by August Trometer and Ray Slakinski. By 2007, audio podcasts were doing what was historically accomplished via radio broadcasts, which had been the source of radio talk shows and news programs since the 1930s. This shift occurred as a result of the evolution of internet capabilities along with increased consumer access to cheaper hardware and software for audio recording and editing.\n\nIn October 2003, Matt Schichter launched his weekly chat show \"The BackStage Pass\". B.B. King, Third Eye Blind, Gavin DeGraw, The Beach Boys, and Jason Mraz were notable guests the first season. The hour long radio show was recorded live, transcoded to 16kbit/s audio for dial-up online streaming. Despite a lack of a commonly accepted identifying name for the medium at the time of its creation, \"The Backstage Pass\" which became known as \"Matt Schichter Interviews\" is commonly believed to be the first podcast to be published online.\n\nIn August 2004, Adam Curry launched his show \"Daily Source Code\". It was a show focused on chronicling his everyday life, delivering news, and discussions about the development of podcasting, as well as promoting new and emerging podcasts. Curry published it in an attempt to gain traction in the development of what would come to be known as podcasting and as a means of testing the software outside of a lab setting. The name \"Daily Source Code\" was chosen in the hope that it would attract an audience with an interest in technology.\n\n\"Daily Source Code\" started at a grassroots level of production and was initially directed at podcast developers. As its audience became interested in the format, these developers were inspired to create and produce their own projects and, as a result, they improved the code used to create podcasts. As more people learned how easy it was to produce podcasts, a community of pioneer podcasters quickly appeared.\n\nIn June 2005, Apple released iTunes 4.9 which added formal support for podcasts, thus negating the need to use a separate program in order to download and transfer them to a mobile device. While this made access to podcasts more convenient and widespread, it also effectively ended advancement of podcatchers by independent developers. Additionally, Apple issued cease and desist orders to many podcast application developers and service providers for using the term \"iPod\" or \"Pod\" in their products' names.\n\nWithin a year, many podcasts from public radio networks like the BBC, CBC Radio One, NPR, and Public Radio International placed many of their radio shows on the iTunes platform. In addition, major local radio stations like WNYC in New York City and WHYY-FM radio in Philadelphia, KCRW in Los Angeles placed their programs on their websites and later on the iTunes platform.\n\nConcurrently, CNET, \"This Week in Tech\", and later Bloomberg Radio, the \"Financial Times\", and other for-profit companies provided podcast content, some using podcasting as their only distribution system.\n\nBetween February 10 and 25 March 2005, Shae Spencer Management, LLC of Fairport, New York filed a trademark application to register the term \"podcast\" for an \"online prerecorded radio program over the internet\". On September 9, 2005, the United States Patent and Trademark Office (USPTO) rejected the application, citing Wikipedia's podcast entry as describing the history of the term. The company amended their application in March 2006, but the USPTO rejected the amended application as not sufficiently differentiated from the original. In November 2006, the application was marked as abandoned.\n\nAs of September 20, 2005, known trademarks that attempted to capitalize on podcast included: ePodcast, GodCast, GuidePod, MyPod, Pod-Casting, Podango, PodCabin, Podcast, Podcast Realty, Podcaster, PodcastPeople, Podgram PodKitchen, PodShop, and Podvertiser.\n\nBy February 2007, there had been 24 attempts to register trademarks containing the word \"PODCAST\" in the United States, but only \"PODCAST READY\" from \"Podcast Ready, Inc.\" was approved.\n\nOn September 26, 2004, it was reported that Apple Inc. had started to crack down on businesses using the string \"POD\", in product and company names. Apple sent a cease and desist letter that week to Podcast Ready, Inc., which markets an application known as \"myPodder\". Lawyers for Apple contended that the term \"pod\" has been used by the public to refer to Apple's music player so extensively that it falls under Apple's trademark cover. Such activity was speculated to be part of a bigger campaign for Apple to expand the scope of its existing iPod trademark, which included trademarking \"IPOD\", \"IPODCAST\", and \"POD\". On November 16, 2006, the Apple Trademark Department stated that \"Apple does not object to third-party usage of the generic term 'podcast' to accurately refer to podcasting services\" and that \"Apple does not license the term\". However, no statement was made as to whether or not Apple believed they held rights to it.\n\nPersonal Audio, a company referred to as a \"patent troll\" by the Electronic Frontier Foundation, filed a patent on podcasting in 2009 for a claimed invention in 1996. In February 2013, Personal Audio started suing high-profile podcasters for royalties, including \"The Adam Carolla Show\" and the \"HowStuffWorks\" podcast. US Congressman Peter DeFazio's previously proposed \"SHIELD Act\" intended to curb patent trolls.\n\nIn October 2013, the EFF filed a petition with the US Trademark Office to invalidate the Personal Audio patent.\n\nOn August 18, 2014, the Electronic Frontier Foundation announced that Adam Carolla had settled with Personal Audio.\n\nOn April 10, 2015, the U.S. Patent and Trademark Office invalidated five provisions of Personal Audio's podcasting patent.\n\nAn enhanced podcast can display images synchronized with audio. These can contain chapter markers, hyperlinks, and artwork, all of which is synced to a specific program or device. When an enhanced podcast is played within its specific program or device, all the appropriate information should be displayed at the same time and in the same window, making it easier to display materials.\n\nA podcast novel (also known as a serialized audiobook or podcast audiobook) is a literary format that combines the concepts of a podcast and an audiobook. Like a traditional novel, a podcast novel is a work of long literary fiction; however, this form of the novel is recorded into episodes that are delivered online over a period of time and in the end available as a complete work for download. The episodes may be delivered automatically via RSS, through a website, blog, or another syndication method. These files are either listened to directly on a user's computer or loaded onto a portable media device to be listened to later.\n\nThe types of novels that are podcasted vary from new works from new authors that have never been printed, to well-established authors that have been around for years, to classic works of literature that have been in print for over a century. In the same style as an audiobook, podcast novels may be elaborately narrated with separate voice actors for each character and sound effects, similar to a radio play. Other podcast novels have a single narrator reading the text of the story with little or no sound effects.\n\nPodcast novels are distributed over the Internet, commonly on a weblog. Podcast novels are released in episodes on a regular schedule (e.g., once a week) or irregularly as each episode is released when completed. They can either be downloaded manually from a website or blog or be delivered automatically via RSS or another method of syndication. Ultimately, a serialized podcast novel becomes a completed audiobook.\n\nSome podcast novelists give away a free podcast version of their book as a form of promotion. Some such novelists have even secured publishing contracts to have their novels printed. Podcast novelists have commented that podcasting their novels lets them build audiences even if they cannot get a publisher to buy their books. These audiences then make it easier to secure a printing deal with a publisher at a later date. These podcast novelists also claim the exposure that releasing a free podcast gains them makes up for the fact that they are giving away their work for free.\n\nA video podcast (sometimes shortened to \"vodcast\") includes video clips. Web television series are often distributed as video podcasts.\n\n\"Dead End Days\" (2003–2004) is commonly believed to be the first video podcast. That serialized dark comedy about zombies was broadcast from 31 October 2003 through 2004.\n\nSince the spread of the Internet and the use of Internet broadband connection TCP, which helps to identify various applications, a faster connection to the Internet has been created and a wide amount of communication has been created. Video podcasts have become extremely popular online and are often presented as short video clips, usually excerpts of a longer recording. Video clips are being used on pre-established websites, and increasing numbers of websites are being created solely for the purpose of hosting video clips and podcasts. Video podcasts are being streamed on intranets and extranets, and private and public networks, and are taking communication through the Internet to new levels.\n\nMost video clips are now submitted and produced by individuals. Video podcasts are also being used for web television, commonly referred to as Web TV, a rapidly growing genre of digital entertainment that uses various forms of new media to deliver to an audience both reruns of shows or series and content created or delivered originally online via broadband and mobile networks, web television shows, or web series. Examples include Amazon Video, Hulu, and Netflix. Other types of video podcasts used for web television may be short-form, anywhere from 2–9 minutes per episode, typically used for advertising, video blogs, amateur filming, journalism, and convergence with traditional media.\n\nVideo podcasting is also helping build businesses, especially in the sales and marketing sectors. Through video podcasts, businesses both large and small can advertise their wares and services in a modern, cost-effective way. In the past, big businesses had better access to expensive studios where sophisticated advertisements were produced, but now even the smallest businesses can create high-quality media with just a camera, editing software, and the Internet.\n\nIn a two-year study, 2012-2013, conducted by a South African university a question was raised; over the years of podcast development, is podcasting socially inclusive. The results of this study concluded with minor quarks, podcasting is socially inclusive.\n\nAn oggcast is a podcast recorded and distributed exclusively in the Vorbis audio codec with the Ogg container format, and/or other similarly free codecs/formats. For example, a podcast distributed both in the non-free MP3 format and the free Ogg format would not technically meet the definition of an oggcast. In contrast, a podcast distributed in both the Vorbis and Speex codecs would meet the strict definition of an oggcast. The term oggcast is a combination of the word \"ogg\" from the term Ogg Vorbis, and the syllable \"cast\", from \"broadcast\".\n\nThe term was coined for the fifth season of the \"Gnu World Order\" by Klaatu in 2010, when the show declared itself \"the world's first oggcast\". At the time, the show was one of the few that released only in free formats, with no MP3 feed as an option. This gave way to other shows using the term, with hosts gathering in the IRC channel on the Freenode network to compare notes.\n\n\"The Linux Link Tech Show\", one of the longer running Linux podcasts still in production, has a program in the Ogg Vorbis format in its archives from January 7, 2004.\n\nOggcasters tend to be broadcasters who prefer not to use audio and video codecs that have patent and/or licensing restrictions, such as the MP3 codec.\n\nRecording and distributing podcasts in the Ogg Vorbis audio format has advantages. Mozilla Firefox and Google Chrome web browsers both support playing Vorbis files directly in the browser without requiring plugins. Vorbis may produce better audio quality with a smaller file size than alternative codecs such as AAC or MP3. However, this has not been proven conclusively. Ogg Vorbis is not bound by patents and is considered \"free software\" in the sense that no corporate entity owns the rights to the format. Some people feel that this is a safer container for their multimedia content for this reason. However, oggcasters can generally not reach as wide of an audience as more traditional podcasters. This is mainly due to the lack of native Ogg Vorbis support in Microsoft's Internet Explorer and Apple's Safari web browser, and the lack of Ogg Vorbis support in many mobile audio devices.\n\nOggcast Planet maintains a central list of oggcasts.\n\nA political podcast focuses on current events, lasts usually a half hour to an hour, often with a relaxed and conversational tone, and features journalists and politicians and pollsters and writers and others with credentials in the public sphere. Most political podcasts have a host-guest interview format and are broadcast each week based on the news cycle. Political podcasts have blossomed in the past few years in the United States because of the long election cycle. Larger news sites such as the \"Radio Atlantic\" and the \"Spectator\" have started weekly political podcasts in recent years, as well as smaller podcasts such as the Bruderhof's \"Life Together\" podcast and Danny Anderson's \"Sectarian Review\" and Pod Save America.\n\nA podguide is an enhanced audio tour podcast. It is a single audio file where each chapter displays a picture and a number of what to look at a certain stopover. The numbers correspond to the numbers on a map that can be downloaded via the link incorporated into the artwork of the chapters in the podguide. Podguides are in the m4a format and can only be listened to through iTunes or an iPod. It is like a soundseeing tour but with pictures and a map, so users can take the tour themselves.\n\nCommunities use collaborative podcasts to support multiple contributors podcasting through generally simplified processes, and without having to host their own individual feeds. A community podcast can also allow members of the community (related to the podcast topic) to contribute to the podcast in many different ways. This method was first used for a series of podcasts hosted by the Regional Educational Technology Center at Fordham University in 2005. Anders Gronstedt explores how businesses like IBM and EMC use podcasts as an employee training and communication channel.\n\nThe podcast industry is very profitable. Over 50 million people listen to podcasts each month. A small, yet efficient number of listeners are also podcast creators. Creating a podcast is reasonably inexpensive. It requires just a microphone, laptop or other personal computer, and a room with some sound blocking. Podcast creators tend to have a good listener base because of their relationships with the listeners.\n\n\n"}
{"id": "3975826", "url": "https://en.wikipedia.org/wiki?curid=3975826", "title": "Postgenderism", "text": "Postgenderism\n\nPostgenderism is a social, political and cultural movement which arose from the eroding of the cultural, biological, psychological and social role of gender, and an argument for why the erosion of binary gender will be liberatory. \n\nPostgenderists argue that gender is an arbitrary and unnecessary limitation on human potential, and\nforesee the elimination of involuntary biological and psychological gendering in the human species\nas a result of social and cultural evolution and through the application of neurotechnology, biotechnology and assistive reproductive technologies.\n\nAdvocates of postgenderism argue that the presence of gender roles, social stratification, and cogno-physical disparities and differences are generally to the detriment of individuals and society. Given the radical potential for advanced assistive reproductive options, postgenderists believe that sex for reproductive purposes will either become obsolete, or that all post-gendered humans will have the ability, if they so choose, to both carry a pregnancy to term \"and\" 'father' a child, which, postgenderists believe, would have the effect of eliminating the need for definite genders in such a society.\n\nPostgenderism as a cultural phenomenon has roots in feminism, masculism, along with the androgyny, metrosexual/technosexual and transgender movements. However, it has been through the application of transhumanist philosophy that postgenderists have conceived the potential for actual morphological changes to the members of the human species and how future humans in a postgender society will reproduce. In this sense, it is an offshoot of transhumanism, posthumanism, and futurism.\n\nOne of the earliest expressions of postgenderism was Shulamith Firestone's \"The Dialectic of Sex\". It argues, [The] end goal of feminist revolution must be, unlike that of the first feminist movement, not just the elimination of male privilege but of the sex distinction itself: genital differences between human beings would no longer matter culturally. (A reversion to an unobstructed pansexuality Freud's 'polymorphous perversity'—would probably supersede hetero/homo/bi-sexuality.) The reproduction of the species by one sex for the benefit of both would be replaced by (at least the option of) artificial reproduction: children would born to both sexes equally, or independently of. either, however one chooses to look at it; the dependence of the child on the mother (and vice versa) would give way to a greatly shortened dependence on a small group of others in general, and any remaining inferiority to adults in physical strength would be compensated for culturally.\n\nAnother important and influential work in this regard was socialist feminist Donna Haraway's essay, \"A Cyborg Manifesto: Science, Technology, and Socialist-Feminism in the Late Twentieth Century\", in Simians, Cyborgs and Women: The Reinvention of Nature (New York; Routledge, 1991), pp. 149–181. In this work, Haraway is interpreted as arguing that women would only be freed from their biological restraints when their reproductive obligations were dispensed with. This may be viewed as Haraway expressing belief that women will only achieve true liberation once they become postbiological organisms, or postgendered. However, Haraway has publicly stated that their use of the word \"post-gender\" has been grossly misinterpreted.\n\nPostgenderism is also often used by George Dvorsky to describe the diverse social, political, and cultural movement that affirm the voluntary elimination of gender in the human species by applying advanced biotechnology and assisted reproductive technologies. In 2008, Dvorsky wrote with James Hughes that \"dyadic gender roles and sexual dimorphism are generally to the detriment of individuals and society\" and that \"greater biological fluidity and psychological androgyny will allow future persons to explore both masculine and feminine aspects of personality.\"\n\nPostgenderists are not exclusively advocates of androgyny, although most believe that a \"mixing\" of both masculine and feminine traits is desirable—essentially the creation of androgynous individuals who exhibit the best of what males and females have to offer in terms of physical and psychological abilities and proclivities. Just what these traits are exactly is a matter of great debate and conjecture.\n\nPostgenderism is not concerned solely with the physical sex or its assumed traits. It is focused on the idea of eliminating or moving beyond gendered identities. In a traditional gender construct one is either a man or woman, but in postgenderism one is neither a man nor woman nor any other assumed gender role. Thus an individual in society is not reduced to a gender role but is simply an agent of humanity who is to be defined (if at all) by one's actions.\n\nHowever, not all postgenderists are against the existence of gender roles in some form; some only argue for the deemphasization of gender roles. People in this form of postgender world would be able to identify as a gender if they decided to, but identifying as one would not be mandatory, and gender roles would have little bearing on how people actually act or are treated in society.\n\nIn regard to potential assistive reproductive technologies, it is believed that reproduction can continue to happen outside of conventional methods, namely intercourse and artificial insemination. Advances such as human cloning, parthenogenesis and artificial wombs may significantly extend the potential for human reproduction.\n\nMany argue that posthuman space will be more virtual than real. Individuals may consist of uploaded minds living as data patterns on supercomputers or users engaged in completely immersive virtual realities. Postgenderists contend that these types of existences are not gender-specific thus allowing individuals to morph their virtual appearances and sexuality at will.\n\nPostgenderists maintain that a genderless society does not imply the existence of a species uninterested in sex and sexuality. It is thought that sexual relations and interpersonal intimacy can and will exist in a postgendered future, but that those activities may take on different form. For example, this theory raises the relationship between gender and technologies such as the latter's role in the dismantling of the conventional gender order. Postgenderism, however, is not directly concerned with the physical action of sex or with sexuality. It is believed to offer a more egalitarian system where individuals are classified according to factors such as age, talents, and interests instead of gender.\n\nIn the 1970 book \"The Dialectic of Sex\", radical feminist Shulamith Firestone wrote that differences in biological reproductive roles are a source of gender inequality. Firestone singled out pregnancy and childbirth, making the argument that an artificial womb would free \"women from the tyranny of their reproductive biology\".\n\n"}
{"id": "39809989", "url": "https://en.wikipedia.org/wiki?curid=39809989", "title": "Real-Time Object-Oriented Modeling", "text": "Real-Time Object-Oriented Modeling\n\nReal-Time Object-Oriented Modeling (ROOM) is a domain specific language.\n\nROOM was developed in the early 1990s for modeling Real-time systems. The initial focus was on telecommunications, even though ROOM can be applied to any event-driven real-time system.\n\nROOM was supported by ObjecTime Developer (commercial) and is now implemented by the official Eclipse project eTrice\n\nWhen UML2 was defined (version 2 of UML with real time extensions), many elements of ROOM were taken over.\n\nROOM is a modeling language for the definition of software systems. It allows the complete code generation for the whole system from the model. ROOM comes with a textual as well as with a graphical notation.\nTypically the generated code is accompanied with manually written code, e.g. for graphical user interfaces (GUI).\nThe code is compiled and linked against a runtime library which provides base classes and basic services (e.g. messaging).\n\nROOM describes a software system along three dimensions: structure, behavior and inheritance. The following sections will explain these three aspects in more detail.\n\nThe structural view in ROOM is composed of \"actors\" or \"capsules\". Actors can communicate with each other using \"ports\". Those ports are connected by \"bindings\". Actors do exchange messages asynchronously via ports and bindings.\nTo each port a unique \"protocol\" is assigned. A protocol in ROOM defines a set of outgoing and a set of incoming messages.\nPorts can be connected with a binding if they belong to the same protocol and are conjugate to each other. That means that one port is sending the outgoing messages of the protocol and receiving the incoming ones. This port is called the \"regular\" port.\nIts peer port, the \"conjugated\" port, receives the outgoing messages and sends the incoming ones of the protocol.\nIn other words, a port is the combination of a \"required\" and a \"provided interface\" in a \"role\" (since one and the same protocol can be used by several ports of an actor).\n\nAn actor can contain other actors (as a composition). In ROOM these are called \"actor references\" or \"actor refs\" for short.\nThis allows to create structural hierarchies of arbitrary depth.\n\nThe actor's ports can be part of its interface (visible from the exterior) or part of its structure (used by itself) or both.\nPorts that are part of the interface only are called \"relay ports\".\nThey are directly connected to a port of a sub actor (they are delegating to the sub actor).\nPorts that are part of the structure only are called \"internal end ports\".\nPorts that belong to both, structure and interface, are called \"external end ports\".\n\nEach actor in ROOM has a behavior which is defined by means of a hierarchical finite-state machine, or just state machine for short.\nA state machine is a directed graph consisting of nodes called \"states\" and edges called \"transitions\".\nState transitions are triggered by incoming messages from an internal or external end port.\nIn this context the messages sometimes are also called \"events\" or \"signals\".\nIf a transition specifies a certain \"trigger\" then it is said to \"fire\" if the state machine is in the source state of the transition and a message of the type specified by the trigger arrives. Afterwards the state is changed to the target state of the transition.\n\nDuring the state change certain pieces of code are executed. The programmer (or modeler) can attach them to the states and transitions.\nIn ROOM this code is written in the so called \"detail level language\", usually the target language of the code generation.\nA state can have \"entry code\" and \"exit code\". During a state change first the exit code of the source state is executed.\nThen the \"action code\" of the firing transition is executed and finally the entry code of the target state.\nA typical part of those codes is the sending of messages through ports of the actor.\n\nState machines in ROOM also have a graphical notation similar to the UML state charts. An example is shown in the diagram in this section.\n\nA state machine can also have a hierarchy in the sense that states can have sub state machines. Similar to the structure this can be extended to arbitrary depth.\nFor details of the semantics of hierarchical state machines we refer to the original book.\n\nAn important concept in the context of state machines is the execution model of \"run-to-completion\". That means that an actor is processing a message completely before it accepts the next message. Since the run-to-completion semantics is guaranteed by the execution environment, the programmer/modeler doesn't have to deal with classical thread synchronization. And this despite the fact that typical ROOM systems are highly concurrent because of the asynchronous communication.\nAnd maybe its worth to stress that the asynchronous nature of ROOM systems is not by accident but reflects the inherent asynchronicity of e.g. the machine being controlled by the software. Definitely this requires another mind set than the one that is needed for functional programming of synchronous systems.\nBut after a short while of getting accustomed it will be evident that asynchronously communicating state machines are perfectly suited for control software.\n\nLike other object oriented programming languages ROOM uses the concept of classes.\nActors are classes which can be instantiated as objects several times in the system.\nOf course each instance of an actor class is in its own state and can communicate with other instances of the same (and other) classes.\n\nSimilar to other modern programming languages ROOM allows inheritance of actor classes.\nIt is a single inheritance as an actor class can be derived from another actor class (its \"base class\").\nIt inherits all features of the base class like ports and actor refs, but also the state machine.\nThe derived actor class can add further states and transitions to the inherited one.\n\nA last powerful concept of ROOM is \"layering\". This notion refers to the vertical layers of a software system consisting of services and their clients. ROOM introduces the notions of \"service access point (SAP)\" for the client side and \"service provision point (SPP)\" for the server side. From the point of view of an actor implementation the SAPs and SPPs work like ports. Like ports they are associated with a protocol. But other than ports they don't have to (and even cannot) be bound explicitly.\nRather, an actor is bound to a concrete service by a \"layer connection\" and this binding of a service is propagated recursively to all sub actors of this actor.\nThis concept is very similar to dependency injection.\n\n"}
{"id": "54550433", "url": "https://en.wikipedia.org/wiki?curid=54550433", "title": "Scott Abel", "text": "Scott Abel\n\nScott Abel is an American content strategist, conference organizer and author. He is the founder of The Content Wrangler, a San Francisco-based digital media company that helps organizations adopt reusable intelligent content technologies and strategies. He currently produces and hosts the Information Development World conference.\n\nIn 2014, Joe Pulizzi's Content Marketing Institute acquired the Intelligent Content Conference, a large North American content management event founded by The Rockley Group and co-produced with Scott Abel. In 2016, UBM plc acquired the Content Marketing Institute for $17.6 million.\n\n"}
{"id": "3199082", "url": "https://en.wikipedia.org/wiki?curid=3199082", "title": "ShapeWriter", "text": "ShapeWriter\n\nShapeWriter (previously known as Shorthand-Aided Rapid Keyboarding (SHARK)) was a keyboard text input method for tablet, handheld PCs, and mobile phones invented by Shumin Zhai and Per Ola Kristensson at IBM Almaden Research Center and the Department of Computer and Information Science at Linköping University.\n\nUsing ShapeWriter text entry software, a user draws words on a graphical keyboard using a pen. Instead of tapping the keys, the user draws a pen gesture that connects all the letters in the desired word. After some usage the user learns the movement pattern for the commonly used words and can write them faster than is possible on a traditional virtual keyboard.\n\nThe first system described by Shumin Zhai and Per Ola Kristensson (2003) was only a prototype system that could recognize about 100 pen gestures for the top 100 words used in the English language. It used a handwriting recognition algorithm that relied on dynamic programming to recognize the word patterns drawn from a lexicon. The next version described by Per Ola Kristensson and Shumin Zhai (2004) has a fundamentally different recognition engine that can recognize 50,000 - 60,000 words with low latency. This system introduced the notion that every word in a large lexicon should be possible to write by tracing the letters. It is this system that was the basis for the software release on IBM alphaWorks that is generally associated with the term \"ShapeWriter\".\n\nShapeWriter was acquired by Nuance Communications, and taken off the market in 2010., its technology presumably incorporated as part of Nuance's FlexT9 app in 2011.\n\nShapeWriter was made available for the iPhone approximately in 2008 and was revised several times (including ShapeWriter Lite and ShapeWriter Pro or Plus) before being pulled due to its sale to Nuance Communications (see following entry).\n\nThose who purchased the iPhone version continued to use it and it also functioned on iPads v. 1 and 2 until 2013.\n\nAs of 2013 it no longer functions on iOS devices and is no longer available in Apple's App Store.\n\nShapeWriter software was made available as a free application for Android (operating system) smartphones through the Android Market. As a touchscreen keyboard replacement, it had over 50,000 users on Android worldwide. It was available only for Android OS versions 1.6 or higher. ShapeWriter for Android was available in 7 European languages including English, Spanish, and German. There was also a Beta release for Android 1.5 phones including the HTC Hero and Droid Eris.\n\nShapeWriter, Inc. was purchased by Nuance Communications and the ShapeWriter software was removed from the Android Market indefinitely on June 20, 2010.\n\n"}
{"id": "35891416", "url": "https://en.wikipedia.org/wiki?curid=35891416", "title": "SpiNNaker", "text": "SpiNNaker\n\nSpiNNaker (Spiking Neural Network Architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the School of Computer Science, University of Manchester. It is composed of 57,600 ARM9 processors (specifically ARM968), each with 18 cores and 128MB of mobile DDR SDRAM, totaling 1,036,800 cores and over 7TB of RAM. The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).\n\nThe completed design is housed in 10 19-inch racks, with each rack holding over 100,000 cores. The cards holding the chips are held in 5 Blade enclosures, and each core emulates 1000 Neurons. In total, the goal is to simulate the behavior of aggregates of up to a billion neurons in real time. This machine requires about 100kW from a 240V supply and an air-conditioned environment.\n\nSpiNNaker is being used as one component of the neuromorphic computing platform for the Human Brain Project.\n\nOn October 14, 2018 the HBP announced that the million core milestone had been achieved.\n\n"}
{"id": "58522731", "url": "https://en.wikipedia.org/wiki?curid=58522731", "title": "Technical and Industrial Cultural Heritage in Norway", "text": "Technical and Industrial Cultural Heritage in Norway\n\nTechnical and Industrial Cultural Heritage in Norway encompasses discontinued industrial and other facilities with great historical and architectural value. It is one of the ten conservation programs for the Norwegian Directorate for Cultural Heritage, which seeks to refurbish and preserve a representative range of facilities linked to Norway's most important industrial routes, which has had a significant impact on local business history.\n\nThe Norwegian Directorate for Cultural Heritage list of priority technical and industrial cultural heritage comprises 15 facilities:\n\n\n"}
{"id": "6271946", "url": "https://en.wikipedia.org/wiki?curid=6271946", "title": "Technological rationality", "text": "Technological rationality\n\nTechnological rationality or technical rationality is a philosophical idea postulated by the Frankfurt School philosopher Herbert Marcuse in his 1941 article, \"Some Implications of Modern Technology\", published first in the journal Studies in Philosophy and Social Sciences, Vol. IX. It gained mainstream repute and a more holistic treatment in his 1964 book \"One-Dimensional Man\". \n\nIt posits that rational decisions to incorporate technological advances into society can, once the technology is ubiquitous, change what is considered rational within that society.\n\n"}
{"id": "6327661", "url": "https://en.wikipedia.org/wiki?curid=6327661", "title": "Technology adoption life cycle", "text": "Technology adoption life cycle\n\nThe technology adoption lifecycle is a sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. The process of adoption over time is typically illustrated as a classical normal distribution or \"bell curve\". The model indicates that the first group of people to use a new product is called \"innovators\", followed by \"early adopters\". Next come the early majority and late majority, and the last group to eventually adopt a product are called \"Laggards\" or \"phobics.\" For example, a phobic may only use a cloud service when it is the only remaining method of performing a required task, but the phobic may not have an in-depth technical knowledge of how to use the service.\n\nThe demographic and psychological (or \"psychographic\") profiles of each adoption group were originally specified by the North Central Rural Sociology Committee, Subcommittee for the Study of the Diffusion of Farm Practices, by agricultural researchers Beal and Bohlen in 1957.\nThe report summarized the categories as:\n\nThe model has subsequently been adapted for many areas of technology adoption in the late 20th century.\n\nThe model has spawned a range of adaptations that extend the concept or apply it to specific domains of interest.\n\nIn his book \"Crossing the Chasm\", Geoffrey Moore proposes a variation of the original lifecycle. He suggests that for discontinuous innovations, which may result in a Foster disruption based on s-curve, there is a gap or chasm between the first two adopter groups (innovators/early adopters), and the vertical markets.\n\nDisruption as it is used today are of the Clayton M. Christensen variety. These disruptions are not s-curve based. \n\nIn educational technology, Lindy McKeown has provided a similar model (a pencil metaphor) describing the ICT uptake in education. In medical sociology, Carl May has proposed normalization process theory that shows how technologies become embedded and integrated in health care and other kinds of organisation.\n\nWenger, White and Smith, in their book \"Digital habitats: Stewarding technology for communities\", talk of technology stewards: people with sufficient understanding of the technology available and the technological needs of a community to steward the community through the technology adoption process.\n\nRayna and Striukova (2009) propose that the choice of initial market segment has crucial importance for crossing the chasm, as adoption in this segment can lead to a cascade of adoption in the other segments. This initial market segment has, at the same time, to contain a large proportion of visionaries, to be small enough for adoption to be observed from within the segment and from other segment and be sufficiently connected with other segments. If this is the case, the adoption in the first segment will progressively cascade into the adjacent segments, thereby triggering the adoption by the mass-market.\n\nOne way to model product adoption is to understand that people's behaviours are influenced by their peers and how widespread they think a particular action is. For many format-dependent technologies, people have a non-zero payoff for adopting the same technology as their closest friends or colleagues. If two users both adopt product A, they might get a payoff \"a\" > 0; if they adopt product B, they get \"b\" > 0. But if one adopts A and the other adopts B, they both get a payoff of 0.\n\nA threshold can be set for each user to adopt a product. Say that a node v in a graph has d neighbors: then v will adopt product A if a fraction p of its neighbors is greater than or equal to some threshold. For example, if v's threshold is 2/3, and only one of its two neighbors adopts product A, then v will not adopt A. Using this model, we can deterministically model product adoption on sample networks.\n\nThe technology adoption lifecycle is a sociological model that is an extension of an earlier model called \"the diffusion process\", which was originally published in 1957 by Joe M. Bohlen, George M. Beal and Everett M. Rogers at Iowa State University and which was originally published only for its application to agriculture and home economics.\nbuilding on earlier research conducted there by Neal C. Gross and Bryce Ryan. Their original purpose was to track the purchase patterns of hybrid seed corn by farmers.\n\nBeal, Rogers and Bohlen together developed a model called \"the diffusion process\" and later, Everett Rogers generalized the use of it in his widely acclaimed book 1962 \"Diffusion of Innovations\" (now in its fifth edition), describing how new ideas and technologies spread in different cultures. Others have since used the model to describe how innovations spread between states in the U.S.\n\n"}
{"id": "26461121", "url": "https://en.wikipedia.org/wiki?curid=26461121", "title": "Techné: Research in Philosophy and Technology", "text": "Techné: Research in Philosophy and Technology\n\nTechné: Research in Philosophy and Technology is a peer-reviewed academic journal with a focus on philosophical analysis of technological systems. The journal was established in 1995 as an electronic journal, under the sponsorship of the Society for Philosophy and Technology. It is published three times per year and online access to the journal is a benefit of membership in the society. The journal is published for the society by the Philosophy Documentation Center. The current editors-in-chief are Neelke Doorn and Diane P. Michelfelder.\n\n"}
{"id": "69477", "url": "https://en.wikipedia.org/wiki?curid=69477", "title": "Timeline of historic inventions", "text": "Timeline of historic inventions\n\nThe timeline of historic inventions is a chronological list of particularly important or significant technological inventions and the people who created the inventions.\n\n\"Note:\" Dates for inventions are often controversial. Inventions are often invented by several inventors around the same time, or may be invented in an impractical form many years before another inventor improves the invention into a more practical form. Where there is ambiguity, the date of the first known working version of the invention is used here.\n\nThe dates listed in this section refer to the earliest evidence of an invention found and dated by archaeologists (or in a few cases, suggested by indirect evidence). Dates are often approximate and change as more research is done, reported, and seen. Older examples of any given technology are found often. The locations listed are for the site where the earliest solid evidence has been found, but especially for the earlier inventions there is little certainty how close that may be to where the invention took place.\n\n\n\"A few non-invention dates are included in italics, for context. This time period is characterized as an ice age with regular periodic warmer periods - interglacial episodes - initially every 41,000 years slowing to \"\n\n\"Note the shift from Ma and ka to BC and AD – 8000 BC is approximately the same as 10 ka.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2279544", "url": "https://en.wikipedia.org/wiki?curid=2279544", "title": "Women in computing", "text": "Women in computing\n\nWomen in computing have shaped the evolution of information technology. They were among the first programmers in the early-20th century, and contributed substantially to the industry. As technology and practices altered, the role of women as programmers has changed, and the recorded history of the field has downplayed their achievements.\n\nSince the 18th century, women have developed scientific computations, including Nicole-Reine Lepaute's prediction of Halley's Comet, and Maria Mitchell's computation of the motion of Venus. The first algorithm intended to be executed by a computer was designed by Ada Lovelace who was a pioneer in the field. Grace Hopper was the first person to design a compiler for a programming language. Throughout the 19th and early-20th century, and up to World War II, programming was predominantly done by women; significant examples include the Harvard Computers, codebreaking at Bletchley Park and engineering at NASA.\n\nAfter the 1960s, the \"soft work\" that had been dominated by women evolved into modern software, and the importance of women decreased. The gender disparity and the lack of women in computing from the late 20th century onward has been examined, but no firm explanations have been established. Nevertheless, many women continued to make significant and important contributions to the IT industry, and attempts were made to readdress the gender disparity in the industry. In the 21st century, women held leadership roles in multiple tech companies, such as Meg Whitman, president and chief executive officer of Hewlett Packard Enterprise, and Marissa Mayer, president and CEO of Yahoo! and key spokesperson at Google.\n\nNicole-Reine Etable de la Brière Lepaute was one of a team of human computers who worked with Alexis-Claude Clairaut and Joseph-Jérôme Le Français de Lalande to predict the date of the return of Halley's Comet. They began work on the calculations in 1757, working throughout the day and sometimes during mealtimes. Their methods were followed by successive human computers. They divided large calculations into \"independent pieces, assembled the results from each piece into a final product\" and then checked for errors. Lepaute continued to work on computing for the rest of her life, working for the \"Connaissance de Temps\" and publishing predictions of solar eclipses.\n\nOne of the first computers for the American \"Nautical Almanac\" was Maria Mitchel. Her work on the assignment was to compute the motion of the planet Venus. The \"Almanac\" never became a reality, but Mitchell became the first astronomy professor at Vassar.\n\nAda Lovelace was the first person to publish an algorithm intended to be executed by the first modern computer, the Analytical Engine created by Charles Babbage. As a result she is often regarded as the first computer programmer. Lovelace was introduced to Babbage's difference engine when she was 17. In 1840, she wrote to Babbage and asked if she could become involved with his first machine. By this time, Babbage had moved on to his idea for the Analytical Engine. A paper describing the Analytical Engine, \"Notions sur la machine analytique\", published by L.F. Menabrea, came to the attention of Lovelace, who not only translated it into English, but corrected mistakes made by Menabrea. Babbage suggested that she expand the translation of the paper with her own ideas, which, signed only with her initials, AAL, \"synthesized the vast scope of Babbage's vision.\" Lovelace imagined the kind of impact of the Analytical Engine might have on society. She drew up explanations of how the engine could handle inputs, outputs, processing and data storage. She also created several proofs to show how the engine would handle calculations of Bernoulli Numbers on its own. The proofs are considered the first examples of a computer program. Lovelace downplayed her role in her work during her life, for example, in signing her contributions with AAL so as not be \"accused of bragging.\"\n\nAfter the Civil War in the United States, more women were hired as human computers. Many were war widows looking for ways to support themselves. Others were hired when the government opened positions to women because of a shortage of men to fill the roles.\nAnna Winlock asked to become a computer for the Harvard Observatory in 1875 and was hired to work for 25 cents an hour. By 1880, Edward Charles Pickering had hired several women to work for him at Harvard because he felt that women could do the job as well as men and he could ask them to volunteer or work for less pay. The women, described as \"Pickering's harem\" and also as the Harvard Computers, performed clerical work that the male employees and scholars considered to be tedious at a fraction of the cost of hiring a man. The women working for Pickering cataloged around ten thousand stars, discovered the Horsehead Nebula and developed the system to describe stars. One of the \"computers,\" Annie Jump Cannon, could classify stars at a rate of three stars per minute. The work for Pickering became so popular that women volunteered to work for free even when the computers were being paid. Even though they performed an important role, the Harvard Computers were paid less than factory workers.\n\nBy the 1890s, women computers were college graduates looking for jobs where they could use their training in a useful way. Florence Tebb Weldon, was part of this group and provided computations relating to biology and evidence for evolution, working with her husband, W.F. Raphael Weldon. Florence Weldon's calculations demonstrated that statistics could be used to support Darwin's theory of evolution. Another human computer involved in biology was Alice Lee, who worked with Karl Pearson. Pearson hired two sisters to work as part-time computers at his Biometrics Lab, Beatrice and Frances Cave-Brown-Cave.\n\nDuring World War I, Karl Pearson and his Biometrics Lab helped produce ballistics calculations for the British Ministry of Munitions. Beatrice Cave-Brown-Cave helped calculate trajectories for bomb shells. In 1916, Cave-Brown-Cave left Pearson's employ and started working full-time for the Ministry. In the United States, women computers were hired to calculate ballistics in 1918, working in a building on the Washington Mall. One of the women, Elizabeth Webb Wilson, worked as the chief computer. After the war, women who worked as ballistics computers for the U.S. government had trouble finding jobs in computing and Wilson eventually taught high school math.\n\nIn the early 1920s, Iowa State College, professor George Snedecor worked to improve the school's science and engineering departments, experimenting with new punch-card machines and calculators. Snedecor also worked with human calculators most of them women, including Mary Clem. Clem coined the term \"zero check\" to help identify errors in calculations. The computing lab, run by Clem, became one of the most powerful computing facilities of the time. \n\nWomen computers also worked at the American Telephone and Telegraph company. These human computers worked with electrical engineers to help figure out how to boost signals with vacuum tube amplifiers. One of the computers, Clara Froelich, was eventually moved along with the other computers to their own division where they worked with a mathematician, Thornton Fry, to create new computational methods. Froelich studied IBM tabulating equipment and desk calculating machines to see if she could adapt the machine method to calculations.\n\nEdith Clarke was the first woman to earn a degree in electrical engineering and who worked as the first professionally employed electrical engineer in the United States. She was hired by General Electric as a full engineer in 1923. Clarke also filed a patent in 1921 for a graphical calculator to be used in solving problems in power lines. It was granted in 1925.\n\nThe National Advisory Committee for Aeronautics (NACA) which became NASA hired a group of five women in 1935 to work as a computer pool. The women worked on the data coming from wind tunnel and flight tests.\n\n\"Tedious\" computing and calculating was seen as \"women's work\" through the 1940s resulting in the term \"kilogirl\", invented by a member of the Applied Mathematics Panel in the early 1940s. A kilogirl of energy was \"equivalent to roughly a thousand hours of computing labor.\" While women's contributions to the United States war effort during World War II was championed in the media, their roles and the work they did was minimized. This included minimizing the complexity, skill and knowledge needed to work on computers or work as human computers. During WWII, women did most of the ballistics computing, seen by male engineers as being below their level of expertise. Black women computers worked as hard (or more often, twice as hard) as their white counterparts, but in segregated situations. By 1943, almost all people employed as computers were women.\n\nNACA expanded its a pool of women human computers in the 1940s. NACA recognized in 1942 that \"the engineers admit themselves that the girl computers do the work more rapidly and accurately than they could.\" In 1943 two groups, segregated by race, worked on the east and west side of Langley Air Force Base. The black women were the West Area Computers. Unlike their white counterparts, the black women were asked by NACA to re-do college courses they had already passed and many never received promotions.\n\nWomen were also working on ballistic missile calculations. In 1948, women such as Barbara Paulson were working on the WAC Corporal, determining trajectories the missiles would take after launch.\n\nWomen worked with cryptography and, after some initial resistance, many operated and worked on the Bombe machines. Joyce Aylard operated the Bombe machine testing different methods to break the Enigma code. Joan Clarke was a cryptographer who worked with her friend, Alan Turing, on the Enigma machine at Bletchley Park. When she was promoted to a higher salary grade, there were no positions in the civil service for a \"senior female cryptanalyst,\" and she was listed as a linguist instead. While Clarke developed a method of increasing the speed of double-encrypted messages, unlike many of the men, her decryption technique was not named after her. Other cryptographers at Bletchley included Margaret Rock, Mavis Lever (later Batey), Ruth Briggs and Kerry Howard. In 1941, Batey's work enabled the Allies to break the Italian's naval code before the Battle of Cape Matapan. In the United States, several faster Bombe machines were created. Women, like Louise Pearsall, were recruited from the WAVES to work on code breaking and operate the American Bombe machines.\n\nHedy Lamarr and co-inventor, George Antheil, worked on a frequency hopping method to help the Navy control torpedoes remotely. The Navy passed on their idea, but Lamarr and Antheil received a patent for the work on August 11, 1942. This technique would later be used again, first in the 1950s at Sylvania Electronic Systems Division and is used in everyday technology such as Bluetooth and Wi-Fi.\n\nThe programmers of the ENIAC computer in 1944, were six female mathematicians; Marlyn Meltzer, Betty Holberton, Kathleen Antonelli, Ruth Teitelbaum, Jean Bartik, and Frances Spence who were human computers at the Moore School's computation lab. Adele Goldstine was their teacher and trainer and they were known as the \"ENIAC girls.\" The women who worked on ENIAC were warned that they would not be promoted into professional ratings which were only for men. Designing the hardware was \"men's work\" and programming the software was \"women's work.\" Sometimes women were given blueprints and wiring diagrams to figure out how the machine worked and how to program it. They learned how the ENIAC worked by repairing it, sometimes crawling through the computer, and by fixing \"bugs\" in the machinery. Even though the programmers were supposed to be doing the \"soft\" work of programming, in reality, they did that and fully understood and worked with the hardware of the ENIAC. When the ENIAC was revealed in 1946, Goldstine and the other women prepared the machine and the demonstration programs it ran for the public. None of their work in preparing the demonstrations was mentioned in the official accounts of the public events. After the demonstration, the university hosted an expensive celebratory dinner to which none of the ENIAC six were invited.\n\nIn Canada, Beatrice Worsley started working at the National Research Council of Canada in 1947 where she was an aerodynamics research officer. A year later, she started working in the new Computational Centre at the University of Toronto. She built a differential analyzer in 1948 and also worked with IBM machines in order to do calculations for Atomic Energy of Canada Limited. She went to study the EDSAC at the University of Cambridge in 1949. She wrote the program that was run the first time EDSAC performed its first calculations on May 6, 1949. \n\nGrace Hopper was the first person to create a compiler for a programming language and one of the first programmers of the Harvard Mark I computer, an electro-mechanical computer based on Analytical Engine. Hopper's work with computers started in 1943, when she started working at the Bureau of Ordnance's Computation Project at Harvard where she programmed the Harvard Mark I. Hopper not only programmed the computer, but created a 500 page comprehensive manual for it. Even though Hopper created the manual which was widely cited and published, she was not specifically credited in the manual. Hopper is often credited with the coining of the term \"bug\" and \"debugging\" when a moth caused the Mark II to malfunction. While a moth was found and the process of removing it called \"debugging,\" the terms were already part of the language of programmers.\n\nGrace Hopper continued to contribute to computer science through the 1950s. She brought the idea of using compilers from her time at Harvard to UNIVAC which she joined in 1949. Other women who were hired to program UNIVAC included Adele Mildred Koss, Frances E. Holberton, Jean Bartik, Frances Morello and Lillian Jay. To program the UNIVAC, Hopper and her team used the FLOW-MATIC programming language, which she developed. Holberton wrote a code, C-10, that allowed for keyboard inputs into a general-purpose computer. Holberton also developed the Sort-Merge Generator in 1951 which was used on the UNIVAC I. The Sort-Merge Generator marked the first time a computer \"used a program to write a program.\" Holberton suggested that computer housing should be beige or oatmeal in color which became a long-lasting trend. Koss worked with Hopper on various algorithms and a program that was a precursor to a report generator.\n\nKlara Dan von Neumann was one of the main programmers of the MANIAC, a more advanced version of ENIAC. Her work helped the field of meteorology and weather prediction.\n\nThe NACA, and subsequently NASA, recruited women computers following World War II. By the 1950s, a team was performing mathematical calculations at the Lewis Research Center in Cleveland, Ohio, including Annie Easley, Katherine Johnson and Kathryn Peddrew. At the National Bureau of Standards, Margaret R. Fox was hired to work as part of the technical staff of the Electronic Computer Laboratory in 1951. \n\nAt Convair Aircraft Corporation, Joyce Currie Little was one of the original programmers for analyzing data received from the wind tunnels. She used punch cards on an IBM 650 which was located in a different building from the wind tunnel. To save time in the physical delivery of the punch cards, she and her colleague, Maggie DeCaro, put on roller skates to get to and from the building faster. \n\nIn Israel, Thelma Estrin worked on the design and development of WEIZAC, one of the world's first large-scale programmable electronic computers. In the Soviet Union the IT industry was dominated by women; a team of them designed the first digital computer in 1951. In the UK, Kathleen Booth worked with her husband, Andrew Booth on several computers at Birkbeck College. Kathleen Booth was the programmer and Andrew built the machines. Kathleen developed Assembly Language at this time.\n\nAdele Mildred Koss, who had worked at UNIVAC with Hopper, started work at Control Data Corporation (CDC) in 1965. There she developed algorithms for graphics, including graphic storage and retrieval.\n\nMary K. Hawes of Burroghs Corporation set up a meeting in 1959 to discuss the creation a computer language that would be shared between businesses. Six people, including Hopper, attended to discuss the philosophy of creating a common business language (CBL). Hopper became involved in developing COBOL (Common Business Oriented Language) where she innovated new symbolic ways to write computer code. Hopper developed programming language that was easier to read and \"self-documenting.\" After COBOL was submitted to the CODASYL Executive Committee, Betty Holberton did further editing on the language before it was submitted to the Government Printing Office in 1960. IBM were slow to adopt COBOL, which hindered its progress but it was accepted as a standard in 1962, after Hopper had demonstrated the compiler working both on UNIVAC and RCA computers. The development of COBOL led to the generation of compilers and generators, most of which were created or refined by women such as Koss, Nora Moser, Deborah Davidson, Sue Knapp, Gertrude Tierney and Jean E. Sammet.\n\nSammet, who worked at IBM starting in 1961 was responsible for developing the programming language, FORMAC. She published a book, \"Programming Languages: History and Fundamentals\" (1969), which was considered the \"standard work on programming languages,\" according to Denise Gürer It was \"one of the most used books in the field,\" according to \"The Times\" in 1972. \n\nBetween 1961 and 1963, Margaret Hamilton began to study software reliability while she was working at the US SAGE air defense system. In 1965, she was responsible for programming the software for the onboard flight software on the Apollo mission computers. After Hamilton had completed the program, the code was sent to Raytheon where \"expert seamstresses\" called the \"Little Old Ladies\" actually hardwired the code by threading copper wire through magnetic rings. Each system could store more than 12,000 words that were represented by the copper wires.\nIn 1964, the British Prime Minister Harold Wilson announced a \"White-Hot\" revolution in technology, that would give greater prominence to IT work. As women still held most computing and programming positions at this time, it was hoped that it would give them more positive career prospects. In 1965, Sister Mary Kenneth Keller became the first American woman to earn a doctorate in computer science. Keller helped develop BASIC while working as a graduate student at Dartmouth, where the university \"broke the 'men only' rule\" so she could use its computer science center.\n\nChristine Darden began working for NASA's computing pool in 1967 having graduated from the Hampton Institute. Women were involved in the development of Whirlwind, including Judy Clapp. She created the prototype for an air defense system for Whirlwind which used radar input to track planes in the air and could direct aircraft courses.\n\nIn 1969, Elizabeth \"Jake\" Feinler, who was working for Stanford, made the first Resource Handbook for ARPANET. This led to the creation of the ARPANET directory, which was built by Feinler with a staff of mostly women. Without the directory, \"it was nearly impossible to navigate the ARPANET.\" \n\nBy the end of the decade, the general demographics of programmers had shifted away from being predominantly women, as they had before the 1940s. Though women accounted for around 30 to 50 percent of computer programmers during the 1960s, few were promoted to leadership roles and women were paid significantly less than their male counterparts. \"Cosmopolitan\" ran an article in the April 1967 issue about women in programming called \"The Computer Girls.\" Even while magazines such as \"Cosmopolitan\" saw a bright future for women in computers and computer programming in the 1960s, the reality was that women were still being marginalized.\n\nIn the early 1970s, Pam Hardt-English led a group to create a computer network they named Resource One and which was part of a group called Project One. Her idea to connect Bay Area bookstores, libraries and Project One was an early prototype of the Internet. To work on the project, Hardt-English obtained an expensive SDS-940 computer as a donation from TransAmerica Leasing Corporation in April 1972. They created an electronic library and housed it in a record store called Leopold's in Berkeley. This became the Community Memory database and was maintained by hacker, Jude Milhon. After 1975, the SDS-940 computer was repurposed by Sherry Reson, Mya Shone, Chris Macie and Mary Janowitz to create a social services database and a Social Services Referral Directory. Hard copies of the directory, printed out as a subscription service, were kept at city buildings and libraries. The database was maintained and in use until 2009.\n\nIn the early 1970s, Elizabeth \"Jake\" Feinler, who worked on the Resource Directory for ARPANET, and her team created the first WHOIS directory. Feinler set up a server at the Network Information Center (NIC) at Stanford which would work as a directory that could retrieve relevant information about a person or entity. She and her team worked on the creation of domains, with Feinler suggesting that domains be divided by categories based on where the computers were kept. For example, military computers would have the domain of .mil, computers at educational institutions would have .edu. Feinler worked for NIC until 1989. \n\nJean E. Sammet served as the first woman president of the Association for Computing Machinery (ACM), holding the position between 1974 and 1976.\n\nAdele Goldberg was one of seven programmers that developed Smalltalk in the 1970s, and wrote the majority of the language's documentation. It was one of the first object-oriented programming languages the base of the current graphic user interface, that has its roots in the 1968 The Mother of All Demos by Douglas Engelbart. Smalltalk was used by Apple to launch Apple Lisa in 1983, the first personal computer with a GUI, and a year later its Macintosh. Windows 1.0, based on the same principles, was launched a few months later in 1985.\n\nIn the late 1970s, women such as Paulson and Sue Finley wrote programs for the Voyager mission. Voyager continues to carry their codes inside its own memory banks as it leaves the solar system. In 1979, Ruzena Bajcsy founded the General Robotics, Automation, Sensing and Perception (GRASP) Lab at the University of Pennsylvania.\n\nIn the mid-70s, Joan Margaret Winters began working at IBM as part of a \"human factors project,\" called SHARE. In 1978, Winters was the deputy manager of the project and went on to lead the project between 1983 and 1987. The SHARE group worked on researching how software should be designed to consider human factors.\n\nErna Schneider Hoover developed a computerized switching system for telephone calls that would replace switchboards. Her software patent for the system, issued in 1971, was one of the first software patents ever issued.\n\nGwen Bell developed the Computer Museum in 1980. The museum, which collected computer artifacts became a non-profit organization in 1982 and in 1984, Bell moved it to downtown Boston. Adele Goldberg served as president of ACM between 1984 and 1986. In 1986, Lixia Zhang was the only woman and graduate student to participate in the early Internet Engineering Task Force (IETF) meetings. Zhang was involved in early Internet development. \n\nSometimes known as the \"Betsy Ross of the personal computer,\" according to the \"New York Times\", Susan Kare worked with Steve Jobs to design the original icons for the Macintosh. Kare designed the moving watch, paintbrush and trash can elements that made using a Mac user-friendly. Kare worked for Apple until the mid 1980s, going on to work on icons for Windows 3.0. Other types of computer graphics were being developed by Nadia Magnenat Thalmann in Canada. Thalmann started working on computer animation to develop \"realistic virtual actors\" first at the University of Montréal in 1980 and later in 1988 at the École Polytechnique Fédérale de Lausanne.\n\nIn the field of human computer interaction (HCI), French computer scientist, Joëlle Coutaz developed the presentation-abstraction-control (PAC) model in 1987. She founded the User Interface group at the Laboratorire de Génie Informatique of IMAG where they worked on different problems relating to user interface and other software tools. \n\nAs Ethernet became the standard for networking computers locally, Radia Perlman, who worked at Digital Equipment Corporation (DEC), was asked to \"fix\" limitations that Ethernet imposed on large network traffic. In 1985, Perlman came up with a way to route information packets from one computer to another in an \"infinitely scalable\" way that allowed large networks like the Internet to function. Her solution took less than a few days to design and write up. The name of the algorithm she created is the Spanning Tree Protocol. In 1988, Stacy Horn, who had been introduced to bulletin board systems (BBS) through The WELL, decided to create her own online community in New York, which she called the East Coast Hang Out (ECHO). Horn invested her own money and pitched the idea for ECHO to others after bankers refused to hear her business plan. Horn built her BBS using UNIX, which she and her friends taught to one another. Eventually ECHO moved an office in Tribeca in the early 1990s and started getting press attention. ECHO's users could post about topics that interested them, chat with on another and were provided email accounts. Around half of ECHO's users were women. ECHO is still online as of 2018.\n\nEurope was somewhat behind other countries in developing an Internet infrastructure. A project was developed in the mid-1980s to create an academic network in Europe using the Open System Interconnection (OSI) standards. Borka Jerman Blažič, a Yugoslavian computer scientist was invited to work on the project. She was involved in establishing a Yugoslav Research and Academic Network (YUNAC) in 1989 and registered the domain of .yu for the country.\n\nComputer and video games became popular in the 1980s, but many were primarily action-oriented and not designed from a woman's point of view. Stereotypical characters such as the damsel in distress featured prominently and consequently were not inviting towards women. Dona Bailey designed \"Centipede\", where the player shoots insects, as a reaction to such games, later saying \"It didn't seem bad to shoot a bug\". Carol Shaw, considered to be the first modern female games designer, released a 3D version of Tic-tac-toe for the Atari 2600 in 1980. Roberta Williams and her husband Ken, founded Sierra Online and pioneered the graphic adventure game format in \"Mystery House\" and the \"King's Quest\" series. The games had a friendly graphical user interface and introduced humor and puzzles. Cited as an important game designer, her influenced spread from Sierra to other companies such as LucasArts and beyond. Brenda Laurel worked on porting games from arcade versions to the Atari 400 and Atari 800 computers in the late 1970s and early 1980s. She then went to work for Activision, writing the manual for \"Maniac Mansion\". \n\n1984 was the year of Women Into Science and Engineering (WISE). A 1984 report by Ebury Publishing reported that in a typical family, only 5% of mothers and 19% of daughters were using a computer at home, compared to 25% of fathers and 51% of sons. To counteract this, the company launched a series of software titles designed towards women and publicised in \"Good Housekeeping\". Anita Borg, who had been noticing that women were under-represented in computer science, founded an email support group, Systers, in 1987.\n\nBy the 1990s, computing was dominated by men. The proportion of female computer science graduates peaked in 1984 around 37 per cent, and then steadily declined. Although the end of the 20th century saw an increase in women scientists and engineers, this did not hold true for computing, which stagnated. Despite this, they were very involved in working on hypertext and hypermedia projects in the late 1980s and early 1990s. A team of women at Brown University, including Nicole Yankelovich and Karen Catlin, developed Intermedia and invented the anchor link. Apple partially funded their project and incorporated their concepts into Apple operating systems. Sun Microsystems Sun Link Service was developed by Amy Pearl. Janet Walker developed the first system to use bookmarks when she created the Symbolics Document Examiner. In 1989, Wendy Hall created a hypertext project called Microcosm, which was based on digitized multimedia material found in the Mountbatten archive. Cathy Marshall worked on the NoteCards system at Xerox PARC. NoteCards went on to influence Apple's HyperCard. As the Internet became the World Wide Web, developers like Hall adapted their programs to include Web viewers. Her Microcosm was especially adaptable to new technologies, including animation and 3-D models. In 1994, Hall helped organize the first conference for the Web. \n\nSarah Allen, the co-founder of After Effects, co-founded a commercial software company called CoSA in 1990. In 1995, she started working on the Shockwave team for Macromedia where she was the lead developer of the Shockwave Mulituser Server, the Flash Media Server and Flash video.\n\nFollowing the increased popularity of the Internet in the 1990s, online spaces were set up to cater for women, including the online community Women's WIRE and the technical and support forum LinuxChix. Women's WIRE, launched by Nancy Rhine and Ellen Pack in October 1993, was the first Internet company to specifically target this demographic. A conference for women in computer-related jobs, the Grace Hopper Celebration of Women in Computing, was first launched in 1994 by Anita Borg.\n\nGame designer Brenda Laurel started working at Interval Research in 1992, and began to think about the differences in the way girls and boys experienced playing video games. After interviewing around 1,000 children and 500 adults, she determined that games weren't designed with girls' interests in mind. The girls she spoke with wanted more games with open worlds and characters they could interact with. Her research led to Interval Research giving Laurel's research team their own company in 1996, Purple Moon. Also in 1996, Mattel's game, \"Barbie Fashion Designer\", became the first best-selling game for girls. Purple Moon's first two games based on a character called Rockett, made it to the 100 best-selling games in the years they were released. In 1999, Mattel bought out Purple Moon.\n\nJaime Levy created the one of the first e-Zines in the early 1990s, starting with \"CyberRag\", which included articles, games and animations loaded onto diskettes that anyone with a Mac could access. Later, she renamed the zine to \"Electronic Hollywood.\" Billy Idol commissioned Levy to create a disk for his album, \"Cyberpunk\". She was hired to be the creative director of the online magazine, \"Word\", in 1995.\n\nCyberfeminists, VNS Matrix, made up of Josephine Starrs, Juliane Pierce, Francesca da Rimini and Virginia Barratt, created art in the early 1990s linking computer technology and women's bodies. In 1997, there was a gathering of cyberfeminists in Kassel, called the First Cyberfeminist International.\n\nIn China, Hu Qiheng, was the leader of the team who installed the first TCP/IP connection for China, connecting to the Internet on April 20, 1994. In 1995, Rosemary Candlin went to write software for CERN in Geneva. In the early 1990s, Nancy Hafkin was an important figure in working with the Association for Progressive Communications (APC) in enabling email connections in 10 African countries. Starting in 1999, Anne-Marie Eklund Löwinder began to work with Domain Name System Security Extensions (DNSSEC) in Sweden. She later made sure that the domain, .se, was the world's first top level domain name to be signed with DNSSEC.\n\nIn the 21st century, several attempts have been made to reduce the gender disparity in IT and get more women involved in computing again. A 2001 survey found that while both sexes use computers and the internet in equal measure, women were still five times less likely to choose it as a career or study the subject beyond standard secondary education. Journalist Emily Chang said a key problem has been personality tests in job interviews and the belief that good programmers are introverts, which tends to self-select the stereotype of an antisocial white male nerd.\n\nIn 2004, the National Center for Women & Information Technology was established by Lucy Sanders to address the gender gap. Carnegie Mellon University has made a concerted attempt to increase gender diversity in the computer science field, by selecting students based on a wide criteria including leadership ability, a sense of \"giving back to the community\" and high attainment in maths and science, instead of traditional computer programming expertise. As well as increase the intake of women into CMU, the programme resulted in better quality students overall, as they found the increased diversity made for a stronger team.\n\nDespite the pioneering work of some designers, video games are still considered biased towards men. A 2013 survey by the International Game Developers Association revealed only 22% of game designers are women, although this is substantially higher than figures in previous decades. Working to bring inclusion to the world of open source project development, Coraline Ada Ehmke drafts the Contributor Covenant in 2014. By 2018, over 40,000 software projects have started using the Contributor Covenant, including TensorFlow, Vue and Linux.\n\nIn 2017, Michelle Simmons founded the first quantum computing company in Australia. The team, which has made \"great strides\" in 2018, plans to develop a 10-qubit prototype silicon quantum integrated circuit by 2022. Also in 2017, Doina Precup became the head of DeepMind Montreal, working on artificial intelligence.\n\nOne of the biggest problems facing women in computing in the modern era is that they often find themselves working in an environment that is largely unpleasant, so they don't stay on in the careers in programming and technology. In 2013, a National Public Radio report said 20% of computer programmers in the US are female. There is no general consensus for any key reason there are less women in computing. In 2017, James Damore was fired from Google after claiming there was a biological reason for a lack of female computer scientists. The following year, Wikipedia was criticised for not having an article about scientist Donna Strickland until shortly after she won the Nobel Prize for Physics, which was attributed to a severe gender disparity of the site's editors.\n\nIn 1991, Massachusetts Institute of Technology undergraduate Ellen Spertus wrote an essay \"Why Are There So Few Women in Computer Science?\", which complained about inherent sexism in IT, which was responsible for a lack of women in computing. She subsequently taught computer science at Mills College, Oakland in order to increase interest in IT for women. A key problem is a lack of female role models in the IT industry, alongside computer programmers in fiction and the media generally being male. The University of Southampton's Wendy Hall has said the attractiveness of computers to women decreased significantly in the 1980s when they \"were sold as toys for boys\", and believes the cultural stigma has remained ever since, and may even be getting worse. Kathleen Lehman, project manager of the BRAID Initiative at UCLA has said a problem is that typically women aim for perfection and feel disillusioned when code does not compile, whereas men may simply treat it as a learning experience. A report in the \"Daily Telegraph\" suggested that women generally prefer people-facing jobs, which many computing and IT positions do not have, while men prefer jobs geared towards objects and tasks.\n\nThe gender disparity in IT is not global. The ratio of female to male computer scientists is significantly higher in India compared to the West. In Europe, Bulgaria and Romania have the highest rates of women going into computer programming. In government universities in Saudi Arabia in 2014, Arab women made up 59% of students enrolled in computer science. However, the ratio of African American female computer scientists in the US is significantly lower than the national average. It has been suggested there is a greater gap in countries where people of both sexes are treated more equally, contradicting any theories that society in general is to blame for any disparity.\n\nThe Association for Computing Machinery Turing Award, sometimes referred to as the \"Nobel Prize\" of computing, was named in honor of Alan Turing. It award has been won by three women between 1966 and 2015.\n\nThe British Computer Society Information Retrieval Specialist Group (BCS IRSG) in conjunction with the British Computer Society created an award in 2008 to commemorate the achievements of Karen Spärck Jones, a Professor Emerita of Computers and Information at the University of Cambridge and one of the most remarkable women in computer science. The KSJ award has been won by four women between 2009 and 2017:\n\n\nSeveral important groups have been established to encourage women in the IT industry. The Association for Women in Computing was one of the first and is dedicated to promoting the advancement of women in computing professions. The established in 1991 focused on increasing the number of women in Computer Science and Engineering (CSE) research and education at all levels. AnitaB.org runs the Grace Hopper Celebration of Women in Computing yearly conference. The National Center for Women & Information Technology is a nonprofit that aims to increase the number of women in technology and computing. The Women in Technology International (WITI) is a global organization dedicated to the advancement of women in business and technology.\n\nSome major societies and groups have offshoots dedicated to women. The Association for Computing Machinery's Council on Women in Computing (ACM-W) has over 36,000 members. BCSWomen is a women-only specialist group of the British Computer Society, founded in 2001. In Ireland, the charity Teen Turn run after school training and work placements for girls, and Women in Technology and Science (WITS) advocate for the inclusion and promotion of women within STEM industries.\n\nThe Women's Technology Empowerment Centre (W.TEC) is a non-profit organisation focused on providing technology education and mentoring to Nigerian women and girls. Black Girls Code is a non-profit focused on providing technology education to young African-American women.\n\nOther organisations dedicated to women in IT include Girl Develop It, a nonprofit organization that provides affordable programs for adult women interested in learning web and software development in a judgment-free environment, Girl Geek Dinners, an International group for women of all ages, Girls Who Code: a national non-profit organization dedicated to closing the gender gap in technology, LinuxChix, a women-oriented community in the open source movement and Systers, a moderated listserv dedicated to mentoring women in the IT industry.\n\n\nCitations\nSources\n\n\n"}
