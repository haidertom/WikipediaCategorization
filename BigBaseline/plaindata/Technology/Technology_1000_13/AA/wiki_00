{"id": "59052343", "url": "https://en.wikipedia.org/wiki?curid=59052343", "title": "2018 in technology and computing", "text": "2018 in technology and computing\n\nSignificant events that have occurred in 2018 in all fields of technology, including computing, robotics, electronics, as well as any other areas of technology as well, including any machines, devices, or other technological developments, occurrences, and items. \n\n\n"}
{"id": "586357", "url": "https://en.wikipedia.org/wiki?curid=586357", "title": "Artificial general intelligence", "text": "Artificial general intelligence\n\nArtificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies. Artificial general intelligence is also referred to as \"strong AI\", \"full AI\" or as the ability of a machine to perform \"general intelligent action\". Academic sources reserve \"strong AI\" to refer to machines capable of experiencing consciousness.\n\nSome references emphasize a distinction between strong AI and \"applied AI\" (also called \"narrow AI\" or \"weak AI\"): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities.\n\nAs of 2017, over forty organizations worldwide are doing active research on AGI.\n\nVarious criteria for intelligence have been proposed (most famously the Turing test) but to date, there is no definition that satisfies everyone. However, there \"is\" wide agreement among artificial intelligence researchers that intelligence is required to do the following:\n\nOther important capabilities include the ability to sense (e.g. see) and the ability to act (e.g. move and manipulate objects) in the world where intelligent behaviour is to be observed. This would include an ability to detect and respond to hazard. Many interdisciplinary approaches to intelligence (e.g. cognitive science, computational intelligence and decision making) tend to emphasise the need to consider additional traits such as imagination (taken as the ability to form mental images and concepts that were not programmed in) and autonomy.\nComputer based systems that exhibit many of these capabilities do exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent), but not yet at human levels.\n\n\nThe most difficult problems for computers are informally known as \"AI-complete\" or \"AI-hard\", implying that solving them is equivalent to the general aptitude of human intelligence, or strong AI, beyond the capabilities of a purpose-specific algorithm.\n\nAI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nAI-complete problems cannot be solved with current computer technology alone, and also require human computation. This property can be useful to test for the presence of humans, as with CAPTCHAs, and for computer security to repel brute-force attacks.\n\nModern AI research began in the mid 1950s. The first generation of AI researchers was convinced that artificial general intelligence was possible and that it would exist in just a few decades. As AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who accurately embodied what AI researchers believed they could create by the year 2001. Of note is the fact that AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time; Crevier quotes him as having said on the subject in 1967, \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved,\" although Minsky states that he was misquoted.\n\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". As the 1980s began, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who had predicted the imminent achievement of AGI had been shown to be fundamentally mistaken. By the 1990s, AI researchers had gained a reputation for making vain promises. They became reluctant to make predictions at all and to avoid any mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s].\"\n\nIn the 1990s and early 21st century, mainstream AI has achieved far greater commercial success and academic respectability by focusing on specific sub-problems where they can produce verifiable results and commercial applications, such as artificial neural networks, computer vision or data mining. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is very heavily funded in both academia and industry.\n\nMost mainstream AI researchers hope that strong AI can be developed by combining the programs that solve various sub-problems using an integrated agent architecture, cognitive architecture or subsumption architecture. Hans Moravec wrote in 1988: \"I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\"\n\nHowever, even this fundamental philosophy has been disputed; for example, Stevan Harnad of Princeton concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: \"The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\"\n\nArtificial general intelligence (AGI) describes research that aims to create machines capable of general intelligent action. The term was introduced by Mark Gubrud in 1997 in a discussion of the implications of fully automated military production and operations. The research objective is much older, for example Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project are regarded as within the scope of AGI. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". As yet, most AI researchers have devoted little attention to AGI, with some claiming that intelligence is too complex to be completely replicated in the near term. However, a small number of computer scientists are active in AGI research, and many of this group are contributing to a series of AGI conferences. The research is extremely diverse and often pioneering in nature. In the introduction to his book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century, but the consensus in the AGI research community seems to be that the timeline discussed by Ray Kurzweil in \"The Singularity is Near\" (i.e. between 2015 and 2045) is plausible. Most mainstream AI researchers doubt that progress will be this rapid. Organizations explicitly pursuing AGI include the Swiss AI lab IDSIA, Nnaisense, Vicarious, Maluuba, the OpenCog Foundation, Adaptive AI, LIDA, and Numenta and the associated Redwood Neuroscience Institute. In addition, organizations such as the Machine Intelligence Research Institute and OpenAI have been founded to influence the development path of AGI. Finally, projects such as the Human Brain Project have the goal of building a functioning simulation of the human brain. A 2017 survey of AGI categorized forty-five known \"active R&D projects\" that explicitly or implicitly (through published research) research AGI, with the largest three being DeepMind, the Human Brain Project, and OpenAI.\n\nA popular approach discussed to achieving general intelligent action is whole brain emulation. A low-level brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model so faithful to the original that it will behave in essentially the same way as the original brain, or for all practical purposes, indistinguishably. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book \"The Singularity Is Near\" predicts that a map of sufficient quality will become available on a similar timescale to the required computing power.\n\n For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 10 (one hundred billion) neurons has on average 7,000 synaptic connections to other neurons. It has been estimated that the brain of a three-year-old child has about 10 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 10 to 5×10 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 10 (100 trillion) synaptic updates per second (SUPS). In 1997 Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 10 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating point operation\" – a measure used to rate current supercomputers – then 10 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011). He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.\n\nThere are some research projects that are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 10 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures in the world, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 10 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project said in 2009 at the TED conference in Oxford. There have also been controversial claims to have simulated a cat brain. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.\n\nHans Moravec addressed the above arguments (\"brains are more complicated\", \"neurons have to be modeled in more detail\") in his 1997 paper \"When will computer hardware match the human brain?\". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.\n\nA fundamental criticism of the simulated brain approach derives from embodied cognition where human embodiment is taken as an essential aspect of human intelligence. Many researchers believe that embodiment is necessary to ground meaning. If this view is correct, any fully functional brain model will need to encompass more than just the neurons (i.e., a robotic body). Goertzel proposes virtual embodiment (like Second Life), but it is not yet known whether this would be sufficient.\n\nDesktop computers using microprocessors capable of more than 10 cps (Kurzweil's non-standard unit \"computations per second\", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), this computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists . There are at least three reasons for this:\n\nIn addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.\n\nAlthough the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.\n\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:\nThe first one is called \"the \"strong\" AI hypothesis\" and the second is \"the \"weak\" AI hypothesis\" because the first one makes the \"stronger\" statement: it assumes something special has happened to the machine that goes beyond all its abilities that we can test. Searle referred to the \"strong AI hypothesis\" as \"strong AI\". This usage is also common in academic AI research and textbooks.\n\nThe weak AI hypothesis is equivalent to the hypothesis that artificial general intelligence is possible. According to Russell and Norvig, \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nIn contrast to Searle, Kurzweil uses the term \"strong AI\" to describe any artificial intelligence system that acts like it has a mind, regardless of whether a philosopher would be able to determine if it \"actually\" has a mind or not.\n\nSince the launch of AI research in 1956, the growth of this field has slowed down over time and has stalled the aims of creating machines skilled with intelligent action at the human level. A possible explanation for this delay is that computers lack a sufficient scope of memory or processing power. In addition, the level of complexity that connects to the process of AI research may also limit the progress of AI research.\n\nWhile most AI researchers believe that strong AI can be achieved in the future, there are some individuals like Hubert Dreyfus and Roger Penrose that deny the possibility of achieving AI. John McCarthy was one of various computer scientists who believe human-level AI will be accomplished, but a date cannot accurately be predicted.\n\nConceptual limitations are another possible reason for the slowness in AI research. AI researchers may need to modify the conceptual framework of their discipline in order to provide a stronger base and contribution to the quest of achieving strong AI. As William Clocksin wrote in 2003: \"the framework starts from Weizenbaum’s observation that intelligence manifests itself only relative to specific social and cultural contexts\".\n\nFurthermore, AI researchers have been able to create computers that can perform jobs that are complicated for people to do, but conversely they have struggled to develop a computer that is capable of carrying out tasks that are simple for humans to do . A problem that is described by David Gelernter is that some people assume that thinking and reasoning are equivalent. However, the idea of whether thoughts and the creator of those thoughts are isolated individually has intrigued AI researchers.\n\nThe problems that have been encountered in AI research over the past decades have further impeded the progress of AI. The failed predictions that have been promised by AI researchers and the lack of a complete understanding of human behaviors have helped diminish the primary idea of human-level AI. Although the progress of AI research has brought both improvement and disappointment, most investigators have established optimism about potentially achieving the goal of AI in the 21st century.\n\nOther possible reasons have been proposed for the lengthy research in the progress of strong AI. The intricacy of scientific problems and the need to fully understand the human brain through psychology and neurophysiology have limited many researchers from emulating the function of the human brain into a computer hardware. Many researchers tend to underestimate any doubt that is involved with future predictions of AI, but without taking those issues seriously can people then overlook solutions to problematic questions.\n\nClocksin says that a conceptual limitation that may impede the progress of AI research is that people may be using the wrong techniques for computer programs and implementation of equipment. When AI researchers first began to aim for the goal of artificial intelligence, a main interest was human reasoning. Researchers hoped to establish computational models of human knowledge through reasoning and to find out how to design a computer with a specific cognitive task.\n\nThe practice of abstraction, which people tend to redefine when working with a particular context in research, provides researchers with a concentration on just a few concepts. The most productive use of abstraction in AI research comes from planning and problem solving. Although the aim is to increase the speed of a computation, the role of abstraction has posed questions about the involvement of abstraction operators.\n\nA possible reason for the slowness in AI relates to the acknowledgement by many AI researchers that heuristics is a section that contains a significant breach between computer performance and human performance. The specific functions that are programmed to a computer may be able to account for many of the requirements that allow it to match human intelligence. These explanations are not necessarily guaranteed to be the fundamental causes for the delay in achieving strong AI, but they are widely agreed by numerous researchers.\n\nThere have been many AI researchers that debate over the idea whether machines should be created with emotions. There are no emotions in typical models of AI and some researchers say programming emotions into machines allows them to have a mind of their own. Emotion sums up the experiences of humans because it allows them to remember those experiences. David Gelernter writes, \"No computer will be creative unless it can simulate all the nuances of human emotion.\" This concern about emotion has posed problems for AI researchers and it connects to the concept of strong AI as its research progresses into the future.\n\nThere are other aspects of the human mind besides intelligence that are relevant to the concept of strong AI which play a major role in science fiction and the ethics of artificial intelligence:\nThese traits have a moral dimension, because a machine with this form of strong AI may have legal rights, analogous to the rights of non-human animals. Also, Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and currently there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is also possible that some of these properties, such as sentience, naturally emerge from a fully intelligent machine, or that it becomes natural to \"ascribe\" these properties to machines once they begin to act in a way that is clearly intelligent. For example, intelligent action may be sufficient for sentience, rather than the other way around.\n\nIn science fiction, AGI is associated with traits such as consciousness, sentience, sapience, and self-awareness observed in living beings. However, according to philosopher John Searle, it is an open question whether general intelligence is sufficient for consciousness. \"Strong AI\" (as defined above by Ray Kurzweil) should not be confused with Searle's \"'strong AI hypothesis\". The strong AI hypothesis is the claim that a computer which behaves as intelligently as a person must also necessarily have a mind and consciousness. AGI refers only to the amount of intelligence that the machine displays, with or without a mind.\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\". However, this prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. AI experts' views on the feasibility of AGI wax and wane, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they'd be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. It is also interesting to note 16.5% of the experts answered with \"never\" when asked the same question but with a 90% confidence instead.\n\nThe creation of artificial general intelligence may have repercussions so great and so complex that it may not be possible to forecast what will come afterwards. Thus the event in the hypothetical future of achieving strong AI is called the technological singularity, because theoretically one cannot see past it. But this has not stopped philosophers and researchers from guessing what the smart computers or robots of the future may do, including forming a utopia by being our friends or overwhelming us in an AI takeover. The latter potentiality is particularly disturbing as it poses an existential risk for mankind.\n\nSmart computers or robots would be able to design and produce improved versions of themselves. A growing population of intelligent robots could conceivably out-compete inferior humans in job markets, in business, in science, in politics (pursuing robot rights), and technologically, sociologically (by acting as one), and militarily.\n\nIf research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\". It would then be even better at improving itself, and would probably continue doing so in a rapidly increasing cycle, leading to an intelligence explosion and the emergence of superintelligence. Such an intelligence would not have the limitations of human intellect, and might be able to invent or discover almost anything.\n\nHyper-intelligent software might not necessarily decide to support the continued existence of mankind, and might be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nOne proposal to deal with this is to make sure that the first generally intelligent AI is a friendly AI that would then endeavor to ensure that subsequently developed AIs were also nice to us. But friendly AI is harder to create than plain AGI, and therefore it is likely, in a race between the two, that non-friendly AI would be developed first. Also, there is no guarantee that friendly AI would remain friendly, or that its progeny would also all be good.\n\n"}
{"id": "3135176", "url": "https://en.wikipedia.org/wiki?curid=3135176", "title": "B2MML", "text": "B2MML\n\nB2MML or Business To Manufacturing Markup Language is an XML implementation of the ANSI/ISA-95 family of standards (ISA-95), known internationally as IEC/ISO 62264. B2MML consists of a set of XML schemas written using the World Wide Web Consortium's XML Schema language (XSD) that implement the data models in the ISA-95 standard.\n\nB2MML is meant to be a common data definition to link ERP and supply chain management systems with manufacturing systems such as Industrial Control Systems and Manufacturing Execution Systems.\n\nB2MML is published by the Manufacturing Enterprise Solutions Association (MESA).\n\n"}
{"id": "2473812", "url": "https://en.wikipedia.org/wiki?curid=2473812", "title": "Ball and beam", "text": "Ball and beam\n\nThe ball and beam system consists of a long beam which can be tilted by a servo or electric motor together with a ball\nrolling back and forth on top of the beam.\nIt is a popular textbook example in control theory.\n\nThe significance of the ball and beam system is that it is a simple system which is open-loop unstable.\nEven if the beam is restricted to be very nearly horizontal, without active feedback, it will swing to one side or the\nother, and the ball will roll off the end of the beam. To stabilize the ball, a control system which\nmeasures the position of the ball and adjusts the beam accordingly must be used.\n\nIn two dimensions, the ball and beam system becomes the ball and plate system, where a ball rolls on top of\na plate whose inclination can be adjusted by tilting it frontwards, backwards, leftwards, or rightwards.\n\n"}
{"id": "4031", "url": "https://en.wikipedia.org/wiki?curid=4031", "title": "Buckminster Fuller", "text": "Buckminster Fuller\n\nRichard Buckminster \"Bucky\" Fuller (; July 12, 1895 – July 1, 1983) was an American architect, systems theorist, author, designer, inventor and futurist.\n\nFuller published more than 30 books, coining or popularizing terms such as \"Spaceship Earth\", \"Dymaxion\" house/car, ephemeralization, synergetic, and \"tensegrity\". He also developed numerous inventions, mainly architectural designs, and popularized the widely known geodesic dome. Carbon molecules known as fullerenes were later named by scientists for their structural and mathematical resemblance to geodesic spheres.\n\nFuller was the second World President of Mensa from 1974 to 1983.\n\nFuller was born on July 12, 1895, in Milton, Massachusetts, the son of Richard Buckminster Fuller and Caroline Wolcott Andrews, and grand-nephew of Margaret Fuller, an American journalist, critic, and women's rights advocate associated with the American transcendentalism movement. The unusual middle name, Buckminster, was an ancestral family name. As a child, Richard Buckminster Fuller tried numerous variations of his name. He used to sign his name differently each year in the guest register of his family summer vacation home at Bear Island, Maine. He finally settled on R. Buckminster Fuller.\n\nFuller spent much of his youth on Bear Island, in Penobscot Bay off the coast of Maine. He attended Froebelian Kindergarten. He had trouble with geometry, being unable to understand the abstraction that a chalk dot on the blackboard represented a mathematical point, or that an imperfectly drawn line with an arrow on the end was meant to stretch off to infinity. He often made items from materials he found in the woods, and sometimes made his own tools. He experimented with designing a new apparatus for human propulsion of small boats. By age 12, he had invented a 'push pull' system for propelling a rowboat by use of an inverted umbrella connected to the transom with a simple oar lock which allowed the user to face forward to point the boat toward its destination. Later in life, Fuller took exception to the term \"invention\".\n\nYears later, he decided that this sort of experience had provided him with not only an interest in design, but also a habit of being familiar with and knowledgeable about the materials that his later projects would require. Fuller earned a machinist's certification, and knew how to use the press brake, stretch press, and other tools and equipment used in the sheet metal trade.\n\nFuller attended Milton Academy in Massachusetts, and after that began studying at Harvard College, where he was affiliated with Adams House. He was expelled from Harvard twice: first for spending all his money partying with a vaudeville troupe, and then, after having been readmitted, for his \"irresponsibility and lack of interest.\" By his own appraisal, he was a non-conforming misfit in the fraternity environment.\n\nBetween his sessions at Harvard, Fuller worked in Canada as a mechanic in a textile mill, and later as a laborer in the meat-packing industry. He also served in the U.S. Navy in World War I, as a shipboard radio operator, as an editor of a publication, and as a crash rescue boat commander. After discharge, he worked again in the meat packing industry, acquiring management experience. In 1917, he married Anne Hewlett. During the early 1920s, he and his father-in-law developed the Stockade Building System for producing light-weight, weatherproof, and fireproof housing—although the company would ultimately fail in 1927.\n\nBuckminster Fuller recalled 1927 as a pivotal year of his life. His daughter Alexandra had died in 1922 of complications from polio and spinal meningitis just before her fourth birthday. Fuller dwelled on her death, suspecting that it was connected with the Fullers' damp and drafty living conditions. This provided motivation for Fuller's involvement in Stockade Building Systems, a business which aimed to provide affordable, efficient housing.\n\nIn 1927, at age 32, Fuller lost his job as president of Stockade. The Fuller family had no savings, and the birth of their daughter Allegra in 1927 added to the financial challenges. Fuller drank heavily and reflected upon the solution to his family's struggles on long walks around Chicago. During the autumn of 1927, Fuller contemplated suicide by drowning in Lake Michigan, so that his family could benefit from a life insurance payment.\n\nFuller said that he had experienced a profound incident which would provide direction and purpose for his life. He felt as though he was suspended several feet above the ground enclosed in a white sphere of light. A voice spoke directly to Fuller, and declared:\n\nFuller stated that this experience led to a profound re-examination of his life. He ultimately chose to embark on \"an experiment, to find what a single individual [could] contribute to changing the world and benefiting all humanity.\"\n\nSpeaking to audiences later in life, Fuller would regularly recount the story of his Lake Michigan experience, and its transformative impact on his life. Historians have been unable to identify direct evidence for this experience within the 1927 papers of Fuller's Chronofile archives, housed at Stanford University. Stanford historian Barry Katz suggests that the suicide story may be a myth which Fuller constructed later in life, to summarize this formative period of his career.\n\nIn 1927 Fuller resolved to think independently which included a commitment to \"the search for the principles governing the universe and help advance the evolution of humanity in accordance with them ... finding ways of \"doing more with less\" to the end that all people everywhere can have more and more.\" By 1928, Fuller was living in Greenwich Village and spending much of his time at the popular café Romany Marie's, where he had spent an evening in conversation with Marie and Eugene O'Neill several years earlier. Fuller accepted a job decorating the interior of the café in exchange for meals, giving informal lectures several times a week, and models of the Dymaxion house were exhibited at the café. Isamu Noguchi arrived during 1929—Constantin Brâncuși, an old friend of Marie's, had directed him there—and Noguchi and Fuller were soon collaborating on several projects, including the modeling of the Dymaxion car based on recent work by Aurel Persu. It was the beginning of their lifelong friendship.\n\nFuller taught at Black Mountain College in North Carolina during the summers of 1948 and 1949, serving as its Summer Institute director in 1949. There, with the support of a group of professors and students, he began reinventing a project that would make him famous: the geodesic dome. Although the geodesic dome had been created 26 years earlier by Dr. Walther Bauersfeld, Fuller was awarded United States patents, even though he neglected to cite Bauersfeld's prior art in his patent applications. He is credited for popularizing this type of structure.\n\nOne of his early models was first constructed in 1945 at Bennington College in Vermont, where he lectured often. In 1949, he erected his first geodesic dome building that could sustain its own weight with no practical limits. It was in diameter and constructed of aluminium aircraft tubing and a vinyl-plastic skin, in the form of an icosahedron. To prove his design, Fuller suspended from the structure's framework several students who had helped him build it. The U.S. government recognized the importance of his work, and employed his firm Geodesics, Inc. in Raleigh, North Carolina to make small domes for the Marines. Within a few years, there were thousands of such domes around the world.\n\nFuller's first \"continuous tension – discontinuous compression\" geodesic dome (full sphere in this case) was constructed at the University of Oregon Architecture School in 1959 with the help of students. These continuous tension – discontinuous compression structures featured single force compression members (no flexure or bending moments) that did not touch each other and were 'suspended' by the tensional members.\n\nFor half of a century, Fuller developed many ideas, designs and inventions, particularly regarding practical, inexpensive shelter and transportation. He documented his life, philosophy and ideas scrupulously by a daily diary (later called the \"Dymaxion Chronofile\"), and by twenty-eight publications. Fuller financed some of his experiments with inherited funds, sometimes augmented by funds invested by his collaborators, one example being the Dymaxion car project.\n\nInternational recognition began with the success of huge geodesic domes during the 1950s. Fuller lectured at NC State University in Raleigh in 1949, where he met James Fitzgibbon, who would become a close friend and colleague. Fitzgibbon was director of Geodesics, Inc. and Synergetics, Inc. the first licensees to design geodesic domes. Thomas C. Howard was lead designer, architect and engineer for both companies.\n\nFuller began working with architect Shoji Sadao in 1954, and in 1964 they co-founded the architectural firm Fuller & Sadao Inc., whose first project was to design the large geodesic dome for the U.S. Pavilion at Expo 67 in Montreal. This building is now the \"Montreal Biosphère\".\nIn 1962, the artist and searcher John McHale wrote the first monograph on Fuller, published by George Braziller in New York.\n\nFrom 1959 to 1970, Fuller taught at Southern Illinois University Carbondale (SIU). Beginning as an assistant professor, he gained full professorship in 1968, in the School of Art and Design. Working as a designer, scientist, developer, and writer, he lectured for many years around the world. He collaborated at SIU with John McHale. In 1965, they inaugurated the World Design Science Decade (1965 to 1975) at the meeting of the International Union of Architects in Paris, which was, in Fuller's own words, devoted to \"applying the principles of science to solving the problems of humanity.\" Later in his SIU tenure, Fuller was also a visiting professor at SIU Edwardsville, where he designed the dome for the campus Religious Center.\n\nFuller believed human societies would soon rely mainly on renewable sources of energy, such as solar- and wind-derived electricity. He hoped for an age of \"omni-successful education and sustenance of all humanity.\" Fuller referred to himself as \"the property of universe\" and during one radio interview he gave later in life, declared himself and his work \"the property of all humanity\". For his lifetime of work, the American Humanist Association named him the 1969 Humanist of the Year.\n\nIn 1976, Fuller was a key participant at UN Habitat I, the first UN forum on human settlements.\n\nFuller was awarded 28 United States patents and many honorary doctorates. In 1960, he was awarded the Frank P. Brown Medal from The Franklin Institute. Fuller was elected as an honorary member of Phi Beta Kappa in 1967, on the occasion of the 50th year reunion of his Harvard class of 1917 (from which he was expelled in his first year). He was elected a Fellow of the American Academy of Arts and Sciences in 1968. In 1968, he was elected into the National Academy of Design as an Associate member, and became a full Academician in 1970. In 1970, he received the Gold Medal award from the American Institute of Architects. In 1976, he received the St. Louis Literary Award from the Saint Louis University Library Associates. He also received numerous other awards, including the Presidential Medal of Freedom presented to him on February 23, 1983, by President Ronald Reagan.\nFuller's last filmed interview took place on April 3, 1983, in which he presented his analysis of Simon Rodia's Watts Towers as a unique embodiment of the structural principles found in nature. Portions of this interview appear in \"I Build the Tower,\" a documentary film on Rodia's architectural masterpiece.\n\nFuller died on July 1, 1983, 11 days before his 88th birthday. During the period leading up to his death, his wife had been lying comatose in a Los Angeles hospital, dying of cancer. It was while visiting her there that he exclaimed, at a certain point: \"She is squeezing my hand!\" He then stood up, suffered a heart attack, and died an hour later, at age 87. His wife of 66 years died 36 hours later. They are buried in Mount Auburn Cemetery in Cambridge, Massachusetts.\n\nBuckminster Fuller was a Unitarian like his grandfather, Unitarian minister Arthur Buckminster Fuller, He was an early environmental activist. He was aware of the Earth's finite resources, and promoted a principle that he termed \"ephemeralization\", which according to futurist and Fuller disciple Stewart Brand, he coined to mean \"doing more with less\". Resources and waste from cruder products could be recycled into making more valuable products, increasing the efficiency of the entire process. Fuller also introduced synergetics, a term which he used broadly as a metaphor for communicating experiences using geometric concepts, and more specifically the empirical study of systems in transformation, with an emphasis on total system behavior unpredicted by the behavior of any isolated components. Fuller coined this term long before the term synergy became popular.\n\nFuller was a pioneer in thinking globally, and he explored principles of energy and material efficiency in the fields of architecture, engineering and design. He cited François de Chardenèdes' opinion that petroleum, from the standpoint of its replacement cost out of our current energy \"budget\" (essentially, the net incoming solar flux), had cost nature \"over a million dollars\" per U.S. gallon (US$300,000 per litre) to produce. From this point of view, its use as a transportation fuel by people commuting to work represents a huge net loss compared to their earnings. An encapsulation quotation of his views might be, \"There is no energy crisis, only a crisis of ignorance.\"\n\nFuller was concerned about sustainability and human survival under the existing socio-economic system, yet remained optimistic about humanity's future. Defining wealth in terms of knowledge, as the \"technological ability to protect, nurture, support, and accommodate all growth needs of life,\" his analysis of the condition of \"Spaceship Earth\" caused him to conclude that at a certain time during the 1970s, humanity had attained an unprecedented state. He was convinced that the accumulation of relevant knowledge, combined with the quantities of major recyclable resources that had already been extracted from the earth, had attained a critical level, such that competition for necessities had become unnecessary. Cooperation had become the optimum survival strategy. He declared: \"selfishness is unnecessary and hence-forth unrationalizable ... War is obsolete.\" He criticized previous utopian schemes as too exclusive, and thought this was a major source of their failure. To work, he thought that a utopia needed to include everyone.\n\nFuller was influenced by Alfred Korzybski's idea of general semantics. In the 1950s, Fuller attended seminars and workshops organized by the Institute of General Semantics, and he delivered the annual Alfred Korzybski Memorial Lecture in 1955. Korzybski is mentioned in the Introduction of his book \"Synergetics\". The two shared a remarkable amount of similarity in their formulations of general semantics.\n\nIn his 1970 book \"I Seem To Be a Verb\", he wrote: \"I live on Earth at present, and I don't know what I am. I know that I am not a category. I am not a thing—a noun. I seem to be a verb, an evolutionary process—an integral function of the universe.\"\n\nFuller wrote that the natural analytic geometry of the universe was based on arrays of tetrahedra. He developed this in several ways, from the close-packing of spheres and the number of compressive or tensile members required to stabilize an object in space. One confirming result was that the strongest possible homogeneous truss is cyclically tetrahedral.\n\nHe had become a guru of the design, architecture, and 'alternative' communities, such as Drop City, the community of experimental artists to whom he awarded the 1966 \"Dymaxion Award\" for \"poetically economic\" domed living structures.\n\nFuller was most famous for his lattice shell structures – geodesic domes, which have been used as parts of military radar stations, civic buildings, environmental protest camps and exhibition attractions. An examination of the geodesic design by Walther Bauersfeld for the Zeiss-Planetarium, built some 28 years prior to Fuller's work, reveals that Fuller's Geodesic Dome patent (U.S. 2,682,235; awarded in 1954), is the same design as Bauersfeld's.\n\nTheir construction is based on extending some basic principles to build simple \"tensegrity\" structures (tetrahedron, octahedron, and the closest packing of spheres), making them lightweight and stable. The geodesic dome was a result of Fuller's exploration of nature's constructing principles to find design solutions. The Fuller Dome is referenced in the Hugo Award-winning novel \"Stand on Zanzibar\" by John Brunner, in which a geodesic dome is said to cover the entire island of Manhattan, and it floats on air due to the hot-air balloon effect of the large air-mass under the dome (and perhaps its construction of lightweight materials).\n\nThe Dymaxion car was a vehicle designed by Fuller, featured prominently at Chicago's 1933-1934 Century of Progress World's Fair. During the Great Depression, Fuller formed the \"Dymaxion Corporation\" and built three prototypes with noted naval architect Starling Burgess and a team of 27 workmen — using donated money as well as a family inheritance.\n\nFuller associated the word \"Dymaxion\" with much of his work, a portmanteau of the words dynamic\", maximum\", and \"tension\" to sum up the goal of his study, \"maximum gain of advantage from minimal energy input.\"\n\nThe Dymaxion was not an automobile \"per se\", but rather the 'ground-taxying mode' of a vehicle that might one day be designed to fly, land and drive — an \"Omni-Medium Transport\" for air, land and water. Fuller focused on the landing and taxiing qualities, and noted severe limitations in its handling. The team made constant improvements and refinements to the platform, and Fuller noted the Dymaxion \"was an invention that could not be made available to the general public without considerable improvements.\"\n\nThe bodywork was aerodynamically designed for increased fuel efficiency and speed as well as light weight, and its platform featured a lightweight cromoly-steel hinged chassis, rear-mounted V8 engine, front-drive and three-wheels. The vehicle was steered via the third wheel at the rear, capable of 90° steering lock. Thus able to steer in a tight circle, the Dymaxion often caused a sensation, bringing nearby traffic to a halt.\n\nShortly after launch, a prototype crashed after being hit by another car, killing the Dymaxion's driver. The other car was driven by a local politician and was illegally removed from the accident scene, leaving reporters who arrived subsequently to blame the Dymaxion's unconventional design — though investigations exonerated the prototype. Fuller would himself later crash another prototype with his young daughter aboard.\n\nDespite courting the interest of important figures from the auto industry, Fuller used his family inheritance to finish the second and third prototypes — eventually selling all three, dissolving \"Dymaxion Corporation\" and maintaining the Dymaxion was never intended as a commercial venture. One of the three original prototypes survives.\n\nFuller's energy-efficient and inexpensive Dymaxion house garnered much interest, but only two prototypes were ever produced. Here the term \"Dymaxion\" is used in effect to signify a \"radically strong and light tensegrity structure\". One of Fuller's Dymaxion Houses is on display as a permanent exhibit at the Henry Ford Museum in Dearborn, Michigan. Designed and developed during the mid-1940s, this prototype is a round structure (not a dome), shaped something like the flattened \"bell\" of certain jellyfish. It has several innovative features, including revolving dresser drawers, and a fine-mist shower that reduces water consumption. According to Fuller biographer Steve Crooks, the house was designed to be delivered in two cylindrical packages, with interior color panels available at local dealers. A circular structure at the top of the house was designed to rotate around a central mast to use natural winds for cooling and air circulation.\n\nConceived nearly two decades earlier, and developed in Wichita, Kansas, the house was designed to be lightweight, adapted to windy climates, cheap to produce and easy to assemble. Because of its light weight and portability, the Dymaxion House was intended to be the ideal housing for individuals and families who wanted the option of easy mobility. The design included a \"Go-Ahead-With-Life Room\" stocked with maps, charts, and helpful tools for travel \"through time and space.\" It was to be produced using factories, workers, and technologies that had produced World War II aircraft. It looked ultramodern at the time, built of metal, and sheathed in polished aluminum. The basic model enclosed of floor area. Due to publicity, there were many orders during the early Post-War years, but the company that Fuller and others had formed to produce the houses failed due to management problems.\n\nIn 1967, Fuller developed a concept for an offshore floating city named Triton City and published a report on the design the following year. Models of the city aroused the interest of President Lyndon B. Johnson who, after leaving office, had them placed in the Lyndon Baines Johnson Library and Museum.\n\nIn 1969, Fuller began the Otisco Project, named after its location in Otisco, New York. The project developed and demonstrated concrete spray with mesh-covered wireforms for producing large-scale, load-bearing spanning structures built on-site, without the use of pouring molds, other adjacent surfaces or hoisting. The initial method used a circular concrete footing in which anchor posts were set. Tubes cut to length and with ends flattened were then bolted together to form a duodeca-rhombicahedron (22-sided hemisphere) geodesic structure with spans ranging to . The form was then draped with layers of ¼-inch wire mesh attached by twist ties. Concrete was sprayed onto the structure, building up a solid layer which, when cured, would support additional concrete to be added by a variety of traditional means. Fuller referred to these buildings as monolithic ferroconcrete geodesic domes. However, the tubular frame form proved problematic for setting windows and doors. It was replaced by an iron rebar set vertically in the concrete footing and then bent inward and welded in place to create the dome's wireform structure and performed satisfactorily. Domes up to three stories tall built with this method proved to be remarkably strong. Other shapes such as cones, pyramids and arches proved equally adaptable.\n\nThe project was enabled by a grant underwritten by Syracuse University and sponsored by US Steel (rebar), the Johnson Wire Corp, (mesh) and Portland Cement Company (concrete). The ability to build large complex load bearing concrete spanning structures in free space would open many possibilities in architecture, and is considered as one of Fuller's greatest contributions.\n\nFuller, along with co-cartographer Shoji Sadao, also designed an alternative projection map, called the Dymaxion map. This was designed to show Earth's continents with minimum distortion when projected or printed on a flat surface.\n\nIn the 1960s, Fuller developed the World Game, a collaborative simulation game played on a 70-by-35-foot Dymaxion map, in which players attempt to solve world problems. The object of the simulation game is, in Fuller's words, to \"make the world work, for 100% of humanity, in the shortest possible time, through spontaneous cooperation, without ecological offense or the disadvantage of anyone.\"\n\nBuckminster Fuller wore thick-lensed spectacles to correct his extreme hyperopia, a condition that went undiagnosed for the first five years of his life. Fuller's hearing was damaged during his Naval service in World War I and deteriorated during the 1960s. After experimenting with bullhorns as hearing aids during the mid-1960s, Fuller adopted electronic hearing aids from the 1970s onward.\n\nIn public appearances, Fuller always wore dark-colored suits, appearing like \"an alert little clergyman\". Previously, he had experimented with unconventional clothing immediately after his 1927 epiphany, but found that breaking social fashion customs made others devalue or dismiss his ideas. Fuller learned the importance of physical appearance as part of one's credibility, and decided to become \"the invisible man\" by dressing in clothes that would not draw attention to himself. With self-deprecating humor, Fuller described this black-suited appearance as resembling a \"second-rate bank clerk\".\n\nWriter Guy Davenport met him in 1965 and described him thus: \"He's a dwarf, with a worker's hands, all callouses and squared fingers. He carries an ear trumpet, of green plastic, with WORLD SERIES 1965 printed on it. His smile is golden and frequent; the man's temperament is angelic, and his energy is just a touch more than that of [Robert] Gallway (champeen runner, footballeur, and swimmer). One leg is shorter than the other, and the prescription shoe worn to correct the imbalance comes from a country doctor deep in the wilderness of Maine. Blue blazer, Khrushchev trousers, and a briefcase full of Japanese-made wonderments; ...\"\n\nFollowing his global prominence from the 1960s onward, Fuller became a frequent flier, often crossing time zones to lecture. In the 1960s and 1970s, he wore three watches simultaneously; one for the time zone of his office in Carbondale, one for the time zone of the location he would next visit, and one for the time zone he was currently in. In the 1970s, Fuller was only in 'homely' locations (his personal home in Carbondale, Illinois; his holiday retreat in Bear Island, Maine; his daughter's home in Pacific Palisades, California) roughly 65 nights per year—the other 300 nights were spent in hotel beds in the locations he visited on his lecturing and consulting circuits.\n\nIn the 1920s, Fuller experimented with polyphasic sleep, which he called \"Dymaxion sleep\". Inspired by the sleep habits of animals such as dogs and cats, Fuller worked until he was tired, and then slept short naps. This generally resulted in Fuller sleeping 30-minute naps every 6 hours. This allowed him \"twenty-two thinking hours a day\", which aided his work productivity. Fuller reportedly kept this Dymaxion sleep habit for two years, before quitting the routine because it conflicted with his business associates' sleep habits. Despite no longer personally partaking in the habit, in 1943 Fuller suggested Dymaxion sleep as a strategy that the United States could adopt to win World War II.\n\nDespite only practicing true polyphasic sleep for a period during the 1920s, Fuller was known for his stamina throughout his life. He was described as \"tireless\" by Barry Farrell in \"Life\" magazine, who noted that Fuller stayed up all night replying to mail during Farrell's 1970 trip to Bear Island. In his seventies, Fuller generally slept for 5–8 hours per night.\n\nFuller documented his life copiously from 1915 to 1983, approximately of papers in a collection called the Dymaxion Chronofile. He also kept copies of all incoming and outgoing correspondence. The enormous Fuller Collection is currently housed at Stanford University.\n\nIn his youth, Fuller experimented with several ways of presenting himself: R. B. Fuller, Buckminster Fuller, but as an adult finally settled on R. Buckminster Fuller, and signed his letters as such. However, he preferred to be addressed as simply \"Bucky\".\n\nBuckminster Fuller spoke and wrote in a unique style and said it was important to describe the world as accurately as possible. Fuller often created long run-on sentences and used unusual compound words (omniwell-informed, intertransformative, omni-interaccommodative, omniself-regenerative) as well as terms he himself invented.\n\nFuller used the word \"Universe\" without the definite or indefinite articles (\"the\" or \"a\") and always capitalized the word. Fuller wrote that \"by Universe I mean: the aggregate of all humanity's consciously apprehended and communicated (to self or others) Experiences.\"\n\nThe words \"down\" and \"up\", according to Fuller, are awkward in that they refer to a planar concept of direction inconsistent with human experience. The words \"in\" and \"out\" should be used instead, he argued, because they better describe an object's relation to a gravitational center, the Earth. \"I suggest to audiences that they say, 'I'm going \"outstairs\" and \"instairs.\"' At first that sounds strange to them; They all laugh about it. But if they try saying in and out for a few days in fun, they find themselves beginning to realize that they are indeed going inward and outward in respect to the center of Earth, which is our Spaceship Earth. And for the first time they begin to feel real 'reality.'\"\n\n\"World-around\" is a term coined by Fuller to replace \"worldwide\". The general belief in a flat Earth died out in classical antiquity, so using \"wide\" is an anachronism when referring to the surface of the Earth—a spheroidal surface has area and encloses a volume but has no width. Fuller held that unthinking use of obsolete scientific ideas detracts from and misleads intuition. Other neologisms collectively invented by the Fuller family, according to Allegra Fuller Snyder, are the terms \"sunsight\" and \"sunclipse\", replacing \"sunrise\" and \"sunset\" to overturn the geocentric bias of most pre-Copernican celestial mechanics.\n\nFuller also invented the word \"livingry,\" as opposed to weaponry (or \"killingry\"), to mean that which is in support of all human, plant, and Earth life. \"The architectural profession—civil, naval, aeronautical, and astronautical—has always been the place where the most competent thinking is conducted regarding livingry, as opposed to weaponry.\"\n\nAs well as contributing significantly to the development of tensegrity technology, Fuller invented the term \"tensegrity\" from \"tensional integrity\". \"Tensegrity describes a structural-relationship principle in which structural shape is guaranteed by the finitely closed, comprehensively continuous, tensional behaviors of the system and not by the discontinuous and exclusively local compressional member behaviors. Tensegrity provides the ability to yield increasingly without ultimately breaking or coming asunder.\"\n\n\"Dymaxion\" is a portmanteau of \"dynamic maximum tension\". It was invented about 1929 by two admen at Marshall Field's department store in Chicago to describe Fuller's concept house, which was shown as part of a house of the future store display. They created the term utilizing three words that Fuller used repeatedly to describe his design – dynamic, maximum, and tension.\n\nFuller also helped to popularize the concept of Spaceship Earth: \"The most important fact about Spaceship Earth: an instruction manual didn't come with it.\"\n\nHis concepts and buildings include:\nAmong the many people who were influenced by Buckminster Fuller are:\nConstance Abernathy,\nRuth Asawa,\nJ. Baldwin,\nMichael Ben-Eli,\nPierre Cabrol,\nJohn Cage,\nJoseph Clinton,\nPeter Floyd,\nMedard Gabel,\nMichael Hays,\nDavid Johnston,\nRobert Kiyosaki,\nPeter Jon Pearce,\nShoji Sadao,\nEdwin Schlossberg,\nKenneth Snelson,\nRobert Anton Wilson and Stewart Brand.\n\nAn allotrope of carbon, fullerene—and a particular molecule of that allotrope C (buckminsterfullerene or buckyball) has been named after him. The Buckminsterfullerene molecule, which consists of 60 carbon atoms, very closely resembles a spherical version of Fuller's geodesic dome. The 1996 Nobel prize in chemistry was given to Kroto, Curl, and Smalley for their discovery of the fullerene.\n\nHe is quoted in the lyric of \"The Tower of Babble\" in the musical \"Godspell\": \"Man is a complex of patterns and processes.\"\n\nThe indie band Driftless Pony Club named their 2011 album, \"Buckminster\", after him. All the songs within the album are based upon his life and works.\n\nOn July 12, 2004, the United States Post Office released a new commemorative stamp honoring R. Buckminster Fuller on the 50th anniversary of his patent for the geodesic dome and by the occasion of his 109th birthday. The stamp's design replicated the January 10, 1964 cover of \"Time Magazine\".\n\nFuller was the subject of two documentary films: \"The World of Buckminster Fuller\" (1971) and \"\" (1996). Additionally, filmmaker Sam Green and the band Yo La Tengo collaborated on a 2012 \"live documentary\" about Fuller, \"The Love Song of R. Buckminster Fuller\".\n\nIn June 2008, the Whitney Museum of American Art presented \"Buckminster Fuller: Starting with the Universe\", the most comprehensive retrospective to date of his work and ideas. The exhibition traveled to the Museum of Contemporary Art, Chicago in 2009. It presented a combination of models, sketches, and other artifacts, representing six decades of the artist's integrated approach to housing, transportation, communication, and cartography. It also featured the extensive connections with Chicago from his years spent living, teaching, and working in the city.\n\nRobert Kiyosaki's 2015 book \"Second Chance\" is largely about Kiyosaki's interactions with Fuller, and Fuller's unusual final book \"Grunch of Giants\".\n\nIn 2012, the San Francisco Museum of Modern Art hosted \"The Utopian Impulse\" – a show about Buckminster Fuller's influence in the Bay Area. Featured were concepts, inventions and designs for creating \"free energy\" from natural forces, and for sequestering carbon from the atmosphere. The show ran January through July.\n\nIn a different note, Fuller's quote \"Those who play with the Devil's toys, will be brought by degree to wield his sword\" was used and referenced as the first display seen in the strategy sci-fi video game \"\" developed by Firaxis Games.\n\n\"The House of Tomorrow\", is a 2017 American independent drama film written and directed by Peter Livolsi, based on Peter Bognanni's 2010 novel of the same name, featuring Asa Butterfield, Alex Wolff, Nick Offerman, Maude Apatow, and Ellen Burstyn. Burstyn's character is obsessed by all things Buckminster Fuller providing retro-futurist tours of her geodesic home, including authentic video of Buckminster Fuller talking and sailing with Ellen Burstyn, who'd actually befriended him in real life.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6987714", "url": "https://en.wikipedia.org/wiki?curid=6987714", "title": "Cebu IT Park", "text": "Cebu IT Park\n\nThe Cebu IT Park (formerly known as Asiatown IT Park) is a 27-hectare mixed use business park in Cebu City, Philippines, envisioned to attract locators in the information technology services. It is developed by \"Cebu Property Ventures and Development Corporation\", a subsidiary of Cebu Holdings, Inc.\n\nTenants include Cebu Bombardier, NEC, SPI Tech, 1&1 Internet Philippines, Inc., Aegis (now acquired by Teleperformance), Convergys (who later acquired both eTelecare and Stream), Qualfon, Promotional USB, Accenture, NCR, IBM, Microsoft, Xlibris/Author Solutions, JP Morgan Chase, and Epson. The main infrastructures found at the park are i1, i2, i3, E-BLOC, TGU Tower, PIPC 15, Skyrise 3, Skyrise 2, Skyrise 1, E-Office One, PIPC 11, PIPC 14, Globe IT, Aegis Tower, the CJRS and The Central Bloc.\n\nIn January 2010, IBM inaugurated its 2nd Global Delivery Center at TGU Tower. IBM established its initial presence in the Philippines in 1937. In 2007, IBM partnered with the Philippines Department of Science and Technology on the Philippine Intellectual Property Policy Strategy, Engineering Research & Development for Technology Program, and the National Technology Business Incubators Program. IBM Philippines Country General Manager James Velasquez said the company recognizes Cebu as the gateway both for its domestic clients in the Visayas and Mindanao, as well as overseas clients.\n\nIt was approved by the Philippine Economic Zone Authority (PEZA) board as an economic zone on April 6, 2000. On February 27, 2001, Presidential Proclamation No. 12 made it an Information Technology Special Economic Zone.\n\nConstruction on eOffice One, the first office modules in Cebu IT Park, began in 2001 and opened in 2002.\n\n\n"}
{"id": "45960", "url": "https://en.wikipedia.org/wiki?curid=45960", "title": "Cloaking device", "text": "Cloaking device\n\nA cloaking device is a hypothetical or fictional stealth technology that can cause objects, such as spaceships or individuals, to be partially or wholly invisible to parts of the electromagnetic (EM) spectrum. However, over the entire spectrum, a cloaked object scatters more than an uncloaked object.\n\nFictional cloaking devices have been used as plot devices in various media for many years.\n\nDevelopments in scientific research show that real-world cloaking devices can obscure objects from at least one wavelength of EM emissions. Scientists already use artificial materials called metamaterials to bend light around an object.\n\n\"\" screenwriter Paul Schneider, inspired in part by the 1958 film \"Run Silent, Run Deep,\" and in part by \"The Enemy Below,\" which in turn had been released the previous year, 1957, imagined cloaking as a space-travel analog of a submarine submerging, and employed it in the 1966 \"Star Trek\" episode \"Balance of Terror\", in which he introduced the Romulan species. (He likewise predicted, in the same episode, that invisibility, \"selective bending of light\" as described above, would have an enormous power requirement.) Another \"Star Trek\" screenwriter, D.C. Fontana, coined the term \"cloaking device\" for the 1968 episode \"The \"Enterprise\" Incident\", which also featured Romulans.\n\nWriters and game designers have since incorporated cloaking devices into many other science-fiction narratives, including \"Doctor Who\", \"Star Wars\", and \"Stargate\".\n\nAn operational, non-fictional cloaking device might be an extension of the basic technologies used by stealth aircraft, such as radar-absorbing dark paint, optical camouflage, cooling the outer surface to minimize electromagnetic emissions (usually infrared), or other techniques to minimize other EM emissions, and to minimize particle emissions from the object. The use of certain devices to jam and confuse remote sensing devices would greatly aid in this process, but is more properly referred to as \"active camouflage\". Alternatively, metamaterials provide the theoretical possibility of making electromagnetic radiation pass freely around the 'cloaked' object.\n\nOptical metamaterials have featured in several recent proposals for invisibility schemes. \"Metamaterials\" refers to materials that owe their refractive properties to the way they are structured, rather than the substances that compose them. Using transformation optics it is possible to design the optical parameters of a \"cloak\" so that it guides light around some region, rendering it invisible over a certain band of wavelengths.\n\nThese spatially varying optical parameters do not correspond to any natural material, but may be implemented using metamaterials. There are several theories of cloaking, giving rise to different types of invisibility.\nIn 2014, scientists demonstrated good cloaking performance in murky water, demonstrating that an object shrouded in fog can disappear completely when appropriately coated with metamaterial. This is due to the random scattering of light, such as that which occurs in clouds, fog, milk, frosted glass, etc., combined with the properties of the metamaterial coating. When light is diffused, a thin coat of metamaterial around an object can make it essentially invisible under a range of lighting conditions.\n\n\"Active camouflage\" (or \"adaptive camouflage\") is a group of camouflage technologies which would allow an object (usually military in nature) to blend into its surroundings by use of panels or coatings capable of changing color or luminosity. Active camouflage can be seen as having the potential to become the perfection of the art of camouflaging things from visual detection.\n\n\"Optical camouflage\" is a kind of active camouflage in which one wears a fabric which has an image of the scene directly behind the wearer projected onto it, so that the wearer appears invisible. The drawback to this system is that, when the cloaked wearer moves, a visible distortion is often generated as the 'fabric' catches up with the object's motion. The concept exists for now only in theory and in proof-of-concept prototypes, although many experts consider it technically feasible.\n\nIt has been reported that the British Army has tested an invisible tank. Mercedes demonstrated an invisible car using LED and camera in 2012.\n\nPlasma at certain density ranges absorbs certain bandwidths of broadband waves, potentially rendering an object invisible. However, generating plasma in air is too expensive and a feasible alternative is generating plasma between thin membranes instead. The Defense Technical Information Center is also following up research on plasma reducing RCS technologies. A plasma cloaking device was patented in 1991.\n\nA prototype Metascreen is a claimed cloaking device, which is just few micrometers thick and to a limited extent can hide 3D objects from microwaves in their natural environment, in their natural positions, in all directions, and from all of the observer's positions. It was prepared at the University of Texas, Austin by Professor Andrea Alù.\n\nThe metascreen consisted of a 66 micrometre thick polycarbonate film supporting an arrangement of 20 micrometer thick copper strips that resembled a fishing net. In the experiment, when the metascreen was hit by 3.6 GHz microwaves, it re-radiated microwaves of the same frequency that were out of phase, thus cancelling out reflections from the object being hidden. The device only cancelled out the scattering of microwaves in the first order. The same researchers published a paper on \"plasmonic cloaking\" the previous year.\n\nUniversity of Rochester physics professor John Howell and graduate student Joseph Choi have announced a scalable cloaking device which uses common optical lenses to achieve visible light cloaking on the macroscopic scale, known as the \"Rochester Cloak\". The device consists of a series of four lenses which direct light rays around objects which would otherwise occlude the optical pathway.\n\nThe concepts of cloaking are not limited to optics but can also be transferred to other fields of physics. For example, it was possible to cloak acoustics for certain frequencies as well as touching in mechanics. This renders an object \"invisible\" to sound or even hides it from touching.\n\n\n\n"}
{"id": "42579502", "url": "https://en.wikipedia.org/wiki?curid=42579502", "title": "Clover Network", "text": "Clover Network\n\nClover is a cloud-based Android point of sale (POS) platform that was launched in April 2012. The company is headquartered in Sunnyvale, CA. Clover was acquired on December 28, 2012 by First Data Corporation. Bank of America Merchant Services was the first to announce it would sell Clover to its merchant base in October 2013. PNC Merchant Services was the second to announce it would sell Clover to its merchant base.\n\nClover was incorporated in October 15, 2010 and raised $5.5M on November 1, 2010. The lead investor was Sutter Hill Ventures with participation from Andreessen Horowitz and a number of angel investors. The company was founded by John Beatty, Leonard Speiser, and Kelvin Zheng. The company secured an additional $3M convertible note from Sutter Hill Ventures in April 2012.\n\nClover launched its first hardware solution, the Clover Station, in 2013 and began shipping it in January 2014. Clover opened its App Marketplace to third party developers in 2014. Gyft announced the launch of its Gift Cloud Service on Clover in April 2014.\n\nFirst Data announced the sale of 17,000 Clover Stations six months after the release of the product. This put Clover ahead of Square Stand in terms of total units shipped.\n\nOn September 9, 2014 Clover announced its work with Apple to support Apple Pay via its Android POS Platform. Clover described the functionality in a blog post that was featured on the home page of Hacker News.\n\nClover announced its second hardware product, the Clover Mobile, at the Money 20/20 conference in November 2014. On March 19, 2015, Clover Mobile won the Gold medal for the Best POS innovation at the PYMNTS.com Innovator Awards ceremony. Clover Mobile features a complete EMVCo compliant payments solution with an on-screen pin-pad, that is fully PCI PTS 4.0 compliant. It also features an EMVCo compliant Contactless payment interface which enables it to support Apple Pay among other contactless payment modes. The device has an ergonomic handle with an integrated barcode scanner - with an external wireless mobile printer accessory designed to keep the device lightweight and reduce fatigue.\n\nClover's third hardware product, the Clover Mini was launched by First Data on June 16, 2015. Like Clover Mobile, Clover Mini is a PCI PTS 4.0 approved, EMVCo compliant contact and contactless next generation payment device. Clover Mini has a built-in receipt printer and is designed to be a terminal replacement with the support for advanced payments interfaces, while also allowing Merchants to install apps from the Clover App Market. Clover Mini also features an USB hub allowing easy interface of peripherals such as external barcode scanners, cash drawer and a Merchant keypad (for cases where the Mini is Customer-facing).\n\nOn December 28, 2012, Clover entered into a merger agreement with First Data Corporation. In a radio interview with Wharton professor Rahul Kapoor the structure of the acquisition was explained. An operating agreement was put into place whereby Clover would receive $100M in funding to run independently. First Data would be responsible for sales and support, while Clover built all new payment hardware/software for the company. The deal utilized a \"Founder's Clause\" that would trigger a very large penalty payment if the parent company interfered with operations of Clover. Thirty two months after the acquisition Clover was mentioned 88 times in the S-1 filed by First Data Corporation.\n"}
{"id": "22256588", "url": "https://en.wikipedia.org/wiki?curid=22256588", "title": "Comparison of Canon EOS digital cameras", "text": "Comparison of Canon EOS digital cameras\n\nThe following tables provide general information as well as a comparison of technical specifications for a number of Canon EOS digital cameras.\n\n"}
{"id": "13315514", "url": "https://en.wikipedia.org/wiki?curid=13315514", "title": "Design technology", "text": "Design technology\n\nDesign technology, or D.T., is the study, design, development, application, implementation, support and management of computer and non-computer based technologies for the express purpose of communicating product design intent and constructability. Design technology can be applied to the problems encountered in construction, operation and maintenance of a product.\n\nAt times there is cross-over between D.T. and Information Technology, whereas I.T. is primarily focused on overall network infrastructure, hardware & software requirements, and implementation, D.T. is specifically focused on supporting, maintaining and training design and engineering applications and tools and working closely with I.T. to provide necessary infrastructure, for the most effective use of these applications and tools.\n\nWithin the building design, construction and maintenance industry (also known as AEC/O/FM), the product is the building and the role of D.T., is the effective application of technologies within all phases and aspects of building process. D.T. processes have adopted Building Information Modeling (BIM) to quicken construction, design and facilities management using technology. So though D.T. encompasses BIM and Integrated Project Delivery, I.P.D., it is more overarching in its directive and scope and likewise looks for ways to leverage and more effectively utilize C.A.D., Virtual Design & Construction, V.D.C., as well as historical and legacy data and systems.\n\nD.T. is also applicable to industrial and product design and the manufacturing and fabrication processes therein.\n\nThere are formal courses of study in some countries known as design and technology that focus on particular areas. In this case the above definition still remains valid, if for instance one takes the subject textiles technology and replace product in the above definition with textile.\n\n"}
{"id": "47886", "url": "https://en.wikipedia.org/wiki?curid=47886", "title": "Disruptive innovation", "text": "Disruptive innovation\n\nIn business, a disruptive innovation is an innovation that creates a new market and value network and eventually disrupts an existing market and value network, displacing established market-leading firms, products, and alliances. The term was defined and first analyzed by the American scholar Clayton M. Christensen and his collaborators beginning in 1995, and has been called the most influential business idea of the early 21st century.\n\nNot all innovations are disruptive, even if they are revolutionary. For example, the first automobiles in the late 19th century were not a disruptive innovation, because early automobiles were expensive luxury items that did not disrupt the market for horse-drawn vehicles. The market for transportation essentially remained intact until the debut of the lower-priced Ford Model T in 1908. The \"mass-produced\" automobile was a disruptive innovation, because it changed the transportation market, whereas the first thirty years of automobiles did not.\n\nDisruptive innovations tend to be produced by outsiders and entrepreneurs in startups, rather than existing market-leading companies. The business environment of market leaders does not allow them to pursue disruptive innovations when they first arise, because they are not profitable enough at first and because their development can take scarce resources away from sustaining innovations (which are needed to compete against current competition). A disruptive process can take longer to develop than by the conventional approach and the risk associated to it is higher than the other more incremental or evolutionary forms of innovations, but once it is deployed in the market, it achieves a much faster penetration and higher degree of impact on the established markets.\n\nBeyond business and economics disruptive innovations can also be considered to disrupt complex systems, including economic and business-related aspects.\n\nThe term disruptive technologies was coined by Clayton M. Christensen and introduced in his 1995 article \"Disruptive Technologies: Catching the Wave\", which he cowrote with Joseph Bower. The article is aimed at management executives who make the funding or purchasing decisions in companies, rather than the research community. He describes the term further in his book \"The Innovator's Dilemma\". \"Innovator's Dilemma\" explored the cases of the disk drive industry (which, with its rapid generational change, is to the study of business what fruit flies are to the study of genetics, as Christensen was advised in the 1990s) and the excavating equipment industry (where hydraulic actuation slowly displaced cable-actuated movement). In his sequel with Michael E. Raynor, \"The Innovator's Solution\", Christensen replaced the term \"disruptive technology\" with \"disruptive innovation\" because he recognized that few technologies are intrinsically disruptive or sustaining in character; rather, it is the \"business model\" that the technology enables that creates the disruptive impact. However, Christensen's evolution from a technological focus to a business-modelling focus is central to understanding the evolution of business at the market or industry level. Christensen and Mark W. Johnson, who cofounded the management consulting firm Innosight, described the dynamics of \"business model innovation\" in the 2008 \"Harvard Business Review\" article \"Reinventing Your Business Model\". The concept of disruptive technology continues a long tradition of identifying radical technical change in the study of innovation by economists, and the development of tools for its management at a firm or policy level.\n\nThe term “disruptive innovation” is misleading when it is used to refer to a product or service at one fixed point, rather than to the evolution of that product or service over time.\n\nIn the late 1990s, the automotive sector began to embrace a perspective of \"constructive disruptive technology\" by working with the consultant David E. O'Ryan, whereby the use of current off-the-shelf technology was integrated with newer innovation to create what he called \"an unfair advantage\". The process or technology change as a whole had to be \"constructive\" in improving the current method of manufacturing, yet disruptively impact the whole of the business case model, resulting in a significant reduction of waste, energy, materials, labor, or legacy costs to the user.\n\nIn keeping with the insight that what matters economically is the business model, not the technological sophistication itself, Christensen's theory explains why many disruptive innovations are \"not\" \"advanced technologies\", which a default hypothesis would lead one to expect. Rather, they are often novel combinations of existing off-the-shelf components, applied cleverly to a small, fledgling value network.\n\nOnline news site TechRepublic suggests to end using the term, and similar related terms, being that it is overused jargon as of 2014.\n\nThe current theoretical understanding of disruptive innovation is different from what might be expected by default, an idea that Clayton M. Christensen called the \"technology mudslide hypothesis\". This is the simplistic idea that an established firm fails because it doesn't \"keep up technologically\" with other firms. In this hypothesis, firms are like climbers scrambling upward on crumbling footing, where it takes constant upward-climbing effort just to stay still, and any break from the effort (such as complacency born of profitability) causes a rapid downhill slide. Christensen and colleagues have shown that this simplistic hypothesis is wrong; it doesn't model reality. What they have shown is that good firms are usually aware of the innovations, but their business environment does not allow them to pursue them when they first arise, because they are not profitable enough at first and because their development can take scarce resources away from that of sustaining innovations (which are needed to compete against current competition). In Christensen's terms, a firm's existing \"value networks\" place insufficient value on the disruptive innovation to allow its pursuit by that firm. Meanwhile, start-up firms inhabit different value networks, at least until the day that their disruptive innovation is able to invade the older value network. At that time, the established firm in that network can at best only fend off the market share attack with a me-too entry, for which survival (not thriving) is the only reward.\n\nChristensen defines a disruptive innovation as a product or service designed for a new set of customers.\nChristensen argues that disruptive innovations can hurt successful, well-managed companies that are responsive to their customers and have excellent research and development. These companies tend to ignore the markets most susceptible to disruptive innovations, because the markets have very tight profit margins and are too small to provide a good growth rate to an established (sizable) firm. Thus, disruptive technology provides an example of an instance when the common business-world advice to \"focus on the customer\" (or \"stay close to the customer\", or \"listen to the customer\") can be strategically counterproductive.\n\nWhile Christensen argued that disruptive innovations can hurt successful, well-managed companies, O'Ryan countered that \"constructive\" integration of existing, new, and forward-thinking innovation could improve the economic benefits of these same well-managed companies, once decision-making management understood the systemic benefits as a whole.\nChristensen distinguishes between \"low-end disruption\", which targets customers who do not need the full performance valued by customers at the high end of the market, and \"new-market disruption\", which targets customers who have needs that were previously unserved by existing incumbents.\n\n\"Low-end disruption\" occurs when the rate at which products improve exceeds the rate at which customers can adopt the new performance. Therefore, at some point the performance of the product overshoots the needs of certain customer segments. At this point, a disruptive technology may enter the market and provide a product that has lower performance than the incumbent but that exceeds the requirements of certain segments, thereby gaining a foothold in the market.\n\nIn low-end disruption, the disruptor is focused initially on serving the least profitable customer, who is happy with a good enough product. This type of customer is not willing to pay premium for enhancements in product functionality. Once the disruptor has gained a foothold in this customer segment, it seeks to improve its profit margin. To get higher profit margins, the disruptor needs to enter the segment where the customer is willing to pay a little more for higher quality. To ensure this quality in its product, the disruptor needs to innovate. The incumbent will not do much to retain its share in a not-so-profitable segment, and will move up-market and focus on its more attractive customers. After a number of such encounters, the incumbent is squeezed into smaller markets than it was previously serving. And then, finally, the disruptive technology meets the demands of the most profitable segment and drives the established company out of the market.\n\n\"New market disruption\" occurs when a product fits a new or emerging market segment that is not being served by existing incumbents in the industry.\n\nThe extrapolation of the theory to all aspects of life has been challenged, as has the methodology of relying on selected case studies as the principal form of evidence. Jill Lepore points out that some companies identified by the theory as victims of disruption a decade or more ago, rather than being defunct, remain dominant in their industries today (including Seagate Technology, U.S. Steel, and Bucyrus). Lepore questions whether the theory has been oversold and misapplied, as if it were able to explain everything in every sphere of life, including not just business but education and public institutions.\n\nIn 2009, Milan Zeleny described high technology as disruptive technology and raised the question of what is being disrupted. The answer, according to Zeleny, is the \"support network\" of high technology. For example, introducing electric cars disrupts the support network for gasoline cars (network of gas and service stations). Such disruption is fully expected and therefore effectively resisted by support net owners. In the long run, high (disruptive) technology bypasses, upgrades, or replaces the outdated support network.\nQuestioning the concept of a disruptive technology, Haxell (2012) questions how such technologies get named and framed, pointing out that this is a positioned and retrospective act.\n\nTechnology, being a form of social relationship, always evolves. No technology remains fixed. Technology starts, develops, persists, mutates, stagnates, and declines, just like living organisms. The evolutionary life cycle occurs in the use and development of any technology. A new high-technology core emerges and challenges existing technology support nets (TSNs), which are thus forced to coevolve with it. New versions of the core are designed and fitted into an increasingly appropriate TSN, with smaller and smaller high-technology effects. High technology becomes regular technology, with more efficient versions fitting the same support net. Finally, even the efficiency gains diminish, emphasis shifts to product tertiary attributes (appearance, style), and technology becomes TSN-preserving appropriate technology. This technological equilibrium state becomes established and fixated, resisting being interrupted by a technological mutation; then new high technology appears and the cycle is repeated.\n\nRegarding this evolving process of technology, Christensen said:\nThe World Bank's 2019 World Development Report on \"The Changing Nature of Work\" examines how technology shapes the relative demand for certain skills in labor markets and expands the reach of firms - robotics and digital technologies, for example, enable firms to automate, replacing labor with machines to become more efficient, and innovate, expanding the number of tasks and products. Joseph Bower explained the process of how disruptive technology, through its requisite support net, dramatically transforms a certain industry.\nFor example, the automobile was high technology with respect to the horse carriage; however, it evolved into technology and finally into appropriate technology with a stable, unchanging TSN. The main high-technology advance in the offing is some form of electric car—whether the energy source is the sun, hydrogen, water, air pressure, or traditional charging outlet. Electric cars preceded the gasoline automobile by many decades and are now returning to replace the traditional gasoline automobile. The printing press was a development that changed the way that information was stored, transmitted, and replicated. This allowed empowered authors but it also promoted censorship and information overload in writing technology.\n\nMilan Zeleny described the above phenomenon. He also wrote that:\nSocial media could be considered a disruptive innovation within sports. More specifically, the way that news in sports circulates nowadays versus the pre-internet era where sports news was mainly on T.V., radio, and newspapers. Social media has created a new market for sports that was not around before in the sense that players and fans have instant access to information related to sports.\n\nHigh technology is a technology core that changes the very architecture (structure and organization) of the components of the technology support net. High technology therefore transforms the qualitative nature of the TSN's tasks and their relations, as well as their requisite physical, energy, and information flows. It also affects the skills required, the roles played, and the styles of management and coordination—the organizational culture itself.\n\nThis kind of technology core is different from regular technology core, which preserves the qualitative nature of flows and the structure of the support and only allows users to perform the same tasks in the same way, but faster, more reliably, in larger quantities, or more efficiently. It is also different from appropriate technology core, which preserves the TSN itself with the purpose of technology implementation and allows users to do the same thing in the same way at comparable levels of efficiency, instead of improving the efficiency of performance.\n\nAs for the difference between high technology and low technology, Milan Zeleny once said:\nHowever, not all modern technologies are high technologies. They have to be used as such, function as such, and be embedded in their requisite TSNs. They have to empower the individual because only through the individual can they empower knowledge. Not all information technologies have integrative effects. Some information systems are still designed to improve the traditional hierarchy of command and thus preserve and entrench the existing TSN. The administrative model of management, for instance, further aggravates the division of task and labor, further specializes knowledge, separates management from workers, and concentrates information and knowledge in centers.\n\nAs knowledge surpasses capital, labor, and raw materials as the dominant economic resource, technologies are also starting to reflect this shift. Technologies are rapidly shifting from centralized hierarchies to distributed networks. Nowadays knowledge does not reside in a super-mind, super-book, or super-database, but in a complex relational pattern of networks brought forth to coordinate human action.\n\nIn the practical world, the popularization of personal computers illustrates how knowledge contributes to the ongoing technology innovation. The original centralized concept (one computer, many persons) is a knowledge-defying idea of the prehistory of computing, and its inadequacies and failures have become clearly apparent. The era of personal computing brought powerful computers \"on every desk\" (one person, one computer). This short transitional period was necessary for getting used to the new computing environment, but was inadequate from the vantage point of producing knowledge. Adequate knowledge creation and management come mainly from networking and distributed computing (one person, many computers). Each person's computer must form an access point to the entire computing landscape or ecology through the Internet of other computers, databases, and mainframes, as well as production, distribution, and retailing facilities, and the like. For the first time, technology empowers individuals rather than external hierarchies. It transfers influence and power where it optimally belongs: at the loci of the useful knowledge. Even though hierarchies and bureaucracies do not innovate, free and empowered individuals do; knowledge, innovation, spontaneity, and self-reliance are becoming increasingly valued and promoted.\n\nMajor opportunities according to researchers and consultants\n\n\n"}
{"id": "1863199", "url": "https://en.wikipedia.org/wiki?curid=1863199", "title": "Doc Searls", "text": "Doc Searls\n\nDavid \"Doc\" Searls (born July 29, 1947), is an American journalist, columnist, and a widely read blogger. He is a co-author of \"The Cluetrain Manifesto\", author of \"\", Editor-in-Chief of \"Linux Journal\", a fellow at the Center for Information Technology & Society (CITS) at the University of California, Santa Barbara, and an alumnus fellow (2006–2010) of the Berkman Center for Internet & Society at Harvard University.\n\nA longtime advocate for open-source software, Searls has been involved with the \"Linux Journal\" since it began publishing in 1994. He became a Contributing Editor in 1996, Senior Editor since 1999, and Editor-in-Chief in 2018. His column \"Linux for Suits\" ran until 2007, and was followed by \"EOF\" inside each issue's back cover. His work with \"Linux Journal\", and as an advocate of free software and open source, earned him a Google-O'Reilly Open Source Award for Best Communicator in 2005. His byline has also appeared in many other publications, including \"OMNI\", \"Wired\", \"PC Magazine\", \"The Standard\", \"The Sun Magazine\", \"Upside\", \"Release 1.0\" and \"The Globe and Mail\".\n\nIn early 1999 Searls joined Christopher Locke, David Weinberger and Rick Levine in writing \"The Cluetrain Manifesto\", an iconoclastic website that was followed in January 2000 by the book with the same title. The book became a business bestseller and was published in nine languages. A 10th Anniversary edition came out in June 2009. Among Searls' contributions to the Manifesto was its first thesis, \"Markets are conversations\" – which is also the title of the \"Cluetrain\" chapter he co-wrote with David Weinberger. Weinberger and Searls co-wrote \"World of Ends: What the Internet Is and How to Stop Mistaking It for Something Else\".\n\nSearls has also been a well-known blogger since October 1999, when he started blogging with help from his friend Dave Winer. In an Online Journalism Review article, J.D. Lasica calls Searls \"one of the deep thinkers in the blog movement.\"\n\nIn \"The World is Flat\", Thomas L. Friedman calls Searls \"one of the most respected technology writers in America.\"\n\nSearls' two academic fellowships both began in 2006. At the Berkman Center for Internet & Society he leads ProjectVRM, which guides independent software development communities working on Vendor Relationship Management (VRM). The purpose of VRM is to equip individuals with tools that provide both independence from vendor \"lock-in\" and better means for engaging with vendors. VRM tools and methods also help individuals engage with government and other non-commercial organizations. At the Center for Information Technology and Society (CITS) at the University of California, Santa Barbara, Searls is studying both the nature of infrastructure and of the Internet as a form of infrastructure.\n\nIn April 2012, his book \"\" was published. Searls coined the term in an article for \"Linux Journal\". He wrote: \"The Intention Economy grows around buyers, not sellers. It leverages the simple fact that buyers are the first source of money, and that they come ready-made. You don't need advertising to make them.\"\n\nThe nickname \"Doc\" is what Searls calls a \"fossil remnant\" of \"Doctor Dave,\" his humorous persona at WDBS (now WXDU) radio at Duke University in Durham, North Carolina, in the late 1970s. Following his work in radio, Searls co-founded Hodskins Simone & Searls (HS&S), which grew to become one of the top high-tech advertising agencies in North Carolina, and then in Silicon Valley. Searls' consultancy, The Searls Group, was spun out of HS&S in the early 1990s. He is a frequent speaker at business and industry events, under the auspices of The Searls Group.\n\nSearls is a 1969 graduate of Guilford College. While Searls' permanent home is in Santa Barbara, he and his family currently live most of the year near his work at Harvard.\n\nSearls' journalism career began in 1971, when he worked as an editor and photographer for \"Wayne Today\" in New Jersey. In recent years his photographic work consists of pushing as many shots as possible into the public domain, through permissive Creative Commons licensing for a batch of photos on Flickr that now total more than 41,000 on his own site alone. (He also posts on three other sites: two for the Berkman Center and one for \"Linux Journal\".) A large percentage of those are shot out the windows of commercial aircraft. There are about 300 of his photos that now appear in Wikimedia Commons (none of which he put there). Many of these also illustrate articles in Wikipedia as well.\n\nHis photos have also appeared in many books, magazines and other media, including NBC's coverage of the 2010 Winter Olympics in Vancouver. There Searls' ice crystal photos served as key elements for the network's coverage. (Searls ran in the credits as a member of the design team.)\n\n\n"}
{"id": "202311", "url": "https://en.wikipedia.org/wiki?curid=202311", "title": "E-services", "text": "E-services\n\nE-services (electronic services) are services which use of information and communication technologies (ICTs). The three main components of e-services are- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.\n\nSince its inception in the late 1980s in Europe and formal introduction in 1993 by the US Government, the term ‘E-Government’ has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization. E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.\n\nE-service (or eservice) is a highly generic term, usually referring to ‘The provision of services via the Internet (the prefix 'e' standing for ‘electronic’, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.’ (Irma Buntantan & G. David Garson, 2004: 169-170; Muhammad Rais & Nazariah, 2003: 59, 70-71).\n\n\"E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.\" (Jeong, 2007).\n\nLu (2001) identifies a number of benefits for e-services, some of these are:\n\n\n\nThe term ‘e-service’ has many applications and can be found in many disciplines. The two dominant application areas of e-services are\n\nE-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).\n\nE-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix “public”: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of “e-Service” in both domains of e-government and e-business.[example: www.eserviceforyou.com]\n\nDepending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to – Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer (Enterprise Application Integration– EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).\n\nMeasuring service quality and service excellence are important in a competitive organizational environment. The SERVQUAL- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:\n\nThe LIRNEasia study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of information society reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.\n\nSome major cost factors are (Lu, 2001):\n\n\nInformation technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.\n\nMany government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).\n\nBut the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,\nissues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an “enabler”, but on the other hand, it should also be regarded as a challenge and a peril in itself. The organizations, public or private, which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc. What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions, needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).\n\nFollowing are a few examples regarding e-services in some developing countries:\n\nBangladesh first e-service system is National E-Service System ([http://www.eservice.gov.bd/[ NESS]]) and 2nd e-Service For you [http://eserviceforyou.com/[eserviceforyou.com]]. \n\nOnly a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,\nhas become one of the continent’s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country\nwhere legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is\npuzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the\nsame region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.\n\nIn South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance. The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans & Yen, 2006:208). In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the Batho Pele portal, SARS e-filing, the e-Natis system, electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors\nwhich collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.\n\nE-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor (MSC) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad (TNB) and Telekom Malaysia Berhad (TM) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one’s own convenience. Also, Electronic Labour Exchange (ELX)is one stop-centre for labor market information, as supervised by the Ministry of Human Resource (MOHR), to enable employers and job seekers to communicate on the same platform.\n\ne-Syariah is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.\n\nIn America, citizens have many options and opportunities to follow and understand government actions through e-government. Government 2.0 (Gov. 2.0) is currently in place to bring the people and governments together to learn new information, increase government transparency, and better means for communicating to one another. Gov. 2.0 offers increased citizen participation through on-line applications such as social media and other apps. Through the internet and websites such as USA.gov, an individual can perform actions such as contacting elected officials, find information about the work force such as retirement plans and labor laws, learn about money and consumer issues such as taxes, loans, and welfare, learn about citizenship and obtaining a visa or passport, and other topics such as health and welfare, education, and environmental issues.\n\nE-commerce is another growing E-service in the United States for both big and small businesses. E-commerce sales are projected to grow 10 to 12 percent annually. Amazon.com is the largest on-line marketplace in the country with annual sales of $79 billion. Wal-Mart is also a widely popular retailer. They have grown their business by having electronic services. Wal-Mart’s sales for E-commerce in 2015 was roughly $13 billion. Apple develops and sells a wide variety of technological goods and services such as cell phones, music players, and computers. Apple’s sales for E-commerce in 2015 was $12 billion. E-services allows businesses to reach new clientele and offer new services. Companies such as eBay and Etsy have achieved great success, with eBay posting a net income in 2016 of nearly $9 billion and Esty claiming roughly $200 million in profits from nearly $2 billion sales. The majority or eBay's business is conducted in the United States but it does a great deal of international business including the United Kingdom and Germany. The global reach of Etsy is seen in nearly every country in the world with 31% of gross merchandise sales occurring outside of the United States.\n\nChina’s recent realization of the continuing growth of internet usage has caused the government to recognize the need to expand their E-government services. Some steps the government wants to take in order to increase their E-government services are to develop more online functions, use government sites to integrate on-line services, have supplementary open data available to citizens to further government transparency, and to combine services from local and country-wide governments for convenience. China’s plan of action to incorporate the internet into everyday business and grow the economy is known as “Internet Plus.” The government plans to have this plan in full effect by 2025 to be the main driving force for economic and social improvements. Internet Plus will help to grow the job market as the government plans to use local citizens for development, and to generate more areas dedicated to technological growth such as Zhongguancun.\n\nBecause of the large population, China has the most internet and cell phone users in the world.(consider rewording) This causes a need for technological growth and a demand for increased E-services. In 2016, Chinese consumers spent more money for on-line goods and services than the United States and United Kingdom combined. There is(are) a wide variety of reasons as to why E-commerce flourishes in China including easy access to mobile internet, low cost of shipping, and a vast selection of cheap, unbranded products. Alibaba is China’s largest on-line marketplace with an annual revenue stream of $16 billion. Its services are globally available in Russia and Brazil through AliExpress. Tencent is another internet company with an annual revenue income of $16 billion. Tencent is used mainly for instant messaging but has other applications as well including mobile games and other digital content. By the end of 2015, Tencent’s WeChat messaging app reached around 700 million users. The biggest competitor for Tencent is Facebook’s WhatsApp. Baidu Is the most visited website in the country and it is used as a search engine and has an annual revenue of $10 billion. In March 2016, there were roughly 663 million users. Google challenges Baidu as the major internet search engines in the world. Huawei is a tech company that produces phones, tablets, and develops the equipment used in fixed-line networks. Huawei has an annual revenue income of $61 billion. It is currently located throughout 100 countries worldwide and in 2015, it filed 3,898 patent applications, more than any other country in the world. The biggest competitors to Huawei is Apple and Samsung.\n\nThe future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth & Sharma (2007) identify, are:\n\n\nThe first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations. For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles. Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)\n\nA considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley’s study (2006) who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality “In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.”\n\nSome of the major keywords of e-service as found in the e-government research are as follows:\n\nUser acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p. 1) as “the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support”. This definition can be brought into the context of e-service where acceptance can be defined as the users’ willingness to use e-service or the willingness to decide when and how to use the e-service.\n\nUsers’ ability to access to the e-service is important theme in the previous literature. For example, Huang (2003) finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006) who suggests the following to improve e-services’ accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site …Focus on the benefits of an accessible Web site to all users.\n\nAccording to Grönlund et al. (2007), for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.\n\nThis theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007) “… benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs”\n\nDigital divide is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009), “we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites.\"\n\nMost of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or e-readiness. According to by Shalini (2009), “the results of the research project reveal that a high index may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not”\n\n``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country’s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative, for instance depending on a country in question's priorities and perspective.\n\nAs opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness “There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies”\n\nSecurity is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services. According to the GAO report of 2002 “security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials—as well as the general public—reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.” By and Large, Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential\n\nAxelsson et al. (2009) argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the stakeholder theory in public settings. The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.\n\nCompared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive\n\n``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.´´\n\nThe perceived effectiveness of e-service can be influenced by public’s view of the social and cultural implications of e-technologies and e-service.\n\nImpacts on individuals’ rights and privacy – as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees' computer usage patterns in order to assess individual or workgroup performance. Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern that access to a wide range of information can be dangerous within politically corrupt government agencies.\n\nImpact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk – such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.\n\nPotential Impacts on Society – despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.\n\nImpact on Social Interaction – advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.\n\nInformation Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information – more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.\n\nThe benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards\n\nEuropean eGovernment Awards program started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the |4th European eGovernment Awards were announced in the award ceremony that took place at the 5th Ministerial eGovernment Conference on 19 November 2009 (Sweden); the winners in their respective categories are:\n\n\nSultan Qaboos Award for excellence in eGovernance (Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.\n\neGovernment Excellence Awards (Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy, eEducation Citizen Awards: Best eContent, eCitizen.\n\nPhilippines e-Service Awards (Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.\n\nThere are some journals particularly interested for “e-Service “. Some of these are:\n\nMajor conferences considering e-service as one of the themes are:\n\n\n"}
{"id": "16754063", "url": "https://en.wikipedia.org/wiki?curid=16754063", "title": "Electric boiler", "text": "Electric boiler\n\nAn electric boiler is a device that uses electrical energy to boil water. \n\nSmall electric water boilers are portable household appliances, used to prepare hot beverages or other food items. They may provide a reservoir of hot water to be kept at a fixed temperature or to allow tea to be steeped at a controlled temperature. They are manually filled with water every time hot water is desired. \n\nAn electric steam boiler is intended to produce steam for process industry or other uses, such as drying, heat treating, and others. These are generally permanently installed systems with connections to a water supply and piping to transport product steam to the point of use. In places where electric power is relatively low cost compared to fossil fuels, it may be economically practical to use an electric boiler for steam central heating. For example, in Winnipeg, Canada, during the Second World War, large central electric steam boilers were used for a district heating system, using surplus hydroelectric power. The intent was to conserve coal fuel for more critical wartime needs. \nElectric boilers may rely on immersion heater resistance heating elements to heat water, or may use electric current passing directly through the water as an electrode boiler. \n\nEven the tabletop appliance will have a control switch or at least an over temperature cut out to prevent fire if the appliance is started with no water in it. Larger installations have more elaborate controls to ensure that sufficient water supply is available to prevent burn out of the heating elements. These may include interlocks, pressure switches or other measures. \n\nA boiler is a pressure vessel and industrial boilers are constructed, operated and inspected in accordance with local regulations to reduce the risk of explosion. For example, permanently installed pressure relief valves will operate if a control malfunction results in excessive pressure in the boiler. \n\nNormally resistance heating elements are insulated from the water going through the boiler; sensitive ground fault leakage current detection may be installed to alarm or shut off power if an insulation failure is detected. Electrode boilers put the water supply in direct contact with the electrical supply; current collectors or other features may be provided in piping to prevent dangerous electrical hazards on connected piping. \n\nAny dissolved minerals in the water will be concentrated by the evaporation and will tend to form a scale coating heating surfaces. Even potable water may contain enough minerals to eventually clog or scale a boiler. Household appliances can be cleaned readily, but permanent boiler installations require control of make-up water supply chemistry to reduce scaling. \n\nElectric heating is generally highly efficient; since there is no stream of waste combustion gases emitting from the boiler, nearly all the purchased energy appears in the product hot water or steam in useful form. This high efficiency even in small sizes partly offsets the generally higher cost of electric energy compared to fossil fuels. An electric boiler can be placed in operation quickly, as there is no furnace to reach high temperatures. Environmental impact is displaced to the source of the grid electricity; however, if fossil fuel is being consumed to make electricity, overall efficiency will be lower than direct use of the fuel. \n\nOperationally, an electric boiler is a convenient process unit that is easy to control and that requires no space for fuel storage nor for an exhaust gas stack. There is no blower for combustion air and only minimal pumps required for operation, so the boiler is quiet. \n"}
{"id": "27047750", "url": "https://en.wikipedia.org/wiki?curid=27047750", "title": "Electrical system design", "text": "Electrical system design\n\nElectrical system design is the design of electrical systems. This can be as simple as a flashlight cell connected through two wires to a light bulb or as involved as the space shuttle. Electrical systems are groups of electrical components connected to carry out some operation. Often the systems are combined with other systems. They might be subsystems of larger systems and have subsystems of their own. For example, a subway rapid transit electrical system is composed of the wayside electrical power supply, wayside control system, and the electrical systems of each transit car. Each transit car’s electrical system is a subsystem of the subway system. Inside of each transit car there are also subsystems, such as the car climate control system.\n\nThe following would be appropriate for the design of a moderate to large electrical system.\n\n"}
{"id": "7709376", "url": "https://en.wikipedia.org/wiki?curid=7709376", "title": "Excitation filter", "text": "Excitation filter\n\nAn excitation filter is a high quality optical-glass filter commonly used in fluorescence microscopy and spectroscopic applications for selection of the excitation wavelength of light from a light source. Most excitation filters select light of relatively short wavelengths from an excitation light source, as only those wavelengths would carry enough energy to cause the object the microscope is examining to fluoresce sufficiently. The excitation filters used may come in two main types — short pass filters and band pass filters. Variations of these filters exist in the form of notch filters or deep blocking filters (commonly employed as emission filters). Other forms of excitation filters include the use of monochromators, wedge prisms coupled with a narrow slit (for selection of the excitation light) and the use of holographic diffraction gratings, etc. [for beam diffraction of white laser light into the required excitation wavelength (selected for by a narrow slit)].\n\nAn excitation filter is commonly packaged with an emission filter and a dichroic beam splitter in a cube so that the group is inserted together into the microscope. The dichroic beam splitter controls which wavelengths of light go to their respective filter.\n"}
{"id": "4575623", "url": "https://en.wikipedia.org/wiki?curid=4575623", "title": "First-out alarm", "text": "First-out alarm\n\nA first-out alarm is an alarm that indicates in some manner that it was the first of a series. This is necessary in circumstances such as an automatic trip or shutdown of equipment, where many alarms will announce as a result of a shutdown. The first-out alarm will clearly identify the root cause of the trip or shutdown.\n\nISA Alarm Standard 18.1 Annunciator Sequences and Specifications\n"}
{"id": "37360402", "url": "https://en.wikipedia.org/wiki?curid=37360402", "title": "Français fondamental", "text": "Français fondamental\n\nFundamental French (français fondamental) is a list of words and grammatical concepts created in beginning of the 1950s for teaching foreigners and people part of l'Union française so that France could improve the spread of the French language. A series of investigations in the 1950s and '60s showed that a small number of words are used the same way orally and in writing in all circumstances; thus a limited number of grammatical rules were necessary for a functional language.\n\n\"Français fondamental\" was developed by the \"Centre d'Etude du Français Élémentaire\", which was renamed to the \"Centre de Recherche et d'Etude pour la Diffusion du Français\" (CREDIF) in 1959. It was headed by Georges Gougenheim, a linguist. The Ministry of Education of France sanctioned and promoted it as a method of learning French. The use of \"Français fondamental\" was common in French textbooks, and especially prevalent in audiovisual learning methods used in the 1960s.\n\nGougenheim, Réné Michea, Paul Rivenc, and Aurélien Sauvageot served as researchers for the project. There are 1,475 words in the \"first degree\" and 1,609 words in the \"second degree.\"\n\nThe core words for usage consist of around 270 grammatical words, 380 nouns, 200 verbs, 100 adjectives, and 50 words for various other uses, making up a total of one thousand words. There is a second group of words which are common to all French speakers, who can use spontaneously use around 1,500 words when needed in particular circumstances.\n\n\"L'Academie francaise\" prefers to distinguish \"fundamental french\" and \"elementary french\" as it says in its page \"Langue francaise - Questions courantes\" (The French Language - Common Questions) in the rubric \"Nombre de mots de la langue francaise\" (number of words in the french language): \"Based on frequency surveys, 'Fundamental French' and 'Elementary French' include just over 1000 to 3000 entrees respectively.\n\nFundamental French certainly had an influence, particularly in teaching the language to foreigners. But it was rejected in 1970 with the renewal of language teaching, without being replaced by a new tool. A seminar organised at \"l'École normale supérieure lettres et sciences humaines de Lyon\" in December 2005 tried to take stock of the developments that had occurred since its creation.\n\nIn a similar spirit but without the apparent affiliation, Radio France Internationale (RFI) released a 10 minute Journal in easy French with a restrained and simple vocabulary (300 words) that gave the context of the events once a day. This step is analogous to that of the Voice of America, who released a program in 1959 in Special English, a form of basic English with a vocabulary restrained to 1500 words.\n\n\n\n"}
{"id": "74554", "url": "https://en.wikipedia.org/wiki?curid=74554", "title": "Hardware description language", "text": "Hardware description language\n\nIn computer engineering, a hardware description language (HDL) is a specialized computer language used to describe the structure and behavior of electronic circuits, and most commonly, digital logic circuits.\n\nA hardware description language enables a precise, formal description of an electronic circuit that allows for the automated analysis and simulation of an electronic circuit. It also allows for the synthesis of a HDL description into a netlist (a specification of physical electronic components and how they are connected together), which can then be placed and routed to produce the set of masks used to create an integrated circuit.\n\nA hardware description language looks much like a programming language such as C; it is a textual description consisting of expressions, statements and control structures. One important difference between most programming languages and HDLs is that HDLs explicitly include the notion of time.\n\nHDLs form an integral part of electronic design automation (EDA) systems, especially for complex circuits, such as application-specific integrated circuits, microprocessors, and programmable logic devices.\n\nDue to the exploding complexity of digital electronic circuits since the 1970s (see Moore's law), circuit designers needed digital logic descriptions to be performed at a high level without being tied to a specific electronic technology, such as CMOS or BJT. HDLs were created to implement register-transfer level abstraction, a model of the data flow and timing of a circuit.\n\nThere are two major hardware description languages: VHDL and Verilog. There are different types of description in them\n\"dataflow, behavioral and structural\".\nExample of dataflow of HDL:\n\nHDLs are standard text-based expressions of the structure of electronic systems and their behaviour over time. Like concurrent programming languages, HDL syntax and semantics include explicit notations for expressing concurrency. However, in contrast to most software programming languages, HDLs also include an explicit notion of time, which is a primary attribute of hardware. Languages whose only characteristic is to express circuit connectivity between a hierarchy of blocks are properly classified as netlist languages used in electric computer-aided design (CAD). HDL can be used to express designs in structural, behavioral or register-transfer-level architectures for the same circuit functionality; in the latter two cases the synthesizer decides the architecture and logic gate layout.\n\nHDLs are used to write executable specifications for hardware. A program designed to implement the underlying semantics of the language statements and simulate the progress of time provides the hardware designer with the ability to model a piece of hardware before it is created physically. It is this executability that gives HDLs the illusion of being programming languages, when they are more precisely classified as specification languages or modeling languages. Simulators capable of supporting discrete-event (digital) and continuous-time (analog) modeling exist, and HDLs targeted for each are available.\n\nIt is certainly possible to represent hardware semantics using traditional programming languages such as C++, which operate on control flow semantics as opposed to data flow, although to function as such, programs must be augmented with extensive and unwieldy class libraries. Generally, however, software programming languages do not include any capability for explicitly expressing time, and thus cannot function as hardware description languages. Before the introduction of System Verilog in 2002, C++ integration with a logic simulator was one of the few ways to use object-oriented programming in hardware verification. System Verilog is the first major HDL to offer object orientation and garbage collection.\n\nUsing the proper subset of hardware description language, a program called a synthesizer, or logic synthesis tool, can infer hardware logic operations from the language statements and produce an equivalent netlist of generic hardware primitives to implement the specified behaviour. Synthesizers generally ignore the expression of any timing constructs in the text. Digital logic synthesizers, for example, generally use clock edges as the way to time the circuit, ignoring any timing constructs. The ability to have a synthesizable subset of the language does not itself make a hardware description language.\n\nThe first hardware description languages appeared in the late 1960s, looking like more traditional languages. The first that had a lasting effect was described in 1971 in C. Gordon Bell and Allen Newell's text \"Computer Structures\". This text introduced the concept of register transfer level, first used in the ISP language to describe the behavior of the Digital Equipment Corporation (DEC) PDP-8.\n\nThe language became more widespread with the introduction of DEC's PDP-16 RT-Level Modules (RTMs) and a book describing their use.\nAt least two implementations of the basic ISP language (ISPL and ISPS) followed.\nISPS was well suited to describe relations between the inputs and the outputs of the design and was quickly adopted by commercial teams at DEC, as well as by a number of research teams both in the USA and among its NATO allies.\n\nThe RTM products never took off commercially and DEC stopped marketing them in the mid-1980s, as new techniques and in particular very-large-scale integration (VLSI) became more popular.\n\nSeparate work done about 1979 at the University of Kaiserslautern produced a language called KARL (\"KAiserslautern Register Transfer Language\"), which included design calculus language features supporting VLSI chip floorplanning and structured hardware design. This work was also the basis of KARL's interactive graphic sister language ABL, whose name was an initialism for \"A Block diagram Language\". ABL was implemented in the early 1980s by the Centro Studi e Laboratori Telecomunicazioni (CSELT) in Torino, Italy, producing the ABLED graphic VLSI design editor. In the mid-1980s, a VLSI design framework was implemented around KARL and ABL by an international consortium funded by the Commission of the European Union.\n\nBy the late 1970s, design using programmable logic devices (PLDs) became popular, although these designs were primarily limited to designing finite state machines. The work at Data General in 1980 used these same devices to design the Data General Eclipse MV/8000, and commercial need began to grow for a language that could map well to them. By 1983 Data I/O introduced ABEL to fill that need.\n\nAs design shifted to VLSI, the first modern HDL, Verilog, was introduced by Gateway Design Automation in 1985. Cadence Design Systems later acquired the rights to Verilog-XL, the HDL simulator that would become the de facto standard of Verilog simulators for the next decade. In 1987, a request from the U.S. Department of Defense led to the development of VHDL (VHSIC Hardware Description Language). VHDL was based on the Ada programming language, as well as on the experience gained with the earlier development of ISPS. Initially, Verilog and VHDL were used to document and simulate circuit designs already captured and described in another form (such as schematic files). HDL simulation enabled engineers to work at a higher level of abstraction than simulation at the schematic level, and thus increased design capacity from hundreds of transistors to thousands.\n\nThe introduction of logic synthesis for HDLs pushed HDLs from the background into the foreground of digital design. Synthesis tools compiled HDL source files (written in a constrained format called RTL) into a manufacturable netlist description in terms of gates and transistors. Writing synthesizable RTL files required practice and discipline on the part of the designer; compared to a traditional schematic layout, synthesized RTL netlists were almost always larger in area and slower in performance. A circuit design from a skilled engineer, using labor-intensive schematic-capture/hand-layout, would almost always outperform its logically-synthesized equivalent, but the productivity advantage held by synthesis soon displaced digital schematic capture to exactly those areas that were problematic for RTL synthesis: extremely high-speed, low-power, or asynchronous circuitry.\n\nWithin a few years, VHDL and Verilog emerged as the dominant HDLs in the electronics industry, while older and less capable HDLs gradually disappeared from use. However, VHDL and Verilog share many of the same limitations: neither is suitable for analog or mixed-signal circuit simulation; neither possesses language constructs to describe recursively-generated logic structures. Specialized HDLs (such as Confluence) were introduced with the explicit goal of fixing specific limitations of Verilog and VHDL, though none were ever intended to replace them.\n\nOver the years, much effort has been invested in improving HDLs. The latest iteration of Verilog, formally known as IEEE 1800-2005 SystemVerilog, introduces many new features (classes, random variables, and properties/assertions) to address the growing need for better test bench randomization, design hierarchy, and reuse. A future revision of VHDL is also in development, and is expected to match SystemVerilog's improvements.\n\nAs a result of the efficiency gains realized using HDL, a majority of modern digital circuit design revolves around it. Most designs begin as a set of requirements or a high-level architectural diagram. Control and decision structures are often prototyped in flowchart applications, or entered in a state diagram editor. The process of writing the HDL description is highly dependent on the nature of the circuit and the designer's preference for coding style. The HDL is merely the 'capture language', often beginning with a high-level algorithmic description such as a C++ mathematical model. Designers often use scripting languages such as Perl to automatically generate repetitive circuit structures in the HDL language. Special text editors offer features for automatic indentation, syntax-dependent coloration, and macro-based expansion of the entity/architecture/signal declaration.\n\nThe HDL code then undergoes a code review, or auditing. In preparation for synthesis, the HDL description is subject to an array of automated checkers. The checkers report deviations from standardized code guidelines, identify potential ambiguous code constructs before they can cause misinterpretation, and check for common logical coding errors, such as floating ports or shorted outputs. This process aids in resolving errors before the code is synthesized.\n\nIn industry parlance, HDL design generally ends at the synthesis stage. Once the synthesis tool has mapped the HDL description into a gate netlist, the netlist is passed off to the back-end stage. Depending on the physical technology (FPGA, ASIC gate array, ASIC standard cell), HDLs may or may not play a significant role in the back-end flow. In general, as the design flow progresses toward a physically realizable form, the design database becomes progressively more laden with technology-specific information, which cannot be stored in a generic HDL description. Finally, an integrated circuit is manufactured or programmed for use.\n\nEssential to HDL design is the ability to simulate HDL programs. Simulation allows an HDL description of a design (called a model) to pass design verification, an important milestone that validates the design's intended function (specification) against the code implementation in the HDL description. It also permits architectural exploration. The engineer can experiment with design choices by writing multiple variations of a base design, then comparing their behavior in simulation. Thus, simulation is critical for successful HDL design.\n\nTo simulate an HDL model, an engineer writes a top-level simulation environment (called a test bench). At minimum, a testbench contains an instantiation of the model (called the device under test or DUT), pin/signal declarations for the model's I/O, and a clock waveform. The testbench code is event driven: the engineer writes HDL statements to implement the (testbench-generated) reset-signal, to model interface transactions (such as a host–bus read/write), and to monitor the DUT's output. An HDL simulator — the program that executes the testbench — maintains the simulator clock, which is the master reference for all events in the testbench simulation. Events occur only at the instants dictated by the testbench HDL (such as a reset-toggle coded into the testbench), or in reaction (by the model) to stimulus and triggering events. Modern HDL simulators have full-featured graphical user interfaces, complete with a suite of debug tools. These allow the user to stop and restart the simulation at any time, insert simulator breakpoints (independent of the HDL code), and monitor or modify any element in the HDL model hierarchy. Modern simulators can also link the HDL environment to user-compiled libraries, through a defined PLI/VHPI interface. Linking is system-dependent (Win32/Linux/SPARC), as the HDL simulator and user libraries are compiled and linked outside the HDL environment.\n\nDesign verification is often the most time-consuming portion of the design process, due to the disconnect between a device's functional specification, the designer's interpretation of the specification, and the imprecision of the HDL language. The majority of the initial test/debug cycle is conducted in the HDL \"simulator\" environment, as the early stage of the design is subject to frequent and major circuit changes. An HDL description can also be prototyped and tested in hardware — programmable logic devices are often used for this purpose. Hardware prototyping is comparatively more expensive than HDL simulation, but offers a real-world view of the design. Prototyping is the best way to check interfacing against other hardware devices and hardware prototypes. Even those running on slow FPGAs offer much shorter simulation times than pure HDL simulation.\n\nHistorically, design verification was a laborious, repetitive loop of writing and running simulation test cases against the design under test. As chip designs have grown larger and more complex, the task of design verification has grown to the point where it now dominates the schedule of a design team. Looking for ways to improve design productivity, the electronic design automation industry developed the Property Specification Language.\n\nIn formal verification terms, a property is a factual statement about the expected or assumed behavior of another object. Ideally, for a given HDL description, a property or properties can be proven true or false using formal mathematical methods. In practical terms, many properties cannot be proven because they occupy an unbounded solution space. However, if provided a set of operating assumptions or constraints, a property checker can prove (or disprove) certain properties by narrowing the solution space.\n\nThe assertions do not model circuit activity, but capture and document the designer's intent in the HDL code. In a simulation environment, the simulator evaluates all specified assertions, reporting the location and severity of any violations. In a synthesis environment, the synthesis tool usually operates with the policy of halting synthesis upon any violation. Assertion based verification is still in its infancy, but is expected to become an integral part of the HDL design toolset.\n\nA HDL is grossly similar to a software programming language, but there are major differences. Most programming languages are inherently procedural (single-threaded), with limited syntactical and semantic support to handle concurrency. HDLs, on the other hand, resemble concurrent programming languages in their ability to model multiple parallel processes (such as flip-flops and adders) that automatically execute independently of one another. Any change to the process's input automatically triggers an update in the simulator's process stack.\n\nBoth programming languages and HDLs are processed by a compiler (often called a synthesizer in the HDL case), but with different goals. For HDLs, \"compiling\" refers to logic synthesis; the process of transforming the HDL code listing into a physically realizable gate netlist. The netlist output can take any of many forms: a \"simulation\" netlist with gate-delay information, a \"handoff\" netlist for post-synthesis placement and routing on a semiconductor die, or a generic industry-standard Electronic Design Interchange Format (EDIF) (for subsequent conversion to a JEDEC-format file).\n\nOn the other hand, a software compiler converts the source-code listing into a microprocessor-specific object code for execution on the target microprocessor. As HDLs and programming languages borrow concepts and features from each other, the boundary between them is becoming less distinct. However, pure HDLs are unsuitable for general purpose application software development, just as general-purpose programming languages are undesirable for modeling hardware.\n\nYet as electronic systems grow increasingly complex, and reconfigurable systems become increasingly common, there is growing desire in the industry for a single language that can perform some tasks of both hardware design and software programming. SystemC is an example of such—embedded system hardware can be modeled as non-detailed architectural blocks (black boxes with modeled signal inputs and output drivers). The target application is written in C or C++ and natively compiled for the host-development system; as opposed to targeting the embedded CPU, which requires host-simulation of the embedded CPU or an emulated CPU.\n\nThe high level of abstraction of SystemC models is well suited to early architecture exploration, as architectural modifications can be easily evaluated with little concern for signal-level implementation issues. However, the threading model used in SystemC relies on shared memory, causing the language not to handle parallel execution or low-level models well.\n\nIn their level of abstraction, HDLs have been compared to assembly languages. There are attempts to raise the abstraction level of hardware design in order to reduce the complexity of programming in HDLs, creating a sub-field called \"high-level synthesis\". \n\nCompanies such as Cadence, Synopsys and Agility Design Solutions are promoting SystemC as a way to combine high-level languages with concurrency models to allow faster design cycles for FPGAs than is possible using traditional HDLs. Approaches based on standard C or C++ (with libraries or other extensions allowing parallel programming) are found in the Catapult C tools from Mentor Graphics, and the Impulse C tools from Impulse Accelerated Technologies.\n\nAnnapolis Micro Systems, Inc.'s CoreFire Design Suite and National Instruments LabVIEW FPGA provide a graphical dataflow approach to high-level design entry and languages such as SystemVerilog, SystemVHDL, and Handel-C seek to accomplish the same goal, but are aimed at making existing hardware engineers more productive, rather than making FPGAs more accessible to existing software engineers.\n\nIt is also possible to design hardware modules using MATLAB and Simulink using the MathWorks HDL Coder tool or Xilinx System Generator (XSG) (formerly Accel DSP) from Xilinx.\n\nThe two most widely used and well-supported HDL varieties used in industry are Verilog and VHDL.\n\nSeveral projects exist for defining printed circuit board connectivity using language based, textual-entry methods.\n\n\n"}
{"id": "15632708", "url": "https://en.wikipedia.org/wiki?curid=15632708", "title": "History of the roller coaster", "text": "History of the roller coaster\n\nRoller coaster amusement rides have origins back to ice slides constructed in 18th-century Russia. Early technology featured sleds or wheeled carts that were sent down hills of snow reinforced by wooden supports. The technology evolved in the 19th century to feature railroad track using wheeled cars that were securely locked to the track. Newer innovations emerged in the early 20th century with side friction and underfriction technologies to allow for greater speeds and sharper turns. By the mid-to-late 20th century, these elements intensified with the introduction of steel roller coaster designs and the ability for them to invert riders.\n\nThe world's oldest roller coasters descended from the \"Russian Mountains,\" which were specially constructed hills of snow located in the gardens of palaces around the Russian capital, Saint Petersburg, in the 18th century. This attraction was called a \"Katalnaya Gorka\" or \"sliding mountain\" in Russian. The slides were built to a height of between and , had a 50 degree drop, and were reinforced by wooden supports. Sometimes wheeled carts were used instead of sleds. These slides became popular with the Russian upper class, and with Catherine II of Russia herself, who had such mountains built in the gardens of the Oranienbaum Palace near St. Petersburg, with a pavilion next to it for drinking tea after the sliding. \"Russian mountains\" remains the term for roller coasters in many languages, such as Spanish (\"\"), Italian (\"\"), and French (\"\"). Ironically, the Russian term for roller coaster, \"\" (amerikanskie gorki), translates literally as \"American mountains.\"\nRussian soldiers occupying Paris from 1815 through 1816, after the defeat of Napoleon at Waterloo, may have introduced the Russian amusement of sledding down steep hills. In July 1817, a French banker named Nicolas Beaujon opened the Parc Beaujon, an amusement park on the Champs Elysees. Its most famous feature was the \"Promenades Aériennes\" or \"Aerial Strolls.\" It featured wheeled cars securely locked to the track, guide rails to keep them on course, and higher speeds. The three-wheel carts were towed to the top of a tower, and then released to descend two curving tracks on either side. King Louis XVIII of France came to see the park, but it is not recorded if he tried the ride. Before long there were seven similar rides in Paris: \"Les Montagnes françaises\" (The French Mountains), \"le Delta\", \"les Montagnes de Belleville\" (The Mountains of Belleville), \"les Montagnes américaines\" (the American Mountains), \"Les Montages lilliputiennes\", (The miniature mountains), \"Les Montagnes susses\" (The Swiss mountains) and \"Les Montagnes égyptiennes\" (The Egyptian mountains).\n\nIn the beginning, these attractions were primarily for the upper classes. In 1845 a new amusement park opened in Copenhagen, Tivoli, which was designed for the middle class. These new parks featured roller coasters as permanent attractions. The first permanent loop track was probably also built in Paris from an English design in 1846, with a single-person wheeled sled running through a 13-foot (4 m) diameter vertical loop. These early single loop designs were called Centrifugal Railways. In 1887, a French entrepreneur, Joseph Oller, the owner of the Moulin Rouge music hall, built \"Les Montagnes Russes à Belleville\" (\"The Russian Mountains of Belleville\") a permanent roller coaster with a length of two hundred meters in the form of a double-eight, later enlarged to four figure-eight-shaped loops.\n\nIn the 1850s, a mining company in Summit Hill, Pennsylvania, constructed the Mauch Chunk gravity railroad, a brakeman-controlled, 8.7-mile (14 km) downhill track used to deliver coal to Mauch Chunk (now known as Jim Thorpe), Pennsylvania. By 1872, the \"Gravity Road\" (as it became known) was selling rides to thrill seekers. Railway companies used similar tracks to provide amusement on days when ridership was low.\n\nUsing this idea as a basis, LaMarcus Adna Thompson began work on a gravity Switchback Railway that opened at Coney Island in Brooklyn, New York in 1884. Passengers climbed to the top of a platform and rode a bench-like car down the track up to the top of another tower where the vehicle was switched to a return track and the passengers took the return trip. This track design was soon replaced with an oval complete circuit. In 1885, Phillip Hinkle introduced the first complete-circuit coaster with a lift hill, the \"Gravity Pleasure Road\", which became the most popular attraction at Coney Island. Not to be outdone, in 1886 LaMarcus Adna Thompson patented his design for a roller coaster that included dark tunnels with painted scenery. \"Scenic Railways\" were soon found in amusement parks across the county, with Frederick Ingersoll's construction company building many of them in the first two decades of the 20th century.\n\nAs it grew in popularity, experimentation in coaster dynamics took off. In the 1880s the concept of a vertical loop was again explored by Lina Beecher, and in 1895 the concept came into fruition with the \"Flip Flap Railway\", located at Sea Lion Park in Brooklyn, and shortly afterward with \"Loop the Loop\" at Olentangy Park near Columbus, Ohio as well as similar coasters in Atlantic City and Coney Island. The rides were incredibly dangerous, and many passengers suffered whiplash. Both were soon dismantled, and looping coasters had to wait for over a half century before making a reappearance.\n\nBy 1919, the first underfriction roller coaster had been developed by John Miller. Soon, roller coasters spread to amusement parks all around the world. Perhaps the best known historical roller coaster, \"The Cyclone\", was opened at Coney Island in 1927. Like \"The Cyclone\", all early roller coasters were made of wood. Many old wooden roller coasters are still operational, at parks such as Kennywood near Pittsburgh, Pennsylvania and Pleasure Beach Blackpool, England. The oldest operating roller coaster is \"Leap-The-Dips\" at Lakemont Park in Pennsylvania, a side friction roller coaster built in 1902. The oldest wooden roller coaster in the United Kingdom is the \"Scenic Railway\" at Dreamland Amusement Park in Margate, Kent and features a system where the brakeman rides the car with wheels. It was severely damaged by fire on April 7, 2008, but was subsequently restored and reopened to the public in 2015. \"Scenic Railway\" at Melbourne's Luna Park built in 1912, is the world's oldest continually-operating roller coaster, and it also still features a system where the brakeman rides the car with wheels. One of only 13 remaining examples of John Miller's work worldwide is the wooden roller coaster at Lagoon in Utah. The coaster opened in 1921 and is the 6th oldest coaster in the world.\n\nThe Great Depression marked the end of the golden age of roller coasters, as amusement parks generally went into a decline that resulted in less demand for new coasters. This lasted until 1972, when \"The Racer\" opened at Kings Island amusement park located in what was then a part of Deerfield Township in Warren County, Ohio. Designed by John C. Allen, the instant success of \"The Racer\" helped to ignite a renaissance for roller coasters, reviving worldwide interest throughout the industry.\n\nIn 1959, the Disneyland theme park introduced a new design breakthrough in roller coasters with the \"Matterhorn Bobsleds\". This was the first roller coaster to use a tubular steel track. Unlike conventional wooden rails, which are generally formed using steel strips mounted on laminated wood, tubular steel can be bent in any direction, which allows designers to incorporate loops, corkscrews, and many other maneuvers into their designs. Most modern roller coasters are made of steel, although wooden roller coasters are still being built along with hybrids of steel and wood.\n\nIn 1975 the first modern-day roller coaster to perform an inverting element opened: Corkscrew, located at Knott's Berry Farm in Buena Park, California. In 1976 the vertical loop made a permanent comeback with the Great American Revolution at Magic Mountain in Valencia, California.\n\nThe roller coasters mentioned here are significant for their role in the amusement industry. They were notable for specific reasons, including:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32906538", "url": "https://en.wikipedia.org/wiki?curid=32906538", "title": "Hoa Lac Hi-tech Park", "text": "Hoa Lac Hi-tech Park\n\nHoa Lac Hi-tech Park is the first and largest hi-tech park in Vietnam with total area of 1586 ha (app. 4000 acres). Located in the area of Hanoi Capital, convenient for transportation to Noi Bai International Airport and Hai Phong deep Seaport, Hoa Lac Hi-tech Park will be developed as model of a science city with over 200,000 people working and living and consists of the following main functional zones:\nCurrently, there are two software companies located here, including FPT Software and Viettel Software. They are two biggest software companies of Vietnam.\n\n\n"}
{"id": "24846359", "url": "https://en.wikipedia.org/wiki?curid=24846359", "title": "IDT Spectrum", "text": "IDT Spectrum\n\nIDT Spectrum, a subsidiary of IDT Corporation, holds and leases fixed wireless spectrum.\n\nIDT Spectrum, Inc. and IDT Spectrum, LLC, are subsidiaries of IDT Corporation (), an international holding company, with interests primarily in the telecommunications and energy industries. In December 2001, IDT Corporation through its subsidiary Winstar Holdings, LLC, acquired FCC spectrum licenses and other assets from the bankruptcy estate of Winstar Communications. Winstar Holdings formed a subsidiary company, Winstar Spectrum, LLC, to hold its spectrum licenses and then caused the spectrum licenses to be assigned to it. As part of an internal corporate reorganization, in December 2004, Winstar Holdings formed IDT Spectrum, LLC, and caused Winstar Spectrum, LLC to transfer and assign to IDT Spectrum, LLC all of its FCC licenses except for six point-to-point licenses that are not used in our business. In January 2005, Winstar Holdings formed IDT Spectrum and contributed to IDT Spectrum all of its interests in IDT Spectrum, LLC, as well as other assets used in its connectivity services.\n\nIDT Spectrum, LLC's primary holdings include 633 spectrum licenses in the 39 GHz range, as well as an additional 16 LMDS licenses in the 28 GHz band, making it the largest single holder of 39 GHz licensed auction spectrum in the United States.\n\nThe FCC's 39 GHz auctioned license band spectrum is primarily licensed in Economic Areas, or EAs. EAs are delineated by the Regional Analysis Division, Bureau of Economic Analysis, U.S. Department of Commerce and are based on 176 metropolitan or micropolitan statistical areas that serve as regional centers of economic activity, plus the surrounding counties that are economically related to these areas. On average, IDT Spectrum holds more than 500 MHz of spectrum in the top 200 U.S. markets (Economic Areas by population) and approximately 940 MHz of spectrum in the top 50 U.S. markets. IDT Spectrum's 39 GHz holdings–are contiguous across the United States (including Alaska, Hawaii and Puerto Rico).\n\nIn October 2010, IDT Spectrum renewed 633 of its 39 GHz licenses, which now expire in October 2020. The majority of IDT Spectrum's 28 GHz LMDS licenses expire in October 2018.\n\nAmong other business activities, IDT Spectrum leases the spectrum to customers who use their own microwave equipment. The licenses held by IDT Spectrum are suited for high bandwidth point to point applications, including the needs of wireless operators to carry traffic from cell sites to network access points–referred to in the industry as backhaul.\n\nThe current President and CEO of IDT Spectrum is Michael Rapaport.\n\n"}
{"id": "295156", "url": "https://en.wikipedia.org/wiki?curid=295156", "title": "Index of electronics articles", "text": "Index of electronics articles\n\nThis is an index of articles relating to electronics and electricity or natural electricity and things that run on electricity and things that use or conduct electricity.\n\n16VSB –\n2VSB –\n32VSB –\n4000 series –\n4VSB –\n555 timer IC –\n7400 series –\n8VSB\n\nAbsolute gain (physics) –\nAcceptance pattern –\nAccess control –\nAccess time –\nAcoustic coupler –\nAdaptive communications –\nAdder –\nAdjacent-channel interference –\nAlarm sensor –\nAliasing –\nAllied Electronics –\nAlternating current –\nAM radio –\nAmateur radio –\nAmbient noise level –\nAmerican Radio Relay League (ARRL) –\nAmmeter –\nAmpere –\nAmplifier –\nAmplitude distortion –\nAmplitude modulation –\nAnalog computer –\nAnalog decoding –\nAnalog –\nAnalog to digital converter –\nAnalogue switch –\nAnalysis of resistive circuits –\nAngular misalignment loss –\nAntenna –\nAntenna aperture –\nAntenna blind cone –\nAntenna gain –\nAntenna height above average terrain –\nAntenna noise temperature –\nAntenna theory –\nAperiodic antenna –\nAperture (antenna) –\nAperture illumination –\nAperture-to-medium coupling loss –\nApollo Guidance Computer –\nArithmetic and logical unit –\nArmstrong oscillator –\nARRL –\nArticulation score –\nAstable –\nAsymmetric Digital Subscriber Line –\nAsynchronous communications system –\nAsynchronous operation –\nAsynchronous start-stop –\nAtmospheric duct –\nAtmospheric waveguide –\nAttenuation –\nAudible ringing tone –\nAudio system measurements –\nAudiophile –\nAutomatic call distributor –\nAutomatic gain control –\nAutomatic link establishment –\nAutomatic number identification –\nAutomatic sounding –\nAutomatic switching system –\nAutovon –\nAvailability –\nAvailable line –\nAvalanche diode –\nAzimuth\n\nBackplane –\nBackscattering –\nBack-to-back connection –\nBackward channel –\nBalance return loss –\nBalanced line –\nBalancing network –\nBall grid array –\nBand gap –\nBand-stop filter –\nBandwidth compression –\nBare particular –\nBarrage jamming –\nBaseband –\nBattery (electricity) –\nBaud –\nBaudot code –\nBCS theory –\nBeam diameter –\nBeam divergence –\nBeam steering –\nBeamwidth –\nBeat frequency oscillator -\nBel –\nBiconical antenna –\nBig ugly dish –\nBilateral synchronization –\nBillboard antenna –\nBinary classification –\nBinary multiplier –\nBinaural recording –\nBipolar junction transistor –\nBipolar signal –\nBit inversion –\nBit pairing –\nBit robbing –\nBit stuffing –\nBit synchronous operation –\nBit-count integrity –\nBits per second –\nBlack facsimile transmission –\nBlack recording –\nBlanketing –\nBluetooth –\nBlu-ray Disc -\nBNC connector –\nBoresight –\nBreadboard –\nBremsstrahlung –\nBridging loss –\nBroadband Internet –\nBroadband wireless access –\nBroadband –\nBroadcasting –\nBurst transmission –\nBusy hour –\nBusy signal –\nBypass\n\nCable modem –\nCable television –\nCaesium standard –\nCall collision –\nCall set-up time –\nCall-second –\nCapacitive coupling –\nCapacitor –\nCapture effect –\nCarbon nanotube –\nCard standards –\nCarrier sense multiple access with collision detection –\nCarrier shift –\nCarrier system –\nCarrier wave –\nCarrier-to-receiver noise density –\nCarson bandwidth rule –\nCassegrain antenna –\nCategory 5 cable –\nCathode ray tube –\nCentral processing unit –\nChadless tape –\nChannel –\nChannel noise level –\nChannel reliability –\nCharacter-count integrity –\nCharacteristic impedance –\nCharge-coupled device –\nChemical vapor deposition –\nChirp –\nChroma subsampling –\nCircuit breaker –\nCircuit noise level –\nCircuit reliability –\nCircuit restoration –\nCircuit switching –\nCircular polarization –\nCirculator –\nCitizens' band radio –\nCladding –\nClapp oscillator –\nClean room –\nClear channel –\nClearing –\nClipping –\nClock gating –\nClock signal –\nClosed waveguide –\nClosed-circuit television –\nCMOS –\nCoaxial cable –\nCo-channel interference –\nCode division multiple access –\nCode word –\nCoherence length –\nCoherence time –\nCoherence –\nCoherent differential phase-shift keying –\nCoherer –\nCoilgun –\nCollinear antenna array –\nCollinear antenna array –\nCollins Radio –\nColpitts oscillator –\nCombat-net radio –\nCombinational logic –\nCombined distribution frame –\nCommon base –\nCommon battery –\nCommon collector –\nCommon control –\nCommon emitter –\nCommonality –\nCommon-mode interference –\nCommunications center –\nCommunications satellite –\nCommunications security –\nCommunications system engineering –\nCommunications system –\nCommunications-electronics –\nCompact audio cassette –\nCompatible sideband transmission –\nComposite image filter -\nComposite video –\nCompulsator –\nComputer –\nConcentrator –\nConditioning equipment –\nConducted interference –\nConduction band –\nConductive coupling –\nConnections per circuit hour –\nConservation of radiance –\nConstant k filter -\nContent delivery –\nContention –\nContinuous Fourier transform –\nContinuous operation –\nContinuous wave –\nConvolution –\nCopper –\nCord circuit –\nCorner reflector –\nCosmic noise –\nCostas loop –\nCoulomb's law –\nCounter (digital) –\nCoupling –\nCovert channel –\nCovert listening device –\nCPU design –\nCQD –\nC-QUAM –\nCritical frequency –\nCross product –\nCrossbar switch –\nCrosstalk –\nCrystal filter –\nCrystal radio receiver –\nCurrent –\nCurrent bias –\nCurrent-to-voltage converter –\nCutback technique –\nCutoff frequency –\nCutoff wavelength\n\nD region –\nD-4 –\nData bank –\nData circuit terminating equipment –\nData compaction –\nData integrity –\nData link –\nData service unit –\nData terminal equipment –\nData transmission circuit –\nData –\nDatasheet –\ndBa –\ndBm –\nDBrn –\nDDR SDRAM –\nDegree of isochronous distortion –\nDelay line –\nDelta modulation –\nDemand assignment –\nDemand factor –\nDemand load –\nDemodulation –\nDemodulator –\nDeparture angle –\nDesign objective –\nDespun antenna –\nDeviation –\nDial-up –\nDiamagnetism –\nDielectric constant –\nDielectric strength –\nDielectric waveguide –\nDielectric –\nDifferential amplifier –\nDiffraction –\nDigital access and cross-connect system –\nDigital Audio Tape –\nDigital circuit –\nDigital filter –\nDigital multiplex hierarchy –\nDigital radio –\nDigital signal processing –\nDigital signal processor –\nDigital to analog converter –\nDigital transmission group –\nDigitizer –\nDIN –\nDiode –\nDIP switch –\nDipole antenna –\nDipole –\nDirect bandgap –\nDirect broadcast satellite –\nDirect current –\nDirect distance dialing –\nDirect ray –\nDirectional antenna –\nDirectional coupler –\nDirective gain –\nDirect-sequence spread spectrum –\nDiscrete Fourier transform –\nDiscrete –\nDispersion-limited operation –\nDisplay device -\nDistortion –\nDistortion-limited operation –\nEmergency locator beacon –\nDistributed switching –\nDiurnal phase shift –\nDiversity reception –\nDOD master clock –\nDoping –\nDouble-sideband suppressed-carrier transmission –\nDouble-slit experiment –\nDrift –\nDrop and insert –\nDropout –\nDual access –\nDual impedance -\nDual in-line package –\nDual-modulus prescaler –\nDual-tone multi-frequency –\nDuobinary signal –\nDuplex –\nDuty cycle –\nDXCC –\nDynamic range\n\nEarphone –\nEarpiece –\nEarth's magnetic field –\nEDIF –\nEEPROM –\nEffective antenna gain contour –\nEffective boresight area –\nEffective data transfer rate –\nEffective Earth radius –\nEffective height –\nEffective input noise temperature –\nEffective isotropically radiated power –\nEffective monopole radiated power –\nEffective radiated power –\nEffective transmission rate –\nEfficiency factor –\nE-layer –\nElectric charge –\nElectric current –\nElectric field –\nElectric motor –\nElectric power transmission –\nElectric power –\nElectrical conduction –\nElectrical conductivity –\nElectrical connector –\nElectrical current –\nElectrical efficiency –\nElement –\nElectrical engineering –\nElectrical generator –\nImpedance –\nInsulation –\nElectrical length –\nLoad –\nElectrical network –\nCircuit –\nElectrical resistance –\nElectrical room –\nElectrical signal –\nElectricity distribution –\nElectricity –\nElectrochemical cell –\nElectrochemistry –\nElectrode –\nElectrodynamics –\nElectrolytic capacitor –\nElectromagnetic environment –\nElectromagnetic induction –\nElectromagnetic interference control –\nElectromagnetic pulse –\nElectromagnetic radiation and health –\nElectromagnetic radiation –\nElectromagnetic spectrum –\nElectromagnetic survivability –\nElectromagnetism –\nElectrometer –\nElectron hole –\nElectron –\nElectronic amplifier –\nElectronic color code –\nColor code –\nElectronic data processing –\nElectronic deception –\nElectronic design automation –\nElectronic filter –\nFilter –\nElectronic imager –\nElectronic mixer –\nElectronic musical instrument –\nElectronic oscillator –\nElectronic power supply –\nElectronic switching system –\nElectronic tagging –\nElectronic test equipment –\nElectronic warfare support measures –\nElectronics –\nElectro-optic effect –\nElectro-optic modulator –\nElectro-optics –\nElectrostatic discharge –\nElectrostatics –\nEmergency Locator Transmitter –\nEmergency Position-Indicating Radio Beacon –\nEmitter coupled logic –\nEnd distortion –\nEndurability –\nEnhanced service –\nEntropy encoding –\nEquilibrium length –\nEquivalent impedance transforms -\nEquivalent noise resistance –\nEquivalent pulse code modulation noise –\nError burst –\nError ratio –\nError-correcting code –\nE-skip –\nExamples of electrical phenomena –\nExtremely Low Frequency (ELF) –\nEye pattern\n\nFab (semiconductors) –\nSemiconductor device fabrication –\nFacsimile converter –\nFading –\nFading distribution –\nFail safe –\nFall time –\nFan-beam antenna –\nFarad –\nFaraday cage –\nFaraday constant –\nFaraday's law of induction –\nFar-field region –\nFault –\nFault management –\nFCC registration program –\nFederal Standard 1037C –\nFeed horn –\nFeedback –\nFerroelectric effect –\nFerromagnetism –\nField (physics) –\nField –\nField effect transistor –\nField strength –\nFPGA Field programmable gate array –\nFilled cable –\nFilter design –\nFilter (signal processing) -\nFlip-flop –\nFluorescent lamp –\nFlutter –\nFlux –\nFlywheel effect –\nFM band –\nFM improvement factor –\nFM improvement threshold –\nFM radio –\nForward error correction –\nFourier series –\nFourier transform (see also List of Fourier-related transforms) –\nFour-wire circuit –\nFour-wire terminating set –\nFractal antenna –\nFrame –\nFrame rate –\nFrame slip –\nFrame synchronization –\nFraming bit –\nFreenet –\nFree-space loss –\nFreeze frame television –\nFrequency assignment –\nFrequency averaging –\nFrequency counter –\nFrequency deviation –\nFrequency frogging –\nFrequency modulation synthesis –\nFrequency modulation –\nFrequency standard –\nFrequency synthesiser –\nFrequency –\nFrequency-division multiplexing –\nFrequency-exchange signaling –\nFrequency-hopping spread spectrum –\nFrequency-shift keying –\nFresnel equations –\nFresnel reflection –\nFresnel zone –\nFront-to-back ratio –\nFuel cell –\nFuse (electrical)\n\nGallium arsenide –\nGalvanic isolation -\nGalvanometer –\nGateway –\nGating –\nGauss –\nGeiger–Müller tube –\nGel electrophoresis –\nGemini Guidance Computer –\nGender changer –\nGlobal Positioning System –\nGlobal system for mobile communications –\nGNU Radio –\nGrade of service –\nGraded-index fiber –\nGround constants –\nGround loop –\nGround plane –\nGround (electricity) –\nGroundwave –\nGuided ray –\nGyrator\n\nHalftone characteristic –\nHall effect –\nHamming code –\nHamming distance –\nHandoff –\nHandshaking –\nHard copy –\nHardware register –\nHarmonic analysis –\nHarmonic oscillator –\nHarmonic –\nHartley oscillator –\nH-channel –\nHeat sink –\nHelical antenna –\nHelmholtz coil –\nHenry (unit) –\nHertz –\nHeterodyne repeater –\nHeterodyne –\nHigh frequency –\nHigh-performance equipment –\nHigh-speed circuit-switched data –\nHop –\nHorn –\nHow to test three-phase electrical supply –\nHow to test three-phase pumps –\nHybrid balance –\nHybrid circuit –\nHybrid coil –\nHybrid coupler –\nHysteresis\n\nIEEE 315-1975 -\nIEEE 802.11 –\nIEEE 802.15 –\nIEEE 802 –\nImage antenna –\nImage impedance -\nImage frequency –\nImage rejection ratio –\nImage response –\nImpedance matching –\nIn-band on-channel –\nIncidental radiator –\nIndependent sideband –\nIndex of cooperation –\nInductive coupling –\nInductive reactance –\nInductor –\nIndustrial Computers -\nInformation transfer –\nInformation-bearer channel –\nInfrared –\nInput/output –\nInsertion gain –\nInsertion loss –\nInside plant –\nIntegrated circuit –\nIntensity modulation –\nIntentional radiator –\nIntercept –\nInterchange circuit –\nIntercharacter interval –\nInterconnect facility –\nInterference –\nInterferometry –\nIntermediate-field region –\nIntermodulation distortion –\nInternational Electrotechnical Commission –\nInteroperability –\nInterposition trunk –\nIntersymbol interference –\nInverse multiplexer –\nInverse-square law –\nIon pump –\nIonosphere –\nISM band –\nIsochronous burst transmission –\nIsochronous signal –\nIsotropic antenna\n\nJam signal –\nJamming –\nJansky –\nJitter\n\nKarnaugh map –\nKendall effect –\nKey pulsing –\nKirchhoff's circuit laws –\nKlystron –\nKnife-edge effect\n\nLaser –\nLaunch angle –\nLaunch numerical aperture –\nLead-lag effect –\nLeaky mode –\nLight bulb –\nLight-dependent resistor –\nLight-emitting diode –\nLightning –\nLimiting –\nLine code –\nLinear feedback shift register –\nLinear regulator –\nLip synchronization –\nList of telephony terminology –\nLists of video game companies -\nLM741 –\nLow noise amplifier –\nLoading characteristic –\nLoading coil –\nLobe –\nLocal battery –\nLogic families –\nLogic gate –\nLogic –\nLog-periodic antenna –\nLong-haul communications –\nLongitudinal redundancy check –\nLong-tailed pair –\nLong-term stability –\nLoop –\nLoop gain –\nLoop-back –\nLow frequency –\nLow-performance equipment –\nLumped element model\n\nMacroelectronics -\nMagnet –\nMagnetic core memory –\nMagnetic field –\nMagnetic flux quantum –\nMagnetic flux –\nMagnetic levitation –\nMagnetism –\nMagneto-optic effect –\nMagnetosphere –\nMagnetron –\nMain distribution frame –\nMain lobe –\nManchester code –\nMaser –\nMask work –\nMaster frequency generator –\nMaximal-ratio combiner –\nMaximum power –\nMaximum usable frequency –\nMaxwell coil –\nMaxwell's demon –\nMaxwell's equations –\nm-derived filter -\nMean time between outages –\nMediation function –\nMedium frequency (MF) –\nMedium-power talker –\nMediumwave –\nMetal -\nMichelson-Morley experiment –\nMicroelectronics –\nMicrophone –\nMicrowave auditory effect –\nMicrowave oven –\nMicrowave –\nMIL-STD-188 –\nMinimum bend radius –\nMode scrambler –\nMode volume –\nModem –\nModular synthesizer –\nModulation factor –\nModulation rate –\nModulation –\nMolecular electronics –\nMonostable –\nMoore's law –\nMorse code –\nMOS Technology 6501 –\nMOS Technology 6502 –\nMOS Technology SID –\nMOS Technology VIC-II –\nMu-law algorithm –\nMulticoupler –\nMulti-element dipole antenna –\nMultimeter –\nMultipath propagation –\nMultiple access –\nMultiple homing –\nMultiplex baseband –\nMultiplexer –\nMultiplexing –\nMultivibrator\n\nN connector –\nNanotechnology –\nNanowire –\nNarrative traffic –\nNarrowband modem –\nNarrowband –\nNational Electrical Code (US) –\nNatural frequency –\nNear-field region –\nNegative resistance –\nNegative-acknowledge character –\nNet gain –\nNetlist –\nNetwork administration –\nNetwork architecture –\nNetwork management –\nNeural network –\nNeutral direct-current telegraph system –\nNI Multisim –\nNickel metal hydride –\nNoise figure –\nNoise level –\nNoise power –\nNoise temperature –\nNoise weighting –\nNoise-cancelling headphone –\nNoise-equivalent power –\nNon-return-to-zero –\nNormalized frequency –\nNorton's theorem -\nNTSC –\nNuclear electromagnetic pulse -\nNuclear magnetic resonance –\nNull –\nNumbers station –\nNumerical aperture –\nNumerically controlled oscillator –\nNyquist interval\n\nOff-hook –\nOff-line –\nOhm –\nOhmmeter –\nOhm's law –\nOliver Heaviside –\nOmnidirectional antenna –\nOne-way trunk –\nOn-hook –\nOn-line –\nOpen circuit –\nOpen spectrum –\nOperational amplifier –\nOptical density –\nOptical fiber –\nOptical path length –\nOptical spectrum –\nOptoelectronic –\nOrthogonal frequency division modulation –\nOrthomode transducer –\nOscilloscope –\nOut-of-band signaling –\nOutside plant –\nOverflow –\nOverhead information –\nOvermodulation –\nOverride –\nOvershoot (signal)\n\nPacket switching –\nPacket-switching node –\nPaired disparity code –\nPAL –\nPar meter –\nParabolic antenna –\nParabolic microphone –\nParallel transmission –\nParasitic element (electrical networks) –\nParity bit –\nPassband –\nPassive radiator -\nPatch bay –\nPath loss –\nPath profile –\nPauli exclusion principle –\nPBER –\nPCB layout guidelines –\nPeak envelope power –\nPeltier effect –\nPerformance measurement period –\nPeriodic antenna –\nPeriscope antenna –\nPermeability –\nPermittivity –\nPersonal Locator Beacon –\nPhantom circuit –\nPhantom loop –\nPhase –\nPhase distortion –\nPhase jitter –\nPhase modulation –\nPhase noise –\nPhase perturbation –\nPhased array –\nPhase-locked loop –\nPhase-shift keying –\nPhilberth-Transformer –\nPhone connector (audio) –\nPhotodiode –\nPhotoelectric effect –\nPhotolithography –\nPhoton –\nPhysical layer –\nPickup –\nPID controller –\nPiezoelectricity –\nPin grid array –\nPirate radio –\nPlanar array –\nPlanck's constant –\nPlesiochronous Digital Hierarchy –\nPoint-to-point construction –\nPolarential telegraph system –\nPolarization –\nPolling –\nPolyphase system –\nPortable people meter –\nPotential difference –\nPotential divider –\nPower –\nPower connector –\nPower supply –\nPreamplifier –\nPreemphasis network –\nPreemphasis –\nPreferred values –\nPreventive maintenance –\nPrimary channel –\nPrimary line constants -\nPrimary time standard –\nPrincipal clock –\nPrinted circuit board –\nProcessor register –\nProduct detector –\nProgrammable logic device –\nPropagation delay –\nPropagation mode –\nPropagation path obstruction –\nPropagation of schema –\nProration –\nPseudorandom noise –\nPseudorandom number sequence –\nPSK31 –\nPulse amplitude –\nPulse duration –\nPulse –\nPulse-address multiple access –\nPulse-code modulation –\nPulsed inductive thruster –\nPulse-width modulation –\nPush-to-talk operation –\nPush-to-type operation –\nPyroelectricity\n\nQ code –\nQRP operation –\nQ-switching –\nQuadrature amplitude modulation –\nQuadrature –\nQuality assurance –\nQuality control –\nQuantum harmonic oscillator –\nQuartz clock –\nQuasi-analog signal –\nQueuing delay\n\nRace hazard –\nRadar –\nRadiation angle –\nRadiation mode –\nRadiation pattern –\nRadiation resistance –\nRadiator –\nRadio beam –\nRadio clock –\nRadio electronics –\nRadio frequency induction –\nRadio frequency –\nRadio horizon range –\nRadio horizon –\nRadio propagation –\nRadio range –\nRadio Row, Manhattan –\nRadio station –\nRadio –\nRadioShack –\nRadiotelephone –\nRadioteletype –\nRadix-64 –\nRailgun –\nRandom access memory –\nRandomizer –\nRay transfer matrix analysis –\nRC circuit –\nRC –\nRCA jack –\nRCA –\nReactance –\nReceive-after-transmit time delay –\nReceived noise power –\nReceiver –\nReceiver attack-time delay –\nReconnaissance satellite –\nRecord medium –\nReference antenna –\nReference circuit –\nReference clock –\nReference noise –\nReference surface –\nReflection coefficient –\nReflections of signals on conducting lines -\nReflective array antenna –\nRefractive index contrast –\nRegenerative circuit –\nRegister transfer level –\nRegistered jack –\nRelational model –\nRelative transmission level –\nRelaxation oscillator –\nRelay –\nRelease time –\nRemote Operations Service Element protocol –\nRemote sensing –\nRepair and maintenance –\nRepeater –\nRepeating coil –\nReproduction speed –\nReradiation –\nResistor color code –\nResistor –\nResonance –\nResponse time –\nResponsivity –\nReturn loss –\nRF connector –\nRF modulator –\nRF power margin –\nRF probe –\nRF shielding –\nRFID –\nRGB color space –\nRhombic antenna –\nRing current –\nRing latency –\nRing modulation –\nRingback signal –\nRingdown –\n\nRL circuit –\nRLC circuit –\nRobot –\nRogowski coil –\nRoot mean square –\nRouting indicator –\nRS-232 –\nRX –\nRydberg formula\n\nS/PDIF –\nSacrificial anode –\nSampling frequency –\nScalar field –\nScanner –\nScanning electron microscope –\nSCART –\nSchematic –\nSchumann resonance –\nScrambler –\nScreen –\nSECAM –\nSecond audio program –\nSecond-order intercept point –\nSecurity management –\nSelf-clocking signal –\nSelf-synchronizing code –\nSemiautomatic switching system –\nSemiconductor device –\nSemiconductor –\nSensitivity –\nSensor Networks –\nSeparate channel signaling –\nSerial access –\nSerial ATA –\nSerial Peripheral Interface Bus –\nSerial transmission –\nSeries and parallel circuits –\nShadow loss –\nShannon limit –\nShannon's theorem –\nShort circuit –\nShortwave –\nShot noise –\nShrinking generator –\nSide lobe –\nSideband –\nSidereal time –\nSiemens –\nSignal (information theory) –\nSignal compression –\nSignal processing gain –\nSignal processing –\nSignal reflection –\nSignal transition –\nSignal-to-crosstalk ratio –\nSignal-to-noise ratio –\nSignature block –\nSignificant condition –\nSilicon bandgap temperature sensor –\nSilicon –\nSimplex circuit –\nSimplex signaling –\nSinc filter –\nSingle frequency networks –\nSingle-phase electric power –\nSingle-frequency signaling –\nSingle-polarized antenna –\nSingle-sideband modulation –\nSkew (antenna) –\nSkin effect –\nSkip zone –\nSkywave –\nSlant range –\nSlewing –\nSlew rate -\nSlot antenna –\nSlow-scan television –\nSoftware-defined radio –\nSolar cell –\nSoldering –\nSolenoid –\nSound card –\nSpace diversity –\nSpace tether –\nSpark gap –\nSpecific detectivity –\nSpecification –\nSpeckle pattern –\nSpectral width –\nSpectrum –\nSpectrum analyzer –\nSpeed of light –\nSpeed of service –\nSPICE –\nSpill-forward feature –\nSpillover –\nSpin glass –\nSpot beam –\nSpread spectrum –\nSpurious emission –\nSquelch –\nStandard telegraph level –\nStandard test signal –\nStandard test tone –\nStanding wave ratio –\nStanding wave –\nStarpath Supercharger –\nStart signal –\nStart-stop transmission –\nStatic electricity –\nSteady-state condition –\nStep-index profile –\nStoletov's law -\nStop signal –\nStopband –\nStore-and-forward switching center –\nStressed environment –\nStrobe light –\nStroke speed –\nSubcarrier –\nSubtractive synthesis –\nSudden ionospheric disturbance –\nSupercomputer –\nSuperconductivity –\nSuperheterodyne receiver –\nSuperparamagnetism –\nSuperposition theorem -\nSupervisory program –\nSuppressed carrier transmission –\nSurface wave –\nSurface-mount technology –\nSurveillance device –\nSurvivability –\nS-Video –\nSwitch –\nSwitched-mode power supply –\nSynchronism –\nSynchronization –\nSynchronizing –\nSynchronous network –\nSynchronous optical networking –\nSynthesizer –\nSystem integrity –\nSystems control\n\nTable of standard electrode potentials –\nTactical communications system –\nTactical communications –\nTactical data information link--A –\nTantalum –\nTape relay –\nT-carrier –\nTechnical control facility –\nTelecommunication –\nCommunications –\nTelecommunications service –\nTeleconference –\nTelegrapher's equations –\nTelegraphy –\nTelemetry –\nTelephone tapping –\nTeletext –\nTeletraining –\nTelevision –\nTelevision reception –\nTEMPEST –\nTensor –\nTesla coil –\nTesla patents –\nTest antenna –\nTether propulsion –\nThermal noise –\nThermistor –\nThévenin's theorem -\nThird-order intercept point –\nTNC connector –\nThree phase –\nTime-assignment speech interpolation –\nTime-division multiple access –\nTime-division multiplexing –\nTime-domain reflectometer –\nTime-out –\nTinfoil hat –\nToll switching trunk –\nTotal harmonic distortion –\nTotal internal reflection –\nTraffic intensity –\nTraffic shaping –\nTransceiver –\nTranscoding –\nTransducer –\nTransformer –\nTransient electromagnetic device –\nTransimpedance amplifier –\nTransistor radio –\nTransistor –\nTransistor-transistor logic –\nTTL –\nTransition metal –\nTransmission coefficient –\nTransmission level point –\nTransmission line –\nTransmission medium –\nMedium –\nTransmit-after-receive time delay –\nTransmitter attack-time delay –\nTransmitter –\nTransmitter-studio link –\nTransparent latch –\nTransponder –\nTransverse redundancy check –\nTraveling-wave tube –\nTRF –\nTriangle wave –\nTrimline telephone –\nTroposphere –\nTropospheric ducting –\nTropospheric wave –\nTuner –\nTwisted pair -\nTX\n\nUltra high frequency –\nUltra Wideband –\nUltraviolet –\nUnavailability –\nUncertainty principle –\nUniform linear array –\nUnijunction transistor –\nUnintentional radiator –\nUplink –\nUpright position (electronics) –\nUser (telecommunications)\n\nVAC –\nVačkář oscillator –\nVacuum tube –\nValence band –\nVariable length buffer –\nVaricap –\nVaristor –\nVDC –\nVector field –\nVeroboard –\nVery high frequency –\nVery-large-scale integration –\nVHSIC hardware description language –\nVideo cassette recorder –\nVideo Game Console -\nVideo Game - \nVideo teleconference –\nVideo teleconferencing unit –\nVideo –\nVintage amateur radio –\nVirtual circuit capability –\nVirtual circuit –\nVirtual ground –\nVoice frequency primary patch bay –\nVoice frequency –\nVolt- \nVoltage bias –\nVoltage-to-current converter –\nVoltmeter –\nVox\n\nWardenclyffe Tower –\nWarner exemption –\nWatt –\nWave impedance –\nWave propagation –\nWave –\nWaveform –\nWaveguide antenna –\nWaveguide –\nWavelength division multiplexing –\nWavelength –\nWheatstone bridge –\nWhip antenna –\nWhite facsimile transmission –\nWideband modem –\nWilliams tube –\nWink pulsing –\nWire wrap –\nWire –\nWireless access point –\nWireless community network –\nWireless network –\nWireless personal area network –\nWireless -\nX-dimension of recorded spot –\nXLR connector\n\nYagi antenna –\nY-delta transform –\nYUV\n\nZero dBm transmission level point –\nZero insertion force (ZIF) -\nZero-dispersion wavelength –\nZigBee –\nZig-zag in-line package –\nZobel network -\nZone melting\nZ-transform –\n"}
{"id": "34955132", "url": "https://en.wikipedia.org/wiki?curid=34955132", "title": "Infopoverty", "text": "Infopoverty\n\nInfopoverty is the name given to the Programme and the World Conference started in 2001, in the ambit of the United Nations, aimed to fight poverty towards the application of ICT, Information and Communication Technologies.\nThe term has been coined in 1998 by Arch. Pierpaolo Saporito, Founder and President of OCCAM - Observatory on Digital Communication and Gerardo Zepeda-Bermudez, Vice-President and Member of the Board.\n\nThe Infopoverty World Conference is the annual international conference, under the auspices of the United Nations, at the United Nations headquarters, New York, organized by OCCAM, together with the European Parliament, UNESCO, and other scientific and university institutions (Infopoverty Institute at Oklahoma University). At its 13th edition, the Infopoverty World Conference saw the participation of more than 100 international organizations,hundreds of from Governments, international and regional organizations, public and private institutions and the civil society and 63 countries.\n\nThe Infopoverty Programme transfers into concrete actions the orientations emerged from the Conference, realizing the ICT Villages, aimed to help the most disadvantaged communities.The first pilot projects were realized at San Ramon and San Pedro, Honduras, in cooperation with the local Ministry of Science and Technologies, following the devastation from Hurricane Mitch.\nThese experiences were instrumental for the definition, thanks to the collaboration with the most important international institutions, of the ICT Village model, validated by the World Summit on the Information Society in Tunis, 2005, with the creation of the Borj Touil Village.\nSubsequently, the model was developed in Sambaina (Madagascar) which was proclaimed Millennium Village Project by the United Nations, and are in progress in Leshoto, in Peru and Ethiopia.\n\n"}
{"id": "55837901", "url": "https://en.wikipedia.org/wiki?curid=55837901", "title": "Jerzy Sikorski", "text": "Jerzy Sikorski\n\nJerzy Sikorski (born July 25, 1935) is a Polish historian, Copernicologist, medievalist, museologist, author, publisher, journalist, and encyclopedist, who writes and publishes primarily in Polish. He is a resident of Olsztyn, Poland.\n\nJerzy Sikorski was born on July 25, 1935 in Vilnius (Polish: Wilno), to Anna Wołk-Lewanowicz (1914–1995) and Feliks Sikorski (1889–1980). Five years later (1940) Sikorski's sister Maria Danuta was born.\n\nIn October 1944, during World War II, Sikorski's mother Anna was arrested by the People's Commissariat for State Security along with 40 other people, when she was exposed as a courier between Vilnius' \"Kedyw\" and the general staff of the Polish Home Army. Early in January 1945 Sikorski's father Feliks was also arrested because he was a soldier of the 1st Polish Corps under the Polish general Józef Dowbor-Muśnicki in the \"Polish-Soviet War,\", but he was released after three months. Upon his release, Feliks made efforts to obtain Anna's release from prison, locate and take care of Jerzy and Maria, and reunite his family. Anna was released from prison camp in June 1946, and the family moved to Olsztyn. They chose Olsztyn based on the fact that most of the Vilnius board of education where Feliks once worked had been evacuated to Olsztyn, and Feliks could resume his position there.\n\nFollowing World War II, Sikorski's mother was prevented from teaching due to political persecution. After Sikorski graduated from high school in 1953, he was repeatedly denied entry to university in spite of passing exams in Gdańsk, Warsaw, Toruń, and Poznań. Each time, he received the characteristic preprinted postcard with \"not accepted due to lack of space\". On the advice of a Polish studies specialist teacher (Polish philologist), Janina Kirkicka, Sikorski was able to enroll in a two-year teacher's college in Olsztyn, but after two weeks the director Bolesław Wytrążek instructed Sikorski to leave the college, due the decision of the local Polish United Workers' Party (PZPR) in Olsztyn. Through , who was the director at the Museum of Warmia and Masuria located at Olsztyn Castle, Sikorski obtained employment (1954–1955) at the , which was under the patronage of the regional museum in Olsztyn. The work at Frombork’s museum and the person of the great Polish Astronomer Nicolas Copernicus, who spent most of his life in Frombork and Warmia region, made a lasting impression on Sikorski. As a result, Copernicus and \"Copernicana\" research became the dominant subject in Sikorski's lifetime work and in his professional affiliations with various research centers. Ultimately, it led to Sikorski's discovery and publication of Copernicus' resting place at Frombork’s cathedral in 1973, which was confirmed archaeologically in 2005.\n\nIn the fall of 1955 Sikorski enrolled at the Nicolaus Copernicus University in Toruń, where he chose to study under professor , who among other scholarly pursuits studied and researched the life and politics of Copernicus' maternal uncle, Bishop Lucas Watzenrode. Sikorski wrote his master’s thesis on Watzenrode, and subsequently wrote his doctoral dissertation \"Polish monarchy and Warmia at the end of the 15th century: Issues in systemic-law and politics\".\n\nUpon completing of his academic studies at the Nicolas Copernicus University in Toruń in 1960, Sikorski accepted a position at the Museum of Warmia at Lidzbark Warmiński Castle, where Copernicus spent at least seven years (1503–1509). Copernicus had strong ties to the city of Lidzbark, and had visited there at the invitation of his maternal uncle Bishop Lucas Watzenrode in 1495. Sikorski worked at the museum from 1961 to 1962.\n\nLidzbark Warmiński Castle was of substantial significance for Jerzy Sikorski's \"Copernicana\" studies in terms of the history of astronomy; as well as because the castle was a strongly fortified seat of the bishops of Warmia and the administration center of their lands from the second half of the 14th century. At that time, Lidzbark city had one of the largest populations in Warmia, similar to Braniewo. In Lidzbark, Sikorski researched historic documents pertaining to the history of Poland's Baltic seashore (Pomerania, Warmia, Mazury, Prussia), and its extant architectural relics, documents, and other materials relating to Nicolaus Copernicus. Following the death of bishop Watzenrode in Toruń on March 29, 1512, Copernicus only sporadically visited Lidzbark, either as an emissary of the Warmia Chapter, or as a personal medical doctor for the successive bishops: (d. 1523), Mauritius Ferber (Polish: Maurycy Ferber 1471–1537), and Johannes Dantiscus (Polish: Jan Dantyszek; 1485–1548).\n\nAn organized index of published books and articles by Sikorski is listed at his web portal, covering four areas of research: 1. Copernicana; 2. Cities and castles of Prussia; 3. Historic tradition of the region; and, 4. The History of Science.\n\nAn additional indexed list of Sikorski's complete articles and papers is available online at the Database of Articles (in Polish), including two English language papers: \"The Empirical Table of Olsztyn the Question of Nicolaus Copernicus' Scientific Workshop\", and \"The Practice of Bishops' Burials in Frombork Cathedral and the Question of the Grave of Nicolaus Copernicus' Uncle Łukasz Watzenrode\".\nIn 1973, in time for the 500th anniversary of Nicolas Copernicus' birth, Sikorski authored a popular monograph on the astronomer's life, work, and times. Three updated editions were published, in 1985, 1995, and 2011.\n\nDr Jerzy Sikorski is credited for history of science consultation in the opening titles to the Polish motion picture \"Kopernik\", released in Toruń, Poland, on February 14, 1973. The film was released on the occasion of the world and United Nations celebrations of the 500th anniversary of Copernicus’ birth that were organized by the International Union of History and Philosophy of Science.\n\nBetween 1966 and 2007 Jerzy Sikorski published a number of articles in \"Mazury-Warmian Communications\" (\"\"), on the history of Poland's ancestral Baltic seashore Pomerania, and the Polish regions of Warmia and Masuria. He also published articles on the life and activities of the Polish astronomer Nicolaus Copernicus. \n\nSikorski was a member of the editorial staff at \"Komunikaty Mazursko-Warmińskie\", and also a member of staff at \"Frombork's Commentaries\" (\"Komentarze Fromborskie\"), and also the substitute editor-in-chief of \"Olsztyn Yearly\" (\"\"),\n\nJerzy Sikorski published a number of short biographies of Warmian Chapter canons and members in the \"Polish Biographical Dictionary\", which is printed by the Polish Academy of Sciences. These included biographies of Preuck Jan (1575–1631), Preuck Jerzy (d. 1556), Reich Feliks (c. 1475–1539), Sculteti Aleksander (1485–1564), Sculteti Bernard (d. 1518), and Sculteti Jan (d. 1526).\n\nReviewing the document \"Locationes mansorum desertorum\" (\"Allocation of abandoned fiefs\"), which was written by Copernicus in the region of Olsztyn Castle, Olsztyn, Poland, Jerzy Sikorski discovered that, out of the 136 names of allocation of fiefs, 60 were Polish names (peasants had only first names) written in phonetically correct Polish. Sikorski also cites an extant document where Copernicus’ witness was Copernicus’ Polish servant: \"Wojciech Cebulski\". These documents indicate that Copernicus spoke and wrote Polish.\nIn a letter to king Sigismund I of Poland handwritten by Nicolas Copernicus in Olsztyn Castle, the administrator, chancellor, and commander in chief of the defense of Olsztyn Castle Copernicus and Warmia Canons together affirm their Polish nationality as subjects to King Sigismund I of Poland against the enemy, the German Teutonic Order. The writers say that they are willing to die defending Olsztyn Castle, Warmia, and Poland from the Teutonic Knights. Nicolas Copernicus is directly associated with Sigismund I of Poland in the wars against the Teutonic Order, the reform of royal mints and the minting of coins, in establishing modern market economy in 16th century Poland, in direct contacts with king's personal medical doctor, with Cracow (Kraków) and the Jagiellonian University, and with the Polish Roman Catholic Church in Cracow.\n\nSikorski wrote \"the intellectual adventure of my life were my discoveries of Copernicus’ Polish ancestry and nationality\".\n\nCopernicus was reportedly buried in Frombork Cathedral, but archaeologists searched there in vain for centuries for his remains. Efforts to locate the remains in 1581, 1626, 1802, 1909, 1939 and 2004 came to nought. In 1973 Dr Jerzy Sikorski published the location of Copernicus' resting place in his book, including the photo of the Altar of Saint Wacław, today Altar of Saint Cross, with a subscription (English translation of Polish) \"The remains of Mikołaj Kopernik rest unnamed next to this altar\".\n\nSikorski's analysis of recovered chapter documents guided the Polish archaeological searches of Frombork in 2004–2005. The Institute of Anthropology and Archaeology at the Pultusk Academy of Humanities of Aleksander Gieysztor, with Polish archaeologists under the direction of , commenced the search for Copernicus’ grave from 16–31 of August, 2004, in an area of 10 square meters, and this search was financially supported by the Archaeologic Foundation of Prof Konrad Jażdżewski in Lódz, Poland. In the second archaeological search conducted in August 2005, Copernicus’ skull and remains were discovered in a grave marked by archaeologists as 13/5. Forensic expert Capt. Dariusz Zajdel of the Polish Police Central Forensic Laboratory used the skull to reconstruct a face that closely resembled the features—including a broken nose and a scar above the left eye—on a Copernicus self-portrait. The expert also determined that the skull belonged to a man who had died around age 70—Copernicus's age at the time of his death.\n\nThe remains were genetically tested in Poland and Sweden and found to match hair samples taken from a book owned by Copernicus which was kept at the library of the University of Uppsala in Sweden.\n\nSikorski discovered the location of Copernicus' Canonic Curia outside the walls of Frombork (the \"Curiae extra muros\"). The building is not extant, having been burned by Teutonic Knights on February 1, 1520, but the foundation remains. \n\nSikorski searched the area nearby with instruments such as ground-penetrating radar in an effort to find Copernicus’ observatory \"pavimentum\", which was believed to have been nearby, but did not find it.\nSikorski was aided in the search by the notebook of Elias Olsen, who was sent by Tycho Brahe to Frombork in 1584 to use the still-extant \"pavimentum\" to obtain astronomic observations for comparison with those of Copernicus. the \"pavimentum\" has still not been located. Its foundations may have been destroyed during extensive ground works near Copernicus’ external curia. \n\nSikorski since 1973 researched and wrote on Copernicus' observatory in Olsztyn, which contains a still-extant plaster astronomical table that was used by Copernicus from 1516 to 1521.\n\nSikorski has been awarded Polish presidential medals for his professional work as a historian and Copernicologist: the Gold Cross of Merit (1975) and Knight's Cross of the Order of Polonia Restituta (1986).\n\nOn the fifteenth anniversary of the Warmia-Mazury Business Club, the President of Poland Lech Kaczyński gave presidential orders and awards to deserving club members. On March 28, 2008, the title \"Personality of the Year 2008 in Warmia and Mazury\" was awarded to Jerzy Sikorski for his research into the life and science of Nicolas Copernicus, and for his 1973 discovery of Copernicus's resting place.\n\n\n"}
{"id": "24529515", "url": "https://en.wikipedia.org/wiki?curid=24529515", "title": "List of DVD recordable manufacturers", "text": "List of DVD recordable manufacturers\n\nThis aims to be a complete list of DVD recordable manufacturers.\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly.\n\n\n\n\n\n\n\n"}
{"id": "13725692", "url": "https://en.wikipedia.org/wiki?curid=13725692", "title": "List of smart cards", "text": "List of smart cards\n\nSome widely used contactless smart cards include Sydney's Opal Card, London's Oyster card, South Korea's T-money, Hong Kong's Octopus card, Stockholm's Access card, Japan's Suica and Pasmo cards, Manila's Beep cards, Nigeria's ETC Card, Paris' Calypso/Navigo, the Dutch OV-Chipkaart, Toronto's Presto card and Lisbon's LisboaViva card, which predate the ISO/IEC 14443 standard. The following tables list smart cards used for public transport and other electronic purse applications.\n\n"}
{"id": "1390149", "url": "https://en.wikipedia.org/wiki?curid=1390149", "title": "Medieval technology", "text": "Medieval technology\n\nMedieval technology is the technology used in medieval Europe under Christian rule. After the Renaissance of the 12th century, medieval Europe saw a radical change in the rate of new inventions, innovations in the ways of managing traditional means of production, and economic growth. The period saw major technological advances, including the adoption of gunpowder, the invention of vertical windmills, spectacles, mechanical clocks, and greatly improved water mills, building techniques (Gothic architecture, medieval castles), and agriculture in general (three-field crop rotation).\n\nThe development of water mills from their ancient origins was impressive, and extended from agriculture to sawmills both for timber and stone. By the time of the \"Domesday Book\", most large villages had turnable mills, around 6,500 in England alone. Water-power was also widely used in mining for raising ore from shafts, crushing ore, and even powering bellows.\n\nEuropean technical advancements from the 12th to 14th centuries were either built on long-established techniques in medieval Europe, originating from Roman and Byzantine antecedents, or adapted from cross-cultural exchanges through trading networks with the Islamic world, China, and India. Often, the revolutionary aspect lay not in the act of invention itself, but in its technological refinement and application to political and economic power. Though gunpowder along with other weapons had been started by Chinese, it was the Europeans who developed and perfected its military potential, precipitating European expansion and eventual imperialism in the Modern Era.\n\nAlso significant in this respect were advances in maritime technology. Advances in shipbuilding included the multi-masted ships with lateen sails, the sternpost-mounted rudder and the skeleton-first hull construction. Along with new navigational techniques such as the dry compass, the Jacob's staff and the astrolabe, these allowed economic and military control of the seas adjacent to Europe and enabled the global navigational achievements of the dawning Age of Exploration.\n\nAt the turn to the Renaissance, Gutenberg’s invention of mechanical printing made possible a dissemination of knowledge to a wider population, that would not only lead to a gradually more egalitarian society, but one more able to dominate other cultures, drawing from a vast reserve of knowledge and experience. The technical drawings of late-medieval artist-engineers Guido da Vigevano and Villard de Honnecourt can be viewed as forerunners of later Renaissance works such as Taccola or da Vinci.\n\nThe following is a list of some important medieval technologies. The approximate date or first mention of a technology in medieval Europe is given. Technologies were often a matter of cultural exchange and date and place of first inventions are not listed here (see main links for a more complete history of each).\n\nCarruca (6th to 9th centuries)\n\nA type of heavy wheeled plough commonly found in Northern Europe. The device consisted of four major parts. The first part was a Coulter (agriculture) at the bottom of the plough. This knife was used to vertically cut into the top sod to allow for the plowshare to work. The plowshare was the second pair of knives which cut the sod horizontally, detaching it from the ground below. The third part was the moldboard, which curled the sod outward. The fourth part of the device was the team of eight oxen guided by the farmer. This type of plough eliminated the need for cross-plowing by turning over the furrow instead of merely pushing it outward. This type of wheeled plough made seed placement more consistent throughout the farm as the blade could be locked in at a certain level relative to the wheels. A disadvantage to this type of plough was its maneuverability. Since this equipment was large and lead by a small herd of oxen, turning the plough was difficult and time-consuming. This caused many farmers to turn away from traditional square fields and adopt a longer, more rectangular field to ensure maximum efficiency.\n\nArd (plough) (5th century)\n\nAn early medieval plough that consisted of a sharpened wooden post pulled by either animals or humans. This lightweight and primitive plough was used primarily before the invention of the carruca. The ard was inefficient in more firm northern soil but did decent work in southern areas where the soil was much softer. Although the ard required the user to apply constant pressure to the plough in order to make sure the edge could break the ground, the soil was merely pushed to the sides instead of being properly turned over.\n\nHorse collar (6th to 9th centuries)\n\nOnce oxen started to be replaced by horses on farms and in fields, the yoke became obsolete due to its shape not working well with a horses' posture. The first design for a horse collar was a throat-and-girth-harness. These types of harnesses were unreliable though due to them not being sufficiently set in place. The loose straps were prone to slipping and shifting positions as the horse was working and often caused asphyxiation. Around the eighth century, the introduction of the rigid collar eliminated the problem of choking. The rigid collar was \"placed over the horses head and rested on its shoulders. This permitted unobstructed breathing and placed the weight of the plow or wagon where the horse could best support it.\"\n\nHorseshoes (9th century)\n\nAllowed horses to carry larger loads and move around with greater traction on hard to walk surfaces. The practice of shoeing horses was initially practiced in the Roman Empire but lost popularity throughout the Middle Ages until around the 11th century. Although horses in the southern lands could easily work while on the softer soil, the rocky soil of the north proved to be damaging to the horses' hooves. Since the north was the problematic area, this is where shoeing horses first became popular. The introduction of gravel roadways was also cause for the popularity of horseshoeing. The loads a shoed horse could take on these roads were significantly higher than one that was barefoot. By the 14th century, not only did horses have shoes, but many farmers were shoeing oxen and donkeys in order to help prolong the life of their hooves. The size and weight of the horseshoe changed significantly over the course of the middle ages. In the 10th century, horseshoes were secured by six nails and weighed around one-quarter of a pound, but throughout the years, the shoes grew larger and by the 14th century, the shoes were being secured with eight nails and weighed nearly half a pound.\n\nCrop rotation (8th century)\n\nAlso called the Two-field system. This system included the farmers' field being divided into two separate crops. One field would grow a crop while the other was allowed to lie fallow and was used to feed livestock and regain lost nutrients. Every year, the two fields would switch in order to ensure fields did not become nutrient deficient. In the 11th century, this system was introduced into Sweden and spread to become the most popular form of farming.\n\nThree-field system (8th century)\n\nThe ideal three-field system is one that separates a section of land into three equal parts. Each one of the three parts holds a different crop. One part holds a spring crop, such as barley or oats, another part holds a winter crop, such as wheat or rye, and the third part is an off-field that is left alone to grow and is used to help feed livestock. By rotating the three crops to a new part of the land after each year, the off-field regains some of the nutrients lost during the growing of the two crops. This system increases agricultural productivity over the Two-field system by only having one-third of the field not being used instead of one half. Another advantage of crop rotation is that many scholars believe it helped increase yields by up to 50%.\n\nWine press (12th century)\n\nThis device was the first practical means of Pressing (wine) on a plane surface. The wine press was an expensive piece of machinery that only the wealthy could afford. The method of Grape stomping was often used as a less expensive alternative. While white wines required the use of a wine press in order to preserve the color of the wine by removing the juices quickly from the skin, red wine did not need to be pressed until the end of the juice removal process since the color did not matter. Many red wine winemakers used their feet to smash the grapes then used a press to remove any juice that remained in the grape skins.\n\nQanat (5th century)\n\nAn underground passage used to water fields, crops, and provide drinking water. These tunnels had a gradual slope which used gravity to pull the water from either an aquifer or water well. This system was originally found in middle eastern areas and is still used today in places where surface water is hard to find.\n\nPendentive architecture (6th century)\n\nA specific spherical form in the upper corners to support a dome. Although the first experimentation was made in the 200s, it was in the 6th century in the Byzantine Empire that its potential was fully achieved.\n\nArtesian well (1126)\n\nA thin rod with a hard iron cutting edge is placed in the bore hole and repeatedly struck with a hammer, underground water pressure forces the water up the hole without pumping. Artesian wells are named after the town of Artois in France, where the first one was drilled by Carthusian monks in 1126.\n\nCentral heating through underfloor channels (9th century)\n\nIn the early medieval Alpine upland, a simpler central heating system where heat travelled through underfloor channels from the furnace room replaced the Roman hypocaust at some places. In Reichenau Abbey a network of interconnected underfloor channels heated the 300 m large assembly room of the monks during the winter months. The degree of efficiency of the system has been calculated at 90%.\n\nRib vault (12th century)\n\nAn essential element for the rise of Gothic architecture, rib vaults allowed vaults to be built for the first time over rectangles of unequal lengths. It also greatly facilitated scaffolding and largely replaced the older groin vault.\n\nChimney (12th century)\n\nThe earliest true chimneys appeared in Northern Europe during the 12th century, and with them came the first true fireplaces.\n\nSegmental arch bridge (1345)\n\nThe Ponte Vecchio in Florence is considered medieval Europe's first stone segmental arch bridge.\nTreadwheel crane (1220s)\n\nThe earliest reference to a treadwheel in archival literature is in France about 1225, followed by an illuminated depiction in a manuscript of probably also French origin dating to 1240. Apart from tread-drums, windlasses and occasionally cranks were employed for powering cranes.\n\nStationary harbour crane (1244)\n\nStationary harbour cranes are considered a new development of the Middle Ages; its earliest use being documented for Utrecht in 1244. The typical harbour crane was a pivoting structure equipped with double treadwheels. There were two types: wooden gantry cranes pivoting on a central vertical axle and stone tower cranes which housed the windlass and treadwheels with only the jib arm and roof rotating. These cranes were placed on docksides for the loading and unloading of cargo where they replaced or complemented older lifting methods like see-saws, winches and yards. Slewing cranes which allowed a rotation of the load and were thus particularly suited for dockside work appeared as early as 1340.\n\nFloating crane\n\nBeside the stationary cranes, floating cranes which could be flexibly deployed in the whole port basin came into use by the 14th century.\n\nMast crane\n\nSome harbour cranes were specialised at mounting masts to newly built sailing ships, such as in Gdańsk, Cologne and Bremen.\n\nWheelbarrow (1170s)\n\nThe wheelbarrow proved useful in building construction, mining operations, and agriculture. Literary evidence for the use of wheelbarrows appeared between 1170 and 1250 in north-western Europe. The first depiction is in a drawing by Matthew Paris in the mid-13th century.\n\nOil paint (by 1125)\n\nAs early as the 13th century, oil was used to add details to tempera paintings and paint wooden statues. Flemish painter Jan van Eyck developed the use of a stable oil mixture for panel painting around 1410.\n\nHourglass (1338)\n\nReasonably dependable, affordable and accurate measure of time. Unlike water in a clepsydra, the rate of flow of sand is independent of the depth in the upper reservoir, and the instrument is not liable to freeze. Hourglasses are a medieval innovation (first documented in Siena, Italy).\n\nMechanical clocks (13th to 14th centuries)\n\nA European innovation, these weight-driven clocks were used primarily in clock towers.\n\nCompound crank\n\nThe Italian physician Guido da Vigevano combines in his 1335 \"Texaurus\", a collection of war machines intended for the recapture of the Holy Land, two simple cranks to form a compound crank for manually powering war carriages and paddle wheel boats. The devices were fitted directly to the vehicle's axle respectively to the shafts turning the paddle wheels.\n\nBlast furnace (1150–1350)\n\nEuropean cast iron first appears in Middle Europe (for instance Lapphyttan in Sweden, Dürstel in Switzerland and the Märkische Sauerland in Germany) around 1150, in some places according to recent research even before 1100. The technique is considered to be an independent European development.\n\nShip mill (6th century)\n\nThe ship mill is a Byzantine invention, designed to mill grains using hydraulic power. The technology eventually spread to the rest of Europe and was in use until ca. 1800.\nPaper mill (13th century)\n\nThe first certain use of a water-powered paper mill, evidence for which is elusive in both Chinese and Muslim paper making, dates to 1282.\n\nRolling mill (15th century)\n\nUsed to produce metal sheet of an even thickness. First used on soft, malleable metals, such as lead, gold and tin. Leonardo da Vinci described a rolling mill for wrought iron.\n\nTidal Mills (6th century)\n\nThe earliest tidal mills were excavated on the Irish coast where watermillers knew and employed the two main waterwheel types: a 6th-century tide mill at Killoteran near Waterford was powered by a vertical waterwheel, while the tide changes at Little Island were exploited by a twin-flume horizontal-wheeled mill (c. 630) and a vertical undershot waterwheel alongside it. Another early example is the Nendrum Monastery mill from 787 which is estimated to have developed seven to eight horsepower at its peak.\n\nVertical windmills (1180s)\n\nInvented in Europe as the pivotable post mill, the first surviving mention of one comes from Yorkshire in England in 1185. They were efficient at grinding grain or draining water. Stationary tower mills were also developed in the 13th century.\n\nWater hammer (12th century at the latest)\n\nUsed in metallurgy to forge the metal blooms from bloomeries and Catalan forges, they replaced manual hammerwork. The water hammer was eventually superseded by steam hammers in the 19th century.\n\nDry compass (12th century)\n\nThe first European mention of the directional compass is in Alexander Neckam's \"On the Natures of Things\", written in Paris around 1190. It was either transmitted from China or the Arabs or an independent European innovation. Dry compass were invented in the Mediterranean around 1300.\n\nAstronomical compass (1269)\n\nThe French scholar Pierre de Maricourt describes in his experimental study \"Epistola de magnete\" (1269) three different compass designs he has devised for the purpose of astronomical observation.\nStern-mounted rudders (1180s)\n\nThe first depiction of a pintle-and-gudgeon rudder on church carvings dates to around 1180. They first appeared with cogs in the North and Baltic Seas and quickly spread to Mediterranean. The iron hinge system was the first stern rudder permanently attached to the ship hull and made a vital contribution to the navigation achievements of the age of discovery and thereafter.\n\nMovable type printing press (1440s)\n\nJohannes Gutenberg's great innovation was not the printing itself, but instead of using carved plates as in woodblock printing, he used separate letters (\"types\") from which the printing plates for pages were made up. This meant the types were recyclable and a page cast could be made up far faster.\n\nPaper (13th century)\n\nPaper was invented in China and transmitted through Islamic Spain in the 13th century. In Europe, the paper-making processes was mechanized by water-powered mills and paper presses (see paper mill).\n\nRotating bookmark (13th century)\n\nA rotating disc and string device used to mark the page, column, and precise level in the text where a person left off reading in a text. Materials used were often leather, velum, or paper.\n\nSpectacles (1280s)\n\nThe first spectacles, invented in Florence, used convex lenses which were of help only to the far-sighted. Concave lenses were not developed prior to the 15th century.\n\nWatermark (1282)\n\nThis medieval innovation was used to mark paper products and to discourage counterfeiting. It was first introduced in Bologna, Italy.\n\nTheory of impetus (6th century)\n\nA scientific theory that was introduced by John Philoponus who made criticism of Aristotelian principles of physics, and it served as an inspiration to medieval scholars as well as to Galileo Galilei who ten centuries later, during the Scientific Revolution, extensively cited Philoponus in his works while making the case as to why Aristotelian physics was flawed. It is the intellectual precursor to the concepts of inertia, momentum and acceleration in classical mechanics.\n\nArabic numerals (13th century)\n\nThe first recorded mention in Europe was in 976, and they were first widely published in 1202 by Fibonacci with his \"Liber Abaci\".\n\nUniversity\n\nThe first medieval universities were founded between the 11th and 13th centuries leading to a rise in literacy and learning. By 1500, the institution had spread throughout most of Europe and played a key role in the Scientific Revolution. Today, the educational concept and institution has been globally adopted.\n\nFunctional button (13th century)\n\nGerman buttons appeared in 13th-century Germany as an indigenous innovation. They soon became widespread with the rise of snug-fitting clothing.\n\nHorizontal loom (11th century)\n\nHorizontal looms operated by foot-treadles were faster and more efficient.\n\nSilk (6th century)\n\nManufacture of silk began in Eastern Europe in the 6th century and in Western Europe in the 11th or 12th century. Silk had been imported over the Silk Road since antiquity. The technology of \"silk throwing\" was mastered in Tuscany in the 13th century. The silk works used waterpower and some regard these as the first mechanized textile mills.\n\nSpinning wheel (13th century)\n\nBrought to Europe probably from India.\n\nChess (1450)\n\nThe earliest predecessors of the game originated in 6th-century AD India and spread via Persia and the Muslim world to Europe. Here the game evolved into its current form in the 15th century.\n\nForest glass (c. 1000)\n\nThis type of glass uses wood ash and sand as the main raw materials and is characterised by a variety of greenish-yellow colours.\n\nGrindstones (834)\n\nGrindstones are a rough stone, usually sandstone, used to sharpen iron. The first rotary grindstone (turned with a leveraged handle) occurs in the \"Utrecht Psalter\", illustrated between 816 and 834. According to Hägermann, the pen drawing is a copy of a late-antique manuscript. A second crank which was mounted on the other end of the axle is depicted in the \"Luttrell Psalter\" from around 1340.\n\nLiquor (12th century)\n\nPrimitive forms of distillation were known to the Babylonians, as well as Indians in the first centuries AD. Early evidence of distillation also comes from alchemists working in Alexandria, Roman Egypt, in the 1st century. The medieval Arabs adopted the distillation process, which later spread to Europe. Texts on the distillation of waters, wine, and other spirits were written in Salerno and Cologne in the twelfth and thirteenth centuries.\n\nLiquor consumption rose dramatically in Europe in and after the mid-14th century, when distilled liquors were commonly used as remedies for the Black Death. These spirits would have had a much lower alcohol content (about 40% ABV) than the alchemists' pure distillations, and they were likely first thought of as medicinal elixirs. Around 1400, methods to distill spirits from wheat, barley, and rye were discovered. Thus began the \"national\" drinks of Europe, including gin (England) and \"grappa\" (Italy). In 1437, \"burned water\" (brandy) was mentioned in the records of the County of Katzenelnbogen in Germany. \n\nMagnets (12th century)\n\nMagnets were first referenced in the \"Roman d'Enéas\", composed between 1155 and 1160.\n\nMirrors (1180)\n\nThe first mention of a \"glass\" mirror is in 1180 by Alexander Neckham who said \"Take away the lead which is behind the glass and there will be no image of the one looking in.\"\n\nIllustrated surgical atlas (1345)\n\nGuido da Vigevano (c. 1280 − 1349) was the first author to add illustrations to his anatomical descriptions. His \"Anathomia\" provides pictures of neuroanatomical structures and techniques such as the dissection of the head by means of trephination, and depictions of the meninges, cerebrum, and spinal cord.\n\nQuarantine (1377)\n\nInitially a 40-day-period, the quarantine was introduced by the Republic of Ragusa as a measure of disease prevention related to the Black Death. It was later adopted by Venice from where the practice spread all around in Europe.\n\nRat traps (1170s)\n\nThe first mention of a rat trap is in the medieval romance \"Yvain, the Knight of the Lion\" by Chrétien de Troyes.\n\nSoap (9th century)\n\nSoap came into widespread European use in the 9th century in semi-liquid form, with hard soap perfected by the Arabs in the 12th century.\n\nQuilted Armour (pre 5th - 14th Century)\n\nThere was a vast amount of armour technology available through the 5th to 16th centuries. \nMost soldiers during this time wore padded or quilted armor. This was the cheapest and most available armor for the majority of soldiers. Quilted armour was usually just a jacket made of thick linen and wool meant to pad or soften the impact of blunt weapons and light blows. Although, this technology predated the 5th century, it was still extremely prevalent because of the low cost and the weapon technology at the time made the bronze armor of the Greeks and Romans obsolete. Quilted armour was also used in conjunction with other types of armour. Usually worn over or under leather, mail, and later plate armour.\n\nCuir Bouilli (5th-10th Century)\n\nHardened leather armour also called Cuir Bouilli was a step up from quilted armour. Made by boiling leather in either water, wax or oil to soften it so it can be shaped, it would then be allowed to dry and become very hard. Large pieces of armour could be made such as breast plates, helmets, and leg guards, but many times smaller pieces would be sewn into the quilting of quilted armour or strips would be sewn together on the outside of a linen jacket. This was not as affordable as the quilted armour but offered much better protection against edged slashing weapons.\n\nChain Mail (11th-16th Century)\n\nThe most common type during the 11th through the 16th centuries was the Hauberk, also known earlier than the 11th century as the Carolingian byrnie. Made of interlinked rings of metal, it sometimes consisted of a coif that covered the head and a tunic that covered the torso, arms, and legs down to the knees. Chain mail was very effective at protecting against light slashing blows but ineffective against stabbing or thrusting blows. The great advantage was that it allowed a great freedom of movement and was relatively light with significant protection over quilted or hardened leather armour. It was far more expensive than the hardened leather or quilted armour because of the massive amount of labor it required to create. This made it unattainable for most soldiers and only the more wealthy soldiers could afford it. Later, toward the end of the 13th century banded mail became popular. Constructed of washer shaped rings of iron overlapped and woven together by straps of leather as opposed to the interlinked metal rings of chain mail, banded mail was much more affordable to manufacture. The washers were so tightly woven together that it was very difficult penetrate and offered greater protection from arrow and bolt attacks. \n\nJazerant (11th century)\n\nThe Jazerant or Jazeraint was an adaptation of chain mail in which the chain mail would be sewn in between layers of linen or quilted armour. Exceptional protection against light slashing weapons and slightly improved protection against small thrusting weapons, but little protection against large blunt weapons such as maces and axes. This gave birth to reinforced chain mail and became more prevalent in the 12th and 13th century. Reinforced armour was made up of chain mail with metal plates or hardened leather plates sewn in. This greatly improved protection from stabbing and thrusting blows. \n\nScale Armour (12th century)\n\nA type of Lamellar armour, was made up entirely of small, overlapping plates. Either sewn together, usually with leather straps, or attached to a backing such as linen, or a quilted armor. Scale armour does not require the labor to produce that chain mail does and therefore is more affordable. It also affords much better protection against thrusting blows and pointed weapons. Though, it is much heavier, more restrictive and impedes free movement. \n\nPlate Armour (14th century)\n\nPlate armour covered the entire body. Although parts of the body were already covered in plate armour as early as 1250, such as the Poleyns for covering the knees and Couters - plates that protected the elbows, the first complete full suit without any textiles was seen around 1410-1430. Components of medieval armour that made up a full suit consisted of a cuirass, a gorget, vambraces, gauntlets, cuisses, greaves, and sabatons held together by internal leather straps. Improved weaponry such as crossbows and the long bow had greatly increased range and power. This made penetration of the chain mail hauberk much easier and more common. By the mid 1400's most plate was worn alone and without the need of a hauberk. Advances in metal working such as the blast furnace and new techniques for carburizing made plate armour nearly impenetrable and the best armour protection available at the time. Although plate armour was fairly heavy, because each suit was custom tailored to the wearer, it was very easy to move around in. A full suit of plate armour was extremely expensive and mostly unattainable for the majority of soldiers. Only very wealthy land owners and nobility could afford it. The quality of plate armour increases as more armour makers became more proficient in metal working. A suit of plate armour became a symbol of social status and the best made were personalized with embellishments and engravings. Plate armour saw continued use in battle until the 17th century.\n\nArched saddle (11th century)\n\nThe arched saddle enabled mounted knights to wield lances underarm and prevent the charge from turning into an unintentional pole-vault. This innovation gave birth to true shock cavalry, enabling fighters to charge on full gallop.\n\nSpurs (11th century)\n\nSpurs were invented by the Normans and appeared at the same time as the cantled saddle. They enabled the horseman to control his horse with his feet, replacing the whip and leaving his arms free. Rowel spurs familiar from cowboy films were already known in the 13th century. Gilded spurs were the ultimate symbol of the knighthood - even today someone is said to \"earn his spurs\" by proving his or her worthiness.\n\nStirrup (6th century)\n\nStirrups were invented by steppe nomads in what is today Mongolia and northern China in the 4th century. They were introduced in Byzantium in the 6th century and in the Carolingian Empire in the 8th. They allowed a mounted knight to wield a sword and strike from a distance leading to a great advantage for mounted cavalry.\n\nCannon (1324)\n\nCannons are first recorded in Europe at the siege of Metz in 1324. In 1350 Petrarch wrote \"these instruments which discharge balls of metal with most tremendous noise and flashes of fire...were a few years ago very rare and were viewed with greatest astonishment and admiration, but now they are become as common and familiar as any other kinds of arms.\"\n\nVolley gun\n\nSee Ribauldequin.\n\nCorned gunpowder (late 14th century)\n\nFirst practiced in Western Europe, corning the black powder allowed for more powerful and faster ignition of cannons. It also facilitated the storage and transportation of black powder. Corning constituted a crucial step in the evolution of gunpowder warfare.\nSupergun (late 14th century)\n\nExtant examples include the wrought-iron Pumhart von Steyr, Dulle Griet and Mons Meg as well as the cast-bronze Faule Mette and Faule Grete (all from the 15th century).\n\nCounterweight trebuchet (12th century)\n\nPowered solely by the force of gravity, these catapults revolutionized medieval siege warfare and construction of fortifications by hurling huge stones unprecedented distances. Originating somewhere in the eastern Mediterranean basin, counterweight trebuchets were introduced in the Byzantine Empire around 1100 CE, and was later adopted by the Crusader states and as well by the other armies of Europe and Asia.\n\nGreek fire (7th century)\n\nAn incendiary weapon which could even burn on water is also attributed to the Byzantines where they installed it on their ships. It played a crucial role in the Byzantine Empire's victory over the Umayyad Caliphate during the Siege of Constantinople (717–718)\n\nGrenade (8th century) \n\nRudimentary incendiary grenades appeared in the Byzantine Empire, as the Byzantine soldiers learned that Greek fire, a Byzantine invention of the previous century, could not only be thrown by flamethrowers at the enemy, but also in stone and ceramic jars. \n\nLongbow with massed, disciplined archery (13th century)\n\nHaving a high rate of fire and penetration power, the longbow contributed to the eventual demise of the medieval knight class. Used particularly by the English to great effect against the French cavalry during the Hundred Years' War (1337–1453).\n\nSteel crossbow (late 14th century)\n\nEuropean innovation. Came with several different cocking aids to enhance draw power, making the weapons also the first hand-held mechanical crossbows.\n\nCombined arms tactics (14th century)\n\nThe battle of Halidon Hill 1333 was the first battle where intentional and disciplined combined arms infantry tactics were employed. The English men-at-arms dismounted aside the archers, combining thus the staying power of super-heavy infantry and striking power of their two-handed weapons with the missiles and mobility of the archers using longbows and shortbows\n. Combining dismounted knights and men-at-arms with archers was the archetypal Western Medieval battle tactics until the battle of Flodden 1513 and final emergence of firearms.\n\n\n\n"}
{"id": "21534962", "url": "https://en.wikipedia.org/wiki?curid=21534962", "title": "Ministry of Science and Technology Development (Zimbabwe)", "text": "Ministry of Science and Technology Development (Zimbabwe)\n\nThe Ministry of Science and Technology Development is a government ministry, responsible for science and technology in Zimbabwe. The incumbent is Henry Dzinotyiwei. Its oversees:\n"}
{"id": "3975118", "url": "https://en.wikipedia.org/wiki?curid=3975118", "title": "OBD-II PIDs", "text": "OBD-II PIDs\n\nOBD-II PIDs (On-board diagnostics Parameter IDs) are codes used to request data from a vehicle, used as a diagnostic tool.\n\nSAE standard J1979 defines many OBD-II PIDs. All on-road vehicles and trucks sold in North America are required to support a subset of these codes, primarily for state mandated emissions inspections. Manufacturers also define additional PIDs specific to their vehicles. Though not mandated, many motorcycles also support OBD-II PIDs.\n\nIn 1996, light duty vehicles (less than ) were the first to be mandated followed by medium duty vehicles (between ) in 2005. They are both required to be accessed through a standardized data link connector defined by SAE J1962.\n\nHeavy duty vehicles (greater than ) made after 2010, for sale in the US are allowed to support OBD-II diagnostics through SAE standard J1939-73 (a round diagnostic connector) according to CARB in title 13 CCR 1971.1. Some heavy duty trucks in North America use the SAE J1962 OBD-II diagnostic connector that is common with passenger cars, notably Mack and Volvo Trucks, however they use 29 bit CAN identifiers (unlike 11 bit headers used by passenger cars).\n\nThere are 10 diagnostic services described in the latest OBD-II standard SAE J1979. Before 2002, J1979 referred to these services as \"modes\". They are as follows:\n\nVehicle manufacturers are not required to support all services. Each manufacturer may define additional services above #9 (e.g.: service 22 as defined by SAE J2190 for Ford/GM, service 21 for Toyota) for other information e.g. the voltage of the traction battery in a hybrid electric vehicle (HEV).\n\nThe table below shows the standard OBD-II PIDs as defined by SAE J1979. The expected response for each PID is given, along with information on how to translate the response into meaningful data. Again, not all vehicles will support all PIDs and there can be manufacturer-defined custom PIDs that are not defined in the OBD-II standard.\n\nNote that services 01 and 02 are basically identical, except that service 01 provides current information, whereas service 02 provides a snapshot of the same data taken at the point when the last diagnostic trouble code was set. The exceptions are PID 01, which is only available in service 01, and PID 02, which is only available in service 02. If service 02 PID 02 returns zero, then there is no snapshot and all other service 02 data is meaningless.\n\nWhen using Bit-Encoded-Notation, quantities like C4 means bit 4 from data byte C. Each bit is numerated from 0 to 7, so 7 is the most significant bit and 0 is the least significant bit.\n\nService 02 accepts the same PIDs as service 01, with the same meaning, but information given is from when the freeze frame was created.\n\nYou have to send the frame number in the data section of the message.\n\nSome of the PIDs in the above table cannot be explained with a simple formula. A more elaborate explanation of these data is provided here:\n\nA request for this PID returns 4 bytes of data. Each bit, from MSB to LSB, represents one of the next 32 PIDs and specifies whether that PID is supported.\n\nFor example, if the car response is BE1FA813, it can be decoded like this:\n\nSo, supported PIDs are: 01, 03, 04, 05, 06, 07, 0C, 0D, 0E, 0F, 10, 11, 13, 15, 1C, 1F and 20\n\nA request for this PID returns 4 bytes of data, labeled A B C and D.\n\nThe first byte(A) contains two pieces of information. Bit A7 (MSB of byte A, the first byte) indicates whether or not the MIL (check engine light) is illuminated. Bits A6 through A0 represent the number of diagnostic trouble codes currently flagged in the ECU.\n\nThe second, third, and fourth bytes(B, C and D) give information about the availability and completeness of certain on-board tests. Note that test availability is indicated by set (1) bit and completeness is indicated by reset (0) bit.\n\nHere are the common bit B definitions, they are test based.\n\nThe third and fourth bytes are to be interpreted differently depending on if the engine is spark ignition (e.g. Otto or Wankel engines) or compression ignition (e.g. Diesel engines). In the second (B) byte, bit 3 indicates how to interpret the C and D bytes, with 0 being spark (Otto or Wankel) and 1 (set) being compression (Diesel).\n\nThe bytes C and D for spark ignition monitors (e.g. Otto or Wankel engines):\nAnd the bytes C and D for compression ignition monitors (Diesel engines):\n\nA request for this PID returns 4 bytes of data. \nThe first byte is always zero. The second, third, and fourth bytes give information about the availability and completeness of certain on-board tests. As with PID 01, the third and fourth bytes are to be interpreted differently depending on the ignition type (B3) – with 0 being spark and 1 (set) being compression. Note again that test availability is represented by a set (1) bit and completeness is represented by a reset (0) bit.\n\nHere are the common bit B definitions, they are test based.\nThe bytes C and D for spark ignition monitors (e.g. Otto or Wankel engines):\nAnd the bytes C and D for compression ignition monitors (Diesel engines):\n\nA request for this PID will return 9 bytes of data.\nThe first byte is a bit encoded field indicating which EGT sensors are supported:\n\nThe first byte is bit-encoded as follows:\n\nThe remaining bytes are 16 bit integers indicating the temperature in degrees Celsius in the range -40 to 6513.5 (scale 0.1), using the usual formula_1 formula (MSB is A, LSB is B). Only values for which the corresponding sensor is supported are meaningful.\n\nThe same structure applies to PID 79, but values are for sensors of bank 2.\n\nA request for this service returns a list of the DTCs that have been set. The list is encapsulated using the ISO 15765-2 protocol.\n\nIf there are two or fewer DTCs (4 bytes) they are returned in an ISO-TP Single Frame (SF). Three or more DTCs in the list are reported in multiple frames, with the exact count of frames dependent on the communication type and addressing details.\n\nEach trouble code requires 2 bytes to describe. The text description of a trouble code may be decoded as follows. The first character in the trouble code is determined by the first two bits in the first byte:\n\nThe two following digits are encoded as 2 bits. The second character in the DTC is a number defined by the following table:\n\nThe third character in the DTC is a number defined by\n\nThe fourth and fifth characters are defined in the same way as the third, but using bits B7-B4 and B3-B0. The resulting five-character code should look something like \"U0158\" and can be looked up in a table of OBD-II DTCs. Hexadecimal characters (0-9, A-F), while relatively rare, are allowed in the last 3 positions of the code itself.\n\nIt provides information about track in-use performance for catalyst banks, oxygen sensor banks, evaporative leak detection systems, EGR systems and secondary air system.\n\nThe numerator for each component or system tracks the number of times that all conditions necessary for a specific monitor to detect a malfunction have been encountered.\nThe denominator for each component or system tracks the number of times that the vehicle has been operated in the specified conditions.\n\nThe count of data items should be reported at the beginning (the first byte).\n\nAll data items of the In-use Performance Tracking record consist of two (2) bytes and are reported in this order (each message contains two items, hence the message length is 4).\n\nIt provides information about track in-use performance for NMHC catalyst, NOx catalyst monitor, NOx adsorber monitor, PM filter monitor, exhaust gas sensor monitor, EGR/ VVT monitor, boost pressure monitor and fuel system monitor.\n\nAll data items consist of two (2) bytes and are reported in this order (each message contains two items, hence message length is 4):\n\nSome PIDs are to be interpreted specially, and aren't necessarily exactly bitwise encoded, or in any scale.\nThe values for these PIDs are enumerated.\n\nA request for this PID returns 2 bytes of data.\nThe first byte describes fuel system #1.\n\nAny other value is an invalid response.\n\nThe second byte describes fuel system #2 (if it exists) and is encoded identically to the first byte.\n\nA request for this PID returns a single byte of data which describes the secondary air status.\n\nAny other value is an invalid response.\n\nA request for this PID returns a single byte of data which describes which OBD standards this ECU was designed to comply with. The different values the data byte can hold are shown below, next to what they mean:\n\nService 01 PID 51 returns a value from an enumerated list giving the fuel type of the vehicle. The fuel type is returned as a single byte, and the value is given by the following table:\n\nAny other value is reserved by ISO/SAE. There are currently no definitions for flexible-fuel vehicle.\n\nThe majority of all OBD-II PIDs in use are non-standard. For most modern vehicles, there are many more functions supported on the OBD-II interface than are covered by the standard PIDs, and there is relatively minor overlap between vehicle manufacturers for these non-standard PIDs.\n\nThere is very limited information available in the public domain for non-standard PIDs. The primary source of information on non-standard PIDs across different manufacturers is maintained by the US-based Equipment and Tool Institute and only available to members. The price of ETI membership for access to scan codes varies based on company size defined by annual sales of automotive tools and equipment in North America:\nHowever, even ETI membership will not provide full documentation for non-standard PIDs. ETI state:\n\nSome OEMs refuse to use ETI as a one-stop source of scan tool information. They prefer to do business with each tool company separately. These companies also require that you enter into a contract with them. The charges vary but here is a snapshot as of April 13th, 2015 of the per year charges:\n\nThe PID query and response occurs on the vehicle's CAN bus. Standard OBD requests and responses use functional addresses. The diagnostic reader initiates a query using CAN ID 7DFh, which acts as a broadcast address, and accepts responses from any ID in the range 7E8h to 7EFh. ECUs that can respond to OBD queries listen both to the functional broadcast ID of 7DFh and one assigned ID in the range 7E0h to 7E7h. Their response has an ID of their assigned ID plus 8 e.g. 7E8h through 7EFh.\n\nThis approach allows up to eight ECUs, each independently responding to OBD queries. The diagnostic reader can use the ID in the ECU response frame to continue communication with a specific ECU. In particular, multi-frame communication requires a response to the specific ECU ID rather than to ID 7DFh.\n\nCAN bus may also be used for communication beyond the standard OBD messages. Physical addressing uses particular CAN IDs for specific modules (e.g., 720h for the instrument cluster in Fords) with proprietary frame payloads.\n\nThe functional PID query is sent to the vehicle on the CAN bus at ID 7DFh, using 8 data bytes. The bytes are:\nThe vehicle responds to the PID query on the CAN bus with message IDs that depend on which module responded. Typically the engine or main ECU responds at ID 7E8h. Other modules, like the hybrid controller or battery controller in a Prius, respond at 07E9h, 07EAh, 07EBh, etc. These are 8h higher than the physical address the module responds to. Even though the number of bytes in the returned value is variable, the message uses 8 data bytes regardless (CAN bus protocol form Frameformat with 8 data bytes).\nThe bytes are:\n\n"}
{"id": "55049050", "url": "https://en.wikipedia.org/wiki?curid=55049050", "title": "Outcome Health", "text": "Outcome Health\n\nOutcome Health is a healthcare technology company founded by Rishi Shah and valued at $5.6 billion in May 2017. It is registered in Delaware as ContextMedia Health LLC.\n\nIn May 2017, a funding round with Goldman Sachs, CapitalG, Pritzker Group, and others invested $600 million in Outcome Health, giving it a $5.6 billion valuation. This is the largest single funding round in Chicago since Groupon in 2011, when it raised $950 million in its fifth funding round.\n\nAccording to a report in The Wall Street Journal unnamed former employees and advertisers accused the company of overcharging their customers for advertisements and misquoting third-party analyses and falsifying documents on the ads' performance. According to the accusations, Outcome Health reported that the ads appeared on more video screens than they had installed. Lanny Davis, a company spokesperson, responded by saying a law firm had been hired to \"review allegations about certain employees’ conduct that have been raised internally.\"\n\nAs of January 2018, Outcome Health decided to settle outstanding investor lawsuits in exchange for having Shah and Agarwal step down.\n\nIn June 2018, Matt McNally, former chief media officer at Publicis Health, was announced as the company's new CEO. \n\n\n\n\n\n\nhttps://www.wsj.com/articles/outcome-healths-investors-receive-subpoenas-from-justice-department-1510276020\n"}
{"id": "18934741", "url": "https://en.wikipedia.org/wiki?curid=18934741", "title": "Outline of communication", "text": "Outline of communication\n\nThe following outline is provided as an overview of and topical guide to communication:\n\nCommunication – purposeful activity of exchanging information and meaning across space and time using various technical or natural means, whichever is available or preferred. Communication requires a sender, a message, a medium and a recipient, although the receiver does not have to be present or aware of the sender's intent to communicate at the time of communication; thus communication can occur across vast distances in time and space.\n\n\n\n\n\nTheories of communication\n\nHistory of communication\n\n\n\n\n\n"}
{"id": "20166782", "url": "https://en.wikipedia.org/wiki?curid=20166782", "title": "PDF/E", "text": "PDF/E\n\nISO 24517-1:2008 is an ISO Standard published in 2008.\n\n\nThis standard defines a format (PDF/E) for the creation of documents used in geospatial, construction and manufacturing workflows<ref name=\"PDF/E-ISO-Standard\"> ISO 24517-1:2008 - Document management -- Engineering document format using PDF -- Part 1: Use of PDF 1.6 (PDF/E-1)</ref> and is based on the PDF Reference version 1.6 from Adobe Systems. The specification also supports interactive media, including animation and 3D.\n\nPDF/E is a subset of PDF, designed to be an open and neutral exchange format for engineering and technical documentation.<ref name=\"pdf/e-ready-guide\">Creating PDF/E-ready files</ref>\n\nThe PDF/E Standard specifies how the Portable Document Format (PDF) should be used for the creation of documents in engineering workflows.\n\nKey benefits of PDF/E include:\n\nThe Standard does not define a method for the creation or conversion from paper or electronic documents to the PDF/E format.\n\nThe Committee managing ISO 24517 (PDF/E) needs subject-matter experts to assist in the development of Part 2 of the Standard.\n\nISO 24517 (PDF/E) was created to meet the needs of organizations who need to reliably create, exchange and review engineering documentation, however, the first part of the standard does not address 3D, video or other dynamic content, nor does it address integrated source data.\n\n\n"}
{"id": "41598", "url": "https://en.wikipedia.org/wiki?curid=41598", "title": "Public land mobile network", "text": "Public land mobile network\n\nA PLMN is identified by the Mobile Country Code (MCC) and the Mobile Network Code (MNC). Each operator providing mobile services has its own PLMN. PLMNs interconnect with other PLMNs and Public switched telephone networks (PSTN) for telephone communications or with internet service providers for data and internet access of which links are defined as interconnect links between providers. These links mostly incorporate SDH digital transmission networks via fiber optic on land and digital microwave links.\n\nAccess to PLMN services is achieved by means of an \"air interface\" involving radio communications between mobile phones or other wireless enabled user equipment and land-based radio transmitters or radio base stations or even fiber optic distributed SDH network\n\nThe PSTN is the world's collection of interconnected voice-oriented public telephone networks, in much the same way that the Internet is the concatenation of the world's public IP-based packet-switched networks. It is both commercially- and government-owned. This aggregation of circuit-switching telephone networks has evolved greatly from the days of Alexander Graham Bell, and in the late 20th century became almost entirely digital in nature — except for the final link from the central (local) telephone office to the user (the local loop). It also extends into mobile as well as fixed telephones.\n\nThe PSTN also furnishes much of the Internet's long-distance infrastructure and, for the majority of users, the access network as well. Because Internet Service Providers (ISPs) pay the long-distance carriers for access to their infrastructure, and share the circuits among many users through packet switching, the end Internet user avoids having to pay usage tolls to anyone other than their ISP.\n\nThe PSTN is largely governed by technical standards created by the ITU-T, and uses E.163/E.164 addresses (usually called telephone numbers) for addressing. A number of large private telephone networks are not connected to the PSTN, and are used for military purposes (such as the Defense Switched Network). There are also private networks run by large companies that are linked to the PSTN, but only through controlled gateways such as private branch exchanges..**\n\nA GSM PLMN may be described by a limited set of access interfaces and a limited set of GSM PLMN connection types to support the telecommunication services described in the GSM 02-series of specifications.\n\nPLMN is a network that is established and operated by an administration or by a recognized operating agency (ROA) for the specific purpose of providing land mobile telecommunications services to the public. A PLMN may be considered as an extension of a fixed network, e.g., the Public Switched Telephone Network (PSTN) or as an integral part of the PSTN. This is just one view-point on PLMN. PLMN mostly refers to the whole system of networking hardware and software that enables wireless communication, irrespective of the service area or service provider (cf. Internet backbone). Sometimes a separate PLMN is defined for each country or for each service provider. This systematic ambiguity (of terminological scope) also affects the \"PSTN\" term. Sometimes it refers to the whole circuit-switched system, while other times it is specific to each country.\n\nPLMN is not a term specific to GSM. In fact, GSM can be treated as an example of a PLMN system. These days (as of January, 2006) many discussions are going on to form the structure of UMTS PLMN for the third-generation systems. Access to PLMN services is achieved by means of an air interface involving radio communications between mobile phones or other wireless-enabled user equipment and land-based radio transmitters or radio base stations. PLMNs interconnect with other PLMNs and PSTNs for telephone communications or with Internet service providers for data and internet access.\n\nA public land mobile network may be defined as a number of mobile services switching center areas within a common numbering plan and a common routing plan. With respect to their functions, the PLMNs may be regarded as independent communications entities, even though different PLMNs may be interconnected through the PSTN/ISDN for the forwarding of calls or network information. The MSCs of a PLMN can be interconnected similarly to allow interaction. A PLMN may have several interfaces with the fixed network (e.g., one for each MSC). Inter-working between two PLMNs may be performed via an international switching center. The PLMN is connected via an NCP to the PSTN/ISDN. If there are two mobile service suppliers in the same country, they can be connected through the same PSTN/ISDN.\n\nThe general objective of a PLMN is to facilitate wireless communication and to interlink the wireless network with the fixed wired network. The PLMN was specified by the European Telecommunications Standard Institute (ETSI) following up with their GSM specification. Even as times changed, the GSM PLMN objectives conceptually remained the same.\n\n\n\nGSM architecture is basically the PLMN architecture itself as the subject is GSM PLMN. Various interfaces between the GSM subsystems are to be considered, along with the signaling system and the various components (both hardware and software).\n\nThe GSM PLMN is divided into signaling network and mobile network. Each of these has various subsystems, which are grouped under three major systems: the Network and Switching Subsystem (NSS), the Base Station Subsystem (BSS), and the operation and support system (OSS).\n\nThe operations and maintenance center (OMC) is connected to all equipment in the switching system and to the BSC. The implementation of OMC is called the operation and support system (OSS). The OSS is the functional entity from which the network operator monitors and controls the system. The purpose of OSS is to offer the customer cost-effective support for centralized, regional, and local operational and maintenance activities that are required for a GSM network. An important function of OSS is to provide a network overview and support the maintenance activities of different operation and maintenance organizations. End.\n\nOther functional elements shown are as follows:\n\nThere are three viewpoints of interoperability between PLMN and PSTN:\n\n\nA PLMN is essential for the effective working of any wireless network, just like the need for PSTN in wireline networks. PLMN facilitates interoperation with its own subsystems in order to perform operation of the GSM (3G), LTE (4G) and 5G systems and any wired network in general.\n\n"}
{"id": "6883009", "url": "https://en.wikipedia.org/wiki?curid=6883009", "title": "RF resonant cavity thruster", "text": "RF resonant cavity thruster\n\nA radio frequency (RF) resonant cavity thruster, also known as an EmDrive, is a hypothesized type of propellant-free thruster that was proposed in 2001 by Roger Shawyer. No plausible theory of operation for such drives has been proposed; the theories that were proposed were shown to be inconsistent with known laws of physics, including conservation of momentum and conservation of energy.\n\nSeveral prototypes of this concept have been constructed and tested, including by the Advanced Propulsion Physics Laboratory at NASA. Initially, a few tests of prototype drives were reported to produce a small apparent thrust, but subsequent testing has failed to reliably reproduce these results.\n\nDue to the lack of both a physically plausible theory of operation and of reliably reproducible evidence, many theoretical physicists and commentators consider the device unworkable, explaining the observed thrust as measurement errors. Various media platforms have referred to the engine as the Impossible Drive or the Impossible Space Drive.\n\nElectromagnetic propulsion designs which operate on the principle of reaction mass have been around since the start of the 20th century. In the 1960s, extensive research was conducted on two designs which emit high-velocity ionized gases in similar ways: ion thrusters that convert propellant to ions and accelerate and eject them via electric potentials, and plasma thrusters that convert propellant to plasma ions and accelerate and eject them via plasma currents. In the latter, plasma can be generated from an intense source of microwave or other radio-frequency (RF) energy, and in combination with a resonant cavity, can be tuned to resonate at a precise frequency.\n\nA low-propellant space drive has long been a goal for space exploration, since the propellant is dead weight that must be lifted and accelerated with the ship all the way from launch until the moment it is used (see Tsiolkovsky rocket equation). Gravity assists, solar sails, and beam-powered propulsion from a spacecraft-remote location such as the ground or in orbit, are useful because they allow a ship to gain speed without propellant. However, some of these methods do not work in deep space. Shining a light out of the ship provides a small force from radiation pressure, i.e., using photons as a form of propellant, but the force is far too weak, for a given amount of input power, to be useful in practice.\n\nA true zero-propellant drive is widely believed to be impossible, but if it existed, it could potentially be used for travel in many environments including deep space. Thus, such drives are a popular concept in science fiction, and their improbability contributes to enthusiasm for exploring such designs.\n\nConventional rocket engines expel propellant, such as when ships move masses of water, aircraft move masses of air, or rockets expel exhaust. A drive which does not expel propellant in order to produce a reaction force, providing thrust while being a closed system with no external interaction, would be a reactionless drive. Such a drive would violate the conservation of momentum and Newton's third law, leading many physicists to believe it to be impossible, labelling the idea pseudoscience. On the other hand, a drive that interacts with an external field would be part of an open system, propellantless but not reactionless, like a sail catching and redirecting existing winds to move a ship.\n\nThe first proposal for an RF resonant cavity thruster came from British engineer Roger Shawyer in 2001. He invented a design with a conical cavity, calling it the EmDrive. Guido Fetta later built the Cannae Drive based on Shawyer's concept a resonant thruster with a pillbox-shaped cavity.\nSince 2008, a few physicists have tested their own models, trying to confirm results claimed by Shawyer and Fetta. Juan Yang at Xi'an's Northwestern Polytechnical University (NWPU) initially reported thrust from a model they built, but retracted her claims in 2016 after a measurement error was identified and an improved setup measured no significant thrust. In 2016, Harold White's group at NASA's Advanced Propulsion Physics Laboratory reported a test of their own model had observed 40–100 μN of thrust from inputs of 40–80 W, in the Journal of Propulsion and Power. In December 2016, Yue Chen, part of the communication satellite division of the China Academy of Space Technology (CAST), said his team had tested several prototypes using an \"experimental verification platform\", observed thrust, and was carrying out in-orbit verification. In September 2017, Chen talked about this CAST project again in an interview on CCTV.\n\nThe plausibility of thrusters that emit no propellant, such as the EmDrive, is controversial, primarily because their operation would violate the conservation of momentum.\n\nMedia coverage of experiments using these designs has been controversial and polarized. The EmDrive first drew attention, both credulous and dismissive, when \"New Scientist\" wrote about it as an \"impossible\" drive in 2006. Media outlets were later criticised for misleading claims that a resonant cavity thruster had been \"validated by NASA\" following White's first tentative test reports in 2014. Scientists have continued to note the lack of unbiased coverage, from both polarized sides.\n\nIn 2006, responding to the \"New Scientist\" piece, mathematical physicist John C. Baez at the University of California, Riverside, and Australian science-fiction writer Greg Egan, said the positive results reported by Shawyer were likely misinterpretations of experimental errors.\n\nIn 2014, White's conference paper suggested that resonant cavity thrusters could work by transferring momentum to the \"quantum vacuum virtual plasma.\" Baez and Carroll criticized this explanation, because in the standard description of vacuum fluctuations, virtual particles do not behave as a plasma; Carroll also noted that the quantum vacuum has no \"rest frame\", providing nothing to push against, so it can't be used for propulsion. In the same way, physicists James F. Woodward and Heidi Fearn published two papers showing that the amount of electron-positron virtual pairs of the quantum vacuum, used by White as a virtual plasma propellant, cannot account for thrusts in any isolated, closed electromagnetic system such as a quantum vacuum thruster.\n\nPhysicists Eric W. Davis at the Institute for Advanced Studies in Austin and Sean M. Carroll at the California Institute of Technology said in 2015 that the thrust reported in papers by both Tajmar and White were indicative of thermal effect errors.\n\nIn May 2018, researchers from the Institute of Aerospace Engineering at Technische Universität Dresden, Germany, concluded that the apparent thrust is clearly an artifact caused by Earth's magnetic field interacting with power cables in the chamber, a result that other experts agree with.\n\nIn 2001, Shawyer founded \"Satellite Propulsion Research Ltd\", in order to work on the EmDrive, a drive that he said used a resonant cavity to produce thrust without propellant. The company was backed by SMART award grants from the UK Department of Trade and Industry. In December 2002, he described a working prototype with an alleged total thrust of about 0.02 newtons powered by an 850 W cavity magnetron. The device could operate for only a few dozen seconds before the magnetron failed, due to overheating.\n\nIn October 2006, Shawyer conducted tests on a new water-cooled prototype and said that it had increased thrust. He planned to have the device ready to use in space by May 2009 and was considering making the resonant cavity a superconductor.\n\n\"New Scientist\" magazine featured the EmDrive on the cover of 8 September 2006 issue. The article portrayed the device as plausible and emphasized the arguments of those who held that point of view. Science fiction author Greg Egan distributed a public letter stating that \"a sensationalist bent and a lack of basic knowledge by its writers\" made the magazine's coverage unreliable, sufficient \"to constitute a real threat to the public understanding of science\". Especially, Egan said he was \"gobsmacked by the level of scientific illiteracy\" in the magazine's coverage, alleging that it used \"meaningless double-talk\" to obfuscate the problem of conservation of momentum. The letter was endorsed by mathematical physicist John C. Baez and posted on his blog. \"New Scientist\" editor Jeremy Webb responded to critics: \"New Scientist\" also published a letter from the former technical director of EADS Astrium: A letter from physicist Paul Friedlander: \n\nIn 2007, the UK Department of Trade and Industry granted SPR an export licence to Boeing in the US. In December 2008, Shawyer was invited to The Pentagon to present on the EmDrive, and in 2009 Boeing confirmed they wanted to license the technology. The UK Ministry of Defence agreed to a technology transfer, and SPR designed, built and tested a thruster for use on a test satellite. According to Shawyer, the 10-month contract was completed by July 2010 and the thruster, giving 18 grams of thrust, was transferred to Boeing. Boeing did not, however, license the technology and communication stopped. Questioned on the matter in 2012, a Boeing representative confirmed that\nBoeing Phantom Works used to explore exotic forms of space propulsion, including Shawyer's drive, but such work has since ceased. They confirmed that \"Phantom Works is not working with Mr. Shawyer,\" adding that the company is no longer pursuing those explorations.\n\nIn 2013 and 2014, Shawyer presented ideas for 'second-generation' EmDrive designs and applications, at the annual International Astronautical Congress. A paper based on his 2014 presentation was published in \"Acta Astronautica\" in 2015. It describes a model for a superconducting resonant cavity and three models for thrusters with multiple cavities, with hypothetical applications for launching space probes.\n\nIn October 2016, a UK patent application describing a new superconducting EmDrive was published, followed by a first international version. Shortly thereafter Shawyer unveiled the creation of \"Universal Propulsion Ltd.\", a new company aimed to develop and commercialise such thrusters, as a joint venture with \"Gilo Industries Group\", a small UK aerospace company designing and selling paramotors and the Parajet Skycar.\n\nThe Cannae Drive (formerly Q-drive), another engine designed to generate propulsion from a resonant cavity without propellant, is another implementation of this idea. Its cavity is also asymmetric, but relatively flat rather than a truncated cone. It was designed by Fetta in 2006 and has been promoted within the US through his company, Cannae LLC, since 2011. In 2016, Fetta announced plans to eventually launch a CubeSat satellite containing a version of the Cannae Drive, which they would run for 6 months to observe how it functions in space.\n\nIn China, researchers working under Yang at NWPU developed their own prototype resonant cavity thruster in 2008, publishing a report in their university's journal on the theory behind such devices. In 2012 they measured thrust from their prototype, however, in 2014 they found this had been an experimental error. A second, improved prototype did not produce any measured thrust.\n\nAt the China Academy of Space Technology, Yue Chen filed several patent applications in 2016 describing various RF resonant cavity thruster designs. These included a method for stacking several short resonant cavities to improve thrust, and a design with a cavity that was a semicylinder instead of a frustum. That December, Chen announced that CAST was conducting tests on a resonant cavity thruster in orbit, without specifying what design was used. In an interview on CCTV in September 2017, Chen Yue showed some testing of a flat cylindrical device corresponding to the patent describing stacked short cavities with internal diaphragms.\n\nThe proposed theory for how the EmDrive works violates the conservation of momentum, which states any interaction cannot have a net force; a consequence of the conservation of momentum is Newton's third law, where for every action there is an equal and opposite reaction. The conservation of momentum is a symmetry of nature.\n\nIn instances where matter appears to violate conservation laws, the apparent non-conservation is in reality an interaction with the vacuum so that overall symmetry in the system is restored. An often cited example of apparent nonconservation of momentum is the Casimir effect; in the standard case where two parallel plates are attracted to each other. However the plates move in opposite directions, so no net momentum is extracted from the vacuum and, moreover, the energy must be put into the system to take the plates apart again.\n\nAssuming homogeneous electric and magnetic fields, it is impossible for the EmDrive, or any other device, to extract a net momentum transfer from either a classical or quantum vacuum.\nExtraction of a net momentum \"from nothing\"\nhas been postulated in an inhomogeneous vacuum, but this remains highly controversial as it will violate Lorentz invariance.\n\nBoth Harold White's\nand Mike McCulloch's theories of how the EmDrive could work rely on these asymmetric or dynamical Casimir effects. However, if these vacuum forces are present, they are expected to be exceptionally tiny based on our current understanding, too small to explain the level of observed thrust.\nIn the event that observed thrust is not due to experimental error, a positive result could indicate new physics.\n\nCritics liken the EmDrive to trying to move a car by getting inside and pushing on the windshield. This violation of the fundamental principles of physics has drawn criticism from the scientific community, leading to various attempts to explain the apparent or observed thrust. However, to date, there is no acceptance or consensus on how or why these cavities produce thrust if they produce thrust at all.\n\nAttempts to explain the thrust (assuming that there is thrust) generally fall into four categories:\n\nThe simplest and most likely explanation is that any thrust detected is due to experimental error or noise. In all of the experiments set up, a very large amount of energy goes into generating a tiny amount of thrust. When attempting to measure a small signal superimposed on a large signal, the noise from the large signal can obscure the small signal and give incorrect results. The strongest early result, from Yang's group in China, was later reported to be caused by an experimental error.\n\nThe largest error source is believed to come from the thermal expansion of the thruster's heat sink; as it expands this would lead to a change in the centre of gravity causing the resonant cavity to move. White's team attempted to model the thermal effect on the overall displacement by using a superposition of the displacements caused by \"thermal effects\" and \"impulsive thrust\" with White saying \"That was the thing we worked the hardest to understand and put in a box\". Despite these efforts, White's team were unable to fully account for the thermal expansion. In an interview with \"Aerospace America\", White comments that \"although maybe we put a little bit of a pencil mark through [thermal errors]... they are certainly not black-Sharpie-crossed-out.\"\n\nTheir method of accounting for thermal effects has been criticized by Millis and Davies, who highlight that there is a lack of both mathematical and empirical detail to justify the assumptions made about those effects. For example, they do not provide data on temperature measurement over time compared to device displacement. The paper includes a graphical chart, but it is based on \"a priori\" assumptions about what the shapes of the \"impulsive thrust\" and \"thermal effects\" should be, and how those signals will superimpose. The model further assumes all noise to be thermal and does not include other effects such as interaction with the chamber wall, power lead forces, and tilting. Because the Eagleworks paper has no explicit model for thrust to compare with the observations, it is ultimately subjective, and its data can be interpreted in more than one way. The Eagleworks test, therefore, does not conclusively show a thrust effect, but cannot rule it out either.\n\nWhite suggested future experiments could run on a Cavendish balance. In such a setup, the thruster could rotate out to much larger angular displacements, letting a thrust (if present) dominate any possible thermal effects. Testing a device in space would also eliminate the center-of-gravity issue.\n\nAnother source of error could have arisen from electromagnetic interaction with the walls of the vacuum chamber. White argued that any wall interaction could only be the result of a well-formed resonance coupling between the device and wall and that the high frequency used imply the chances of this would be highly dependent on the device's geometry. As components get warmer due to thermal expansion, the device's geometry changes, shifting the resonance of the cavity. In order to counter this effect and keep the system in optimal resonance conditions, White used a phase-locked loop system (PLL). Their analysis assumed that using a PLL ruled out significant electromagnetic interaction with the wall.\n\nAnother potential source of error was a Lorentz force arising from power leads. Many previous experiments used cups with Galinstan metal alloy, which is liquid at room temperature, to supply electrical power to the device in lieu of solid wires. Martin Tajmar and his graduate student Fiedler characterized and attempted to quantify possible sources of error in their experiment at Dresden University of Technology. They ran multiple tests on their experimental setup, including measurements of the force along different axes with respect to the power supply current. While eliminating or accounting for many other sources of error in previous experiments, such as replacing a magnetic damping mechanism with an oil damper, less efficient but significantly less interacting with electromagnetic field, the study remained inconclusive as to the effects of electromagnetic interaction with the apparatus' power feed, at the same time noting it as possibly the most significant source of noise. White's power setup may have been different, but their paper does not state if the connections are all coaxially aligned with the stand's rotation axis, which would be required to minimize errors from Lorentz forces, and it gives no data from equivalent tests with power into a dummy load so these influences can be compared with those seen in the Tajmar-Fiedler run.\n\nWhite's 2016 paper went through about a year of peer review involving five referees. Peer review does not mean the results or observations are true, only that the referees looked at the experiment, results and interpretation and found it to be sound and sensible. Brice Cassenti, a professor at the University of Connecticut and an expert in advanced propulsion, spoke to one of the referees, and reported the referee did not believe the results point to any new physics, but that the results were puzzling enough to publish. Cassenti believes there is a mundane explanation for the results, but the probability of the results being valid is slim but not zero.\n\nWhite's paper was published in the \"Journal of Propulsion and Power\". Marc Millis and Eric Davies who ran NASA's previous advanced propulsion project, the Breakthrough Propulsion Physics Program have commented that while White used techniques that would be acceptable for checking the electric propulsion of Hall thrusters, the tests were not sufficient to demonstrate that any new physics effect exists.\n\nIn 2004, Shawyer reported seven independent positive reviews from experts at BAE Systems, EADS Astrium, Siemens and the IEE, but these were disputed. In a letter to \"New Scientist\", the then-technical director of EADS Astrium (Shawyer's former employer) denied this: \n\nIn 2011, Fetta tested a superconducting version of the Cannae drive. The RF resonant cavity was suspended inside a liquid helium-filled dewar. The weight of the cavity was monitored by load cells. Fetta theorized that when the device was activated and produced upward thrust, the load cells would detect the thrust as a change in weight. When the drive was energized by sending 10.5 watt power pulses of RF power into the resonant cavity, there was, as predicted, a reduction in compressive force on the load cells consistent with thrust of 8–10 mN.\n\nNone of these results have been published in the scientific literature, or replicated by independent researchers. They have been posted on their inventors' websites.\n\nIn 2015, Shawyer published an article in \"Acta Astronautica\", summarising existing tests on the EmDrive. Of seven tests, four produced a measured force in the intended direction and three produced thrust in the opposite direction. Furthermore, in one test, thrust could be produced in either direction by varying the spring constants in the measuring apparatus.\n\nIn 2008, a team of Chinese researchers led by Juan Yang (杨涓), professor of propulsion theory and engineering of aeronautics and astronautics at Northwestern Polytechnical University (NWPU) in Xi'an, China, said that they had developed a valid electro-magnetic theory behind a microwave resonant cavity thruster. A demonstration version of the drive was built and tested with different cavity shapes and at higher power levels in 2010. Using an aerospace engine test stand usually used to precisely test spacecraft engines like ion drives, they reported a maximum thrust of 720 mN at 2,500 W of input power. Yang noted that her results were tentative, and said she \"[was] not able to discuss her work until more results are published\". This positive result was over 100x more thrust per input power than any other experiment, and inspired other groups to try to replicate their work.\n\nIn a 2014 follow-up experiment (published in 2016), Yang could not reproduce the 2010 observation and suggested it was due to experimental error. In that experiment they refined their experimental setup, using a three-wire torsion pendulum to measure thrust, and tested two different power setups. In one trial, the power system was outside the cavity, and they observed a \"thrust\" of 8–10 mN. In a second trial, the power system was within the cavity, and they measured no such thrust. Instead they observed an insignificant thrust below their noise threshold of 3 mN, fluctuating between ±0.7 mN with a measurement uncertainty of 80%, with 230 W of input power. They concluded that they were unable to measure significant thrust; that \"thrust\" measured when using external power sources (as in their 2010 experiment) could be noise; and that it was important to use self-contained power systems for these experiments, and more sensitive pendulums with lower torsional stiffness.\n\nSince 2011, White has had a team at NASA known as the Advanced Propulsion Physics Laboratory, or Eagleworks Laboratories, which is devoted to studying exotic propulsion concepts. The group has investigated ideas for a wide range of untested and fringe proposals, including Alcubierre drives, drives that interact with the quantum vacuum, and RF resonant cavity thrusters.\n\nIn 2014, the group began testing resonant cavity thrusters of their own design and sharing some of their results. In November 2016, they published their first peer-reviewed paper on this work, in the \"Journal of Propulsion and Power\".\n\nIn July 2014, White reported tentative positive results for evaluating a tapered RF resonant cavity. Testing was performed using a low-thrust torsion pendulum able to detect force at the micronewton level within a sealed but unevacuated vacuum chamber (the RF power amplifier used an electrolytic capacitor unable to operate in a hard vacuum). The experimenters recorded directional thrust immediately upon application of power.\n\nTheir first tests of this tapered cavity were conducted at very low power (2% of Shawyer's 2002 experiment). A net mean thrust over five runs was measured at 91.2 µN at 17 W of input power. The experiment was criticized for its small data set and for not having been conducted in vacuum, to eliminate thermal air currents.\n\nThe group announced a plan to upgrade their equipment to higher power levels, to use vacuum-capable RF amplifiers with power ranges of up to 125 W, and to design a new tapered cavity that could be in the 0.1 N/kW range. The test article was to be subject to independent verification and validation at Glenn Research Center, the Jet Propulsion Laboratory and the Johns Hopkins University Applied Physics Laboratory. , this validation has not happened.\n\nIn 2015, Paul March from Eagleworks made new results public, measured with a torsional pendulum in a hard vacuum: about 50 µN with 50 W of input power at 5.0×10 torr. The new RF power amplifiers were said to be made for hard vacuum, but failed rapidly due to internal corona discharges. Without funding to replace or upgrade them, measurements were scarce for a time.\n\nThey conducted further experiments in vacuum, a set of 18 observations with 40-80W of input power. They published the results in the American Institute of Aeronautics and Astronautics's peer-reviewed \"Journal of Propulsion and Power\", under the title \"Measurement of Impulsive Thrust from a Closed Radio-Frequency Cavity in Vacuum\". This was released online in November 2016, with print publication in December. The study said that the system was \"consistently performing with a thrust-to-power ratio of 1.2±0.1mN/kW\", and enumerated many potential sources of error.\n\nThe paper suggested that pilot-wave theory (a controversial, non-mainstream deterministic interpretation of quantum mechanics) could explain how the device produces thrust. Commenters pointed out that just because a study reporting consistent thrust was published with peer-review does not necessarily mean that the drive functions as claimed. Physicist Ethan Siegal commented on the paper, saying that the drive most likely does not violate conservation of momentum as this would \"make physics fall apart\" but rather that there is something else going on. He said that \"Whether it's new physics [or] the effect's cause simply hasn't been determined yet, more and better experiments will be the ultimate arbiter\". Physicist Chris Lee was very critical of the work, saying that the paper had a small data set and a number of missing details he described as 'gaping holes'. Electrical Engineer George Hathaway analyzed and criticized the scientific method described in the paper.\n\nWhite's 2014 tests also evaluated two Cannae drive prototypes. One had radial slots engraved along the bottom rim of the resonant cavity interior, as required by Fetta's hypothesis to produce thrust; another \"null\" test article lacked those radial slots. Both drives were equipped with an internal dielectric. A third test article, the experimental control, had an RF load but no resonant cavity interior. These tests took place at atmospheric pressure.\n\nAbout the same net thrust was reported for both the device with radial slots and the device without slots. Thrust was not reported for the experimental control. Some considered the positive result for the non-slotted device a possible flaw in the experiment, as the null test device had been expected to produce less or no thrust based upon Fetta's hypothesis of how thrust was produced by the device. In the complete paper, however, White concluded that the test results proved that \"thrust production was not dependent upon slotting\".\n\nIn July 2015, an aerospace research group at the Dresden University of Technology (TUD) under Martin Tajmar reported results for an evaluation of an RF resonant tapered cavity similar to the EmDrive. Testing was performed first on a knife-edge beam balance able to detect force at the micronewton level, atop an antivibration granite table at ambient air pressure; then on a torsion pendulum with a force resolution of 0.1 mN, inside a vacuum chamber at ambient air pressure and in a hard vacuum at .\n\nThey used a conventional ISM band 2.45 GHz 700 W oven magnetron, and a small cavity with a low Q factor (20 in vacuum tests). They observed small positive thrusts in the positive direction and negative thrusts in the negative direction, of about 20 µN in a hard vacuum. However, when they rotated the cavity upwards as a \"null\" configuration, they observed an anomalous thrust of hundreds of micronewtons, significantly larger than the expected result of zero thrust. This indicated a strong source of noise which they could not identify. This led them to conclude that they could not confirm or refute claims about such a thruster. At the time they considered future experiments with better magnetic shielding, other vacuum tests and improved cavities with higher \"Q\" factors.\n\nIn 2018, the TU Dresden research team presented a conference paper summarizing the results from the most recent experiments on their upgraded test rig, which seemed to show that their measured thrust was a result of experimental error from insufficiently shielded components interacting with the earth's magnetic field. In their experiments, they measured thrust values consistent with previous experiments, and the thrust reversed appropriately when the thruster was rotated by 180°. However, the team also measured thrust perpendicular to the expected direction when the thruster was rotated by 90°, and did not measure a reduction in thrust when an attenuator was used to reduce the power that actually entered the resonant cavity by a factor of 10,000, which they said \"clearly indicates that the \"thrust\" is not coming from the EMDrive but from some electromagnetic interaction.\" They concluded that \"magnetic interaction from not sufficiently shielded cables or thrusters are a major factor that needs to be taken into account for proper μN thrust measurements for these type of devices,\" and they plan on conducting future tests at higher power and at different frequencies, and with improved shielding and cavity geometry.\n\nIn August 2016, Cannae announced plans to launch its thruster on a 6U cubesat which they would run for 6 months to observe how it functions in space. Cannae has formed a company called Theseus for the venture and partnered with LAI International and SpaceQuest Ltd. to launch the satellite. No launch date has yet been announced.\n\nIn November 2016, the \"International Business Times\" published an unconfirmed report that the U.S. government was testing a version of the EmDrive on the Boeing X-37B and that the Chinese government has made plans to incorporate the EmDrive on its orbital space laboratory Tiangong-2. The US Air Force has only confirmed that the X-37B mission in question did an electric propulsion system test using a Hall-effect thruster, a type of ion thruster that uses a gaseous propellant.\n\nIn December 2016, Yue Chen told a reporter at China's \"Science and Technology Daily\" that his team was testing an EmDrive in orbit, and that they had been funding research in the area for five years. Chen noted that their prototype's thrust was at the \"micronewton to millinewton level\", which would have to be scaled up to at least 100–1000 millinewtons for a chance of conclusive experimental results. Despite this, he said his goal was to complete validation of the drive, and then to make such technology available in the field of satellite engineering \"as quickly as possible\".\n\nThe drive is featured in \"Salvation\", an American Sci-Fi suspense drama. It is described as a theoretically impossible piece of technology, but the protagonists manage to make it work using an exotic space-originated crystal extracted from an asteroid.\n\n"}
{"id": "23007105", "url": "https://en.wikipedia.org/wiki?curid=23007105", "title": "RTFB", "text": "RTFB\n\nThe most usual meaning is in reference to instruction manuals, and means \"Read the F'ing Book\" more politely rendered as \"Read the Fine Book.\"\n\n\"RTFB\" is a parodical extension to the Internet slang term \"RTFM\" and its extension \"RTFS\".\n\n\"RTFB\" is short for \"read the fucking binary\", in a similar way as \"RTFM\" means \"read the fucking manual\" and \"RTFS\" means \"read the fucking source\". While both \"RTFM\" and \"RTFS\" have legitimate uses, \"RTFB\" is usually intended only as a parody of them. While any user, no matter how tech-savvy, can be expected to read the manual, and experienced computer programmers can deduce the working logic of a computer program directly from reading its human-readable source code, \"RTFB\" expects users to directly read the machine language code that the microprocessor executes natively. Today's microprocessors are usually so complex that, except for trivial textbook programs, this is a difficult task even for experienced programmers, and because of anti-reverse-engineering laws, may even be illegal.\n\nRTFB has also been used meaning \"Read the fucking Bible\" in relation to WWJD (What Would Jesus Do) jokes.\n\nA more recent use, applied to members of the legislative branch, means \"read the fucking bill\". It is a reaction to the practice of passing massive legislation where the final bill is available for only a few hours before it is voted upon.\n\n"}
{"id": "9060625", "url": "https://en.wikipedia.org/wiki?curid=9060625", "title": "SXML", "text": "SXML\n\nSXML is an alternative syntax for writing XML data (more precisely, XML Infosets) as S-expressions, to facilitate working with XML data in Lisp and Scheme. An associated suite of tools implements XPath, SAX and XSLT for SXML in Scheme and are available in the GNU Guile implementation of that language.\n\nTextual correspondence between SXML and XML for a sample XML snippet is shown below:\n\nCompared to other alternative representations for XML and its associated languages, SXML has the benefit of being directly parsable by existing Scheme implementations. The associated tools and documentation were praised in many respects by David Mertz in his IBM developerWorks column, though he also criticized the preliminary nature of its documentation and system.\n\nTake the following simple XHTML page:\nAfter translating it to SXML, the same page now looks like this:\n\nEach element's tag pair is replaced by a set of parentheses. The tag's name is not repeated at the end, it is simply the first symbol in the list. The element's contents follow, which are either elements themselves or strings. There is no special syntax required for XML attributes. In SXML they are simply represented as just another node, which has the special name of <nowiki>@</nowiki>. This can't cause a name clash with an actual <nowiki>\"@\"</nowiki> tag, because <nowiki>@</nowiki> is not allowed as a tag name in XML. This is a common pattern in SXML: anytime a tag is used to indicate a special status or something that is not possible in XML, a name is used that does not constitute a valid XML identifier.\n\nWe can also see that there's no need to \"escape\" otherwise meaningful characters like & and > as &amp; and &gt; entities. All string content is automatically escaped because it is considered to be pure content, and has no tags or entities in it. This also means it is much easier to insert autogenerated content and that there is no danger that we might forget to escape user input when we display it to other users (which could lead to all kinds of cross-site scripting attacks or other development annoyances).\n\n"}
{"id": "28904", "url": "https://en.wikipedia.org/wiki?curid=28904", "title": "Short Message Peer-to-Peer", "text": "Short Message Peer-to-Peer\n\nShort Message Peer-to-Peer (SMPP) in the telecommunications industry is an open, industry standard protocol designed to provide a flexible data communication interface for the transfer of short message data between External Short Messaging Entities (ESMEs), Routing Entities (REs) and Message Centres.\n\nSMPP is often used to allow third parties (e.g. value-added service providers like news organizations) to submit messages, often in bulk, but it may be used for SMS peering as well. SMPP is able to carry short messages including EMS, voicemail notifications, Cell Broadcasts, WAP messages including WAP Push messages (used to deliver MMS notifications), USSD messages and others. Because of its versatility and support for non-GSM SMS protocols, like UMTS, IS-95 (CDMA), CDMA2000, ANSI-136 (TDMA) and iDEN, SMPP is the most commonly used protocol for short message exchange outside SS7 networks.\n\nSMPP (Short Message Peer-to-Peer) was originally designed by Aldiscon, a small Irish company that was later acquired by Logica (since 2016, after a number of changes Mavenir). The protocol was originally created by a developer, Ian J Chambers, to test functionality of the SMSC without using SS7 test equipment to submit messages. In 1999, Logica formally handed over SMPP to the SMPP Developers Forum, later renamed as The SMS Forum and now disbanded. The SMPP protocol specifications are still available through the website which also carries a notice stating that it will be taken down at the end of 2007. As part of the original handover terms, SMPP ownership has now returned to Mavenir due to the disbanding of the SMS forum.\n\nTo date SMPP development is suspended and SMS forum is disbanded. From SMS forum website:\n\nJuly 31, 2007 - The SMS Forum, a non-profit organization with a mission to develop, foster and promote SMS (short message service) to the benefit of the global wireless industry will disband by July 27, 2007\n\nA press release, attached to the news, also warns that site will be suspended soon. In spite of this the site is still mostly functioning and specifications can still be downloaded (as of 31 January 2012).\n\nThe site has ceased operation according to Cormac Long, former technical moderator and webmaster for the SMS Forum. Please contact Mavenir for the SMPP specification. The files may also be available from other websites, including SMSCarrier - Documentation for SMPP APIs.\n\nContrary to its name, the SMPP uses the client-server model of operation. The Short Message Service Center (SMSC) usually acts as a server, awaiting connections from ESMEs. When SMPP is used for SMS peering, the sending MC usually acts as a client.\n\nThe protocol is based on pairs of request/response PDUs (protocol data units, or packets) exchanged over OSI layer 4 (TCP session or X.25 SVC3) connections. The well-known port assigned by the IANA for SMPP when operating over TCP is 2775, but multiple arbitrary port numbers are often used in messaging environments.\n\nBefore exchanging any messages, a bind command must be sent and acknowledged. The bind command determines in which direction will be possible to send messages; bind_transmitter only allows client to submit messages to the server, bind_receiver means that the client will only receive the messages, and bind_transceiver (introduced in SMPP 3.4) allows message transfer in both directions. In the bind command the ESME identifies itself using system_id, system_type and password; the address_range field designed to contain ESME address is usually left empty. The bind command contains interface_version parameter to specify which version of SMPP protocol will be used.\n\nMessage exchange may be synchronous, where each peer waits for a response for each PDU being sent, or asynchronous, where multiple requests can be issued without waiting and acknowledged in a skew order by the other peer; the number of unacknowledged requests is called a \"window\"; for the best performance both communicating sides must be configured with the same window size.\n\nThe SMPP standard has evolved during the time. The most commonly used versions of SMPP are:\n\n\nThe applicable version is passed in the interface_version parameter of a bind command.\n\nThe SMPP PDUs are binary encoded for efficiency. They start with a header which may be followed by a body:\n\nEach PDU starts with a header. The header consists of 4 fields, each of length of 4 octets:\n\n\nAll numeric fields in SMPP use the big endian order, which means that the first octet is the Most Significant Byte (MSB).\n\nThis is an example of the binary encoding of a 60-octet \"submit_sm\" PDU. The data is shown in Hex octet values as a single dump and followed by a header and body break-down of that PDU.\n\nThis is best compared with the definition of the submit_sm PDU from the SMPP specification in order to understand how the encoding matches the field by field definition.\n\nThe value break-downs are shown with decimal in parentheses and Hex values after that. Where you see one or several hex octets appended, this is because the given field size uses 1 or more octets encoding.\n\nAgain, reading the definition of the submit_sm PDU from the spec will make all this clearer.\n\n 'command_length', (60) ... 00 00 00 3C\n\n 'service_type', () ... 00\n\nNote that the text in the short_message field must match the data_coding. When the data_coding is 8 (UCS2), the text must be in UCS-2BE (or its extension, UTF-16BE). When the data_coding indicates a 7-bit encoding, each septet is stored in a separate octet in the short_message field (with the most significant bit set to 0). SMPP 3.3 data_coding exactly copied TP-DCS values of GSM 03.38, which make it suitable only for GSM 7-bit default alphabet, UCS2 or binary messages; SMPP 3.4 introduced a new list of data_coding values:\n\nThe meaning of the data_coding=4 or 8 is the same as in SMPP 3.3. Other values in the range 1-15 are reserved in SMPP 3.3. Unfortunately, unlike SMPP 3.3, where data_coding=0 was unambiguously GSM 7-bit default alphabet, for SMPP 3.4 and higher the GSM 7-bit default alphabet is missing in this list, and data_coding=0 may differ for various Short message service centers—it may be ISO-8859-1, ASCII, GSM 7-bit default alphabet, UTF-8 or even configurable per ESME. When using data_coding=0, both sides (ESME and SMSC) must be sure they consider it the same encoding. Otherwise it is better not to use data_coding=0. It may be tricky to use GSM 7-bit default alphabet, some Short message service centers requires data_coding=0, others e.g. data_coding=241.\n\nDespite its wide acceptance, the SMPP has a number of problematic features:\n\n\nAlthough data_coding values in SMPP 3.3 are based on the GSM 03.38, since SMPP 3.4 there is no data_coding value for GSM 7-bit alphabet (GSM 03.38). However, it is common for DCS=0 to indicate the GSM 7-bit alphabet, particularly for SMPP connections to SMSCs on GSM mobile networks.\n\nAccording to SMPP 3.4 and 5.0 the data_coding=0 means ″SMSC Default Alphabet″. Which encoding it really is, depends on the type of the SMSC and its configuration.\n\nOne of the encodings in CDMA standard C.R1001 is Shift-JIS used for Japanese. SMPP 3.4 and 5.0 specifies three encodings for Japanese (JIS, ISO-2022-JP and Extended Kanji JIS), but none of them is identical with CDMA MSG_ENCODING 00101. It seems that the Pictogram encoding (data_coding=9) is used to carry the messages in Shift-JIS in SMPP.\n\nWhen a submit_sm fails, the SMSC returns a submit_sm_resp with non-zero value of command_status and ″empty″ message_id.\n\n\nFor the best compatibility, any SMPP implementation should accept both variants of negative submit_sm_resp regardless of the version of SMPP standard used for the communication.\n\nComment from Cormac Long \nThe original intention of error scenarios was that no body would be returned in the PDU response. This was the standard behavior exhibited on all Aldiscon/Logica SMSC and also in most of the other vendors. When SMPP 3.4 was being taken on by the WAP forum, several clarifications were requested on whether a body should be included with NACKed response and measures were taken to clarify this in several places in the specification including the submit_sm section and also in the bind_transceiver section. What should have been done was to add the clarification that we eventually added in V5.0.. that bodies are not supposed to be included in error responses. Some vendors have been very silly in their implementations including bodies on rejected bind_transmitter responses but not on bind_transceiver responses etc. The recommendation I would make to vendors.. as suggested above.. accept both variants. But its also wise to allow yourself issue NACKed submit_sm_resp and deliver_sm_resp PDUs with and without an empty body. In the case of these two PDUs, that empty body will look like a single NULL octet at the end of the stream. The reason you may need this ability to include what I call dummy bodies with NACKed requests is that the other side of the equation may be unable or unwilling to change their implementation to tolerate the missing body.\n\nThe only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose. While SMPP 3.3 states that Message ID is a C-Octet String (Hex) of up to 8 characters (plus terminating '\\0'), the SMPP 3.4 states that the id field in the Delivery Receipt Format is a C-Octet String (Decimal) of up to 10 characters. This splits SMPP implementations to 2 groups:\n\n\nSince introduction of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In order to achieve the highest possible degree of compatibility and interoperability any implementation should apply the Internet robustness principle: ″Be conservative in what you send, be liberal in what you accept″. It should use a minimal set of features which are necessary to accomplish a task. And if the goal is communication and not quibbling, each implementation should overcome minor nonconformities with standard:\n\n\nInformation applicable to one version of SMPP can often be found in another version of SMPP; e.g. the only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose.\n\nThe SMPP protocol is designed on a clear-text binary protocol which needs to be considered if using for potentially sensitive information such as one-time passwords via SMS. There are, however, implementations of SMPP over secure SSL/TLS if required.\n\n\n"}
{"id": "4148166", "url": "https://en.wikipedia.org/wiki?curid=4148166", "title": "Social positioning method", "text": "Social positioning method\n\nThe social positioning method (SPM) studies space-time behaviour by analysing the location coordinates of mobile phones and the social characteristics of the people carrying them. The SPM methods and experiments were developed in Estonia by Positium and Institute of Geography University of Tartu during 2003-2006 (\"Ahas, R. & Mark, Ü. (2005). Location based services - new challenges for planning and public administration? Futures 37, 547-561\").\n\nThe biggest advantage of mobile positioning-based methods is that mobile phones are widespread, positioning works inside buildings, and collection of movement data is done by a third party at regular intervals. Positioning data is digital; it is easy to trace many people at the same time and it is possible to analyse movements in real time. The disadvantage of mobile positioning today is relatively low preciseness, the boom in the generation of phones with a GPS will raise positioning accuracy.\n\nThe most important problems of SPM are related to data security, as well as concerns about non-authorized personal surveillance. These problems can be solved with further development of location-based services (LBS) and relevant legal and organisational regulation. Today mobile positioning can be applied only by obtaining participants’ personal acceptance.\n"}
{"id": "2052772", "url": "https://en.wikipedia.org/wiki?curid=2052772", "title": "Solenoid valve", "text": "Solenoid valve\n\nA solenoid valve is an electromechanical device in which the solenoid uses an electric current to generate a magnetic field and thereby operate a mechanism which regulates the opening of fluid flow in a valve.\n\nSolenoid valves differ in the characteristics of the electric current they use, the strength of the magnetic field they generate, the mechanism they use to regulate the fluid, and the type and characteristics of fluid they control. The mechanism varies from linear action, plunger-type actuators to pivoted-armature actuators and rocker actuators. The valve can use a two-port design to regulate a flow or use a three or more port design to switch flows between ports. Multiple solenoid valves can be placed together on a manifold.\n\nSolenoid valves are the most frequently used control elements in fluidics. Their tasks are to shut off, release, dose, distribute or mix fluids. They are found in many application areas. Solenoids offer fast and safe switching, high reliability, long service life, good medium compatibility of the materials used, low control power and compact design.\n\nThere are many valve design variations. Ordinary valves can have many ports and fluid paths. A 2-way valve, for example, has 2 ports; if the valve is open, then the two ports are connected and fluid may flow between the ports; if the valve is closed, then ports are isolated. If the valve is open when the solenoid is not energized, then the valve is termed normally open (N.O.). Similarly, if the valve is closed when the solenoid is not energized, then the valve is termed normally closed. There are also 3-way and more complicated designs. A 3-way valve has 3 ports; it connects one port to either of the two other ports (typically a supply port and an exhaust port).\n\nSolenoid valves are also characterized by how they operate. A small solenoid can generate a limited force. If that force is sufficient to open and close the valve, then a direct acting solenoid valve is possible. An approximate relationship between the required solenoid force \"F\", the fluid pressure \"P\", and the orifice area \"A\" for a direct acting solenoid valve is:\n\nWhere \"d\" is the orifice diameter. A typical solenoid force might be . An application might be a low pressure (e.g., ) gas with a small orifice diameter (e.g., for an orifice area of and approximate force of ).\nWhen high pressures and large orifices are encountered, then high forces are required. To generate those forces, an internally piloted solenoid valve design may be possible. In such a design, the line pressure is used to generate the high valve forces; a small solenoid controls how the line pressure is used. Internally piloted valves are used in dishwashers and irrigation systems where the fluid is water, the pressure might be and the orifice diameter might be .\n\nIn some solenoid valves the solenoid acts directly on the main valve. Others use a small, complete solenoid valve, known as a pilot, to actuate a larger valve. While the second type is actually a solenoid valve combined with a pneumatically actuated valve, they are sold and packaged as a single unit referred to as a solenoid valve. Piloted valves require much less power to control, but they are noticeably slower. Piloted solenoids usually need full power at all times to open and stay open, where a direct acting solenoid may only need full power for a short period of time to open it, and only low power to hold it.\n\nA direct acting solenoid valve typically operates in 5 to 10 milliseconds. The operation time of a piloted valve depends on its size; typical values are 15 to 150 milliseconds.\n\nPower consumption and supply requirements of the solenoid vary with application, being primarily determined by fluid pressure and line diameter. For example, a popular 3/4\" 150 psi sprinkler valve, intended for 24 VAC (50 - 60 Hz) residential systems, has a momentary inrush of 7.2 VA, and a holding power requirement of 4.6 VA. Comparatively, an industrial 1/2\" 10000 psi valve, intended for 12, 24, or 120 VAC systems in high pressure fluid and cryogenic applications, has an inrush of 300 VA and a holding power of 22 VA. Neither valve lists a minimum pressure required to remain closed in the un-powered state.\n\nWhile there are multiple design variants, the following is a detailed breakdown of a typical solenoid valve design.\n\nA solenoid valve has two main parts: the solenoid and the valve. The solenoid converts electrical energy into mechanical energy which, in turn, opens or closes the valve mechanically. A direct acting valve has only a small flow circuit, shown within section E of this diagram (this section is mentioned below as a pilot valve). In this example, a diaphragm piloted valve multiplies this small pilot flow, by using it to control the flow through a much larger orifice. \n\nSolenoid valves may use metal seals or rubber seals, and may also have electrical interfaces to allow for easy control. A spring may be used to hold the valve opened (normally open) or closed (normally closed) while the valve is not activated.\nThe diagram to the right shows the design of a basic valve, controlling the flow of water in this example. At the top figure is the valve in its closed state. The water under pressure enters at A. B is an elastic diaphragm and above it is a weak spring pushing it down. The diaphragm has a pinhole through its center which allows a very small amount of water to flow through it. This water fills the cavity C on the other side of the diaphragm so that pressure is equal on both sides of the diaphragm, however the compressed spring supplies a net downward force. The spring is weak and is only able to close the inlet because water pressure is equalized on both sides of the diaphragm.\n\nOnce the diaphragm closes the valve, the pressure on the outlet side of its bottom is reduced, and the greater pressure above holds it even more firmly closed. Thus, the spring is irrelevant to holding the valve closed.\n\nThe above all works because the small drain passage D was blocked by a pin which is the armature of the solenoid E and which is pushed down by a spring. If current is passed through the solenoid, the pin is withdrawn via magnetic force, and the water in chamber \"C\" drains out the passage \"D\" faster than the pinhole can refill it. The pressure in chamber C drops and the incoming pressure lifts the diaphragm, thus opening the main valve. Water now flows directly from A to F. \n\nWhen the solenoid is again deactivated and the passage D is closed again, the spring needs very little force to push the diaphragm down again and the main valve closes. In practice there is often no separate spring; the elastomer diaphragm is molded so that it functions as its own spring, preferring to be in the closed shape.\n\nFrom this explanation it can be seen that this type of valve relies on a differential of pressure between input and output as the pressure at the input must always be greater than the pressure at the output for it to work. Should the pressure at the output, for any reason, rise above that of the input then the valve would open regardless of the state of the solenoid and pilot valve.\n\nSolenoid valve designs have many variations and challenges.\n\nCommon components of a solenoid valve:\n\nThe core or plunger is the magnetic component that moves when the solenoid is energized. The core is coaxial with the solenoid. The core's movement will make or break the seals that control the movement of the fluid. When the coil is not energized, springs will hold the core in its normal position.\n\nThe plugnut is also coaxial.\n\nThe core tube contains and guides the core. It also retains the plugnut and may seal the fluid. To optimize the movement of the core, the core tube needs to be nonmagnetic. If the core tube were magnetic, then it would offer a shunt path for the field lines. In some designs, the core tube is an enclosed metal shell produced by deep drawing. Such a design simplifies the sealing problems because the fluid cannot escape from the enclosure, but the design also increases the magnetic path resistance because the magnetic path must traverse the thickness of the core tube twice: once near the plugnut and once near the core. In some other designs, the core tube is not closed but rather an open tube that slips over one end of the plugnut. To retain the plugnut, the tube might be crimped to the plugnut. An O-ring seal between the tube and the plugnut will prevent the fluid from escaping.\n\nThe solenoid coil consists of many turns of copper wire that surround the core tube and induce the movement of the core. The coil is often encapsulated in epoxy. The coil also has an iron frame that provides a low magnetic path resistance.\nThe valve body must be compatible with the fluid; common materials are brass, stainless steel, aluminum, and plastic.\n\nThe seals must be compatible with the fluid.\n\nTo simplify the sealing issues, the plugnut, core, springs, shading ring, and other components are often exposed to the fluid, so they must be compatible as well. The requirements present some special problems. The core tube needs to be non-magnetic to pass the solenoid's field through to the plugnut and the core. The plugnut and core need a material with good magnetic properties such as iron, but iron is prone to corrosion. Stainless steels can be used because they come in both magnetic and non-magnetic varieties. For example, a solenoid valve might use 304 stainless steel for the body, 305 stainless steel for the core tube, 302 stainless steel for the springs, and 430 F stainless steel (a magnetic stainless steel) for the core and plugnut.\n\nMany variations are possible on the basic, one-way, one-solenoid valve described above:\n\nSolenoid valves are used in fluid power pneumatic and hydraulic systems, to control cylinders, fluid power motors or larger industrial valves. Automatic irrigation sprinkler systems also use solenoid valves with an automatic controller. Domestic washing machines and dishwashers use solenoid valves to control water entry into the machine. They are also often used in paintball gun triggers to actuate the CO2 hammer valve. Solenoid valves are usually referred to simply as \"solenoids.\" \n\nSolenoid valves can be used for a wide array of industrial applications, including general on-off control, calibration and test stands, pilot plant control loops, process control systems, and various original equipment manufacturer applications. \n\nIn 1910, ASCO Numatics became the first company to develop and manufacture the solenoid valve.\n\n\n\n"}
{"id": "14171448", "url": "https://en.wikipedia.org/wiki?curid=14171448", "title": "Specification (technical standard)", "text": "Specification (technical standard)\n\nA specification often refers to a set of documented requirements to be satisfied by a material, design, product, or service. A specification is often a type of technical standard.\n\nThere are different types of technical or engineering specifications (specs), and the term is used differently in different technical contexts. They often refer to particular documents, and/or particular information within them. The word \"specification\" is broadly defined as \"to state explicitly or in detail\" or \"to be specific\". \n\nUsing the term \"specification\" without a clear indication of what kind is confusing and considered bad practice.\n\nA requirement specification is a documented requirement, or set of documented requirements, to be satisfied by a given material, design, product, service, etc. It is a common early part of engineering design and product development processes, in many fields.\n\nA functional specification is a kind of requirement specification, and may show functional block diagrams.\n\nA design or product specification describes the features of the \"solutions\" for the Requirement Specification, referring to either a designed solution or final produced solution. It is often used to guide fabrication/production. Sometimes the term \"specification\" is here used in connection with a data sheet (or \"spec sheet\"), which may be confusing. A data sheet describes the technical characteristics of an item or product, often published by a manufacturer to help people choose or use the products. A data sheet is not a technical specification in the sense of informing how to produce.\n\nAn \"in-service\" or \"maintained as\" specification, specifies the conditions of a system or object after years of operation, including the effects of wear and maintenance (configuration changes).\n\nSpecifications are a type of technical standard that may be developed by any of various kinds of organizations, both public and private. Example organization types include a corporation, a consortium (a small group of corporations), a trade association (an industry-wide group of corporations), a national government (including its military, regulatory agencies, and national laboratories and institutes), a professional association (society), a purpose-made standards organization such as ISO, or vendor-neutral developed generic requirements. It is common for one organization to \"refer to\" (\"reference\", \"call out\", \"cite\") the standards of another. Voluntary standards may become mandatory if adopted by a government or business contract.\n\nIn engineering, manufacturing, and business, it is vital for suppliers, purchasers, and users of materials, products, or services to understand and agree upon all requirements. \n\nA specification may refer to a standard which is often referenced by a contract or procurement document, or an otherwise agreed upon set of requirements (though still often used in the singular). In any case, it provides the necessary details about the specific requirements.\n\nStandards for specifications may be provided by government agencies, standards organizations (ASTM, ISO, CEN, DoD, etc.), trade associations, corporations, and others. The following British standards apply to specifications:\n\nA design/product specification does not necessarily prove a product to be correct or useful in every context. An item might be verified to comply with a specification or stamped with a specification number: this does not, by itself, indicate that the item is fit for other, non-validated uses. The people who use the item (engineers, trade unions, etc.) or specify the item (building codes, government, industry, etc.) have the responsibility to consider the choice of available specifications, specify the correct one, enforce compliance, and use the item correctly. Validation of suitability is necessary.\n\nSometimes a guide or a standard operating procedure is available to help write and format a good specification. A specification might include:\n\nSpecifications in North America form part of the contract documents that accompany and govern the construction of building and infrastructure projects. Specifications describe the quality and performance of building materials, using code citations and published standards, whereas the drawings or Building Information Model (BIM) illustrates quantity and location of materials. The guiding master document of names and numbers is the latest edition of MasterFormat. This is a consensus document that is jointly sponsored by two professional organizations: Construction Specifications Canada and Construction Specifications Institute based in the United States and updated every two years.\n\nWhile there is a tendency to believe that \"Specifications overrule Drawings\" in the event of discrepancies between the text document and the drawings, the actual intent must be made explicit in the contract between the Owner and the Contractor. The standard AIA (American Institute of Architects) and EJCDC (Engineering Joint Contract Documents Committee) states that the drawings and specifications are complementary, together providing the information required for a complete facility. Many public agencies, such as the Naval Facilities Command (NAVFAC) state that the specifications overrule the drawings. This is based on the idea that words are easier for a jury (or mediator) to interpret than drawings in case of a dispute.\n\nThe standard listing of construction specifications falls into 50 Divisions, or broad categories of work types and work results involved in construction. The divisions are subdivided into sections, each one addressing a specific material type (concrete) or a work product (steel door) of the construction work. A specific material may be covered in several locations, depending on the work result: stainless steel (for example) can be covered as a sheet material used in Flashing and Sheet Metal in Division 07; it can be part of a finished product, such as a handrail, covered in Division 05; or it can be a component of building hardware, covered in Division 08. The original listing of specification divisions was based on the time sequence of construction, working from exterior to interior, and this logic is still somewhat followed as new materials and systems make their way into the construction process.\n\nEach Section is subdivided into three distinct Parts: \"General\", \"Products\" and \"Execution\". The MasterFormat and Section Format system can be successfully applied to residential, commercial, civil, and industrial construction. Although many Architects find the rather voluminous commercial style of specifications too lengthy for most residential projects and therefore either produce more abbreviated specifications of their own or use ArCHspec (which was specifically created for residential projects). Master specification systems are available from multiple vendors such as Arcom, Visispec, BSD, and Spectext. These systems were created to standardize language across the United States and are usually subscription based.\n\nSpecifications can be either \"performance-based\", whereby the specifier restricts the text to stating the performance that must be achieved by the completed work, \"prescriptive\" where the specifier states the specific criteria such as fabrication standards applicable to the item, or \"proprietary\", whereby the specifier indicates specific products, vendors and even contractors that are acceptable for each workscope. In addition, specifications can be \"closed\" with a specific list of products, or \"open\" allowing for substitutions made by the Contractor. Most construction specifications are a combination of performance-based and proprietrary types, naming acceptable manufacturers and products while also specifying certain standards and design criteria that must be met.\n\nWhile North American specifications are usually restricted to broad descriptions of the work, European ones and Civil work can include actual work quantities, including such things as area of drywall to be built in square meters, like a bill of materials. This type of specification is a collaborative effort between a specwriter and a quantity surveyor. This approach is unusual in North America, where each bidder performs a quantity survey on the basis of both drawings and specifications. In many countries on the European continent, content that might be described as \"specifications\" in the United States are covered under the building code or municipal code. Civil and infrastructure work in the United States often includes a quantity breakdown of the work to be performed as well. \n\nAlthough specifications are usually issued by the architect's office, specification writing itself is undertaken by the architect and the various engineers or by specialist specification writers. Specification writing is often a distinct professional trade, with professional certifications such as \"Certified Construction Specifier\" (CCS) available through the Construction Specifications Institute and the Registered Specification Writer (RSW) through Construction Specifications Canada. Specification writers are either employees of or sub-contractors to architects, engineers, or construction management companies. Specification writers frequently meet with manufacturers of building materials who seek to have their products specified on upcoming construction projects so that contractors can include their products in the estimates leading to their proposals.\n\nIn February 2015, ArCHspec went live, from ArCH (Architects Creating Homes), a nationwide American professional society of Architects whose purpose is to improve residential architecture. ArCHspec was created specifically for use by Licensed Architects while designing SFR (Single Family Residential) architectural projects. Unlike the more commercial CSI (50+ division commercial specifications), ArCHspec utilizes the more recognizable 16 traditional Divisions, plus a Division 0 (Scope & Bid Forms) and Division 17 (low voltage). Many architects, up to this point, did not provide specifications for residential designs, which is one of the reasons ArCHspec was created: to fill a void in the industry with more compact specifications for residential use. Shorter form specifications documents suitable for residential use are also available through Arcom, and follow the 50 division format, which was adopted in both the United States and Canada starting in 2004. The 16 division format is no longer considered standard, and is not supported by either CSI or CSC, or any of the subscription master specification services, data repositories, product lead systems, and the bulk of governmental agencies.\n\nSpecifications in Egypt form part of contract documents. The Housing and Building National Research Center (HBRC) is responsible for developing construction specifications and codes. The HBRC has published more than 15 books which cover building activities like earthworks, plastering, etc.\n\nSpecifications in the UK are part of the contract documents that accompany and govern the construction of a building. They are prepared by construction professionals such as architects, architectural technologists, structural engineers, landscape architects and building services engineers. They are created from previous project specifications, in-house documents or master specifications such as the National Building Specification (NBS). The National Building Specification is owned by the Royal Institute of British Architects (RIBA) through their commercial group RIBA Enterprises (RIBAe). NBS master specifications provide content that is broad and comprehensive, and delivered using software functionality that enables specifiers to customize the content to suit the needs of the project and to keep up to date.\n\nUK project specification types fall into two main categories prescriptive and performance. Prescriptive specifications define the requirements using generic or proprietary descriptions of what is required, whereas performance specifications focus on the outcomes rather than the characteristics of the components.\n\nSpecifications are an integral part of Building Information Modeling and cover the non-geometric requirements.\n\nPharmaceutical products can usually be tested and qualified by various Pharmacopoeia. Current existing pharmaceutical standards include:\n\nIf any pharmaceutical product is not covered by the above standards, it can be evaluated by the additional source of Pharmacopoeia from other nations, from industrial specifications, or from a standardized formulary such as\n\n\nA similar approach is adopted by the food manufacturing, of which Codex Alimentarius ranks the highest standards, followed by regional and national standards.\n\nThe coverage of food and drug standards by ISO is currently less fruitful and not yet put forward as an urgent agenda due to the tight restrictions of regional or national constitution\n\nSpecifications and other standards can be externally imposed as discussed above, but also internal manufacturing and quality specifications. These exist not only for the food or pharmaceutical product but also for the processing machinery, quality processes, packaging, logistics (cold chain), etc. and are exemplified by \"ISO 14134\" and \"ISO 15609\"\n\nThe converse of explicit statement of specifications is a process for dealing with observations that are out-of-specification. The United States Food and Drug Administration has published a non-binding recommendation that addresses just this point.\n\nAt the present time, much of the information and regulations concerning food and food products remain in a form which makes it difficult to apply automated information processing, storage and transmission methods and techniques.\n\nData systems that can process, store and transfer information about food and food products need formal specifications for the representations of data about food and food products in order to operate effectively and efficiently.\n\nDevelopment of formal specifications for food and drug data with the necessary and sufficient clarity and precision for use specifically by digital computing systems have begun to emerge from government agencies and standards organizations.\n\nThe United States Food and Drug Administration has published specifications for a \"Structured Product Label\" which drug manufacturers must by mandate use to submit electronically the information on a drug label.\n\nRecently, ISO has made some progress in the area of food and drug standards and formal specifications for data about regulated substances through the publication of \"ISO 11238\"\n\nIn many contexts, particularly software, specifications are needed to avoid errors due to lack of compatibility, for instance, in interoperability issues.\n\nFor instance, when two applications share Unicode data, but use different normal forms or use them incorrectly, in an incompatible way or without sharing a minimum set of interoperability specification, errors and data loss can result. For example, Mac OS X has many components that prefer or require only decomposed characters (thus decomposed-only Unicode encoded with UTF-8 is also known as \"UTF8-MAC\"). In one specific instance, the combination of OS X errors handling composed characters, and the samba file- and printer-sharing software (which replaces decomposed letters with composed ones when copying file names), has led to confusing and data-destroying interoperability problems.\n\nApplications may avoid such errors by preserving input code points, and only normalizing them to the application's preferred normal form for internal use.\n\nSuch errors may also be avoided with algorithms normalizing both strings before any binary comparison.\n\nHowever errors due to file name encoding incompatibilities have always existed, due to a lack of minimum set of common specification between software hoped to be inter-operable between various file system drivers, operating systems, network protocols, and thousands of software packages.\n\nA formal specification is a mathematical description of software or hardware that may be used to develop an implementation. It describes \"what\" the system should do, not (necessarily) \"how\" the system should do it. Given such a specification, it is possible to use formal verification techniques to demonstrate that a candidate system design is correct with respect to that specification. This has the advantage that incorrect candidate system designs can be revised before a major investment has been made in actually implementing the design. An alternative approach is to use provably correct refinement steps to transform a specification into a design, and ultimately into an actual implementation, that is correct by construction.\n\nIn (hardware, software, or enterprise) systems development, an architectural specification is the set of documentation that describes the structure, behavior, and more views of that system.\n\nA program specification is the definition of what a computer program is expected to do. It can be \"informal\", in which case it can be considered as a user manual from a developer point of view, or \"formal\", in which case it has a definite meaning defined in mathematical or programmatic terms. In practice, many successful specifications are written to understand and fine-tune applications that were already well-developed, although safety-critical software systems are often carefully specified prior to application development. Specifications are most important for external interfaces that must remain stable.\n\nIn software development, a functional specification (also, functional spec or specs or functional specifications document (FSD)) is the set of documentation that describes the behavior of a computer program or larger software system. The documentation typically describes various inputs that can be provided to the software system and how the system responds to those inputs.\n\nWeb services specifications are often under the umbrella of a quality management system.\n\nThese types of documents define how a specific document should be written, which may include, but is not limited to, the systems of a document naming, version, layout, referencing, structuring, appearance, language, copyright, hierarchy or format, etc. Very often, this kind of specifications is complemented by a designated template.\n\n"}
{"id": "16017237", "url": "https://en.wikipedia.org/wiki?curid=16017237", "title": "Style guide", "text": "Style guide\n\nA style guide (or manual of style) is a set of standards for the writing and design of documents, either for general use or for a specific publication, organization, or field. (It is often called a style sheet, though that term has other meanings.)\n\nA style guide establishes and enforces style to improve communication. To do that, it ensures consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography. For academic and technical documents, a guide may also enforce the best practice in ethics (such as authorship, research ethics, and disclosure), pedagogy (such as exposition and clarity), and compliance (technical and regulatory).\n\nStyle guides are common for general and specialized use, for the general reading and writing audience, and for students and scholars of various academic disciplines, medicine, journalism, the law, government, business, and specific industries. House style refers to the internal style manual of a particular publisher or organization.\n\nStyle guides vary widely in scope and size.\n\nThis variety in scope and length is enabled by the cascading of one style over another, in a way analogous to how styles cascade in web development and in desktop publishing (e.g., how inline styles in HTML cascade over CSS styles).\n\nA short style guide is often called a \"style sheet\". A comprehensive guide tends to be long and is often called a \"style manual\" or \"manual of style\" (\"MOS\" or \"MoS\"). In many cases, a project such as one book, journal, or monograph series typically has a short style sheet that cascades over the somewhat larger style guide of an organization such as a publishing company, whose content is usually called \"house style\". Most house styles, in turn, cascade over an \"industry-wide or profession-wide style manual\" that is even more comprehensive. Some examples of these industry style guides include the following: \n\nFinally, these reference works cascade over the orthographic norms of the language in use (for example, English orthography for English-language publications). This, of course, may be subject to national variety such as the different varieties of American English and British English.\n\nSome style guides focus on specific topic areas such as graphic design, including typography. Website style guides cover a publication's visual and technical aspects along with text.\n\nStyle guides that cover usage may suggest ways of describing people that avoid racism, sexism, and homophobia. Guides in specific scientific and technical fields cover nomenclature, which specifies names or classifying labels that are preferred because they are clear, standardized, and ontologically sound (e.g., taxonomy, chemical nomenclature, and gene nomenclature).\n\nMost style guides are revised periodically to accommodate changes in conventions and usage. The frequency of updating and the revision control are determined by the subject matter. For style manuals in reference work format, new editions typically appear every 1 to 20 years. For example, the AP Stylebook is revised annually, and the Chicago, APA, and ASA manuals are in their 17th, 6th, and 4th editions, respectively. Many house styles and individual project styles change more frequently, especially for new projects.\n\nSeveral basic style guides for technical and scientific communication have been defined by international standards organizations. One example is ISO 215 \"Documentation — Presentation of contributions to periodicals and other serials\".\n\nThe European Union publishes an \"Interinstitutional style guide\"—encompassing 24 languages across the European Union. This manual is \"obligatory\" for all those employed by the institutions of the EU who are involved in preparing EU documents and works. The Directorate-General for Translation of the European Commission publishes its own \"English Style Guide\", intended primarily for English-language authors and translators, but aiming to serve a wider readership as well.\n\n\nGeneral\n\nJournalism\n\nLaw\n\n\n\nIn the United States, most public-facing corporate communication and journalism writing is written with styles following \"The Associated Press Stylebook\". Book publishers and authors of journals requiring reference sections generally choose the Chicago Manual of Style, while scholarly writing often follows the \"MLA Style Manual and Guide to Scholarly Publishing\". One of the most popular grammar guides used in third-person writing is \"The Elements of Style\". The Associated Press Stylebook is written to be used together with The Elements of Style to provide a very complete grammar and English style reference with no conflicts.\n\n\n\n\n\n\n\nDespite the near uniform use of the Bluebook, nearly every state has appellate court rules that specify citation methods and writing styles specific to that state - and the Supreme Court of the United States has its own citation method. However, in most cases these are derived from the Bluebook.\n\nThere are also several other citation manuals available to legal writers in wide usage in the United States. Virtually all large law firms maintain their own citation manual and several major publishers of legal texts (West, Lexis-Nexis, Hein, \"et al.\") maintain their own systems.\n\n\n\n\n\nGuidelines for citing web content also appear in comprehensive style guides such as Oxford/Hart, Chicago and MLA.\n\n"}
{"id": "23421828", "url": "https://en.wikipedia.org/wiki?curid=23421828", "title": "Technological revolution", "text": "Technological revolution\n\nA technological revolution is a period in which one or more technologies is replaced by another technology in a short amount of time. It is an era of accelerated technological progress characterized by new innovations whose rapid application and diffusion cause an abrupt change in society.\n\nA technological revolution is made of interconnected technological changes. \n\nA technological revolution increases productivity and efficiency. It may involve material or ideological changes caused by the introduction of a device or system. Some examples of its potential impact are business management, education, social interactions, finance and research methodology; it is not limited strictly to technical aspects. Technological revolution rewrites the material conditions of human existence and can reshape culture. It can play a role of a trigger of a chain of various and unpredictable changes:\n\nWhat distinguishes a technological revolution from a random collection of technology systems and justifies conceptualizing it as a revolution are two basic features:\n\n1. The strong interconnectedness and interdependence of the participating systems in their technologies and markets.\n\n2. The capacity to transform profoundly the rest of the economy (and eventually society).\n\nThe consequences of a technological revolution are not necessarily positive. For example, innovations, such as the use of coal as an energy source, can have negative environmental impact and cause technological unemployment. The concept of technological revolution is based on the idea that technological progress is not linear but undulatory. Technological revolution can be:\n\n\nThe concept of universal technological revolutions is a key factor in the Neo-Schumpeterian theory of long economic waves/cycles (Carlota Perez, Tessaleno Devezas, Daniel Šmihula and others).\n\nThe most known example of technological revolution was the Industrial Revolution in the 19th century, the scientific-technical revolution about 1950–1960, the Neolithic revolution, the Digital revolution and so on. The notion of \"technological revolution\" is frequently overused, therefore it is not easy to define which technological revolutions having occurred during world history were really crucial and influenced not only one segment of human activity, but had a universal impact. One universal technological revolution should be composed from several sectoral technological revolutions (in science, industry, transport and the like).\n\nWe can identify several universal technological revolutions which occurred during the modern era in Western culture:\n\nAttempts to find comparable periods of well defined technological revolutions in the pre-modern era are highly speculative. Probably one of the most systematic attempts to suggest a timeline of technological revolutions in pre-modern Europe was done by Daniel Šmihula: \n\nAfter 2000 there became popular the idea that a sequence of technological revolutions is not over and in the forthcoming future we will witness the dawn of a new universal technological revolution. The main innovations should develop in the fields of nanotechnologies, alternative fuel and energy systems, biotechnologies, genetic engineering, new materials technologies and so on .\n\nSometimes the notion of \"technological revolution\" is used for the Second Industrial Revolution in the period about 1900, but in this case the designation \"technical revolution\" would be more proper. When the notion of technical revolution is used in more general meaning it is almost identical with technological revolution, but technological revolution requires material changes in used tools, machines, energy sources, production processes. Technical revolution can be restricted to changes in management, organisation and so called non-material technologies (e.g. a progress in mathematics or accounting).\n\n\n"}
{"id": "17163802", "url": "https://en.wikipedia.org/wiki?curid=17163802", "title": "Technology dynamics", "text": "Technology dynamics\n\nTechnology dynamics is broad and relatively new scientific field that has been developed in the framework of the postwar science and technology studies field. It studies the process of technological change. Under the field of Technology Dynamics the process of technological change is explained by taking into account influences from \"internal factors\" as well as from \"external factors\". Internal factors relate technological change to unsolved technical problems and the established modes of solving technological problems and external factors relate it to various (changing) characteristics of the social environment, in which a particular technology is embedded.\n\nFor the last three decades, it has been argued that technology development is neither an autonomous process, determined by the \"inherent progress\" of human history, nor a process completely determined by external conditions like the prices of the resources that are needed to operate (develop) a technology, as it is theorized in neoclassical economic thinking. In mainstream neoclassical economic thinking, technology is seen as an exogenous factor: at the moment a technology is required, the most appropriate version can be taken down from the shelf based on costs of labor, capital and eventually raw materials.\n\nConversely, modern technology dynamics studies generally advocate that technologies are not \"self-evident\" or market-demanded, but are the upshot of a particular path of technology development and are shaped by social, economic and political factors. in this sense, technology dynamics aims at overcoming distinct \"internal\" and \"external\" points of views by presenting co-evolutionary approach regarding technology development.\n\nIn general, technology dynamics studies, besides giving a \"thick description\" of technology development, uses constructivist viewpoints emphasizing that technology is the outcome of particular social context. Accordingly, Technology Dynamics emphasizes the significance and possibility of regaining social control of technology, and also provides mechanisms needed to adapt to and steer the development of certain technologies. In that respect, it uses insights from retrospective studies to formulate hypotheses of a prospective nature on technology development of emerging technologies, besides formulating prescriptive policy recommendations.\n\nAn important feature of relevant theories of technological change therein is that they underline the quasi-evolutionary character of technological change: change based on technological variation and social selection in which technological knowledge, systems and institutions develop in interaction with each other. Processes of 'path dependence' are crucial in explaining technological change.\n\nFollowing these lines, there have been different approaches and concepts used under the field of technology dynamics.\n\n\nBased on the analysis of the various perspectives, one can aim at developing interventions in the dynamics of a technology. Some approaches have been developed targeting on interventions in technological change:\n\n\n\n"}
{"id": "30059402", "url": "https://en.wikipedia.org/wiki?curid=30059402", "title": "Technopolis Gusev", "text": "Technopolis Gusev\n\nTechnopolis GS is a project to create a modern electronics industrial park in the Kaliningrad region of Russia.\n\nIn 2007, in Gusev, for the first time in Russia, was established a production of set-top boxes for receiving satellite and terrestrial television broadcasts. Several months later, the General Satellite Corporation started building a plant in Gusev to manufacture household electronics products. The agreement between the mayor of Gusev, Nikolay Tsukanov, and the president of the General Satellite Corporation, Andrey Tkachenko, was signed in 2008, in order to create a modern industrial park in the city. The idea was approved by local authorities and was presented at the International Investment Forum \"Sochi-2008\".\n\nThe decision to locate the new production facilities in Gusev, according to Andrey Tkachenko, was made for two reasons. \nFirst, the experience of the first similar plant that had been working in Gusev as part of the special economic zone - the official residence of the Corporation - was successful. Second, the administration of the city took a great interest in cooperation for the creation of Technopolis and in a comprehensive solution to the problem of the territory development.\n\nIn 2009, two plants, JSC NPO Digital Television Systems (DTS) and Prankor, Ltd., were run - their products have no analogues in Russia. The plants produce set-top boxes for receiving the satellite and terrestrial TV broadcasts, and the production conveyor ensures a complete production cycle: starting with the motherboard and all the way up to the case and the satellite antennas as well. Formerly, most of these devices in the Russian market were imported, usually from China.\n\nTo create in Gusev a new effective pole for innovative development of Russia.\n\n\n\n400 hectares. Over 3000 jobs.\n\n— Opening the Plant for the household radio-electronics products' manufacture; \n— Commissioning the Plant for manufacturing metal and plastic products by stamping.\n— Opening a Factory for the production of corrugated cardboard and packaging;\n— Commissioning the House-Building Plant.\n— Erection of the first stage of the cottage settlement in Gusev;\n— Commissioning the social and business center of Technopolis.\n— Inaugurating a microelectronics Factory (microprocessors assembly and casing);\n— Construction of the second stage of the cottage settlement in Gusev;\n— Integrated land improvement and social infrastructure construction;\n— Opening a branch of SPSU, organization of educational and research process at the University's Education and Research Complex.\n— Inaugurating a printed circuit board producing Plant;\n— Setting on foot a Customs Warehouse.\n\n\n\n\n\nThe market observers, in general, welcomed the General Satellite initiative and the investment prospects. \"Industrial parks development, on one hand, is very expensive, and on the other – it is extremely knowledge-intensive”, - Pavel Zhavoronkov, an analyst at the investment firm Sovlink, said, while making a comment on the General Satellite Corporation plans. - “Based on the payroll size, 1500 to 2400 million rubles will be spent till the year 2011. The average payroll should be 1.95 billion, which will \"swallow\" almost 40% of the capital raised. The establishment of such an Industrial Park is possible only if stringent standards and practices are observed in the construction and cash flow. From the perspective of the country, it is an invaluable contribution to the development and science.\"\nWhile implementing such projects, developers are often faced with many difficulties, of which the most serious are legislation \"gaps\" and official circumlocution6. In this case, the local officials led by Georgi Boos – the Kaliningrad region Governor – proved to be a considerable support for the Project.\n\n"}
{"id": "40076768", "url": "https://en.wikipedia.org/wiki?curid=40076768", "title": "Telepharmacy", "text": "Telepharmacy\n\nTelepharmacy is the delivery of pharmaceutical care via telecommunications to patients in locations where they may not have direct contact with a pharmacist. It is an instance of the wider phenomenon of telemedicine, as implemented in the field of pharmacy. Telepharmacy services include drug therapy monitoring, patient counseling, prior authorization and refill authorization for prescription drugs, and monitoring of formulary compliance with the aid of teleconferencing or videoconferencing. Remote dispensing of medications by automated packaging and labeling systems can also be thought of as an instance of telepharmacy. Telepharmacy services can be delivered at retail pharmacy sites or through hospitals, nursing homes, or other medical care facilities.\n\nThe term can also refer to the use of videoconferencing in pharmacy for other purposes, such as providing education, training, and management services to pharmacists and pharmacy staff remotely.\n\nA primary appeal of telepharmacy is its potential to expand access to pharmacy care in smaller rural communities, some of which cannot support a full-time pharmacist or cannot easily recruit a pharmacist to reside in their region. Telepharmacy can potentially give patients in remote locations access to professional pharmacy care that could not be received locally, which can lower costs and improve patient safety through better patient counseling, drug administration monitoring, and compliance monitoring. Sharing of pharmacists between sites can also decrease costs in existing facilities, which might no longer need to employ a full-time pharmacist.\n\nThe potential costs of telepharmacy are broadly the same as those associated with all forms of telemedicine: potentially decreased human interaction between medical professionals and patients, an increased risk of error when medical services are delivered in the absence of a registered professional, and an increased risk that protected health information may be compromised through electronic information storage and transmission.\n\nThe implementation of telepharmacy varies by region and jurisdiction. Factors including geography, laws and regulations, and economics influence its implementation.\n\nA form of telepharmacy has been in use by Australia's Royal Flying Doctor Service since 1942. Medical chests containing medications and equipment are placed in remote communities where they can be administered to patients during a telehealth consultation. Some 3,500 chests were distributed around Australia as of 2006. In one year, Queensland recorded 21,470 telehealth consultations, of which 13.7% resulted in administration of a medication from a medical chest. The medication types administered most often are antibiotics, analgesics and gastrointestinal medications. This system improves access to both emergency and routine medical care in remote parts of Australia and reduces the need for patients to travel to seek medical care.\n\nAnother application of telepharmacy in Queensland has been the provision of pharmaceutical reviews in rural hospitals that lack on-staff pharmacists. Although broader use of telepharmacy could help alleviate a shortage of pharmacists, Australia has lagged the United States in its implementation of telepharmacy, partly because doctors, nurses, and other health care workers provide pharmacy services in rural and remote areas where there are no pharmacists.\n\nImplementation of telepharmacy in the United States began in the 2000s. A combination of factors, including changes in Medicare reimbursement for medications and the recession of 2007–8, led to a decline in the number of independent pharmacies in rural areas. In response to the need for alternative means of delivering pharmacy in services in rural communities lacking a full-time pharmacist, several midwestern and northwestern states with extensive rural areas have led much of the development of policy and implementation methods for telepharmacy.\n\nIn 2001, North Dakota became the first U.S. state to pass regulations allowing retail pharmacies to operate without requiring a pharmacist to be physically present. The next year, state agencies and grants established the North Dakota Telepharmacy Project, which now supports more than fifty remote retail and hospital pharmacy sites throughout North Dakota. In this program, a licensed pharmacist at a central site communicates with remote site pharmacy technicians and patients through videoconferencing. A 2004 study of the program found that telepharmacy delivered the same quality of pharmacy services as traditional facilities, and a study of the operation of one North Dakota telepharmacy business from 2002 through 2004 found that, while medication inventory turnover was lower than the industry average, the remote sites were able to be operated profitably. The success and expansion of this program were an inspiration and model for programs and laws in other states.\n\nThe Community Health Association of Spokane, a network of community health centers in Spokane, Washington, started a telepharmacy program in 2001. The program delivers remote medication dispensing and health counseling to patients at six urban and rural clinics; remote site personnel are connected to pharmacists at the base site by videoconferencing. A survey found that most patients at the remote sites strongly agreed or agreed that they would have had difficulty affording their medications without this program.\n\nThe Alaska Native Medical Center, a hospital in Anchorage, Alaska, providing telehealth services to Alaska Native populations, established a telepharmacy program in 2003 to improve its pharmaceutical services in rural native settlements. The American Society of Health-System Pharmacists gave the program its 2006 Award for Excellence in Medication-Use Safety, concluding that the use of telepharmacy had improved access to pharmaceutical care and enabled pharmacists to monitor medication safety and encourage medication adherence, as well as making pharmacy care more cost-effective.\n\nThe U.S. Navy Bureau of Medicine operates a large-scale telepharmacy program for the use of service personnel. After piloting the program in 2006 at Naval Hospital Pensacola in Florida and Naval Hospital Bremerton in Washington, in 2010 the Navy expanded it to more sites throughout the world. This program represents the largest implementation of telepharmacy to date.\n\nCalifornia passed a Telehealth Advancement Act in 2011 to update the state's legal definitions of telehealth, simplify approval processes for telehealth services, and broaden the range of medical services that may be provided via telehealth. The law establishes legal parity between the direct and remote delivery of pharmacy care. Iowa's first telepharmacy opened in September 2012 after receiving a three-year waiver from the Iowa Board of Pharmacy that allows the facility to operate without a pharmacist on-site.\n\nA 2010 study of the various American states' rural health offices found that telepharmacy in rural medical facilities varied in prevalence across the United States but was still not widespread, and that many states had not yet clearly defined regulations for telepharmacy in hospitals. Adoption and implementation of telepharmacy methods has been slow compared to the spread of the basic technologies involved (internet access, audio/video compression algorithms, microphones and video cameras), despite periodic predictions of a forthcoming boom in the industry. Aside from more intangible factors (such as physicians' and pharmacists' personal uneasiness with the lack of physical interaction with patients), the major obstacles to telepharmacy implementation appear to have been the lack of clear legal regulations for telepharmacy, and the lack of network and software systems to manage (and secure) all of the data used in a professional pharmacy. As of 2010, many of the telepharmacy facilities in active operation were operating as pilot programs or under temporary waivers issued by state regulators because many states still had no clear legal framework for the regulation of remote pharmaceutical sites without pharmacists. Even in states that had regulated retail telepharmacy practices, regulations were often not in place to permit the implementation of telepharmacy in hospital settings. For some pharmacy facilities that might otherwise consider telepharmacy, the cost and complexity of the infrastructure needed to manage patient data across multiple sites can be prohibitive. In addition to the computer hardware required for patient data storage, distribution and teleconferencing, telepharmacy programs must deploy network security tools and procedures adequate to protect patient medical information in compliance with HIPAA and other patient privacy regulations. In 2010 the North Dakota Telepharmacy Project estimated that the computer hardware needed for a typical retail installation costs US$17,300 per site, with an additional cost of US$5,000 to buy a mobile cart for a hospital installation.\n\nAdoption of telepharmacy in Canada began as a response to a nationwide shortage of pharmacists. Canada's first telepharmacy service was started by a hospital in Cranbrook, British Columbia, in June 2003 in order to assist a hospital in a nearby town that was unable to hire a pharmacist. To meet the need for service, a hospital pharmacist in Cranbrook began using telepharmacy technology to oversee pharmacy technicians at the other hospital. A similar service was subsequently extended to other small hospitals in the province; it is also used to provide coverage when a hospital's sole pharmacist is absent due to illness or vacation. Remote dispensing machines for medication began operation in Ontario, Canada, in 2007. After a patient inserts a prescription into the dispensing machine, the prescription is scanned and the patient is connected by telephone videoconference to a pharmacist at a remote site. The pharmacist reviews the prescription, discusses the patient's medication history, and authorizes the machine to dispense medication to the patient. The machines proved successful, with one assessment revealing that 96% of patients using them had their prescription filled in under five minutes. As of 2009, a hospital in Ontario, Canada, was using telepharmacy services in addition to retaining a pharmacist at the hospital; the telepharmacist reviews medication orders, while the on-site pharmacist works with patients and oversees medication safety in the facility. Thus telepharmacy support allows the on-site pharmacist to focus on the more sensitive and nuanced tasks for which physical presence is most helpful.\n\nAfter their success in Canada, remote medication dispensing machines were scheduled to be tested at several hospital locations in the United Kingdom beginning in 2010. In 2013, Maxor National Pharmacy Services, a U.S. company, reported that its remote dispensing machines for medication were being used in Bahrain, Belgium, Cuba, England, Germany, Guam, Italy, Japan, Spain and Venezuela.\n\nIn 2010, Mannings drugstores became the first in Hong Kong to use videoconferencing to allow patients at outlets without full-time pharmacists to consult with pharmacists at other sites.\n"}
{"id": "1714828", "url": "https://en.wikipedia.org/wiki?curid=1714828", "title": "Theory of operation", "text": "Theory of operation\n\nA theory of operation is a description of how a device or system should work. It is often included in documentation, especially maintenance/service documentation, or a user manual. It aids troubleshooting by providing the troubleshooter with a mental model of how the system is supposed to work. The troubleshooter can then more easily identify discrepancies, to aid diagnosis of problem.\n\nIBM Redbooks are \"Theories of operation\" of their products.\n\n"}
{"id": "28774446", "url": "https://en.wikipedia.org/wiki?curid=28774446", "title": "Water Data Transfer Format", "text": "Water Data Transfer Format\n\nWater Data Transfer Format (WDTF) is a data delivery standard implemented by the Australian Bureau of Meteorology (BoM) that was jointly developed with the CSIRO. The standard, released in 2009, specifies both the format of and the techniques used to deliver Australian water data measurements to the BoM. Some private organisations and government agencies in Australia that collect water data are mandated to deliver it to the BoM according to the (Australian) Water Act 2007.\n\nAn external meteorological data source that delivers data in WDTF-compliant forms is the CSIRO Land & Water's Automatic Weatherstation Network. Data from this weatherstation network can be viewed in a web browser, downloaded at text values in CSV format, downloaded in a condensed XML format for machine-to-machine communications, or downloaded as WDTF-compliant data.\nThe use of WDTF is an example of work in the field of Irrigation Informatics.\n\n"}
{"id": "6828605", "url": "https://en.wikipedia.org/wiki?curid=6828605", "title": "Woodward effect", "text": "Woodward effect\n\nThe Woodward effect, also referred to as a Mach effect, is part of a hypothesis proposed by James F. Woodward in 1990. The hypothesis states that transient mass fluctuations arise in any object that absorbs internal energy while undergoing a proper acceleration. Harnessing this effect could generate a reactionless thrust, which Woodward and others claim to measure in various experiments.\n\nHypothetically, the Woodward effect would allow for field propulsion spacecraft engines that would not have to expel matter. Such a proposed engine, is sometimes called a Mach effect thruster (MET) or a Mach Effect Gravitational Assist (MEGA) drive. So far, experimental results have not strongly supported this hypothesis, but experimental research on this effect, and its potential applications, continues. The anomalous thrust detected in some RF resonant cavity thruster (EmDrive/Cannae drive) experiments may be explained by the same type of Mach effect proposed by Woodward.\n\nThe Space Studies Institute was selected as part of NASA's Innovative Advanced Concepts program as a Phase I proposal in April 2017 for Mach Effect research. The year after, NASA awarded a NIAC Phase II grant to the SSI to further develop these propellantless thrusters.\n\nThe effect is controversial within mainstream physics because the underlying mathematics appears faulty, and the effect, if found to be real, would violate momentum conservation and energy conservation.\n\nAccording to Woodward, at least three Mach effects are theoretically possible: vectored impulse thrust, open curvature of spacetime, and closed curvature of spacetime.\n\nThe first effect, the Woodward effect, is the minimal energy effect of the hypothesis. The Woodward effect is focused primarily on proving the hypothesis and providing the basis of a Mach effect impulse thruster. In the first of three general Mach effects for propulsion or transport, the Woodward effect is an impulse effect usable for in-orbit satellite station-keeping, spacecraft reaction control systems, or at best, thrust within the solar system. The second and third effects are open and closed space-time effects. Open curved space-time effects can be applied in a field generation system to produce warp fields. Closed-curve space-time effects would be part of a field generation system to generate wormholes.\n\nThe third Mach effect is a closed-curve spacetime effect or closed timelike curve called a benign wormhole. Closed-curve space is generally known as a wormhole or black hole. Prompted by Carl Sagan for the scientific basis of wormhole transport in the movie \"Contact\", Kip Thorne developed the theory of benign wormholes. The generation, stability, and traffic control of transport through a benign wormhole is only theoretical at present. One difficulty is the requirement for energy levels approximating a \"Jupiter size mass\".\n\nKenneth Nordtvedt showed in 1988 that gravitomagnetism, which is an effect predicted by general relativity but hadn't yet been observed at that time and was even challenged by the scientific community, is inevitably a real effect because it is a direct consequence of the gravitational vector potential. He subsequently showed that the gravitomagnetism interaction (not to be confused with the Nordtvedt effect), like inertial frame dragging and the Lense–Thirring precession, is typically a Mach effect.\n\nThe Woodward effect is based on the relativistic effects theoretically derived from Mach's principle on inertia within general relativity, attributed by Albert Einstein to Ernst Mach. Mach's Principle is generally defined as \"the local inertia frame that is completely determined by the dynamic fields in the Universe.\" The conjecture comes from a thought experiment:\n\nA formulation of Mach's principle was first proposed as a vector theory of gravity, modeled on Maxwell's formalism for electrodynamics, by Dennis Sciama in 1953, who then reformulated it in a tensor formalism equivalent to general relativity in 1964.\n\nIn this paper, Sciama stated that instantaneous inertial forces in all accelerating objects are produced by a primordial gravity-based inertial radiative field created by distant cosmic matter and propagating both forward \"and\" backward in time at light speed:\nSciama's inertial-induction idea has been shown to be correct in Einstein's general relativity for any Friedmann–Robertson–Walker cosmology. According to Woodward, the derivation of Mach effects is relativistically invariant, so the conservation laws are satisfied, and no \"new physics\" is involved besides general relativity.\n\nAs previously formulated by Sciama, Woodward suggests that the Wheeler–Feynman absorber theory would be the correct way to understand the action of instantaneous inertial forces in Machian terms.\n\nThe Wheeler-Feynman absorber theory is an interpretation of electrodynamics that starts from the idea that a solution to the electromagnetic field equations has to be symmetric with respect to time-inversion, as are the field equations themselves. Wheeler and Feynman showed that the propagating solutions to classical wave equations can either be \"retarded\" (i.e. propagate forward in time) or \"advanced\" (propagate backward in time). The absorber theory has been used to explain quantum entanglement and led to the transactional interpretation of quantum mechanics, as well as the Hoyle-Narlikar theory of gravity, a Machian version of Einstein's general relativity. Fred Hoyle and Jayant Narlikar originally developed their cosmological model as a quasi steady state model of the universe, adding a \"Creation field\" generating matter out of empty space, an hypothesis contradicted by recent observations. When the C-field is not used, ignoring the parts regarding mass creation, the theory is no longer steady state and becomes a Machian extension of general relativity. This modern development is known as the \"Gravitational Absorber Theory\".\n\nAs the gravitational absorber theory reduces to general relativity in the limit of a smooth fluid model of particle distribution, both theories make the same predictions. Except in the Machian approach, a mass changing effect emerges from the general equation of motion, from which Woodward's transient mass equation can be derived. A resulting force suitable for Mach effect thrusters can then be calculated.\n\nWhile the Hoyle-Narlikar derivation of the Mach effect transient terms is done from a fully nonlinear, covariant formulation, it has been shown Woordward's transient mass equation can also be retrieved from linearized general relativity.\n\nThe following has been detailed by Woodward in various peer-reviewed papers throughout the last twenty years.\n\nAccording to Woodward, a transient mass fluctuation arises in an object when it absorbs \"internal\" energy as it is accelerated. Several devices could be built to store internal energy during accelerations. A measurable effect needs to be driven at a high frequency, so macroscopic mechanical systems are out of question since the rate at which their internal energy could be modified is too limited. The only systems that could run at a high frequency are electromagnetic energy storage devices. For fast transient effects, batteries are ruled out. A magnetic energy storage device like an inductor using a high-permeability core material to transfer the magnetic energy could be specially built. But capacitors are preferable to inductors because compact devices storing energy at a very high energy density without electrical breakdown are readily available. Shielding electrical interferences are easier than shielding magnetic ones. Ferroelectric materials can be used to make high-frequency electro-mechanical actuators, and they are themselves capacitors so they can be used for both energy storage and acceleration. Finally, capacitors are cheap and available in various configurations. So Mach effect experiments have always relied on capacitors so far.\n\nWhen the dielectric of a capacitor is submitted to a varying electric power (charge or discharge), Woodward's hypothesis predicts a transient mass fluctuation arises according to the transient mass equation (TME):\n\nwhere:\n\nThis equation is not the full Woodward equation as seen in the book. There is a third term, formula_8which Woodward discounts because his gauge setsformula_9; the derivatives of this quantity must therefore be negligible.\n\nThe previous equation shows that when the dielectric material of a capacitor is cyclically charged then discharged while being accelerated, its mass density fluctuates, by around plus or minus its rest mass value. Therefore, a device can be made to oscillate either in a linear or orbital path, such that its mass density is higher while the mass is moving forward, and lower while moving backward, thus creating an acceleration of the device in the forward direction, i.e. a thrust. This effect, used repeatedly, does not expel any particle and thus would represent a type of apparent propellantless propulsion, which seems to be in contradiction with Newton's third law of motion. However, Woodward states there is no violation of momentum conservation in Mach effects:\nTwo terms are important for propulsion on the right-hand side of the previous equation:\n\nApplications of propellantless propulsion include straight-line thruster or impulse engine, open curved fields for starship warp drives, and even the possibility of closed curved fields such as traversable benign wormholes.\n\nThe mass of the electron is positive according to the mass–energy equivalence \"E\" = \"mc\" but this invariant mass is made from the bare mass of the electron \"clothed\" by a virtual photon cloud. According to quantum field theory, as those virtual particles have an energy more than twice the bare mass of the electron, mandatory for pair production in renormalization, the nonelectromagnetic bare mass of the \"unclothed\" electron has to be \"negative\".\n\nUsing the ADM formalism, Woodward proposes that the physical interpretation of the \"wormhole term\" in his transient mass equation could be a way to expose the negative bare mass of the electron, in order to produce large quantities of exotic matter that could be used in a warp drive to propel a spacecraft or generate traversable wormholes.\n\nCurrent spacecraft achieve a change in velocity by the expulsion of propellant, the extraction of momentum from stellar radiation pressure or the stellar wind or the utilisation of a gravity assist (\"slingshot\") from a planet or moon. These methods are limiting in that rocket propellants have to be accelerated as well and are eventually depleted, and the stellar wind or the gravitational fields of planets can only be utilized locally in the Solar System. In interstellar space and bereft of the above resources, different forms of propulsion are needed to propel a spacecraft, and they are referred to as advanced or .\n\nIf the Woodward effect is confirmed and if an engine can be designed to use applied Mach effects, then a spacecraft may be possible that could maintain a steady acceleration into and through interstellar space without the need to carry along propellants. Woodward presented a paper about the concept at the NASA Breakthrough Propulsion Physics Program Workshop conference in 1997, and continued to publish on this subject thereafter.\n\nEven ignoring for the moment the impact on interstellar travel, future spacecraft driven by impulse engines based on Mach effects would represent an astounding breakthrough in terms of interplanetary spaceflight alone, enabling the rapid colonization of the entire solar system. Travel times being limited only by the specific power of the available power supplies and the acceleration human physiology can endure, they would allow crews to reach any moon or planet in our solar system in less than three weeks. For example, a typical one-way trip at an acceleration of 1 g from the Earth to the Moon would last only about 4 hours; to Mars, 2 to 5 days; to the asteroid belt, 5 to 6 days; and to Jupiter, 6 to 7 days.\n\nAs shown by the transient mass fluctuation equation above, exotic matter could be theoretically created. A large quantity of negative energy density would be the key element needed to create warp drives as well as traversable wormholes. As such, if proven to be scientifically valid, practically feasible and scaling as predicted by the hypothesis, the Woodward effect could not only be used for interplanetary travel, but also for apparent faster-than-light interstellar travel:\n\nTwo patents have been issued to Woodward and associates based on how the Woodward effect might be used in practical devices for producing thrust:\n\nWoodward and his associates have claimed since the 1990s to have successfully measured forces at levels great enough for practical use and also claim to be working on the development of a practical prototype thruster. No practical working devices have yet been publicly demonstrated.\n\nThe NIAC contract awarded in 2017 by NASA for the development of Mach effect thrusters is a primary three-task effort, two experimental and one analytical:\n\n\nA former type of Mach effect thruster was the Mach-Lorentz thruster (MLT). It used a charging capacitor embedded in a magnetic field created by a magnetic coil. A Lorentz force, the cross product between the electric field and the magnetic field, appears and acts upon the ions inside the capacitor dielectric. In such electromagnetic experiments, the power can be applied at frequencies of several megahertz, unlike PZT stack actuators where frequency is limited to tens of kilohertz. The photograph shows the components of a Woodward effect test article used in a 2006 experiment.\n\nHowever, a problem with some of these devices was discovered in 2007 by physicist Nembo Buldrini, who called it the \"Bulk Acceleration Conjecture\":\n\nTo address this issue, Woodward started to design and build a new kind of device known as a MET (Mach Effect Thruster) and later a MEGA drive (Mach Effect Gravitational Assist drive), using capacitors and a series of thick PZT disks. This ceramic is piezoelectric, so it can be used as an electromechanical actuator to accelerate an object placed against it: its crystalline structure expands when a certain electrical polarity is applied, then contracts when the opposite field is applied, and the stack of discs vibrates.\n\nIn the first tests, Woodward simply used a capacitor between two stacks of PZT disks. The capacitor, while being electrically charged to change its internal energy density, is shuttled back and forth between the PZT actuators. Piezoelectric materials can also generate a measurable voltage potential across their two faces when pressed, so Woodward first used some small portions of PZT material as little accelerometers put on the surface of the stack, to precisely tune the device with the power supply. Then Woodward realized that PZT material and the dielectric of a capacitor were very similar, so he built devices that are made exclusively of PZT disks, without any conventional capacitor, applying different signals to different portions of the cylindrical stack. The available picture taken by his graduate student Tom Mahood in 1999 shows a typical all-PZT stack with different disks:\nDuring forward acceleration and before the transient mass change in the capacitor decays, the resultant increased momentum is transferred forward to a bulk \"reaction mass\" through an elastic collision (the brass end cap on the left in the picture). Conversely, the following decrease in the mass density takes place during its backward movement.\nWhile operating, the PZT stack is isolated in a Faraday cage and put on a sensitive torsion arm for thrust measurements, inside a vacuum chamber. Throughout the years, a wide variety of different types of devices and experimental setups have been tested. The force measuring setups have ranged from various load cell devices to ballistic pendulums to multiple torsion arm pendulums, in which movement is actually observed. Those setups have been improved against spurious effects by isolating and canceling thermal transfers, vibration and electromagnetic interference, while getting better current feeds and bearings. Null tests were also conducted.\n\nIn the future, Woodward plans to scale thrust levels, switching from the current piezoelectric dielectric ceramics (PZT stacks) to new high-κ dielectric nanocomposite polymers, like PMN, PMN-PT or CCTO. Nevertheless, such materials are new, quite difficult to find, and are electrostrictive, not piezoelectric.\n\nIn 2013, the Space Studies Institute announced the Exotic Propulsion Initiative, a new project privately funded that aims to replicate Woodward's experiments and then, if proven successful, fully develop exotic propulsion. Gary Hudson, president and CEO of SSI, presented the program at the 2014 NASA Institute for Advanced Concepts Symposium, and a NIAC phase I grant was awarded in April 2017 to develop a better theoretical model and technical solutions for greater efficiency as a TRL-1 technology: reduction of heating and longer operating time using \"chirped pulses\"; and design of a dedicated electronic circuit with better frequency impedance matching. The concept of an interstellar mission to Proxima Centauri b was also detailed. Consecutively to these achievements, a NIAC Phase II grant was awarded in March 2018 to test an improved design with a higher operational frequency to increase the output thrust.\n\nAnother type of claimed propellantless thruster, called the EmDrive by its inventor British engineer Roger Shawyer, has been proposed to work due to a Mach effect:\n\nThe asymmetric resonant microwave cavity would act as a capacitor where:\nWhen a polymer insert is placed asymmetrically in the cavity, its dielectric properties result in greater asymmetry, while decreasing the cavity \"Q\" factor.\nThe cavity's acceleration is a function of all the above factors, and the model can explain the acceleration of the cavity with and without a dielectric.\n\nFrom his initial paper onward Woodward has claimed that this effect is detectable with modern technology. He and others have performed and continue to perform experiments to detect the small forces that are predicted to be produced by this effect. So far some groups claim to have detected forces at the levels predicted and other groups have detected forces at much greater than predicted levels or nothing at all. To date there has been no announcement conclusively confirming proof for the existence of this effect or ruling it out.\n\n\n\n\n\n\n\n\n\n\n\nAll inertial frames are in a state of constant, rectilinear motion with respect to one another; they are not accelerating in the sense that an accelerometer at rest in one would detect zero acceleration. Despite their ubiquitous nature, inertial frames are still not fully understood. That they exist is certain, but what causes them to exist – and whether these sources could constitute reaction-media – are still unknown. Marc Millis, of the NASA Breakthrough Propulsion Physics Program, stated \" \"For example, the notion of thrusting without propellant evokes objections of violating conservation of momentum. This, in turn, suggests that space drive research must address conservation of momentum. From there it is found that many relevant unknowns still linger regarding the source of the inertial frames against which conservation is referenced. Therefore, research should revisit the unfinished physics of inertial frames, but in the context of propulsive interactions.\" \" Mach's principle is generally defined within general relativity as \"the local inertia frame is completely determined by the dynamic fields in the universe.\" Rovelli evaluated a number of versions of \"Mach's principle\" that exist in the literature. Some are partially correct and some have been dismissed as incorrect.\n\nA challenge to the mathematical foundations of Woodward's hypothesis were raised in a paper published by the Oak Ridge National Laboratory in 2001. In the paper, John Whealton noted that the experimental results of Oak Ridge scientists can be explained in terms of force contributions due to time-varying thermal expansion, and stated that a laboratory demonstration produced 100 times the Woodward effect without resorting to non-Newtonian explanations. In response, Woodward published a criticism of Whealton's math and understanding of the physics involved, and built an experiment attempting to demonstrate the flaw.\n\nA rate of change in momentum represents a force, whereby F \"ma. Whealton et al. use the technical definition, Fd(\"mv)/d\"t\", which can be expanded to F\"m\" dv/d\"t\" + d\"m\"/d\"t\" v. This second term has both delta mass and v, which is measured instantaneously; this term will, in general, cancel out the force from the inertial response terms predicted by Woodward. Woodward argued that the d\"m\"/d\"t\" v term does not represent a physical force on the device, because it vanishes in a frame where the device is momentarily stationary.\n\nIn an appendix to his thesis, Mahood argues that the unexpectedly small magnitude of the results in his experiments are a confirmation of the cancellation predicted by Whealton; the results are instead due to higher-order mass transients which are not exactly cancelled. Mahood would later describe this argument as \"one of the very few things I've done in my life that I actually regret\".\n\nAlthough the momentum and energy exchange with distant matter guarantees global conservation of energy and momentum, this field exchange is supplied at no material cost, unlike the case with conventional fuels. For this reason, when the field exchange is ignored, a propellantless thruster behaves locally like a free energy device. This is immediately apparent from basic Newtonian analysis: if constant power produces constant thrust, then input energy is linear with time and output (kinetic) energy is quadratic with time. Thus there exists a break-even time (or distance or velocity) of operation, above which more energy is output than is input. The longer it is allowed to accelerate, the more pronounced will this effect become, as simple Newtonian physics predicts.\n\nConsidering those conservation issues, a Mach effect thruster relies on Mach's principle, hence it is not an electrical to kinetic transducer, i.e. it does not convert electric energy to kinetic energy. Rather, a Mach effect thruster is a gravinertial transistor that controls the flow of gravinertial flux, in and out of the active mass of the thruster. The primary power into the thruster is contained in the flux of the gravitational field, not the electricity that powers the device. Failing to account for this flux, is much the same as failing to account for the wind on a sail. Mach effects are relativistic by nature, and considering a spaceship accelerating with a Mach effect thruster, the propellant is not accelerating with the ship, so the situation should be treated as an accelerating and therefore non-inertial reference frame, where F does not equal \"m\"a. Keith H. Wanser, professor of physics at California State University, Fullerton, published a paper in 2013 concerning the conservation issues of Mach effect thrusters.\n\nIn 2009, Harold \"Sonny\" White of NASA proposed the Quantum Vacuum Fluctuation (QVF) conjecture, a non-relativistic hypothesis based on quantum mechanics to produce momentum fluxes even in empty outer space. Where Sciama's gravinertial field of Wheeler–Feynman absorber theory is used in the Woodward effect, the White conjecture replaces the Sciama gravinertial field with the quantum electrodynamic vacuum field. The local reactive forces are generated and conveyed by momentum fluxes created in the QED vacuum field by the same process used to create momentum fluxes in the gravinertial field. White uses MHD plasma rules to quantify this local momentum interaction where in comparison Woodward applies condensed matter physics.\n\nBased on the White conjecture, the proposed theoretical device is called a quantum vacuum plasma thruster (QVPT) or Q-thruster. No experiments have been performed to date. Unlike a Mach effect thruster instantaneously exchanging momentum with the distant cosmic matter through the advanced/retarded waves (Wheeler–Feynman absorber theory) of the radiative gravinertial field, Sonny's \"Q-thruster\" would appear to violate momentum conservation, for the thrust would be produced by pushing off virtual \"Q\" particle/antiparticle pairs that would annihilate after they have been pushed on. However, it would not necessarily violate the law of conservation of energy, as it requires an electric current to function, much like any \"standard\" MHD thruster, and cannot produce more kinetic energy than its equivalent net energy input.\n\nWoodward and Fearn showed why the amount of electron-positron virtual pairs of the quantum vacuum, used by White as a virtual plasma propellant, cannot account for thrusts in any isolated, closed electromagnetic system such as the QVPT or the EmDrive.\n\nWoodward's claims in his papers and in space technology conference press releases of a potential breakthrough technology for spaceflight have generated interest in the popular press\nand university news as well as the space news media. Woodward also gave a video interview for the TV show Ancient Aliens, season 6, episode 12. However doubters do exist.\n\n\n"}
