{"id": "4645594", "url": "https://en.wikipedia.org/wiki?curid=4645594", "title": "Advanced Technology Leisure Application Simulator", "text": "Advanced Technology Leisure Application Simulator\n\nThe Advanced Technology Leisure Application Simulator, or ATLAS, is a large hydraulic motion simulator. It was designed, as the name implies, for the theme park industry. The ATLAS is a product of Rediffusion Simulation in Sussex, England, now owned by Thales Group and known as Thales Training & Simulation. Disney filed multiple patents on their variant of the device, including US Utility Patent #5161104 .\n\nThe ATLAS was derived from military flight simulation technology. It uses six hydraulic actuators to provide a broad range of movement.\n\nIn the later half of the 1980s, Walt Disney Imagineering bought and refined this technology for two theme park attractions; Star Tours at Disneyland (and later Disney's Hollywood Studios, Tokyo Disneyland, and Disneyland Paris) and Body Wars at Epcot. The technology was also used in 2016 for the Iron Man Experience at Hong Kong Disneyland. The Disney attractions feature large, 40-person cabins hidden from outside view, arranged lengthwise with four or six simulators per installation. There are four simulators at Disneyland's Star Tours and EPCOT's Body Wars, while the remaining Star Tours installations have six. Body Wars is now defunct and the simulators have been removed from the building in the years since the closure of the Wonders of Life pavilion.\n"}
{"id": "1509102", "url": "https://en.wikipedia.org/wiki?curid=1509102", "title": "Assisted GPS", "text": "Assisted GPS\n\nAssisted GPS or Augmented GPS (abbreviated generally as A-GPS and less commonly as aGPS) is a system that often significantly improves the startup performance—i.e., time-to-first-fix (TTFF)—of a GPS satellite-based positioning system. A-GPS is extensively used with GPS-capable cellular phones, as its development was accelerated by the U.S. FCC's 911 requirement to make cell phone location data available to emergency call dispatchers.\n\nStandalone/self-ruling GPS devices depend solely on information from satellites. A-GPS augments that by using cell tower data to enhance quality and precision when in poor satellite signal conditions. In exceptionally poor signal conditions, for example in urban areas, satellite signals may exhibit multipath propagation where signals skip off structures, or are weakened by meteorological conditions or tree canopy. Some standalone GPS navigators used in poor conditions can't fix a position because of satellite signal fracture and must wait for better satellite reception. A regular GPS unit may need as long as 12.5 minutes (the time needed to download the GPS almanac and ephemerides) to resolve the problem and be able to provide a correct location.\n\nAn assisted GPS system can address these problems by using external data. Utilizing this system can come at a cost to the user. For billing purposes, network providers often count this as a data access, which can cost money, depending on the plan.\n\nTo be precise, A-GPS features depend mostly on an internet network or connection to an ISP (or CNP, in the case of CP/mobile-phone device linked to a cellular network provider data service). A mobile (cell phone, smart phone) device with just an L1 front-end radio receiver and no GPS acquisition, tracking, and positioning engine only works when it has an internet connection to an ISP/CNP, where the position fix is calculated offboard the device itself. It doesn't work in areas with no coverage or internet link (or nearby base transceiver station (BTS) towers, in the case on CNP service coverage area). Without one of those resources, it can't connect to the A-GPS servers usually provided by CNPs. On the other hand, a mobile device with a GPS chipset requires no data connection to capture and process GPS data into a position solution, since it receives data directly from the GPS satellites and is able to calculate a position fix itself. However, the availability of a data connection can provide assistance to improve the performance of the GPS chip on the mobile device. \n\nAssistance falls into two categories:\n\nAs an additional benefit, in mobile station assisted implementations, the amount of processing and software required for a GPS receiver can be reduced by offloading most of the work onto the assistance server.\n\nA typical A-GPS-enabled receiver uses a data connection (Internet or other) to contact the assistance server for aGPS information. If it also has functioning autonomous GPS, it may use standalone GPS, which is sometimes slower on time to first fix, but does not depend on the network, and therefore can work beyond network range and without incurring data-usage fees. Some A-GPS devices do not have the option of falling back to standalone or autonomous GPS.\n\nMany mobile phones combine A-GPS and other location services, including Wi-Fi Positioning System and cell-site multilateration and sometimes a hybrid positioning system.\n\nHigh-Sensitivity GPS is an allied technology that addresses some of these issues in a way that does not require additional infrastructure. However, unlike some forms of A-GPS, high-sensitivity GPS cannot provide a fix instantaneously when the GPS receiver has been off for some time.\n\nStandalone GPS provides first position in approximately 30–40 seconds. A standalone GPS needs orbital information of the satellites to calculate the current position. The data rate of the satellite signal is only 50 bit/s, so downloading orbital information like ephemerides and the almanac directly from satellites typically takes a long time, and if the satellite signals are lost during the acquisition of this information, it is discarded and the standalone system has to start from scratch. In A-GPS, the network operator deploys an A-GPS server, a cache server for GPS data. These A-GPS servers download the orbital information from the satellite and store it in the database. An A-GPS-capable device can connect to these servers and download this information using mobile-network radio bearers such as GSM, CDMA, WCDMA, LTE or even using other radio bearers such as Wi-Fi. Usually the data rate of these bearers is high, hence downloading orbital information takes less time.\n\nA-GPS has two modes of operation:\n\nA-GPS protocols are part of Positioning Protocol defined by two different standardization bodies, 3GPP and Open Mobile Alliance (OMA).\n\nDefined by the OMA to support positioning protocols in Packet Switched Networks. Two generations of User Plane Protocol have been defined.\n\n\nGPS assisted with Indoor Location Technologies\n"}
{"id": "4572695", "url": "https://en.wikipedia.org/wiki?curid=4572695", "title": "Berlin key", "text": "Berlin key\n\nThe Berlin key (also known as, German, \"Schließzwangschlüssel\", or, English, \"forced locking key\") is a key for a type of door lock. It was designed to force people to close and lock their doors (usually a main entrance door or gate leading into a common yard or tenement block). Its particularity comes from the fact that it has two key blades (the part which activates the bolt), one at each end of the key, rather than the usual single blade. After unlocking the lock, the key must be pushed all the way through the lock and retrieved on the other side of the door after it has been closed and locked again. The mechanism makes the retrieval of the key impossible when the door is unlocked. Also, locking an open door is usually not possible.\n\nInvented by the Berliner locksmith Johannes Schweiger, the Berlin key was massively produced by the Albert Kerfin & Co company starting in 1912. With the advent of more recent locking technologies, this kind of lock and key is becoming less common, but it can still be widely found in the tenement buildings of Berlin, Germany. \n\nThe key is subject of the book \"The Berlin Key\" by Science and Technology studies professor Bruno Latour. \n\n"}
{"id": "2681207", "url": "https://en.wikipedia.org/wiki?curid=2681207", "title": "BiteStrip", "text": "BiteStrip\n\nThe BiteStrip is a disposable self-use home test for Sleep Bruxism. Its indications correlate well with comparable indications from formal sleep lab studies.\n\nThe device is a miniature but complete sleep EMG (electromyography) monitor, including two pre-gelled EMG electrodes, an amplifier, a micro-processor-based real time data acquisition and analysis hardware and software, and a permanent chemical display unit. The entire system is integrated on a small piece of lightweight plastic film attached to the user’s cheek. By analyzing the jaw muscles’ EMG waveforms in real time during the night and presenting the result on the built-in display, the device doesn’t need a large data memory, and the downloading and analysis phases, common to all current sleep recorders, are eliminated. Test results indicate the number of bruxing events detected per hour of sleep (bruxing index or BI). This indication appears as permanent numbers encoded onto the electro-chemical display for easy reading. The device itself can serve as the medical record of the test. Self-test letters indicate technical and clinical validity of the study.\n\nSleep Bruxism (SB) is a serious medical disorder, characterized by involuntary grinding and clenching of teeth during sleep. It is often accompanied by unpleasant grinding sounds heard by the bed-partner or roommate. Symptoms include wearing of teeth, temporomandibular joint (TMJ) dysfunction or pain, chewing difficulties, headaches and daytime sleepiness. The prevalence of SB is estimated at 14%-20% in children and 8% in adults. Diagnosis of SB is usually based on clinical examination and patient history. However, none of the signs and symptoms may be considered conclusive. Another alternative has been to send the patient to a sleep lab for an overnight test.\n"}
{"id": "7147897", "url": "https://en.wikipedia.org/wiki?curid=7147897", "title": "CALS (DOD)", "text": "CALS (DOD)\n\nCALS (Continuous Acquisition and Life-cycle Support) is a United States Department of Defense initiative for electronically capturing military documentation and linking related information. \n\nThe initiative has developed a number of standard specifications\n(protocols) for the exchange of electronic data with commercial suppliers. These standards are often referred to as simply \"CALS\". CALS standards have been adopted by several other allied nations. \n\nThe CALS initiative has endorsed IGES and STEP as formats for digital data. \n\nCALS includes standards for electronic data interchange, electronic technical documentation, and guidelines for process improvement. \n\n\nCALS was known formerly as \"Computer-aided Acquisition and Logistic Support\".\n\nSources and references that need formatted properly:\nOffice of the Assistant Director for Telecommunications\nand Information Systems\nHeadquarters Defense Logistics Agency\nCameron Station\nAlexandria, VA 22314 \n\n\nOverview articles\n\n"}
{"id": "498635", "url": "https://en.wikipedia.org/wiki?curid=498635", "title": "Citizen journalism", "text": "Citizen journalism\n\nThe concept of citizen journalism (also known as \"public\", \"participatory\", \"democratic\", \"guerrilla\" or \"street\" journalism) is based upon public citizens \"playing an active role in the process of collecting, reporting, analyzing, and disseminating news and information.\" Similarly, Courtney C. Radsch defines citizen journalism \"as an alternative and activist form of news gathering and reporting that functions outside mainstream media institutions, often as a response to shortcomings in the professional journalistic field, that uses similar journalistic practices but is driven by different objectives and ideals and relies on alternative sources of legitimacy than traditional or mainstream journalism\". Jay Rosen proposes a simpler definition: \"When the people formerly known as the audience employ the press tools they have in their possession to inform one another.\"\n\nCitizen journalism should not be confused with Community journalism or Civic journalism, both of which are practiced by professional journalists; Collaborative journalism which is the practice of professional and non-professional journalists working together; and Social journalism that denotes a digital publication with a hybrid of professional and non-professional journalism.\n\nCitizen journalism is a specific form of both citizen media and user-generated content. By juxtaposing the term \"citizen\", with its attendant qualities of civic-mindedness and social responsibility, with that of \"journalism\", which refers to a particular profession, Courtney C. Radsch argues that this term best describes this particular form of \"online\" and \"digital\" journalism conducted by amateurs, because it underscores the link between the practice of journalism and its relation to the political and public sphere.\n\nNew media technology, such as social networking and media-sharing websites, in addition to the increasing prevalence of cellular telephones, have made citizen journalism more accessible to people worldwide. Recent advances in new media have started to have a profound political impact. Due to the availability of technology, citizens often can report breaking news more quickly than traditional media reporters. Notable examples of citizen journalism reporting from major world events are, the 2010 Haiti earthquake, the Arab Spring, the Occupy Wall Street movement, the 2013 protests in Turkey, the Euromaidan events in Ukraine, and Syrian Civil War and the 2014 Ferguson unrest.\n\nCritics of the phenomenon, including professional journalists and news organizations, claim that citizen journalism is unregulated, too subjective, amateur, and haphazard in quality and coverage.\n\nCitizen journalism, as a form of alternative media, presents a \"radical challenge to the professionalized and institutionalized practices of the mainstream media\".\n\nAccording to Terry Flew, there have been three elements critical to the rise of citizen journalism: open publishing, collaborative editing, and distributed content. Mark Glaser, a freelance journalist who frequently writes on new media issues, said in 2006:\n\nThe idea behind citizen journalism is that people without professional journalism training can use the tools of modern technology and the global distribution of the Internet to create, augment or fact-check media on their own or in collaboration with others. For example, you might write about a city council meeting on your blog or in an online forum. Or you could fact-check a newspaper article from the mainstream media and point out factual errors or bias on your blog. Or you might snap a digital photo of a newsworthy event happening in your town and post it online. Or you might videotape a similar event and post it on a site such as YouTube. The accessibility of online media has also enhanced the interest for journalism among youth and many websites, like 'Far and Wide' a publication focusing on travel and international culture, as well as WorldWeekly a news blog covering a range of topics from world politics to science, are founded and run by students.\n\nIn \"What is Participatory Journalism?\", J. D. Lasica classifies media for citizen journalism into the following types:\n\n\nThe literature of citizen, alternative, and participatory journalism is most often situated in a democratic context and theorized as a response to corporate news media dominated by an economic logic. Some scholars have sought to extend the study of citizen journalism beyond the Western, developed world, including Sylvia Moretzsohn, Courtney C. Radsch, and Clemencia Rodríguez. Radsch, for example, wrote that \"Throughout the Arab world, citizen journalists have emerged as the vanguard of new social movements dedicated to promoting human rights and democratic values.\"\n\nOne criticism of the term \"citizen journalism\" to describe this concept is that the word \"citizen\" has a conterminous relation to the nation-state. The fact that many millions of people are considered stateless and often, are without citizenship (such as refugees or immigrants without papers) limits the concept to those recognised only by governments. Additionally, the global nature of many participatory media initiatives, such as the Independent Media Center, makes talking of journalism in relation to a particular nation-state largely redundant as its production and dissemination do not recognise national boundaries.\nSome additional names given to the concept based on this analysis are, \"grassroots media,\" \"people's media,\" or \"participatory media.\"\n\nAccording to Vincent Campbell, theories of citizenship can be categorized into two core groups: those that consider journalism \"for\" citizenship and those that consider journalism \"as\" citizenship. The classical model of citizenship is the base of the two theories of citizenship. The classical model is rooted in the ideology of informed citizens and places emphasis on the role of journalists rather than on citizens. The classical model has 4 main characteristics: journalists' role of informing citizens; citizens are assumed to be informed if they regularly attend to the news they are supplied with; more informed citizens are more likely to participate; and the more informed citizens participate, the more democratic a state is more likely to be.\n\nThe first characteristic suggesting that the role of journalism is to inform citizens upholds the theory that journalism is \"for\" citizens. One of the main issues with this first theory is that there is a normative judgement surrounding the amount and nature of information that citizens should have as well as what the relationship between the two should be. One branch of journalism \"for\" citizens is the \"monitorial citizen\" coined by Michael Schudson. The \"monitorial citizen\" suggests that citizens appropriately and strategically select what news and information they consume. The \"monitorial citizen\" along with other forms of this ideology conceive individuals as those who do things with information to enact change and citizenship. However, this production of information does not equal to an act of citizenship, but instead an act of journalism. Therefore, citizens and journalists are portrayed as distinctive roles whereas journalism is used by citizens \"for\" citizenship and conversely, journalists serve citizens.\n\nThe second theory considers journalism \"as\" citizenship. This theory focuses on the different aspects of citizen identity and activity and understands citizen journalism as directly constituting citizenship. The term \"liquid citizenship\" coined by Zizi Papacharissi depicts how the lifestyles that individuals engage in allow them to interact with other individuals and organizations, which thus remaps the conceptual periphery of civic, political, and social. This \"liquid citizenship\" allows the interactions and experiences that individuals face to become citizen journalism where they create their own forms of journalism. An alternative approach of journalism as citizenship rests between the distinction between \"dutiful\" citizens and \"actualizing\" citizens. \"Dutiful\" citizens engage in traditional citizenship practices, while \"actualizing\" citizens engage in non-traditional citizenship practices. This alternative approach suggests that \"actualizing\" citizens are less likely to use traditional media and more likely to use online and social media as sources of information, discussion, and participation. Thus, journalism in the form of online and social media practices become a form of citizenship for actualizing citizens.\n\nCriticisms have been made against citizen journalism, especially from among professionals in the field. Citizen journalists are often portrayed as unreliable, biased and untrained – as opposed to professionals who have \"recognition, paid work, unionized labour and behaviour that is often politically neutral and unaffiliated, at least in the claim if not in the actuality\".\nCitizen journalists gather material by being on the streets. Their tools can be narrowed down to a camera, social media and an instinct to start recording whenever something seems newsworthy or out of order. Much of their knowledge regarding the issues that are raised are obtained through their experience as a part of the community.\n\nHowever, some major news reporting agencies, threatened by the speed with which news is reported and delivered by citizen journalism, have launched campaigns to bring in readers and financial support. For example, Bill Johnson, president of Embarcadero Media, which publishes several northern California newspapers, issued an online statement asking readers to subscribe to local newspapers in order to keep them financially solvent. Johnson put special emphasis on the critical role played by local newspapers, which, he argues, \"reflect the values of the residents and businesses, challenge assumptions, and shine a light on our imperfections and aspirations.\"\n\nThe idea that every citizen can engage in acts of journalism has a long history in the United States. The contemporary citizen journalist movement emerged after journalists began to question the predictability of their coverage of events such as the 1988 U.S. presidential election. Those journalists became part of the public, or civic, journalism movement, which sought to counter the erosion of trust in the news media and the widespread disillusionment with politics and civic affairs.\n\nInitially, discussions of public journalism focused on promoting journalism that was \"for the people\" by changing the way professional reporters did their work. According to Leonard Witt, however, early public journalism efforts were \"often part of 'special projects' that were expensive, time-consuming, and episodic. Too often these projects dealt with an issue and moved on. Professional journalists were driving the discussion. They would have the goal of doing a story on welfare-to-work (or the environment, or traffic problems, or the economy), and then they would recruit a cross-section of citizens and chronicle their points of view. Since not all reporters and editors bought into this form of public journalism, and some outright opposed it, reaching out to the people from the newsroom was never an easy task.\" By 2003, in fact, the movement seemed to be petering out, with the Pew Center for Civic Journalism closing its doors.\n\nTraditionally, the term \"citizen journalism\" has had a history of struggle with deliberating on a concise and mutually agreed upon definition. Even today, the term lacks a clear form of conceptualization. Although the term lacks conceptualization, alternative names of the term are unable to comprehensively capture the phenomenon. For example, one of the interchangeable names with \"citizen journalism\" is \"user-generated content\" (UGC). However, the issue with this alternative term is that it eliminates the potential civic virtues of citizen journalism and considers it to be stunted and proprietorial.\n\nWith today's technology the citizen journalist movement has found new life as the average person can capture news and distribute it globally. As Yochai Benkler has noted, \"the capacity to make meaning – to encode and decode humanly meaningful statements – and the capacity to communicate one's meaning around the world, are held by, or readily available to, at least many hundreds of millions of users around the globe.\" Professor Mary-Rose Papandrea, a constitutional law professor at Boston College, notes in her article, \"Citizen Journalism and the Reporter's Privilege,\" that:\n\nIn 1999, activists in Seattle created a response to the WTO meeting being held there. These activists understood the only way they could get into the corporate media was by blocking the streets. Then they realized that a scant 60 seconds of coverage would show them being carted off by the police, but without any context to explain why they were protesting. They knew they had to create an alternative media model.\n\nSince then, the Indymedia movement has experienced exponential growth, and IMCs have been created in more than 200 cities all over the world.\nSimultaneously, journalism \"by the people\" began to flourish, enabled by emerging internet and networking technologies, such as weblogs, chat rooms, message boards, wikis, and mobile computing. A relatively new development is the use of convergent polls, allowing editorials and opinions to be submitted and voted on. Over time, the poll converges on the most broadly accepted editorials and opinions. In South Korea, OhmyNews became popular and commercially successful with the motto, \"Every Citizen is a Reporter.\" Founded by Oh Yeon-ho on February 22, 2000, it has a staff of 40 or more traditional reporters and editors who write about 20% of its content, with the rest coming from other freelance contributors who mostly are ordinary citizens. OhmyNews now has an estimated 50,000 contributors, and has been credited with transforming South Korea's conservative political environment.\n\nIn 2000, The Raven launched a Web television station aimed at participatory journalism, reporting on events in the Daytona Beach area. In 2001, themeparkinsider.com became the first online publication to win a major journalism award for a feature that was reported and written entirely by readers, earning an Online Journalism Award from the Online News Association and Columbia Graduate School of Journalism for its \"Accident Watch\" section, where readers tracked injury accidents at theme parks and shared accident prevention tips.\n\nDuring the 2004 U.S. presidential election, both the Democratic and Republican parties issued press credentials to citizen bloggers covering the convention, marking a new level of influence and credibility for nontraditional journalists. Some bloggers also began \"watchdogging\" the work of conventional journalists, monitoring their work for biases and inaccuracy.\n\nA recent trend in citizen journalism has been the emergence of what blogger Jeff Jarvis terms hyperlocal journalism, as online news sites invite contributions from local residents of their subscription areas, who often report on topics that conventional newspapers tend to ignore. \"We are the traditional journalism model turned upside down,\" explains Mary Lou Fulton, the publisher of the Northwest Voice in Bakersfield, California. \"Instead of being the gatekeeper, telling people that what's important to them 'isn't news', we're just opening up the gates and letting people come on in. We are a better community newspaper for having thousands of readers who serve as the eyes and ears for the Voice, rather than having everything filtered through the views of a small group of reporters and editors.\"\n\nIn June 2009, hundreds of thousands of Iranians took to the streets of Tehran, to protest the election outcome. Through citizen journalism and technological developments, the internet and social media were considered to be large sparks of this movements, although there was a lack of real outcomes politically.\n\nCitizen journalism played a role in the uprisings of the Arab Spring A study of women cyber-activists in several Arab countries found that \"a significant proportion of cyberactivism revolves around influencing the mainstream media agenda, as an increasingly symbiotic relationship between citizen and professional journalism has developed throughout the Arab Spring.\"\n\nOccupy protests were influenced by live interactive media coverage through citizen journalists such as Tim Pool, Jerry Nelson and The Citizen Journals on Facebook.\n\nAccording to Jay Rosen, citizen journalists are \"the people formerly known as the audience,\" who \"\"were\" on the receiving end of a media system that ran one way, in a broadcasting pattern, with high entry fees and a few firms competing to speak very loudly while the rest of the population listened in isolation from one another— and who \"today\" are not in a situation like that \"at all\". ... The people formerly known as the audience are simply the public made realer, less fictional, more able, less predictable.\"\n\nAbraham Zapruder, who filmed the assassination of President John Fitzgerald Kennedy with a home-movie camera, is sometimes presented as an ancestor to citizen journalists. Egyptian citizen Wael Abbas was awarded several international reporting prizes for his blog Misr Digital (Digital Egypt) and a video he publicized of two policemen beating a bus driver helped lead to their conviction.\n\nPublic Journalism is now being explored via new media, such as the use of mobile telephones. Mobile telephones have the potential to transform reporting and places the power of reporting in the hands of the public. Mobile telephony provides low-cost options for people to set up news operations.\n\nDuring 9/11 many eyewitness accounts of the terrorist attacks on the World Trade Center came from citizen journalists. Images and stories from citizen journalists close to the World Trade Center offered content that played a major role in the story.\n\nIn 2004, when the 9.1-magnitude underwater earthquake caused a huge tsunami in Banda Aceh Indonesia and across the Indian Ocean, a weblog-based virtual network of previously unrelated bloggers emerged that covered the news in real-time, and became a vital source for the traditional media for the first week after the tsunami.\nA large amount of news footage from many people who experienced the tsunami was widely broadcast,\nas well as a good deal of \"on the scene\" citizen reporting and blogger analysis that was subsequently picked up by the major media outlets worldwide.\nSubsequent to the citizen journalism coverage of the disaster and aftermath, researchers have suggested that citizen journalists may, in fact, play a critical role in the disaster warning system itself, potentially with higher reliability than the networks of tsunami warning equipment based on technology alone which then require interpretation by disinterested third parties.\n\nThe microblog Twitter played an important role during the 2009 Iranian election protests, after foreign journalists had effectively been \"barred from reporting\". Twitter delayed scheduled maintenance during the protests that would have shut down coverage in Iran due to the role it played in public communication.\n\nSometimes citizen journalists are, at the same time, bloggers and after some time they often become professional journalists, just as Paweł Rogaliński, a prized Polish blogger and journalist did.\n\nToday, individually produced citizen journalism exists in the form of social media platforms such as blogs, YouTube, and Twitter. These social media platforms encourage and facilitate engagement with other citizens who participate in creating content through commenting, liking, linking, and sharing. This practice is considered to be the 21st century version of individualized citizen journalism. The first wave of this type of citizen journalism came about in the form of amateur news bloggers. These bloggers often created content and narrative that challenged and critiqued the mainstream news outlets. The majority of the content produced by these amateur news bloggers was not actually original content, but curated information that was primarily monitored and edited by these various bloggers. However, recently there has been a decline in the amateur news blogger due to social media platforms that are much easier to run and maintain. These social media platforms allow individuals to easily share and create and content.\n\nWikimedia Foundation hosts a participatory journalism web site, Wikinews. The website allows contributors to write news which undergo a peer review prior to publications in some language editions (English, German, Russian) but not in others (Norwegian).\n\nCitizen journalists also may be activists within the communities they write about. This has drawn some criticism from traditional media institutions such as \"The New York Times\", which have accused proponents of public journalism of abandoning the traditional goal of objectivity. Many traditional journalists view citizen journalism with some skepticism, believing that only trained journalists can understand the exactitude and ethics involved in reporting news. See, e.g., Nicholas Lemann, Vincent Maher, and Tom Grubisich.\n\nAn academic paper by Vincent Maher, the head of the New Media Lab at Rhodes University, outlined several weaknesses in the claims made by citizen journalists, in terms of the \"three deadly E's\", referring to ethics, economics, and epistemology.\n\nAn analysis by language and linguistics professor, Patricia Bou-Franch, found that some citizen journalists resorted to abuse-sustaining discourses naturalizing violence against women. She found that these discourses were then challenged by others who questioned the gendered ideologies of male violence against women.\n\nAn article in 2005 by Tom Grubisich reviewed ten new citizen journalism sites and found many of them lacking in quality and content. Grubisich followed up a year later with, \"Potemkin Village Redux.\" He found that the best sites had improved editorially and were even nearing profitability, but only by not expensing editorial costs. Also according to the article, the sites with the weakest editorial content were able to expand aggressively because they had stronger financial resources.\n\nAnother article published on Pressthink examined Backfence, a citizen journalism site with three initial locations in the D.C. area, which reveals that the site has only attracted limited citizen contributions. The author concludes that, \"in fact, clicking through Backfence's pages feels like frontier land -– remote, often lonely, zoned for people but not home to any. The site recently launched for Arlington, Virginia. However, without more settlers, Backfence may wind up creating more ghost towns.\"\n\nDavid Simon, a former Baltimore Sun reporter and writer-producer of the popular television series, \"The Wire,\" criticized the concept of citizen journalism—claiming that unpaid bloggers who write as a hobby cannot replace trained, professional, seasoned journalists.\nAn editorial published by \"The Digital Journalist\" web magazine expressed a similar position, advocating to abolish the term \"citizen journalist\", and replacing it with \"citizen news gatherer\".\nWhile the fact that citizen journalists can report in real time and are not subject to oversight opens them to criticism about the accuracy of their reporting, news stories presented by mainstream media also misreport facts occasionally that are reported correctly by citizen journalists. As low as 32% of the American population have a fair amount of trust in the media.\n\nJournalism has been affected significantly due to citizen journalism. This is because citizen journalism allows people to post as much content as they want, whenever they want. In order to stay competitive, traditional news sources are forcing their journalist to compete. This means that journalist now have to write, edit and add pictures into their content and they must do so at a rapid pace, as it is perceived by news companies that it's essential for journalist to produce content at the same rate that citizens can post content on the internet. This is hard though, as many news companies are facing budget cuts and cannot afford to pay journalists the proper amount for the amount of work they do. Despite the uncertainties of a job in journalism and rising tuition costs there has been a 35% increase in journalism majors throughout the past few years according to Astra Taylor in her book The People's Platform.\n\nEdward Greenberg, a New York City litigator, notes higher vulnerability of unprofessional journalists in court compared to the professional ones:\nThe view stated above does not mean that professional journalists are fully protected by shield laws. In the 1972 Branzburg v. Hayes case the Supreme Court of the United States invalidated the use of the First Amendment as a defense for reporters summoned to testify before a grand jury. In 2005, the reporter's privilege of Judith Miller and Matthew Cooper was rejected by the appellate court.\n\nCitizen journalism has largely increased during the last decade of the twentieth century and throughout the twenty-first century. This rise of participation can be associated with the creation of the internet which introduced new ways in communicating and engaging news. Due to this shift in technology, individuals were able to access more news than previously and at a much faster rate. This larger quantity also made it so there was a larger variety of sources which people were able to consume media and news.\n\nNatalie Fenton discusses the role of citizen journalism within the digital age and has three characteristics associated with the topic: speed and space, multiplicity and poly-centrality and interactivity and participation. These characteristics were due to the invention of the internet, which, made way for amateur and citizen journalist to make a name for themselves within the industry. This was happening throughout the 1990s, however, once the mid 2000s began, the introduction of technologies such as the smartphone increased the ability to access the internet and made it so that individuals were able to use it globally and on the go. With these technological advancements, individuals were able to participate in journalism, like never before. Pictures or videos could be uploaded online in a matter of minutes and this paved the way for social media to grow as a strong producer in the industry.\n\nIn 2017, there are a number of different social media platforms through which people can access for their news. Many large corporations have even started to shift their focus onto internet sites, such as Facebook or YouTube and this has also made it easier for the existence of Alternative Media groups to exist. This transition into a digital realm of media has created many new possibilities for people to participate in journalism and it is due to the technological advancements such as the internet and smartphones.\n\nAs society continues to move forward in a digital age new possibilities in the realm of technology emerge and can be associated with the journalism industry. New devices such as Virtual Reality, open new avenues, which media companies and people will be able to participate with journalism. As society continues to move towards embracing technology as part of their lives, citizen journalism should increase in accessibility and participation.\n\nDan Gillmor, the former technology columnist for the \"San Jose Mercury News\", is one of the foremost proponents of citizen journalism and founded a nonprofit, the Center for Citizen Media, to help promote it.\n\nThe Canadian Broadcasting Corporation's French-language television network also has organized a weekly public affairs program called, \"5 sur 5\", which has been organizing and promoting citizen-based journalism since 2001. On the program, viewers submit questions on a wide variety of topics, and they, accompanied by staff journalists, get to interview experts to obtain answers to their questions.\n\nJay Rosen, a journalism professor at New York University, was one of public journalism's earliest proponents. From 1993 to 1997, he directed the \"Project on Public Life and the Press\", funded by the Knight Foundation and housed at NYU. He also currently runs the PressThink weblog.\n\nProfessor Charles Nesson, William F. Weld Professor of Law at Harvard Law School and the founder of the Berkman Center for Internet & Society, chairs the Advisory Board for Jamaican citizen journalism startup On the Ground News Reports.\n\nOne of the leading proponents for citizen journalism in Australia is Margo Kingston, author and former political journalist for the \"Sydney Morning Herald\". Kingston launched one of the world's first mainstream citizen journalism platforms, Webdiary, in 2000, well before \"The New York Times\", \"The Washington Post\" and \"The Guardian\". Kingston resigned from Webdiary in 2005 but the site continues and has been preserved in Pandora, Australia's National Web Archive. After a period of retirement, Kingston returned to citizen journalism in 2013 by co-publishing a new site No Fibs. It was on this site that Kingston published an exclusive story that the Australian Prime Minister, Tony Abbott, had inappropriately claimed expenses for promoting his book.\n\nIn March 2014, blogger and novelist James Wesley Rawles launched a web site that provides free press credentials for citizen journalists called the Constitution First Amendment Press Association (CFAPA). According to David Sheets of the Society for Professional Journalists, Rawles keeps no records on who gets these credentials.\n\nMaurice Ali, a citizen journalist from Canada, founded one of the first international citizen journalist associations called the International Association of Independent Journalists Inc. (IAIJ) in 2003. The association through its President (Maurice Ali) have published studies and articles on citizen journalism, attended and spoken at UNESCO and United Nations events as advocates of citizen journalism worldwide.\n\n"}
{"id": "10371312", "url": "https://en.wikipedia.org/wiki?curid=10371312", "title": "Clymer repair manual", "text": "Clymer repair manual\n\nClymer repair manuals are vehicle repair manuals that often focus on powersport vehicles such as motorcycles, all-terrain vehicles, personal water craft and snowmobiles. Clymer also has several books dedicated to small engines and \"outdoor power equipment\" such as leaf blowers, chainsaws and other lawn and garden power equipment.\n\nClymer repair manuals are named after their creator Floyd Clymer, who is described in the Motorcycle Hall of Fame as:\n\n\"a pioneer in the sport of motorcycling. He was a racer, a motorcycle dealer and distributor, a magazine publisher, a racing promoter, an author and a motorcycle manufacturer.\"\nClymer repair manuals are categorized as an aftermarket product or non-OEM. Unlike OEM manuals, Clymer repair manuals are written specifically for the do it yourself as well as the professional and experienced mechanic. OEM manuals are often designed for a professional technician, who often has at their disposal an array of specialized tools, equipment and knowledge.\n\nOne valuable step in the creation process of a Clymer repair manuals is the complete disassembly and reassembly of the machine. This helps the writers for Clymer provide easy-to-follow instructions that allow the manual user to safely and efficiently service and repair their machine. This high level of detail sets Clymer apart from its factory \"OEM\" counterparts.\n\nIn 2013, Haynes Publishing Group acquired Clymer repair manuals from Penton Media.\n\nClymer currently has over three hundred repair manuals that cover thousands of models. Some of the most popular models are the Honda TRX ATVs, International Harvester Farm Tractors, BMW K1200 Series, Harley-Davidson FLH & FLT Twin Cam 88 & 103 models and the MerCruiser Stern Drive Marine Engines.\n\nHere are some of the manufacturers covered in the Clymer library:\n\n\n\n"}
{"id": "25678313", "url": "https://en.wikipedia.org/wiki?curid=25678313", "title": "Context-aware services", "text": "Context-aware services\n\nContext-aware services is a computing technology which incorporates information about the current location of a mobile user to provide more relevant services to the user. An example of a context-aware service could be a real-time traffic update or even a live video feed of a planned route for a motor vehicle user. Context can refer to real-world characteristics, such as temperature, time or location. This information can be updated by the user (manually) or from communication with other devices and applications or sensors on the mobile device.\n"}
{"id": "16764104", "url": "https://en.wikipedia.org/wiki?curid=16764104", "title": "Courreges ZOOOP", "text": "Courreges ZOOOP\n\nThe Zooop is a three-seat electric car produced by the Paris-based fashion house Maison de Courrèges.\n\nThe Courreges Zooop performance is 150 kW and weighing just 690 kilograms, also features a range of 450 kilometers. Remarkably, the car is not produced for a car manufacturer, but it is produced by the famous fashion design house Maison de Courrèges, which peculiarly did scarce promotion for its new model outside its native country.\n"}
{"id": "788203", "url": "https://en.wikipedia.org/wiki?curid=788203", "title": "Cryptography standards", "text": "Cryptography standards\n\nThere are a number of standards related to cryptography. Standard algorithms and protocols provide a focus for study; standards for popular applications attract a large amount of cryptanalysis.\n\n\n\n\n\n\n\n\n\n"}
{"id": "24023788", "url": "https://en.wikipedia.org/wiki?curid=24023788", "title": "Crystatech", "text": "Crystatech\n\nCrystaTech Inc. is a supplier of process technology to the energy industry. CrystaTech commercializes the patented Crystasulf process. CrystaSulf is the first commercially available product to provide low cost hydrogen sulfide (HS) removal from gas streams.\n\nThe company was founded in 1999 and is financially backed by the Gas Technology Institute and major energy companies through sponsored clean energy technology development. The corporate office is located in Austin, Texas.\n\nCrystaTech is a member of the Gas Processors Suppliers Association.\n\nRegional offices are in Alberta, Canada and Houston, Texas. All early stage R&D takes place at the Gas Technology Institute in Des Plaines, Illinois. Representative customers include Total, Petrobank Energy and Resources Ltd., Queensland Energy Resources, U.S. Department of Energy, Luminant, and American Electric Power.\n\nKey People\n\n\n\n"}
{"id": "16089380", "url": "https://en.wikipedia.org/wiki?curid=16089380", "title": "Cut-off factor", "text": "Cut-off factor\n\nCut-off factor (AKA \"cut-off length\") is a factor used to calculate the length of a hose cut to achieve the desired overall length of hose plus fittings. It is commonly seen in hydraulic hose and fitting specifications. The cut-off factor is specific to a particular hose fitting.\n\nThe formula used in calculating the optimum overall length is:\n\nformula_1.\n\nIn this formula, C1 represents the cut-off factor of the first hose end and C2 represents the cut-off factor of the second hose end.\n"}
{"id": "195113", "url": "https://en.wikipedia.org/wiki?curid=195113", "title": "Digital divide", "text": "Digital divide\n\nA digital divide is an economic and social inequality with regard to access to, use of, or impact of information and communication technologies (ICT). The divide within countries (such as the digital divide in the United States) may refer to inequalities between individuals, households, businesses, or geographic areas, usually at different socioeconomic levels or other demographic categories. The divide between differing countries or regions of the world is referred to as the global digital divide, examining this technological gap between developing and developed countries on an international scale.\n\nThe term \"digital divide\" describes a gap in terms of access to and usage of information and communication technology. It was traditionally considered to be a question of having or not having access, but with a global mobile phone penetration of over 95%, it is becoming a relative inequality between those who have more and less bandwidth and more or fewer skills. Conceptualizations of the digital divide have been described as \"who, with which characteristics, connects how to what\":\nDifferent authors focus on different aspects, which leads to a large variety of definitions of the digital divide. \"For example, counting with only 3 different choices of subjects (individuals, organizations, or countries), each with 4 characteristics (age, wealth, geography, sector), distinguishing between 3 levels of digital adoption (access, actual usage and effective adoption), and 6 types of technologies (fixed phone, mobile... Internet...), already results in 3x4x3x6 = 216 different ways to define the digital divide. Each one of them seems equally reasonable and depends on the objective pursued by the analyst\".\nThe \"digital divide\" is also referred to by a variety of other terms which have similar meanings, though may have a slightly different emphasis: digital inclusion, digital participation, basic digital skills, media literacy and digital accessibility.\n\nThe National Digital Inclusion Alliance, a US-based nonprofit organization, has found the term \"digital divide\" to be problematic, since there are a multiplicity of divides. Instead, they chosen to use the term \"digital inclusion\", providing a definition:\nDigital Inclusion refers to the activities necessary to ensure that all individuals and communities, including the most disadvantaged, have access to and use of Information and Communication Technologies (ICTs). This includes 5 elements: 1) affordable, robust broadband internet service; 2) internet-enabled devices that meet the needs of the user; 3) access to digital literacy training; 4) quality technical support; and 5) applications and online content designed to enable and encourage self-sufficiency, participation and collaboration.\n\nThe infrastructure by which individuals, households, businesses, and communities connect to the Internet address the physical mediums that people use to connect to the Internet such as desktop computers, laptops, basic mobile phones or smartphones, iPods or other MP3 players, gaming consoles such as Xbox or PlayStation, electronic book readers, and tablets such as iPads.\n\nTraditionally the nature of the divide has been measured in terms of the existing numbers of subscriptions and digital devices. Given the increasing number of such devices, some have concluded that the digital divide among individuals has increasingly been closing as the result of a natural and almost automatic process. Others point to persistent lower levels of connectivity among women, racial and ethnic minorities, people with lower incomes, rural residents, and less educated people as evidence that addressing inequalities in access to and use of the medium will require much more than the passing of time. Recent studies have measured the digital divide not in terms of technological devices, but in terms of the existing bandwidth per individual (in kbit/s per capita). \n\nAs shown in the Figure on the side, the digital divide in kbit/s is not monotonically decreasing, but re-opens up with each new innovation. For example, \"the massive diffusion of narrow-band Internet and mobile phones during the late 1990s\" increased digital inequality, as well as \"the initial introduction of broadband DSL and cable modems during 2003–2004 increased levels of inequality\". This is because a new kind of connectivity is never introduced instantaneously and uniformly to society as a whole at once, but diffuses slowly through social networks. As shown by the Figure, during the mid-2000s, communication capacity was more unequally distributed than during the late 1980s, when only fixed-line phones existed. The most recent increase in digital equality stems from the massive diffusion of the latest digital innovations (i.e. fixed and mobile broadband infrastructures, e.g. 3G and fiber optics FTTH).\nMeasurement methodologies of the digital divide, and more specifically an Integrated Iterative Approach General Framework (Integrated Contextual Iterative Approach – ICI) and the digital divide modeling theory under measurement model DDG (Digital Divide Gap) are used to analyze the gap existing between developed and developing countries, and the gap among the 27 members-states of the European Union.\n\nInstead of tracking various kinds of digital divides among fixed and mobile phones, narrow- and broadband Internet, digital TV, etc., it has recently been suggested to simply measure the amount of kbit/s per actor. This approach has shown that the digital divide in kbit/s per capita is actually widening in relative terms: \"While the average inhabitant of the developed world counted with some 40 kbit/s more than the average member of the information society in developing countries in 2001, this gap grew to over 3 Mbit/s per capita in 2010.\" \n\nThe upper graph of the Figure on the side shows that the divide between developed and developing countries has been diminishing when measured in terms of subscriptions per capita. In 2001, fixed-line telecommunication penetration reached 70% of society in developed OECD countries and 10% of the developing world. This resulted in a ratio of 7 to 1 (divide in relative terms) or a difference of 60% (divide in measured in absolute terms). During the next decade, fixed-line penetration stayed almost constant in OECD countries (at 70%), while the rest of the world started a catch-up, closing the divide to a ratio of 3.5 to 1. The lower graph shows the divide not in terms of ICT devices, but in terms of kbit/s per inhabitant. While the average member of developed countries counted with 29 kbit/s more than a person in developing countries in 2001, this difference got multiplied by a factor of one thousand (to a difference of 2900 kbit/s). In relative terms, the fixed-line capacity divide was even worse during the introduction of broadband Internet at the middle of the first decade of the 2000s, when the OECD counted with 20 times more capacity per capita than the rest of the world. This shows the importance of measuring the divide in terms of kbit/s, and not merely to count devices. The International Telecommunications Union concludes that \"the bit becomes a unifying variable enabling comparisons and aggregations across different kinds of communication technologies\".\n\nHowever, research shows that the digital divide is more than just an access issue and cannot be alleviated merely by providing the necessary equipment. There are at least three factors at play: information accessibility, information utilization and information receptiveness. More than just accessibility, individuals need to know how to make use of the information and communication tools once they exist within a community. Information professionals have the ability to help bridge the gap by providing reference and information services to help individuals learn and utilize the technologies to which they do have access, regardless of the economic status of the individual seeking help.\n\nInternet connectivity can be utilized at a variety of locations such as homes, offices, schools, libraries, public spaces, Internet cafe and others. There are also varying levels of connectivity in rural, suburban, and urban areas.\n\nCommon Sense Media, a nonprofit group based in San Francisco, surveyed almost 1,400 parents and reported in 2011 that 47 percent of families with incomes more than $75,000 had downloaded apps for their children, while only 14 percent of families earning less than $30,000 had done so.\n\nThe gap in a digital divide may exist for a number of reasons. Obtaining access to ICTs and using them actively has been linked to a number of demographic and socio-economic characteristics: among them income, education, race, gender, geographic location (urban-rural), age, skills, awareness, political, cultural and psychological attitudes. Multiple regression analysis across countries has shown that income levels and educational attainment are identified as providing the most powerful explanatory variables for ICT access and usage. Evidence was found that Caucasians are much more likely than non-Caucasians to own a computer as well as have access to the Internet in their homes. As for geographic location, people living in urban centers have more access and show more usage of computer services than those in rural areas. Gender was previously thought to provide an explanation for the digital divide, many thinking ICT were male gendered, but controlled statistical analysis has shown that income, education and employment act as confounding variables and that women with the same level of income, education and employment actually embrace ICT more than men (see Women and ICT4D). However, each nation has its own set of causes or the digital divide. For example, the digital divide in Germany is unique because it is not largely due to difference in quality of infrastructure.\n\nOne telling fact is that \"as income rises so does Internet use ...\", strongly suggesting that the digital divide persists at least in part due to income disparities. Most commonly, a digital divide stems from poverty and the economic barriers that limit resources and prevent people from obtaining or otherwise using newer technologies.\n\nIn research, while each explanation is examined, others must be controlled in order to eliminate interaction effects or mediating variables, but these explanations are meant to stand as general trends, not direct causes. Each component can be looked at from different angles, which leads to a myriad of ways to look at (or define) the digital divide. For example, measurements for the intensity of usage, such as incidence and frequency, vary by study. Some report usage as access to Internet and ICTs while others report usage as having previously connected to the Internet. Some studies focus on specific technologies, others on a combination (such as Infostate, proposed by Orbicom-UNESCO, the Digital Opportunity Index, or ITU's ICT Development Index). Based on different answers to the questions of who, with which kinds of characteristics, connects how and why, to what there are hundreds of alternatives ways to define the digital divide. \"The new consensus recognizes that the key question is not how to connect people to a specific network through a specific device, but how to extend the expected gains from new ICTs\". In short, the desired impact and \"the end justifies the definition\" of the digital divide.\n\nDuring the mid-1990s the US Department of Commerce, National Telecommunications & Information Administration (NTIA) began publishing reports about the Internet and access to and usage of the resource. The first of three reports is entitled \"Falling Through the Net: A Survey of the ‘Have Nots’ in Rural and Urban America\" (1995), the second is \"Falling Through the Net II: New Data on the Digital Divide\" (1998), and the final report \"Falling Through the Net: Defining the Digital Divide\" (1999). The NTIA’s final report attempted to clearly define the term digital divide; \"the digital divide—the divide between those with access to new technologies and those without—is now one of America's leading economic and civil rights issues. This report will help clarify which Americans are falling further behind, so that we can take concrete steps to redress this gap.\" Since the introduction of the NTIA reports, much of the early, relevant literature began to reference the NTIA’s digital divide definition. The digital divide is commonly defined as being between the \"haves\" and \"have-nots.\"\n\nThe Facebook Divide, a concept derived from the \"digital divide\", is the phenomenon with regard to access to, use of, or impact of Facebook on individual society and among societies. It is suggested at the International Conference on Management Practices for the New Economy (ICMAPRANE-17) on February 10–11, 2017. Additional concepts of Facebook Native and Facebook Immigrants are suggested at the conference. The Facebook Divide, Facebook native, Facebook immigrants, and Facebook left-behind are concepts for social and business management research. Facebook Immigrants are utilizing Facebook for their accumulation of both bonding and bridging social capital. These Facebook Native, Facebook Immigrants, and Facebook left-behind induced the situation of Facebook inequality. In February 2018, the Facebook Divide Index was introduced at the ICMAPRANE conference in Noida, India, to illustrate the Facebook Divide phenomenon.\n\nOvercoming the divide \n\nAn individual must be able to connect in order to achieve enhancement of social and cultural capital as well as achieve mass economic gains in productivity. Therefore, access is a necessary (but not sufficient) condition for overcoming the digital divide. Access to ICT meets significant challenges that stem from income restrictions. The borderline between ICT as a necessity good and ICT as a luxury good is roughly around the \"magical number\" of US$10 per person per month, or US$120 per year, which means that people consider ICT expenditure of US$120 per year as a basic necessity. Since more than 40% of the world population lives on less than US$2 per day, and around 20% live on less than US$1 per day (or less than US$365 per year), these income segments would have to spend one third of their income on ICT (120/365 = 33%). The global average of ICT spending is at a mere 3% of income. Potential solutions include driving down the costs of ICT, which includes low cost technologies and shared access through Telecentres.\n\nFurthermore, even though individuals might be capable of accessing the Internet, many are thwarted by barriers to entry such as a lack of means to infrastructure or the inability to comprehend the information that the Internet provides. Lack of adequate infrastructure and lack of knowledge are two major obstacles that impede mass connectivity. These barriers limit individuals' capabilities in what they can do and what they can achieve in accessing technology. Some individuals have the ability to connect, but they do not have the knowledge to use what information ICTs and Internet technologies provide them. This leads to a focus on capabilities and skills, as well as awareness to move from mere access to effective usage of ICT.\n\nThe United Nations is aiming to raise awareness of the divide by way of the World Information Society Day which has taken place yearly since May 17, 2006. It also set up the Information and Communications Technology (ICT) Task Force in November 2001. Later UN initiatives in this area are the World Summit on the Information Society, which was set up in 2003, and the Internet Governance Forum, set up in 2006.\n\nIn the year 2000, the United Nations Volunteers (UNV) programme launched its Online Volunteering service, which uses ICT as a vehicle for and in support of volunteering. It constitutes an example of a volunteering initiative that effectively contributes to bridge the digital divide. ICT-enabled volunteering has a clear added value for development. If more people collaborate online with more development institutions and initiatives, this will imply an increase in person-hours dedicated to development cooperation at essentially no additional cost. This is the most visible effect of online volunteering for human development.\n\nSocial media websites serve as both manifestations of and means by which to combat the digital divide. The former describes phenomena such as the divided users demographics that make up sites such as Facebook and Myspace or Word Press and Tumblr. Each of these sites host thriving communities that engage with otherwise marginalized populations. An example of this is the large online community devoted to Afrofuturism, a discourse that critiques dominant structures of power by merging themes of science fiction and blackness. Social media brings together minds that may not otherwise meet, allowing for the free exchange of ideas and empowerment of marginalized discourses.\n\nAttempts to bridge the digital divide include a program developed in Durban, South Africa, where very low access to technology and a lack of documented cultural heritage has motivated the creation of an \"online indigenous digital library as part of public library services.\" This project has the potential to narrow the digital divide by not only giving the people of the Durban area access to this digital resource, but also by incorporating the community members into the process of creating it.\n\nTo address the divide The Gates Foundation began the Gates Library Initiative. The Gates Foundation focused on providing more than just access, they placed computers and provided training in libraries. In this manner if users began to struggle while using a computer, the user was in a setting where assistance and guidance was available. Further, the Gates Library Initiative was \"modeled on the old-fashioned life preserver: The support needs to be around you to keep you afloat.\"\n\nIn nations where poverty compounds effects of the digital divide, programs are emerging to counter those trends. Prior conditions in Kenya—lack of funding, language and technology illiteracy contributed to an overall lack of computer skills and educational advancement for those citizens. This slowly began to change when foreign investment began. In the early 2000s, The Carnegie Foundation funded a revitalization project through the Kenya National Library Service (KNLS). Those resources enabled public libraries to provide information and communication technologies (ICT) to their patrons. In 2012, public libraries in the Busia and Kiberia communities introduced technology resources to supplement curriculum for primary schools. By 2013, the program expanded into ten schools.\n\nCommunity Informatics (CI) provides a somewhat different approach to addressing the digital divide by focusing on issues of \"use\" rather than simply \"access\". CI is concerned with ensuring the opportunity not only for ICT access at the community level but also, according to Michael Gurstein, that the means for the \"effective use\" of ICTs for community betterment and empowerment are available. Gurstein has also extended the discussion of the digital divide to include issues around access to and the use of \"open data\" and coined the term \"data divide\" to refer to this issue area.\n\nOnce an individual is connected, Internet connectivity and ICTs can enhance his or her future social and cultural capital. Social capital is acquired through repeated interactions with other individuals or groups of individuals. Connecting to the Internet creates another set of means by which to achieve repeated interactions. ICTs and Internet connectivity enable repeated interactions through access to social networks, chat rooms, and gaming sites. Once an individual has access to connectivity, obtains infrastructure by which to connect, and can understand and use the information that ICTs and connectivity provide, that individual is capable of becoming a \"digital citizen\".\n\nIn the United States, research provided by Sungard Availability Services notes a direct correlation between a company's access to technological advancements and its overall success in bolstering the economy. The study, which includes over 2,000 IT executives and staff officers, indicates that 69 percent of employees feel they do not have access to sufficient technology in order to make their jobs easier, while 63 percent of them believe the lack of technological mechanisms hinders their ability to develop new work skills. Additional analysis provides more evidence to show how the digital divide also affects the economy in places all over the world. A BCG Report suggests that in countries like Sweden, Switzerland, and the U.K., the digital connection among communities is made easier, allowing for their populations to obtain a much larger share of the economies via digital business. In fact, in these places, populations hold shares approximately 2.5 percentage points higher. During a meeting with the United Nations a Bangladesh representative expressed his concern that poor and undeveloped countries would be left behind due to a lack of funds to bridge the digital gap.\n\nThe digital divide also impacts children's ability to learn and grow in low-income school districts. Without Internet access, students are unable to cultivate necessary tech skills in order to understand today's dynamic economy. Federal Communication Commission's Broadband Task Force created a report showing that about 70% of teachers give students homework that demand access to broadband. Even more, approximately 65% of young scholars use the Internet at home to complete assignments as well as connect with teachers and other students via discussion boards and shared files. A recent study indicates that practically 50% of students say that they are unable to finish their homework due to an inability to either connect to the Internet, or in some cases, find a computer. This has led to a new revelation: 42% of students say they received a lower grade because of this disadvantage. Finally, according to research conducted by the Center for American Progress, \"if the United States were able to close the educational achievement gaps between native-born white children and black and Hispanic children, the U.S. economy would be 5.8 percent—or nearly $2.3 trillion—larger in 2050\".\n\nFurthermore, according to the 2012 Pew Report \"Digital Differences\", a mere 62% of households who make less than $30,000 a year use the Internet, while 90% of those making between $50,000 and $75,000 had access. Studies also show that only 51% of Hispanics and 49% of African Americans have high-speed Internet at home. This is compared to the 66% of Caucasians that too have high-speed Internet in their households. Overall, 10% of all Americans don't have access to high-speed Internet, an equivalent of almost 34 million people. Supplemented reports from the Guardian demonstrate the global effects of limiting technological developments in poorer nations, rather than simply the effects in the United States. Their study shows that the rapid digital expansion excludes those who find themselves in the lower class. 60% of the world's population, almost 4 billion people, have no access to the Internet and are thus left worse off.\n\nSince gender, age, racial, income, and educational gaps in the digital divide have lessened compared to past levels, some researchers suggest that the digital divide is shifting from a gap in access and connectivity to ICTs to a knowledge divide. A knowledge divide concerning technology presents the possibility that the gap has moved beyond access and having the resources to connect to ICTs to interpreting and understanding information presented once connected.\n\nThe second-level digital divide, also referred to as the production gap, describes the gap that separates the consumers of content on the Internet from the producers of content. As the technological digital divide is decreasing between those with access to the Internet and those without, the meaning of the term digital divide is evolving. Previously, digital divide research has focused on accessibility to the Internet and Internet consumption. However, with more and more of the population with access to the Internet, researchers are examining how people use the Internet to create content and what impact socioeconomics are having on user behavior.\nNew applications have made it possible for anyone with a computer and an Internet connection to be a creator of content, yet the majority of user generated content available widely on the Internet, like public blogs, is created by a small portion of the Internet using population. Web 2.0 technologies like Facebook, YouTube, Twitter, and Blogs enable users to participate online and create content without having to understand how the technology actually works, leading to an ever-increasing digital divide between those who have the skills and understanding to interact more fully with the technology and those who are passive consumers of it. Many are only nominal content creators through the use of Web 2.0, posting photos and status updates on Facebook, but not truly interacting with the technology.\n\nSome of the reasons for this production gap include material factors like the type of Internet connection one has and the frequency of access to the Internet. The more frequently a person has access to the Internet and the faster the connection, the more opportunities they have to gain the technology skills and the more time they have to be creative.\n\nOther reasons include cultural factors often associated with class and socioeconomic status. Users of lower socioeconomic status are less likely to participate in content creation due to disadvantages in education and lack of the necessary free time for the work involved in blog or web site creation and maintenance. Additionally, there is evidence to support the existence of the second-level digital divide at the K-12 level based on how educators' use technology for instruction. Schools' economic factors have been found to explain variation in how teachers use technology to promote higher-order thinking skills.\n\nThe global digital divide describes global disparities, primarily between developed and developing countries, in regards to access to computing and information resources such as the Internet and the opportunities derived from such access. As with a smaller unit of analysis, this gap describes an inequality that exists, referencing a global scale.\n\nThe Internet is expanding very quickly, and not all countries—especially developing countries—are able to keep up with the constant changes. The term \"digital divide\" doesn't necessarily mean that someone doesn’t have technology; it could mean that there is simply a difference in technology. These differences can refer to, for example, high-quality computers, fast Internet, technical assistance, or telephone services. The difference between all of these is also considered a gap.\n\nIn fact, there is a large inequality worldwide in terms of the distribution of installed telecommunication bandwidth. In 2014 only 3 countries (China, US, Japan) host 50% of the globally installed bandwidth potential (see pie-chart Figure on the right). This concentration is not new, as historically only 10 countries have hosted 70–75% of the global telecommunication capacity (see Figure). The U.S. lost its global leadership in terms of installed bandwidth in 2011, being replaced by China, which hosts more than twice as much national bandwidth potential in 2014 (29% versus 13% of the global total).\n\nThe global digital divide is a special case of the digital divide, the focus is set on the fact that \"Internet has developed unevenly throughout the world\" causing some countries to fall behind in technology, education, labor, democracy, and tourism. The concept of the digital divide was originally popularized in regard to the disparity in Internet access between rural and urban areas of the United States of America; the \"global\" digital divide mirrors this disparity on an international scale.\n\nThe global digital divide also contributes to the inequality of access to goods and services available through technology. Computers and the Internet provide users with improved education, which can lead to higher wages; the people living in nations with limited access are therefore disadvantaged. This global divide is often characterized as falling along what is sometimes called the north-south divide of \"northern\" wealthier nations and \"southern\" poorer ones.\n\nSome people argue that basic necessities need to be considered before achieving digital inclusion, such as an ample food supply and quality health care. Minimizing the global digital divide requires considering and addressing the following types of access:\nInvolves \"the distribution of ICT devices per capita…and land lines per thousands\". Individuals need to obtain access to computers, landlines, and networks in order to access the Internet. This access barrier is also addressed in Article 21 of the Convention on the Rights of Persons with Disabilities by the United Nations. \nThe cost of ICT devices, traffic, applications, technician and educator training, software, maintenance and infrastructures require ongoing financial means.\nFinancial access and \"the levels of household income play a significant role in widening the gap\" \nEmpirical tests have identified that several socio-demographic characteristics foster or limit ICT access and usage. Among different countries, educational levels and income are the most powerful explanatory variables, with age being a third one. \n\nWhile a Global Gender Gap in access and usage of ICT's exist, empirical evidence show that this due to unfavorable conditions with respect to employment, education and income and not to technophobia or lower ability. On the contrary, in the contexts under study, women with the prerequsites for access and usage turn out to be more active users of digital tools than men.\nIn order to use computer technology, a certain level of information literacy is needed. Further challenges include information overload and the ability to find and use reliable information. \nComputers need to be accessible to individuals with different learning and physical abilities including complying with Section 508 of the Rehabilitation Act as amended by the Workforce Investment Act of 1998 in the United States. \nIn illustrating institutional access, Wilson states \"the numbers of users are greatly affected by whether access is offered only through individual homes or whether it is offered through schools, community centers, religious institutions, cybercafés, or post offices, especially in poor countries where computer access at work or home is highly limited\". \nGuillen & Suarez argue that \"democratic political regimes enable a faster growth of the Internet than authoritarian or totalitarian regimes\". The Internet is considered a form of e-democracy and attempting to control what citizens can or cannot view is in contradiction to this. Recently situations in Iran and China have denied people the ability to access certain websites and disseminate information. Iran has prohibited the use of high-speed Internet in the country and has removed many satellite dishes in order to prevent the influence of Western culture, such as music and television.\nMany experts claim that bridging the digital divide is not sufficient and that the images and language needed to be conveyed in a language and images that can be read across different cultural lines. A 2013 study conducted by Pew Research Center noted how participants taking the survey in Spanish were nearly twice as likely to not use the internet.\n\nIn the early 21st century, residents of developed countries enjoy many Internet services which are not yet widely available in developing countries, including:\n\n\nThere are four specific arguments why it is important to \"bridge the gap\":\n\n\nWhile these four arguments are meant to lead to a solution to the digital divide, there are a couple other components that need to be considered. The first one is rural living versus suburban living. Rural areas used to have very minimal access to the Internet, for example. However, nowadays, power lines and satellites are used to increase the availability in these areas. Another component to keep in mind is disabilities. Some people may have the highest quality technologies, but a disability they have may keep them from using these technologies to their fullest extent.\n\nUsing previous studies (Gamos, 2003; Nsengiyuma & Stork, 2005; Harwit, 2004 as cited in James), James asserts that in developing countries, \"internet use has taken place overwhelmingly among the upper-income, educated, and urban segments\" largely due to the high literacy rates of this sector of the population. As such, James suggests that part of the solution requires that developing countries first build up the literacy/language skills, computer literacy, and technical competence that low-income and rural populations need in order to make use of ICT.\n\nIt has also been suggested that there is a correlation between democrat regimes and the growth of the Internet. One hypothesis by Gullen is, \"The more democratic the polity, the greater the Internet use...Government can try to control the Internet by monopolizing control\" and Norris \"et al.\" also contends, \"If there is less government control of it, the Internet flourishes, and it is associated with greater democracy and civil liberties.\n\nFrom an economic perspective, Pick and Azari state that \"in developing nations…foreign direct investment (FDI), primary education, educational investment, access to education, and government prioritization of ICT as all important\". Specific solutions proposed by the study include: \"invest in stimulating, attracting, and growing creative technical and scientific workforce; increase the access to education and digital literacy; reduce the gender divide and empower women to participate in the ICT workforce; emphasize investing in intensive Research and Development for selected metropolitan areas and regions within nations\".\n\nThere are projects worldwide that have implemented, to various degrees, the solutions outlined above. Many such projects have taken the form of Information Communications Technology Centers (ICT centers). Rahnman explains that \"the main role of ICT intermediaries is defined as an organization providing effective support to local communities in the use and adaptation of technology. Most commonly an ICT intermediary will be a specialized organization from outside the community, such as a non-governmental organization, local government, or international donor. On the other hand, a social intermediary is defined as a local institution from within the community, such as a community-based organization.\n\nOther proposed solutions that the Internet promises for developing countries are the provision of efficient communications within and among developing countries, so that citizens worldwide can effectively help each other to solve their own problems. Grameen Banks and Kiva loans are two microcredit systems designed to help citizens worldwide to contribute online towards entrepreneurship in developing communities. Economic opportunities range from entrepreneurs who can afford the hardware and broadband access required to maintain Internet cafés to agribusinesses having control over the seeds they plant.\n\nAt the Massachusetts Institute of Technology, the IMARA organization (from Swahili word for \"power\") sponsors a variety of outreach programs which bridge the Global Digital Divide. Its aim is to find and implement long-term, sustainable solutions which will increase the availability of educational technology and resources to domestic and international communities. These projects are run under the aegis of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and staffed by MIT volunteers who give training, install and donate computer setups in greater Boston, Massachusetts, Kenya, Indian reservations the American Southwest such as the Navajo Nation, the Middle East, and Fiji Islands. The CommuniTech project strives to empower underserved communities through sustainable technology and education. According to Dominik Hartmann of the MIT's Media Lab, interdisciplinary approaches are needed to bridge the global digital divide.\n\nBuilding on the premise that any effective solution must be decentralized, allowing the local communities in developing nations to generate their own content, one scholar has posited that social media—like Facebook, YouTube, and Twitter—may be useful tools in closing the divide. As Amir Hatem Ali suggests, \"the popularity and generative nature of social media empower individuals to combat some of the main obstacles to bridging the digital divide\". Facebook’s statistics reinforce this claim. According to Facebook, more than seventy-five percent of its users reside outside of the US. Moreover, more than seventy languages are presented on its website. The reasons for the high number of international users are due to many the qualities of Facebook and other social media. Amongst them, are its ability to offer a means of interacting with others, user-friendly features, and the fact that most sites are available at no cost. The problem with social media, however, is that it can be accessible, provided that there is physical access. Nevertheless, with its ability to encourage digital inclusion, social media can be used as a tool to bridge the global digital divide.\n\nSome cities in the world have started programs to bridge the digital divide for their residents, school children, students, parents and the elderly. One such program, founded in 1996, was sponsored by the city of Boston and called the Boston Digital Bridge Foundation. It especially concentrates on school children and their parents, helping to make both equally and similarly knowledgeable about computers, using application programs, and navigating the Internet.\n\nFree Basics is a partnership between social networking services company Facebook and six companies (Samsung, Ericsson, MediaTek, Opera Software, Nokia and Qualcomm) that plans to bring affordable access to selected Internet services to less developed countries by increasing efficiency, and facilitating the development of new business models around the provision of Internet access. In the whitepaper realised by Facebook's founder and CEO Mark Zuckerberg, connectivity is asserted as a \"human right\", and Internet.org is created to improve Internet access for people around the world.\n\n\"Free Basics provides people with access to useful services on their mobile phones in markets where internet access may be less affordable. The websites are available for free without data charges, and include content about news, employment, health, education and local information etc. By introducing people to the benefits of the internet through these websites, we hope to bring more people online and help improve their lives.\"\n\nHowever, Free Basics is also accused of violating net neutrality for limiting access to handpicked services. Despite a wide deployment in numerous countries, it has been met with heavy resistance notably in India where the Telecom Regulatory Authority of India eventually banned it in 2016.\n\nSeveral projects to bring internet to the entire world with a satellite constellation have been devised in the last decade, one of these being Starlink by Elon Musk's company SpaceX. Unlike Free Basics, it would provide people with a full internet access and would not be limited to a few selected services. In the same week Starlink was announced, serial-entrepreneur Richard Branson announced his own project OneWeb, a similar constellation with approximately 700 satellites that has already procured communication frequency licenses for their broadcast spectrum and could possibly be operational as early as in 2019.\n\nThe biggest hurdle of these projects is the astronomical financial and logistical costs of launching so many satellites. After the failure of previous satellite-to-consumer space ventures, satellite industry consultant Roger Rusch said \"It's highly unlikely that you can make a successful business out of this.\" Musk has publicly acknowledged this business reality, and indicated in mid-2015 that while endeavoring to develop this technically-complicated space-based communication system he wants to avoid overextending the company and stated that they are being measured in the pace of development.\n\nOne Laptop Per Child (OLPC) is another attempt to narrow the digital divide. This organization, founded in 2005, provides inexpensively produced \"XO\" laptops (dubbed the \"$100 laptop\", though actual production costs vary) to children residing in poor and isolated regions within developing countries. Each laptop belongs to an individual child and provides a gateway to digital learning and Internet access. The XO laptops are designed to withstand more abuse than higher-end machines, and they contain features in context to the unique conditions that remote villages present. Each laptop is constructed to use as little power as possible, have a sunlight-readable screen, and is capable of automatically networking with other XO laptops in order to access the Internet—as many as 500 machines can share a single point of access.\n\nSeveral of the 67 principles adopted at the World Summit on the Information Society convened by the United Nations in Geneva in 2003 directly address the digital divide:\n\n\n\n\n"}
{"id": "564695", "url": "https://en.wikipedia.org/wiki?curid=564695", "title": "Discrete system", "text": "Discrete system\n\nA discrete system is a system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models.\n\nA computer is a finite state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals.\n\n\n"}
{"id": "4980221", "url": "https://en.wikipedia.org/wiki?curid=4980221", "title": "Electronic serial number", "text": "Electronic serial number\n\n\"Electronic serial numbers\" (\"ESNs\") were created by the U.S. Federal Communications Commission (FCC) to uniquely identify mobile devices, from the days of AMPS in the United States starting in the early 1980s. The administrative role was taken over by the Telecommunications Industry Association in 1997 and is still maintained by them. ESNs are currently mainly used with CDMA phones (and were previously used by AMPS and TDMA phones), compared to International Mobile Equipment Identity (IMEI) numbers used by all GSM phones.\n\nThe first 8 bits of the ESN were originally the manufacturer code, leaving 24 bits for the manufacturer to assign up to 16,777,215 codes to mobiles. To allow more than 256 manufacturers to be identified, the manufacturer code was extended to 14 bits, leaving 18 bits for the manufacturer to assign up to 262,144 codes. Manufacturer code 0x80 is reserved from assignment and is used instead as an 8-bit prefix for pseudo-ESNs (pESN). The remaining 24 bits are the least significant bits of the SHA-1 hash of a mobile equipment identifier (MEID). Pseudo-ESNs are not guaranteed to be unique (the MEID is the unique identifier if the phone has a pseudo-ESN).\n\nESNs are often represented as either 11-digit decimal numbers or 8 digit hexadecimal numbers. For the decimal format the first three digits are the decimal representation of the first 8 bits (between 00 and 255 inclusive) and the next 8 digits are derived from the remaining 24 bits and will be between 0000000 and 16777215 inclusive. The decimal format of pseudo ESNs will therefore begin with 128. The decimal format separately displays 8 bit manufacturer codes in the first 3 digits, but 14 bit codes are not displayed as separate digits. The hexadecimal format displays an ESN as 8 digits and also does not separately display 14 bit manufacturer codes which occupy 3.5 hexadecimal digits.\n\nAs ESNs have essentially run out, a new serial number format, MEID, was created by 3GPP2 and was first implemented by Verizon in 2006. MEIDs are 56 bits long, the same length as the IMEI and, in fact, MEID was created to be a superset of IMEI. The main difference between MEID and IMEI is that the MEID allows hexadecimal digits while IMEI allows only decimal digits – \"IMEI shall consist of decimal digits (0 through 9) only\".\n\nThe last of the previously unused ESN codes were allocated in November 2008. Applications for assignments were accepted until June 30, 2010 using reclaimed ESN codes, those previously assigned to AMPS or TDMA phones and therefore not present on CDMA2000 systems. Reclaimed codes have also been used for UIMID assignments. Codes are assigned according to industry guidelines.\n\nAlthough ESN assignments may still occur in the future based on applications received before June 30, 2010, there have not been any assignments made since December 31, 2010.\n\n"}
{"id": "732339", "url": "https://en.wikipedia.org/wiki?curid=732339", "title": "External Short Messaging Entity", "text": "External Short Messaging Entity\n\nExternal Short Messaging Entity (ESME) is a term originally coined by Aldiscon to describe an external application that connects to a Short Message Service Center (SMSC) to engage in the sending and/or receiving of SMS messages.\n\nSME is a term used in many cellular circles to describe a network entity (mobile/cell phone) that can send/receive messages. ESME (pronounced EZ-mee) is essentially one of these but without all the wireless aspects; i.e. it is connected via TCP/IP, X.25 or similar. On SMPP 3.4 protocol specifications ESME refers only to external sources and sinks of short messages as Voice Processing Systems, WAP Proxy Servers or Message Handling computers, and it specifically excludes SMEs which are located within the Mobile Network, i.e., a mobile station (MS).\n\nTypical examples of ESMEs are systems that send automated marketing messages to mobile users and voting systems that process SMS votes (\"Pop Idol\", \"Big Brother\").\n\nSMSC uses protocols such as SMPP, UCP, OIS, CIMD, SMCI all of which denote the concept of an ESME connecting to an SMSC.\n\nESME always connects to SMSC using a TCP/IP, X.25, etc. and then binds for the service it needs from SMSC.\n\nFor SMPP it can bind for Receiving only service, Transmitting only service or both (Transceiver service). Before SMPP 3.4 it was required to have two different connections, one for Transmitting and the other one for Receiving. Starting with SMPP 3.4 a Tranceiver connection is enough for both.\n\nThe relation between ESME and SMSC is somehow a master-slave relation because SMSC is providing services to ESME, and usually ESME just uses these services from SMSC.\nOne of the functions of the SMSC is to store and forward the messages while the ESME doesn't have this function. When a message is sent by an ESME to SMSC towards its destination, this message may stay in a SMSC queue until its destination will become available. During this time the ESME has the options to cancel the message in queue, to replace it or to check its status. ESME can also send a message to multiple destinations which will be handled by SMSC.\n\nESME are usually termination points of an SMS network while SMSC are the core of it. SMSC can connect between them while ESME only connects to an SMSC.\nSMPP protocol is designed exactly in this manner for connecting a small end of the SMS network (which is an ESME) to the entire SMS network (which is done through the SMSC)\n\nESME is submitting MTs to SMSC, while SMSC is delivering MOs to ESME.\n\nAn example of how the routing can be done at SMSC level, but not mandatory as this depends a lot on the implementation of SMSC and the way the connection inside the SMSC is between routing part of the SMSC and SMPP interface can be as below:\nDuring the service agreement between ESME and service provider(SMSC side) one unique short code will be allocated to ESME. At the SMSC end smpp server will have list of all ESME address and active connection. When you send any message to short code, messages first comes to SMSC, SMSC decodes it according to GSM 3.4 spec, then one of the modules in SMSC checks the destination address and if it is short code then that module routes messages to SMPP server part of the SMSC. Now SMPP server will have all active connection, according to destination address it selects the ESME - SMPP server connection object, that object will be responsible to encode message according to SMPP protocol and forward to ESME.\n\nCommunication between SMSC and ESME can be on either SMPP or HTTP.\nIf you have SMPP account, you could connect to the SMPP IP+Port on TCP/IP and the SMPP will push MOs to ESME on SMPP connection, and ESME will push MTs on the same connection in reverse.\nIf you have HTTP account with the operator's SMSC, then the SMSC will submit MO to an URL given by you and to push MTs SMSC will give you on URL.\n"}
{"id": "36464288", "url": "https://en.wikipedia.org/wiki?curid=36464288", "title": "Ground radar", "text": "Ground radar\n\nGround radar (cf. airborne radar system) is a radar positioned on the ground and used for air defense (e.g., ground-controlled interception), command guidance (e.g., ground-directed bombing), air traffic control (i.e., radar control), instrument landing systems, radar bomb scoring, etc.. Ground radar may refer to:\n"}
{"id": "39217725", "url": "https://en.wikipedia.org/wiki?curid=39217725", "title": "Hacking Health", "text": "Hacking Health\n\nHacking Health is a social organization that pairs innovators with healthcare experts to build solutions to front-line healthcare problems through the use of emerging technology.\n\nThe organization started off in Montreal, Quebec, Canada in 2012 with a weekend-long hackathon to encourage collaboration between healthcare professionals and IT experts. Since then, Hacking Health events have been held in cities across Canada, the USA and internationally.\n\nHeld over a weekend, Hacking Health hackathons consist of 200-300 participants where designers and developers collaborate with doctors, nurses, clinic managers and other healthcare professionals to develop prototypes that can be put to test in clinics and hospitals. The event also attracts industry professionals, venture capitalists and entrepreneurs.\n\nHeld typically in the evening, Hacking Health Café meetups are regular events to bring together entrepreneurs in the healthcare industry and foster relationships between technology talent and healthcare experts. These shorter, casual events help interested parties stay up-to-date on local collaborations, projects and start-ups.\n\n\nHacking Health has spread globally since 2013 to Cape Town, Strasbourg, Hong Kong, Zurich, Bucharest, and Detroit.\n\n2013\n\nStrasbourg (France)\n\n2014\n\n\n2015\n\n\n2016\n\n\n2017\n\n\n"}
{"id": "525028", "url": "https://en.wikipedia.org/wiki?curid=525028", "title": "High tech", "text": "High tech\n\nHigh technology, often abbreviated to high tech (adjective forms high-technology, high-tech or hi-tech) is technology that is at the cutting edge: the most advanced technology available. The opposite of high tech is \"low technology\", referring to simple, often traditional or mechanical technology; for example, a slide rule is a low-tech calculating device.\n\nThe phrase was used in a 1958 \"The New York Times\" story advocating \"atomic energy\" for Europe: \"... Western Europe, with its dense population and its high technology ...\" Robert Metz used the term in a financial column in 1969: \"Arthur H. Collins of Collins Radio] controls a score of high technology patents in variety of fields.\" and in a 1971 article used the abbreviated form, \"high tech.\"\n\nA widely-used classification of high-technological manufacturing industries is provided by the OECD. It is based on the intensity of research and development activities used in these industries within OECD coutries, resulting in four distinct categories.\n\n"}
{"id": "16593931", "url": "https://en.wikipedia.org/wiki?curid=16593931", "title": "History of manufactured fuel gases", "text": "History of manufactured fuel gases\n\nThe history of gaseous fuel, important for lighting, heating, and cooking purposes throughout most of the 19th century and the first half of the 20th century, began with the development of analytical and pneumatic chemistry in the 18th century. The manufacturing process for \"synthetic fuel gases\" (also known as \"manufactured fuel gas\", \"manufactured gas\" or simply \"gas\") typically consisted of the gasification of combustible materials, usually coal, but also wood and oil. The coal was gasified by heating the coal in enclosed ovens with an oxygen-poor atmosphere. The fuel gases generated were mixtures of many chemical substances, including hydrogen, methane, carbon monoxide and ethylene, and could be burnt for heating and lighting purposes. Coal gas, for example, also contains significant quantities of unwanted sulfur and ammonia compounds, as well as heavy hydrocarbons, and so the manufactured fuel gases needed to be purified before they could be used.\n\nThe first attempts to manufacture fuel gas in a commercial way were made in the period 1795–1805 in France by Philippe LeBon, and in England by William Murdoch. Although precursors can be found, it was these two engineers who elaborated the technology with commercial applications in mind. Frederick Winsor was the key player behind the creation of the first gas utility, the London-based Gas Light and Coke Company, incorporated by royal charter in April 1812.\n\nManufactured gas utilities were founded first in England, and then in the rest of Europe and North America in the 1820s. The technology increased in scale. After a period of competition, the business model of the gas industry matured in monopolies, where a single company provided gas in a given zone. The ownership of the companies varied from outright municipal ownership, such as in Manchester, to completely private corporations, such as in London and most North American cities. Gas companies thrived during most of the nineteenth century, usually returning good profits to their shareholders, but were also the subject of many complaints over price.\n\nThe most important use of manufactured gas in the early 19th century was for gas lighting, as a convenient substitute for candles and oil lamps in the home. Gas lighting became the first widespread form of street lighting. For this use, gases that burned with a highly luminous flame, \"illuminating gases\", were needed, in contrast to other uses (e.g. as fuel) where the heat output was the main consideration. Accordingly some gas mixtures of low intrinsic luminosity, such as blue water gas, were enriched with oil to make them more suitable for street lighting. \n\nIn the second half of the 19th century, the manufactured fuel gas industry diversified out of lighting and into heat and cooking. The threat from electrical light in the later 1870s and 1880s drove this trend strongly. The gas industry did not cede the gas lighting market to electricity immediately, as the invention of the Welsbach mantle, a refractory mesh bag heated to incandescence by a mostly non-luminous flame within, dramatically increased the efficiency of gas lighting. Acetylene was also used from about 1898 for gas cooking and gas lighting (see Carbide lamp) on a smaller scale, although its use too declined with the advent of electric lighting, and LPG for cooking. Other technological developments in the late nineteenth century include the use of water gas and machine stoking, although these were not universally adopted.\n\nIn the 1890s, pipelines from natural gas fields in Texas and Oklahoma were built to Chicago and other cities, and natural gas was used to supplement manufactured fuel gas supplies, eventually completely displacing it. Gas ceased to be manufactured in North America by 1966 (with the exception of Indianapolis and Honolulu), while it continued in Europe until the 1980s. \"Manufactured gas\" is again being evaluated as a fuel source, as energy utilities look towards coal gasification once again as a potentially cleaner way of generating power from coal, although nowadays such gases are likely to be called \"synthetic natural gas\".\n\nPneumatic chemistry developed in the eighteenth century with the work of scientists such as Stephen Hales, Joseph Black, Joseph Priestley, and Antoine-Laurent Lavoisier, and others. Until the eighteenth century, gas was not recognized as a separate state of matter. Rather, while some of the mechanical properties of gases were understood, as typified by Robert Boyle's experiments and the development of the air pump, their chemical properties were not. Gases were regarded in keeping the Aristotelean tradition of four elements as being air, one of the four fundamental elements. The different sorts of airs, such as putrid airs or inflammable air, were looked upon as atmospheric air with some impurities, much like muddied water.\n\nAfter Joseph Black realized that carbon dioxide was in fact a different sort of gas altogether from atmospheric air, other gases were identified, including hydrogen by Henry Cavendish in 1766. Alessandro Volta expanded the list with his discovery of methane in 1776. It had also been known for a long time that inflammable gases could be produced from most combustible materials, such as coal and wood, through the process of distillation. Stephen Hales, for example, had written about the phenomenon in the \"Vegetable Staticks\" in 1722. In the last two decades of the eighteenth century, as more gases were being discovered and the techniques and instruments of pneumatic chemistry became more sophisticated, a number of natural philosophers and engineers thought about using gases in medical and industrial applications. One of the first such uses was ballooning beginning in 1783, but other uses soon followed.\n\nOne of the results of the ballooning craze of 1783–1784 was the first implementation of lighting by manufactured gas. A professor of natural philosophy at the University of Louvain Jan Pieter Minckeleers and two of his colleagues were asked by their patron, the Duke of Arenberg, to investigate ballooning. They did so, building apparatus to generate lighter than air inflammable gases from coal and other inflammable substances. In 1785 Minckeleers used some of this apparatus to gasify coal to light his lecture hall at the university. He did not extend gas lighting much beyond this, and when he was forced to flee Leuven during the Brabant Revolution, he abandoned the project altogether.\n\nPhilippe LeBon was a French civil engineer working in the public engineering corps who became interested while at university in distillation as an industrial process for the manufacturing of materials such as tar and oil. He graduated from the engineering school in 1789, and was assigned to Angoulême. There, he investigated distillation, and became aware that the gas produced in the distillation of wood and coal could be useful for lighting, heating, and as an energy source in engines. He took out a patent for distillation processes in 1794, and continued his research, eventually designing a distillation oven known as the \"thermolamp\". He applied for and received a patent for this invention in 1799, with an addition in 1801. He launched a marketing campaign in Paris in 1801 by printing a pamphlet and renting a house where he put on public demonstrations with his apparatus. His goal was to raise sufficient funds from investors to launch a company, but he failed to attract this sort of interest, either from the French state or from private sources. He was forced to abandon the project and return to the civil engineering corps. Although he was given a forest concession by the French government to experiment with the manufacture of tar from wood for naval use, he never succeed with the thermolamp, and died in uncertain circumstances in 1805.\n\nAlthough the thermolamp received some interest in France, in Germany interest was the greatest. A number of books and articles were written on the subject in the period 1802–1812. There were also thermolamps designed and built in Germany, the most important of which were by Zachaus Winzler, an Austrian chemist who ran a saltpetre factory in Blansko. Under the patronage of the aristocratic zu Salm family, he built a large one in Brno. He moved to Vienna to further his work. The thermolamp, however, was used primarily for making charcoal and not for the production of gases.\n\nWilliam Murdoch (sometimes Murdock) (1754–1839) was an engineer working for the firm of Boulton & Watt, when, while investigating distillation processes sometime in 1792–1794, he began using coal gas for illumination. He was living in Redruth in Cornwall at the time, and made some small scale experiments with lighting his own house with coal gas. He soon dropped the subject until 1798, when he moved to Birmingham to work at Boulton & Watt's home base of Soho. Boulton & Watt then instigated another small-scale series of experiments. With ongoing patent litigation and their main business of steam engines to attend to, the subject was dropped once again. Gregory Watt, James Watt's second son, while traveling in Europe saw Lebon's demonstrations and wrote a letter to his brother, James Watt Jr., informing him of this potential competitor. This prompted James Watt Jr. to begin a gaslight development program at Boulton & Watt that would scale up the technology and lead to the first commercial applications of gaslight.\n\nAfter an initial installation at the Soho Foundry in 1803–1804, Boulton & Watt prepared an apparatus for the textile firm of Philips & Lee in Salford near Manchester in 1805–1806. This was to be their only major sale until late 1808. George Augustus Lee was a major motivating force behind the development of the apparatus. He had an avid interest in technology, and had introduced a series of technological innovations at the Salford Mill, such as iron frame construction and steam heating. He continued to encourage the development of gaslight technology at Boulton & Watt.\n\nThe first company to provide manufactured gas to consumer as a utility was the London-based Gas Light and Coke Company. It was founded through the efforts of a German émigré, Frederick Winsor, who had witnessed Lebon's demonstrations in Paris. He had tried unsuccessfully to purchase a thermolamp from Lebon, but remained taken with the technology, and decided to try his luck, first in his hometown of Brunswick, and then in London in 1804. Once in London, Winsor began an intense campaign to find investors for a new company that would manufacture gas apparatus and sell gas to consumers. He was successful in finding investors, but the legal form of the company was a more difficult problem. By the Bubble Act of 1720, all joint-stock companies above a certain number of shareholders in England needed to receive a royal charter to incorporate, which meant that an act of Parliament was required.\n\nWinsor waged his campaign intermittently to 1807, when the investors constituted a committee charged with obtaining an act of Parliament. They pursued this task over the next three years, encountering adversities en route, the most important of which was the resistance by Boulton & Watt in 1809. In that year, the committee made a serious attempt to get the House of Commons to pass a bill empowering the king to grant the charter, but Boulton & Watt felt their gaslight apparatus manufacturing business was threatened and mounted an opposition through their allies in Parliament. Although a parliamentary committee recommended approval, it was defeated on the third reading.\n\nThe following year, the committee tried again, succeeding with the acquiescence of Boulton & Watt because they renounced all powers to manufacture apparatus for sale. The act required that the company raise £100,000 before they could request a charter, a condition it took the next two years to fill. George III granted the charter in 1812.\n\nFrom 1812 to approximately 1825, manufactured gas was predominantly an English technology. A number of new gas utilities were founded to serve London and other cities in the UK after 1812. Liverpool, Exeter, and Preston were the first in 1816. Others soon followed; by 1821, no town with population over 50,000 was without gaslight. Five years later, there were only two towns over 10,000 that were without gaslight.\nIn London, the growth of gaslight was rapid. New companies were founded within a few years of the Gas Light and Coke Company, and a period of intense competition followed as companies competed for consumers on the boundaries of their respective zones of operations. Frederick Accum, in the various editions of his book on gaslight, gives a good sense of how rapidly the technology spread in the capital. In 1815, he wrote that there were 4000 lamps in the city, served by 26 miles (42 km) of mains. In 1819, he raised his estimate to 51,000 lamps and 288 miles (463 km) of mains. Likewise, there were only two gasworks in London in 1814, and by 1822, there were seven and by 1829, there were 200 companies. The government did not regulate the industry as a whole until 1816, when an act of Parliament created and a post of inspector for gasworks, the first holder of which was Sir William Congreve. Even then, no laws were passed regulating the entire industry until 1847, although a bill was proposed in 1822, which failed due to opposition from gas companies. The charters approved by Parliament did, however, contain various regulations such as how the companies could break up the pavement, etc.\n\nFrance's first gas company was also promoted by Frederick Winsor after he had to flee England in 1814 due to unpaid debts and tried to found another gas company in Paris, but it failed in 1819. The government was also interested in promoting the industry, and in 1817 commissioned Chabrol de Volvic to study the technology and build a prototype plant, also in Paris. The plant provided gas for lighting the hôpital Saint Louis, and the experiment was judged successful. King Louis XVIII then decided to give further impulse to the development of the French industry by sending people to England to study the situation there, and to install gaslight at a number of prestigious buildings, such as the Opera building, the national library, etc. A public company was created for this purpose in 1818. Private companies soon followed, and by 1822, when the government moved to regulate the industry, there were four in operation in the capital. The regulations passed then prevented the companies from competing, and Paris was effectively divided between the various companies operating as monopolies in their own zones.\n\nGaslight spread to other European countries. In 1817, a company was founded in Brussels by P. J. Meeus-Van der Maelen, and began operating the following year. By 1822, there were companies in Amsterdam and Rotterdam using English technology. In Germany, gaslight was used on a small scale from 1816 onwards, but the first gaslight utility was founded by English engineers and capital. In 1824, the Imperial Continental Gas Association was founded in London to establish gas utilities in other countries. Sir William Congreve, one if its leaders, signed an agreement with the government in Hanover, and the gas lamps were used on streets for the first time in 1826.\n\nGaslight was first introduced to the US in 1816 in Baltimore by Rembrandt and Rubens Peale, who lit their museum with gaslight, which they had seen on a trip to Europe. The brothers convinced a group of wealthy people to back them in a larger enterprise. The local government passed a law allowing the Peales and their associates to lay mains and light the streets. A company was incorporated for this purpose in 1817. After some difficulties with the apparatus and financial problems, the company hired an English engineer with experience in gaslight. It began to flourish, and by the 1830s, the company was supplying gas to 3000 domestic customers and 100 street lamps. Companies in other cities followed, the second being Boston Gas Light in 1822 and New York Gas Light Company in 1825. A gas works was built in Philadelphia in 1835.\n\nGas lighting was one of the most debated technologies of the first industrial revolution. In Paris, as early as 1823, controversy forced the government to devise safety standards (Fressoz, 2007). The residues produced from distilled coal were either drained into rivers or stored in basins which polluted (and still pollute) the soil.\n\nCase law in the UK and the US clearly held though, the construction and operation of a gas-works was not the creation of a public nuisance \"in se\", due to the reputation of gas-works as highly undesirable neighbors, and the noxious pollution known to issue from such, especially in the early days of manufactured gas, gas-works were on extremely short notice from the courts that (detectable) contamination outside of their grounds – especially in residential districts – would be severely frowned upon. Indeed, many actions for the abatement of nuisances brought before the courts did result in unfavorable verdicts for gas manufacturers – in one study on early environmental law, actions for nuisance involving gas-works resulted in findings for the plaintiffs 80% of the time, compared with an overall plaintiff victory rate of 28.5% in industrial nuisance cases.\n\nInjunctions both preliminary and permanent could and were often issued in cases involving gas works. For example, the ill reputation of gas-works became so well known that in \"City of Cleveland vs. Citizens' Gas Light Co.\", 20 N. J. Eq. 201, a court went so far as to enjoin a \"future\" gas-works not yet even built – preventing it from causing \"annoying and offensive vapours and odors\" in the first place. The injunction not only regulated the gas manufacturing process – forbidding the use of lime purification – but also provided that if nuisances of any sort were to issue from the works – a permanent injunction forbidding the production of gas would issue from the court. Indeed, as the Master of the Rolls, Lord Langdale, once remarked in his opinion in \"Haines v. Taylor\", 10 Beavan 80, that \"I have been rather astonished to hear the effects of gas works treated as nothing...every man, in these days, must have sufficient experience, to enable him to come to the conclusion, that, whether a nuisance or not, a gas manufactory is a very disagreeable thing. Nobody can doubt that the volatile products which arise from the distillation of coal are extremely offensive. It is quite contrary to common experience to say they are not so...every man knows it.\"\nHowever, as time went by, gas-works began to be seen as more as a double-edged sword – and eventually as a positive good, as former nuisances were abated by technological improvements, and the full benefits of gas became clear. There were several major impetuses that drove this phenomenon:\n\n\nBoth the era of consolidation of gas-works through high-pressure distribution systems (1900s–1930s) and the end of the era of manufactured gas (1955–1975) saw gas-works being shut down due to redundancies. What brought about the end of manufactured gas was that pipelines began to be built to bring natural gas directly from the well to gas distribution systems. Natural gas was superior to the manufactured gas of that time, being cheaper – extracted from wells rather than manufactured in a gas-works – more user friendly – coming from the well requiring little, if any, purification – and safer – due to the lack of carbon monoxide in the distributed product. Upon being shut down, few former manufactured gas plant sites were brought to an acceptable level of environmental cleanliness to allow for their re-use, at least by contemporary standards. In fact, many were literally abandoned in place, with process wastes left \"in situ\", and never adequately disposed of.\n\nAs the wastes produced by former manufactured gas plants were persistent in nature, they often (as of 2009) still contaminate the site of former manufactured gas plants: the waste causing the most concern today is primarily coal tar (mixed long-chain aromatic and aliphatic hydrocarbons, a byproduct of coal carbonization), while \"blue billy\" (a noxious byproduct of lime purification contaminated with cyanides) as well as other lime and coal tar residues are regarded as lesser, though significant environmental hazards. Some former manufactured gas plants are owned by gas utilities today, often in an effort to prevent contaminated land from falling into public use, and inadvertently causing the release of the wastes therein contained. Others have fallen into public use, and without proper reclamation, have caused – often severe – health hazards for their users. When and where necessary, former manufactured gas plants are subject to environmental remediation laws, and can be subject to legally mandated cleanups.\n\nThe basic design of gaslight apparatus was established by Boulton & Watt and Samuel Clegg in the period 1805–1812. Further improvements were made at the Gas Light and Coke Company, as well as by the growing number of gas engineers such as John Malam and Thomas Peckston after 1812. Boulton & Watt contributed the basic design of the retort, condenser, and gasometer, while Clegg improved the gasometer and introduced lime purification and the hydraulic main, another purifier.\n\nThe retort bench was the construction in which the retorts were located for the carbonization (synonymous with pyrolysis) of the coal feedstock and the evolution of coal gas. Over the years of manufactured gas production, advances were made that turned the retort-bench from little more than coal-containing iron vessels over an open fire to a massive, highly efficient, partially automated, industrial-scale, capital-intensive plant for the carbonization of large amounts of coal. Several retort benches were usually located in a single \"retort house\", which there was at least one of in every gas works.\n\nInitially, retort benches were of many different configurations due to the lack of long use and scientific and practical understanding of the carbonization of coal. Some early retorts were little more than iron vessels filled with coal and thrust upon a coal fire with pipes attached to their top ends. Though practical for the earliest gas works, this quickly changed once the early gas-works served more than a few customers. As the size of such vessels grew – the need became apparent for efficiency in refilling retorts – and it was apparent that filling one-ended vertical retorts was easy; removing the coke and residues from them after the carbonization of coal was far more difficult. Hence, gas retorts transitioned from vertical vessels to horizontal tubular vessels.\n\nRetorts were usually made of cast iron during the early days. Early gas engineers experimented extensively with the best shape, size, and setting. No one form of retort dominated, and many different cross-sections remained in use. After the 1850s, retorts generally became made of fire clay due to greater heat retention, greater durability, and other positive qualities. Cast-iron retorts were used in small gas works, due to their compatibility with the demands there, with the cast-iron retort's lower cost, ability to heat quickly to meet transient demand, and \"plug and play\" replacement capabilities. This outweighed the disadvantages of shorter life, lower temperature margins, and lack of ability to be manufactured in non-cylindrical shapes. Also, general gas-works practice following the switch to fire-clay retorts favored retorts that were shaped like a \"D\" turned 90 degrees to the left, sometimes with a slightly pitched bottom section.\n\nWith the introduction of the fire-clay retort, higher heats could be held in the retort benches, leading to faster and more complete carbonization of the coal. As higher heats became possible, advanced methods of retort bench firing were introduced, catalyzed by the development of the open hearth furnace by Siemens, circa 1855–1870, leading to a revolution in gas-works efficiency.\n\nSpecifically, the two major advances were:\n\nThese two advances turned the old, \"directly fired\" retort bench into the advanced, \"indirectly fired\", \"regenerative\" or \"generative\" retort bench, and lead coke usage within the retort benches (in the larger works) to drop from upwards of 40% of the coke made by the retorts to factors as low as 15% of the coke made by the retorts, leading to an improvement in efficiency of an order of magnitude. These improvements imparted an additional capital cost to the retort bench, which caused them to be slowly incorporated in the smaller gas-works, if they were incorporated at all.\n\nFurther increases in efficiency and safety were seen with the introduction of the \"through\" retort, which had a door at front and rear. This provided for greater efficiency and safety in loading and unloading the retorts, which was a labor-intensive and often dangerous process. Coal could now be pushed out of the retort – rather than pulled out of the retort. One interesting modification of the \"through\" retort was the \"inclined\" retort – coming into its heyday in the 1880s – a retort set on a moderate incline, where coal was poured in at one end, and the retort sealed; following pyrolysis, the bottom was opened and the coke poured out by gravity. This was adopted in some gas-works, but the savings in labor were often offset by the uneven distribution and pyrolysis of the coal as well as clumping problems leading to failure of the coal to pour out of the bottom following pyrolysis that were exacerbated in certain coal types. As such, inclined retorts were rendered obsolescent by later advances, including the retort-handling machine and the vertical retort system.\n\nSeveral advanced retort-house appliances were introduced for improved efficiency and convenience. The compressed-air or steam-driven clinkering pick was found to be especially useful in removing clinker from the primary combustion area of the indirectly fired benches – previously clinkering was an arduous and time-consuming process that used large amounts of retort house labor. Another class of appliances introduced were apparatuses – and ultimately, machines – for retort loading and unloading. Retorts were generally loaded by using an elongated scoop, into which the coal was loaded – a gang of men would then lift the scoop and ram it into the retort. The coal would then be raked by the men into a layer of even thickness and the retort sealed. Gas production would then ensue – and from 8 – 12 hours later, the retort would be opened, and the coal would be either pulled (in the case of \"stop-ended\" retorts) or pushed (in the case of \"through\" retorts) out of the retort. Thus, the retort house had heavy manpower requirements – as many men were often required to bear the coal-containing scoop and load the retort.\n\n\"(TBD: Brief description of advanced retort loading apparatus; more detailed description of retort-handling machine.)\"\n\n\"Coming soon: The introduction of the coke-oven system, and, finally, the vertical retort system.\"\n\nFrom the retort, the gas would first pass through a tar/water \"trap\" (similar to a trap in plumbing) called a hydraulic main, where a considerable fraction of coal tar was given up and the gas was significantly cooled. Then, it would pass through the main out of the retort house into an atmospheric or water-cooled condenser, where it would be cooled to the temperature of the atmosphere or the water used. At this point, it enters the exhauster house and passes through an \"exhauster\", an air pump which maintains the hydraulic mains and, consequently, the retorts at a negative pressure (with a zero pressure being atmospheric). It would then be washed in a \"washer\" by bubbling it through water, to extract any remaining tars. After this, it would enter a purifier. The gas would then be ready for distribution, and pass into a gasholder for storage.\n\nWithin each retort-house, the retort benches would be lined up next to one another in a long row. Each retort had a loading and unloading door. Affixed to each door was an ascension pipe, to carry off the gas as it was evolved from the coal within. These pipes would rise to the top of the bench where they would terminate in an inverted \"U\" with the leg of the \"U\" disappearing into a long, trough-shaped structure (with a covered top) made of cast iron called a hydraulic main that was placed atop the row of benches near their front edge. It ran continuously along the row of benches within the retort house, and each ascension pipe from each retort descended into it.\n\nThe hydraulic main had a level of a liquid mixture of (initially) water, but, following use, also coal tar, and ammoniacal liquor. Each retort ascension pipe dropped under the water level by at least a small amount, perhaps by an inch, but often considerably more in the earlier days of gas manufacture. The gas evolved from each retort would thus bubble through the liquid and emerge from it into the void above the liquid, where it would mix with the gas evolved from the other retorts and be drawn off through the foul main to the condenser.\n\nThere were two purposes to the liquid seal: first, to draw off some of the tar and liquor, as the gas from the retort was laden with tar, and the hydraulic main could rid the gas of it, to a certain degree; further tar removal would take place in the condenser, washer/scrubber, and the tar extractor. Still, there would be less tar to deal with later. Second, the liquid seal also provided defense against air being drawn into the hydraulic main: if the main had no liquid within, and a retort was left open with the pipe not shut off, and air were to combine with the gas, the main could explode, along with nearby benches.\n\nHowever, after the early years of gas, research proved that a very deep, excessive seal on the hydraulic main threw a backpressure upon all the retorts as the coal within was gasifying, and this had deleterious consequences; carbon would likely deposit onto the insides of retorts and ascension pipes; and the bottom layer of tar with which the gas would have to travel through in a deeply sealed main robbed the gas of some of its illuminating value. As such, after the 1860s, hydraulic mains were run at around 1 inch of seal, and no more.\n\nLater retort systems (many types of vertical retorts, especially ones in continuous operation) which had other anti-oxygen safeguards, such as check valves, etc., as well as larger retorts, often omitted the hydraulic main entirely and went straight to the condensers – as other apparatus and buildings could be used for tar extraction, the main was unnecessary for these systems.\n\nAir Cooled Condensers\n\nCondensers were either air cooled or water cooled. Air cooled condensers were often made up from odd lengths of pipe and connections. The main varieties in common use were classified as follows:\n(a) Horizontal types \n\n(b) Vertical types\n\n(c) Annular types \n\n(d) The battery condenser.\n\nThe horizontal condenser was an extended foul main with the pipe in a zigzag pattern from end to end of one of the retort-house walls. Flange connections were essential as blockages from naphthalene or pitchy deposits were likely to occur. The condensed liquids flowed down the sloping pipes in the same direction as the gas. As long as gas flow was slow, this was an effective method for the removal of naphthalene. Vertical air condensers had gas and tar outlets.\n\nThe annular atmospheric condenser was easier to control with respect to cooling rates. The gas in the tall vertical cylinders was annular in form and allowed an inside and outside surface to be exposed to cooling air. The diagonal side pipes conveyed the warm gas to the upper ends of each annular cylinder. Butterfly valves or dampers were fitted to the top of each vertical air pipe, so that the amount of cooling could be regulated.\n\nThe battery condenser was a long and narrow box divided internally by baffle-plates which cause the gas to take a circuitous course. The width of the box was usually about 2 feet, and small tubes passed from side to side form the chief cooling surface. The ends of these tubes were left open to allow air to pass through. The obstruction caused by the tubes played a role in breaking up and throwing down the tars suspended in the gas.\n\nTypically, plants using cast-iron mains and apparatus allowed 5 square feet of superficial area per 1,000 cubic feet of gas made per day. This could be slightly reduced when wrought iron or mild steel was used.\n\nWater Cooled Condensers\n\nWater cooled condensers were almost constructed from riveted mild steel plates (which form the outer shell) and steel or wrought-iron tubes. There were two distinct types used: \n\n(a) Multitubular condensers. \n(b) Water-tube condensers.\n\nUnless the cooling water was exceptionally clean, the water-tube condenser was preferred. The major difference between the multitubular and water-tube condenser was that in the former the water passed outside and around the tubes which carry the hot gas, and in the latter type, the opposite was the case. Thus when only muddy water pumped from rivers or canals was available; the water-tube condenser was used. When the incoming gas was particularly dirty and contained an undesirable quantity of heavy tar, the outer chamber was liable to obstruction from this cause.\n\nThe hot gas was saturated with water vapor and accounted for the largest portion of the total work of condensation. Water vapor has to lose large quantities of heat, as did any liquefiable hydrocarbon. Of the total work of condensation, 87% was accounted for in removing water vapor and the remainder was used to cool permanent gases and to condensing liquefiable hydrocarbon.\n\nAs extremely finely divided particles were also suspended in the gas, it was impossible to separate the particulate matter solely by a reduction of vapor pressure. The gas underwent processes to remove all traces of solid or liquid matter before it reached the wet purification plant. Centrifugal separators, such as the Colman Cyclone apparatus were utilized for this process in some plants.\n\nThe hydrocarbon condensates removed in the order heavy tars, medium tars and finally light tars and oil fog. About 60-65% of the tars would be deposited in the hydraulic main. Most of this tar was heavy tars. The medium tars condensed out during the passage of the products between the hydraulic and the condenser. The lighter tars oil fog would travel considerably further.\n\nIn general, the temperature of the gas in the hydraulic main varies between 140-160 F. The constituents most liable to be lost were benzene, toluene, and, to some extent, xylene, which had an important effect on the ultimate illuminating power of the gas. Tars were detrimental for the illuminating power and were isolated from the gas as rapidly as possible.\n\nMaintained hydraulic main and condenser at negative pressure.\n\nThere were several types of exhausters.\n\n\nFinal extractions of minor deleterious fractions. \nScrubbers which utilized water were designed in the 25 years after the foundation of the industry. It was discovered that the removal of ammonia from the gas depended upon the way in which the gas to be purified was contacted by water. This was found to be best performed by the Tower Scrubber. This scrubber consisted of a tall cylindrical vessel, which contained trays or bricks which were supported on grids. The water, or weak gas liquor, trickled over these trays, thereby keeping the exposed surfaces thoroughly wetted. The gas to be purified was run through the tower to be contacted with the liquid. In 1846 George Lowe patented a device with revolving perforated pipes for supplying water or purifying liquor. At a later date, the Rotary Washer Scrubber was introduced by Paddon, who used it at Brighton about 1870. This prototype machine was followed by others of improved construction ; notably by Kirkham, Hulett, and Chandler, who introduced the well-known Standard Washer Scrubber, Holmes, of Huddersfield, and others. The Tower Scrubber and the Rotary Washer Scrubber made it possible to completely remove ammonia from the gas.\n\nCoal gas coming directly from the bench was a noxious soup of chemicals, and removal of the most deleterious fractions was important, for improving the quality of the gas, for preventing damage to equipment or premises, and for recovering revenues from the sale of the extracted chemicals. Several offensive fractions being present in a distributed gas might lead to problems – Tar in the distributed gas might gum up the pipes (and could be sold for a good price), ammoniacal vapours in the gas might lead to corrosion problems (and the extracted ammonium sulfate was a decent fertilizer), naphthalene vapours in the gas might stop up the gas-mains, and even carbon dioxide in the gas was known to decrease illumination; thus various facilities within the gas-works were tasked with the removal of these deleterious effluents. But these do not compare to the most hazardous contaminant in the raw coal gas: the sulfuret of hydrogen (hydrogen sulfide, HS). This was regarded as utterly unacceptable for several reasons:\n\nAs such, the removal of the sulfuret of hydrogen was given the highest level of priority in the gas-works. A special facility existed to extract the sulfuret of hydrogen – known as the purifier. The purifier was arguably the most important facility in the gas-works, if the retort-bench itself is not included.\n\nOriginally, purifiers were simple tanks of lime-water, also known as cream or milk of lime, where the raw gas from the retort bench was bubbled through to remove the sulfuret of hydrogen. This original process of purification was known as the \"wet lime\" process. The lime residue left over from the \"wet lime\" process was one of the first true \"toxic wastes\", a material called \"blue billy\". Originally, the waste of the purifier house was flushed into a nearby body of water, such as a river or a canal. However, after fish kills, the nauseating way it made the rivers stink, and the truly horrendous stench caused by exposure of residuals if the river was running low, the public clamoured for better means of disposal. Thus it was piled into heaps for disposal. Some enterprising gas entrepreneurs tried to sell it as a weed-killer, but most people wanted nothing to do with it, and generally, it was regarded as waste which was both smelly and poisonous, and gas-works could do little with, except bury. But this was not the end of the \"blue billy\", for after burying it, rain would often fall upon its burial site, and leach the poison and stench from the buried waste, which could drain into fields or streams. Following countless fiascoes with \"blue billy\" contaminating the environment, a furious public, aided by courts, juries, judges, and masters in chancery, were often very willing to demand that the gas-works seek other methods of purification – and even pay for the damages caused by their old methods of purification.\n\nThis led to the development of the \"dry lime\" purification process, which was less effective than the \"wet lime\" process, but had less toxic consequences. Still, it was quite noxious. Slaked lime (calcium hydroxide) was placed in thick layers on trays which were then inserted into a square or cylinder-shaped purifier tower which gas was then passed through, from the bottom to the top. After the charge of slaked lime had lost most of its absorption effectiveness, the purifier was then shut off from the flow of gas, and either was opened, or air was piped in. Immediately, the sulfur-impregnated slaked lime would react with the air to liberate large concentrations of sulfuretted hydrogen, which would then billow out of the purifier house, and make the gas-works, and the district, stink of sulfuretted hydrogen. Though toxic in sufficient concentrations or long exposures, the sulfuret was generally just nauseating for short exposures at moderate concentrations, and was merely a health hazard (as compared to the outright danger of \"blue billy\") for the gas-works employees and the neighbors of the gas-works. The sulfuretted lime was not toxic, but not greatly wanted, slightly stinking of the odor of the sulfuret, and was spread as a low grade fertilizer, being impregnated with ammonia to some degree. The outrageous stinks from many gas-works led many citizens to regard them as public nuisances, and attracted the eye of regulators, neighbors, and courts.\n\nThe \"gas nuisance\" was finally solved by the \"iron ore\" process. Enterprising gas-works engineers discovered that bog iron ore could be used to remove the sulfuretted hydrogen from the gas, and not only could it be used for such, but it could be used in the purifier, exposed to the air, whence it would be rejuvenated, without emitting noxious sulfuretted hydrogen gas, the sulfur being retained in the iron ore. Then it could be reinserted into the purifier, and reused and rejuvenated multiple times, until it was thoroughly embedded with sulfur. It could then be sold to the sulfuric acid works for a small profit. Lime was sometimes still used after the iron ore had thoroughly removed the sulfuret of hydrogen, to remove carbonic acid (carbon dioxide, CO), the bisulfuret of carbon (carbon disulfide, CS), and any ammonia still aeroform after its travels through the works. But it was not made noxious as before, and usually could fetch a decent rate as fertilizer when impregnated with ammonia. This finally solved the greatest pollution nuisances of the gas-works, but still lesser problems remained – not any that the purifier house could solve, though.\n\nPurifier designs also went through different stages throughout the years.\n\n Gasholders were constructed of a variety of materials, brick, stone, concrete, steel, or wrought iron. The holder or floating vessel is the storage reservoir for the gas, and it serves the purpose of equalizing the distribution of the gas under pressure, and ensures a continuity of supply, while gas remains in the holder. They are cylindrical like an inverted beaker and work up and down in the tank. In order to maintain a true vertical position, the vessel has rollers which work on guide-rails attached to the tank sides and to the columns surrounding the holder. \nGasholders may be either single or telescopic in two or more lifts. When it is made in the telescopic form, its capacity could be increased to as much as four times the capacity of the single-lift holder for equal dimensions of tank. The telescopic versions were found to be useful as they conserved ground space and capital.\n\nThe gasworks had numerous small appertunances and facilities to aid with divers gas management tasks or auxiliary services.\n\nAs the years went by, boilers (for the raising of steam) became extremely common in most gas-works above those small in size; the smaller works often used gas-powered internal combustion engines to do some of the tasks that steam performed in larger workings.\n\nSteam was in use in many areas of the gasworks, including:\nFor the operation of the exhauster;\nFor scurfing of pyrolysis char and slag from the retorts and for clinkering the producer of the bench;\nFor the operation of engines used for conveying, compressing air, charging hydraulics, or the driving of dynamos or generators producing electric current;\nTo be injected under the grate of the producer in the indirectly fired bench, so as to prevent the formation of clinker, and to aid in the water-gas shift reaction, ensuring high-quality secondary combustion;\nAs a reactant in the (carburetted) water gas plant, as well as driving the equipment thereof, such as the numerous blowers used in that process, as well as the oil spray for the carburettor;\nFor the operation of fire, water, liquid, liquor, and tar pumps;\nFor the operation of engines driving coal and coke conveyor-belts;\nFor clearing of chemical obstructions in pipes, including naphthalene & tar as well as general cleaning of equipment;\nFor heating cold buildings in the works, for maintaining the temperature of process piping, and preventing freezing of the water of the gasholder, or congealment of various chemical tanks and wells.\n\nHeat recovery appliances could also be classed with boilers. As the gas industry applied scientific and rational design principles to its equipment, the importance of thermal management and capture from processes became common. Even the small gasworks began to use heat-recovery generators, as a fair amount of steam could be generated for \"free\" simply by capturing process thermal waste using water-filled metal tubing inserted into a strategic flue.\n\nAs the electric age came into being, the gas-works began to use electricity – generated on site – for many of the smaller plant functions previously performed by steam or gas-powered engines, which were impractical and inefficient for small, sub-horsepower uses without complex and failure-prone mechanical linkages. As the benefits of electric illumination became known, sometimes the progressive gasworks diversified into electric generation as well, as coke for steam-raising could be had on-site at low prices, and boilers were already in the works.\n\nAccording to Meade, the gasworks of the early 20th century generally kept on hand several weeks of coal. This amount of coal could cause major problems, because coal was liable to spontaneous combustion when in large piles, especially if they were rained upon, due to the protective dust coating of the coal being washed off, exposing the full porous surface area of the coal of slightly to highly activated carbon below; in a heavy pile with poor heat transfer characteristics, the heat generated could lead to ignition. But storage in air-entrained confined spaces was not highly looked upon either, as residual heat removal would be difficult, and fighting a fire if it was started could result in the formation of highly toxic carbon monoxide through the water-gas reaction, caused by allowing water to pass over extremely hot carbon (HO + C = H + CO), which would be dangerous outside, but deadly in a confined space.\n\nCoal storage was designed to alleviate this problem. Two methods of storage were generally used; underwater, or outdoor covered facilities. To the outdoor covered pile, sometimes cooling appurtenances were applied as well; for example, means to allow the circulation of air through the depths of the pile and the carrying off of heat. Amounts of storage varied, often due to local conditions. Works in areas with industrial strife often stored more coal, while nations whose proletariat was under \"control\" stored less. Other variables included national security; for instance, the gasworks of Tegel in Berlin had some 1 million tons of coal (6 months of supply) in gigantic underwater bunker facilities half a mile long (Meade 2e, p. 379); as Berlin is on the North German Plain, perhaps this was due to what happened to Paris in the Franco-Prussian War of 1870–1871.\n\nMachine stoking or power stoking was used to replace labor and minimize disruptions due to labor disputes. Each retort typically required two sets of three stokers. Two of the stokers were required to lift the point of the scoop into the retort, while the third would push it in and turn it over. Coal would be introduced from each side of the retort. The coke produced would be removed from both sides also. Gangs of stokers worked 12-hour shifts, although the labor was not continuous. The work was also seasonal, with extra help being required in the winter time. Machine stoking required more uniform placement of the retorts. Increasing cost of labor increased the profit margin in experimenting with and instituting machine stoking.\n\nThe chemical industries demanded coal tar, and the gas-works could provide it for them; and so the coal tar was stored on site in large underground tanks. As a rule, these were single wall metal tanks – that is, if they were not porous masonry. In those days, underground tar leaks were seen as merely a waste of tar; out of sight was truly out of mind; and such leaks were generally addressed only when the loss of revenue from leaking tar \"wells\", as these were sometimes called, exceeded the cost of repairing the leak. This practice of bygone days has caused representatives of present-day gas utilities to dive under tables and utter minced oaths when terms like \"purportedly responsible party\", \"BTEX\", \"aquifer plume\", or \"Superfund\" are mentioned.\n\nAmmoniacal liquor was stored on site as well, in similar tanks. Sometimes the more progressive gasworks would have an ammonium sulfate plant, to convert the liquor into fertilizer, which was sold to farmers.\n\nThis large-scale gas meter precisely measured gas as it issued from the works into the mains. It was of the utmost importance, as the gasworks balanced the account of issued gas versus the amount of paid for gas, and strived to detect why and how they varied from one another. Often it was coupled with a dynamic regulator to keep pressure constant, or even to modulate the pressure at specified times (a series of rapid pressure spikes was sometimes used with appropriately equipped street-lamps to automatically ignite or extinguish such remotely).\n\nThis device injected a fine mist of naphtha into the outgoing gas so as to avoid the crystallization of naphthalene in the mains, and their consequent blockage. Naphtha was found to be a rather effective solvent for these purposes, even in small concentrations. Where troubles with naphthalene developed, as it occasionally did even after the introduction of this minor carburettor, a team of workers was sent out to blow steam into the main and dissolve the blockage; still, prior to its introduction, naphthalene was a very major annoyance for the gasworks.\n\nThis steam or gas engine powered device compressed the gas for injection into the high-pressure mains, which in the early 1900s began to be used to convey gas over greater distances to the individual low pressure mains, which served the end-users. This allowed the works to serve a larger area and achieve economies of scale.\n\n"}
{"id": "9703269", "url": "https://en.wikipedia.org/wiki?curid=9703269", "title": "History of multitrack recording", "text": "History of multitrack recording\n\nMultitrack recording of sound is the process in which sound and other electro-acoustic signals are captured on a recording medium such as magnetic tape, which is divided into two or more audio tracks that run parallel with each other. Because they are carried on the same medium, the tracks stay in perfect synchronisation, while allowing multiple sound sources to be recorded asynchronously. The first system for creating stereophonic sound (using telephone technology) was demonstrated by Clément Ader in Paris in 1881. The pallophotophone, invented by Charles A. Hoxie and first demonstrated in 1922, recorded optically on 35 mm film, and some versions used a format of as many as twelve tracks in parallel on each strip. The tracks were recorded one at a time in separate passes and were not intended for later mixdown or stereophony; as with later half-track and quarter-track monophonic tape recording, the multiple tracks simply multiplied the maximum recording time possible, greatly reducing cost and bulk. British EMI engineer Alan Blumlein patented systems for recording stereophonic sound and surround sound on disc and film in 1933. The history of modern multitrack audio recording using magnetic tape began in 1943 with the invention of stereo tape recording, which divided the recording head into two tracks.\n\nThe next major development in multitrack recording came in the mid-1950s, when the Ampex corporation devised the concept of 8-track recording, utilizing its \"Sel-Sync\" (Selective Synchronous) recording system, and sold the first such machine to musician Les Paul. However, for the next 35 years, multitrack audio recording technology was largely confined to specialist radio, TV and music recording studios, primarily because multitrack tape machines were both very large and very expensive - the first Ampex 8-track recorder, installed in Les Paul's home studio in 1957, cost a princely US$10,000 - roughly three times the US average yearly income in 1957, and equivalent to around $90,000 in 2016. However, this situation changed radically in 1979 with the introduction of the TASCAM Portastudio, which used the inexpensive compact audio cassette as the recording medium, making good-quality 4-track (and later 8-track) multitrack recording available to the average consumer for the first time. Ironically, by the time the Portastudio had become popular, electronics companies were already introducing digital audio recording systems, and by the 1990s, computer-based digital multitrack recording systems such as Pro Tools and Cubase were being adopted by the recording industry, and soon became standard. By the early 2000s, rapid advances in home computing and digital audio software were making digital multitrack audio recording systems available to the average consumer, and high-quality digital multitrack recording systems like GarageBand were being included as a standard feature on home computers.\nStereo sound recording on tape was perfected in 1943 by German audio engineers working for the AEG corporation. Around 250 stereo tape recordings were made during this period (of which only three have survived), but the technology remained a closely guarded secret within Germany until the end of World War II. After the war, American audio engineer John T. Mullin and the Ampex corporation pioneered the commercial development of tape recording in the USA, and the technology was rapidly taken up by radio and the music industry due to its superior sound fidelity and because tape - being a linear recording medium - could be easily edited, by physically cutting and splicing the tape, to remove unwanted elements and create a 'perfect' recording. 2-track tape recording was rapidly adopted for modern music in the 1950s because it enabled signals from two or more separate microphones to be recorded simultaneously, enabling stereophonic recordings to be made and edited conveniently, which in turn facilitated the rapid expansion of the consumer high-fidelity (\"HiFi\") market. Stereo (either true binaural two-microphone stereo or multimixed) quickly became the norm for commercial classical recordings and radio broadcasts, although many pop music and jazz recordings continued to be issued in monophonic sound until the late 1960s.\n\nMuch of the credit for the development of multitrack recording goes to guitarist, composer and technician Les Paul, who lent his name to Gibson's first solid-body electric guitar. His experiments with tapes and recorders in the early 1950s led him to order the first custom-built eight-track recorder from Ampex, and his pioneering recordings with his then wife, singer Mary Ford. But it was Patti Page who was the first vocalist to record her own voice, sound on sound, with a song called 'Confess', in 1947: Bill Putnam, an engineer for Mercury Records, was able to overdub Page's voice, due to his well-known use of technology. Thus, Page became the first pop artist to overdub her vocals on a song. This was months before Les Paul and Mary Ford had their first multi-voiced release. Paul was the first to make use of the technique of multitracking to record separate elements of a musical piece asynchronously — that is, separate elements could be recorded at different times. Paul's technique enabled him to listen to the tracks he had already taped and record new parts in time alongside them. In 1963, solo jazz pianist, Bill Evans, recorded Conversations with Myself, an innovative solo album using the unconventional (in jazz solo recordings) technique of overdubbing over himself, in effect creating a two-piano duet of jazz improvisations.\n\nDespite Ampex having created the first 8-track tape machines for Les Paul and Atlantic Records, multitrack recording was taken up in a more limited way in the industry via 3-track recorders. These proved extremely useful for popular music, since they enabled backing music to be recorded on two tracks (either to allow the overdubbing of separate parts, or to create a full stereo backing track) while the third track was reserved for the lead vocalist. Three-track recorders remained in widespread commercial use until the mid-1960s and many famous pop recordings — including many of Phil Spector's so-called \"Wall of Sound\" productions and early Motown hits — were taped on 3-track recorders.\n\nThe next evolution was 4-track recording, which was the studio standard through the mid 1960s. Many of the most famous recordings by The Beatles and The Rolling Stones were recorded on 4-track, and the engineers at London's Abbey Road Studios became particularly adept at the technique called \"reduction mixes\" in the UK and \"bouncing down\" in the United States, in which multiple tracks were recorded onto one 4-track machine and then mixed together and transferred (bounced down) to one track of a second 4-track machine. In this way, it was possible to record literally dozens of separate tracks and combine them into finished recordings of great complexity.\n\nBy the mid-1960s, the ready availability of the most up-to-date multitrack recorders - which were by then standard equipment in the leading Los Angeles recording studios - enabled Brian Wilson of The Beach Boys to become one of the first pop producers to exploit the huge potential of multitrack recording. During the group's most innovative period of music-making, from 1964 to 1967, Wilson developed elaborate techniques for assembling the band's songs, which combined elements captured on both 4-track and 8-track recorders, as well as making extensive use of tape editing. By 1964, Wilson's increasingly complex arrangements had far outstripped the group's limited musical abilities - singer-guitarist Carl Wilson was the only group member who regularly contributed to these tracking sessions - so Wilson began routinely recording all the instrumental backing tracks for his songs using the team of top-rank professional studio musicians who came to be known as \"The Wrecking Crew\". For the group's landmark \"Pet Sounds\" album in 1965, Wilson recorded all the album's elaborate backing tracks using The Wrecking Crew and other session musicians, while the Beach Boys were away touring; the session musicians typically performed these instrumental tracks as ensemble performances, which were recorded and mixed live, direct to a 4-track recorder. When the other Beach Boys returned from touring, they moved to Columbia's Hollywood studio, which was equipped with the latest 8-track technology; by this time, Wilson and his engineers had 'reduced' the pre-recorded 4-track backing tracks to a mono mix, which was then dubbed onto one track of the 8-track master tape; Wilson then recorded the vocal tracks, assigning one individual track to each of the six vocalists (including soon-to-be permanent member Bruce Johnston), leaving the eighth track available for final 'sweetening' elements, such as additional vocal or instrumental touches, and lastly, all these elements were mixed down to the mono master tape. Nearly all of the Beach Boys' 4-track and 8-track masters from this period are preserved in Capitol's archive, allowing the label to release several expansive boxed sets of this music; \"The Pet Sounds Sessions\" (1997), includes nearly all the separate backing and vocal tracks from the album, as well as new stereo mixes of all the songs, while the 9-CD \"The Smile Sessions\" (2011) features a wide cross-section of the huge amount of instrumental and vocal material (totalling around 50 hours of recordings) that was recorded for the group's never-completed 1967 \"magnum opus\", \"Smile\".\n\nAll of the Beatles classic mid-1960s recordings, including the albums \"Revolver\" and \"Sgt Pepper's Lonely Hearts Club Band\", were recorded in this way. There were limitations, however, because of the build-up of noise during the bouncing-down process, and the Abbey Road engineers are still justly famed for the ability to create dense multitrack recordings while keeping background noise to a minimum.\n\n4-track tape also led to a related development, quadraphonic sound, in which each of the four tracks was used to simulate a complete 360-degree surround sound. A number of albums including Pink Floyd's \"The Dark Side of the Moon\" and Mike Oldfield's \"Tubular Bells\" were released both in stereo and quadrophonic format in the 1970s, but 'quad' failed to gain wide commercial acceptance. Although it is now sometimes considered a gimmick, it was the direct precursor of the surround sound technology that has become standard in many modern home theater systems.\n\nIn a professional audio setting today, such as a recording studio, audio engineers may use 64 tracks or more for their recordings, utilizing one or more tracks for each instrument played.\n\nThe combination of the ability to edit via tape splicing, and the ability to record multiple tracks, revolutionized studio recording. It became common studio recording practice to record on multiple tracks, and mix down afterward. The convenience of tape editing and multitrack recording led to the rapid adoption of magnetic tape as the primary technology for commercial musical recordings. Although 33⅓ rpm and 45 rpm vinyl records were the dominant consumer format, recordings were customarily made first on magnetic tape, then transferred to disc, with Bing Crosby leading the way in the adoption of this method in the United States.\n\nThe original Ampex 8-track recorder (not to be confused with 8-track tape endless-loop cartridge players), model 5258, was an internal Ampex project. It was based on an Ampex 1\" data recorder transport with modified Ampex model 350 electronics. It stood over tall and weighed . 8 tracks were chosen because that was the number of recording tracks with guard tracks that would fit on a recording tape, the widest tape available at the time.\n\nThe first of the Ampex 8-track recorders was sold to Les Paul for $10,000 in 1957 and was installed in his home recording studio by David Sarser. It became known as the \"Octopus\".\n\nThe second Ampex model 5258 8-track was sold to Atlantic Records at Tom Dowd's insistence in late 1957. Atlantic was the first record company to use a multi-track (as opposed to stereo or 3-track) recorder in their studio.\n\nMulti-track recording differs from overdubbing and sound on sound because it records separate signals to individual tracks. Sound on sound which Les Paul invented adds a new performance to an existing recording by placing a second playback head in front of the erase head to play back the existing track before erasing it and re-recording a new track.\n\nMulti-track recorders also differ from early stereo and three track recorders that were available at the time in that they can record individual tracks while preserving the other tracks. The original multi-channel recorders could only record all tracks at once.\n\nThe earliest multitrack recorders were analog magnetic tape machines with two or three tracks. Elvis Presley was first recorded on multitrack during 1957, as RCA's engineers were testing their new machines. Buddy Holly's last studio session in 1958 employed three-track, resulting in his only stereo releases not to include overdubs. The new three-track system allowed the lead vocal to be recorded on a dedicated track, while the remaining two tracks could be used to record the backing tracks in full stereo.\n\nFrank Zappa experimented in the early 1960s with a custom built 5-track recorder built by engineer Paul Buff in his Pal Recording Studio in Rancho Cucamonga, California. Buff later went on to work in larger Hollywood studios. However, recorders with four or more tracks were restricted mainly to major American recording studios until the mid-to-late 1960s, mainly because of import restrictions and the high cost of the technology. In England, pioneering independent producer Joe Meek produced all of his innovative early 1960s recordings using monophonic and two-track recorders. Like Meek, EMI house producer George Martin was considered an innovator for his use of two-track as a means to making better mono records, carefully balancing vocals and instruments; Abbey Road Studios installed Telefunken four-track machines in 1959 and 1960 (replaced in 1965 by smaller, more durable Studer machines), but The Beatles would not have access to them until late 1963, and all recordings prior to their first world hit single \"I Want to Hold Your Hand\" (1964) were made on two-track machines.\n\nThe artistic potential of the multitrack recorder came to the attention of the public in the 1960s, when artists such as the Beatles and the Beach Boys began to multitrack extensively, and from then on virtually all popular music was recorded in this manner. The technology developed very rapidly during these years. At the start of their careers, the Beatles and Beach Boys each recorded live to mono, two-track (the Beatles), or three-track (the Beach Boys); by 1965 they used multitracking to create pop music of unprecedented complexity.\n\nThe Beach Boys' acclaimed 1966 LP \"Pet Sounds\" relied on multitrack recorders for its innovative production. Brian Wilson pretaped all the instrumental backing tracks with a large ensemble, recording the performances live, direct to a four-track recorder. These four-track backing tapes were then 'dubbed down' to one track of an eight-track tape. Six of the remaining seven tracks were then used to individually record the vocals of each member of The Beach Boys, and the eighth track was reserved for any final 'sweetening' overdubs of instruments or voices.\n\n3M introduced the 1-inch eight-track version of their model M-23 recorder in 1966, probably the first mass-produced machine of this format. It remained in production until 1970 and was used by many top studios worldwide including Abbey Road Studios in London. Both Pete Townshend and John Lennon had 3M 8-track machines in their home project studios c. 1969-1970. Ampex began mass production of their competing 1-inch eight-track MM1000 in 1967. One of the first 8-track machines in Los Angeles was built by Scully Recording Instruments of Bridgeport, Connecticut and installed at American Recorders in late 1967. The debut album by Steppenwolf was recorded there and was released in January 1968.\n\nBecause The Beatles did not gain access to eight-track recorders until 1968, their groundbreaking \"Sgt Pepper's Lonely Hearts Club Band\" LP (1967) was created using pairs of four-track machines; the group also used vari-speed (also called pitch shift) to achieve unique sounds, and they were the first group in the world to use an important offshoot of multitrack recording, the Automatic Double Tracking (ADT) system invented by Abbey Road staff engineer Ken Townsend in 1966.\n\nOther artists began experimenting with multitrack's possibilities also, with the Music Machine (of \"Talk Talk\" fame) recording on a custom-built ten-track setup, and Pink Floyd collaborating with former Beatles recording engineer Norman \"Hurricane\" Smith, who produced their first albums.\n\nThe first eight-track recorder in the UK was built by Scully and installed at London's Advision Studios in early 1968. Among the first eight-track recordings made there were the single \"Dogs\" by The Who and the album \"My People Were Fair and Had Sky in Their Hair... But Now They're Content to Wear Stars on Their Brows\" by the band Tyrannosaurus Rex. Trident Studios obtained their first eight-track recorder soon afterward. It was during The Beatles' recording of their \"White Album\" sessions of late 1968 that EMI's Abbey Road Studios finally had eight-track recorders installed, until then the group went to Trident to record with eight-tracks. The Beatles used eight-track to record portions of the \"White Album\", the single \"Hey Jude\" and the later \"Abbey Road\".\n\nOther western countries also lagged well behind the USA – in Australia, the largest local recording label, Festival Records, did not install a four-track recorder until late 1966; the first eight-track recorders did not appear there until the late 1960s.\n\nIn 1967 Ampex built its first prototype 16-track professional audio recorder at the request of Mirasound Studios in New York City. This machine used reels of 2-inch tape on a modified tape transport system originally built for video recording. In 1968 it introduced the 16-track production model MM-1000, the first commercially available 16-track machine. Machines of this size are difficult to move and costly to maintain. Prices were very high, typically $10,000 to $30,000 U.S. dollars.\n\nOne of the first 16-track recorders was installed at CBS Studios in New York City where it was used to record the second album by \"Blood, Sweat & Tears\" released in December 1968. The Grateful Dead released their first 16-track recordings \"Aoxomoxoa\" in June 1969 and \"Live/Dead\" in November 1969. TTG Studios in Los Angeles built its own 16-track machine in 1968. This was used on Frank Zappa's album \"Hot Rats\" released in October 1969. \"Volunteers\" by Jefferson Airplane was released in November 1969. The back of the Jefferson Airplane album cover includes a picture of the 16-track MM-1000.\n\nAdvision and Trident were also among the first in the U.K. to install 16-track machines. Trident installed its first 16-track machine in late 1969. \"After The Flood\", a song from the Van der Graaf Generator album \"The Least We Can Do Is Wave To Each Other\", was recorded at this studio on 16 tracks in December 1969. Production of 16-track machines boomed and the number of studios worldwide using these machines exploded during 1970 and 1971. By the end of 1971 there were at least 21 studios in London using 16-track recorders in conjunction with Dolby noise reduction. Groups using Trident at this time also included Genesis and David Bowie as well as Queen who experimented with multitracking extensively most prominently on their albums Queen II and \"A Night at the Opera\".\n\nAustralia's first sixteen-track recorder was installed at Armstrong's Studios in Melbourne in 1971; Festival installed Australia's first 24-track recorder at its Sydney studio in 1974. During the 1970s, sixteen, twenty-four, and thirty-two tracks became common in professional studios, with recording tape reaching two and three inches (5.08 cm - 7.62 cm) wide. The so-called \"golden age\" of large format professional analog recorders would last into the 1990s when the technology was mostly replaced with digital tape machines, and later on, computer systems using hard disk drives instead of tape. Some music producers and musicians still prefer working with the sound of vintage analog recording equipment despite the additional costs and difficulties involved.\n\nLarge format analog multitrack machines can have up to 24 tracks on a tape two inches wide which is the widest analog tape that is generally available. Prototype machines, by MCI in 1978, using 3\" tape for 32 tracks never went into production, though Otari made a 32 track 2\" MX-80. A few studios still operate large format analog recorders, though much of the time their use is only to copy sounds onto a modern digital format. Maintaining these machines has become increasingly difficult as new parts are rarely available. New tape is still available but prices have risen significantly in recent years.\n\nIn 1972 TEAC marketed their consumer 4-channel quadraphonic tape recorders for use as home multitrack recorders. The result were the popular TEAC 2340 and 3340 models. Both used ¼ inch tape. The 2340 ran at either 3¾ or 7½ inches per second and used 7-inch reels while the 3340 ran at 7½ or 15 inches per second and used 10½ inch reels. The 2340 was priced at under U.S. $1,000 making it very popular for home use.\n\nThe advent of the compact audio cassette (developed in 1963) ultimately led to affordable, portable four-track machines such as the Tascam Portastudio which debuted in 1979. Cassette-based machines could not provide the same audio quality as reel-to-reel machines, but served as a useful tool for professional and semi-pro musicians in making song demos. The Portastudio had a revolutionary effect on the emerging punk rock genre, because it enabled young bands to make recordings without signing to a record label. In the early years of punk, many bands self-produced their own recordings and sold them at gigs and by putting advertisements in underground zines. Bruce Springsteen's 1982 album \"Nebraska\" was made this way, with Springsteen choosing the album's earlier demo versions over the later studio recordings.\n\nThe familiar tape cassette was designed to accommodate four channels of audio – in a commercially recorded cassette these four tracks would normally constitute the stereo channels (each consisting of two tracks) for both 'sides' of the cassette – in a four-track cassette recorder all four tracks of a cassette are utilized together, often with the tape running at twice the normal speed (3¾ instead of 1⅞ inches per second) for increased fidelity. A separate signal can be recorded on to each of four tracks. (As such, the four-track machine does not utilize the two separate sides of the cassette in the conventional sense; if the cassette is inserted the other way round, all four tracks play in reverse.) As with professional machines, two or more tracks can be bounced down to one. When recording is complete, the volume level of each track is optimized, electronic effects such as reverb are added to certain tracks where desired, each track is separately 'panned' to the desired point in the stereo field and the resulting stereo signal is mixed down to a separate stereo machine (such as a conventional cassette recorder).\n\nBy the early 1970s, Thomas Stockham of Sound Stream Digital, created the first practical use of pulse code modulation, also called PCM digital recording, for high fidelity purposes. The first to be released were rereleased cleaned up versions of acoustic recordings made by the great tenor, Enrico Caruso. Early computer algorithms were used in the process of cleaning up the scratchy old 78 RPM records. The process could not be done in \"real time\" as the early computers were not very powerful or fast, compared to 2010-era computers. All of the data had to be stored on linear digital tape and then played back, in real time. The actual ingest from the 78 RPM records to the digital tape was also done in real-time. The computer processing to clean up the surface noise, pops and scratches took the early computers quite some time to process.\n\nBy the late 1970s, 3M introduced the first digital multi-track recorder. It utilized 1-inch wide tape and recorded 32 tracks. Unlike analog tape, edits could not be accomplished with a grease pencil, razor blades and splicing tape. So a secondary 4 track editing & mix down recorder was also created with an electronically-controlled edit controller to make effective digital edits. This early system used a 16-bit digital \"word\". The only converters of the day were 12 bit & 4 bit. So two were cascaded/daisy-chained to create the necessary 16-bit \"word\" for 96 DB of dynamic range. The signal was then sampled faster than any other digital recordings made up until that time at 50,000 times per second (50 kHz). It was known to be the best sounding of all the later digital multi-track recorders because their use of 50 kHz sampling did not become the industry standards later established as 44.1 kHz for CD's & 48 kHz for digital video.\n\nThe accepted world standard was created by Sony along with Philips. Sony created a 24 track digital recorder and Mitsubishi Corporation created a different 32 track digital recorder. The Mitsubishi recorded their data differently and it could be edited, the old-fashioned analog way, with a razor blade and splicing tape. The Sony used 1/2 inch tape whereas the Mitsubishi used 1 inch wide tape. So the first recordings that were released produced on the 3M, 32 track digital recorder were still analog vinyl releases, since the CD had yet to be invented. These professional linear tape digital recorders established the \"DASH\" format meaning, \" Digital Audio Stationary Head\". By the time the other manufacturers released their digital multi-track recorders, the CD had already been developed. The sampling rate dictates the upper range of the frequency response, whereas the bit depth dictates the dynamic range and signal to noise ratios.\n\nStarting in 1992, the ALESIS Corporation, a company which made digital drum machines and inexpensive analog audio mixers introduced the first multitrack, eight track, project studio, digital 8 track machine. It was named the ADAT, after the earlier 2 track digital recorders of the time known as DAT (Digital Audio Tape), which were based upon a small spinning head, similar to a consumer video recorder. The ADAT machine recorded its data in an already well-established consumer format based on VHS videotape recorder technology. Eight separate data tracks were recorded within the same bandwidth it took to record a TV show on a home video recorder (VHS). Numerous machines could be electronically locked together with a single cable. You could plug in enough 8 track machines together to create one giant 128 track machine. And like the professional studio recorders before it, a large full function remote control was also available.\n\nThe following year, the TEAC/TASCAM Corporation, introduced their DA-88. Those used the smaller 8 mm video format tapes. Those recorded four duplexed pairs of data tracks and would require a \"read before write\" function for overdubbing purposes of adjacent tracks. A full size remote and remote metering was also made available. Later units introduced by both companies provided for higher bit depths such as 20 & 24 bit. These machines like the early home studio TEAC's before them, slashed the prices of professional digital multitrack recording. It changed the recording industry forever.\n\nBy the late 1990s, dedicated multitrack recorders faded with the introduction of the Macintosh operating system and Windows operating systems in personal computers. Some of the first companies jumping on board with this technology were New England Digital and Digidesign, from the US & Fairlight, from Australia. Through the 1990s, multitrack recorders became digital, using a variety of technologies and media types. These including digital tape format (such as ADAT), or in some cases Minidiscs.\n\nSome of the leading providers of multitrackers were Tascam (hard drive or cassette based), Alesis (ADAT digital tape based), Roland/Boss (hard drive based), Fostex (hard drive based), Yamaha (hard drive based), and Korg.\n\nA highly competitive market and rapidly falling costs for this equipment has made it common to find multitrack recording technology outside a typical recording studio.\n\nThe first software-based digital multi-track recorder, called Deck, was released in 1990. The core engine technology and much of the user interface was programmed and designed by Josh Rosen, Mats Myrberg and John Dalton from a small San Francisco based company. They formed the platform upon which Pro Tools was built in 1991. The same technology lay behind the 1992 release of Cubase Audio, the first version to offer audio support in addition to MIDI sequencing capabilities.\n\nWhile hardware costs have fallen the power of the personal computer have increased, so that in the 2010s, a good quality home computer is sufficiently powerful to serve as a complete multitrack recorder, if a band or performer has a USB microphone or a regular microphone and a sound card adapter, using inexpensive hardware and software. Since 2012, the multitracking software GarageBand is offered as a free download for all of Apple's new computers or $4.99 for older models. GarageBand added to the many free or under $100 solutions available to the Windows platform that run on less expensive but often more powerful hardware. In a price range between $150 and usually under $1000, superior software mimicking complex recording studios that once could cost a hundred thousand dollars, or more, are also available However solutions this powerful are not needed for most applications. This is a far cry from the days when multitrack recorders cost thousands of dollars and few people could afford them. In early (circa) 2000 the availability of low cost CakeWalk for windows provided opportunities for many to get a start in multi-track digital recording on the Windows for around $50 to $100. After hardware became more powerful, more capabilities became available, including more digital channels and plug in effects.\n\nIn the 2010s, the availability of inexpensive software coupled with low cost hardware solutions, have enabled many singer-songwriters to self-produce their first recordings without paying high fees for a professional recording studio or audio engineer. The most popular DAW today are: Pro Tools, Reaper, Live, Cubase, Logic and Digital Performer.\n\n"}
{"id": "34466806", "url": "https://en.wikipedia.org/wiki?curid=34466806", "title": "ITyphoon", "text": "ITyphoon\n\n\n\n"}
{"id": "31816986", "url": "https://en.wikipedia.org/wiki?curid=31816986", "title": "Librestream", "text": "Librestream\n\nLibrestream Technologies Inc. is a privately owned, venture capital–backed company based in Winnipeg, Canada. Librestream provides technologies that enable mobile enterprise collaboration.\n\nLibrestream is notable in that its technology, consisting of unique hand-held mobile devices and accompanying software, extends traditional video conferencing and collaborative services to places previously unreachable such as an off-shore oil rig or a manufacturing plant floor a continent away.\n\nMobile collaboration is a technology-based process of communicating utilizing electronic assets and accompanying software designed for use in remote locations. Newest generation hand-held electronic devices include video, audio, and telestration (on-screen drawing) capabilities broadcast over secure networks, enabling multi-party conferencing in real time.\n\nDiffering from traditional video conferencing, mobile collaboration utilizes wireless, cellular and broadband technologies enabling effective collaboration independent of location. Where traditional video conferencing has been limited to boardrooms, offices, and lecture theatres, recent technological advancements have extended the capabilities of video conferencing for use with discreet, hand-held mobile devices, permitting true mobile collaborative possibilities.\n\nThe origins of Librestream date back to the late 1980s in Winnipeg, Canada, when Kerry Thacher co-founded Ubitrex Corporation, a small high-tech start-up that designed and developed a clinical information system for use in hospitals to capture patient data. The system included a hand-held device designed for use by clinicians. Subsequent to successful clinical use of this system, Ubitrex was sold in 1994 to U.S-based Continental Healthcare Systems.\n\nTwo years later, Thacher bought back the device side of the business from Continental and formed AirWire Mobile Technologies Inc. which continued to support the substantial base of hospitals using the technology. Only a few weeks into AirWire’s operation, Symbol Technologies Inc., a large U.S-based manufacturer of mobile devices, took notice of AirWire’s unique expertise and contracted with Airwire to create a product to help Symbol pursue the healthcare market.\n\nIn 1999, Symbol bought AirWire. Over the next few years, the team, continuing to operate out of Winnipeg, successfully designed and developed a number of mobile devices, one of which delivered Voice over IP (VoIP), a significant advancement in mobile device technology at that time. This activity led to the development of high-volume hand-held mobile expertise in Winnipeg for the first time, a capability that would later form the foundation for the creation of Librestream.\n\nIn August 2003 Thacher left Symbol, and weeks later Symbol closed its Winnipeg office. Although many of the engineers received offers from Symbol to re-locate to New York, all declined. Later in 2003, a group of eight professionals from the former Winnipeg arm of Symbol gathered to plan their next venture. In addition to Thacher, this group included Bill Gillanders, Rob McConnell, Don Freiling, Tim Braun, Kent Wotherspoon, Conway Wieler, and Chris Kavanagh.\n\nTwo things were clear: the emergence and continued growth of the Internet was certain, and there existed a tangible opportunity to develop enhanced video-handling capabilities for a next generation of hand-held devices. The group was convinced that together they could design and develop the necessary technology to allow individuals, regardless of location, the ability to collaborate in new ways. The technology would extend the boundaries of traditional video-conferencing beyond the boardroom to workplaces previously unreachable such as a manufacturing plant floor a continent away.\n\nLed by the founding team’s vision, Librestream (a combination of ‘free’ and stream’) was formed. Backed by venture capital and individual investors, Librestream developed their first alpha product, the MCD-1000, coupled with desktop collaboration software, MCA, in 2006. Eventually the mobile collaborative device evolved into the Onsight 1000, one of multiple rugged hand-held devices in the Onsight product line. The desktop collaboration software evolved into the Onsight Expert application.\n\nIn 2006, Librestream formed a marketing partnership with Tandberg, one of the leading video conferencing industry players at that time. The arrangement offered Librestream industry exposure and allowed the breadth and depth of the product to grow in response to evolving requirements. After the arrangement expired in July 2010, Librestream continued to develop the technology and support their growing customer base.\n\nBy 2010, Librestream’s customer base grew to include global enterprises, many of them Fortune 500 firms, in industries such as manufacturing, energy, healthcare, insurance, government and public safety.\n\nThe impact of mobile collaboration technology is significant in its potential to change the way people work. Live, visual interaction removes traditional restrictions of distance and time. Business processes are optimized through accelerated problem resolution, reductions in downtimes and travel, improvements in customer service and increased productivity.\n\nLibrestream works with three strategic partners. With Cisco Systems, Librestream is a Registered Cisco Developer Network member, and Onsight is a core component of the Cisco Manufacturing Mobile Video Collaboration (MMVC) solution. With Inmarsat, Librestream has successfully tested the Onsight system over the Inmarsat BGAN satellite network to provide mobile collaboration to land and maritime satellite customers. With Verizon Wireless, Librestream has tested and optimized the Onsight mobile devices for 4G LTE networks.\n\n"}
{"id": "1074436", "url": "https://en.wikipedia.org/wiki?curid=1074436", "title": "List of Cape Canaveral and Merritt Island launch sites", "text": "List of Cape Canaveral and Merritt Island launch sites\n\nCape Canaveral and adjacent Merritt Island on Florida's Atlantic coast are home to two American spaceports, one civilian and one military, servicing several active launch sites.\n\nThe civilian John F. Kennedy Space Center, operated by NASA, has one launch complex with two pads on Merritt Island. From 1968–1975, it was the site of 13 Saturn V launches, three manned Skylab flights and the Apollo-Soyuz Test Project; all Space Shuttle flights from 1981–2011, and one Ares 1-X flight in 2009. \nThe military Cape Canaveral Air Force Station (CCAFS), operated by the 45th Space Wing of the U.S. Air Force, was the site of all U.S. manned launches before Apollo 8, as well as many other early Department of Defense (DoD) and NASA launches. For the DoD, it plays a secondary role to Vandenberg AFB in California, but is the launch site for many NASA unmanned space probes, as those spacecraft are typically launched on Air Force launchers. Active launch vehicles are in bold.\n\nMuch of the support activity for CCAFS occurs at Patrick Air Force Base to the south, its reporting base.\n\n, the U.S. Air Force committed to lease Cape Canaveral Air Force Station Space Launch Complex 36 to Space Florida for future use by the Athena III launch system. It is not known if the plan was subsequently implemented. Blue Origin leased Complex 36 in 2015, with plans to launch its reusable orbital vehicle from there by 2020. \n\n"}
{"id": "59024", "url": "https://en.wikipedia.org/wiki?curid=59024", "title": "List of knots", "text": "List of knots\n\nThis list of knots includes many alternate names for common knots and lashings. Knot names have evolved over time and there are many conflicting or confusing naming issues. The overhand knot, for example, is also known as the thumb knot. The figure-eight knot is also known as the savoy knot or the Flemish knot.\n\n\n\n\n\n"}
{"id": "43738567", "url": "https://en.wikipedia.org/wiki?curid=43738567", "title": "List of ovens", "text": "List of ovens\n\nThis is a list of ovens. An oven is a thermally insulated chamber used for the heating, baking or drying of a substance, and most commonly used for cooking. Kilns and furnaces are special-purpose ovens, used in pottery and metalworking, respectively.\n\nBaking is a food cooking method that uses prolonged dry heat by convection, rather than by thermal radiation, normally in an oven, but also in hot ashes, or on hot stones. Bread is a commonly baked food.\n\nAn earth oven, or cooking pit, is one of the most simple and long-used cooking structures. At its simplest, an earth oven is a pit in the ground used to trap heat and bake, smoke, or steam food. Earth ovens have been used in many places and cultures in the past, and the presence of such cooking pits is a key sign of human settlement often sought by archaeologists. They remain a common tool for cooking large quantities of food where no equipment is available.\nIndustrial ovens are heated chambers used for a variety of industrial applications, including drying, curing, or baking components, parts or final products. Industrial ovens can be used for large or small volume applications, in batches or continuously with a conveyor line, and a variety of temperature ranges, sizes and configurations.\nA kiln is a thermally insulated chamber, a type of oven, that produces temperatures sufficient to complete some process, such as hardening, drying, or chemical changes. Various industries and trades use kilns to harden objects made from clay into pottery, bricks etc. Various industries use rotary kilns for pyroprocessing—to calcinate ores, produce cement, lime, and many other materials.\n"}
{"id": "7697949", "url": "https://en.wikipedia.org/wiki?curid=7697949", "title": "List of technology centers", "text": "List of technology centers\n\nThis is a list of technology centers throughout the world. Governmental planners and business networks like to use the name \"silicon\" or \"valley\" to describe their own areas as a result of the success of Silicon Valley in California. Nevertheless, there are a few qualitative differences between these places, and metrics may be applied to measure their dominance.\n\nThese metrics include:\n\n\nCameroon\n\nEgypt\n\nKenya\n\nMauritius\n\nMorocco\n\nSouth Africa\n\nZambia\n\nBolivia\n\nBrazil\n\nCanada\nChile\nGuatemala\n\nMexico\n\nUnited States\n\nChina\n\nHong Kong\n\nIndia\n\nIran\n\n\nIsrael\n\nJapan\n\nMalaysia\n\nMyanmar\n\n\nPakistan\n\nPhilippines\n\nQatar\n\nSaudi Arabia\n\nSingapore\n\nSouth Korea\n\nTaiwan\n\n\nThailand\n\nUnited Arab Emirates\n\nVietnam\n\n\nAustria\n\nBelarus\n\nCzech Republic\n\nFinland\n\nFrance\nGermany\n\nHungary\nIreland\n\nItaly\n\nNetherlands\n\nPortugal\n\nRussia\n\nRomania\n\nSlovakia\n\nSpain\n\nSweden\n\nTurkey\n\nUnited Kingdom\n\n\nUkraine\n\n\nThe following list contains places with \"Silicon\" names, that is, places with nicknames inspired by the \"Silicon Valley\" nickname given to part of the San Francisco Bay Area:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUkraine\n\n\n\n\n\n\n"}
{"id": "6916771", "url": "https://en.wikipedia.org/wiki?curid=6916771", "title": "MVDS", "text": "MVDS\n\nMVDS is an acronym for terrestrial \"Multipoint Video Distribution System.\"\n\nMVDS currently is a part of broader MWS (Multimedia Wireless System) standards.\nIn European Union MWS works in 10.7 - 13.5 and 40.5 - 43.5 GHz frequency bands.\n\nResearch for 42 GHz frequency has been done under the European Commition EMBRACE (Efficient Millimetre Broadband Radio Access for Convergence and Evolution) initiative.\n\n\n\n\n"}
{"id": "48144", "url": "https://en.wikipedia.org/wiki?curid=48144", "title": "Microcomputer", "text": "Microcomputer\n\nA microcomputer is a small, relatively inexpensive computer with a microprocessor as its central processing unit (CPU). It includes a microprocessor, memory, and minimal input/output (I/O) circuitry mounted on a single printed circuit board. Microcomputers became popular in the 1970s and 1980s with the advent of increasingly powerful microprocessors. The predecessors to these computers, mainframes and minicomputers, were comparatively much larger and more expensive (though indeed present-day mainframes such as the IBM System z machines use one or more custom microprocessors as their CPUs). Many microcomputers (when equipped with a keyboard and screen for input and output) are also personal computers (in the generic sense).\n\nThe abbreviation \"micro\" was common during the 1970s and 1980s, but has now fallen out of common usage.\n\nThe term \"microcomputer\" came into popular use after the introduction of the minicomputer, although Isaac Asimov used the term in his short story \"The Dying Night\" as early as 1956 (published in \"The Magazine of Fantasy and Science Fiction\" in July that year). Most notably, the microcomputer replaced the many separate components that made up the minicomputer's CPU with one integrated microprocessor chip.\n\nThe French developers of the Micral N (1973) filed their patents with the term \"Micro-ordinateur\", a literal equivalent of \"Microcomputer\", to designate a solid state machine designed with a microprocessor.\nIn the USA, the earliest models such as the Altair 8800 were often sold as kits to be assembled by the user, and came with as little as 256 bytes of RAM, and no input/output devices other than indicator lights and switches, useful as a proof of concept to demonstrate what such a simple device could do. \nHowever, as microprocessors and semiconductor memory became less expensive, microcomputers in turn grew cheaper and easier to use:\nAll these improvements in cost and usability resulted in an explosion in their popularity during the late 1970s and early 1980s.\nA large number of computer makers packaged microcomputers for use in small business applications. By 1979, many companies such as Cromemco, Processor Technology, IMSAI, North Star Computers, Southwest Technical Products Corporation, Ohio Scientific, Altos Computer Systems, Morrow Designs and others produced systems designed either for a resourceful end user or consulting firm to deliver business systems such as accounting, database management, and word processing to small businesses. This allowed businesses unable to afford leasing of a minicomputer or time-sharing service the opportunity to automate business functions, without (usually) hiring a full-time staff to operate the computers. A representative system of this era would have used an S100 bus, an 8-bit processor such as an Intel 8080 or Zilog Z80, and either CP/M or MP/M operating system.\nThe increasing availability and power of desktop computers for personal use attracted the attention of more software developers. In time, and as the industry matured, the market for personal computers standardized around IBM PC compatibles running DOS, and later Windows.\nModern desktop computers, video game consoles, laptops, tablet PCs, and many types of handheld devices, including mobile phones, pocket calculators, and industrial embedded systems, may all be considered examples of microcomputers according to the definition given above.\n\nEveryday use of the expression \"microcomputer\" (and in particular the \"micro\" abbreviation) has declined significantly from the mid-1980s and has declined in commonplace usage since 2000. The term is most commonly associated with the first wave of all-in-one 8-bit home computers and small business microcomputers (such as the Apple II, Commodore 64, BBC Micro, and TRS 80). Although, or perhaps because, an increasingly diverse range of modern microprocessor-based devices fit the definition of \"microcomputer\", they are no longer referred to as such in everyday speech.\n\nIn common usage, \"microcomputer\" has been largely supplanted by the term \"personal computer\" or \"PC\", which specifies a computer that has been designed to be used by one individual at a time, a term first coined in 1959. IBM first promoted the term \"personal computer\" to differentiate themselves from other microcomputers, often called \"home computers\", and also IBM's own mainframes and minicomputers. However, following its release, the IBM PC itself was widely imitated, as well as the term. The component parts were commonly available to producers and the BIOS was reverse engineered through cleanroom design techniques. IBM PC compatible \"clones\" became commonplace, and the terms \"personal computer\", and especially \"PC\", stuck with the general public, often specifically for a DOS or (nowadays) Windows-compatible computer.\n\nSince the advent of microcontrollers (monolithic integrated circuits containing RAM, ROM and CPU all onboard), the term \"micro\" is more commonly used to refer to that meaning.\n\nMonitors, keyboards and other devices for input and output may be integrated or separate. Computer memory in the form of RAM, and at least one other less volatile, memory storage device are usually combined with the CPU on a system bus in one unit. Other devices that make up a complete microcomputer system include batteries, a power supply unit, a keyboard and various input/output devices used to convey information to and from a human operator (printers, monitors, human interface devices). Microcomputers are designed to serve only one user at a time, although they can often be modified with software or hardware to concurrently serve more than one user. Microcomputers fit well on or under desks or tables, so that they are within easy access of users. Bigger computers like minicomputers, mainframes, and supercomputers take up large cabinets or even dedicated rooms.\n\nA microcomputer comes equipped with at least one type of data storage, usually RAM. Although some microcomputers (particularly early 8-bit home micros) perform tasks using RAM alone, some form of secondary storage is normally desirable. In the early days of home micros, this was often a data cassette deck (in many cases as an external unit). Later, secondary storage (particularly in the form of floppy disk and hard disk drives) were built into the microcomputer case.\n\nAlthough they did not contain any microprocessors, but were built around transistor-transistor logic (TTL), Hewlett-Packard calculators as far back as 1968 had various levels of programmability comparable to microcomputers. The HP 9100B (1968) had rudimentary conditional (if) statements, statement line numbers, jump statements (go to), registers that could be used as variables, and primitive subroutines. The programming language resembled assembly language in many ways. Later models incrementally added more features, including the BASIC programming language (HP 9830A in 1971). Some models had tape storage and small printers. However, displays were limited to one line at a time. The HP 9100A was referred to as a personal computer in an advertisement in a 1968 Science magazine, but that advertisement was quickly dropped. HP was reluctant to sell them as \"computers\" because the perception at that time was that a computer had to be big in size to be powerful, and thus decided to market them as calculators. Additionally, at that time, people were more likely to buy calculators than computers, and, purchasing agents also preferred the term \"calculator\" because purchasing a \"computer\" required additional layers of purchasing authority approvals. HP virtual museum\n\nThe Datapoint 2200, made by CTC in 1970, was also comparable to microcomputers. While it contains no microprocessor, the instruction set of its custom TTL processor was the basis of the instruction set for the Intel 8008, and for practical purposes the system behaves approximately as if it contains an 8008. This is because Intel was the contractor in charge of developing the Datapoint's CPU, but ultimately CTC rejected the 8008 design because it needed 20 support chips.\n\nAnother early system, the Kenbak-1, was released in 1971. Like the Datapoint 2200, it used discrete transistor–transistor logic instead of a microprocessor, but it functioned like a microcomputer in some ways. It was marketed as an educational and hobbyist tool, but it was not a commercial success; production ceased shortly after introduction.\n\nIn late 1972, a French team headed by François Gernelle within a small company, Réalisations & Etudes Electroniqes (R2E), developed and patented a computer based on a microprocessor – the Intel 8008 8-bit microprocessor. This Micral-N was marketed in early 1973 as a \"Micro-ordinateur\" or \"microcomputer\", mainly for scientific and process-control applications. About a hundred Micral-N were installed in the next two years, followed by a new version based on the Intel 8080. Meanwhile, another French team developed the Alvan, a small computer for office automation which found clients in banks and other sectors. The first version was based on LSI chips with an Intel 8008 as peripheral controller (keyboard, monitor and printer), before adopting the Zilog Z80 as main processor.\n\nIn late 1972, a Sacramento State University team led by Bill Pentz built the Sac State 8008 computer, able to handle thousands of patients' medical records. The Sac State 8008 was designed with the Intel 8008. It had a full set of hardware and software components: a disk operating system included in a series of programmable read-only memory chips (PROMs); 8 Kilobytes of RAM; IBM's Basic Assembly Language (BAL); a hard drive; a color display; a printer output; a 150 bit/s serial interface for connecting to a mainframe; and even the world's first microcomputer front panel.\n\nIn early 1973, Sord Computer Corporation (now Toshiba Personal Computer System Corporation) completed the SMP80/08, which used the Intel 8008 microprocessor. The SMP80/08, however, did not have a commercial release. After the first general-purpose microprocessor, the Intel 8080, was announced in April 1974, Sord announced the SMP80/x, the first microcomputer to use the 8080, in May 1974.\n\nVirtually all early microcomputers were essentially boxes with lights and switches; one had to read and understand binary numbers and machine language to program and use them (the Datapoint 2200 was a striking exception, bearing a modern design based on a monitor, keyboard, and tape and disk drives). Of the early \"box of switches\"-type microcomputers, the MITS Altair 8800 (1975) was arguably the most famous. Most of these simple, early microcomputers were sold as electronic kits—bags full of loose components which the buyer had to solder together before the system could be used.\n\nThe period from about 1971 to 1976 is sometimes called the of microcomputers. Many companies such as DEC, National Semiconductor, Texas Instruments offered their microcomputers for use in terminal control, peripheral device interface control and industrial machine control. There were also machines for engineering development and hobbyist personal use. In 1975, the Processor Technology SOL-20 was designed, which consisted of one board which included all the parts of the computer system. The SOL-20 had built-in EPROM software which eliminated the need for rows of switches and lights. The MITS Altair just mentioned played an instrumental role in sparking significant hobbyist interest, which itself eventually led to the founding and success of many well-known personal computer hardware and software companies, such as Microsoft and Apple Computer. Although the Altair itself was only a mild commercial success, it helped spark a huge industry.\n\nBy 1977, the introduction of the second generation, known as home computers, made microcomputers considerably easier to use than their predecessors because their predecessors' operation often demanded thorough familiarity with practical electronics. The ability to connect to a monitor (screen) or TV set allowed visual manipulation of text and numbers. The BASIC language, which was easier to learn and use than raw machine language, became a standard feature. These features were already common in minicomputers, with which many hobbyists and early produces were familiar.\n\nIn 1979, the launch of the VisiCalc spreadsheet (initially for the Apple II) first turned the microcomputer from a hobby for computer enthusiasts into a business tool. After the 1981 release by IBM of its IBM PC, the term personal computer became generally used for microcomputers compatible with the IBM PC architecture (PC compatible).\n\n"}
{"id": "8053931", "url": "https://en.wikipedia.org/wiki?curid=8053931", "title": "Mobile ticketing", "text": "Mobile ticketing\n\nMobile ticketing is the process whereby customers can order, pay for, obtain and/or validate tickets using mobile phones. Mobile tickets reduce the production and distribution costs connected with traditional paper-based ticketing channels and increase customer convenience by providing new and simple ways to purchase tickets.\n\nMobile tickets should not be confused with E-Tickets (electronic tickets) which are used by airlines since 1994, they can be sent by e-mail, printed and shown at the check-in desk at the airport to obtain a boarding pass.\n\nMany train and bus operators in Europe have created phone apps in which tickets can be bought and stored. These include but are not limited to SJ, DSB, NSB, DB and selected local transit authorities.\n\nPhilips and Sony developed near field communication (NFC) in 2002. It is build on the same basis as common contactless smartcards. Philips published an early paper on NFC in 2004. In 2004, the NFC Forum was established.\nNFC incorporated in a mobile phone allows all kind of novel contactless applications, mobile ticketing being an important one of them.\nMobile Tickets can be purchased via internet and will be downloaded in a few seconds to the mobile phone, be it in an sms with a 2-D barcode or to the connected NFC chip. In case of NFC at entrance the phone just has to be touched to the scanning device (in fact it makes contact within 10 cm). The GSM Association, GSMA, published a whitepaper on M-Ticketing in 2011.\nIt describes extensively the use and advantages of M-Ticketing, principally the use of NFC technology. They state that NFC is the best technology but \"it is expected however that M-Ticketing services using SMS and Bar Code implementations will be prevalent until the point that a critical mass of NFC enabled handsets is available.\"\n\n"}
{"id": "51410157", "url": "https://en.wikipedia.org/wiki?curid=51410157", "title": "Modern Muse", "text": "Modern Muse\n\nThe Modern Muse Charity is an online platform designed to encourage the next generation of female business leaders and entrepreneurs.\n\nThe platform, www.modernmuse.org, is aimed at girls aged 8 and above; it features the professional pathways of women from all walks of life, working across business and society, with a focus on encouraging girls into STEM (science, technology, engineering and maths) and digital careers.\n\nMuses are women of all professional levels, across all sectors. They offer information and insights about their career paths. Via an online forum, questions can be asked of the Muses.\n\nModern Muse seek to improve social mobility through engagement with education and by providing insight to the variety of career options, work experience, job placements, internships and apprenticeships available. Modern Muse was developed in partnership with BP and Deloitte’s Super Pioneers programme. Keytree and FDM are also founding partners.\n\nThe Modern Muse platform offers young women access to female role models. The Modern Muse platform also allows teachers and parents to register and use the platform. A Muse may be any woman in the workplace who is passionate about their career.\n\nFounding Modern Muse Patrons include: \n\nVia the Modern Muse platform:\nBy reaching out to young women, Modern Muse aims to increase the number of women running businesses by 100,000 in the next ten years and to help them achieve their full potential.\n\nKaren Gill and Maxine Benson co-founded and own Everywoman, as well as Modern Muse. Both Gill and Benson were awarded an MBE in 2009 in recognition of her service to women’s enterprise.\n\n"}
{"id": "13591771", "url": "https://en.wikipedia.org/wiki?curid=13591771", "title": "Monochrome monitor", "text": "Monochrome monitor\n\nA monochrome monitor is a type of CRT computer monitor which was very common in the early days of computing, from the 1960s through the 1980s, before color monitors became popular. They are still widely used in applications such as computerized cash register systems, owing to the age of many registers. Green screen was the common name for a monochrome monitor using a green \"P1\" phosphor screen.\n\nAbundant in the early-to-mid-1980s, they succeeded Teletype terminals and preceded color CRTs and later LCDs as the predominant visual output device for computers.\n\nUnlike color monitors, which display text and graphics in multiple colors through the use of alternating-intensity red, green, and blue phosphors, monochrome monitors have only one color of phosphor (\"mono\" means \"one\", and \"chrome\" means \"color\"). All text and graphics are displayed in that color. Some monitors have the ability to vary the brightness of individual pixels, thereby creating the illusion of depth and color, exactly like a black-and-white television.\n\nTypically, only a limited set of brightness levels was provided to save display memory which was very expensive in the '70s and '80s. Either normal/bright or normal/dim (1 bit) per character as in the VT100 or black/white per pixel in the Macintosh 128K or black, dark gray, light gray, white (2bit) per pixel like the NeXT MegaPixel Display.\n\nMonochrome monitors are commonly available in three colors: if the P1 phosphor is used, the screen is green monochrome. If the P3 phosphor is used, the screen is amber monochrome. If the P4 phosphor is used, the screen is white monochrome (known as \"paper white\"); this is the same phosphor as used in early television sets.\nAn amber screen was claimed to give improved ergonomics, specifically by reducing eye strain; this claim appears to have little scientific basis. However, the color amber is a softer light, and would be less disruptive to a user's circadian rhythm. \n\nWell-known examples of early monochrome monitors are the VT100 from Digital Equipment Corporation, released in 1978, the Apple Monitor III in 1980, and the IBM 5151, which accompanied the IBM PC model 5150 upon its 1981 release.\n\nThe 5151 was designed to work with the PC's Monochrome Display Adapter (MDA) text-only graphics card, but the third-party Hercules Graphics Card became a popular companion to the 5151 screen because of the Hercules' comparatively high-resolution bitmapped 720×348 pixel monochrome graphics capability, much used for business presentation graphics generated from spreadsheets like Lotus 1-2-3. This was much higher resolution than the alternative IBM Color Graphics Adapter 320×200 pixel, or 640×200 pixel graphic standard. It could also run most programs written for the CGA card's standard graphics modes. Monochrome monitors continued to be used, even after the introduction of higher resolution color IBM Enhanced Graphics Adapter and Video Graphics Array standards in the late 1980s, for dual-monitor applications.\n\nPixel for pixel, the monochrome monitors produce sharper text and images than color CRT monitors. This is because a monochrome monitor is made up of a continuous coating of phosphor and the sharpness can be controlled by focusing the electron beam; whereas on a color monitor, each pixel is made up of three phosphor dots (one red, one blue, one green) separated by a mask. Monochrome monitors were used in almost all dumb terminals and are still widely used in text-based applications such as computerized cash registers and point of sale systems because of their superior sharpness and enhanced readability.\n\nSome green screen displays were furnished with a particularly full/intense phosphor coating, making the characters very clear and sharply defined (thus easy to read) but generating an afterglow-effect (sometimes called a \"ghost image\") when the text scrolled down the screen or when a screenful of information was quickly replaced with another as in word processing page up/down operations. Other green screens avoided the heavy afterglow-effects, but at the cost of much more pixelated character images. The 5151, amongst others, had brightness and contrast controls to allow the user to set their own compromise.\n\nThe ghosting effects of the now-obsolete green screens have become an eye-catching visual shorthand for computer-generated text, frequently in \"futuristic\" settings. The opening titles of the first Ghost in the Shell film and the Matrix source code of the Matrix trilogy science fiction films prominently feature computer displays with ghosting green text. Green text is also featured in 's computer in \"Lost\" series.\n\nMonochrome monitors are particularly susceptible to screen burn (hence the advent, and name, of the screensaver), because the phosphors used are very high intensity. Another effect of the high-intensity phosphors is an effect known as \"ghosting\", wherein a dim afterglow of the screen's contents is briefly visible after the screen has been blanked. This has a certain place in pop culture, as evidenced in movies such as \"The Matrix\".\n\nThis ghosting effect is deliberate on some monitors, known as \"long persistence\" monitors. These use the relatively long decay period of the phosphor glow to reduce flickering and eye strain.\n"}
{"id": "2995702", "url": "https://en.wikipedia.org/wiki?curid=2995702", "title": "Multimedia Broadcast Multicast Service", "text": "Multimedia Broadcast Multicast Service\n\nMultimedia Broadcast Multicast Services (MBMS) is a point-to-multipoint interface specification for existing and upcoming 3GPP cellular networks, which is designed to provide efficient delivery of broadcast and multicast services, both within a cell as well as within the core network. For broadcast transmission across multiple cells, it defines transmission via single-frequency network configurations. The specification is referred to as Evolved Multimedia Broadcast Multicast Services (eMBMS) when transmissions are delivered through an LTE (Long Term Evolution) network. eMBMS is also known as LTE Broadcast.\n\nTarget applications include mobile TV and radio broadcasting, live streaming video services, as well as file delivery and emergency alerts.\n\nQuestions remain whether the technology is an optimization tool for the operator or if an operator can generate new revenues with it. Several studies have been published on the domain identifying both cost savings and new revenues.\n\nIn 2013, Verizon announced that it would launch eMBMS services in 2014, over its nationwide (United States) LTE networks. AT&T subsequently announced plans to use the 700 MHz Lower D and E Block licenses it acquired in 2011 from Qualcomm for an LTE Broadcast service.\n\nSeveral major operators worldwide have been lining-up to deploy and test the technology. The frontrunners being Verizon in the United States, Kt and Reliance in Asia, and recently EE and Vodafone in Europe.\n\nIn January 2014, Korea’s Kt launched the first commercial LTE Broadcast service. The solution includes Kt’s internally developed eMBMS Bearer Service, and Samsung mobile devices fitted with the Expway Middleware as the eMBMS User Service.\n\nIn February 2014, Verizon demonstrated the potential of LTE Broadcast during Super Bowl XLVIII, using Samsung Galaxy Note 3s, fitted with Expway's eMBMS User Service.\n\nIn July 2014, Nokia demonstrated the use of LTE Broadcast to replace Traditional Digital TV. This use case remains controversial as some study are doubting about the capability of LTE Broadcast to address this use case efficiently in its current version.\n\nAlso in July 2014, BBC Research & Development and EE demonstrated LTE Broadcast during the XX Commonwealth Games in Glasgow, Scotland using equipment from Huawei and Qualcomm.\n\nIn August 2014, Ericsson and Polkomtel successfully tested LTE Broadcast technology by streaming the opening game of the 2014 World Volleyball Championship to hundreds of guests at Warsaw’s National Stadium in Poland on August 30.\n\nIn June 2015, BBC Research & Development and EE demonstrated LTE Broadcast during the FA Cup final in the U.K.\nIn September 2015, Verizon demonstrated eMBMS by broadcasting INDYCAR races.\n\nIn October 2015, Verizon commercially launched their Go90 eMBMS service. Go90 offers both On-Demand and LiveTV, in both Unicast and Broadcast, and supports more than 10 different LTE Broadcast mobile devices.\n\nIn February 2016, Akamai demonstrated with Expway, delivery of video streams across LTE networks with live on the fly switching from unicast to broadcast, at Mobile World Congress 2016.\n\nIn April 2016, Verizon, Telstra, KT and EE launched the LTE Broadcast Alliance.\n\nDespite many technology trials and demonstrations, as of September 2018 there are no commercial deployments of 3G MBMS or 4G eMBMS anywhere in the world.\n\nMain competing technologies of MBMS include DVB-H/DVB-T, DVB-SH, DMB, ESM-DAB, and MediaFLO. However, due to spectrum scarcity and the cost of building new broadcast infrastructure some of these technologies may not be viable. MediaFLO has been deployed commercially in the US by Verizon Wireless through their relationship with MediaFLO USA, Inc. (a subsidiary of Qualcomm) however the service was shut down in early 2011. DMB and DVB-H trials have been ongoing for more than a year now, like those during the football 2006 championships in Germany.\n\nHuawei's proprietary CMB is a precursor to the Multimedia Broadcast Multicast Service. It was specified in 3GPP R6 and is using existing UMTS infrastructure. Huawei says that CMB is based on existing UMTS infrastructure and real time streaming application protocol.\n\nThe most significant competition is from services that stream individual video feeds to users over uni-cast data connections. While less efficient in certain situations, particularly the traditional case where everyone watches the same stream simultaneously, the user convenience of individual streaming has taken over the vast majority of the mobile media streaming market.\n\nThe MBMS feature is split into the MBMS Bearer Service and the MBMS User Service and has been defined to be offered over both UTRAN (i.e. WCDMA, TD-CDMA and TD-SCDMA) and LTE (where it is often referred to as eMBMS). The MBMS Bearer Service includes a Unicast and a Broadcast Mode. MBMS Operation On-Demand (MOOD) allows dynamic switching between Unicast and Broadcast over LTE, based on configured triggers. The MBMS Bearer Service uses IP multicast addresses for the IP flows. The advantage of the MBMS Bearer Service compared to unicast bearer services (interactive, streaming, etc.) is that the transmission resources in the core and radio networks are shared. One MBMS packet flow is replicated by GGSN, SGSN and RNCs. MBMS may use an advanced counting scheme to decide, whether or not zero, one or more dedicated (i.e. unicast) radio channels lead to a more efficient system usage than one common (i.e. broadcast) radio channel.\n\n\nThe MBMS User Service is basically the MBMS Service Layer and offers two different data Delivery Methods:\n\n\nMBMS has been standardized in various groups of 3GPP (Third Generation Partnership Project), and the first phase standards are found in UMTS release 6. As Release 6 was functionally frozen by the 3rd quarter of 2004, practical network implementations may be expected by the end of 2007, and the first functional mobile terminals supporting MBMS are estimated to be available by also end of 2007.\n\neMBMS has been standardized in various groups of 3GPP as part of LTE release 9. The LTE version of MBMS, referred to as Multicast-broadcast single-frequency network (MBSFN), supports broadcast only services and is based on a Single Frequency Network (SFN) based OFDM waveform and so is functional similar to other broadcast solutions such as DVB-H, -SH and -NGH.\n\nMBMS Bearer Service (Distribution Layer):\n\n\n\nMBMS User Service (Service Layer):\n\n"}
{"id": "7528474", "url": "https://en.wikipedia.org/wiki?curid=7528474", "title": "Outline of vehicles", "text": "Outline of vehicles\n\nThe following outline is provided as an overview of and topical guide to vehicles:\n\nVehicle – non-living means of transportation. Vehicles are most often man-made, although some other means of transportation which are not made by man can also be called vehicles; examples include icebergs and floating tree trunks.\n\n\nAircraft\n\nLand vehicle\n\nWatercraft\n\n\n\n\n"}
{"id": "54064049", "url": "https://en.wikipedia.org/wiki?curid=54064049", "title": "Phanteks", "text": "Phanteks\n\nPhanteks is a Dutch company which mainly produces PC cases, fans and other case accessories. The company has a base in the United States.\n\nPhanteks's first product was the PH-TC14PE, a CPU cooler, which gave the young company a good reputation right from the beginning. Later, Phanteks started its case product line, beginning with the Enthoo Series.\n\nThe company has several different versions of the Enthoo series and a few of the Eclipse series. Furthermore, their cases' cooling solutions have been extended by more CPU coolers, fans and accessories. Since the beginning of 2017, the company also produces liquid cooling blocks and fittings. One of their most recent computer cases, shown at Computex 2018, is the Evolv X. The Evolv X is the successor of the Evolv ATX; while similar to the original in design on the outside, the Evolv X has been redesigned from the inside out.\n\n"}
{"id": "41598", "url": "https://en.wikipedia.org/wiki?curid=41598", "title": "Public land mobile network", "text": "Public land mobile network\n\nA PLMN is identified by the Mobile Country Code (MCC) and the Mobile Network Code (MNC). Each operator providing mobile services has its own PLMN. PLMNs interconnect with other PLMNs and Public switched telephone networks (PSTN) for telephone communications or with internet service providers for data and internet access of which links are defined as interconnect links between providers. These links mostly incorporate SDH digital transmission networks via fiber optic on land and digital microwave links.\n\nAccess to PLMN services is achieved by means of an \"air interface\" involving radio communications between mobile phones or other wireless enabled user equipment and land-based radio transmitters or radio base stations or even fiber optic distributed SDH network\n\nThe PSTN is the world's collection of interconnected voice-oriented public telephone networks, in much the same way that the Internet is the concatenation of the world's public IP-based packet-switched networks. It is both commercially- and government-owned. This aggregation of circuit-switching telephone networks has evolved greatly from the days of Alexander Graham Bell, and in the late 20th century became almost entirely digital in nature — except for the final link from the central (local) telephone office to the user (the local loop). It also extends into mobile as well as fixed telephones.\n\nThe PSTN also furnishes much of the Internet's long-distance infrastructure and, for the majority of users, the access network as well. Because Internet Service Providers (ISPs) pay the long-distance carriers for access to their infrastructure, and share the circuits among many users through packet switching, the end Internet user avoids having to pay usage tolls to anyone other than their ISP.\n\nThe PSTN is largely governed by technical standards created by the ITU-T, and uses E.163/E.164 addresses (usually called telephone numbers) for addressing. A number of large private telephone networks are not connected to the PSTN, and are used for military purposes (such as the Defense Switched Network). There are also private networks run by large companies that are linked to the PSTN, but only through controlled gateways such as private branch exchanges..**\n\nA GSM PLMN may be described by a limited set of access interfaces and a limited set of GSM PLMN connection types to support the telecommunication services described in the GSM 02-series of specifications.\n\nPLMN is a network that is established and operated by an administration or by a recognized operating agency (ROA) for the specific purpose of providing land mobile telecommunications services to the public. A PLMN may be considered as an extension of a fixed network, e.g., the Public Switched Telephone Network (PSTN) or as an integral part of the PSTN. This is just one view-point on PLMN. PLMN mostly refers to the whole system of networking hardware and software that enables wireless communication, irrespective of the service area or service provider (cf. Internet backbone). Sometimes a separate PLMN is defined for each country or for each service provider. This systematic ambiguity (of terminological scope) also affects the \"PSTN\" term. Sometimes it refers to the whole circuit-switched system, while other times it is specific to each country.\n\nPLMN is not a term specific to GSM. In fact, GSM can be treated as an example of a PLMN system. These days (as of January, 2006) many discussions are going on to form the structure of UMTS PLMN for the third-generation systems. Access to PLMN services is achieved by means of an air interface involving radio communications between mobile phones or other wireless-enabled user equipment and land-based radio transmitters or radio base stations. PLMNs interconnect with other PLMNs and PSTNs for telephone communications or with Internet service providers for data and internet access.\n\nA public land mobile network may be defined as a number of mobile services switching center areas within a common numbering plan and a common routing plan. With respect to their functions, the PLMNs may be regarded as independent communications entities, even though different PLMNs may be interconnected through the PSTN/ISDN for the forwarding of calls or network information. The MSCs of a PLMN can be interconnected similarly to allow interaction. A PLMN may have several interfaces with the fixed network (e.g., one for each MSC). Inter-working between two PLMNs may be performed via an international switching center. The PLMN is connected via an NCP to the PSTN/ISDN. If there are two mobile service suppliers in the same country, they can be connected through the same PSTN/ISDN.\n\nThe general objective of a PLMN is to facilitate wireless communication and to interlink the wireless network with the fixed wired network. The PLMN was specified by the European Telecommunications Standard Institute (ETSI) following up with their GSM specification. Even as times changed, the GSM PLMN objectives conceptually remained the same.\n\n\n\nGSM architecture is basically the PLMN architecture itself as the subject is GSM PLMN. Various interfaces between the GSM subsystems are to be considered, along with the signaling system and the various components (both hardware and software).\n\nThe GSM PLMN is divided into signaling network and mobile network. Each of these has various subsystems, which are grouped under three major systems: the Network and Switching Subsystem (NSS), the Base Station Subsystem (BSS), and the operation and support system (OSS).\n\nThe operations and maintenance center (OMC) is connected to all equipment in the switching system and to the BSC. The implementation of OMC is called the operation and support system (OSS). The OSS is the functional entity from which the network operator monitors and controls the system. The purpose of OSS is to offer the customer cost-effective support for centralized, regional, and local operational and maintenance activities that are required for a GSM network. An important function of OSS is to provide a network overview and support the maintenance activities of different operation and maintenance organizations. End.\n\nOther functional elements shown are as follows:\n\nThere are three viewpoints of interoperability between PLMN and PSTN:\n\n\nA PLMN is essential for the effective working of any wireless network, just like the need for PSTN in wireline networks. PLMN facilitates interoperation with its own subsystems in order to perform operation of the GSM (3G), LTE (4G) and 5G systems and any wired network in general.\n\n"}
{"id": "40679416", "url": "https://en.wikipedia.org/wiki?curid=40679416", "title": "Puerto Rico Science, Technology and Research Trust", "text": "Puerto Rico Science, Technology and Research Trust\n\nThe Puerto Rico Science, Technology and Research Trust is a private non-profit organization created in 2004 to encourage and promote innovation, transfer and commercialization of technology and creation of jobs in the technology sector, based in San Juan, Puerto Rico.\n\nThe Trust is responsible for Puerto Rico’s public policy for science, technology, research and development. \n"}
{"id": "47395801", "url": "https://en.wikipedia.org/wiki?curid=47395801", "title": "Readdle", "text": "Readdle\n\nReaddle is a Ukrainian mobile application development company. The company research and development is based in Odesa, Ukraine. The operation is mostly built around the App Store, cumulatively generating over 100 million downloads. The company's two main products are PDF Expert and Spark.\n"}
{"id": "15682002", "url": "https://en.wikipedia.org/wiki?curid=15682002", "title": "Seven stages of action", "text": "Seven stages of action\n\nSeven stages of action is a term coined by the usability consultant Donald Norman. \nHe explains this phrase in chapter two of his book \"The Design of Everyday Things\", in the context of explaining the psychology of a person behind the task performed by him or her.\n\nThe history behind the action cycle starts from a conference in Italy attended by Donald Norman.\nThis excerpt has been taken from the book \"The Design of Everyday Things\":\n\nI am in Italy at a conference. I watch the next speaker attempt to thread a film onto a projector that he never used before. He puts the reel into place, then takes it off and reverses it. Another person comes to help. Jointly they thread the film through the projector and hold the free end, discussing how to put it on the takeup reel. Two more people come over to help and then another. The voices grow louder, in three languages: Italian, German and English. One person investigates the controls, manipulating each and announcing the result. Confusion mounts. I can no longer observe all that is happening. The conference organizer comes over. After a few moments he turns and faces the audience, who had been waiting patiently in the auditorium. \"Ahem,\" he says, \"is anybody expert in projectors?\" Finally, fourteen minutes after the speaker had started to thread the film (and eight minutes after the scheduled start of the session) a blue-coated technician appears. He scowls, then promptly takes the entire film off the projector, rethreads it, and gets it working.\nNorman pondered on the reasons that made something like threading of a projector difficult to do. To examine this, he wanted to know what happened when something implied nothing. In order to do that, he examined the structure of an action. So to get something done, a notion of what is wanted – the goal that is to be achieved, needs to be started. Then, something is done to the world i.e. take action to move oneself or manipulate someone or something. Finally, the checking is required if the goal was made. This led to formulation of Stages of Execution and Evaluation.\n\n\"Execution\" formally means to perform or do something. Norman explains that a person sitting on an armchair while reading a book at dusk, might need more light when it becomes dimmer and dimmer. To do that, he needs to switch on the button of a lamp i.e. get more light (the goal). To do this, one must need to specify on how to move one's body, how to stretch to reach the light switch and how to extend one's finger to push the button. The goal has to be translated into an intention, which in turn has to be made into an action sequence.\n\nThus, formulation of stages of execution:\n\n\"Evaluation\" formally means to examine and calculate. Norman explains that after turning on the light, we evaluate if it is actually turned on. A careful judgement is then passed on how the light has affected our world i.e. the room in which the person is sitting on the armchair while reading a book.\n\nThe formulation of the stages of evaluation can be described as:\n\nSeven Stages of Action constitute four stages of execution, three stages of evaluation and our goals.\n\n1. Forming the goal\n\n2. Forming the intention\n\n3. Specifying an action\n\n4. Executing the action\n\n5. Perceiving the state of the world\n\n6. Interpreting the state of the world\n\n7. Evaluating the outcome \n\nThe difference between the intentions and the allowable actions is the \"Gulf of execution\".\n\n\"Consider the movie projector example: one problem resulted from the Gulf of Execution. The person wanted to set up the projector. Ideally, this would be a simple thing to do. But no, a long, complex sequence was required. It wasn't all clear what actions had to be done to accomplish the intentions of setting up the projector and showing the film.\"\nThe \"Gulf of evaluation\" reflects the amount of effort that the person must exert to interpret the physical state of the system and to determine how well the expectations and intentions have been met.\n\n\"In the movie projector example there was also a problem with the Gulf of Evaluation. Even when the film was in the projector, it was difficult to tell if it had been threaded correctly.\"\nThe seven-stage structure is referenced as design aid to act as a basic checklist for designers' questions to ensure that the Gulfs of Execution and Evaluation are bridged.\n\nThe Seven Stages of Action can be broken down into 4 main principles of good design:\n\n\n"}
{"id": "28057675", "url": "https://en.wikipedia.org/wiki?curid=28057675", "title": "Skolkovo Innovation Center", "text": "Skolkovo Innovation Center\n\nThe Skolkovo Innovation Center is a high technology business area that is being built at Mozhaysky District in Moscow, Russia. Although historically Russia has been successful with development of science and technology, its lack of entrepreneur spirit led to government intervention of patents and nonproliferation of Russian tech companies beyond the scope of regional service. As corporations and individuals become \"residents\" of the city, with proposed projects and ideas receiving financial assistance. \nSkolkovo was first announced on 12 November 2009 by then Russian President Dmitry Medvedev. The complex is headed by Viktor Vekselberg and co-chaired by former Intel CEO Craig Barrett.\n\nIn March 2010, Vekselberg announced the necessity of developing a special legal order in Skolkovo and emphasized the need to offer a tax holiday lasting 5–7 years.\n\nIn April 2010, Russian Prime Minister Dmitry Medvedev charged the government with working out specific legal, administrative, tax and customs regulations on Skolkovo.\n\nIn May 2010, Dmitry Medvedev introduced two bills regulating working conditions in Skolkovo. The bills were adopted by the State Duma in September of that year and, on 28 September 2010, the President of the Russian Federation signed the bills into federal law.\n\nIn August 2010, Dmitry Medvedev introduced a bill easing migratory policies in regards to Skolkovo.\n\nOn 20 August 2010, a new government decree regulating visas for participants of the Skolkovo project was published. According to this decree, specialized and highly skilled foreign nationals who arrive in Russia with the purpose of securing employment at Skolkovo will be granted a visa for a term of up to 30 days. In the event of successful job placement they can then obtain a work visa for a term of 3 years.\n\nA new highway was opened connecting Skolkovo to the MKAD in June 2010. Railway transport will be available via Belorussky Rail Terminal and Kiyevsky Rail Terminal. A link to Vnukovo International Airport is also planned.\n\nThe innovation center will be financed primarily from the Russian federal budget. The center's 2010 budget was 3.9 Billion RUB. An additional 22 Billion RUB is planned for 2012 and 17.3 Billion RUB in 2013.\n\nSkolkovo includes five \"clusters\" specializing in different areas. These include IT, Energy, Nuclear Technologies, Biomedicine and Space Technologies.\n\nThe IT cluster is tasked with creating an effective model for successful commercialization of IT technologies in Russia. Over 450 companies have signed up for the IT cluster.IT ecosystem includes over 50 fast growing cyber-security startup companies with more than 700 employees in total\n\nThe Energy Efficient Technologies cluster aims to introduce breakthrough technologies focused on the reduction of energy consumption by industrial, housing and municipal infrastructure facilities. Today over 80 companies are on board for the energy efficiency cluster.\n\nThe Nuclear Technologies cluster aims to encourage the competitiveness of nuclear power markets and develop breakthrough technologies and products.\n\nThe strategic goal of this cluster is to create an ecosystem for biomedical innovation. In order to achieve this goal, the best practices of leading biotechnology and biomedical research centers were studied. More than 215 companies have signed on for the Biomedical Technologies cluster.\n\nThe Space Technology and Telecommunications cluster is intended to strengthen Russia's position in the respective industries. The scope of activity is wide: from space tourism to satellite navigation systems. Russian companies aim to increase their market share in this global market, the total volume of which is estimated at $300 billion.\n\nThere are examples of cooperation between the clusters. For example, in 2012 clusters of Information Technologies and Biomedical Technologies organized joint contest on Mobile Diagnostic Device \"Skolkovo M.D.\" and FRUCT was named the contest winner.\n\nThe main elements of The City are the University and a Technopark. The City will also feature a Congress Center, office buildings, laboratories, fitness centers and stores. The City will measure roughly 400 hectares and have a permanent population of 21,000. Employees, including commuters from Moscow and surrounding regions, will comprise about 31,000 people.\n\nAt least 50% of the energy consumed by the city will come from renewable sources. The well-developed water system uses significantly less water by Russian standards without compromising comfort or hygiene. The transport system prioritizes walking and cycling. The use of vehicles with internal combustion engines is prohibited in the city. Energy passive and active buildings that do not require energy from the outside and even produce more energy than they consume will be built at Skolkovo. Household and municipal waste will be disposed of in the most environmentally friendly way possible – leveraging the use of plasma incinerator technology.\n\nIn July 2012, IBM and five leading Russian innovation companies: the Skolkovo Foundation, Rusnano, Rostelecom, Russian Venture Company and ITFY, all signed a collaboration agreement to foster a culture of applied research and commercialization and attract key talent and investment from around the world in the area of microelectronics.\n\nThe agreement will give the Electronics Technology Center access to IBM’s intellectual property for chip design. IBM will also provide cloud computing technologies to form the basis of a new virtual design environment to be used to develop new microelectronic devices such as sensors to be used in smarter infrastructure projects, industry and consumer electronics.\n\nThe cloud will help unite Russia’s dispersed microelectronics development teams and provide access to advanced technologies and best practice and foster global collaboration. Russian chip designers and fabless design houses will be able to access new semiconductor technologies, including automation tools, design kits, libraries and intellectual property. The center will also provide access to a wide variety of semiconductor production processes offered by many different foundries.\n\nThe agreement was signed by Victor Vekselberg, President of the Skolkovo Foundation; Anatoly Chubais, CEO and Chairman of the Executive Board of Rosnano; Alexander Provotorov, President and Chairman of the Management Board of Rostelecom; Igor Agamirzian, CEO of Russian Venture Company; Evgeny Babayan, Chairman of the Board, ITFY; Leonid Svatkov, CEO ITFY; Bruno Di Leo, Senior Vice President IBM; and Kirill Korniliev, Country General Manager, IBM Russia & CIS.\n\nThe ETC will initially focus on microelectronics design; however in the future it may be extended to other fields where cloud computing can support collaborative development projects.\n\nSkolkovo's Open University (OpUS) isn't an educational institution in the typical sense of the word, because graduating students don't receive a diploma. Instead, OpUS is a source of prospective Masters and PhD candidates, for the Skolkovo University of Science and Technology (SkTech), and interns for Skolkovo partner companies. The educational plan of OpUS includes lecture series, master classes and courses by leading scientists, thinkers and practitioners. Students acquire knowledge in the priority research and development areas of Skolkovo (information technology, biomedicine, energetics, space and nuclear technology). In addition, they have an opportunity to gain knowledge in academic and innovative competencies (foresight, forecasting, thinking, projecting), entrepreneur competence, experience in teamwork on projecting and solving inter-disciplinary problems.\n\nOpUS was opened on 21 April 2011 in Moscow. Selection for Winter 2011-2012 students was carried out in Saint Petersburg and Tomsk. There are currently more than 250 students enrolled in OpUS.\n\nInternational partners include:\n\n\n\n"}
{"id": "9553620", "url": "https://en.wikipedia.org/wiki?curid=9553620", "title": "Soft energy technology", "text": "Soft energy technology\n\nSoft energy technologies may be seen as \"appropriate\" renewable technologies. Soft energy technologies are not simply renewable energy technologies, as there are many renewable energy technologies which are not regarded as \"soft\".\n\nMore specifically, soft energy technologies have five defining characteristics. They rely on renewable energy resources, are diverse and designed for maximum effectiveness in particular circumstances, are flexible and relatively simple to understand, are matched to end-use needs in terms of scale, and are matched to end-use needs in terms of quality. An energy technology must satisfy all five of these criteria to be soft.\n\nResidential solar energy technologies are prime examples of soft energy technologies and rapid deployment of simple, energy conserving, residential solar energy technologies is fundamental to a soft energy strategy. \"Active\" residential solar technologies use special devices to collect and convert the sun's rays to useful energy and are located near the users they supply. Passive residential solar technologies (passive house) involve the natural transfer (by radiation, convection and conduction) of solar energy without the use of mechanical devices.\n\nThe term soft is not meant to be vague, speculative, or ephemeral, but rather sustainable, flexible, resilient, and benign. Soft technology impacts are generally seen to be more \"gentle, pleasant and manageable\" than high technology impacts. These impacts range from the individual and household level to those affecting the very fabric of society at the national and international level. More specifically, favourable socio-political impacts include:\n\nThe use of soft energy technologies, in conjunction with energy efficiency, and the transitional use of fossil fuel technology, comprise the soft energy path.\n\n\n\n"}
{"id": "47763930", "url": "https://en.wikipedia.org/wiki?curid=47763930", "title": "Specification for human interface for semiconductor manufacturing equipment", "text": "Specification for human interface for semiconductor manufacturing equipment\n\nThis specification is usually called SEMI E95-0200 standard. It was originally published in February 2000, and the latest technical revision is SEMI E95-1101.\n\nThis standard addresses the area of processing content with the direct intention of developing common software standards, so that problems involving operator training, operation specifications, and efficient development can be resolved more easily.\n\nSemiconductor Equipment and Materials International\n"}
{"id": "2108299", "url": "https://en.wikipedia.org/wiki?curid=2108299", "title": "Systems philosophy", "text": "Systems philosophy\n\nSystems philosophy is a discipline aimed at constructing a new philosophy (in the sense of worldview) by using systems concepts. The discipline was first described by Ervin Laszlo in his 1972 book \"Introduction to Systems Philosophy: Toward a New Paradigm of Contemporary Thought\". It has been described as the \"reorientation of thought and world view ensuing from the introduction of \"systems\" as a new scientific paradigm\".\n\nSoon after Laszlo founded systems philosophy it was placed in context by Ludwig von Bertalanffy, one of the founders of general system theory, when he categorized three domains within systemics namely:\n\nSystems philosophy consists of four main areas:\n\nThe term \"systems philosophy\" is often used as a convenient shorthand to refer to \"the philosophy of systems\", but this usage can be misleading. The philosophy of systems is in fact merely the element of systems philosophy called \"systems ontology\" by von Bertalanffy and \"systems metaphysics\" by Laszlo. Systems ontology provides important grounding for systems thinking but does not encompass the essential focus of systems philosophy, which is about articulating a worldview grounded in systems perspectives and humanistic concerns.\n\nSystems philosophy was founded by Ervin Laszlo in 1972 with his book \"Introduction to Systems Philosophy: Toward a New Paradigm of Contemporary Thought\". The \"Foreword\" was written by Ludwig von Bertalanffy.\n\n\"Systems philosophy\", in Ervin Laszlo's sense of the term, means using the systems perspective to model the nature of reality, and to use this to solve important human problems (Laszlo, 1972). Laszlo developed the idea behind systems philosophy independently of von Bertalanffy's work on \"General System Theory\" (published in 1968), but they met before \"Introduction to Systems Philosophy\" was published and the decision to call the new discipline \"systems philosophy\" was their joint one. Writing \"Introduction to Systems Philosophy\" took five years, and in his autobiography Laszlo calls it \"my major work\".\n\nLaszlo's \"great idea\", that made systems philosophy possible, was that the existence of a general system theory that captures the \"patterns\" that recur across the Systemics, who themselves capture \"patterns\" that recur across the specialized disciplines, entails that the world is organised as a whole, and thus has an underlying unity. In this light, nature's special domains (as characterized by the specialized sciences) are contingent expressions or arrangements or projections of an underlying intelligibly ordered reality. If the nature of this underlying unity and the way it conditions phenomenal reality could be understood, it would provide a powerful aid to solving pressing sociological problems and answering deep philosophical questions.\n\nIn the subsequent years, systems philosophy has been developed in four important ways, discussed below.\n\nThe first development was due to Ervin Laszlo himself, and is grounded in the concern that the way in which global resources are exploited does not take global systemic effects into account, and appears likely to have catastrophic global consequences. Work in this area is focused on developing models and interventions that can bring about human thriving in a sustainable way on a global scale. Laszlo promotes work in this area through the Club of Budapest International Foundation, of which he is the founder and President, and the journal \"World Futures: The Journal of General Evolution\", of which he is the editor.\n\nA contemporary of Laszlo, Hasan Ozbekhan in the original proposal to the Club of Rome identified 49 \"Continuous Critical Problems\" (CCPs) that intertwine to generate the Global Problematique. This work was shoved aside by the Club as too humanistic and it adopted the system dynamics approach of Jay Forrester. This decision resulted in the volume \"The Limits to Growth\".\n\nOzbekhan sat down with Alexander Christakis and revisited the 49 CCPs in 1995 using the methodology of \"Structured Dialogic Design\" (SDD) which was not available in 1970. They generated an influence map that identified leverage points for alleviating the global problematique. Subsequently, an online class at Flinders University generated an influence map that bore remarkable similarities to that produce by Ozbekhan and Christakis. In 2013, Reynaldo Trevino and Bethania Arango aligned the \"15 Global Challenges of the Millennium Project\" with the 49 CCPs and generated actions that that shows the influence among the challenges and identifies actions for addressing the leverage points.\n\nThe second strand was inspired by Leo Apostel, and is grounded in the concern that disciplinary worldviews are becoming increasingly fragmented, thus undermining the potential for the inter-disciplinary and trans-disciplinary work required to address the world's pressing social, cultural and economic problems. This effort was initiated via the publication in 1994 by Apostel et al. of the book \"Worldviews: from fragmentation to integration.\" Apostel promoted this agenda by forming the Worldviews Group and founding what is now the Leo Apostel Center for Interdisciplinary Studies in the Free University of Brussels. The work of these units is focused on developing systematic models of the structure and nature of worldviews and using this to promote work towards a unified perspective on the world.\n\nThe third initiative was led by Gerald Midgley, and reflects concerns that developments in philosophy of language, philosophy of science and philosophy of sociology suggested that objectivity in modelling reality is an unattainable ideal, because human values condition what is included or excluded in any investigation (\"content selection\"), and condition how subjects of interest are delineated (\"boundary critique\"). The implication that it may be impossible \"in practice\" to obtain objective agreement about the nature of reality and about the \"rightness\" of theories inspired Midgley to develop practices for systemic interventions that could bypass these debates by focusing on the \"processes\" involved in making boundary judgements in practical situations. This supports systematic intervention practices that exploit, rather than trying to unify, the plurality of theories and methods that reflect different value-conditioned perspectives. This perspective is grounded in the recognition that values have to be overtly taken into account in a realistic systems paradigm, contrary to the mechanism that is still widely used in modelling the behavior of natural systems. The central text of this approach is Midgley's 2000 book \"Systemic Intervention: Philosophy, Methodology, Practice\". This approach is now called critical systems thinking (\"critical\" in the sense of \"reflective\"), and is a major focus of the University of Hull's Centre for Systems Studies, of which Midgley is the Director.\n\nThe fourth development was initiated by David Rousseau, and is grounded in the concern that the value relativism dominating academic discourse is problematic for social and individual welfare, is contrary to the holistic implications of systems philosophy, and is inconsistent with universalist aspects of moral intuitions and spiritual experiences. He is promoting research towards elucidating the ontological foundations of values and normative intuitions, so as to incorporate values into Laszlo's model of the natural systems in a way that is holistic (as Apostel advocated), non-reductive (as Midgley advocates), and empirically supported (as William James advocated). Rousseau promotes this work through the Center for Systems Philosophy, of which he is the founder and Director, and collaborative projects with the University of Hull, where he is a Visiting Fellow in the Centre for Systems Studies and a full member of the Centre for Spirituality Studies.\n\nThe relationship of general system theory (GST) to systems philosophy (SP) has been the subject of a technical debate within the field of systems studies.\n\nGST was presented in 1969 by Von Bertalanffy as a theory that encapsulates \"models, principles, and laws that apply to generalized systems or their subclasses, irrespective of their particular kind, the nature of their component elements, and the relationships or \"forces\" between them. ... It [is] a theory, not of systems of a more or less special kind, but of universal principles applying to systems in general\", so that the subject matter of GST is \"the derivation of those principles which are valid for \"systems\" in general\". However, by the early 1970s he was seeking to broaden the term to stand for the general subject of \"systems inquiry\", arguing that \"systems science\" (which includes the Systemics and the 'classical' version of GST), \"systems technology\" and \"systems philosophy\" are \"aspects\" of GST that \"are not separable in content but distinguishable in intention\". This perspective is supported by modern von Bertalanffy scholars such as David Pouvreau.\n\nAn alternative perspective defends the original intent behind GST, and considers systems philosophy to be an endeavor that has a distinct objective from that of GST. This perspective follows the implications Ervin Laszlo laid out in his \"Introduction to Systems Philosophy\", and regards systems philosophy as following up on an implication of GST, namely that there is an organized reality underlying the phenomenal world, and that GST can guide us to towards an understanding of it which systems philosophy seeks to elucidate. From this perspective GST \"is the foundation upon which we can build ... systems philosophy\". This view was taken up by other systems scientists such as Béla H. Bánáthy, who regarded systems philosophy as one of four distinct \"conceptual domains\" of \"systems inquiry\" alongside \"theory\", \"methodology\" and \"application\", and the systems philosopher David Rousseau, who following Laszlo reiterated that GST provides a formal model of the nature of Nature, but that an \"understanding\" of the nature of Nature requires an interpretation of GST involving concrete commitments that systems philosophy aims to provide.\n\nDavid Pouvreau has suggested that this quandary can be resolved by the coinage of the new term \"general systemology\", to replace the usage of GST in the sense of the encompassing conception that the later Von Bertalanffy envisaged.\n\nAn important debate in systems philosophy reflects on the nature of natural systems, and asks whether reality is really composed of objectively real systems, or whether the concept of \"natural systems\" merely reflects a way in which humans might regard the world in terms relative to their own concerns.\n\nErvin Laszlo's original conception of systems philosophy was as \"a philosophy of natural systems\", and as such to use the systems paradigm to show how nature is organized, and how that organization gives rise to the functional properties that we find exercised in the processes in Nature. However, this was immediately problematic, because it clearly is the case that natural systems are \"open systems\", and continuously exchange matter and energy with their environment. This might make it look as if the boundary between a system and its environment is a function of the interests of the observer, and not something inherent in an actually existing system. This was taken by some to mean that system boundaries are subjective constructions, e.g., C. West Churchman argued that \"boundaries are social or personal constructs that define the limits of the knowledge that is taken as pertinent in an analysis\".\n\nErvin Laszlo acknowledged the problem without conceding to an ultimate relativism, saying \"we can conceive of no radical separation between forming and being formed, and between substance and space and time…the universe is conceived as a continuum [in which] spatio-temporal events disclose themselves as \"stresses\" or \"tensions\" within the constitutive matrix…the cosmic matrix evolves in patterned flows…some flows hit upon configurations of intrinsic stability and thus survive, despite changes in their evolving environment…these we call \"systems\".\" In this way Ervin Laszlo accommodated the intrinsic continuity of the cosmos understood as a plenum while insisting that it contained real systems whose properties emerge from the inherent dynamics of the universe.\n\nAlthough solving social problems means taking social norms and perspectives into account, systems philosophy proposes that these problems have a \"proper\" solution because they are about real systems: as Alexander Laszlo pointed out, natural systems are \"a complex of interacting parts that are interrelated in such a way that the interactions between them sustain a boundary-maintaining entity\". In this way, the identity of a system is maintained over time despite continuing interactions with a changing environment. Systems can be destroyed or transformed, but absent radical interactions (e.g. the fission of an atom or the death of an organism) their identity is dynamically maintained by internal (autopoietic) processes. Although we can draw the boundaries around \"conceptual\" systems in ways that serve our needs or purposes, nature has (according to systems philosophy) intrinsic ways of drawing boundaries, and if we mismatch these in our models our 'solutions' might not work very well in practice.\n\nIn this way the answer to the \"ontological\" question about natural systems (do they exist?) is made conditional on epistemological virtue considerations: systems can be argued to exist if systems practice produces positive results in the real world. This debate in systems philosophy thus parallels the wider discussion in academia about the existence of a real world and the possibility of having objective knowledge about it (see e.g. the \"science wars\"), in which the technological success of science is often used as an argument favoring realism over relativism or constructivism. The systemic debate is far from resolved, as indeed is the case with the wider debate about constructivism, because natural systems include ones that exhibit values, purposes, and intentionality, and it is unclear how to explain such properties given what is known about the foundational nature of natural systems. This debate is therefore connected with the ones in \"philosophy of mind\" about the grounding of consciousnesses, and in \"axiology\" about the grounding of values.\n\n\n\n"}
{"id": "29991137", "url": "https://en.wikipedia.org/wiki?curid=29991137", "title": "Technological and industrial history of 21st-century Canada", "text": "Technological and industrial history of 21st-century Canada\n\nThe technological and industrial history of Canada encompasses the country's development in the areas of transportation, communication, energy, materials, public works, public services (health care), domestic/consumer and defense technologies. The 21st century has become the Internet Age is way both literal and metaphorical. The technology that dominates this period of time is wireless technology, cloud computing, HD/3D TV, mega oil, \"greentech\" and nanotechnology. Most technologies diffused in Canada came from other places; only a small number actually originated in Canada. For more about those with a Canadian origin, see Invention in Canada.\n\nTechnology is a major cultural determinant, no less important in shaping human lives than philosophy, religion, social organization, or political systems. In the broadest sense, these forces are also aspects of technology. The French sociologist Jacques Ellul defined \"la technique\" as the totality of all rational methods in every field of human activity so that, for example, education, law, sports, propaganda, and the social sciences are all technologies in that sense. At the other end of the scale, common parlance limits the term's meaning to specific industrial arts.\n\nThe Internet has become an essential part of daily life and is found in most Canadian homes, businesses and government offices. In December 2006, there were 22,000,000 Internet users representing 65.9% of the population and 7,675,533 Internet broadband connections. In 1988, the first .ca Canadian web address, upei.ca, was assigned by John Demco of the University of British Columbia (UBC) to the University of Prince Edward Island. The one millionth .ca address, krauslaw.ca was assigned in 2008 by the Canadian Internet Registration Authority, formed in 1998, to Brent Kraus of Calgary for the promotion of his law firm. As of the end of 2010, Canadians, on a per-capita basis, were the most intensive users of Internet in the world. \nDuring this period, the web search engine became an integral part of use of the Internet. The first such programme, the \"Archie search engine\", was developed by McGill University student Alan Emtage in 1990. Since then, search engines, which have been mostly developed in the US, have evolved and become more versatile and powerful. Notable engines include Lycos (1994), Alta Vista (1995), Magellan (1995), Google (1998), Yahoo! Search (2004), MSN Search (2005), and Bing (2009). These Internet tools are available to web users from countries around the world, including Canada.\n\nE-mail, a very popular feature of the Internet, predated that technology by decades. E-mail type functionality was a feature of a computer sharing technology developed at MIT in the US in 1961. It was also part of the US-developed Semi Automatic Ground Environment (SAGE) component of the North American air defense system created in the fifties and sixties and which included a facility at RCAF North Bay, Ontario. However, it only became a publicly used service with the development of the Internet. A number of US providers now offer this worldwide service to Canadian users, including MSN Hotmail in 1996, Yahoo! Mail in 1997, AOL Mail in 2004 and Gmail in 2004.\n\nThe development of special software allowed the Internet to be used to make computer to computer phone calls. In August 2003, a service known as Skype became available to Internet users around the world including Canada. It has since become extremely popular.\n\nOther web sites including those for social networking such as Facebook (2004), which as of 2008 has 17 million Canadian profiles, MySpace (2003), with 4.5 million Canadian profiles as of 2008, and Twitter (2006). Video and photo sharing sites such as YouTube (2005), with 14.5 million Canadian visits per month, and Vancouver-developed Flickr (2004), have become extremely popular in Canada. The popular Canadian-developed on-line dating service Lavalife went on line for the first time in 1997. In 2010, more than 2 million Canadians were members of LinkedIn, a social networking website, developed in the US in 2003 to enable workers to network for professional and career reasons. In recognition of the importance of the Canadian market, the company opened a Canadian office in 2010.\n\nThe big Canadian banks, including the Royal Bank of Canada, the Toronto Dominion Bank, the Canadian Imperial Bank of Commerce and the Bank of Nova Scotia made their customer accounts available on line as the web gained prominence. On-line investing has grown in popularity in the new century a number of Canadian firms offer sites for this service including, QTrade Investor, BMO InvestorLine, E*Trade Canada (now Scotia iTrade), TD Waterhouse, Credential Direct, RBC Direct Investing, CIBC Investor’s Edge, Disnat, ScotiaMcLeod, National Bank Direct Brokerage, and Virtual Brokers.\n\nOther businesses established a retail presence, notably Amazon.com in 1995, which became popular enough in Canada to merit a separate Canadian site, Amazon.ca, beginning in 2002. The Internet auction site eBay, launched in 1995, gave rise to a Canadian spin-off eBay.ca in 2000. PayPal has been operated by eBay since 2002, and is widely used by Canadians to cover the financial aspect of eBay transactions.\n\nMost large Canadian corporations, including telephone and utility companies, now provide on-line customer billing.\n\nThe Government of Canada has been especially notable in establishing a very diverse and friendly on-line presence for the public. Initially, the basis for this service was a suite of technologies referred to as the Government Enterprise Network (GENet). In the fall of 2003, the government began to replace these with improved technologies known collectively as the Secure Channel Network (SCNet), which make available a wide range of services. For example, in recent years it has become possible for Canadians to file their yearly income tax returns using an Internet service provided by Revenue Canada known as NETFILE.\n\nThe Internet has also become an important source of information, marked by the popularity of such sites as Wikipedia and Google Earth. Wikipedia, an on-line encyclopedia, was established in the US in 2001 by Jimmy Wales and Larry Sanger and presently has over 5,000,000 articles in English and a large number in other languages. Many articles have been contributed to both the English and French language versions of Wikipedia by Canadians, and many of these relate to important aspects of Canadian life. There are thousands of Canadians who use the service every day in both English and French. Google Earth, a virtual globe, is an on-line feature offered by Google since 2005. It provides aerial views of the Earth and is viewed by thousands of web users, many of which are Canadian, every day. Another Internet information service, the telephone directory Canada 411.ca, has become very popular since introduced in recent years. This has led to the delivery of the residential paper telephone book, introduced to Canada in 1878, being cancelled in major cities in 2010.\n\nIn 2010, the Government of Ontario announced its intention of beginning to offer on-line Internet gambling to the residents of Ontario in 2012. It will join the PlayNow.com Internet gambling site established by the government of British Columbia in 2010.\n\nThe Internet has been the target of cyberattacks over the years. One of the most notable attacks was made by 15-year-old hacker Michel Calce, alias \"Mafiaboy\", from the home of his parents in Montreal. Using a technique known as \"distributed denial-of-service\" (DDoS), he paralyzed the websites of Yahoo, CNN, E*Trade, Dell, eBay and Amazon in February 2000.\n\nStar Choice (Shaw Direct) of Calgary, Alberta, and Expressvu Bell TV of Montreal began offering Canada-wide direct-to-home digital satellite television service in 1997. As of 2008, they had 900,000 and 1.8 million subscribers respectively. Star Choice broadcast the first high-definition television programme in Canada in 2000 and began broadcasting HD full-time in 2004. HD channels have been continuously added since that date. In April 2009, Star Choice changed its name to Shaw Direct. By 2009 satellite delivered Bell TV was delivering 45 HD channels.\n\nRogers Cable, Canada's largest cable company, began to offer its Digital Television service in 2001. Video on demand (VOD), a technology that allows digital cable subscribers to order and watch movies at a time of their choice, has been available to Canadians since 2002, the year that Rogers Communication Inc., began to offer its Rogers on Demand service. By 2009 the service was available to 3.5 million homes. Shaw Communications Inc., Canada's second largest cable company offers a similar service. Rogers introduced personal video recorders (PVRs) to customers in 2003.\n\nThe CBC began broadcasting digital over-the-air HDTV in 2005. A national government regulatory body, the Canadian Radio, Television and Telecommunications Commission, has stated that all over-the-air TV broadcasting will be digital by August 2011.\n\nDuring this period, efforts to convert over-the-air AM and FM radio to digital technology failed. A technique known as Digital Audio Broadcasting (DAB) (see Countries using DAB/DMB), was introduced to Canada in November 1999. However the technology never caught on, partly because of the chicken and egg phenomenon. When the technology was introduced, there were few listeners equipped with DAB receivers, and this in turn provided little incentive for broadcasters to convert their very successful AM and FM operations to DAB. DAB came to an end in Canada in 2010. However digital satellite radio has been successful. Two Toronto-based companies, Sirius Canada and XM Canada introduced direct-to-home/car, digital satellite radio service in December 2005 and by 2008 had 750,000 and 400,000 subscribers respectively. In 1999, Telesat launched the first of four Nimiq direct broadcast satellites which provide the space-based satellite transmitters for these services.\n\nIn 2003, Bell Canada introduced an improved speech recognition system for its 310-2355 customer routing service in Ontario. Bell Canada users speak with the programme through \"Emily\", a young female-sounding artificial voice. In 2005, Skype, a voice-and-video-over-Internet technology became available to users around the world, including Canadians. The technique, which bypasses the traditional telephone network, allows people to use the Internet as a type of telephone and to both talk to and see each other during calls. It is used mainly for long-distance communication.\n\nThe proliferation of multiple communications technologies has itself created the need to combine them effectively, resulting in a new technology, unified communications. This technique blends instant messaging, e-mail, voice mail, short message service, web-conferencing, fax, audio, video, cellphone, VIOP and other telecommunications services into a single system. Cooke Aquaculture Inc. of Blacks Harbour New Brunswick uses just such a system, developed by Cisco Systems Canada Co. to manage its fish farm operations.\n\nDigital media were first introduced to Canada in the 1980s, when the CD and DVD became popular with consumers.\n\nThe traditional media began to develop an on-line presence in the new century. Newspapers including Canada's two English-language \"national\", dailies, \"The Globe and Mail\" and the National Post went on line as did the weekly Maclean's news magazine. The French-language press did the same including the daily \"La Presse\" and the bi-weekly \"L'actualité\" newsmagazine.\n\nTelevision broadcasters got into the game, including the English-language national networks, the Canadian Broadcasting Corporation (CBC), CTV Television Network (CTV) and Canwest/Global and the French-language networks, Radio Canada, TVA (TV network) and TQS (now V). In 2009, a number of news services, including Thomson Reuters and Canadian Press, began to offer wireless Internet news services formatted for access by hand-held 3G devices such as the Blackberry.\n\nIn 2009, Indigo Books and Music began to offer a digital book service known as Shortcovers. In late 2009, that electronic service was expanded and renamed Kobo (Kobo Inc.). It offers customers about 2,000,000 book titles in electronic form that can be viewed on an electronic reader. A variety of electronic books or readers have gained a place in Canada beginning with the introduction of the Sony Librie reader in 2004 and the Kindle in 2009. Kobo intends to introduce its own reader in 2010 with code based on an open source concept. In 2010 a number of Canadian libraries, including the Ottawa Public Library began offering books on loan via a downloadable Kindle format.\n\nWith the release of the iPad in Canada in 2010, digital media providers have begun to format their digital media offerings to make them compatible for iPad viewing.\n\nThe downloading of music from the Internet to computers and other storage devices including the iPod, has become very popular in recent years. Music can be downloaded peer-to-peer or from about 500 on-line sites in 40 countries. In Canada one site of note, Puretracks, has been offering a library of about 1.3 million popular songs in Windows Media Audio and MP3 format for download since 2003.\n\nThe film industry has also moved to adopt digital cinema technology. The technology of cinematic special effects has become a notable feature of the film production, with over 2300 Canadian companies, including, Side Effects Software, Toon Boom Animation, Image Engine, (Vancouver), Intelligent Creatures, (Toronto), Intrigue FX and Rainmaker Digital Effects (CIS) in Vancouver, being involved in the field. The National Film Board of Canada began to digitize its extensive archives and later in 2008 will announce the availability of its films on-line.\n\nIn Toronto, Cineplex Entertainment, through Technicolor Digital Cinema has installed the Canadian made Christie CP2000 DLP Cinema projector in the Scotiabank Theatre in Toronto, making it the first Canadian cinema operating this new technology, which provides sharp images and uncompressed digital sound. It can also project 3-D features with Real D Cinema. Cineplex plans to have 25 cinemas across Canada equipped with this new technology in the near future. A Montreal company, D-Box began to offer motion seats for cinemas in 2008. These seats physically move in a way designed to enhance the movie-going experience. The movement is induced by a digital signal specially embedded in the film which activates pistons in the seat that produce the physical movement. Canadian cinemas equipped with the devices in 2010 include, the Cinéma Beloeil, in Beloeil, Quebec and the Cineplex Odeon Queensway Theatre in Etobicoke, Ontario\n\nCompanies such as Electronic Arts, Ubisoft Montreal, BioWare and Next Level Games are active in the technologies related to the development and manufacture of video games. As of 2010 video game publishers and developers in Canada were found in major cities across the country including: Vancouver 47, Toronto 33, Montreal 22, Ottawa 13.\n\nThe use of mobile devices for accessing the Internet through a wireless local-area-network, wireless LAN in Canada has increased dramatically in recent years.\n\nGeographic areas having access to a wireless local-area-network are often referred to as having Wi-Fi service.\n\nBy 2006 Internet providers began making \"mobile\" Internet connection available to their customers with companies such as Bell Canada offering their \"unplugged\" service. This type of service uses the laptop computer and plug-in modem to allow mobile Internet connection in many places across Canada. \"Wireless\" Internet communications have also been facilitated through the introduction of the widely popular Research In Motion, BlackBerry handheld email and telephone machine and the introduction in 2008, by Rogers, of the \"Rocket\" wireless Internet stick for laptops.\n\nIn 2007 Canadian wireless carriers began to convert their DAVE! systems from the CDMA standard which restricted the user to service within North America to the GSM standard used by most carriers around the world. Videotron Telecom Ltee., one of the winners of the Canadian government wireless spectrum auction of 2008, announced that it would invest C$255 million to build a wireless network in Quebec, using the High Speed Packet Access, (HSPA) technical standard.\n\nIn 2009, 3G wireless Internet technology became widely available to Canadians through national networks operated by Bell Mobility, Rogers, and Telus. The use of the Netbook a small portable computer that takes advantage of 3G technology to provide access the Internet became popular in Canada beginning in 2009.\n\nAs of 2009, the downloading of applications and data (music, videos, etc.) via smartphone is becoming increasingly popular in Canada. The bandwidth represented by this use represents up to 40 time the bandwidth used by cellphones for voice calls, putting a tremendous load on existing cellphone networks and driving Rogers Communications Inc., Bell Canada and Telus to invest heavily in expanding the capacity of their networks.\n\nIn 2008 the government of Canada, as part of an effort to increase competition in the mobile communications industry, gave a number of new companies including, Public Mobile Holdings Inc., Globalive Communications Inc. and DAVE Wireless Inc. approval to establish new wireless operations in Canada to compete with the three incumbents. Bell Mobility introduced a smartphone with the Google developed Android operating system in 2009. In 2010 Google made its Nexus One available to Canadian consumers who can obtain these devices from Rogers Wireless, Telus, Bell and Wind Mobile.\n\nAs the result of a CRTC decision, cellphone providers in Canada, as of 2010, were able to locate within a radius of 300 metres, the geographic position of a handset used for making a 911 call.\n\nIn 2009, Canada's three cell phone companies, Rogers Communications Inc., Bell Canada and Telus created a jointly owned company, Enstream LP, which offers a cash transfer service via cellphone. To use the service the subscriber first downloads special software called Zoompass, from Enstream to his or her phone. With this software, a Bell, Fido, PC Mobile, Rogers, Solo Mobile, or Telus subscriber can then withdraw up to $1000 daily from his bank account, or credit card account and transfer the amount to another subscriber who uses the same Zoompass software. Enstream plans to make the service increasingly flexible with the end goal of converting the cellphone into an electronic wallet or purse. In August 2010, Telus began offering the FaceTime service for its iPhone 4 customers.\n\nFoursquare, a cellphone-based mobile social networking service, was introduced to Canada in 2010. The service allows cellphone users to download the Foursquare software to their mobile phones and use it to stay connected with friends and colleagues using the same software and to obtain information on their physical location.\n\nTelus Canada began offering a telephone service called Tigits across Canada in early 2011. Tigits provides a temporary anonymous telephone number for those who subscribe to the service. Developed by Toronto businessman, Sean Miller, Tigits allows the subscriber to protect his/her real number by giving the temporary Tigits number to others. When a Tigits subscriber calls the other person, the person called sees only the Tigits number on his digital display and not the real number of the person calling. When the other person calls the Tigits number of the Tigits subscriber the call is forwarded to the real number of the Tigits subscriber, thus protecting his/her anonymity.\n\nThe University of Montreal has recently experimented with ways to improve the administration of justice by creating a digital court room in which mock trials are held using modern technology to speed the proceedings. The \"courtroom\" has facilities for filing documents electronically. Witnesses can testify by video link or holographically from a remote location. Documents can be served on parties through social media. According to those involved with the project, barriers to its application in real courtroom settings are not technological but rather emotional, with judges and lawyers being resistant to change.\n\nEvidence that cloud computing had begun to take hold in Canada by 2009 is reflected in the organization of the first Canadian Cloud Computing Conference held on 9 February of that year, in Toronto. Cloud computing involves the use of information processing and data storage on computers that are located away from the site of the user and owner of the data. The user, a corporation or individual, communicates with the remote computer through the Internet. The growing popularity of the small netbook computer is in part due to the fact that it is ideally suited to take advantage of cloud computing. The technique allows the user to focus more on processing and storage than on equipment and software acquisition and maintenance. However, it also raises questions relating to privacy and security, in that confidential data may be transmitted, processed and stored on facilities beyond the geographic reach of the owner of the data. Cloud computing providers offer three types of service, platform-as-a-service (PaaS), software-as-a-service (SaaS) and infrastructure-as-a-service (IaaS). As of 2009, Toronto area companies appeared to be the leaders of cloud computing in Canada. An initiative based in Kitchener/Waterloo has brought together a group of computer service providers to create Canadian Cloud Computing, which has developed the Trusted Canadian Cloud. This cloud computing service, which uses facilities based exclusively in Canada, was first demonstrated at Canada 3.0 Digital Media Conference held in Stratford, Ontario in 2010.\n\nQuantum computing in Canada is also gaining a foothold as evidenced by D-Wave Systems, a Burnaby-based quantum computing company founded in 1999 where in May 2013 it was announced that a collaboration between NASA, Google and the Universities Space Research Association (USRA) launched a Quantum Artificial Intelligence Lab using a 512 qubit D-Wave Two that would be used for research into machine learning, among other fields of study. Though the field is still in its infancy, experiments have been carried out in which quantum computational operations were executed on a very small number of qubits. Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes, such as cryptanalysis.\n\nThe use of facial recognition technology (FRT) has grown in Canada in recent years. Nineteen of 27 Ontario Lottery and Gaming Corporation casinos, which receive 40,000,000 visitors a year, are using FRT to automatically identify 15,000 problem gamblers who have voluntarily placed themselves on a self-exclusion list. The Canadian Bankers Association has been using FRT since 2008 to investigate debit card fraud. The Insurance Corporation of British Columbia has been using FRT since 2008 to counter fraudulent attempts by individuals to obtain a driver's license. In the summer of 2010, the technology was used by the Toronto Police to identify suspects wanted for vandalism or violent acts committed during the G-20 Summit there. The Department of Foreign Affairs has begun to issue e-passports with a chip that will enable the use of facial recognition technology beginning in 2012. Social media organizations such a Facebook, with millions of Canadian users have also adopted the use of FRT in their operations. All these applications and others raise privacy concerns.\n\nInternet television began to make inroads in Canada in 2009 with communications providers including, Bell TV, Telus, Rogers Communications and Quebecor (Videotron) investing in the Internet bandwidth necessary to provide their subscribers with TV programmes and movies. Bell TV and Rogers Communications Inc. introduced Internet TV to their subscribers in the fall of 2009. Telus began to offer its rebranded Optik, IPTV service in June 2010. That same month, Quebecor began to offer the Illico Web service with 32 channels (24 in French) to its subscribers through its Videotron subsidiary. The company has stated that the new service will eventually become a \"mirror\" of its TV offerings. Bell began providing a rebranded IPTV service called Fibe, in September 2010. In 2010, Shaw Communications announced its intention to begin providing its customers with Internet TV. Halifax based EastLink is also investing in IPTV.\n\nThe delivery of movies and television programmes through the Internet in Canada was also given a boost with the introduction a streaming video Internet service by the US based Netflix, via its Canadian server at Netflix.ca in 2010. Canadian-owned movie delivery services were also introduced. Cineplex began to offer a movie download service and a streaming on-line Internet video service was introduced at Zip.ca.\n\nThe first efforts towards providing Canadian viewers with 3D TV were made in 2010. Early in the year two international consumer product manufacturers, Samsung and Sony began to market flat screen, digital, high definition, 3D television sets in Canada. The 3D effect is only available with the use of a pair of special glasses worn by the viewer. Each company has its own proprietary viewing standard, so that the glasses of one company cannot be used to view the 3D television of another. DVD movies recorded in 3D provide what is, as yet, a limited source of programming.\n\nBroadcasters also began to take steps to provide 3D programming to viewers equipped with these new 3D sets. The World Cup soccer championship played in South Africa was broadcast in 3D by the host broadcaster and the signal was offered in Canada by a number of TV providers. On 27 July 2010, the satellite delivered Bell TV began to offer its subscribers a full-time 3D Oasis pay TV channel. The CBC also announced in August 2010 that it would broadcast the first Canadian produced 3D programme on 20 September 2010. It would be available to all viewers in Canada with an HD television set but the 3D effect would only be available when the programme was viewed with special glasses which will be distributed free at Canada Post Offices across Canada before the programme.\n\nOn 2 August 2010, a Toronto company, InteraXon, announced that it had developed technology to control machines by human thought. The technique involves the use of a head set to detect \"alpha\" and \"beta\" brain waves. The head set in turn produces a \"control signal\" that can be used to program electrical appliances ranging from lights to home appliances to computers. The technology was demonstrated at the 2010 Winter Olympic Games in Vancouver, where visitors used their brain waves to control the lighting on three landmarks, the CN tower in Toronto, the Parliament Buildings in Ottawa and Niagara Falls. The technology is based on the research of Dr. Steve Mann, a University of Toronto professor who initially developed thought-controlled computing technology. The company foresees that the headset will evolve into a small wireless Bluetooth device that will be available in consumer electronics retail stores within two years.\n\nAnalog technology has dominated the history of the communications system in Canada for almost 160 years. It formed the basis for the telegraph, beginning in the 1850s, the telephone in the 1880s, recorded sound, the 20th century, radio, the 1920s, computers and television, the 1950s and cable TV in the 1960s.\n\nHowever, digital technology has slowly replaced analog technology in all these domains in the past 40 years. The transformation began with the telephone system, in the 1970s and microchips and microcomputers in the early 1980s. Indeed, it was the combination of the telephone system and computers through a common digital link that permitted the latter machines to communicate with each other at distance. Further digital advances lead to the digital camera, CD, DVD and mobile communications, later in that decade, the Internet in the 1990s as well as land based and satellite TV and radio, wireless communications etc., in the first decade of the new century.\n\nWith the CRTC mandated end to analog television broadcasting in Canada in August 2011, the analog age in Canada will for all practical purposes come to an end.\n\nEnergy concerns have had a large impact on automobile manufacturers. Fuel efficient hybrid vehicles such as the Chevrolet Tahoe, Saturn Vue, Toyota Prius, Toyota Camry Hybrid, Toyota Highlander Hybrid, Ford Escape Hybrid, Honda Insight and Honda Civic Hybrid have become available to Canadian consumers since the start of the 21st century and the rising cost of gasoline is making them increasingly attractive in spite of their generally higher cost. As of 2009, the Ford Fusion Hybrid was the most fuel efficient mid-sized car available in Canada. In 2008 Ford Canada began the operation of the Flex assembly line, using the Flex technique at its plant in Oakville, Ontario. This technology allows the production of three different automobile types, in this case, the Ford Edge, the Ford Flex and the Ford Lincoln MKX, on the same assembly line. In 2004 Mercedes-Benz introduced the diminutive and fuel efficient Smart Fortwo automobile to the Canadian market. Multinational car manufacturers have also announced their intentions to introduce the all-electric car to world markets including Canada. General Motors has announced the availability of its Chevrolet Volt in Canada in 2011 as has Mitsubishi for its MiEV, while Nissan has announced the Canadian introduction of the Nissan Leaf in 2012.\n\nThe management of automobile traffic in large urban areas through the use of \"smart\" electronic traffic management systems has become popular in recent years. Such systems are now in place in Toronto (1993), Ottawa, Calgary and Halifax. The city of Montreal will take the first steps for the installation of such a system in 2011. 500 video cameras and other street and highway mounted sensors will provide information for a central computer which will be used to control traffic lights to improve traffic flow and reduce accidents. Another traffic management tool involves the use of satellite tracking. Promoted by a Toronto company, Skymeter Corp, but not yet deployed in Canada, the system, a type of toll road in the sky, is designed to reduce traffic jams through the use of automobile based transponder/GPS systems and satellites. The satellite tracks the route of a particular vehicle at a particular time and then charges the user based on a systems of \"tolls\" based on the roads taken and the time of day. The tolls are publicized for users and designed to encourage road use in a way that minimizes traffic jams. The computerized billing system keeps an account of the charges and bills the customer incurs on a regular basis.\n\nIn 2010, the City of Montreal began to deploy 800 wireless, networked, solar-powered, Linux based, electronic parking payment stations to replace up to 10,000 existing mechanical parking meters. Each parking space has a code and the motorist can pay the required parking rate, with cash or credit card, from any station in the city (he/she must of course remember his code). The system, developed by 8D Technologies of Quebec also allows metre maids to check for parking violations by wirelessly interrogating a parking station with a hand-held device from his/her vehicle. The devise provides a digital map of all parking spaces near the station and marks those spaces with vehicles in violation with a red symbol. Other cities across Canada are installing similar machines.\n\nA light-rail urban passenger train known as the O-Train, began operation in Ottawa in 2001 providing limited service in a north-south corridor, today's Trillium Line. There are plans to expand the system to serve the downtown core as well as the western and eastern suburbs of the city by 2016.\n\nGlobal positioning technology has become an important feature of business and consumer life. After 23 years of military development, the U.S. military global positioning system became operational in 1995. Originally designed for the precise targeting of weapons and other military purposes, the U.S. government made the system available to civilians in 1996. Industrial users such as transportation companies and resource companies began to make use of the technology for the tracking of vehicles and the location of field operations. Receivers for the consumer market, were also produced and made available in Canada and became popular with outdoorsmen and women. In 2004 a GPS feature became available on some mobile phones and stand alone units for car navigation were available to Canadians by 2008.\n\nThe 11 September 2001 terrorist attack on the U.S. has resulted in increased security along the Canada-U.S. border. In 2004, Canada and the U.S. signed the Canada-U.S. Agreement on Science and Technology Cooperation for Critical Infrastructure Protection and Border Security designed to speed the introduction of a number of electronic, wireless, computer and detection technologies to scrutinize cross-border traffic while at the same time limiting the disruption to the flow of people and goods. The use of these technologies is particularly important at the Windsor Detroit border crossing which is the busiest in the world.\n\nIn 2008, the Government of Canada announced the initiation of two important transportation projects. In the first instance the government stated that it will acquire, for the Canadian Coast Guard, a new $700 million, CCG Polar Class icebreaker for patrolling the Northwest Passage. The ship will enter service in 2017. The government also announced the construction of a second international bridge between Windsor, Ontario and Detroit, Michigan, to help relieve the pressure on the heavily overloaded, 80-year-old Ambassador Bridge. The $5 billion project will include connections from the Canadian ends of both bridges to the nearby Highway 401 (Ontario). As of December 2010 construction had yet to start.\n\nThe field of transportation also saw the Premiers of Ontario and Quebec in 2007 talking of yet another study of a high speed train in the Quebec City – Windsor Corridor.\n\nBetween 2006 and 2009, Air Canada \"made over\" the cabins of all its aircraft providing each passenger seat with a number of new technologies including, a Personal AVOD (with a 230 mm touch-screen LCD) offering 200 hours of video and audio entertainment, interactive games, \na three-prong 120 V AC plug for laptops, a USB port and XM Radio Canada. The largest airplane in the world the Airbus A380, in this case operated by Emirates Airline, began regular service between Toronto's Lester B. Pearson Airport and Dubai in 2009. By 2009, most major airports in Canada were equipped with stand alone self-service customer check-in kiosks, which provided the passenger with a boarding pass for his/her flight. This represented the further extension of the technique known as the e-ticket which became the standard for purchasing an aircraft ticket several years earlier. In January 2010, the Government of Canada announced the use of full body scanner for the security checking of passengers boarding planes in Canada bound for the US. The scanners will be installed at the airports in Montreal, Toronto, Vancouver, Calgary, Edmonton and Halifax. The use of biometrics will become an important technique in the screening of those wishing to enter Canada. It is planned that between 2011 and 2013 the Department of Immigration and Citizenship will begin to deploy digital face and fingerprint scanning systems at overseas Canadian Visa offices for the issuance of visas to those who intending to visit Canada.\n\nAir navigation coverage has recently been improved through the deployment of Automated Dependent Surveillance-Broadcast (ADS-B) technology in parts of northern Canada. First introduced in the Hudson Bay area in January 2009, the service will eventually be expanded to cover all of northern Canada. The technique involves the use of ground-based transmitter/receivers and special electronic equipment aboard aircraft flying through northern airspace. This special equipment automatically transmits information relating to the aircraft position (determined by a GPS on the airplane) every second, to the ground-based receiving station, a number of which are located in the north. The station then transmits this information to an area control centre, operated by Nav Canada, Canada's national air navigation system operator, where the it is displayed on \"radar\" screens which are used by air traffic controllers to monitor Canadian airspace.\n\nThe importance of the shipping container has been emphasized by recent developments in Winnipeg. \"CentrePort Canada, an 8,000-hectare inland port being developed on the city’s edge is a one-stop shop for air, truck and rail shipments and is designed to reroute North American trade through the middle of the country...CentrePort (recently) announced an agreement with two Chinese partners, including the country’s largest private shipping company, Minsheng International Freight Co...(that)...will create a new container-based rail system that will quickly move crops from the Canadian prairies into the Chinese market.\" Rail services will be provided by Canadian Pacific Railway and Canadian National Railway. A new highway, the CentrePort Canada Way, is under construction to divert the heavy truck traffic associated with the new facility away from urban roads.\n\nIn this century, the largest engineering undertaking by far is the tar sands project in northern Alberta. This has seen the investment of up to $60 billion to develop and build gigantic tar sand mining, transportation, separation and refining facilities to produce oil from the gritty bitumen tar. The project is highly controversial for a number of reasons not the least of which is environmental. As of 2005, operations included the Suncor Mine, Syncrude Mine, Shell Canada Mine and others producing 760,000 barrels of oil a day. A large number of corporations from a number of countries plan to invest in the tar sands, including Suncor Energy, Syncrude, Shell/Chevron/Marathon, and Petro-Canad. Recovery techniques include steam-assisted gravity drainage (SAGD) and cyclic steam stimulation (CSS). More recently Cenovus Energy of Calgary has developed the \"Solvent Aided Process\" SAP for heavy oil recovery. This involved injecting butane or other organic solvent, along with steam into a horizontal chamber dug in the oil sands. The solvent and steam allow the oil to flow into another chamber below the first. It is then pumped to the surface from this chamber.\n\nOil sands recovery techniques create huge amounts of contaminated waste water, which is stored in \"tailing ponds\". In 2010, there were about 170 square kilometres of these ponds in the oil sands region of Alberta. Left to a normal process of degradation it would take decades for this waste to become environmentally safe. On 27 August 2010, Shell Canada announced the opening of a commercial plant designed to speed the cleaning of the waste in these ponds, at its oil sands production facility in Alberta. The plant uses a technique developed by Shell Canada at a cost of C$30 million. Known as \"atmospheric fines drying\" or AFD, it takes the thick liquid output of the oil production process and over a period of several weeks with the use of a special flocculant and drying techniques, reduces it to a safe dirt-like compound. Shell Canada is making this technology available free of charge to other oil sands production companies.\n\nGeological formations of shale gas are being explored as a new source of energy. A technique known as horizontal drilling is used to create a horizontal bore hole, through a formation. Water under high pressure is then pumped into the bore hole where it fractures the shale and allows the gas to escape the rock and seep up the bore hole. There are a number of shale gas fields in Canada including the Shallow Colorado basin in Alberta, Saskatchewan and Manitoba, the Bakken, in Saskatchewan and Manitoba, the Antrim in Southern Ontario and the Utica in south east Quebec. The technique is not without problems for the fracturing can affect aquifers causing contamination and deviation. To date efforts at production in Canada have been limited to exploratory wells.\n\nCanaport, the first liquified natural gas (LNG) port terminal facility of its kind in Canada, began operation in Saint John, New Brunswick in 2009. LNG is seen as a substitute for conventional gas.\n\nIn 2008, the Government of Ontario announced plans for the construction of two new reactors at the existing Darlington nuclear power facility, but suspended the project in 2009. Competing designs included the ACR-1000 by Atomic Energy of Canada Ltd., the EPR by the French company Areva Group and the AP1000 by the US based Westinghouse Electric Co. Llc.. The government of Saskatchewan is considering the construction of two nuclear reactors in Lloydminster and the government of New Brunswick is proposing the addition of another reactor at its Point Lepreau nuclear power facility.\n\nIn 2006, the Government of Ontario instructed the provincial hydro utility to provide all of its customers with digital smart hydro meters by 2010 as a first step towards the creation of a Smart Grid, which would conserve electricity. The project had been largely completed as of that date. In BC, BC Hydro announced in 2010 the replacement of existing hydro metres with digital smart meters for its 1.8 million residential and commercial customers, by 2012.\n\nIn Vancouver, the Vancouver Fuel Cell Vehicle Program, a pilot project, was introduced in 2005 to study the use of hydrogen as a power source for cars. The three-year undertaking, a first in Canada for fuel cell powered automobiles, studies the operation of a fleet of five Ford Focus FCV’s (fuel cell vehicles), in \"real world\" conditions, in Vancouver and Victoria. The project is the initiative of a consortium made up of the Governments of Canada and British Columbia, Fuel Cells Canada, and Ford Motor Company.\n\nConcerns with energy efficiency have also led to the introduction of the compact fluorescent lamp for domestic, commercial and industrial use and the federal government stated in 2007 that the sale of incandescent light bulbs would be phased out by 2012. The technology of the LED lamp has been known for a century. In recent years, it has become a popular replacement for incandescent bulbs because of its low power consumption. RenewABILITY Energy of Waterloo has developed a technique for recovering heat from domestic waste water. Known as the Power-Pipe, it channels hot waste water through cold water waiting to be used and heats it.\n\nThe use of clean-burning biofuels such as ethanol has become significant in recent years. At the present time, Canada's largest manufacturers of ethanol include GreenField Ethanol and Husky Energy, which produce 500 million litres and 260 million litres of ethanol a year respectively from corn and wheat. Other companies are also at work in the field, including Enerkem of Montreal, which makes ethanol from old telephone poles at a facility in Westbury, Quebec and Iogen of Ottawa, which makes cellulosic ethanol from wheat straw. Since 2007, the Government of Ontario has required that all gasoline sold in the province contains at least 5% ethanol.\nA federal regulatory change in 2009 will require all oil refiners in Canada to provide an ethanol content of at least 5% in their gasoline by September 2010. The Fischer-Tropsch process is the basis for a proposal by AP Fuels of Montreal to establish five biorefineries in Canada. The plan calls for the use of this technique to transform certain types of trees, notably popular and birch, into gas and then to liquid-biodiesel, which burns with reduced CO output.\n\nThe technology of \"clean\" coal has also become important. Western Canada has abundant coal supplies but the use of coal in recent years has been criticized for environmental reasons. To counter this criticism, coal and coal-fired electricity producers have formed the Canadian Clean Power Coalition. This organization promotes a number of projects which use a variety of \"clean\" coal technologies. These include the EPCOR Integrated Gasification Combined Cycle (IGCC) plant for the Genesse Power Station in Alberta. The IGCC plant gasifies coal and uses the clean gas to drive a gas turbine. The process also produces steam, which is used to turn a steam turbine. Both turbines are used to produce electricity. The process also captures CO from the gas combustion, which is in turn used for enhanced oil recovery or is sequestered underground.\n\nEnergy concerns have inspired the development of wind farms that use modern windmills to generate electricity from this renewable resource. One of the first modern windmills was built at Cap Chat in Quebec in the eighties, but most wind farms have been built since 2000. As of 2008, 10 megawatt wind farms in Canada were distributed as follows: Alberta 10, Quebec 5, Ontario 5, PEI 4, Saskatchewan 3, Manitoba 2 and Nova Scotia 2. In 2008 Hydro-Québec announced the construction of 1000 windmills at 15 new sites located mostly in the St. Lawrence River Valley. By 2015, that utility expects that 10% of the province's electricity will be provided by wind power. In 2008, in British Columbia, BC Hydro has issued a Clean Power Call for proposals for environmentally friendly energy production and one company, Naikun Wind Energy, has responded with Canada’s first plan to develop off-shore wind power by installing windmills at sea in the Hecate Strait off the north coast of B.C.\n\nIn 2010, the Government of Ontario signed an agreement with Samsung and the Korea Electric Power Corporation to build and operate wind and solar electrical generating farms across southern Ontario. The C$5 to C$7 billion project is described as the largest of its type in the world and will begin with installations in Chatham-Kent and Essex-Haldimand counties in southwestern Ontario. It is foreseen that the wind turbines will generate up to 2,000 MW and the solar power facilities up to 500 MW. This will permit the closure of all of the coal-fired electric generating plants in Ontario by 2014. \nA private company, OptiSolar Farms Canada Inc., is using silicon solar panels to develop what will become the largest solar power farm in North America. The facility, under construction in a field near Sarnia, will begin to produce 60 megawatts of electricity for Ontario consumers by the end of 2008.\n\nThe use of geothermal energy has grown in Canada in recent years although its overall importance as an energy source is still very small. The use of geothermal energy in Canada falls into two broad categories: commercial use to produce electricity and consumer use for home heating. In Canada, the former is limited to a facility in Meager Mountain British Columbia, a site with a potential for 100–250 MW, which has recently (2010) begun to produce for the BC Hydro grid. In the case of consumer use, a hole similar to that used for a domestic water well is drilled in the ground near the residence in question. Water is pumped to the surface and passed through a heat exchanger where some of its heat is removed and transferred to a closed loop water system in the house. The cooler water is then returned to the ground. The water in the closed loop is circulated throughout the structure where it passes through radiators and heats the house.\n\nThe undesirable environmental effects of industrial processes and atmospheric pollution in particular, have become a topic of increasing public concern in the new millennium. Among the most notable polluters in Canada in 2006 were electric power generators: ATCO, Emera (Nova Scotia Power), Ontario Power Generation, SaskPower and TransAlta, mining companies: HudBay Minerals, Teck Cominco, Vale Inco and Xstrata, oil and gas companies: Imperial Oil, Shell Canada, and Trans Canada, oil sands companies: Syncrude and Suncor and the manufacturing enterprise, SMC Canada.\n\nEfforts to reduce the release of CO gas into the atmosphere lead to the initiation of the Weyburn-Midale CO Project in Saskatchewan in 2000. Presently the world’s largest CO sequestration effort, this $80 million undertaking involves the injection of waste CO gas from industrial processes into the ground for storage instead its release into the atmosphere. There are presently two underground sequestration facilities, one at Weyburn operated by Encana and the other at Midale operated by Apache Canada.\n\nIn recent years bio-waste has been used for the production of heat and electricity. Sanitary landfill sites are notable in this regard. Often, systems for the collection of methane gas are progressively installed as the sites are filled. This gas is then used at on site cogeneration facilities for the production of heat and electricity. A number of landfill sites including those in Kanata, Petrolia, Watford and Napanee, Ontario and Sainte-Sophie, Drummondville and Magog in Quebec have been selected for the location of cogeneration facilities.\nIn Ottawa the cogeneration facility at the Pickard (Sewage Treatment) Centre which has been in operation since 1998, provides all the heat and electrical energy needed to operate the centre.\n\nOstara Nutrient Recovery Technologies Inc. of Vancouver has developed techniques to recover phosphorus and other nutrients from waste water. Since 2007, these have been put to use at the Gold Bar Treatment Plant in Edmonton, the world's first industrial scale waste-water nutrient treatment facility. The recovered products are recycled and sold as environmentally safe commercial fertilizer. Other Ostara nutrient recovery projects are underway at Lulu Island (Vancouver), Penticton, B.C. and in the US.\n\nEfforts to save fuel have also led to efforts to reduce the weight of vehicles through the increased use of composite material. Aircraft manufacturers have been especially notable in this regard and produced new large but relatively light aircraft such as the Boeing B-787 Dreamliner with this new material. Orders for this new machine have been made by a number of major world airlines, including Air Canada. In 2008, Bombardier of Montreal announced the production of the new C Series of 100- to 130-seat passenger jets which will also make extensive use of composites. They will also be used extensively in the 7000 and 8000 series of long range business jets announced by that company in 2010.\n\n3D printing has become an important industrial process in Canada. The technique uses a computer to drive the 3D printing device. This machine builds 3D shaped objects through successive passes of a \"printing head\" which lays down layers of plastic or other material to progressively build a 3D physical object. As of 2010, about 100, 3D printers were in use with manufacturing enterprises in Canada.\n\nThe techniques of diamond mining have been introduced to Canada in recent years. Over 600 kimberlite formations have been found throughout Canada. Open pit mining techniques have been used to produce diamonds from two of these, Ekati, beginning in 1998 and Diavik in 2003.\n\nNanotechnology involves the manipulation of atoms and molecules to produce processes and products for human use. At present, the field is the subject of much research, but the use of these processes and products in Canada is not yet widespread. However the technology remains important because of its potential for great future influence. Some nano-products have made their way to the market in items such as cosmetics, and certain industrial products available in Canada.\n\nMost of the activity in Canada is found in research. In 2001, the Canadian government established the National Institute for Nanotechnology in Edmonton. The Institute conducts nano-research in a number of fields including the life sciences, supramolecular assembly, molecular scale devices and nano-sensors. As of 2010, a number of Canadian universities offer engineering degrees in nanotechnology. Of particular note is the Waterloo Institute for Nanotechnology which will be in operation in 2011 and will conduct research related to nano-engineered materials, nano-electronics design and fabrication, nano-instrumentation and nano-biosystems.\n\nThe use of nanomaterials is not without controversy. As of February 2009, the Government of Canada requires all industries to report the use of nanomaterials in their products. In 2010 the government banned the use of manufactured nano-materials and nanotechnology in organic food production.\n\nThe construction of skyscrapers has continued apace in recent years with Toronto and Calgary accounting for most of the new structures. These include: Bankers Hall West, Calgary, 2000, the TransCanada Tower, Calgary, 2001, One Wall Centre, Vancouver, 2001, One King Street West, Toronto, 2005, West 1, Toronto, 2005, Harbourview Estates 2, Toronto, 2005, Residences of College Park 1, Toronto, 2006, Living Shangri-La, Vancouver, 2008, the Hilton Fallsview Hotel Tower, Niagara Falls, 2008, Quantum 2 (Minto Midtown), Toronto, 2008, the Bay Adelaide Centre West, Toronto, 2009, the RBC Centre, Toronto, 2009, Success, Toronto, 2009, Montage, Toronto, 2009, the Ritz-Carlton, Toronto, 2010, Centennial Place, Calgary, 2010, Maple Leaf Square North and South, Toronto, 2010, Jamieson Place, Calgary, 2010, Festival Tower, Toronto, 2010, The Bow (skyscraper), Calgary, 2011, Trump International Hotel and Tower, Toronto, 2011, The Uptown Residences, Toronto, 2011, Eighth Avenue Place (Calgary), 2011, the Four Seasons, Toronto, 2011, The Private Residences, Vancouver, 2011, the Burano, Toronto, 2011, Absolute World North and South, Mississauga, 2011, the Marriott Courtyard Hotel, Montreal, 2012, the Shangri-La Toronto, 2012, and the L Tower, Toronto, 2012.\n\nNew hydro-electric projects have been completed as well including the 230-MW Rocher-de-Grand-Mère station, on Quebec's Saint-Maurice River (2004).\n\nNew bridges and roads of note include the Golden Ears Bridge, Vancouver, 2009, the Middle Arm Bridge, Vancouver, 2009 the North Arm Bridge, Vancouver, 2009 and the Sea to Sky Highway, Vancouver/Whistler, 2009. In 2009, in northern Quebec, Hydro-Québec initiated construction of the C$6 billion Romaine River Complex, a series of four rock filled hydro generating dams that will be completed between 2014 and 2020.\n\nA different type of public facility was introduced to the citizens of Toronto in 2010, when the city approved a contract for the installation of 20 self-cleaning public toilets. The first of their kind in Canada, each of the devices, which are placed throughout the city, resembles a bus shelter. The user pays 25 cents for twenty minutes of occupancy. The facility cleans itself automatically after each use.\n\nIn 2001, the Federal government created Canada Health Infoway, in independent, not-for-profit, federally funded organization composed of the 14 Canadian federal, provincial and territorial Deputy Ministers of health. Infoway has a mandate to accelerate the Canada-wide use of electronic health records and electronic health information systems. As of 2008, more than $1.3 billion has been invested in the system. By 2010, Infoway plans to have electronic health records for 50% of the population available to authorized health professionals, and expects to have electronic health records for all Canadians by 2016. The project involves undertakings in a number of fields, including diagnostic imaging systems, drug information systems, telehealth, laboratory information systems and public health surveillance.\n\nTelus, one of Canada's largest telephone companies, announced an agreement with Microsoft of Canada for the use of the latters' HealthVault (2007) consumer health records software in 2009. Telus intends to use the software to allow its 11 million Canadian subscribers to access information relating to their health care.\n\nMedical technology in Ontario was improved in 2009 with the implementation of the government operated ePrescribing system a service that allows doctors to send prescriptions for patient pharmaceuticals directly to the pharmacist through a private computer network. This technique eliminates the problem with illegible handwriting, thus improving patient safety. The system has been initially introduced in Sault Ste. Marie and Collingwood with plans for making it available province-wide by 2012. eHealth Ontario, announced in 2010, the signing of a C$46 million contract for the establishment of a diabetes registry, for the management of patients with this disease. The registry will eventually be expanded for the management of patients with other chronic diseases.\n\nSince 2008, Real Time Radiology (based in Mississauga, Ontario) has provided interpretation of medical images to remote sites on a Canada-wide basis. Through use of the Internet and a highly automated computer process, a team of 50 radiologists working for the company across Canada interprets medical images sent from distant locations where the services of a radiologist are not available. The results are returned electronically to the remote locations and form the basis for patient treatment there. The Gattuso Rapid Diagnostic Centre at the Princess Margaret Hospital in Toronto, through the acquisition of new diagnostic equipment that can prepare tissue samples for pathological analysis within hours, began offering same day breast cancer diagnosis for patients in 2009. \n\nTechniques for the mass production of drugs were improved in the early part of the new century. In Ste. Foy, Quebec, the international drug maker GlaxoSmithKline established a manufacturing complex for the mass production of vaccines. As of 2009, the facility is capable of producing 14,000,000 doses per month. The facility may be used for the production of a vaccine for the H1N1 flu virus for the entire population of Canada (around 35,000,000 people as of 2014), should that become necessary. Also in 2009, public preparations for a possible pandemic included the placement of containers of liquid hand sanitizer for use in public places.\n\nThe PharmaTrust prescription medication dispending machine was introduced to the Canadian public in 2008. The apparatus, which physically resembles and functions like an ATM or soft drink dispenser, allows a user to purchase and receive medically approved prescription drugs, without visiting a pharmacy. Developed by PCA Services Inc. of Oakville, Ontario, one of the first has been installed in the Sunnybrook Health Sciences Centre, in Toronto.\n\nLasers made their way into routine dentistry by the middle of the first decade, offering faster treatments, less pain and more precise results. They are used to remove tartar, treat soft tissues such as gums and to prepare cavities for filling. Of particular interest in the latter instance is the fact that this treatment is so painless that the use of a needle to inject a local anesthetic is usually unnecessary. Laser treatment results in little bleeding, a lower risk of infection and a quicker healing. Another innovation was the use of computer milled ceramic implants for repairing cavities. The use of a non-toxic chemical such as hydrogen peroxide or carbamide peroxide for tooth bleaching has become popular in the new century.\n\nIn 2002, two Vancouver doctors, dermatologist Alastair Carruthers and ophthalmologist Jean Carruthers, pioneered the cosmetic use of the well known botulinum toxin. The pair noticed that subcutaneous injections of small amounts of the toxin had the effect of removing age wrinkles from the skin. The Botox procedure, as it became known, quickly gained popularity around the world.\n\nCommercial DNA profiling has become available in Canada in recent years. For a fee, it is possible to order a number of specific tests including those for paternity, maternity, siblingship and ancestry. Companies offering this service include Genetrack Biolabs established in Vancouver, B.C. in 2003 and DNA Canada of Kingston, Ontario, established in 2005.\n\nEstablished in 2002 in Burnaby, British Columbia, Lifebank Cryogenics Corporation provides, on a commercial basis, a client-based service for the processing and cryogenic storage of stem cells from the umbilical cords of new-born babies. The cells may be of help in the treatment of disease that might affect the donor.\n\nDomestic construction has witnessed the introduction of improved building techniques and the smart home (home automation). Both the hydraulic lift and the concrete pump/crane, are now commonly used for home construction. Furthermore, homes are built with the electronics necessary for Internet connection throughout the premises. Household systems, such as heating and cooling, lighting, communications, entertainment and even food storage and cooking are now all linked to each other through the web. In the kitchen the glass-topped stove has become popular. The living room has seen the introduction of the very large flat screen, digital plasma TV, LCD TV and LED TV technologies, which have undergone dramatic price reduction in the last few years and have replaced the cathode-ray TV in consumer appliance/electronic stores. Also popular with consumers is the iPod portable music player introduced to Canadians in 2001 and the iPhone which was made available to Canadians by Rogers Wireless in 2008. The digital camera which was introduced to Canadians in the eighties has for the most part replaced the film camera in recent years. The electronic book or E-book has gained a place in Canada beginning with the introduction of the Sony Librie reader in 2004 and the Kindle in 2009. In 2010 the iPad wireless web surfing device became available to Canadians. Other such devices have been introduced in Canada including the BlackBerry PlayBook (available in 2011). The Blu-ray Disc and associated player have been marketed in Canada since 2009. The \"Guitar Hero\" music video game released in 2007 has enjoyed great success in Canada as has the Wii video game released that same year.\n\nAlthough 3D video games based on anaglyph image technology have been available in Canada since their introduction to the market in 1987, their popularity increased in 2009 partly as a result of marketing efforts by the maker of the 3D film \"Avatar\". Popular formats include Windows (3D), PS3 (3D), PSP, Wii, Xbox 360 (3D), DS and iPhone. Users must wear special glasses with a different coloured lens over each eye in order to experience the 3D effect.\n\nIn 2008, the large Canadian banks, including the Bank of Nova Scotia, the Royal Bank of Canada, the Toronto-Dominion Bank and the Canadian Imperial Bank of Commerce, began issuing Visa credit cards with an embedded microchip for enhanced security. Also in 2008, MasterCard Canada introduced the PayPass electronic payment system to Canada. The system uses a card/tag/phone equipped with an embedded computer chip and radio frequency antennae which is tapped on a PayPass reader at participating grocery stores, convenience stores, fast food restaurants or gas stations. The card/tag/phone, wirelessly transmits information about the customer to the reader which in turn electronically charges the appropriate sum to the customer's account. A similar concept using cell phones equipped with Near Field Communications (NFC) was introduced in 2009. Known as payWave, the technique is the result of cooperation between Visa, the Royal Bank of Canada and Rogers Communications. It is intended for fast, mobile, low-cost \"micro-payment\" transactions of items such as fast food, coffee, and subway tokens.\n\nBeginning in 2006, omega-3 oil became an additive in a number of foods sold in Canada.\n\nThe personal blood level alcohol tester or breathalyser was introduced to Canadians in 2010. The device, known as the BAQ Tracker, works the same way as those used by police. The user blows into a tube on the small portable hand-held machine and a digital readout of his or her blood alcohol level instantly appears on a display. Developed by Ladybug Technologies of Cambridge, Ontario, it sells for about $300.\n\nIn the 21st century, Canada's government has shown renewed interest in the acquisition of military technology, especially with its commitment to the war in Afghanistan. Equipment has been improved, including the CF-18 fighter with addition of laser-guided bombs and there are plans to update the Aurora patrol aircraft. The air force has also taken possession of the gigantic new C-17 Globemaster III long-range transport aircraft and has begun to renew the fleet of Hercules transport aircraft. The army has acquired the new Leopard 2 tank and C-777 long-range gun, and in 2009 announced the acquisition of the Close Combat Vehicle. In 2003, the Forces took possession of their first tactical unmanned aerial vehicle (TUAV), the French-designed CU-161 Sperwer, and the Heron UAV in February 2009. Used for the war in Afghanistan, these machines provide an intelligence, surveillance and reconnaissance (ISR) capability for the Forces. In 2008, the Air Force announced that it would acquire its first attack helicopters (Griffons equipped with light machine guns) for service there as well. In 2006, the Navy undertook the Halifax Class Modernization/Frigate Equipment Life Extension Project (HCM/FELIX) to modernize its 12 Halifax Class Frigates. New equipment will include improved computer fire control systems, sensors and the decoy-based Rheinmettal Multi Ammunition Softkill System, a passive missile defence system. Acquisitions pending include the CH-148 Cyclone ASW helicopter, the Chinook helicopter, new Arctic patrol vessels for the navy and a new ice breaker for the Canadian Coast Guard. In July 2010, the Government of Canada announced the C$9 billion purchase of 65 F-35A fighters for delivery beginning in 2016.\n\nIn 2009, the Canadian government announced a C$880 million upgrade, including new facilities, of the signal intelligence capability of the Communications Security Establishment Canada in Ottawa, to be completed by 2015. The importance of electronic warfare on the battlefield, as demonstrated in the War in Afghanistan, was highlighted in April 2010 by the formation of 21 Electronic Warfare Regiment at CFB Kingston. The unit, the first new regiment to be formed in the Canadian Army since WWII, is being equipped with the most modern electronic warfare technology and will practice both defensive and offensive electronic warfare.\n\nThe Polar Epsilon project, approved in 2005 and slated to be fully operational by 2011, uses Radarsat 2 to provide military commanders with imagery of Canada's Arctic. Another surveillance project, Polar Breeze (until recently classified secret), will use shore-based sea surface search radar, satellite-based (Radarsat 2) imagery and underwater listening devices to monitor sea surface and underwater traffic in the choke points of the Northwest Passage. The Canadian Forces have also acquired updated electronic equipment to conduct more advanced electronic warfare to face the new cybernetic threat and conduct cybernetic warfare (cyber-warfare).\n\nThe Taser (also known as the conducted energy weapon) has been adopted for use by Canadian police forces, including the RCMP, in recent years. The technology presently deployed was developed in the US in 1999. Intended for use as a \"non-lethal\" weapon, the Taser fires darts trailing wires connected to a battery in the hand-held pistol. The darts strike and lodge themselves in the suspect. The battery delivers, through the wires, a jolt of electricity that incapacitates the suspect. Its use in Canada has led to considerable controversy following the deaths of four individuals who were tasered by police in separate incidents in 2007.\n\nIn the earlier parts of Canada's history, the state often played a crucial role in the diffusion of these technologies, in some cases through a monopoly enterprise, in others with a private \"partner\". In more recent times, the need for the role of the state has diminished in the presence of a larger private sector.\n\nIn the latter part of the 20th century, there is evidence that Canadian values prefer public expenditures on social programmes at the expense of public spending on the maintenance and expansion of public technical infrastructure. This can be seen in the fact that in 2008 the Federation of Canadian Municipalities estimated that it would take $123 billion to restore and repair aging urban infrastructure across Canada.\n\n"}
{"id": "13530107", "url": "https://en.wikipedia.org/wiki?curid=13530107", "title": "Technological momentum", "text": "Technological momentum\n\nTechnological momentum is a theory about the relationship between technology and society over time. The term, which is considered a fourth technological determinism variant, was originally developed by the historian of technology Thomas P. Hughes. The idea is that relationship between technology and society is reciprocal and time-dependent so that one does not determine the changes in the other but both influence each other.\n\nHughes's thesis is a synthesis of two separate models for how technology and society interact. One, \"technological determinism\", claims that society itself is modified by the introduction of a new technology in an irreversible and irreparable way—for example, the introduction of the automobile has influenced the manner in which American cities are designed, a change that can clearly be seen when comparing the pre-automobile cities on the East Coast to the post-automobile cities on the West Coast. Technology, under this model, self-propagates as well—there is no turning back once the adoption has taken place, and the very existence of the technology means that it will continue to exist in the future.\n\nThe other model, \"social determinism\", claims that society itself controls how a technology is used and developed—for example, the rejection of nuclear power technology in the USA amid the public fears after the Three Mile Island incident.\n\n\"Technological momentum\" takes the two models and adds \"time\" as the unifying factor. In Hughes's theory, when a technology is young, deliberate control over its use and scope is possible and enacted by society. However, as a technology matures, and becomes increasingly enmeshed in the society where it was created, its own deterministic force takes hold, achieving technological momentum in the process. According to Hughes this inertia, which is particularly the case for large technological systems with their technological and social components, makes them difficult to influence and steer as they start to go more on their own way, assuming deterministic traits in the process. In other words, Hughes's says that the relationship between technology and society always starts with a \"social determinism\" model, but evolves into a form of \"technological determinism\" over time and as its use becomes more prevalent and important.\n\nSince its introduction by Hughes, the \"technological momentum\" concept has been applied by a number of other historians of technology. For instance, it is considered an effective approach to reconciling the apparently opposite perspectives of the autonomy of technology and the social and political motivations behind technological choices. It is able to describe how socially and politically conditioned technological institutions become independent and autonomous over time.\n\n"}
{"id": "26149061", "url": "https://en.wikipedia.org/wiki?curid=26149061", "title": "Tectonic weapon", "text": "Tectonic weapon\n\nA tectonic weapon is a hypothetical device or system which could create earthquakes, volcanoes, or other seismic events in specified locations by interfering with the Earth's natural geological processes. It was defined in 1992 by Aleksey Vsevolovidich Nikolayev, corresponding member Russian Academy of Sciences: \"A tectonic or seismic weapon would be the use of the accumulated tectonic energy of the Earth's deeper layers to induce a destructive earthquake\". He added \"to set oneself the objective of inducing an earthquake is extremely doubtful\".\n\nTheoretically, the tectonic weapon functions by creating a powerful charge of elastic energy in the form of deformed volume of the Earth's crust in a region of tectonic activity. This then becomes an earthquake once triggered by a nuclear explosion in the epicenter or a vast electric pulse. As to the question of whether a nuclear explosion can trigger an earthquake, there was the analysis of local seismic recordings within a couple of miles of nuclear tests in the 1960s at Nevada that showed nuclear explosions caused some tectonic stress. An account provided by a member of the Nevada Commission on Nuclear Projects, also claimed that a 1968 underground nuclear test called Faultless successfully induced an earthquake. The United States Geological Survey stated that it produced fresh fault rupture some 1200 meters long. There is also a theory that 1998 earthquake in Afghanistan was triggered by thermonuclear tests conducted in Indian and Pakistani test sites 2-20 days prior. \n\nRoger Clark, lecturer in geophysics at Leeds University said in the respected journal Nature in 1996, responding to a newspaper report that there had been two secret Soviet programs, \"Mercury\" and \"Volcano\", aimed at developing a \"tectonic weapon\" that could set off earthquakes from great distance by manipulating electromagnetism, said \"We don't think it is impossible, or wrong, but past experience suggests it is very unlikely\". According to Nature these programs had been \"unofficially known to Western geophysicists for several years\". According to the story the Mercury program began in 1987, three tests were conducted in Kyrgyzstan, and Volcano's last test occurred in 1992.\n\nSuch weapons, whether or not they exist or are feasible, are a source of concern in official circles. For example, US Secretary of Defense William S. Cohen, said on 28 April 1997 at the Conference on Terrorism, Weapons of Mass Destruction, and U.S. Strategy, University of Georgia, while discussing the dangers of false threats, \"Others are engaging even in an eco-type of terrorism whereby they can alter the climate, set off earthquakes, volcanoes remotely through the use of electromagnetic waves.\"\n\nNew Zealand's unsuccessful Project Seal programme during World War II attempted to create tsunami waves as a weapon. It was reported in 1999 that such a weapon might be viable.\n\nNikola Tesla claimed a small steam-powered mechanical oscillator he was experimenting with in 1898 produced earthquake-like effects, but this has never been replicated. The television show \"MythBusters\" in Episode 60 .E2.80.93 made a small machine based on the same principle but powered by electricity rather than steam; it produced vibrations in a large structure detectable 100 feet away, but no significant shaking, and they judged the effect to be a busted myth.\n\nThe 1978 Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques is an international treaty ratified by 75 states, and signed by a further 17, that prohibits use of environmental modification techniques to cause earthquakes and tsunamis, amongst other phenomena.\n\nAfter natural tectonic phenomena such as the 2010 Haiti earthquake, conspiracy theories, usually relating to the armed forces of the United States and formerly the Soviet Union, often arise, though no evidence is advanced. After the Haiti earthquake it was widely reported that president Hugo Chávez of Venezuela made unsupported allegations that it had been caused by testing of a US tectonic weapon. The newspaper Komsomolskaya Pravda of Moscow reported on page 1 on 30 May 1992 that \"a geophysical or tectonic weapon was actually developed in the USSR despite the UN Convention\", but that Chief Seismologist Major-General V Bochrov of the USSR Ministry of Defence categorically rejected any hints on the existence of tectonic weapons.\n\nWhile the British Tallboy and Grand Slam bombs of World War II were called \"earthquake bombs\", the name came from their way of destroying very hardened targets by shaking their foundations as an earthquake would; they were never intended to cause an actual earthquake.\n\n"}
{"id": "50177657", "url": "https://en.wikipedia.org/wiki?curid=50177657", "title": "The Industries of the Future", "text": "The Industries of the Future\n\nThe Industries of the Future is a 2016 non-fiction book written by Alec Ross, an American technology policy expert and the former Senior Advisor for Innovation to Secretary of State Hillary Clinton during her time as Secretary of State. Ross is also a senior fellow at Columbia University, a former night-shift janitor, and a Baltimore teacher. Ross launched a campaign for the Governor of Maryland in 2017. The book explores the forces that will change the world in robotics, genetics, digital currency, coding and big data.\n\nThe Industries of the Future by Senior Policy Advisor Alec Ross explores the geopolitical, cultural and generational changes that will be driven by the key industries over the next twenty years. Ross is a Distinguished Visiting Fellow at Johns Hopkins University and was the Senior Advisor for Innovation to Secretary of State Hillary Clinton. During his time as Senior Advisor for Innovation he visited forty-one countries looking at the technological advances. He has been a guest lecturer at a number of institutions including the United Nations, University of Oxford, Harvard Law School, and Stanford Business School. Ross started his career as a teacher through Teach for America and in 2000 he co-founded a technology-focused nonprofit organization called One Economy.\n\nThe book explores several industries including robotics, genetics, coding and big data. Ross explores how advances in robotics and life sciences will change the way we live—robots, artificial intelligence and machine learning will have impact on our lives. According to Ross, dramatic advances in life sciences will increase our life expectancy—but not all will benefit from such changes. Ross spends time exploring \"Code\" and how the codefication of money and also weapons (computer security) will both benefit and potentially disrupt our international economies. Ross also looks at how data will be \"the raw material of the information age.\"\n\nRoss also focuses on globalization and geopolitical economics. He explores how competitiveness and how societies, families and individuals will need to thrive. Ross gives attention to the importance of women stating that \"the states and societies that do the most for women are those that will be best positioned to compete and succeed in the industries of the future.\" The book also touches on how to prepare children for \"success in a world of increasing change and competition.\"\n\nRoss discusses the shift of robotics from being manual and repetitive to cognitive and non-repetitive. He believes that breakthroughs in mathematical modeling and cloud robotics (machine learning and Artificial Intelligence) make robotics acceptable. In the book Ross describes how other cultures have different reactions to robotics and he uses Japan's use of robotics in elder-care as an example. He also expects that less developed countries may be able to leapfrog technologies in robotics much like they did with cell and mobile technologies.\n\nAccording to Ross, the last trillion dollar industry was created out of computer code; the next trillion dollar industry will be created out of genome code. In the book Ross describes how genome code is already being used to fix humans from curing cancer to hacking the brain to growing organs. He also describes the difference between the United States investment in genome research with that of China.\n\nRoss then turns to the \"code-ification\" of money, markets and trust. He describes the transition from cash to mobile and online banking. He also discusses the sharing economy from eBay to AirBnB and then gives an overview of BitCoin and blockchain technology. Ross also focuses on cybersecurity and the weaponization of code with a focus on a move from cold war to \"code war.\" Ross states that he expects the total market size of the cyberindustry to reach $175 billion by the end of 2017.\n\nAlec Ross has said that he intended to give a balanced point of view with the book that it is neither a Utopian or Dystopian vision of the future which is why he opened the book with the struggles he witnessed growing up in West Virginia. On writing the book Ross said that he knows his parents would have wanted a book like this in the sixties that would describe what globalization would do and he wished that he had a book like this when he graduated from college that would have explored the Internet and digitization on the economy.\n\nThe editors for the book were Jonathan Karp and Jonathan Cox of Simon & Schuster.\n\n\"The Industries of the Future\" has received mainly positive reviews from the likes of \"Forbes\", \"New York Journal of Books\", and \"Financial Times\". \"Forbes\" contributor Peter Decherney said the book \"reads like a portable TED conference at which you've been seated next to the smartest guy in the room.\" The book was also listed on the Forbes list—\"16 New Books for Leaders to Begin in 2016\". Tara D. Sonenshine in the \"New York Journal of Books\" called the book a good place to start \"if you want to know how to survive and thrive in the fast-paced world of today and how to anticipate the opportunities of tomorrow's information age.\" Sonenshine also called out the book for focusing on women and multiculturalism. In an article titled \"Is predicting the future futile or necessary?\" by Stephen Cave in the \"Financial Times\" is more critical, saying that Ross focuses on industries with already considerable coverage and investment but Cave points out that \"rarely can the future be predicted by extending current trajectories.\"\n\nThe following trends are covered in the book:\n"}
{"id": "46301241", "url": "https://en.wikipedia.org/wiki?curid=46301241", "title": "TiE Silicon Valley", "text": "TiE Silicon Valley\n\nTiE Silicon Valley (TiE SV) is the largest and founding chapter of the TiE brand, a non-profit organization dedicated to fostering entrepreneurship. It provides technology entrepreneurs with mentoring services, networking opportunities, startup-related education, funding, and incubating.\n\nTiE SV was founded in 1992 by a group of successful entrepreneurs, corporate executives, and senior professionals with roots in the South Asian or Indus region and was named TiE for \"The Indus Entrepreneurs.\" It has since moved away from that focus and is open and inclusive.\n\nTiE SV received the 2014 Innovation Catalyst Award by VC Taskforce, which recognizes venture community leaders for being catalysts of innovation and entrepreneurship. Past recipients of this award include female venture capitalist Ann Winblad, the Draper family, and Reid Hoffman.\n\nTiE SV is a network of general members, Charter Members, and sponsors.\n\nCharter members are successful, veteran entrepreneurs who are looking to give back and assist the next generation of entrepreneurs by offering their time, knowledge, and resources. As of December 2014, TiE SV has more than 300 Charter members. This membership level is by invitation only. Since inception, this group has created upwards of a quarter trillion dollars of value worldwide.\n\nTiEcon is TiE SV's flagship annual conference. Since 2008, more than 4,000 people attend the conference from over 40 countries. It is widely considered the world's largest conference for entrepreneurs. The conference features two full days of networking and programming with thousands of entrepreneurs, venture capitalists, industry executives, and thought leaders.\n\nEach TiEcon, the world's 50 most promising technology startups are honored as the \"TiE50.\" These companies are selected from more than 1,600 companies screened worldwide. As of 2011, 94% of TiE50 companies had been funded, attracted over $20 billion in investments, and 42 of the companies exited. At TiEcon 2011, Cloudera, the leading provider of Apache Hadoop-based data management software and services, was announced as a TiE50 winner in the software/cloud computing category.\n\nTiEcon was listed by Worth Magazine in their September 2011 issue to be among the 10 Best Conferences for Ideas and Entrepreneurship.\n\nTiE Angels is an early stage Angel investment group formed in 2010 by Charter Members of TiE SV. There are about 100 investors that invest through TiE Angels. There is no TiE SV fund and individuals invest in their personal capacity. TiE Angels was ranked by CB Insights as one of the Top 20 Angel groups in the nation in August 2014.\n\nMost of TiE Angels investments are under $1 million, with a focus on enterprise solutions that leverage the technological expertise of TiE members. In its first year of existence, TiE Angels invested in 11 companies with a total of about $4.5 million. CloudVolumes, which was purchased by VMware in August 2014, was backed by TiE Angels and several individual Angel investors. TiE Angels also backed CRISI Medical which was acquired by BD Medical in March 2015.\n\nTiE LaunchPad is TiE SV's accelerator program for early stage startups. LaunchPad accepts eight companies per batch, and startups are seeded with $50,000 in convertible notes and offered optional working space, infrastructure, and additional support services for a five month duration. Companies also get assistance in fundraising by presenting to TiE's network of investors at a Demo Day at the end of the program. More than 50 Charter members serve as mentors to LaunchPad companies.\n\nThe Billion Dollar Babies Program is an initiative through TiE SV to mentor product companies out of India who are achieving significant domestic traction and wish to scale their products globally. It is managed by BV Jagadeesh, Raju Reddy (founder of Sierra Atlantic), and TiE SV President Venktesh Shukla. The pilot round of this program began January 2015.\n"}
{"id": "33287451", "url": "https://en.wikipedia.org/wiki?curid=33287451", "title": "Timeline of motor vehicle brands", "text": "Timeline of motor vehicle brands\n\nThis is a chronological index for the start year for motor vehicle brands (up to 1969). For manufacturers that went on to produce many models, it represents the start date of the whole brand; for the others, it usually represents the date of appearance of the main (perhaps only) model that was produced.\n\nThis also gives an idea of what motor vehicles were appearing on the streets in each country around each date (allowing, too, for imports from other countries). Moreover, by showing which models were contemporary, it gives a first indication of how individual designers were being influenced by each other, and a flavor of the entrepreneurial spirit and dynamics of the pioneering days of motor vehicle manufacture.\n\nWithin each year, and country of origin, the lists are structured according to the type of vehicle first introduced. These include the following types: steam, electric, hybrid electric, internal-combustion, touring car, roadster, tonneau, phaeton, cyclecar, light car, voiturette, runabout, high wheeler, buggy, tricar, motor quadricycle, motor tricycle, motorcycle, coach, bus, fire-engine, truck, tractor, racing car, avant-train.\n\nThomas Rickett's steam powered car was particularly notable in the history of motor vehicle production in as much that several examples were made, and it was also advertised.\n\nUK. Steam: Rickett\n\nUSA. Steam: Ware Steam Wagon\n\nThe Bollée family played a significant part in the history of motor vehicle manufacture; the father with his steam car, and one of his sons, in 1895, with an internal-combustion engine design.\n\nFrance. Steam bus: Amédée Bollée\n\nFrance. Steam: De Dion-Bouton (later internal-combustion, with a patent in 1889)\n\nFrance. Internal-combustion: Delamare-Deboutteville\n\nKarl Benz's vehicle was the first true automobile, entirely designed as such, rather than simply being a motorized stage coach or horse carriage. This is why he was granted his patent, and is regarded as its inventor. His wife and sons became the first true motorists, in 1889, when they took the car out for the specific task of paying a family visit.\n\nGermany. Internal-combustion: Benz\n\nUK. Internal-combustion: Butler\n\nCzech. Internal-combustion: Laurin & Klement (later Skoda)\n\nUSA. Electric: Armstrong Electric\n\n Motorcycle: Alexander Leutner & Co.\n\nUK. Motorcycle: New Imperial\n\nThe first Daimler car was a converted carriage, but with innovations that are still adopted today (cushioned engine mountings, fan cooling, finned-radiator water cooling).\n\nFrance. Steam: Peugeot (later internal-combustion, and the first to be entered in an organised race, albeit for bicycles, Paris–Brest–Paris)\n\nGermany. Internal-combustion: Daimler (DMG)\n\nUK. Internal-combustion: Santler\n\nUSA. Internal-combustion rotary engine: Adams-Farwell\n\nPanhard and Levassor's design of a front-mounted engine established the layout of the majority of cars since then.\n\nFrance. Internal-combustion: Panhard-Levassor\n\nUSA. Steam: Black; steam tractor: Avery; internal-combustion: Buckeye gasoline buggy\n\nFrance. Electric (and later internal-combustion): Jeantaud\n\nUK. Steam: Straker-Squire (also known as Brazil Straker)\n\nUSA. Internal-combustion: Elmore, Duryea\n\nFrance. Internal-combustion: Audibert & Lavirotte, Berliet, Delahaye\n\nUK. Electric: Garrard & Blumfield\n\nUSA. Electric: Electrobat\nFrance. Internal-combustion: Léon Bollée, Corre, Rochet-Schneider\n\nUK. Internal-combustion: Knight, Lanchester\n\nUSA. Electric: Morris & Salom\n\nUSA. Internal-combustion: De La Vergne\n\nIn the UK, the Locomotives on Highways Act 1896 replaced the hugely restrictive Locomotive Acts of 1861, 1865 and 1878 (the so-called \"Red Flag acts\") thereby finally freeing up the automotive industry in the UK (and, incidentally, was also the origin of the celebrations of the first London to Brighton Veteran Car Run). Knight had been convicted under the old act, the previous year, for not having a man precede his vehicle with a red flag, and Walter Arnold was the first person to be convicted, in January 1896, for exceeding the speed limit. Meanwhile, Serpollet was issued with what was effectively the first driving licence.\n\nFrance. Steam: Gardner-Serpollet; internal-combustion: Clément-Gladiator, Dalifol, Darracq, Lorraine-Dietrich, Triouleyre; voiturette: Dalifol & Thomas, Goujon, Léon Bollée; motorcycle: Clément and Gladiator\n\nItaly. Internal-combustion: Enrico Bernardi\n\nRussia. Internal-combustion: \n\nUK. Steam: Leyland; internal-combustion: Anglo-French, Arnold, Arrol-Johnston, Atkinson and Philipson; motorcycle: Excelsior, motor tricycle: Ariel\n\nUSA. Internal-combustion: Altham, Black, Electric & internal-combustion: Brewster, Haynes-Apperson\n\nFrance. Steam: Montier & Gillet; electric: Krieger; internal-combustion: Grivel, Juzan, Société Parisienne, Mors; voiturette: Decauville, Richard; avant-train: Amiot\n\nUK. Steam: Toward & Philipson; Electric: Bushbury Electric, Neale; electric phaeton: Electric Motive Power; internal-combustion: Belsize; bus: Thomas Harrington\n\nUSA. Electric: Pope; Internal-combustion: Autocar, Oldsmobile, Plass, Winton\n\nCzech Republic. Internal-combustion: Präsident (Tatra)\n\nBelgium. Internal-combustion: Delecroix, Métallurgique\n\nFrance. Internal-combustion: Ailloud, Astresse, Auge, David & Bourgeois, De Dietrich, Lufbery, Poron, Tourey; voiturette: Le Blon, De Riancey; trucks and tractors: Latil; avant-train: Ponsard-Ansaloni\n\nGermany. Electric: Kühlstein; internal-combustion: AWE, Wartburg\n\nItaly. Internal-combustion: Ceirano GB & C; motor tricycle/quadricycle: Prinetti & Stucchi\n\nUK. Electric: Oppermann; internal-combustion: Alldays & Onions, Grose, James and Browne, Madelvic, Star; tricar: Humber; motor tricycle/quadricycle: Arsenal, Eadie, Leuchters; motorcycle: Swift,\n\nUSA. Steam: American Waltham; electric: Riker; internal-combustion: Rutenber, St. Louis; buggy: Stearns\n\nBelgium. Voiturette: Vivinus\n\nFrance. Electric: Bouquet, Garcin & Schivre, Monnard; internal-combustion: Allard-Latour, Esculape, La Lorraine, Luc Court, Marot-Gardon, Raouval, Renault (including the first saloon car), Turcat-Méry; light car: Naptholette; voiturette: Andre Py, Cochotte, Populaire, Rouxel; alcohol fuelled: L'Alkolumine\n\nGermany. Internal-combustion: Opel\n\nItaly. Internal-combustion: Fiat\n\nRussia. Electric: Kukushka\n\nUK. Electric: Joel-Rosenthal; internal-combustion: Accles-Turrell, Geering; voiturette: Argyll; motor tricycle/quadricycle: Allard, Anglo-American; motorcycle: Coventry-Eagle, OK-Supreme, Quadrant, Royal Enfield\n\nUSA. Steam: Century, Grout, Kensington, Keystone, Kidder, Leach, Liquid Air, Locomobile, Mobile (pre Stanley Steamer), Strathmore, Victor Steam, Waltham Steam; electric: American Electric, Baker, Columbia (taxi), Electric Vehicle, Quinby, Stearns, US Automobile, Van Wagoner, Woods; internal-combustion: American, Black, Bramwell-Robinson, Gasmobile, Gurley, Holyoke, International, Media, Oakman-Hertel, Packard (Ohio), Quick, Sintz\n\nBelgium. Hybrid: Pieper; internal-combustion: Nagant, Pipe; voiturette: Antoine\n\nCanada. Electric: Canadian Motor\n\nFrance. Internal-combustion: Ader, Ardent, Chenard-Walcker, Maillard, Nanceene, Otto; voiturette: Chainless, Soncin; motorcycle: Buchet, Castoldi\n\nGermany. Internal-combustion: Adler, Albion; voiturette: AGG; motorcycle (later trucks): Phänomen\n\nItaly. Internal-combustion: Isotta Fraschini\n\nUK. Internal-combustion: Hewinson-Bell, Napier, Smith & Dowse; voiturette: Billings-Burns; motorcycle: Rex-Acme\n\nUSA. Steam: Tractobile, Kent's Pacemaker, Porter Stanhope, Skene, Steamobile; electric: Hewitt-Lindstrom, National; internal-combustion: Auburn, Canda, California, Eureka, Holley, Keystone, Knox, Lozier, Peerless, Rambler, Stearns-Knight; tractor: Samson; truck: Detroit\n\nCanada. Light car: Queen\n\nFrance. Internal-combustion: Charron, Corre La Licorne; voiturette: L'Ardennais, Guerraz, Henry-Dubray, Korn et Latil, Malliary; light car: Denis de Boisse\n\nGermany. Internal-combustion: Horch, Stoewer; motorcycle: NSU\n\nUK. Electric: Electromobile; internal-combustion: Asquith, Imperial, John O'Gaunt, Sunbeam, paraffin fuelled: Ralph Lucas; cyclecar: Campion; light car: Ralph Gilbert; voiturette: Wolseley; motorcycle: Matchless, Singer\n\nUSA. Steam: Aultman, Binney & Burnham, Covert, Desberon, Hidley, Hudson, Reading Steamer, Stearns, White; internal-combustion: Altman, Apperson, Buffalo, Buffum, De Dion, Empire, Marion, Pierce-Arrow, Schaum; touring car: Austin; runabout: Stevens-Duryea; high wheeler: Holsman; motorcycle: Indian\n\nBelgium. Internal-combustion: Minerva\n\nFrance. Internal-combustion: Motobloc, Richard-Brasier\n\nGermany. Internal-combustion: Aachener, AEG, Argus, Beaufort, NAG; motorised tricycle/quadricycle: Cyklon\n\nRussia. Electric: Dux\n\nSpain. Internal-combustion: Anglada\n\nUK. Steam: Vapomobile; internal-combustion: Abingdon, Armstrong, Karminski, Maudslay, Rover, Vulcan; voiturette: Esculapeus, tricar: Advance; motorcycle: Norton, Triumph\n\nUSA. Steam: Clipper, Hoffman, Richmond, Stanley; electric: Studebaker; internal-combustion: Blood, Brennan, Cadillac, Cameron, Cannon, Clarkmobile, Franklin (automobile), Gaeth, Hammer-Sommer, Kirk, Marmon, Reber; runabout: Glide (automobile), Smith, Standard Steel; touring car: Spaulding; light car: Greenleaf, Orient; buggy: American, Union; compound expansion: Eisenhuth; truck: Rapid\n\nBelgium. Internal-combustion: Excelsior\n\nFrance. Internal-combustion: Ariès, Clément-Bayard, Delaunay-Belleville, Hotchkiss, Regal, Talbot; light car: Henry Bauchet\n\nGermany. Internal-combustion bus/truck: Büssing\n\nUK. Electric: Lems; steam (and internal-combustion): Albany; internal-combustion: Attila, Elswick, Kyma, Lea-Francis, Lee Stroyer, Standard, Vauxhall, Whitlock; avant-train: Adams; motorcycle: Chater-Lea, New Hudson, Wilkinson Sword\n\nUSA. Steam: Jaxon; internal-combustion: American Chocolate (Walter), Bates, Ford, Lenawee, Marble-Swift, Matheson, Vermont, Wilson; touring car: Acme, Berg, Logan, Michigan, Iroquois, Jackson, Phelps, Premier; roadster: Buckmobile; runabout: Dingfelder, Eldredge, Marr, Mitchell, Overland, Sandusky, Tincher\n\nCanada. Internal-combustion: Russell\n\nFrance. Internal-combustion: Cottin & Desgouttes, Grégoire; voiturette: Lavie; motor tricycle: La Va Bon Train\n\nGermany. Internal-combustion: Alliance, Wenkelmobil\n\nItaly. Internal-combustion: Itala\n\nSpain. Internal-combustion: Hispano-Suiza\n\nUK. Electric: Imperial; internal-combustion: Arbee, Armstrong Whitworth, Ascot, Calthorpe, Chambers, Crossley, Croxted, Iden, Motor Carrier, Queen; voiturette: Achilles; light car: Gilburt; tricar: Garrard; motorcycle: Phelon & Moore, Zenith\n\nUSA. Steam: Empire Steamer; electric: Berwick, Marquette; internal-combustion: American, American Mercedes, American Napier, Christie, Cleveland, Corbin, Detroit Wheeler, Dolson, Lambert, Luverne, Maxwell, Moline, Orlo, Oscar Lear, Pierce-Racine, Queen, Sampson, Schacht, Sinclair-Scott (Maryland), Standard, Studebaker-Garford, Twyford Stanhope; touring car: Brew-Hatcher, Crane-Simplex, Crestmobile, Detroit Auto, Frayer-Miller, Jeffery, Pungs Finch, Richmond, Royal, Thomas, Upton; runabout: Courier, Fredonia, Northern, Pierce, Pope-Tribune; tractor: Holt\n\nFrance. Internal-combustion: Alliance, Brasier, Charlon, Couverchel, Delage, Eudelin, Rolland-Pilain, Sizaire-Naudin; touring car: Rebour; light car: Helbé, Urric; voiturette: Eureka; motorcycle: Herdtle & Bruneau\n\nGermany. Steam: Altmann; internal-combustion: Ehrhardt, Hansa, Hexe, Solidor\n\nItaly. Internal-combustion: Diatto, Zust\n\nUK. Electric: Alexandra, Ekstromer; internal-combustion: Adams, Austin, Edismith, Riley, Sunbeam-Talbot, Talbot; light car: One of the Best; tricar: Anglian; motorcycle: Velocette\n\nUSA. Electric: Rauch and Lang; internal-combustion: Aerocar, Ardsley, Ariel, Cartercar, Corwin, Crown, Harrison, Haynes, Silent Knight, Pullman, Rainier, Selden, Soules, Stoddard-Dayton; touring car: Detroit-Oxford, Diamond T, Gas-au-lec, Lambert, REO, USA Daimler; roadster: Walker, Western; light car: Bell, buggy: Deal, Hammer; motorcycle: Excelsior-Henderson, Harley-Davidson, Shawmobile\nBelgium. Internal-combustion: Imperia; hybrid: Auto-Mixte\n\nFrance. Internal-combustion: AM, Ampère, Antoinette, Lion-Peugeot, Unic; light car: Doriot, Flandrin & Parant; voiturette and motorcycle: Alcyon\n\nGermany. Internal-combustion: AAG\n\nItaly. Internal-combustion: Aquila Italiana, Fial, Peugeot-Croizat, SCAT, SPA, Standard\n\nUK. Internal-combustion: All-British, Ladas, Marlborough, Rolls-Royce; light car: Jowett; tricar: Addison, Armadale; dual-control car: Academy; hybrid bus: Tilling-Stevens; motorcycle: Dot\n\nUSA. Steam: Doble, Ross; electric: Babcock; internal-combustion: ALCO, American, American Simplex, Apollo, Atlas, Bliss, Car de Luxe, Deere, Dorris, Dragon, Frontenac, Hol-Tan, Jewell, Kissel, Model, Moore (Ball-Bearing Car); touring car: Heine-Velox, Moon; roadster: Colburn; light car: Janney; high wheeler: ABC, Black, McIntyre, Success\n\nBelgium. Internal-combustion: Springuel\n\nCanada. Internal-combustion: McLaughlin\n\nFrance. Internal-combustion: Ariane, Jean-Bart, Lahaussois, Lutier, Marie de Bagneux, Prod'homme, Sinpar, Sixcyl; voiturette: Couteret, Obus, La Radieuse; voiturette tricar: Guerry et Bourguignon, Lurquin-Coudert; tricar: Austral, Mototri Contal; hybrid: AL; amphibious: Ravailler; racing car: De Bazelaire\n\nUK. Internal-combustion: Dalgliesh-Gullane, Hillman; truck: Commer; motorcycle: Douglas\n\nUSA. Electric: American Juvenile Electric, Detroit Electric; internal-combustion: Allen Kingston, Anderson, Carter Twin-Engine, Continental, Corbitt, Fuller, Griswold, Maryland, Kiblinger, Oakland, Regal, Speedwell; high wheeler: Eureka, Hatfield, Single Center, Staver; roadster: CVI; runabout: Albany, Colt Runabout, Kermath, Marvel, Nielson\nFrance. Internal-combustion: Le Pratic, X; phaeton: Siscart; voiturette: Roussel\n\nGermany. Internal-combustion: Allright, Brennabor, Fafnir, Lloyd\n\nItaly. Internal-combustion: Lancia, Marca-Tre-Spade, Temperino\n\nRussia. Internal-combustion: Russo-Balt\n\nUK. Internal-combustion: Arno, Sheffield-Simplex, Valveless; touring car: Argon; light car: Alex; motorcycle: Premier\n\nUSA. Internal-combustion: Bendix, Coates-Goshen, Correja, Cunningham, De Luxe, General Motors, Gyroscope, Havers, Imperial, Paige, Sears, Velie; touring car: Moyer; high wheeler: Cole, De Schaum, DeWitt, Hobbie Accessible, Michigan; runabout: Simplo; cyclecar: Browniekar; buggy: Davis\n\nFrance. Internal-combustion: Bugatti, FL, La Ponette, Le Zèbre\n\nItaly. Racing car: Brixia-Zust; motorcycle: Della Ferrera\n\nNetherlands. Internal-combustion: Entrop\n\nUK. Internal-combustion: Pilot\n\nUSA. Internal-combustion: Abbott-Detroit, Anhut, Black Crow, Crow-Elkhart, Cutting, EMF, Everitt, Fuller, GJG, Hupmobile, Inter-State, Lion, Pilot; touring car: Crawford, Fal-Car, Piggins, Standard Six; roadster: Coyote, Hudson, Kauffman; runabout: Brush; small car: Herreshoff, Hitchcock, KRIT; light car: Courier; buggy: Paterson; raceabout: Mercer; racing car: McFarlan; truck: Chase, Sanford-Herbert\n\nCanada. Internal-combustion: Gareau\n\nFrance. Internal-combustion: Ageron, Damaizin & Pujos, Margaria, Mathis, Plasson; light car: Simplicia; cyclecar: Bédélia\n\nGermany. Internal-combustion: Ansbach, Apollo, Audi\n\nItaly. Internal-combustion: Alfa Romeo, Chiribiri\n\nUK. Steam: AMC; internal-combustion: Morgan, Siddeley-Deasy; cyclecar: GN\n\nUSA. Electric: Grinnell; internal-combustion: Alpena, Cavac, De Mot, Flanders, Great Eagle, Kline Kar, Lexington, Maytag-Mason, Parry, Spaulding, United States; touring car: Carhartt, Chalmers, Detroit-Dearborn, Etnyre, Faulkner-Blanchard, Great Southern; tonneau: Henry, Midland; roadster: Ames, King-Remick, Penn; runabout: Empire; cyclecar: Autoette; high wheeler: Anchor Buggy; buggy: Aldo\n\nCanada. Internal-combustion: Clinton\n\nFrance. Cyclecar: Enders\n\nGermany. Internal-combustion: Excelsior-Mascot, Podeus; rotary valve: Standard\n\nItaly. Motorcycle: Benelli\nUK. Internal-combustion: Aberdonia, AGR, Airedale, GWK, Newton-Bennett, Roper-Corbet; cyclecar: Alvechurch, Autotrix, Lambert; motorcycle: Beardmore, Coventry-Victor, Levis, Rudge-Whitworth, Villiers\n\nUSA. Electric: Hupp-Yeats, Century, Dayton Electric; internal-combustion: America, Ann Arbor, Chevrolet, Day, Gaylord, American Jonz (automobile) (The American), King, Komet, Marathon, Overland OctoAuto, Nyberg, Pilgrim of Providence, Rayfield, Stutz, Virginian, Willys; tractor: Mogul; fire-engine: Ahrens-Fox,\n\nCanada. Internal-combustion: Amherst\n\nFrance. Electric: Anderson Electric, internal-combustion: Albatros, Alda, Arista, Cognet de Seynes, Hédéa, La Roulette, SCAP; light car: Luxior, truck: Laffly, avant-train: Ponts,\n\nHungary. Internal-combustion: Raba\n\nItaly. Internal-combustion: Storero\n\nSpain. Internal-combustion: Abadal\n\nUK. Steam: Sheppee; internal-combustion: ABC; cyclecar: Adamson, Arden, Chota, Coventry Premier, Crouch, Hampton, HCE, Tiny, Tyseley; motorcycle: NUT, Sunbeam\n\nUSA. Electric: Argo Electric, Buffalo Electric, Church-Field; internal-combustion: Anna, Briggs-Detroiter, Crane & Breed, Pathfinder, Standard; touring car: Miller, Westcott; light-car: Lad's Car, Little; tricar: American Tri-Car, motorcycle: Cyclone; truck: Brockway, Palmer-Moore\n\nBelgium. Internal-combustion: Alatac\n\nFrance. Internal-combustion: Ajax, Alba, Alva, Rougier; cyclecar: Jouvie\n\nSpain. Cyclecar: David\n\nUK. Internal-combustion: Morris, Perry, Woodrow, WW; light car: Ace, Lucar; cyclecar: Armstrong, Athmac, Baker & Dale, Bantam, BPD, Britannia, Broadway, Carlette, Dallison, Dewcar, LAD, Lester Solus, Vee Gee, Warne, Wilbrook, Wrigley; motocycle: Montgomery\n\nUSA. Electric: American Electric; internal-combustion: Allen (Ohio), Allen (Philadelphia), Chandler, Flyer, Grant, Lyons-Knight, Monarch; cyclecar: Car-Nation, Coey, Detroit Cyclecar, Downing-Detroit, Dudly Bug, Gadabout, JPL, Little Detroit Speedster, Little Princess, Twombly; touring car: Keeton; roadster: Saxon, Scripps-Booth; sports car: Duesenberg; motocycle: Bi-Autogo\n\nFrance. Internal-combustion: Ascot, Donnet-Zedel; light car: Nardini\n\nJapan. Internal-combustion: DAT\n\nItaly. Maserati\n\nUK. Internal-combustion: Trojan, Utopian; light car: Bifort, cyclecar: Bradwell, Buckingham, Carden, Hill & Stanier, Imperial, Projecta, Simplic; motocycle: ABC\n\nUSA. Electric: Ward; internal-combustion: Ajax, American, Benham, Dile, Keystone, Light, Monroe, MPM, Partin, Willys-Knight; touring car: Alter; roadster: Metz, Vulcan; light car: Fischer, Lincoln; cyclecar: Argo, Arrow, Biesel, CAC, Cricket, Davis, Dodge, Engler, Excel, Hawk, Logan, LuLu, Malcolm Jones, Mercury, Motor Bob, O-We-Go, Xenia\nCanada. Internal-combustion: Gray-Dort, Regal\n\nUK. Internal-combustion: Atalanta; sports car: Aston Martin\n\nUSA. Electric: Menominee, hybrid electric: Owen Magnetic, internal-combustion: All-Steel, Apple, Biddle, Bour-Davis, Briscoe, Dort, Elcar, Herff-Brooks, Hollier, Ross, Smith Flyer, light car: Bell, Harvard, cyclecar: Koppin, racing car: Frontenac,\n\nRussia: AMO\n\nUSA. Electric: Belmont; internal-combustion: Aland, American Junior, Auto Red Bug, Bush, Daniels, Dixie Flyer, Hackett, HAL, Jordan, Liberty, Sun, Yale; touring car: Barley, Marion-Handley,\n\nGermany: BMW\n\nCanada. Internal-combustion: Moose Jaw Standard\n\nUK. Cyclecar: Gibbons\n\nUSA. Internal-combustion: Able, Amalgamated, American, Anderson, Columbia, Commonwealth, Piedmont, Shad-Wyck, Templar; touring car: Harroun, Nelson, Olympian; light car: Gem; truck: Nash,\n\nItaly: trucks OM\n\nUK. Internal-combustion: All British Ford; motorcycle: Cotton\n\nUSA. Steam: Bryan, internal-combustion: Essex; motorcycle: Ner-a-Car\n\nFrance. Internal-combustion: Avions Voisin, Butterosi, Citroen, Leyat, Salmson; cyclecar: ASS, Soriano-Pedroso\n\nGermany. Internal-combustion: AGA, Anker\n\nUK. Internal-combustion: Alvis, Angus-Sanderson, Armstrong Siddeley, Ashton-Evans, Bentley, Dawson, Eric-Campbell, Maiflower, Ruston-Hornsby, Willys Overland Crossley; cyclecar: Aero Car, Ashby, AV, Castle Three, Economic, Tamplin; motorcycle: Brough Superior, Coventry-Victor, Dunelt, Duzmo\n\nUSA. Internal-combustion: Amco, Argonne, Climber, Du Pont, Graham-Paige; truck: Huffman\n\nBelgium. Light car: ALP\n\nFrance. Electric: Electricar; internal-combustion: Janémian, Jouffret, Radior; cyclecar: Able, Ajams, Astatic, La Comfortable, De Marçay, Elfe, Kevah, Santax; sports car: Fonlupt\n\nGermany. Internal-combustion: Joswin, Selve; touring car: Steiger\n\nUK. Internal-combustion: Aeroford, Cubitt, Galloway, Palmerston, Payze; light car: Albert; cyclecar: Allwyn, Archer, Baughan, Bell, Black Prince, Blériot-Whippet, Bound, Cambro, CFB, Winson; sports car: Sports Junior\n\nUSA: Ace, Alsace, Aluminum, Astra, Binghamton Electric, Carroll, Colonial, Colonial/Shaw, Friend, Gardner, Gray Light Car, LaFayette, Lorraine, Mason Truck, Sheridan, Standard Steam Car, Stanwood\n\nCanada: Brock Six, London Six\n\nFrance: Amilcar, Ballot, Bernardet, Coadou et Fleury, Colda, Le Favori, Georges Irat, Hinstin, Janoir, Madoz, Quo Vadis, Le Roitelet, Solanet\n\nGermany: Alfi, Arimofa, Atlantic, Pawi, Rumpler Tropfenwagen, Zündapp\n\nItaly: Ansaldo, Aurea, IENA\n\nJapan: Ales\n\nUK: Amazon, Barnard, Scott Sociable, Skeoch\n\nUSA. Steam: Coats, Davis, internal combustion: Adria, Aero Car, Ajax, Automatic, Birmingham, Colonial, Davis Totem, Durant, Earl, Handley-Knight, Jacquet Flyer, Kessler, Wills Sainte Claire\n\nCanada: Colonial\n\nFrance: Astra, Bucciali, Induco, JG, Vaillant\n\nGermany: Juho, Komet\n\nUK: Abbey, Abingdon, Albatros, Alberford, Aster, Atomette, Autogear, Baby Blake, Bean, Bow-V-Car, Christchurch-Campbell, Clyno, Frazer Nash, Gwynne, Packman & Poppe, Wigan-Barlow, Xtra\n\nUSA. Steam: Alena, American Steamer, Endurance, internal combustion: ABC, Anahuac, Ansted-Lexington, Checker, DAC, Dagmar, Detroit, Gray, Jewett, Kess-Line 8, Rickenbacker, Star, Stewart-Coats\n\nBelgium: ADK, De Wandre, Juwel\n\nCanada. Steam: Brooks\n\nFrance: Bell, Henou, Willème\n\nGermany: Alan, Kenter, Pilot, motorcycle: BMW\n\nUK: Astral, Urecar\n\nUSA: Flint, Rugby\n\nCzech Republic: Skoda\n\nFrance: AEM, AS, Le Cabri, De Sanzy, Elgé, Jean Gras, Jousset\n\nGermany: Amor, Ehrhardt-Szawe, Tempo\n\nJapan: Otomo\n\nUK: HRD, Morris, Paydell\n\nUSA. Steam: American; internal-combustion: Chrysler, Junior R, Pennant\nBelgium: Jeecy-Vea\n\nFrance: Heinis, Jack Sport\n\nGermany: Hanomag, Sablatnig-Beuchelt, Seidel-Arop\n\nItaly: Amilcar Italiana, Maggiora, Moretti\n\nUK: Brocklebank, Invicta, Jappic, McEvoy, MG\n\nUSA: Empire Steam Car, Ajax, Diana\n\nFrance: Alma, Arzac, Chaigneau-Brasier, Constantinesco, Lambert, Ratier, SAFAF, Sensaud de Lavaud, Tracta\n\nGermany: Daimler-Benz, Gutbrod, Mercedes-Benz\n\nUK: Arab, HP, Marendaz, Swallow\n\nUSA: Ansted, Divco, Dodgeson\nFrance: Rosengart, Silva-Coroner\n\nUK: Arrol-Aster, Avro, Streamline (Burney Car)\n\nUSA: Falcon-Knight, Graham-Paige, LaSalle\n\nSweden: Volvo\n\nGermany: BMW, DKW\n\nUK: Ascot, Vincent\n\nUSA: DeSoto, Plymouth\nFrance: Alphi, Michel Irat\n\nGermany: Borgward\n\nItaly: Ferrari\n\nSoviet Union. Motorcycle: Izh\n\nSpain: National Pescara\n\nUK: Alta\n\nUSA: American Austin, Blackhawk, Cord, Roosevelt, Ruxton, Viking, Windsor\n\nBelgium: Astra\n\nFrance: AER, Virus\n\nGermany: Ardie-Ganz\n\nSoviet Union: KIM\n\nGermany: Maikäfer\n\nSoviet Union: ZIS\n\nJapan: Datsun\n\nUK: Squire\n\nUSA: De Vaux, Hoffman (Detroit automobile)\n\nItaly: Nardi\n\nPoland: Polski Fiat\n\nSoviet Union: GAZ\n\nUK: Vale Special\n\nUSA: Allied, De Vaux Continental, Jaeger\n\nFrance: Tracford\n\nGermany: Standard Superior\n\nUK: André, Railton\n\nUSA: Continental\n\nFrance: Simca\n\nGermany: Auto Union, Bungartz Butz\n\nJapan: Ohta Jidosha\n\nUK: Aveling-Barford, British Salmson, Rytecraft\n\nFrance: Talbot-Lago\n\nGermany: Henschel\n\nUK: Autovia, Batten, Jensen, Reliant\n\nUSA: Stout Scarab\n\nFrance: Darl'mat, Monocar\n\nUK: Allard, HRG, Lammas, Lloyd, Skirrow\n\nFrance: Ardex, Danvignes\n\nGermany: Volkswagen\n\nUK: Atalanta\n\nFrance: DB, Rolux\n\nUK: Nuffield\n\nSoviet Union: SMZ\n\nUSA: Albatross, Crosley, truck: Peterbilt\n\nUK: DMW\n\nSoviet Union: UAZ; motorcycle: IMZ-Ural\n\nBrazil. Trucks: F.N.M.\n\nSoviet Union. Trucks: Ural\n\nSoviet Union. Motorcycle: Dnipro\n\nUK: Bristol, Healey\n\nUSA: Kaiser-Frazer\n\nFrance: Chappe et Gessalin, Mochet, Rovin\n\nGermany: Messerschmitt\n\nHungary: Csepel\n\nItaly: Bandini, Cisitalia, Stanguellini; Trucks: Astra\n\nSoviet Union: Moskvitch; motorcycle: ZiD\n\nSpain: Pegaso\n\nUK: Cooper\n\nUSA: American Motors Incorporated, Frazer\n\nCanada: Studebaker\n\nFrance: Aerocarene, Alamagny\n\nItaly: Innocenti, Lambretta, Maserati, O.S.C.A.\n\nSoviet Union. Trucks: Minsk Automobile Plant\n\nUK: Ambassador, Ausfod, Buckler\n\nUSA: Airscoot, Davis, Playboy\nFrance: J-P Wimille\n\nGermany: Fend Flitzer\n\nItaly: Fimer, Iso Rivolta, Siata\n\nSoviet Union. Trucks: BelAZ\n\nUK: EMC, Land Rover, Rochdale, Thundersley Invacar\n\nUSA: Autoette, Keller, Tucker Sedan\n\nFrance: Atlas\n\nIndia: AUTOPRD\n\nSoviet Union: RAF\n\nJapan. Motorcycle: Honda\n\nUK: Dellow, Jaguar Cars\n\nUSA: Aerocar, Airway, Glasspar G2; scooter: PMC\n\nFrance: Autobleu\n\nGermany: Fuldamobil, Kersting-Modellbauwerkstätten, Kleinschnittger, Staunau\n\nSpain: SEAT\n\nUK: Marauder, Paramount\n\nUSA: Muntz\n\nFrance: Atlas, Automobiles Marathon, Le Piaf, Reyonnah\n\nGermany: Glas\n\nPoland: FSO\n\nSoviet Union. Trucks: KAZ; motorcycle: Minsk\n\nUK: Arnott, Russon, Turner\n\nUSA: Nash-Healey\n\nFrance: Martin-Spéciale, Poinard\n\nGermany: Brütsch, Champion\n\nSoviet Union: PAZ\n\nUK: Austin-Healey, BMC, Greeves, Lotus\n\nUSA: Allstate, Woodill\n\nGermany: EMW\n\nUSA: Eshelman, Fina-Sport\n\nFrance: Alpine, Facel Vega\n\nSpain: Serveta\n\nUK: Astra, Fairthorpe, Rodley, Swallow Doretti\n\nUSA: AMC, Studebaker-Packard\n\nBelgium: Meeussen\n\nFrance: Saviem, VELAM\n\nGermany: Goggomobil, Zwickau\n\nItaly: Autobianchi\n\nSoviet Union: LAZ, LuAZ\n\nUK: Ashley, Elva\n\nUSA: Tri5's\n\nFrance: Arista\n\nGermany: Heinkel Kabine\n\nSoviet Union: ZiL, KAG; scooter: TMZ, Vyatka\n\nUK: Berkeley, Tourette\n\nUSA: Auto Cub, Devin, Dual-Ghia\n\nFrance: Arbel, Atla\n\nGermany: Neckar, Trabant\n\nUK: Peerless (Warwick), Scootacar, Tornado\n\nUSA: Aurora, Hackney\n\nSoviet Union: KAvZ; trucks: BAZ, KrAZ\n\nUK: Gill, Frisky\n\nUSA: Edsel, Streco Turnpike Cruiser\n\nIndia: Vehicle Factory Jabalpur\n\nSoviet Union: LiAZ\n\nUK: Bristol Siddeley, Gilbern, Marcos\n\nUSA: Argonaut, Nu-Klea Starlite\n\nIndia: Ideal Jawa\n\nUK: Ausper, Brabham, Rickman\n\nUSA. Replica veteran car: Gaslight\n\nGermany: Amphicar\n\nSoviet Union: ZAZ\n\nUK: Diva\n\nCanada: Acadian\n\nFrance: Automobiles René Bonnet\n\nSoviet Union: AvtoKuban\n\nJapan. Motorcycle: Kawasaki\n\nUSA: Apollo\n\nItaly: ATS, Scuderia Serenissima, Lamborghini\n\nUK: Bond, Gordon-Keeble\n\nUSA: Exner Revival Cars; trucks: Marmon\n\nItaly: ASA\n\nSoviet Union: ErAZ\n\nUSA: Fiberfab\n\nFrance: Matra\n\nIndia: Heavy Vehicles Factory\n\nItaly: Ferves\n\nSoviet Union: IzhAvto\n\nSpain: IPV\n\nUK: Jago, Peel\n\nBulgaria: Bulgarrenault\n\nItaly: Bizzarrini\n\nSoviet Union: Lada; trucks: MoAZ\n\nRomania: Dacia\n\nUK: Norton-Villiers, Trident, Unipower\n\nTATA MOTORS\n\nItaly: Autozodiaco, LMX Sirex\n\nTurkey: Tofaş\n\nUK: Piper\n\nUSA: Savage GT\n\nSoviet Union. Trucks: Kamaz\n\nUK: Enfield\n\n"}
