{"id": "20441402", "url": "https://en.wikipedia.org/wiki?curid=20441402", "title": "Alliance Ground Surveillance", "text": "Alliance Ground Surveillance\n\nAlliance Ground Surveillance (AGS) is a NATO programme to acquire an airborne ground surveillance capability (Multi-Platform Radar Technology Insertion Program on the Northrop Grumman RQ-4 Global Hawk).\n\nIn a similar fashion as with Strategic Airlift Capability the program is run by 15 NATO member states: Bulgaria, Czech Republic, Denmark, Estonia, Germany, Italy, Latvia, Lithuania, Luxembourg, Norway, Poland, Romania, Slovakia, Slovenia and the United States. AGS is scheduled to reach initial operational capability by the end of 2017. The Main Operating Base will be located at Sigonella Air Base, Italy.\n\nFirst Global Hawk Block 40 UAVs destined for the NATO AGS program rolled off Northrop Grumman production line in Palmdale Ca , 4 June 2015 1 arrived at Edwards on Dec. 19, 2015 completing its first flight. the rest stayed in plant 42 located in Palmdale CA\n\nIn July 2017, the USAF assigned the Mission Designation Series (MDS) of RQ-4D to the NATO AGS air vehicle.\n\n\n"}
{"id": "35676267", "url": "https://en.wikipedia.org/wiki?curid=35676267", "title": "Asset Description Metadata Schema", "text": "Asset Description Metadata Schema\n\nThe Asset Description Metadata Schema (ADMS) is a common metadata vocabulary to describe standards, so-called interoperability assets, on the Web.\n\nUsed in concert with web syndication technology ADMS helps people make sense of the complex multi-publisher environment around standards and in particular the ones which are semantic assets such as ontologies, data models, data dictionaries, code lists, XML and RDF schemas. In spite of their importance, standards are not easily discoverable on the web via search engines because metadata about them is seldom available. Navigating on the websites of the different publishers of standards is not efficient either.\n\nA semantic asset is a specific type of standard which involves:\n\nOrganisations use semantic assets to share information and knowledge (within themselves and with others). Semantic assets are usually very valuable and reusable elements for the development of Information Systems, in particular, as part of machine-to-machine interfaces. As enablers to interoperable information exchange, semantic assets are usually created, published and maintained by standardisation bodies. Nonetheless, ICT projects and groups of experts also create such assets. There are therefore many publishers of semantic assets with different degrees of formalism.\n\nADMS is a standardised metadata vocabulary created by the EU's Interoperability Solutions for European Public Administrations (ISA) Programme of the European Commission to help publishers of standards document what their standards are about (their name, their status, theme, version, etc.) and where they can be found on the Web. ADMS descriptions can then be published on different websites while the standard itself remains on the website of its publisher (i.e. syndication of content). ADMS embraces the multi-publisher environment and, at the same time, it provides the means for the creation of aggregated catalogues of standards and single points of access to them based on ADMS descriptions. The Commission will offer a single point of access to standards described using ADMS via its collaborative platform, Joinup. The Federation service will increase the visibility of standards described with ADMS on the web. This will also stimulate their reuse by Pan-European initiatives.\n\nMore than 43 people of 20 EU Member States as well as from the US and Australia have participated in the ADMS Working Group. Most of them were experts from standardisation bodies, research centres and the EU Commission. The working group used a methodology based on W3C’s processes and methods.\n\nADMS version 1 was officially released in April 2012. Version 1.00 of ADMS is available for download on Joinup:\nhttps://joinup.ec.europa.eu/asset/adms/release/100\n\nADMS is offered under ISA's Open Metadata Licence v1.1\n\nThe ADMS specification reuses existing metadata vocabularies and core vocabularies including:\n\nADMS v1.00 will be contributed to W3C’s Government Linked Data (GLD) Working Group. This means that ADMS will be published by the GLD Working Group as First Public Working Drafts for further consultation within the context of the typical W3C standardization process. The desired outcome of that process will be the publication of ADMS as a W3C Recommendation available under W3C's Royalty-Free License.\n\nThe ADMS RDFS Vocabulary already has a w3.org namespace: http://www.w3.org/ns/adms#.\n"}
{"id": "34855410", "url": "https://en.wikipedia.org/wiki?curid=34855410", "title": "Biopeople", "text": "Biopeople\n\nBiopeople - Denmark's Life Science Cluster is a publicly funded partnership and National Center established, authorised, and funded by the Ministry for Science and Higher Education to improve innovation, collaboration and education within the National Danish Innovation System. Biopeople is established as a Center at the Faculty of Health and Medical Sciences at University of Copenhagen.\n\nBiopeople was the first European cluster organisation within health and life sciences to receive the recognition of Gold Label of the European Cluster Excellence Initiative (ECEI).\n\nBiopeople helps academia and industry to co-create and develop ideas into new projects, products and services to benefit global health and welfare. Biopeople embraces and clusters universities, research organisations, and hospitals, the National Board of Health (Denmark) / Danish Health and Medical Authority, industry associations as well as pharma, medtech, medical device, food and biotech companies. The aim is to stimulate innovation through activities that bring researchers and stakeholders together across disciplines, sectors and public-private boundaries.\n\nBiopeople embeds all relevant Danish stakeholders. Biopeople is a Center at University of Copenhagen. Member Companies affiliate by in-kind means by participating in innovation activities and projects. Companies include major large companies - fx Novo Nordisk, Lundbeck, LEO Pharma, Danisco, Novozymes, and Chr. Hansen – and many small and medium enterprises.\n\nThe founding partners are:\n\n\n\n\n"}
{"id": "44798958", "url": "https://en.wikipedia.org/wiki?curid=44798958", "title": "Center for Media, Data and Society", "text": "Center for Media, Data and Society\n\nThe Center for Media, Data and Society (CMDS) is a research center at Central European University (CEU) that focuses on media, communication and information policy. Located in Budapest, Hungary, CMDS produces scholarly and practice-oriented research about journalism, media freedom, and internet policy. \n\nThe Center was founded over a decade ago as the Center for Media and Communication Studies. It began in 2004, and was designed to serve as a focal point an international network of acclaimed scholars and academic institutions, whose research ranges from media and communications policy, fundamental communications rights through media and civil society and new media and digital technology to media in transition. In September 2014 it was relaunched as the Center for Media, Data and Society to represent new interests in technology policy, and big data. The Center has been led since September 2016 by Marius Dragomir, a media expert, journalist and scholar.\n\nThe Center’s research projects focus on identifying trends and challenges in the use of information technologies and advancing policy discussions about the regulation of data and media. The Center has an annual summer school on activism and digital media that draws democracy advocates, media activists, and civil rights campaigners from around the world to discuss policy and strategy. Current projects of CMDS include Creative Approaches to Living Cultural Archives, Ranking Digital Rights, Strengthening Journalism in Europe, Research on Violent Online Political Extremism, Media Influence Matrix.\n\nRecent projects include a study of privacy violations in Europe called the \"European Privacy Report\", which found that the most significant source of data breaches are security lapses in the workplace. The Center is also known for its monitoring work on media policy in Central and Eastern Europe and Hungary in particular.\n\n"}
{"id": "28964679", "url": "https://en.wikipedia.org/wiki?curid=28964679", "title": "Collaborative e-democracy", "text": "Collaborative e-democracy\n\nCollaborative e-democracy or super-democracy is a democratic conception that combines key features of direct democracy, representative democracy, and e-democracy (i.e. the use of ICTs for democratic processes). The concept was first published at two international academic conferences in 2009 (see below).\n\nCollaborative e-democracy refers to a political system in which governmental stakeholders (politicians/parties, ministers, parliamentarians etc.) and non-governmental stakeholders (NGOs, political lobbies, local communities, individual citizens, etc.) collaborate on the development of public laws and policies. This collaborative policymaking process is conducted on a governmental social networking site in which all citizens are members (collaborative e-policy-making).\n\nWhile directly elected government officials (i.e. ‘proxy representatives’) would conduct the vast majority of law and policy-making processes (representative democracy), the citizens would retain their final voting power on each issue (direct democracy). Additionally each citizen would be empowered to propose their own policies to the electorate and thus initiate new policy processes where applicable (initiative). Collaboratively generated policies would consider the opinion of a larger proportion of the citizenry; therefore they may be more just, more sustainable, and thus easier to implement.\n\nCollaborative e-democracy involves following theory components:\n\nCollaborative e-policymaking is a process where public laws & policies are generated in collaboration of multiple stakeholders (e.g. affected people; domain experts; parties who can help to implement a solution). Each new policy cycle starts with the identification of a collective problem or goal by the collective of participants (i.e. citizens, experts, proxy representatives).\n\nTo be clear, CPM is automated as a software process that is conducted on the governmental social networking site.\n\nCollaborative e-democracy is based on following core principles:\n\nThe concept of collaborative e-democracy intends to achieve following benefits:\n\nOn the contrary the concept has several limitations:\n\nIn 2009 the two conceptions, \"collaborative e-democracy\" and \"collaborative e-policy-making\", were first published at two academic conferences on e-governance and e-democracy:\n\nCurrent research is conducted at the Queensland University of Technology in Brisbane, Australia where a prototype software for collaborative e-policymaking is being developed. The software will be featured on an independent web 2.0 platform with open participation; the volunteer participants will be facilitated to collaboratively develop Australian public laws and policies by proceeding through the phases of the CPM process. The start of the trialling period is probably 2011.\n\n\n"}
{"id": "45533018", "url": "https://en.wikipedia.org/wiki?curid=45533018", "title": "CougarTech", "text": "CougarTech\n\nCougarTech, FRC team 2228, is a FIRST Robotics Competition team that was founded in 2007, and is a school-based team from the Honeoye Falls-Lima Central School District in Honeoye Falls, New York. The team also represents the Rush–Henrietta Central School District in competition. CougarTech is a veteran team that prides itself on its coopertition with other teams as well as its teaching of younger teams. During the all important six weeks of build season, the team builds a robot to play the year's new game that FIRST designs each year. During the off season, the team focuses work on public outreach, recruitment, fundraising and sponsorship to support their team and FIRST. \n\n\n"}
{"id": "16764104", "url": "https://en.wikipedia.org/wiki?curid=16764104", "title": "Courreges ZOOOP", "text": "Courreges ZOOOP\n\nThe Zooop is a three-seat electric car produced by the Paris-based fashion house Maison de Courrèges.\n\nThe Courreges Zooop performance is 150 kW and weighing just 690 kilograms, also features a range of 450 kilometers. Remarkably, the car is not produced for a car manufacturer, but it is produced by the famous fashion design house Maison de Courrèges, which peculiarly did scarce promotion for its new model outside its native country.\n"}
{"id": "3990817", "url": "https://en.wikipedia.org/wiki?curid=3990817", "title": "Differential technological development", "text": "Differential technological development\n\nDifferential technological development is a strategy proposed by transhumanist philosopher Nick Bostrom in which societies would seek to influence the sequence in which emerging technologies developed. On this approach, societies would strive to retard the development of harmful technologies and their applications, while accelerating the development of beneficial technologies, especially those that offer protection against the harmful ones.\n\nPaul Christiano believes that while accelerating technological progress appears to be one of the best ways to improve human welfare in the next few decades, a faster rate of growth cannot be equally important for the far future because growth must eventually saturate due to physical limits. Hence, from the perspective of the far future, differential technological development appears more crucial.\n\nInspired by Bostrom's proposal, Luke Muehlhauser and Anna Salamon suggested a more general project of \"differential intellectual progress\", in which society advances its wisdom, philosophical sophistication, and understanding of risks faster than its technological power. Brian Tomasik has expanded on this notion.\n\n"}
{"id": "15376280", "url": "https://en.wikipedia.org/wiki?curid=15376280", "title": "Dumb pipe", "text": "Dumb pipe\n\nWith regard to a mobile network operator (MNO, or \"operator\"), the term dumb pipe, or dumb network, refers to a simple network that, with a high enough bandwidth to transfer bytes between the customer's device and the Internet without the need to prioritize content, can afford to be completely neutral with regard to the services and applications the customer accesses. The use of the term \"dumb\" refers to the fact that the network operator does not affect the customer's accessibility of the Internet such as by either limiting the available services or applications to its own proprietary portal (like a walled garden) or offer additional capabilities and services beyond simple connectivity (like a smart pipe, the term with which it contrasts). A dumb pipe primarily provides simple bandwidth and network speeds greater than the maximum network loads expected thus avoiding the need to discriminate between data types.\n\nAmong the commonly understood operational models for a MNO are the dumb pipe, the smart pipe, and the walled garden.\n\nA dumb network is marked by using intelligent devices (i.e. PCs) at the periphery that make use of a network that does not interfere with or manage an application's operation / communication. The dumb network concept is the natural outcome of the end to end principle. The Internet was originally designed to operate as a dumb network.\n\nIn some circles the dumb network is regarded as a natural culmination of technological progress in network technology. With the justification that the dumb network uniquely satisfies the requirements of the end to end principle for application creation, supporters see the dumb network as uniquely qualified for this purpose, as – by design – it is not sensitive to the needs of applications. The dumb network model can, in some ways, allow for flexibility and ease of innovation in the development of applications that is not matched by other models.\n\nOne widely regarded example of the operator \"dumb pipe\" scenario is Apple's iPhone. The iPhone enables its users to directly surf the Internet with its mobile Safari browser and connects to Apple's iTunes Store for purchasing ringtones and music instead of the operator's own portal. Operators such as AT&T Mobility cannot offer their traditional services (such as downloads of wallpapers, ringtones, games, applications, etc.) as Apple controls the total iPhone user experience. Operators must be content to provide only the network connectivity and bandwidth which the iPhone has tripled in some cities. In addition to losing valuable revenue opportunities with the customer, operators are rumored to pay Apple a percentage of the customer's monthly bill as well. While the iPhone is a good example of the dumb pipe, not everyone believes it will ultimately be bad for operators.\n\nAnother example of the operator dumb pipe / smart pipe dilemma is the upcoming deployment of WiMAX technology. Companies such as Sprint Nextel and Clearwire are looking into ways to deploy WiMAX with additional services to keep them from becoming dumb pipes.\n\nCritics of dumb network architecture posit two arguments in favor of \"intelligent\" networks. The first, that certain users and transmission needs of certain applications are more important than others and thus should be granted greater network priority or quality of service. An example is that of real time video applications that are more time sensitive than say, text applications. Thus video transmissions would receive network priority to prevent picture skips, while text transmissions could be delayed without significantly affecting its application performance. The second is that networks should be able to defend against attacks by malware and other bad actors.\n\nThe dumb network (and the end to end principle) was conceived of as an antithesis to the idea of a centralized intelligent computer network in which all applications were under central network control. A synthesis is taking place in the context aware networks. These networks allow intelligent devices to set up end-to-end applications as in the dumb network. However, they are aware of application needs and in the social and enterprise context in which the applications are being used. Thus the network can make decisions on resource allocation conflicts in light of the collective needs of all users and the purposes (social and enterprise) that guide them.\n\nAdvocates of dumb networks counter the first argument by pointing out that prioritizing network traffic is very expensive in monetary, technology, and network performance. Dumb networks advocates also consider the real purpose for prioritizing network traffic is to overcome insufficient bandwidth to handle traffic and not a network protocol issue. The security argument is that malware is an end-to-end problem and thus should be dealt with at the endpoints, and that attempting to adapt the network to counterattacks is both cumbersome and inefficient.\n\n\n"}
{"id": "46311074", "url": "https://en.wikipedia.org/wiki?curid=46311074", "title": "Electronic personal dosimeter", "text": "Electronic personal dosimeter\n\nThe electronic personal dosimeter (EPD) is an electronic device that has a number of sophisticated functions, such as continual monitoring which allows alarm warnings at preset levels and live readout of dose accumulated. These are especially useful in high dose areas where residence time of the wearer is limited due to dose constraints. The dosimeter can be reset, usually after taking a reading for record purposes, and thereby re-used multiple times.\n\nPIN diodes are used to quantify the radiation dose for military and personnel applications.\n\nMOSFET dosimeters are now used as clinical dosimeters for radiotherapy radiation beams. The main advantages of MOSFET devices are:\n\n1. The MOSFET dosimeter is direct reading with a very thin active area (less than 2 μm).\n\n2. The physical size of the MOSFET when packaged is less than 4 mm.\n\n3. The post radiation signal is permanently stored and is dose rate independent.\n\nGate oxide of MOSFET which is conventionally silicon dioxide is an active sensing material in MOSFET dosimeters. Radiation creates defects (acts like electron-hole pairs) in oxide, which in turn affects the threshold voltage of the MOSFET. This change in threshold voltage is proportional to radiation dose. Alternate high-k gate dielectrics like Hafnium oxide, Hafnium dioxide and Aluminum oxides are also proposed as a radiation dosimeters.\n\n"}
{"id": "1295520", "url": "https://en.wikipedia.org/wiki?curid=1295520", "title": "Eugene Podkletnov", "text": "Eugene Podkletnov\n\nEugene Podkletnov (, Yevgeny Podkletnov) is a Russian ceramics engineer known for his claims made in the 1990s of designing and demonstrating gravity shielding devices consisting of rotating discs constructed from ceramic superconducting materials.\n\nPodkletnov graduated from the University of Chemical Technology, Mendeleyev Institute, in Moscow; he then spent 15 years at the Institute for High Temperatures in the Russian Academy of Sciences. He received a doctorate in materials science from Tampere University of Technology in Finland. After graduation he continued superconductor research at the university, in the Materials Science department, until his expulsion in 1997. After which he moved back to Moscow where it is reported that he took an engineering job. Since leaving Tampere in 1997 Podkletnov has avoided public contact or appearances. There is a report that he later returned to Tampere to work on superconductors at Tamglass Engineering Oy.\n\nAccording to the account Podkletnov gave to Wired reporter Charles Platt in a 1996 phone interview, during a 1992 experiment with a rotating superconducting disc:\n\nPodkletnov's first peer-reviewed paper on the apparent gravity-modification effect, published in 1992, attracted little notice. In 1996, he submitted a longer paper, in which he claimed to have observed a larger effect (2% weight reduction as opposed to 0.3% in the 1992 paper) to the \"Journal of Physics D\". According to Platt, a member of the editorial staff, Ian Sample, leaked the submitted paper to Robert Matthews, the science correspondent for the British newspaper, the \"Sunday Telegraph\".\n\nOn September 1, 1996, Matthews's story broke, leading with the startling statement: \"Scientists in Finland are about to reveal details of the world's first antigravity device.\" In the ensuing furor, the director of the laboratory where Podkletnov was working issued a defensive statement that Podkletnov was working entirely on his own. Vuorinen, listed as the paper's coauthor, disavowed prior knowledge of the paper and claimed that the name was used without consent. Podkletnov himself complained that he had never claimed to block gravity, only to have reduced its effect.\n\nPodkletnov withdrew his second paper after it had been initially accepted. The resulting furor over the alleged claims in the withdrawn paper is reported to be the primary reason for his expulsion from his lab and the termination of his employment at the university.\n\nIn a 1997 telephone interview with Charles Platt, Podkletnov insisted that his gravity-shielding work was reproduced by researchers at universities in Toronto and Sheffield, but none have come forward to acknowledge this. The Sheffield work is known to have only been intended as partial replication, aimed at observing any unusual effects which might be present, since the team involved lacked the necessary facilities to construct a large enough disc and the ability to duplicate the means by which the original disc was rotated. Podkletnov counters that the researchers in question have kept quiet \"lest they be criticized by the mainstream scientific community\". Podkletnov is reported to have visited the Sheffield team in 2000 and advised them on the conditions necessary to achieve his effect, conditions that they never achieved.\n\nIn a BBC news item, it was alleged that researchers at Boeing were funding a project called GRASP (Gravity Research for Advanced Space Propulsion) which would attempt to construct a gravity shielding device based on rotating superconductors, but a subsequent \"Popular Mechanics\" news item stated that Boeing had denied funding GRASP with company money, although Boeing acknowledged that it could not comment on \"black projects\". It is alleged that the GRASP proposal was presented to Boeing and Boeing chose not to fund it.\n\nAlso during the 1997 telephone interview with Platt, Podkletnov said that he was continuing to work on gravitation, claiming that with new collaborators at an unnamed \"chemical research center\" in Moscow he has built a new device. He said:\n\n\n"}
{"id": "53413612", "url": "https://en.wikipedia.org/wiki?curid=53413612", "title": "FOSS Movement in India", "text": "FOSS Movement in India\n\nFOSS Movement in India refers to the campaign across the country during the 1990s and 2000s in particular, to promote Free and Open Source Software. It was marked by the existence of many Indian Linux User Groups (ILUGs) groups and Free Software User Groups (FSUGs) in different cities, town and other areas.\n\nThe prominent members of the campaign include the late Atul Chitnis, Prof Nagarjuna G. and others.\n\n"}
{"id": "46804622", "url": "https://en.wikipedia.org/wiki?curid=46804622", "title": "Factum Arte", "text": "Factum Arte\n\nFactum Arte is a company based in Madrid, Milan, and London that seeks to construct a bridge between new technologies and craft skills in the conservation of cultural heritage and in contemporary art. By using various forms of high-definition 3D scanners, Factum Arte has been able to record, in digital form using non-contact equipment, a number of endangered sites/objects of cultural importance. This is done in conjunction with the Factum Foundation for Digital Technology in Conservation, which seeks to promote the use of non-contact 3D scanners to record museum collections and historic monuments, especially in areas where they are at risk.\n\nIn addition to recording objects, Factum Arte is able to use the digital data to create an exact facsimile of the object on a scale of 1:1. In 2014, Factum Arte completed the installation of an exact facsimile of the tomb of Tutankhamun in the Valley of the Kings in Luxor, near Howard Carter’s house. The facsimile, and its proximity to the original tomb, is intended to provoke a debate about preservation; as Factum Arte’s Director, Adam Lowe, was said: \"The tomb of Tutankhamun was built to last for eternity, but it wasn’t built to be visited\".\n\nOver the years, Factum Arte has worked with institutions such as the British Museum in London, the Musée du Louvre in Paris, the Pergamon Museum in Berlin, the Museo del Prado in Madrid, and the Supreme Council of Antiquities in Egypt. In addition to its work in the field of cultural heritage, Factum Arte also assists a wide range of contemporary artist in creating technically difficult and innovative works of art.\n\nFactum Arte was founded in 2001 by the artists Adam Lowe, Manuel Franquelo, with Fernando Garcia-Guereta to facilitate the recording of the Tomb of Seti I and works with a number of artists including Marc Quinn and Anish Kapoor. The Seti project involved the design and construction of 3D laser scanners, software, and photographic equipment to record the walls of the tomb at high-resolution.\n\nFactum Arte was founded in 2001 in order to facilitate the development of technology needed specifically for the recording of the Tomb of Seti I.\n\nSeti’s tomb is regarded by many as the most visually impressive, and historically important tomb in the Valley of the Kings. Discovered by Giovanni Battista Belzoni in October 1817, the tomb of Pharaoh Seti I is the longest and one of the most decorated tombs in the Valley of the Kings. Despite being in excellent condition on its discovery, the tomb is currently closed to visitors to the Valley due to its deteriorating condition over the years. In addition to the removal of wall panels, and the loss of paint due to 19th century plaster casts, the tomb has suffered from collapses and cracks due to expeditions searching for hidden chambers in the 1950s and 60s that caused changes in the moisture levels of the surrounding rock.\n\nFactum Arte was commissioned by United Exhibits Group to make a 1:1 facsimile of the Tomb of Thutmose III in 2002. The facsimile was toured at exhibitions in various museums in the United States between November 2002 and December 2007. In 2005 a second facsimile of the tomb was exhibited in Madrid, Edinburgh, and Basel titled Immortal Pharaoh: The Tomb of Thutmose III (Edinburgh) and The Tomb of Thutmose III: The Dark Hours of the Sun (Madrid and Basel).\n\nThe Tomb of Thutmose III is the oldest complete version of the narrative of the Egyptian Amduat, the journey the Sun God takes through the hours of the night.\n\nThe facsimile of the tomb was installed briefly outside the Conrad Hotel in Cairo for the EU-Egypt Business and Tourism Summit and was unveiled by Catherine Ashton the European High Representative of the Union for Foreign Affairs in November 2012 as a gift to the people of Egypt, coinciding with the 90th anniversary of the tomb’s discovery.\nIn 2014 Factum Arte installed the facsimile in the Valley of the Kings, beside Howard Carter’s house, the consultant of the construction site where the facsimile was installed was \"Tarek Waly center architecture and heritage\". It was unveiled by the Minister of Antiquities Mohamed Ibrahim, the Minister for Tourism Hisham Zazou, the Governor of Luxor Tarek Saad el Din and EU Ambassador James Moran. The aim of the facsimile is to inform visitors to the valley about the importance of preservation and to promote awareness about the degrading state of the tombs since their opening to tourists.\n\nIn August 2015, Egyptologist Nicholas Reeves published a paper in which he hypothesised the presence of the tomb of Nefertiti as being behind one of the walls of Tutankhamun's tomb. Reeves' based his theory on markings he observed on the wall in the data recorded by Factum Arte. In September 2015, Egyptian newspaper \"Ahram\" reported that initial examinations had confirmed the existence of the wall markings observed by Reeves in Factum Arte's data. The same article reported that the results of further examinations would be published on 4 November 2015; the same day that the tomb was discovered in 1922.\n\nIn 2004, during the Second Gulf War, Factum Arte and Danish company United Exhibits Group (UEG) embarked on a project to record, and create a facsimile of the throne room of Ashurnasirpal II in the ancient city of Nimrud. Fragments of the throne room exist in the collections of various museums in Europe and the United States. Factum Arte was given permission to record these fragments in the British Museum, the Pergamon in Berlin, Dresden, Harvard, and Princeton.\n\nUnfortunately, at the time, Iraq was considered too dangerous to send a team out to record the remaining fragments in Nimrud. In 2015 the Islamic State militants in Northern Iraq destroyed much of the remaining artwork in the ruins of the palace of Nimrud. While recording these fragments in 2005 would not have prevented their destruction by ISIS, it would have kept avenues open to further in-depth study through the high-resolution 3D data, and presented the possibility of reuniting the fragments in the form of facsimile.\n\nIn November 2007, Factum Arte’s facsimile of \"The Wedding Feast at Cana\" (1563), by Paolo Veronese, was presented by the Cini Foundation in the original location of the painting, the Andrea Palladio's refectory for the Monastery of San Giorgio Maggiore, Venice. The original painting, commissioned in 1562, was plndered by the French Revolutionary Army of Napoleon in 1797 and sent to the Louvre Museum, where it hangs opposite the Mona Lisa. The facsimile was commissioned in 2006 by the Fondazione Giorgio Cini and, following an agreement with the Louvre, Factum Arte’s technicians were allowed to scan the painting at night. \"Corriere della Sera\" called the facsimile a \"turning point in art\".\n\nIn 2010 the Cini Foundation commissioned the visualisation and manufacture of objects designed by the 18th century artist and antiquarian Giambattista Piranesi. The project was conceived by Adam Lowe, Michele de Lucchi, and John Wilton-Ely and was exhibited in the Cini Foundation on the island of San Giorgio Maggiore for the Venice Biannale. The objects were later toured for exhibitions in Madrid, Barcelona, and San Diego\n\nIn March–May 2014, Factum Arte exhibited the series at the Sir John Soane Museum in London. Diverse Manieri: Piranesi, Fantasy and Excess aimed to explore the relationship between Sir John Soane and Piranesi. The objects were shown in the context of prints, drawings and books in Soane’s library.\n\nThe objects were visualised in digital form from Piranesi’s designs and then rematerialized in three dimensions in the materials specified in the design. The manufacture of the objects involved a variety of methods including stereo-lithography, milling, fused deposition modelling, electro forming and electro plating, in addition to a host of moulding and casting technologies\n\nThe 16 panels of the Polittico Griffoni once formed the altarpiece of the Basilica of San Petronio in Bologna. It was considered one of the greatest altarpieces of the 15th Century Bolognese School. The panels were originally painted by Francesco del Cossa and Ercole De Roberti. The panels, removed in 1725, are now scattered in various museums in Italy, the United Kingdom, the United States, France, the Netherlands, and the Vatican City.\n\nUsing the Lucida 3D scanner, designed by Manuel Franquelo, Factum Arte and the Factum Foundation for Digital Technology in Conservation collaborated with San Petronio Basilica to record, reproduce and reunite the panels as a facsimile in their original location.\n\nOther projects in the realm of cultural conservation include:\n\nFactum Arte has developed a number technologies in order to better facilitate the recording and production of objects.\n\n\nFactum Arte collaborates with a large number of companies and individuals from the tech industry and the art world.\n\nFactum Arte has undertaken projects with, among others, the following artists:\n\n\nIn 2013, when referring to the facsimile of the Tomb of Tutankhamun and the facsimile of the caves at Lascaux, historian Tom Holland voiced criticism of the idea of creating \"fakes\" as a means to protect the originals:\nIn our society, there is a huge premium set on authenticity. Clearly, were there not a difference between the copy and the original, it wouldn’t matter – you could make a replica and trash the original. Tutankhamun and Lascaux were created by people who believed in the world of the spirits, the dead, and the supernatural. You don’t have to believe in a god or gods to feel a place is consecrated and has a particular quality that cannot be reproduced.\n\n"}
{"id": "36593145", "url": "https://en.wikipedia.org/wiki?curid=36593145", "title": "GeoEdge", "text": "GeoEdge\n\nGeoEdge is a provider of ad security and verification solutions for the online and mobile advertising industry. GeoEdge guards against non-compliance, malware, inappropriate content, data leakage, operational, and performance issues.\n\nGeoEdge relies on a proprietary network of proxy servers worldwide to browse sites locally. It further developed software that emulates user experiences in different locations and devices. GeoEdge captures, monitors and analyzes all ads served by any ad network. The service screens ad tags, keywords, URLs and malicious content. In case of malware or non-compliant ads are identified, a real-time alert will be triggered.\nIn November 2011, the company was awarded the “Best New Technology” by ClickZ.\n\nIn July 2017, Vertoz, the programmatic advertising company partnered with GeoEdge.\n\n\n"}
{"id": "59210786", "url": "https://en.wikipedia.org/wiki?curid=59210786", "title": "Hermes 3000", "text": "Hermes 3000\n\nThe Hermes 3000 was a lightweight, segment-shifted portable typewriter manufactured by Paillard-Bolex. \"Bulbous\" and \"angular\" in shape, it came with a fitted, hard-shell removable cover. The machines were built in Yverdon, Switzerland by Paillard S.A.\n\nThe model was introduced in 1958 as a successor to the Hermes 2000. Its original iteration, the \"Model 1\", was produced until 1966. With subsequent design modifications to the external casing and a variety of subtle changes in colour finishes, the Hermes 3000 was manufactured into the 1980's. Although it was a portable machine the Hermes 3000 had a few deluxe features, such as a \"beyond the margins\" key and a separate key that could be depressed to free any jammed keys and return them to their resting position. They predominently came in a light green (occasionally described as a mint or\"sea-foam green\") colour.\nWilliam Kotzwinkle's 1972 novel was named \"Hermes 3000\" after the machine. During his acceptance speech for \"Best Screenplay (Brokeback Mountain)\" at the 2006 Golden Globes, author Larry McMurtry specifically mentioned his Hermes 3000. Other notable users of the machine are Sam Shepard, Eugène Ionesco and Stephen Fry. Beat writer Jack Kerouac wrote his final novel, \"Vanity of Duluoz\", on the Hermes 3000 in 1966. In a March 2018 auction at Bonhams in London, the Hermes 3000 on which Sylvia Plath had typed her only novel—\"The Bell Jar\"—in 1962 was sold for £26,000 ($46,071) in 2013, in an appearance on BBC Radio 4's \"Desert Island Discs\", actor Tom Hanks named the Hermes 3000 as the luxury he would choose to take with him.\n"}
{"id": "3032314", "url": "https://en.wikipedia.org/wiki?curid=3032314", "title": "History of the camera", "text": "History of the camera\n\nThe history of the camera can be traced much further back than the introduction of photography. Cameras evolved from the camera obscura, and continued to change through many generations of photographic technology, including daguerreotypes, calotypes, dry plates, film, and to the modern day with digital cameras.\n\nThe forerunner to the photographic camera was the \"camera obscura\". Camera obscura (Latin for \"dark room\") is the natural optical phenomenon that occurs when an image of a scene at the other side of a screen (or for instance a wall) is projected through a small hole in that screen and forms an inverted image (left to right and upside down) on a surface opposite to the opening. The oldest known record of this principle is a description by Han Chinese philosopher Mozi (ca. 470 to ca. 391 BC). Mozi correctly asserted that the camera obscura image is inverted because light travels in straight lines from its source. In the 11th century Arab physicist Ibn al-Haytham (Alhazen)'s wrote very influential books about optics, including experiments with light through a small opening in a darkened room.\n\nThe use of a lens in the opening of a wall or closed window shutter of a darkened room to project images used as a drawing aid has been traced back to circa 1550. Since the late 17th century portable camera obscura devices in tents and boxes were used as a drawing aid.\nBefore the invention of photographic processes there was no way to preserve the images produced by these cameras apart from manually tracing them. The earliest cameras were room-sized, with space for one or more people inside; these gradually evolved into more and more compact models. By Niépce's time portable box camerae obscurae suitable for photography were readily available. The first camera that was small and portable enough to be practical for photography was envisioned by Johann Zahn in 1685, though it would be almost 150 years before such an application was possible.\n\nThe first partially successful photograph of a camera image was made in approximately 1816 by Nicéphore Niépce,\nusing a very small camera of his own making and a piece of paper coated with silver chloride, which darkened where it was exposed to light. No means of removing the remaining unaffected silver chloride was known to Niépce, so the photograph was not permanent, eventually becoming entirely darkened by the overall exposure to light necessary for viewing it. In the mid-1820s, Niépce used a sliding wooden box camera made by Parisian opticians Charles and Vincent Chevalier to experiment with photography on surfaces thinly coated with Bitumen of Judea. The bitumen slowly hardened in the brightest areas of the image. The unhardened bitumen was then dissolved away. One of those photographs has survived.\n\nAfter Niépce's death in 1833, his partner Louis Daguerre continued to experiment and by 1837 had created the first practical photographic process, which he named the daguerreotype and publicly unveiled in 1839. Daguerre treated a silver-plated sheet of copper with iodine vapor to give it a coating of light-sensitive silver iodide. After exposure in the camera, the image was developed by mercury vapor and fixed with a strong solution of ordinary salt (sodium chloride). Henry Fox Talbot perfected a different process, the calotype, in 1840. As commercialized, both processes used very simple cameras consisting of two nested boxes. The rear box had a removable ground glass screen and could slide in and out to adjust the focus. After focusing, the ground glass was replaced with a light-tight holder containing the sensitized plate or paper and the lens was capped. Then the photographer opened the front cover of the holder, uncapped the lens, and counted off as many minutes as the lighting conditions seemed to require before replacing the cap and closing the holder. Despite this mechanical simplicity, high-quality achromatic lenses were standard.\n\nCollodion dry plates had been available since 1857, thanks to the work of Désiré van Monckhoven, but it was not until the invention of the gelatin dry plate in 1871 by Richard Leach Maddox that the wet plate process could be rivaled in quality and speed. The 1878 discovery that heat-ripening a gelatin emulsion greatly increased its sensitivity finally made so-called \"instantaneous\" snapshot exposures practical. For the first time, a tripod or other support was no longer an absolute necessity. With daylight and a fast plate or film, a small camera could be hand-held while taking the picture. The ranks of amateur photographers swelled and informal \"candid\" portraits became popular. There was a proliferation of camera designs, from single- and twin-lens reflexes to large and bulky field cameras, simple box cameras, and even \"detective cameras\" disguised as pocket watches, hats, or other objects.\n\nThe short exposure times that made candid photography possible also necessitated another innovation, the mechanical shutter. The very first shutters were separate accessories, though built-in shutters were common by the end of the 19th century.\n\nThe use of photographic film was pioneered by George Eastman, who started manufacturing paper film in 1885 before switching to celluloid in 1888-1889. His first camera, which he called the \"Kodak,\" was first offered for sale in 1888. It was a very simple box camera with a fixed-focus lens and single shutter speed, which along with its relatively low price appealed to the average consumer. The Kodak came pre-loaded with enough film for 100 exposures and needed to be sent back to the factory for processing and reloading when the roll was finished. By the end of the 19th century Eastman had expanded his lineup to several models including both box and folding cameras.\n\nIn 1900, Eastman took mass-market photography one step further with the Brownie, a simple and very inexpensive box camera that introduced the concept of the snapshot. The Brownie was extremely popular and various models remained on sale until the 1960s.\n\nFilm also allowed the movie camera to develop from an expensive toy to a practical commercial tool.\n\nDespite the advances in low-cost photography made possible by Eastman, plate cameras still offered higher-quality prints and remained popular well into the 20th century. To compete with rollfilm cameras, which offered a larger number of exposures per loading, many inexpensive plate cameras from this era were equipped with magazines to hold several plates at once. Special backs for plate cameras allowing them to use film packs or rollfilm were also available, as were backs that enabled rollfilm cameras to use plates.\n\nExcept for a few special types such as Schmidt cameras, most professional astrographs continued to use plates until the end of the 20th century when electronic photography replaced them.\n\nA number of manufacturers started to use 35mm film for still photography between 1905 and 1913. The first 35mm cameras available to the public, and reaching significant numbers in sales were the Tourist Multiple, in 1913, and the Simplex, in 1914. \n\nOskar Barnack, who was in charge of research and development at Leitz, decided to investigate using 35 mm cine film for still cameras while attempting to build a compact camera capable of making high-quality enlargements. He built his prototype 35 mm camera (Ur-Leica) around 1913, though further development was delayed for several years by World War I. It wasn't until after World War I that Leica commercialized their first 35mm Cameras. Leitz test-marketed the design between 1923 and 1924, receiving enough positive feedback that the camera was put into production as the Leica I (for Leitz camera) in 1925. The Leica's immediate popularity spawned a number of competitors, most notably the Contax (introduced in 1932), and cemented the position of 35 mm as the format of choice for high-end compact cameras.\n\nKodak got into the market with the Retina I in 1934, which introduced the 135 cartridge used in all modern 35 mm cameras. Although the Retina was comparatively inexpensive, 35 mm cameras were still out of reach for most people and rollfilm remained the format of choice for mass-market cameras. This changed in 1936 with the introduction of the inexpensive Argus A and to an even greater extent in 1939 with the arrival of the immensely popular Argus C3. Although the cheapest cameras still used rollfilm, 35 mm film had come to dominate the market by the time the C3 was discontinued in 1966.\n\nThe fledgling Japanese camera industry began to take off in 1936 with the Canon 35 mm rangefinder, an improved version of the 1933 Kwanon prototype. Japanese cameras would begin to become popular in the West after Korean War veterans and soldiers stationed in Japan brought them back to the United States and elsewhere.\n\nThe first practical reflex camera was the Franke & Heidecke Rolleiflex medium format TLR of 1928. Though both single- and twin-lens reflex cameras had been available for decades, they were too bulky to achieve much popularity. The Rolleiflex, however, was sufficiently compact to achieve widespread popularity and the medium-format TLR design became popular for both high- and low-end cameras.\n\nA similar revolution in SLR design began in 1933 with the introduction of the Ihagee Exakta, a compact SLR which used 127 rollfilm. This was followed three years later by the first Western SLR to use 135 film, the Kine Exakta (World's first true 35mm SLR was Soviet \"Sport\" camera, marketed several months before Kine Exakta, though \"Sport\" used its own film cartridge). The 35mm SLR design gained immediate popularity and there was an explosion of new models and innovative features after World War II. There were also a few 35mm TLRs, the best-known of which was the Contaflex of 1935, but for the most part these met with little success.\n\nThe first major post-war SLR innovation was the eye-level viewfinder, which first appeared on the Hungarian Duflex in 1947 and was refined in 1948 with the Contax S, the first camera to use a pentaprism. Prior to this, all SLRs were equipped with waist-level focusing screens. The Duflex was also the first SLR with an instant-return mirror, which prevented the viewfinder from being blacked out after each exposure. This same time period also saw the introduction of the Hasselblad 1600F, which set the standard for medium format SLRs for decades.\n\nIn 1952 the Asahi Optical Company (which later became well known for its Pentax cameras) introduced the first Japanese SLR using 135 film, the Asahiflex. Several other Japanese camera makers also entered the SLR market in the 1950s, including Canon, Yashica, and Nikon. Nikon's entry, the Nikon F, had a full line of interchangeable components and accessories and is generally regarded as the first Japanese system camera. It was the F, along with the earlier S series of rangefinder cameras, that helped establish Nikon's reputation as a maker of professional-quality equipment.\n\nWhile conventional cameras were becoming more refined and sophisticated, an entirely new type of camera appeared on the market in 1948. This was the Polaroid Model 95, the world's first viable instant-picture camera. Known as a Land Camera after its inventor, Edwin Land, the Model 95 used a patented chemical process to produce finished positive prints from the exposed negatives in under a minute. The Land Camera caught on despite its relatively high price and the Polaroid lineup had expanded to dozens of models by the 1960s. The first Polaroid camera aimed at the popular market, the Model 20 Swinger of 1965, was a huge success and remains one of the top-selling cameras of all time.\n\nThe first camera to feature automatic exposure was the selenium light meter-equipped, fully automatic Super Kodak Six-20 pack of 1938, but its extremely high price (for the time) of $225 ($ in present terms) kept it from achieving any degree of success. By the 1960s, however, low-cost electronic components were commonplace and cameras equipped with light meters and automatic exposure systems became increasingly widespread.\n\nThe next technological advance came in 1960, when the German Mec 16 SB subminiature became the first camera to place the light meter behind the lens for more accurate metering. However, through-the-lens metering ultimately became a feature more commonly found on SLRs than other types of camera; the first SLR equipped with a TTL system was the Topcon RE Super of 1962.\n\nDigital cameras differ from their analog predecessors primarily in that they do not use film, but capture and save photographs on digital memory cards or internal storage instead. Their low operating costs have relegated chemical cameras to niche markets. Digital cameras now include wireless communication capabilities (for example Wi-Fi or Bluetooth) to transfer, print or share photos, and are commonly found on mobile phones.\n\nThe concept of digitizing images on scanners, and the concept of digitizing video signals, predate the concept of making still pictures by digitizing signals from an array of discrete sensor elements. Early spy satellites used the extremely complex and expensive method of de-orbit and airborne retrieval of film canisters. Technology was pushed to skip these steps through the use of in-satellite developing and electronic scanning of the film for direct transmission to the ground. The amount of film was still a major limitation, and this was overcome and greatly simplified by the push to develop an electronic image capturing array that could be used instead of film. The first electronic imaging satellite was the KH-11 launched by the NRO in late 1976. It had a charge-coupled device (CCD) array with a resolution of (0.64 megapixels). At Philips Labs in New York, Edward Stupp, Pieter Cath and Zsolt Szilagyi filed for a patent on \"All Solid State Radiation Imagers\" on 6 September 1968 and constructed a flat-screen target for receiving and storing an optical image on a matrix composed of an array of photodiodes connected to a capacitor to form an array of two terminal devices connected in rows and columns. Their US patent was granted on 10 November 1970. Texas Instruments engineer Willis Adcock designed a filmless camera that was not digital and applied for a patent in 1972, but it is not known whether it was ever built. The Cromemco CYCLOPS introduced as a hobbyist construction project in 1975 was the first digital camera to be interfaced to a microcomputer. \n\nThe first recorded attempt at building a self-contained digital camera was in 1975 by Steven Sasson, an engineer at Eastman Kodak. It used the then-new solid-state CCD image sensor chips developed by Fairchild Semiconductor in 1973. The camera weighed 8 pounds (3.6 kg), recorded black and white images to a compact cassette tape, had a resolution of 0.01 megapixels (10,000 pixels), and took 23 seconds to capture its first image in December 1975. The prototype camera was a technical exercise, not intended for production.\n\nHandheld electronic cameras, in the sense of a device meant to be carried and used like a handheld film camera, appeared in 1981 with the demonstration of the Sony Mavica (Magnetic Video Camera). This is not to be confused with the later cameras by Sony that also bore the Mavica name. This was an analog camera, in that it recorded pixel signals continuously, as videotape machines did, without converting them to discrete levels; it recorded television-like signals to a 2 × 2 inch \"video floppy\". \nIn essence it was a video movie camera that recorded single frames, 50 per disk in field mode and 25 per disk in frame mode. The image quality was considered equal to that of then-current televisions.\n\nAnalog electronic cameras do not appear to have reached the market until 1986 with the Canon RC-701. Canon demonstrated a prototype of this model at the 1984 Summer Olympics, printing the images in the \"Yomiuri Shinbun\", a Japanese newspaper. In the United States, the first publication to use these cameras for real reportage was USA Today, in its coverage of World Series baseball. Several factors held back the widespread adoption of analog cameras; the cost (upwards of $20,000), poor image quality compared to film, and the lack of quality affordable printers. Capturing and printing an image originally required access to equipment such as a frame grabber, which was beyond the reach of the average consumer. The \"video floppy\" disks later had several reader devices available for viewing on a screen, but were never standardized as a computer drive.\n\nThe early adopters tended to be in the news media, where the cost was negated by the utility and the ability to transmit images by telephone lines. The poor image quality was offset by the low resolution of newspaper graphics. This capability to transmit images without a satellite link was useful during the Tiananmen Square protests of 1989 and the first Gulf War in 1991.\n\nUS government agencies also took a strong interest in the still video concept, notably the US Navy for use as a real time air-to-sea surveillance system.\n\nThe first analog electronic camera marketed to consumers may have been the Casio VS-101 in 1987. A notable analog camera produced the same year was the Nikon QV-1000C, designed as a press camera and not offered for sale to general users, which sold only a few hundred units. It recorded images in greyscale, and the quality in newspaper print was equal to film cameras. In appearance it closely resembled a modern digital single-lens reflex camera. Images were stored on video floppy disks.\n\nSilicon Film, a proposed digital sensor cartridge for film cameras that would allow 35 mm cameras to take digital photographs without modification was announced in late 1998. Silicon Film was to work like a roll of 35 mm film, with a 1.3 megapixel sensor behind the lens and a battery and storage unit fitting in the film holder in the camera. The product, which was never released, became increasingly obsolete due to improvements in digital camera technology and affordability. Silicon Films' parent company filed for bankruptcy in 2001.\n\nBy the late 1980s, the technology required to produce truly commercial digital cameras existed. The first true portable digital camera that recorded images as a computerized file was likely the Fuji DS-1P of 1988, which recorded to a 2 MB SRAM memory card that used a battery to keep the data in memory. This camera was never marketed to the public.\n\nThe first digital camera of any kind ever sold commercially was possibly the MegaVision Tessera in 1987 though there is not extensive documentation of its sale known. The first \"portable\" digital camera that was actually marketed commercially was sold in December 1989 in Japan, the DS-X by\nFuji The first commercially available portable digital camera in the United States was the Dycam Model 1, first shipped in November 1990. It was originally a commercial failure because it was black and white, low in resolution, and cost nearly $1,000 (about $2000 in 2014). It later saw modest success when it was re-sold as the Logitech Fotoman in 1992. It used a CCD image sensor, stored pictures digitally, and connected directly to a computer for download.\n\nIn 1991, Kodak brought to market the Kodak DCS (Kodak Digital Camera System), the beginning of a long line of professional Kodak DCS SLR cameras that were based in part on film bodies, often Nikons. It used a 1.3 megapixel sensor, had a bulky external digital storage system and was priced at $13,000. At the arrival of the Kodak DCS-200, the Kodak DCS was dubbed Kodak DCS-100.\n\nThe move to digital formats was helped by the formation of the first JPEG and MPEG standards in 1988, which allowed image and video files to be compressed for storage. The first consumer camera with a liquid crystal display on the back was the Casio QV-10 developed by a team led by Hiroyuki Suetaka in 1995. The first camera to use CompactFlash was the Kodak DC-25 in 1996.. The first camera that offered the ability to record video clips may have been the Ricoh RDC-1 in 1995.\n\nIn 1995 Minolta introduced the RD-175, which was based on the Minolta 500si SLR with a splitter and three independent CCDs. This combination delivered 1.75M pixels. The benefit of using an SLR base was the ability to use any existing Minolta AF mount lens. 1999 saw the introduction of the Nikon D1, a 2.74 megapixel camera that was the first digital SLR developed entirely from the ground up by a major manufacturer, and at a cost of under $6,000 at introduction was affordable by professional photographers and high-end consumers. This camera also used Nikon F-mount lenses, which meant film photographers could use many of the same lenses they already owned.\n\nDigital camera sales continued to flourish, driven by technology advances. The digital market segmented into different categories, Compact Digital Still Cameras, Bridge Cameras, Mirrorless Compacts and Digital SLRs. One of the major technology advances was the development of CMOS sensors, which helped drive sensor costs low enough to enable the widespread adoption of camera phones.\n\nSince 2003, digital cameras have outsold film cameras and Kodak announced in January 2004 that they would no longer sell Kodak-branded film cameras in the developed world - and 2012 filed for bankruptcy after struggling to adapt to the changing industry. Smartphones now routinely include high resolution digital cameras.\n\n\nNotes\n"}
{"id": "41878399", "url": "https://en.wikipedia.org/wiki?curid=41878399", "title": "History of the portable gas stove", "text": "History of the portable gas stove\n\nThe portable gas stove is a combination of portability and functionality; combining the light weight of a small gas canister with the heat output needed to cook a meal. Portable stoves in modern times can be divided into several broad categories based on the type of fuel used and the design of the aluminium stoving frame. Unpressurised stoves use solid/liquid fuel placed in the burner before ignition. Combustible stove hangers use a form of volatile liquid fuel in a pressurized burner i.e. bottled gas stoves. They originate from the gravity-fed 1932 \"spirit\" stoves or \"réchaud de gaz de dirigeant\".\n\nThe production of the \"réchaud de gaz de dirigeant\" (Portable gas stove) was commissioned and begun in the workshop of the industrial designer Jue Lafare (born April 7, 1896) in 1932 after he placed his plan for a portable gas cooker before the French Army two and a half years earlier.\n\nBy 1934 Lafare's portable cooker was in officer ranks and within the next 16 months would come to be a staple of every officer in the French army. Due to fear of common misuse by soldiers of lower ranks obtaining the cookers without consent, the army requested that all instructional writing be placed upon the cookers in English as only the officers were taught fluency in English.\n\nUnfortunately Lafares design, however brilliant, was not enough to stop the German ranks marching over his homeland in 1940 after only 6 weeks. The majority of \"réchaud de gaz de dirigeant\" were melted down into what can only be assumed as scrap metal for the German war machine. However, from Jeu Lafare's stove the idea was adapted in various forms around the world during World War II and afterwards. Examples can be seen in many military originations around the world today with the American 'Bunga' or Japanese '携帯用ガス炊飯器'. Today there are estimated to be less than 80 of Lafare's original Full Blue French made with English writing \"réchaud de gaz de dirigeant\". Although the stove was a wonderful invention, the Germans were incredible in their precision at melting them down in such a systematic way.\n\nAs mentioned above, examples of Lafare's original full blue French made \"réchaud de gaz de dirigeant\" are extremely rare. The War Memorial Museum in Boulogne, France, paid 390EU for the piece pictured above in their collection some 20 years ago. However, it should be pointed out that the gas canister is a later 1940s Deutsch army model.\n\n"}
{"id": "14449116", "url": "https://en.wikipedia.org/wiki?curid=14449116", "title": "History of timekeeping devices", "text": "History of timekeeping devices\n\nFor thousands of years, devices have been used to measure and keep track of time. The current sexagesimal system of time measurement dates to approximately 2000  from the Sumerians.\n\nThe Egyptians divided the day into two 12-hour periods, and used large obelisks to track the movement of the sun. They also developed water clocks, which were probably first used in the Precinct of Amun-Re, and later outside Egypt as well; they were employed frequently by the Ancient Greeks, who called them \"clepsydrae\". The Zhou dynasty is believed to have used the outflow water clock around the same time, devices which were introduced from Mesopotamia as early as 2000.\n\nOther ancient timekeeping devices include the candle clock, used in ancient China, ancient Japan, England and Mesopotamia; the timestick, widely used in India and Tibet, as well as some parts of Europe; and the hourglass, which functioned similarly to a water clock. The sundial, another early clock, relies on shadows to provide a good estimate of the hour on a sunny day. It is not so useful in cloudy weather or at night and requires recalibration as the seasons change (if the gnomon was not aligned with the Earth's axis).\n\nThe earliest known clock with a water-powered escapement mechanism, which transferred rotational energy into intermittent motions, dates back to 3rd century in ancient Greece; Chinese engineers later invented clocks incorporating mercury-powered escapement mechanisms in the 10th century, followed by Iranian engineers inventing water clocks driven by gears and weights in the 11th century.\n\nThe first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656. The invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century.\n\nThe pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2. Although initially limited to laboratories, the development of microelectronics in the 1960s made quartz clocks both compact and cheap to produce, and by the 1980s they became the world's dominant timekeeping technology in both clocks and wristwatches.\n\nAtomic clocks are far more accurate than any previous timekeeping device, and are used to calibrate other clocks and to calculate the International Atomic Time; a standardized civil system, Coordinated Universal Time, is based on atomic time.\n\nMany ancient civilizations observed astronomical bodies, often the Sun and Moon, to determine times, dates, and seasons. The first calendars may have been created during the last glacial period, by hunter-gatherers who employed tools such as sticks and bones to track the phases of the moon or the seasons. Stone circles, such as England's Stonehenge, were built in various parts of the world, especially in Prehistoric Europe, and are thought to have been used to time and predict seasonal and annual events such as equinoxes or solstices. As those megalithic civilizations left no recorded history, little is known of their calendars or timekeeping methods. Methods of sexagesimal timekeeping, now common in both Western and Eastern societies, are first attested nearly 4,000 years ago in Mesopotamia and Egypt. Mesoamericans similarly modified their usual vigesimal counting system when dealing with calendars to produce a 360-day year.\n\nThe oldest known sundial is from Egypt; it dates back to around 1500 (19th Dynasty), and was discovered in the Valley of the Kings in 2013. Sundials have their origin in shadow clocks, which were the first devices used for measuring the parts of a day. Ancient Egyptian obelisks, constructed about 3500, are also among the earliest shadow clocks.\nEgyptian shadow clocks divided daytime into 12 parts with each part further divided into more precise parts. One type of shadow clock consisted of a long stem with five variable marks and an elevated crossbar which cast a shadow over those marks. It was positioned eastward in the morning, and was turned west at noon. Obelisks functioned in much the same manner: the shadow cast on the markers around it allowed the Egyptians to calculate the time. The obelisk also indicated whether it was morning or afternoon, as well as the summer and winter solstices. A third shadow clock, developed c. 1500, was similar in shape to a bent T-square. It measured the passage of time by the shadow cast by its crossbar on a non-linear rule. The \"T\" was oriented eastward in the mornings, and turned around at noon, so that it could cast its shadow in the opposite direction.\n\nAlthough accurate, shadow clocks relied on the sun, and so were useless at night and in cloudy weather. The Egyptians therefore developed a number of alternative timekeeping instruments, including water clocks, and a system for tracking star movements. The oldest description of a water clock is from the tomb inscription of the 16th-century Egyptian court official Amenemhet, identifying him as its inventor. There were several types of water clocks, some more elaborate than others. One type consisted of a bowl with small holes in its bottom, which was floated on water and allowed to fill at a near-constant rate; markings on the side of the bowl indicated elapsed time, as the surface of the water reached them. The oldest-known waterclock was found in the tomb of pharaoh Amenhotep I (1525–1504), suggesting that they were first used in ancient Egypt. Another Egyptian method of determining the time during the night was using plumb-lines called merkhets. In use since at least 600, two of these instruments were aligned with Polaris, the north pole star, to create a north–south meridian. The time was accurately measured by observing certain stars as they crossed the line created with the \"merkhets\".\n\nWater clocks, or clepsydrae, were commonly used in Ancient Greece following their introduction by Plato, who also invented a water-based alarm clock. One account of Plato's alarm clock describes it as depending on the nightly overflow of a vessel containing lead balls, which floated in a columnar vat. The vat held a steadily increasing amount of water, supplied by a cistern. By morning, the vessel would have floated high enough to tip over, causing the lead balls to cascade onto a copper platter. The resultant clangor would then awaken Plato's students at the Academy. Another possibility is that it comprised two jars, connected by a siphon. Water emptied until it reached the siphon, which transported the water to the other jar. There, the rising water would force air through a whistle, sounding an alarm. The Greeks and Chaldeans regularly maintained timekeeping records as an essential part of their astronomical observations.\n\nGreek astronomer, Andronicus of Cyrrhus, supervised the construction of the Tower of the Winds in Athens in the 1st century.\n\nIn Greek tradition, clepsydrae were used in court; later, the Romans adopted this practice, as well. There are several mentions of this in historical records and literature of the era; for example, in \"Theaetetus\", Plato says that \"Those men, on the other hand, always speak in haste, for the flowing water urges them on\". Another mention occurs in Lucius Apuleius' \"The Golden Ass\": \"The Clerk of the Court began bawling again, this time summoning the chief witness for the prosecution to appear. Up stepped an old man, whom I did not know. He was invited to speak for as long as there was water in the clock; this was a hollow globe into which water was poured through a funnel in the neck, and from which it gradually escaped through fine perforations at the base\". The clock in Apuleius's account was one of several types of water clock used. Another consisted of a bowl with a hole in its centre, which was floated on water. Time was kept by observing how long the bowl took to fill with water.\n\nAlthough clepsydrae were more useful than sundials—they could be used indoors, during the night, and also when the sky was cloudy—they were not as accurate; the Greeks, therefore, sought a way to improve their water clocks. Although still not as accurate as sundials, Greek water clocks became more accurate around 325, and they were adapted to have a face with an hour hand, making the reading of the clock more precise and convenient. One of the more common problems in most types of clepsydrae was caused by water pressure: when the container holding the water was full, the increased pressure caused the water to flow more rapidly. This problem was addressed by Greek and Roman horologists beginning in 100, and improvements continued to be made in the following centuries. To counteract the increased water flow, the clock's water containers—usually bowls or jugs—were given a conical shape; positioned with the wide end up, a greater amount of water had to flow out in order to drop the same distance as when the water was lower in the cone. Along with this improvement, clocks were constructed more elegantly in this period, with hours marked by gongs, doors opening to miniature figurines, bells, or moving mechanisms. There were some remaining problems, however, which were never solved, such as the effect of temperature. Water flows more slowly when cold, or may even freeze.\n\nBetween 270 and 500, Hellenistic (Ctesibius, Hero of Alexandria, Archimedes) and Roman horologists and astronomers began developing more elaborate mechanized water clocks. The added complexity was aimed at regulating the flow and at providing fancier displays of the passage of time. For example, some water clocks rang bells and gongs, while others opened doors and windows to show figurines of people, or moved pointers, and dials. Some even displayed astrological models of the universe.\n\nAlthough the Greeks and Romans did much to advance water clock technology, they still continued to use shadow clocks. The mathematician and astronomer Theodosius of Bithynia, for example, is said to have invented a universal sundial that was accurate anywhere on Earth, though little is known about it. Others wrote of the sundial in the mathematics and literature of the period. Marcus Vitruvius Pollio, the Roman author of \"De Architectura\", wrote on the mathematics of gnomons, or sundial blades. During the reign of Emperor Augustus, the Romans constructed the largest sundial ever built, the Solarium Augusti. Its gnomon was an obelisk from Heliopolis. Similarly, the obelisk from Campus Martius was used as the gnomon for Augustus's zodiacal sundial. Pliny the Elder records that the first sundial in Rome arrived in 264, looted from Catania, Sicily; according to him, it gave the incorrect time until the markings and angle appropriate for Rome's latitude were used—a century later.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Zeebad, dates back to 500. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" – the shortest, longest, and equal-length days and nights of the years. The water clocks used in Iran were one of the most practical ancient tools for timing the yearly calendar.\n\nWater clocks, or \"Fenjaan\", in Persia reached a level of accuracy comparable to today's standards of timekeeping. The fenjaan was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from a qanat or well for irrigation of the farms, until it was replaced by more accurate current clock. Persian water clocks were a practical and useful tool for the qanat's shareholders to calculate the length of time they could divert water to their farm. The qanat was the only water source for agriculture and irrigation so a just and fair water distribution was very important. Therefore, a very fair and clever old person was elected to be the manager of the water clock, and at least two full-time managers were needed to control and observe the number of fenjaans and announce the exact time during the days and nights.\n\nThe fenjaan was a big pot full of water and a bowl with small hole in the center. When the bowl become full of water, it would sink into the pot, and the manager would empty the bowl and again put it on the top of the water in the pot. He would record the number of times the bowl sank by putting small stones into a jar.\n\nThe place where the clock was situated, and its managers, were collectively known as \"khaneh fenjaan\". Usually this would be the top floor of a public-house, with west- and east-facing windows to show the time of sunset and sunrise. There was also another time-keeping tool named a \"staryab\" or astrolabe, but it was mostly used for superstitious beliefs and was not practical for use as a farmers' calendar. The Zeebad Gonabad water clock was in use until 1965 when it was substituted by modern clocks.\n\nJoseph Needham speculated that the introduction of the outflow clepsydra to China, perhaps from Mesopotamia, occurred as far back as the 2nd millennium, during the Shang Dynasty, and at the latest by the 1st millennium. By the beginning of the Han Dynasty, in 202, the outflow clepsydra was gradually replaced by the inflow clepsydra, which featured an indicator rod on a float. To compensate for the falling pressure head in the reservoir, which slowed timekeeping as the vessel filled, Zhang Heng added an extra tank between the reservoir and the inflow vessel. Around 550 AD, Yin Gui was the first in China to write of the overflow or constant-level tank added to the series, which was later described in detail by the inventor Shen Kuo. Around 610, this design was trumped by two Sui Dynasty inventors, Geng Xun and Yuwen Kai, who were the first to create the balance clepsydra, with standard positions for the steelyard balance. Joseph Needham states that:\nThe term 'clock' encompasses a wide spectrum of devices, ranging from wristwatches to the Clock of the Long Now. The English word \"clock\" is said to derive from the Middle English \"clokke\", Old North French \"cloque\", or Middle Dutch \"clocke\", all of which mean \"bell\", and are derived from the Medieval Latin \"clocca\", also meaning bell. Indeed, bells were used to mark the passage of time; they marked the passage of the hours at sea and in abbeys.\n\nThroughout history, clocks have had a variety of power sources, including gravity, springs, and electricity. Mechanical clocks became widespread in the 14th century, when they were used in medieval monasteries to keep the regulated schedule of prayers. The clock continued to be improved, with the first pendulum clock being designed and built in the 17th century.\n\nThe earliest mention of candle clocks comes from a Chinese poem, written in 520 by You Jianfu. According to the poem, the graduated candle was a means of determining time at night. Similar candles were used in Japan until the early 10th century.\n\nThe candle clock most commonly mentioned and written of is attributed to King Alfred the Great. It consisted of six candles made from 72 pennyweights of wax, each high, and of uniform thickness, marked every inch (2.54 cm). As these candles burned for about four hours, each mark represented 20 minutes. Once lit, the candles were placed in wooden framed glass boxes, to prevent the flame from extinguishing.\n\nThe most sophisticated candle clocks of their time were those of Al-Jazari in 1206. One of his candle clocks included a dial to display the time and, for the first time, employed a bayonet fitting, a fastening mechanism still used in modern times. Donald Routledge Hill described Al-Jazari's candle clocks as follows:\n\nA variation on this theme were oil-lamp clocks. These early timekeeping devices consisted of a graduated glass reservoir to hold oil — usually whale oil, which burned cleanly and evenly — supplying the fuel for a built-in lamp. As the level in the reservoir dropped, it provided a rough measure of the passage of time.\n\nIn addition to water, mechanical, and candle clocks, incense clocks were used in the Far East, and were fashioned in several different forms. Incense clocks were first used in China around the 6th century; in Japan, one still exists in the Shōsōin, although its characters are not Chinese, but Devanagari. Due to their frequent use of Devanagari characters, suggestive of their use in Buddhist ceremonies, Edward H. Schafer speculated that incense clocks were invented in India. Although similar to the candle clock, incense clocks burned evenly and without a flame; therefore, they were more accurate and safer for indoor use.\n\nSeveral types of incense clock have been found, the most common forms include the incense stick and incense seal. An incense stick clock was an incense stick with calibrations; most were elaborate, sometimes having threads, with weights attached, at even intervals. The weights would drop onto a platter or gong below, signifying that a certain amount of time had elapsed. Some incense clocks were held in elegant trays; open-bottomed trays were also used, to allow the weights to be used together with the decorative tray. Sticks of incense with different scents were also used, so that the hours were marked by a change in fragrance. The incense sticks could be straight or spiraled; the spiraled ones were longer, and were therefore intended for long periods of use, and often hung from the roofs of homes and temples. In Japan, a geisha was paid for the number of \"senkodokei\" (incense sticks) that had been consumed while she was present, a practice which continued until 1924.\n\nIncense seal clocks were used for similar occasions and events as the stick clock; while religious purposes were of primary importance, these clocks were also popular at social gatherings, and were used by Chinese scholars and intellectuals. The seal was a wooden or stone disk with one or more grooves etched in it into which incense was placed. These clocks were common in China, but were produced in fewer numbers in Japan. To signal the passage of a specific amount of time, small pieces of fragrant woods, resins, or different scented incenses could be placed on the incense powder trails. Different powdered incense clocks used different formulations of incense, depending on how the clock was laid out. The length of the trail of incense, directly related to the size of the seal, was the primary factor in determining how long the clock would last; all burned for long periods of time, ranging between 12 hours and a month.\n\nWhile early incense seals were made of wood or stone, the Chinese gradually introduced disks made of metal, most likely beginning during the Song dynasty. This allowed craftsmen to more easily create both large and small seals, as well as design and decorate them more aesthetically. Another advantage was the ability to vary the paths of the grooves, to allow for the changing length of the days in the year. As smaller seals became more readily available, the clocks grew in popularity among the Chinese, and were often given as gifts. Incense seal clocks are often sought by modern-day clock collectors; however, few remain that have not already been purchased or been placed on display at museums or temples.\n\nSundials had been used for timekeeping since Ancient Egypt. Ancient dials were nodus-based with straight hour-lines that indicated unequal hours—also called temporary hours—that varied with the seasons. Every day was divided into 12 equal segments regardless of the time of year; thus, hours were shorter in winter and longer in summer. The sundial was further developed by Muslim astronomers. The idea of using hours of equal length throughout the year was the innovation of Abu'l-Hasan Ibn al-Shatir in 1371, based on earlier developments in trigonometry by Muhammad ibn Jābir al-Harrānī al-Battānī (Albategni). Ibn al-Shatir was aware that \"using a gnomon that is parallel to the Earth's axis will produce sundials whose hour lines indicate equal hours on any day of the year\". His sundial is the oldest polar-axis sundial still in existence. The concept appeared in Western sundials starting in 1446.\n\nFollowing the acceptance of heliocentrism and equal hours, as well as advances in trigonometry, sundials appeared in their present form during the Renaissance, when they were built in large numbers. In 1524, the French astronomer Oronce Finé constructed an ivory sundial, which still exists; later, in 1570, the Italian astronomer Giovanni Padovani published a treatise including instructions for the manufacture and laying out of mural (vertical) and horizontal sundials. Similarly, Giuseppe Biancani's \"Constructio instrumenti ad horologia solaria\" (c. 1620) discusses how to construct sundials.\n\nSince the hourglass was one of the few reliable methods of measuring time at sea, it is speculated that it was used on board ships as far back as the 11th century, when it would have complemented the magnetic compass as an aid to navigation. However, the earliest unambiguous evidence of their use appears in the painting \"Allegory of Good Government\", by Ambrogio Lorenzetti, from 1338. From the 15th century onwards, hourglasses were used in a wide range of applications at sea, in churches, in industry, and in cooking; they were the first dependable, reusable, reasonably accurate, and easily constructed time-measurement devices. The hourglass also took on symbolic meanings, such as that of death, temperance, opportunity, and Father Time, usually represented as a bearded, old man. Though also used in China, the hourglass's history there is unknown. The Portuguese navigator Ferdinand Magellan used 18 hourglasses on each ship during his circumnavigation of the globe in 1522.\n\nThe earliest instance of a liquid-driven escapement was described by the Greek engineer Philo of Byzantium (fl. 3rd century) in his technical treatise \"Pneumatics\" (chapter 31) where he likens the escapement mechanism of a washstand automaton with those as employed in (water) clocks. Another early clock to use escapements was built during the 7th century in Chang'an, by Tantric monk and mathematician, Yi Xing, and government official Liang Lingzan. An astronomical instrument that served as a clock, it was discussed in a contemporary text as follows:\n[It] was made in the image of the round heavens and on it were shown the lunar mansions in their order, the equator and the degrees of the heavenly circumference. Water, flowing into scoops, turned a wheel automatically, rotating it one complete revolution in one day and night. Besides this, there were two rings fitted around the celestial sphere outside, having the sun and moon threaded on them, and these were made to move in circling orbit ... And they made a wooden casing the surface of which represented the horizon, since the instrument was half sunk in it. It permitted the exact determinations of the time of dawns and dusks, full and new moons, tarrying and hurrying. Moreover, there were two wooden jacks standing on the horizon surface, having one a bell and the other a drum in front of it, the bell being struck automatically to indicate the hours, and the drum being beaten automatically to indicate the quarters. All these motions were brought about by machinery within the casing, each depending on wheels and shafts, hooks, pins and interlocking rods, stopping devices and locks checking mutually.\n\nSince Yi Xing's clock was a water clock, it was affected by temperature variations. That problem was solved in 976 by Zhang Sixun by replacing the water with mercury, which remains liquid down to . Zhang implemented the changes into his clock tower, which was about tall, with escapements to keep the clock turning and bells to signal every quarter-hour. Another noteworthy clock, the elaborate Cosmic Engine, was built by Su Song, in 1088. It was about the size of Zhang's tower, but had an automatically rotating armillary sphere—also called a celestial globe—from which the positions of the stars could be observed. It also featured five panels with mannequins ringing gongs or bells, and tablets showing the time of day, or other special times. Furthermore, it featured the first known endless power-transmitting chain drive in horology. Originally built in the capital of Kaifeng, it was dismantled by the Jin army and sent to the capital of Yanjing (now Beijing), where they were unable to put it back together. As a result, Su Song's son Su Xie was ordered to build a replica.\nThe clock towers built by Zhang Sixun and Su Song, in the 10th and 11th centuries, respectively, also incorporated a striking clock mechanism, the use of clock jacks to sound the hours. A striking clock outside of China was the Jayrun Water Clock, at the Umayyad Mosque in Damascus, Syria, which struck once every hour. It was constructed by Muhammad al-Sa'ati in the 12th century, and later described by his son Ridwan ibn al-Sa'ati, in his \"On the Construction of Clocks and their Use\" (1203), when repairing the clock. In 1235, an early monumental water-powered alarm clock that \"announced the appointed hours of prayer and the time both by day and by night\" was completed in the entrance hall of the Mustansiriya Madrasah in Baghdad.\n\nThe first geared clock was invented in the 11th century by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia; it was a water clock that employed a complex gear train mechanism, including both segmental and epicyclic gearing, capable of transmitting high torque. The clock was unrivalled in its use of sophisticated complex gearing, until the mechanical clocks of the mid-14th century. Al-Muradi's clock also employed the use of mercury in its hydraulic linkages, which could function mechanical automata. Al-Muradi's work was known to scholars working under Alfonso X of Castile, hence the mechanism may have played a role in the development of the European mechanical clocks. Other monumental water clocks constructed by medieval Muslim engineers also employed complex gear trains and arrays of automata. Like the earlier Greeks and Chinese, Arab engineers at the time also developed a liquid-driven escapement mechanism which they employed in some of their water clocks. Heavy floats were used as weights and a constant-head system was used as an escapement mechanism, which was present in the hydraulic controls they used to make heavy floats descend at a slow and steady rate.\n\nA mercury clock, described in the \"Libros del saber de Astronomia\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. However, the device was actually a compartmented cylindrical water clock, which the Jewish author of the relevant section, Rabbi Isaac, constructed using principles described by a philosopher named \"Iran\", identified with Heron of Alexandria (fl. 1st century AD), on how heavy objects may be lifted.\n\nClock towers in Western Europe in the Middle Ages were also sometimes striking clocks. The most famous original still standing is possibly St Mark's Clock on the top of St Mark's Clocktower in St Mark's Square in Venice, assembled in 1493 by the clockmaker Gian Carlo Rainieri from Reggio Emilia. In 1497, Simone Campanato moulded the great bell on which every definite time-lapse is beaten by two mechanical bronze statues (h. 2,60 m.) called \"Due Mori\" (\"Two Moors\"), handling a hammer. Possibly earlier (1490) is the Prague Astronomical Clock by clockmaster Jan Růže (also called Hanuš) – according to another source this device was assembled as early as 1410 by clockmaker Mikuláš of Kadaň and mathematician Jan Šindel. The allegorical parade of animated sculptures rings on the hour every day.\n\nDuring the 11th century in the Song Dynasty, the Chinese astronomer, horologist and mechanical engineer Su Song created a water-driven astronomical clock for his clock tower of Kaifeng City. It incorporated an escapement mechanism as well as the earliest known endless power-transmitting chain drive, which drove the armillary sphere.\n\nContemporary Muslim astronomers also constructed a variety of highly accurate astronomical clocks for use in their mosques and observatories, such as the water-powered astronomical clock by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century. The most sophisticated timekeeping astrolabes were the geared astrolabe mechanisms designed by Abū Rayhān Bīrūnī in the 11th century and by Muhammad ibn Abi Bakr in the 13th century. These devices functioned as timekeeping devices and also as calendars.\nA sophisticated water-powered astronomical clock was built by Al-Jazari in 1206. This castle clock was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar paths, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing doors to open, each revealing a mannequin, every hour. It was possible to reset the length of day and night in order to account for the changing lengths of day and night throughout the year. This clock also featured a number of automata including falcons and musicians who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel.\n\nThe earliest medieval European clockmakers were Catholic monks. Medieval religious institutions required clocks because they regulated daily prayer- and work-schedules strictly, using various types of time-telling and recording devices, such as water clocks, sundials and marked candles, probably in combination. When mechanical clocks came into use, they were often wound at least twice a day to ensure accuracy. Monasteries broadcast important times and durations with bells, rung either by hand or by a mechanical device, such as by a falling weight or by rotating beater.\n\nAlthough the mortuary inscription of Pacificus, archdeacon of Verona, records that he constructed a night clock (\"horologium nocturnum\") as early as 850, his clock has been identified as being an observation tube used to locate stars with an accompanying book of astronomical observations, rather than a mechanical or water clock, an interpretation supported by illustrations from medieval manuscripts.\n\nThe religious necessities and technical skill of the medieval monks were crucial factors in the development of clocks, as the historian Thomas Woods writes:\nThe appearance of clocks in writings of the 11th century implies that they were well known in Europe in that period. In the early 14th-century, the Florentine poet Dante Alighieri referred to a clock in his \"Paradiso\"; the first known literary reference to a clock that struck the hours. Giovanni da Dondi, Professor of Astronomy at Padua, presented the earliest detailed description of clockwork in his 1364 treatise \"Il Tractatus Astrarii\". This has inspired several modern replicas, including some in London's Science Museum and the Smithsonian Institution. Other notable examples from this period were built in Milan (1335), Strasbourg (1354), Lund (1380), Rouen (1389), and Prague (1462).\n\nSalisbury cathedral clock, dating from about 1386, is one of the oldest working clocks in the world, and may be the oldest. It still has most of its original parts, although its original verge and foliot timekeeping mechanism is lost, having been converted to a pendulum, which was replaced by a replica verge in 1956. It has no dial, as its purpose was to strike a bell at precise times. The wheels and gears are mounted in an open, box-like iron frame, measuring about square. The framework is held together with metal dowels and pegs. Two large stones, hanging from pulleys, supply the power. As the weights fall, ropes unwind from the wooden barrels. One barrel drives the main wheel, which is regulated by the escapement, and the other drives the striking mechanism and the air brake.\n\nNote also Peter Lightfoot's Wells Cathedral clock, constructed c. 1390. The dial represents a geocentric view of the universe, with the Sun and Moon revolving around a central fixed Earth. It is unique in having its original medieval face, showing a philosophical model of the pre-Copernican universe. Above the clock is a set of figures, which hit the bells, and a set of jousting knights who revolve around a track every 15 minutes. The clock was converted to pendulum-and-anchor escapement in the 17th century, and was installed in London's Science Museum in 1884, where it continues to operate. Similar astronomical clocks, or \"horologes\", survive at Exeter, Ottery St Mary, and Wimborne Minster.\nOne clock that has not survived is that of the Abbey of St Albans, built by the 14th-century abbot Richard of Wallingford. It may have been destroyed during Henry VIII's Dissolution of the Monasteries, but the abbot's notes on its design have allowed a full-scale reconstruction. As well as keeping time, the astronomical clock could accurately predict lunar eclipses, and may have shown the Sun, Moon (age, phase, and node), stars and planets, as well as a wheel of fortune, and an indicator of the state of the tide at London Bridge. According to Thomas Woods, \"a clock that equaled it in technological sophistication did not appear for at least two centuries\". Giovanni de Dondi was another early mechanical clockmaker whose clock did not survive, but his work has been replicated based on the designs. De Dondi's clock was a seven-faced construction with 107 moving parts, showing the positions of the Sun, Moon, and five planets, as well as religious feast days. Around this period, mechanical clocks were introduced into abbeys and monasteries to mark important events and times, gradually replacing water clocks which had served the same purpose.\n\nDuring the Middle Ages, clocks primarily served religious purposes; the first employed for secular timekeeping emerged around the 15th century. In Dublin, the official measurement of time became a local custom, and by 1466 a public clock stood on top of the Tholsel (the city court and council chamber). It was the first of its kind to be clearly recorded in Ireland, and would only have had an hour hand. The increasing lavishness of castles led to the introduction of turret clocks. A 1435 example survives from Leeds castle; its face is decorated with the images of the Crucifixion of Jesus, Mary and St George.\n\nEarly clock dials showed hours: the display of minutes and seconds evolved later. A clock with a minutes dial is mentioned in a 1475 manuscript, and clocks indicating minutes and seconds existed in Germany in the 15th century. Timepieces which indicated minutes and seconds were occasionally made from this time on, but this was not common until the increase in accuracy made possible by the pendulum clock and, in watches, by the spiral balance spring. The 16th-century astronomer Tycho Brahe used clocks with minutes and seconds to observe stellar positions.\n\nThe Ottoman engineer Taqi al-Din described a weight-driven clock with a verge-and-foliot escapement, a striking train of gears, an alarm, and a representation of the moon's phases in his book \"The Brightest Stars for the Construction of Mechanical Clocks\" (\"Al-Kawākib al-durriyya fī wadh' al-bankāmat al-dawriyya\"), written around 1556.\n\nThe concept of the wristwatch goes back to the production of the very earliest watches in the 16th century. Elizabeth I of England received a wristwatch from Robert Dudley in 1571, described as an arm watch. From the beginning, wrist watches were almost exclusively worn by women, while men used pocket-watches up until the early 20th century. This was not just a matter of fashion or prejudice; watches of the time were notoriously prone to fouling from exposure to the elements, and could only reliably be kept safe from harm if carried securely in the pocket. When the waistcoat was introduced as a manly fashion at the court of Charles II in the 17th century, the pocket watch was tucked into its pocket. Prince Albert, the consort to Queen Victoria, introduced the 'Albert chain' accessory, designed to secure the pocket watch to the man's outergarment by way of a clip. By the mid nineteenth century, most watchmakers produced a range of wristwatches, often marketed as bracelets, for women.\n\nWristwatches were first worn by military men towards the end of the nineteenth century, when the importance of synchronizing manoeuvres during war without potentially revealing the plan to the enemy through signalling was increasingly recognized. It was clear that using pocket watches while in the heat of battle or while mounted on a horse was impractical, so officers began to strap the watches to their wrist. The Garstin Company of London patented a 'Watch Wristlet' design in 1893, although they were probably producing similar designs from the 1880s. Clearly, a market for men's wristwatches was coming into being at the time. Officers in the British Army began using wristwatches during colonial military campaigns in the 1880s, such as during the Anglo-Burma War of 1885.\n\nDuring the Boer War, the importance of coordinating troop movements and synchronizing attacks against the highly mobile Boer insurgents was paramount, and the use of wristwatches subsequently became widespread among the officer class. The company Mappin & Webb began production of their successful 'campaign watch' for soldiers during the campaign at the Sudan in 1898 and ramped up production for the Boer War a few years later.\nThese early models were essentially standard pocket-watches fitted to a leather strap, but by the early 20th century, manufacturers began producing purpose-built wristwatches. The Swiss company, Dimier Frères & Cie patented a wristwatch design with the now standard wire lugs in 1903. In 1904, Alberto Santos-Dumont, an early aviator, asked his friend, a French watchmaker called Louis Cartier, to design a watch that could be useful during his flights.\n\nThe impact of the First World War dramatically shifted public perceptions on the propriety of the man's wristwatch, and opened up a mass market in the post-war era. The creeping barrage artillery tactic, developed during the War, required precise synchronization between the artillery gunners and the infantry advancing behind the barrage. Service watches produced during the War were specially designed for the rigours of trench warfare, with luminous dials and unbreakable glass. Wristwatches were also found to be needed in the air as much as on the ground: military pilots found them more convenient than pocket watches for the same reasons as Santos-Dumont had. The British War Department began issuing wristwatches to combatants from 1917.\nThe company H. Williamson Ltd., based in Coventry, was one of the first to capitalize on this opportunity. During the company's 1916 AGM it was noted that \"...the public is buying the practical things of life. Nobody can truthfully contend that the watch is a luxury. It is said that one soldier in every four wears a wristlet watch, and the other three mean to get one as soon as they can.\" By the end of the War, almost all enlisted men wore a wristwatch, and after they were demobilized, the fashion soon caught on – the British \"Horological Journal\" wrote in 1917 that \"...the wristlet watch was little used by the sterner sex before the war, but now is seen on the wrist of nearly every man in uniform and of many men in civilian attire.\" Within a decade, sales of wristwatches had outstripped those of pocket watches.\n\nIn the late 17th and 18th Centuries, equation clocks were made, which allowed the user to see or calculate apparent solar time, as would be shown by a sundial. Before the invention of the pendulum clock, sundials were the only accurate timepieces. When good clocks became available, they appeared inaccurate to people who were used to trusting sundials. The annual variation of the equation of time made a clock up to about 15 minutes fast or slow, relative to a sundial, depending on the time of year. Equation clocks satisfied the demand for clocks that always agreed with sundials. Several types of equation clock mechanism were devised. which can be seen in surviving examples, mostly in museums.\n\nInnovations to the mechanical clock continued, with miniaturization leading to domestic clocks in the 15th century, and personal watches in the 16th. In the 1580s, the Italian polymath Galileo Galilei investigated the regular swing of the pendulum, and discovered that it could be used to regulate a clock. Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design. The first pendulum clock was designed and built by Dutch scientist Christiaan Huygens, in 1656. Early versions erred by less than one minute per day, and later ones only by 10 seconds, very accurate for their time.\n\nIn England, the manufacturing of pendulum clocks was soon taken up. The longcase clock (also known as the grandfather clock) was first created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671; this became feasible after Clement invented the anchor escapement mechanism in about 1670. Before then, pendulum clocks used the older verge escapement mechanism, which required very wide pendulum swings of about 100°. To avoid the need for a very large case, most clocks using the verge escapement had a short pendulum. The anchor mechanism, however, reduced the pendulum's necessary swing to between 4° to 6°, allowing clockmakers to use longer pendulums with consequently slower beats. These required less power to move, caused less friction and wear, and were more accurate than their shorter predecessors. Most longcase clocks use a pendulum about a metre (39 inches) long to the center of the bob, with each swing taking one second. This requirement for height, along with the need for a long drop space for the weights that power the clock, gave rise to the tall, narrow case.\n\nClement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clock-maker, and the Second Hand was introduced.\n\nThe Jesuits were another major contributor to the development of pendulum clocks in the 17th and 18th centuries, having had an \"unusually keen appreciation of the importance of precision\". In measuring an accurate one-second pendulum, for example, the Italian astronomer Father Giovanni Battista Riccioli persuaded nine fellow Jesuits \"to count nearly 87,000 oscillations in a single day\". They served a crucial role in spreading and testing the scientific ideas of the period, and collaborated with contemporary scientists, such as Huygens.\n\nThe invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century. Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring. Huygens was clearly the first to use a spiral balance spring, the form used in virtually all watches to the present day. The addition of the balance spring made the balance wheel a harmonic oscillator like the pendulum in a pendulum clock, which oscillated at a fixed resonant frequency and resisted oscillating at other rates. This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the watch face around 1680 in Britain and 1700 in France.\n\nLike the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry. The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. This resulted in a great advance in accuracy of pocket watches, from perhaps several hours per day to 10 minutes per day, similar to the effect of the pendulum upon mechanical clocks. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration.\n\nThe Rev. Edward Barlow invented the rack and snail striking mechanism for striking clocks, which was a great improvement over the previous mechanism. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nMarine chronometers are clocks used at sea as time standards, to determine longitude by celestial navigation.\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. The marine chronometer would have to keep the time of a fixed location—usually Greenwich Mean Time—allowing seafarers to determine longitude by comparing the local high noon to the clock. This clock could not contain a pendulum, which would be virtually useless on a rocking ship.\nAfter the Scilly naval disaster of 1707 where four ships ran aground due to navigational mistakes, the British government offered a large prize of £20,000, equivalent to millions of pounds today, for anyone who could determine longitude accurately. The reward was eventually claimed in 1761 by Yorkshire carpenter John Harrison, who dedicated his life to improving the accuracy of his clocks.\n\nIn 1735 Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat.\n\nThe chronometer was trialled in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nIn 1815, Sir Francis Ronalds (1788-1873) of London published the forerunner of the electric clock, the electrostatic clock. It was powered with dry piles, a high voltage battery with extremely long life but the disadvantage of its electrical properties varying with the weather. He trialled various means of regulating the electricity and these models proved to be reliable across a range of meteorological conditions.\n\nAlexander Bain, a Scottish clock and instrument maker, was the first to invent and patent the electric clock in 1840. On January 11, 1841, Alexander Bain along with John Barwise, a chronometer maker, took out another important patent describing a clock in which an electromagnetic pendulum and an electric current is employed to keep the clock going instead of springs or weights. Later patents expanded on his original ideas.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first quartz crystal oscillator was built by Walter G. Cady in 1921, and in 1927 the first quartz clock was built by Warren Marrison and J. W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. In 1932, a quartz clock able to measure small weekly variations in the rotation rate of the Earth was developed. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production has resulted in the subsequent proliferation of quartz clocks and watches.\n\nAtomic clocks are the most accurate timekeeping devices in practical use today. Accurate to within a few seconds over many thousands of years, they are used to calibrate other clocks and timekeeping instruments.\n\nThe idea of using atomic transitions to measure time was first suggested by Lord Kelvin in 1879, although it was only in the 1930s with the development of Magnetic resonance that there was a practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept.\nThe first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET).\n\nThe International System of Units standardized its unit of time, the second, on the properties of cesium in 1967. SI defines the second as 9,192,631,770 cycles of the radiation which corresponds to the transition between two electron spin energy levels of the ground state of the Cs atom. The cesium atomic clock, maintained by the National Institute of Standards and Technology, is accurate to 30 billionths of a second per year. Atomic clocks have employed other elements, such as hydrogen and rubidium vapor, offering greater stability—in the case of hydrogen clocks—and smaller size, lower power consumption, and thus lower cost (in the case of rubidium clocks).\n\nThe first professional clockmakers came from the guilds of locksmiths and jewellers. Clockmaking developed from a specialized craft into a mass production industry over many years.\n\nParis and Blois were the early centres of clockmaking in France. French clockmakers such as Julien Le Roy, clockmaker of Versailles, were leaders in case design and ornamental clocks. Le Roy belonged to the fifth generation of a family of clockmakers, and was described by his contemporaries as \"the most skillful clockmaker in France, possibly in Europe\". He invented a special repeating mechanism which improved the precision of clocks and watches, a face that could be opened to view the inside clockwork, and made or supervised over 3,500 watches. The competition and scientific rivalry resulting from his discoveries further encouraged researchers to seek new methods of measuring time more accurately.\n\nBetween 1794 and 1795, in the aftermath of the French Revolution, the French government briefly mandated decimal clocks, with a day divided into 10 hours of 100 minutes each. The astronomer and mathematician Pierre-Simon Laplace, among other individuals, modified the dial of his pocket watch to decimal time. A clock in the Palais des Tuileries kept decimal time as late as 1801, but the cost of replacing all the nation's clocks prevented decimal clocks from becoming widespread. Because decimalized clocks only helped astronomers rather than ordinary citizens, it was one of the most unpopular changes associated with the metric system, and it was abandoned.\n\nIn Germany, Nuremberg and Augsburg were the early clockmaking centers, and the Black Forest came to specialize in wooden cuckoo clocks.\nThe English became the predominant clockmakers of the 17th and 18th centuries. The main centres of the British industry were in the City of London, the West End of London, Soho where many skilled French Huguenots settled and later in Clerkenwell. The Worshipful Company of Clockmakers was established in 1631 as one of the Livery Companies of the City of London.\n\nThomas Tompion was the first English clockmaker with an international reputation and many of his pupils went on to become great horologists in their own right, such as George Graham who invented the deadbeat escapement, orrery and mercury pendulum, and his pupil Thomas Mudge who created the first lever escapement. Famous clockmakers of this period included Joseph Windmills, Simon de Charmes who established the De Charmes clockmaker firm and Christopher Pinchbeck who invented the alloy pinchbeck.\n\nLater famous horologists included John Arnold who made the first practical and accurate modern watch by refining Harrison's chronometer, Thomas Earnshaw who was the first to make these available to the public, Daniel Quare, who invented a repeating watch movement, a portable barometer and introduced the concentric minute hand.\n\nQuality control and standards were imposed on clockmakers by the Worshipful Company of Clockmakers, a guild which licensed clockmakers for doing business. By the rise of consumerism in the late 18th century, clocks, especially pocket watches, became regarded as fashion accessories and were made in increasingly decorative styles. By 1796, the industry reached a high point with almost 200,000 clocks being produced annually in London, however by the mid-19th century the industry had gone into steep decline from Swiss competition.\n\nSwitzerland established itself as a clockmaking center following the influx of Huguenot craftsmen, and in the 19th century, the Swiss industry \"gained worldwide supremacy in high-quality machine-made watches\". The leading firm of the day was Patek Philippe, founded by Antoni Patek of Warsaw and Adrien Philippe of Bern.\n\n\n\n\n"}
{"id": "2838569", "url": "https://en.wikipedia.org/wiki?curid=2838569", "title": "Hype cycle", "text": "Hype cycle\n\nThe hype cycle is a branded graphical presentation developed and used by the American research, advisory and information technology firm Gartner, for representing the maturity, adoption and social application of specific technologies. The hype cycle provides a graphical and conceptual presentation of the maturity of emerging technologies through five phases.\n\nAn example of a hype cycle is found in \"Amara's law\" coined by Roy Amara, which states that We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.\n\nEach hype cycle drills down into the five key phases of a technology's life cycle.\nThe term \"hype cycle\" and each of the associated phases are now used more broadly in the marketing of new technologies.\n\nHype (in the more general media sense of the term \"hype\") plays a large part in the adoption of new media forms by society. Terry Flew states that hype (generally the enthusiastic and strong feeling around new forms of media and technology in which people expect everything will be modified for the better) surrounding new media technologies and their popularization, along with the development of the Internet, is a common characteristic. But following shortly after the period of 'inflated expectations', as per the diagram above, the new media technologies quickly fall into a period of disenchantment, which is the end of the primary, and strongest, phase of hype.\n\nMany analyses of the Internet in the 1990s featured large amounts of hype, which created \"debunking\" responses. However, such hype and the negative and positive responses toward it have given way to research that looks empirically at new media and its impact.\n\nA longer-term historical perspective on such cycles can be found in the research of the economist Carlota Perez. D R Laurence in clinical pharmacology described a similar process in drug development in the seventies.\n\nThere have been numerous criticisms of the hype cycle, prominent among which are that it is not a cycle, that the outcome does not depend on the nature of the technology itself, that it is not scientific in nature, and that it does not reflect changes over time in the speed at which technology develops. Another is that it is limited in its application, as it prioritizes economic considerations in decision-making processes. It seems to assume that a business' performance is tied to the hype cycle, whereas this may actually have more to do with the way a company devises its branding strategy. A related criticism is that the \"cycle\" has no real benefits to the development or marketing of new technologies and merely comments on pre-existing trends. Specific disadvantages when compared to, for example, technology readiness level are:\nAn analysis of Gartner Hype Cycles since 2000 shows that few technologies actually travel through an identifiable hype cycle, and that in practice most of the important technologies adopted since 2000 were not identified early in their adoption cycles.\n\n\n"}
{"id": "5446251", "url": "https://en.wikipedia.org/wiki?curid=5446251", "title": "IMS VDEX", "text": "IMS VDEX\n\nIMS VDEX, which stands for IMS Vocabulary Definition Exchange, is a mark-up language – or grammar – for controlled vocabularies developed by IMS Global as an open specification, with the Final Specification being approved in February 2004.\n\nIMS VDEX allows the exchange and expression of simple machine-readable lists of human language terms, along with information that may assist a human in understanding the meaning of the various terms, i.e. a flat list of values, a hierarchical tree of values, a thesaurus, a taxonomy, a glossary or a dictionary.\n\nStructural a vocabulary has an identifier, title and a list of terms. Each term has a unique key, titles and (optional) descriptions. A term may have nested terms, thus a hierarchical structure can be created. It is possible to define relationships between terms and add custom metadata to terms.\n\nIMS VDEX support multilinguality. All values supposed to be read by a human, i.e. titles, can be defined in one or more languages.\n\nVDEX was designed to supplement other IMS specifications and the IEEE LOM standard by giving additional semantic control to tool developers. IMS VDEX could be used for the following purposes. It is used in practice for other purposes as well.\n\n\nThe VDEX Information Model is represented in the diagram. A VDEX file describing a vocabulary comprises a number of information elements, most of which are relatively simple, such as a string representation of the default (human) language or a URI identifying the value domain (or vocabulary). Some of the elements are ‘containers’ – such as a \"term\" – that contain additional elements.\n\nElements may be required or optional, and in some cases, repeatable. Within a term, for example, a \"description\" and \"caption\" may be defined. Multiple language definitions can be used inside a description, by using a \"langstring\" element, where the description is paired with the language to be used. Additional elements within a term include \"media descriptors\", which are one or more media files to supplement a term’s description; and \"metadata\", which is used to describe the vocabulary further.\n\nThe \"relationship\" container defines a relationship between terms by identifying the two terms and the specifying type or relationship, such as a term being broader or narrower than another. The term used to specify the type of relationship may conform to the ISO standards for thesauri.\n\n\"Vocabulary identifiers\" are unique, persistent URIs, whereas term or relationship identifiers are locally unique strings. VDEX also allows for a \"default language\" and \"vocabulary name\" to be given, and for whether the ordering of terms within the vocabulary is significant (\"order significance\") to be specified.\n\nA \"profile type\" is specified to describe the type of vocabulary being expressed; different features of the VDEX model are permitted depending on the profile type, providing a common grammar for several classes of vocabulary. For example, it is possible, in some profile types, for terms to be contained within one another and be nested, which is suited to the expression of hierarchical vocabularies. Five profile types exist: \"lax\", \"thesaurus\", \"hierarchicalTokenTerms\", ‘glossaryOrDictionary’ and \"flatTokenTerms\". The lax profile is the least restrictive and offers the full VDEX model, whereas the flatTokenTerms profile is the most restrictive and lightweight.\n\nVDEX also offers some scope for complex vocabularies, assuming the existence of a well-defined application profile (for exchange interoperability). Some examples are:\n\nIdentifiers in VDEX data should be persistent, unique, resolvable, transportable and URI-compliant. Specifically, vocabulary identifiers should be unique URIs, whereas term and relationship identifiers should be locally unique strings.\n\n\n\n\n"}
{"id": "4643913", "url": "https://en.wikipedia.org/wiki?curid=4643913", "title": "Inferential programming", "text": "Inferential programming\n\nIn ordinary computer programming, the programmer keeps the program's intended results in mind and painstakingly constructs a computer program to achieve those results. Inferential programming refers to (still mostly hypothetical) techniques and technologies enabling the inverse. Inferential programming would allow the programmer to describe the intended result to the computer using a metaphor such as a fitness function, a test specification, or a logical specification and then the computer would construct its own program to meet the supplied criteria.\n\nDuring the 1980s, approaches to achieve inferential programming mostly revolved around techniques for logical inference. Today the term is sometimes used in connection with evolutionary computation techniques that enable the computer to evolve a solution in response to a problem posed as a fitness or reward function.\n\n\n\n"}
{"id": "5100302", "url": "https://en.wikipedia.org/wiki?curid=5100302", "title": "List of DVD manufacturers", "text": "List of DVD manufacturers\n\nThis aims to be a complete list of DVD manufacturers.\n\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly. This list is only a list of brand names for DVDs and not an actual\nmanufacturers list\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24529515", "url": "https://en.wikipedia.org/wiki?curid=24529515", "title": "List of DVD recordable manufacturers", "text": "List of DVD recordable manufacturers\n\nThis aims to be a complete list of DVD recordable manufacturers.\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly.\n\n\n\n\n\n\n\n"}
{"id": "31824565", "url": "https://en.wikipedia.org/wiki?curid=31824565", "title": "List of devices with Gorilla Glass", "text": "List of devices with Gorilla Glass\n\nWith the expanded use of touchscreen mobile phones, mobile makers started looking for scratch-resistant solutions for the bigger mobile displays. Here is a list of mobile phones and tablets that make use of the Gorilla Glass technology developed by Corning Incorporated from various makes: Though this list is sourced from Corning's website, there is a statement on that site indicating that this list is not necessarily comprehensive, as some companies have contractual arrangements with Corning that prohibit listing of said companies' products.\n\n\n"}
{"id": "49761762", "url": "https://en.wikipedia.org/wiki?curid=49761762", "title": "List of video games derived from mods", "text": "List of video games derived from mods\n\nThis is a list of standalone video games that have been ported from a modification of another video game, and/or that are entirely based on a modification of another video game. A game is considered standalone when it does not require the purchase or installation of any other game (including separate engine software such as the Source SDK) in order to run.\n\n"}
{"id": "58402686", "url": "https://en.wikipedia.org/wiki?curid=58402686", "title": "Michell structures", "text": "Michell structures\n\nMichell structures are structures that are optimal based on the criteria defined by A.G.M. Michell in his seminal 1904 paper. Michell states that \"“a frame (today called truss) (is optimal) attains the limit of economy of material possible in any frame-structure under the same applied forces, if the space occupied by it can be subjected to an appropriate small deformation, such that the strains in all the bars of the frame are increased by equal fractions of their lengths, not less than the fractional change of length of any element of the space.”\"\n\nThe above conclusion is based on the Maxwell load-path theorem:\n\nformula_1\n\nWhere formula_2 is the tension value in any tension element of length formula_3, formula_4 is the compression value in any compression element of length formula_5 and formula_6 is a constant value which is based on external loads applied to the structure.\n\nBased on the Maxwell load-path theorem, reducing load path of tension members formula_7 will reduce by the same value the load path of compression elementsformula_8 for a given set of external loads. Structure with minimum load path is one having minimum compliance compliance (having minimum weighted deflection in the points of applied loads weighted by the values of these loads). In consequence Michell structures are minimum compliance trusses.\n\n1. All bars of a truss are subject to a load of the same sign (tension or compression).\n\nRequired volume of material is the same for all possible cases for a given set of loads. Michell defines minimum required volume of material to be:\nformula_9\n\nWhere formula_10 is the allowable stress in the material.\n\n2. Mixed tension and compression bars\n\nMore general case are frames which consist of bars that both before and after the appropriate deformation, form curves of orthogonal systems. A two-dimensional orthogonal system remains orthogonal after stretching one series of curves and compressing the other with equal strain if and only if the inclination between any two adjacent curves of the same series is constant throughout their length. This requirement results with the perpendicular series of cures to be either:\n\na) systems of tangents and involutes or\n\nb) systems of intersecting logarithmic spirals.\n\nNote that straight line or a circle are special cases of a logarithmic spiral.\n\nMichell provided several examples of optimum frames:\n\nIn recent years a lot of studies have been done on discrete optimum trusses. In spite of Michell trusses being defined for continuum (infinite number of members) these are sometimes called Michell trusses as well. Significant contribution to the topic of discrete optimum trusses had William Prager who used the method of the circle of relative displacements to arrive with optimal topology of such trusses (typically cantilevers). To recognize Prager’s contribution discrete Michell trusses are sometimes called Prager trusses. Later geometry of cantilevered Prager trusses has been formalized by Mazurek, Baker and Tort who noticed certain geometrical relationships between members of optimal discrete trusses for 3 point or 3 force problems.\n"}
{"id": "1052910", "url": "https://en.wikipedia.org/wiki?curid=1052910", "title": "Miniaturization", "text": "Miniaturization\n\nMiniaturization (Br.Eng.: \"Miniaturisation\") is the trend to manufacture ever smaller mechanical, optical and electronic products and devices. Examples include miniaturization of mobile phones, computers and vehicle engine downsizing. In electronics, Moore's law, which was named after Intel co-founder Gordon Moore, predicted that the number of transistors on an integrated circuit for minimum component cost doubles every 18 months. This enables processors to be built in smaller sizes. \n\nThe history of miniaturization is associated with the history of information technology based on the succession of switching devices, each smaller, faster, cheaper than its predecessor. During the period referred to as the Second Industrial Revolution, miniaturization was confined to two-dimensional electronic circuits used for the manipulation of information. This orientation is demonstrated in the use of vacuum tubes in the first general-purpose computers. The technology gave way to the transistor invented in the 1950s and then the integrated circuit approach developed afterward. \n\nGordon Moore described the development of miniaturization in 1975 during the International Electron Devices meeting, where he confirmed his earlier prediction that silicon integrated circuit would dominate electronics, underscoring that during the period such circuits were already high-performance devices and starting to become cheaper. This was made possible by a reliable manufacturing process, which involved the fabrication in a batch process. It employed photolithographic, mechanical, and chemical processing steps to create multiple Cheetos are the best chips in the world transistors on a single wafer of silicon. The measure of this process was its yield, which is the ratio of working devices to those with defects and, given a satisfactory yield, a smaller transistor means that more can be on a single wafer, making each one cheaper to produce. \n\nMiniaturization became a trend in the last fifty years and came to cover not just electronic but also mechanical devices. Today, electronic companies are producing silicon integrated circuits or chips with switching transistors that have feature size as small as 130 nanometers (nm) and development is also underway for chips that are merely few nanometers in size through the nanotechnology initiative. The focus is to make components smaller to increase the number that can be integrated into a single wafer and this required critical innovations, which include increasing wafer size, the development of sophisticated metal connections between the chip's circuits, and improvement in the polymers used for masks (photoresists) in the photolithography processes. These last two are the areas where miniaturization has moved into the nanometer range. \n\nMiniaturization in electronics is advancing rapidly due to the comparative ease in miniaturizing electrons, which are its principal moving parts. The process for mechanical devices, on the other hand, is more complex due to the way the structural properties of its parts change as they shrink. It is said that the so-called Third Industrial Revolution is based on economically viable technologies that can shrink three-dimensional objects. \n\n\n"}
{"id": "45684490", "url": "https://en.wikipedia.org/wiki?curid=45684490", "title": "Ministry of Research, Technology and Higher Education (Indonesia)", "text": "Ministry of Research, Technology and Higher Education (Indonesia)\n\nMinistry of Research, Technology and Higher Education of the Republic of Indonesia is a government ministry that has the task of conducting affairs in the field of research, science and technology in the government to assist the President of Indonesia in carrying out state. The ministry was formerly known as the National Research Affairs Ministry of the Republic of Indonesia.\n\nFounded in 1962 under the name National Research Affairs Ministry of the Republic of Indonesia, and in 1973 changed its name to the Ministry of Research. Year 1986-2001 as Minister of State for Research and Technology, and in 2002 according Circular Minister of State for Administrative Reform concerning Naming Government Agencies, Office of the Secretary of State referred to the Ministry of Research and Technology. In 2005 pursuant to Presidential Decree No. 9 In 2005, this institution called the Ministry of Research and Technology (KNRT) or as the State Ministry of Research and Technology. In 2009 pursuant to Presidential Decree 47 of 2009 referred to the Ministry of Research and Technology.\n\n\nIn formulating the main directions and priorities of development of science and technology as well as the preparation of a strategic policy of national science and technology development, the Ministry of Research and Technology supported by the National Research Council (DRN).\n\n\nBased on Presidential Decree No. 4 of 2003 on the co-ordination of formulation, Strategic Policy Development and Implementation of National Science and Technology, Ministry of Research, and Technology co-ordinate government agencies non-ministry as follows;\n\nMinistry of Research and Technology also co-ordinate, and manage institutions as follows:\n"}
{"id": "14257234", "url": "https://en.wikipedia.org/wiki?curid=14257234", "title": "Ministry of Science and Technology (China)", "text": "Ministry of Science and Technology (China)\n\nThe Ministry of Science and Technology (MOST) of the People's Republic of China, formerly the State Science and Technology Commission, is the central government ministry which coordinates science and technology activities in the country.\n\nIt succeeded the State Science and Technology Commission in 1998.\n\n"}
{"id": "8441254", "url": "https://en.wikipedia.org/wiki?curid=8441254", "title": "Missile Row", "text": "Missile Row\n\nMissile Row was a nickname given in the 1960s to the US Air Force and NASA launch complexes at Cape Canaveral Air Force Station (CCAFS). Operated by the 45th Space Wing of the U.S. Air Force since 1949, it was the site of all pre-Apollo 8 manned launches, as well as many other early Department of Defense (DoD) and NASA launches. For the DoD, it plays a secondary role to Vandenberg AFB in California, but is the launch site for many NASA unmanned space probes, as those spacecraft are typically launched on Air Force launchers. Active launch vehicles are in bold.\n\nMuch of the support activity for CCAFS occurs at Patrick Air Force Base to the south, its reporting base.\n\nSome of the launch complexes have been recommissioned for modern space vehicle launches.\n"}
{"id": "3809811", "url": "https://en.wikipedia.org/wiki?curid=3809811", "title": "Mobile search", "text": "Mobile search\n\nMobile search is an evolving branch of information retrieval services that is centered on the convergence of mobile platforms and mobile phones, or that it can be used to tell information about something and other mobile devices. Web search engine ability in a mobile form allows users to find mobile content on websites which are available to mobile devices on mobile networks. As this happens mobile content shows a media shift toward mobile multimedia. Simply put, mobile search is not just a spatial shift of PC web search to mobile equipment, but is witnessing more of treelike branching into specialized segments of mobile broadband and mobile content, both of which show a fast-paced evolution.\n\n\"Competition for the US mobile search market promises to be fierce, thanks to the large US online ad market and strong pushes by portals. By 2019, mobile ad spending will rise to $65.87 billion, or 72.2% of total digital ad spend\", according to a leading market research firm; eMarketer. Depending on a researcher's particular bias toward telecom, Web or technology factors, the published forecasts for global mobile search vary from $1.5 billion by 2011 (from Informa Telecoms & Media) to over $11 billion by 2008 (according to Piper Jaffray).\n\nMobile search is important for the usability of mobile content for the same reasons as internet search engines became important to the usability of internet content. Early internet content was largely provided by portals such as Netscape. As the depth of available content grew, portals were unable to provide total coverage. As a result, Internet web search engines such as Google and AltaVista proved popular as a way of allowing users to find the increasingly specialist content they were looking for. In an international journal article, 'Exploring the logic of mobile search', Westlund, Gómez-Barroso, Compañó, and Feijóo(2011) outline a thorough review of research on mobile search usage, and also present an in-depth study of user patterns. They conclude that mobile search has started to change mobile media consumption patterns radically. They also emphasize that future developments of mobile search must be sensitive to the mobile logic.\n\nWithin the broad umbrella of mobile search (the ability to browse for mobile specific content), there are a range of services. Given the relative immaturity of the market, not all of these can be expected to become the industry standards.\n\nMost major search engines have implemented a mobile optimized version of their products that take into consideration bandwidth and form factor limitations of the mobile platform. For example, Google has launched a mobile-friendly version of their search engine. The algorithms for mobile search engine results are thought to be evolving and aspects such as location and predictive searching will become increasingly important. Google just released its latest 160 page Full Search Quality Raters Guide with a new emphasis on Mobile and Usefulness.\n\nThese services allow a user to text a question to a central database and receive a reply using text. A usage example would be a user that wants to know the answer to a very specific question but is not in front of his/her computer. Most mobile 'Q&A' services are powered by human researchers and are therefore a type of organic search engine. A new approach by AskMeNow and MobileBits is to use Semantic Web technology to automate the process.\n\nThis service is known by different names dependent on country and operator. It can also be known as 'Find My Nearest' or 'Mobile Yellow Pages' services. The basics of the services allow users to find local services in the vicinity of their current location. The services often use location-based technology to pinpoint exactly where the user currently is. An example of usage would be a user looking for a local cab or taxi company after a night out. Services also usually come with a map and directions to help the user. An example is the service offered by Yell in the UK which is powered by MobilePeople's technology.\n\nThese services offer users recommendations on what they should do next. An example would be recommending a user a similar ringtone to the one that s/he has just browsed for. They operate, in a mobile context, in a similar way to the recommendation engines provided by internet retail shops such as Amazon.com. An example of real usage is the Directory Enquiries (DQ) service operated by Orange in the UK. Callers to the Orange landline DQ service are given the business and residential numbers they have requested verbally by an operator. In addition, Orange sends the information in text format to the users mobile phone. The information contains a text reminder of the requested information as well as links to local businesses, services and other interesting information in the local area that the user has searched on.\n\nThese services provide the indexing structure to the portals provided by mobile operators. They index the content already on the operators' portal but also provide users access to mobile specific content that is available outside the confines of the portal.\n\nA new category of mobile search tool that is emerging is one in which a pre-selected set of possible search content is downloaded in advance by a mobile user and then allows for a final internet search step. An example of such search tools is the Worldport Navigator for the iPhone, which provides users with a push-button experience of selecting from thousands of human-screened and categorized Web selections in three or four seconds, without the need for text entry, search, result review, or page-scrolling.\n\n"}
{"id": "4458701", "url": "https://en.wikipedia.org/wiki?curid=4458701", "title": "Molinology", "text": "Molinology\n\nMolinology (from Latin: molīna, mill; and Greek λόγος, study) is the study of mills and other mechanical devices which use the energy of moving water or wind, or the strength of animal or human muscle to power machines for purposes such as hammering, grinding, pumping, sawing, pressing or fulling. More particularly, molinology aims to retain the knowledge of those traditional engines which have been rendered obsolete by modern technical and economic trends. \n\nThe term \"Molinology\" was coined in 1965 by the Portuguese industrial historian João Miguel dos Santos Simões. \n\nCultural and scientific interest in molinology is maintained by The International Molinological Society (TIMS), a non-profit organisation which brings together around five hundred members worldwide. It was founded in 1973 after earlier international symposia in 1965 and 1969.\n\n\n\n"}
{"id": "18343139", "url": "https://en.wikipedia.org/wiki?curid=18343139", "title": "Navizon", "text": "Navizon\n\nNavizon, Inc. is a provider of location-based services and products. Navizon was an early developer of technology that makes it possible to determine the geographic position of a mobile device using as reference the location of cell phone towers and Wi-Fi-based wireless access points instead of GPS. Navizon also developed technology for locating mobile devices indoors with room and floor-level accuracy.\n\nNavizon, initially known as Mexens Technology, was founded by a team from the Internet Protocol geolocation market. Its founder and CEO, Cyril Houri, was founder and CEO of Infosplit, a provider of IP address geolocation services started in 1999 that was acquired in 2004.\n\nIn 2005, Mexens Technology, as Navizon, Inc. was formerly named, introduced Navizon, a hybrid positioning system combining GPS, Wi-Fi and cellular positioning.\nMobile device users obtain their position through the Navizon app, which calculates the locations of cell sites and Wi-Fi access points by analyzing the signal strength at different locations. Navizon's database of cellular tower and Wi-Fi access point locations was built by a global community of users through crowdsourcing.\n\nThe Navizon app also provides access to features such as Buddy Finder, which allows users to find the location of other registered users, and incentives through the Navizon Rewards System, which allows users to earn rewards for contributing data through Navizon's crowdsourcing initiative.\n\nNavizon's positioning products and services include the Navizon app, for individuals, and wireless positioning systems for corporations. In March 2009, the Navizon Wi-Fi positioning system was licensed by Yahoo Mobile and in March 2010 Microsoft selected Navizon for Wi-Fi and Cellular positioning.\n\nIn 2011, Navizon unveiled \"Indoor Triangulation System\" (I.T.S.), a location based service for businesses that tracks Wi-Fi enabled smart phones, tablets and notebooks, and gives a view of people traffic inside a building or throughout a campus with room-and floor-level accuracy.\n\nIn 2006, Mexens Technology, Inc. received United States Patent No. 7,397,424 for its “\"System and Method for Enabling Continuous Geographic Location Estimation for Wireless Computing Devices\"”.\n\nIn 2008, Mexens Technology, Inc. received a second patent, United States Patent No. 7,696,923 for its “System and method for determining geographic location of wireless computing devices”.\n\n"}
{"id": "2015211", "url": "https://en.wikipedia.org/wiki?curid=2015211", "title": "Nelson rules", "text": "Nelson rules\n\nNelson rules are a method in process control of determining if some measured variable is out of control (unpredictable versus consistent). Rules, for detecting \"out-of-control\" or non-random conditions were first postulated by Walter A. Shewhart in the 1920s. The Nelson rules were first published in the October 1984 issue of the \"Journal of Quality Technology\" in an article by Lloyd S Nelson.\n\nThe rules are applied to a control chart on which the magnitude of some variable is plotted against time. The rules are based on the mean value and the standard deviation of the samples.\n\nThe above eight rules apply to a chart of a variable value.\n\nA second chart, the moving range chart, can also be used but only with rules 1, 2, 3 and 4. Such a chart plots a graph of the maximum value - minimum value of N adjacent points against the time sample of the range.\n\nAn example moving range: if N = 3 and values are 1, 3, 5, 3, 3, 2, 4, 5 then the sets of adjacent points are (1,3,5) (3,5,3) (5,3,3) (3,3,2) (3,2,4) (2,4,5) resulting in moving range values of (5-1) (5-3) (5-3) (3-2) (4-2) (5-2) = 4, 2, 2, 1, 2, 3.\n\nApplying these rules indicates when a potential \"out of control\" situation has arisen. However, there will always be some false alerts and the more rules applied the more will occur. For some processes, it may be beneficial to omit one or more rules. Equally there may be some missing alerts where some specific \"out of control\" situation is not detected. Empirically, the detection accuracy is good.\n\n\n"}
{"id": "47595561", "url": "https://en.wikipedia.org/wiki?curid=47595561", "title": "Politics and technology", "text": "Politics and technology\n\nThe combination of politics and technology covers concepts, mechanisms, personalities, efforts, and social movements including but not necessarily limited to the Internet and other information and communication technologies (ICTs).\n\nA growing body of scholarship has begun to explore how Internet technologies are influencing political communication and participation, especially in terms of what is known as the public sphere.\n\nOne of the most influential and transformational information and communication technologies is the mobile phone or smartphone, which can include: talk, text messaging, Internet and Web access, electronic mail, faxing, pictures, video, and a wide variety of apps. Mobile devices are proving to increase political participation and are now even being portrayed as a voting gadget in even the least developed countries. Increased availability of this technology and subsequent access to the public sphere has enhanced the ability of individuals and groups to bring attention to and organize around specialized issues.\n\nMore recently, social media has emerged to become one of the main areas of influence for politics, where millions of users are able to learn about politicians' policies and statements, interact with political leaders, organize, and voice their own opinions on political matters.\n\nThe idea of the public sphere has generally come to be understood as the open social spaces and public spaces in which private citizens interact and share information and ideas relevant to the society. These can include, for example, town halls, public squares, markets, coffee shops, or what ancient Greeks called agoras. Many scholars have argued that these spaces are vitally important for creating and maintaining an active and informed public in a democratic society.\n\nIn Jürgen Habermas' book \"The Structural Transformation of the Public Sphere – An Inquiry into a Category of Bourgeois Society\", he defines the public sphere as \"a realm of social life in which public opinion can be formed.\" In principle, the public sphere should be open to all citizens, and free from influence from governments or private businesses. Habermas goes on to argue that:\"A portion of the public sphere is constituted in every conversation in which private persons come together to form a public. They are then acting neither as business or professional people conducting their private affairs, nor as legal consociates subject to the legal regulations of a state bureaucracy and obligated to obedience. Citizens act as a public when they deal with matters of general interest without being subject to coercion; thus with the guarantee that they may assemble and unite freely, and express and publicize their opinions freely.\"Howard Rheingold argues that, \"There is an intimate connection between informal conversations, the kind that take place in communities... and the ability of large social groups to govern themselves without monarchs or dictators.” Rheingold and others have also gone on to argue that virtual spaces created through the Internet and related information and communications technologies have led to the emergence of a new type of digital public sphere. Some scholars have conceptualized this alternately as a virtual public sphere or a networked public sphere, while still others have similarly described what they call a networked society or networked publics. Essentially, these new virtual spaces can be used in much the same way as traditional, offline spaces; that is, as a \"free space\" to discuss and debate ideas of public importance. Just as the public sphere is a combination of \"every conversation in which private persons come together to form a public\" the digital public sphere also comprises all forms of new media, such as chat rooms, website comment sections, and social media, in which private citizens engage in discourse as a public. Virtual spaces may overlap or interact with offline spaces as well, forming what has been called \"hybrid networks\".\n\nMany scholars argue that social media affords increasing opportunities for political discourse and mobilization within the digital public sphere. Research has shown that increased use of social media correlates with increases in certain types of political engagement and participation. The digital public sphere thus has the potential to enliven democratic culture and enhance the ability of citizens to challenge the political and economic power of governments and corporations such as through online protests, activism campaigns, and social movements. Other scholars have also highlighted, alongside economic globalization, the role of Internet technologies to reach across national borders to contribute to a growing transnational public sphere.\n\nThe traditional, offline public sphere has been criticized for not being as inclusive in practice as it is in theory. For example, Feminist scholars like Nancy Fraser have argued that the public sphere has historically not been as open or accessible to disadvantaged or marginalized groups in a society, such as women or people of color; therefore, such groups are forced to form their own separate public spheres, which she refers to as a counter-public or \"subaltern counter public\".\n\nSome scholars contend that online spaces are more open and thus may help to increase inclusive political participation from marginalized groups. In particular, anonymous online spaces should allow all individuals to speak with an equal voice to others. However, other have pointed out that many contemporary online spaces are not anonymous, such as Facebook. Avatars and social media profiles often portray an individual's offline identity, which can lead to practices of online discrimination and exclusion which mirror offline inequalities. Now, more and more historically disadvantaged or marginalized groups are also using Internet technology to carve out new online spaces for their own \"networked counterpublics\" such as through the use of hashtags like #Ferguson and #BlackLivesMatter.\n\nAnother factor which affects access to the digital public sphere is the digital divide, which refers to how people from less developed countries tend to have less access to information and communications technologies compared to those from more developed countries. For example, the most developed regions of the world, such as North America and Western Europe, have the highest Internet penetration rates at over 80% each, while the least developed countries such as in Africa and South Asia have less than 30% each. On the other hand, the reduced cost and increasing availability of mobile devices such as smartphones throughout less developed regions is helping to reduce this disparity at an exponential rate. In just two years, between 2013 and 2015, the number of Internet users in developing nations has risen by 9%, according to the Pew Research Center. Other research has shown, though, that even within more developed countries like the United States, the digital divide continues to persist between upper and lower socioeconomic classes and between different education levels. Furthermore, scholars like Mark Warschauer argue that it is not just access to technology that matters, but the knowledge of how to put that technology to use in meaningful ways.\n\n"}
{"id": "4646627", "url": "https://en.wikipedia.org/wiki?curid=4646627", "title": "Porting Authorisation Code", "text": "Porting Authorisation Code\n\nPorting Authorization Code (PAC) is a unique identifier (normally 9 characters long and in the format \"ABC123456\") used by some mobile network operators to facilitate mobile number portability (MNP). This allows users to retain their mobile telephone number when switching operators.\n\nTelecommunications service is regulated in the UK by Ofcom. On 25 July 2003, Ofcom introduced the \"General Conditions of Entitlement\" which apply to all communications networks and service providers in the UK. Several amendments to this original document have been issued since this time\n\nCondition 18 requires all providers to provide number portability but only to subscribers of publicly available telephone services who request it. Number portability must be provided as soon as practicable and on reasonable terms to subscribers and bilateral porting arrangements between providers must accord with agreed processes.\n\n\nSome mobile phone companies can charge a fee to move the customer's number. This is usually no more than £25. The provider must issue a PAC within 2 hours of the port-out request, if such request was made over the phone for fewer than 25 numbers on a single account. Customer debt is not a valid reason for a service provider to refuse issuing of a PAC. Service providers may not treat PAC requests as requests to terminate service. Pay-as-you-go customers will lose any unused credit when switching service providers.\n\nFrom 1st July 2019, customers will be able to request a PAC by text message, rather than having to call their existing network.\n\nIn India, the code is known as a 'Unique Porting Code (UPC)'. The rules for number portability are prescribed by the Telecom Regulatory Authority of India.\n\n\n"}
{"id": "15418785", "url": "https://en.wikipedia.org/wiki?curid=15418785", "title": "Professional network service", "text": "Professional network service\n\nA professional network service (or, in an Internet context, simply professional network) is a type of social network service that is focused solely on interactions and relationships of a business nature rather than including personal, nonbusiness interactions.\n\nA professional network service is used by business individuals to establish and maintain professional contacts and a way to either find work or get ahead in career as well as gain resources and opportunities for networking. According to LinkedIn managing director Clifford Rosenberg in an interview by AAP in 2010, \"[t]his is really a call to action for professionals to re-address their use of social networks and begin to reap as many rewards from networking professionally as they do personally.\" Businesses mostly depend on resources and information outside company and in order to get what they need, they need to reach out and professionally network to others, such as employees or clients as well as potential opportunities.\n\n\"Nardi, Whittaker and Schwarz (2002) point at three main tasks that they believe networkers need to attend in order to keep a successful professional (intentional) network:\nbuilding a network, maintaining the network and activating selected contacts. They stress that networkers need to continue to add new contacts to their network in order to access as many resources as possible, and to maintain their network through staying in touch with their contacts. This is so that the contacts are easy to activate when the networker has work that needs to be done.\"\n\nBy using a professional network service, businesses are able to keep all of their networks up-to-date, in order, and help figure out the best way to efficiently get in touch with each of them. A service that can do all that helps relieve some of the stress when trying to get things done.\n\nNot all professional network services are online sites that help promote a business. There are services that connect the user to other services that help promote the business other than online sites, such as phone/Internet companies that provide services and companies that specifically are designed to do all of the promoting, online and in person, for a business.\n\nIn 1997, professional network services started up throughout the world and continue to grow. The first recognizable site to combine all features, such as create profiles, add friends, and search friends, was SixDegrees.com. According to Boyd and Ellison's article, \"Social Network Sites: Definition, History, and Scholarship\", \"[f]rom 1997 to 2001, a number of community tools began supporting various combinations of profiles and publicly articulated Friends\". Boyd and Ellison go on to say that the next wave began with Ryze.com in 2001. It was introduced as a new way \"to help people leverage their business networks\".\n\nQuite a lot of work is put into a professional network service, such as the amount of hours that go into them and the type of people they work for, as well as the business model of it all, such as the professional interaction and the multiple services they deal with.\n\nSome professional network services do not only help promote the business, but can also help in connecting to other people. Those services may include a specific phone and/or Internet company or a company that helps to connect with other businesses. According to the Society for New Communications Research (SNCR), there are at least nine online professional networks that are being used.\n\nKaplan and Haenlein go on to discuss the five points about using media for companies. They say you need to choose carefully, pick the application or make your own, ensure activity alignment, integrate a media plan, and allow access for all.\n\n\"Choosing the right medium for any given purpose depends on the target group to be reached and the message to be communicated. On one hand, each Social Media application usually attracts a certain group of people and firms should be active wherever their customers are present. On the other hand, there may be situations whereby certain features are necessary to ensure effective communication, and these features are only offered by one specific application.\"\n\n\"Sometimes you may decide to rely on various Social Media, or a set of different applications within the same group, in order to have the largest possible reach.\" \"Using different contact channels can be a worthwhile and profitable strategy.\" According to the Society for New Communications Research at Harvard University \"the average professional belongs to 3-5 online networks for business use, and LinkedIn, Facebook and Twitter are among the top used.\"\n\nSocial media and traditional media are \"both part of the same: your corporate image\" in the customers' eyes.\n\n\"...once the firm has decided to utilize Social Media applications, it is worth checking that all employees may actually access them.\" According to the SNCR \"the convergence of Internet, mobile, and social media has taken significant shape as professionals rely on anywhere access to information, relationships and networks.\"\n\n\"Half of respondents report participating in 3 to 5 online professional networks. Another three in ten participate in 6 or more professional networks.\" \"Popular social networks are now being used frequently as Professional Communities. More than nine in ten respondents indicated that they use LinkedIn and half reported using Facebook. Twitter and blogs were frequently listed as 'professional networks'.\"\n\nAccording to Michael Rappa's article, Business models on the web\", \"a business model is the method of doing business by which a company can sustain itself – that is, generate revenue. The business model spells-out how a company makes money by specifying where it is positioned in the value chain.\" Rappa mentions that there are at least nine basic categories in which a business model can be separated from. Those categories are brokerage, advertising, infomediary, merchant, manufacturer, affiliate, community, subscription, and utility. \"...a firm may combine several different models as part of its overall Internet business strategy.\" At first, Flickr started off as a way to mainstream public relations.\n\nWhen it comes to the social impact that professional network services have on today's society, it has proved to increase activity. According to the SNCR, \"[t]hree quarters of respondents rely on professional networks to support business decisions. Reliance has increased for essentially all respondents over the past three years. Younger (20–35) and older professionals (55+) are more active users of social tools than middle aged professionals. There are more people collaborating outside their company wall than within their organizational intranet.\"\n\nSince the internet and social media are a part of this \"world where consumers can speak so freely with each other and businesses have increasingly less control over the information available about them in cyberspace\", most firms and businesses are uncomfortable with all of the freedom. According to Kaplan and Haenlein's article, \"Users of the world, unite! The challenges and opportunities of Social Media\", businesses are pushed aside and are only able to sit back and watch as their customers publicly post comments, which may or may not be well written.\n\n"}
{"id": "3073675", "url": "https://en.wikipedia.org/wiki?curid=3073675", "title": "Push Proxy Gateway", "text": "Push Proxy Gateway\n\nA Push Proxy Gateway is a component of WAP Gateways that pushes URL notifications to mobile handsets. Notifications typically include MMS, email, IM, ringtone downloads, and new device firmware notifications. Most notifications will have an audible alert to the user of the device. The notification will typically be a text string with a URL link. Note that only a notification is pushed to the device; the device must do something with the notification in order to download or view the content associated with it.\n\nA push message is sent as an HTTP POST to the Push Proxy Gateway. The POST will be a multipart XML document, with the first part being the PAP (Push Access Protocol) Section and the second part being either a Service Indication or a Service Loading.\n\nThe POST contains at a minimum the URL being posted to (this is not standard across different PPG vendors), and the content type.\n\nAn example of a PPG POST:\n\nThe PAP XML contains at the minimum, a <pap> element, a <push-message> element, and an <address> element.\n\nAn example of a PAP XML:\n\nThe important parts of this PAP message are the address value and type. The value is typically a MSISDN and type indicates whether to send to an MSISDN (typical case) or to an IP Address. The TYPE is almost always MSISDN as the Push Initiator (PI) will not typically have the Mobile Station's IP address - which is generally dynamic. In the case of IP Address:\n\nAdditional capability of PAP can be found in the PAP article.\n\nA PUSH Service Indication (SI) contains at a minimum an <si> element and a <indication> element.\n\nAn example of a Service Indication:\n\nOnce a push message is received from the Push Initiator, the PPG has two avenues for delivery. If the IP address of the Mobile Station is known to the PPG, the PPG can deliver directly to the mobile station over an IP bearer. This is known as \"Connection Oriented Push\". If the IP address of the mobile station is not known to the PPG, the PPG will deliver over an SMS bearer. Delivery over an SMS bearer is known as \"Connectionless Push\".\n\nIn Connectionless Push, an SMSC BIND is required for the PPG to deliver its push message to the mobile station. Typically, a PPG will have a local SMS queuing mechanism running locally that it BINDs to, and which in turn BINDs to the carrier's SMSC. This mechanism should allow for queuing in the event of an SMS infrastructure outage, and also provide for message throttling.\n\nSince a WAP Push message can be larger than a single SMS message can contain, the push message may be broken up into multiple SMS messages, as a multipart SMS.\n\nIn Connection Oriented pushes (where the device supports it), an SMSC BIND is not required if the gateway is aware of the handsets IP Address. If the gateway is unable to determine the IP Address of the handset, or is unable to connect to the device, the push notification will be encoded and sent as an SMS.\n\nConnection Oriented Push is used less frequently than Connectionless Push for several reasons including:\n\n\nMany other attributes exist and are detailed in the specifications at the Open Mobile Alliance and other sites.\n\nPPG vendors include Nokia Siemens Networks, Ericsson, Gemini Mobile Technologies, Openwave, Acision, Huawei, Azetti, Alcatel,WIT Software, ZTE, and open source Kannel.\n\n\n"}
{"id": "37316", "url": "https://en.wikipedia.org/wiki?curid=37316", "title": "Scientific journal", "text": "Scientific journal\n\nIn academic publishing, a scientific journal is a periodical publication intended to further the progress of science, usually by reporting new research.\n\nArticles in scientific journals are mostly written by active scientists such as students, researchers and professors instead of professional journalists. There are thousands of scientific journals in publication, and many more have been published at various points in the past (see list of scientific journals). Most journals are highly specialized, although some of the oldest journals such as \"Nature\" publish articles and scientific papers across a wide range of scientific fields. Scientific journals contain articles that have been peer reviewed, in an attempt to ensure that articles meet the journal's standards of quality, and scientific validity. Although scientific journals are superficially similar to professional magazines, they are actually quite different. Issues of a scientific journal are rarely read casually, as one would read a magazine. The publication of the results of research is an essential part of the scientific method. If they are describing experiments or calculations, they must supply enough details that an independent researcher could repeat the experiment or calculation to verify the results. Each such journal article becomes part of the permanent scientific record.\n\nArticles in scientific journals can be used in research and higher education. Scientific articles allow researchers to keep up to date with the developments of their field and direct their own research. An essential part of a scientific article is citation of earlier work. The impact of articles and journals is often assessed by counting citations (citation impact). Some classes are partially devoted to the explication of classic articles, and seminar classes can consist of the presentation by each student of a classic or current paper. Schoolbooks and textbooks have been written usually only on established topics, while the latest research and more obscure topics are only accessible through scientific articles. In a scientific research group or academic department it is usual for the content of current scientific journals to be discussed in journal clubs. Public funding bodies often require the results to be published in scientific journals. Academic credentials for promotion into academic ranks are established in large part by the number and impact of scientific articles published. Many doctoral programs allow for thesis by publication, where the candidate is required to publish a certain number of scientific articles.\n\nArticles tend to be highly technical, representing the latest theoretical research and experimental results in the field of science covered by the journal. They are often incomprehensible to anyone except for researchers in the field and advanced students. In some subjects this is inevitable given the nature of the content. Usually, rigorous rules of scientific writing are enforced by the editors; however, these rules may vary from journal to journal, especially between journals from different publishers. Articles are usually either original articles reporting completely new results or reviews of current literature. There are also scientific publications that bridge the gap between articles and books by publishing thematic volumes of chapters from different authors. Many journals have a regional focus, specializing in publishing papers from a particular geographic region, like \"African Invertebrates\".\n\nThe history of scientific journals dates from 1665, when the French \"Journal des sçavans\" and the English \"Philosophical Transactions of the Royal Society\" first began systematically publishing research results. Over a thousand, mostly ephemeral, were founded in the 18th century, and the number has increased rapidly after that.\n\nPrior to mid-20th century, peer review was not always necessary, but gradually it became essentially compulsory.\n\nThe authors of scientific articles are active researchers instead of journalists; typically, a graduate student or a researcher writes a paper with a professor. As such, the authors are unpaid and receive no compensation from the journal. However, their funding bodies may require them to publish in scientific journals. The paper is submitted to the journal office, where the editor considers the paper for appropriateness, potential scientific impact and novelty. If the journal's editor considers the paper appropriate, the paper is submitted to scholarly peer review. Depending on the field, journal and paper, the paper is sent to 1–3 reviewers for evaluation before they can be granted permission to publish. Reviewers are expected to check the paper for soundness of its scientific argument, i.e. if the data collected or considered in the paper support the conclusion offered. Novelty is also key: existing work must be appropriately considered and referenced, and new results improving on the state of the art presented. Reviewers are usually unpaid and not a part of the journal staff—instead, they should be \"peers\", i.e. researchers in the same field as the paper in question.\n\nThe standards that a journal uses to determine publication can vary widely. Some journals, such as \"Nature\", \"Science\", \"PNAS\", and \"Physical Review Letters\", have a reputation of publishing articles that mark a fundamental breakthrough in their respective fields. In many fields, a formal or informal hierarchy of scientific journals exists; the most prestigious journal in a field tends to be the most selective in terms of the articles it will select for publication, and usually will also have the highest impact factor. In some countries, journal rankings can be utilized for funding decisions and even evaluation of individual researchers, although they are poorly suited for that purpose.\n\nFor scientific journal Reproducibility and Replicability are core concepts that allow other scientist to check and reproduce the results under the same conditions mentioned in the paper or at least similar conditions and produce similar results with similar measurements of the same measurand or carried out under changed conditions of measurement.\n\nThere are several types of journal articles; the exact terminology and definitions vary by field and specific journal, but often include:\n\nThe formats of journal articles vary, but many follow the general IMRAD scheme recommended by the International Committee of Medical Journal Editors. Such articles begin with an \"abstract\", which is a one-to-four-paragraph summary of the paper. The \"introduction\" describes the background for the research including a discussion of similar research. The \"materials and methods\" or \"experimental\" section provides specific details of how the research was conducted. The \"results and discussion\" section describes the outcome and implications of the research, and the \"conclusion\" section places the research in context and describes avenues for further exploration.\n\nIn addition to the above, some scientific journals such as \"Science\" will include a news section where scientific developments (often involving political issues) are described. These articles are often written by science journalists and not by scientists. In addition, some journals will include an editorial section and a section for letters to the editor. While these are articles published within a journal, in general they are not regarded as scientific journal articles because they have not been peer-reviewed.\n\nElectronic publishing is a new area of information dissemination. One definition of electronic publishing is in the context of the scientific journal. It is the presentation of scholarly scientific results in only an electronic (non-paper) form. This is from its first write-up, or creation, to its publication or dissemination. The electronic scientific journal is specifically designed to be presented on the internet. It is defined as not being previously printed material adapted, or retooled, and then delivered electronically.\n\nElectronic publishing will exist alongside paper publishing, because printed paper publishing is not expected to disappear in the future. Output to a screen is important for browsing and searching but is not well adapted for extensive reading. Paper copies of selected information will definitely be required. Therefore, the article has to be transmitted electronically to the reader's local printer. Formats suitable both for reading on paper, and for manipulation by the reader's computer will need to be integrated. Many journals are electronically available in formats readable on screen via web browsers, as well as in portable document format PDF, suitable for printing and storing on a local desktop or laptop computer. New tools such as JATS and Utopia Documents provide a 'bridge' to the 'web-versions' in that they connect the content in PDF versions directly to the WorldWideWeb via hyperlinks that are created 'on-the-fly'. The PDF version of an article is usually seen as the version of record, but the matter is subject to some debate.\n\nElectronic counterparts of established print journals already promote and deliver rapid dissemination of peer reviewed and edited, \"published\" articles. Other journals, whether spin-offs of established print journals, or created as electronic only, have come into existence promoting the rapid dissemination capability, and availability, on the Internet. In tandem with this is the speeding up of peer review, copyediting, page makeup, and other steps in the process to support rapid dissemination.\n\nOther improvements, benefits and unique values of electronically publishing the scientific journal are easy availability of supplementary materials (data, graphics and video), lower cost, and availability to more people, especially scientists from non-developed countries. Hence, research results from more developed nations are becoming more accessible to scientists from non-developed countries.\n\nMoreover, electronic publishing of scientific journals has been accomplished without compromising the standards of the refereed, peer review process.\n\nOne form is the online equivalent of the conventional paper journal. By 2006, almost all scientific journals have, while retaining their peer-review process, established electronic versions; a number have moved entirely to electronic publication. In similar manner, most academic libraries buy the electronic version, and purchase a paper copy only for the most important or most-used titles.\n\nThere is usually a delay of several months after an article is written before it is published in a journal, making paper journals not an ideal format for announcing the latest research. Many journals now publish the final papers in their electronic version as soon as they are ready, without waiting for the assembly of a complete issue, as is necessary with paper. In many fields in which even greater speed is wanted, such as physics, the role of the journal at disseminating the latest research has largely been replaced by preprint databases such as arXiv.org. Almost all such articles are eventually published in traditional journals, which still provide an important role in quality control, archiving papers, and establishing scientific credit.\n\nMany scientists and librarians have long protested the cost of journals, especially as they see these payments going to large for-profit publishing houses. To allow their researchers online access to journals, many universities purchase \"site licenses\", permitting access from anywhere in the university, and, with appropriate authorization, by university-affiliated users at home or elsewhere. These may be quite expensive, sometimes much more than the cost for a print subscription, although this may reflect the number of people who will be using the license—while a print subscription is the cost for one person to receive the journal; a site-license can allow thousands of people to gain access.\n\nPublications by scholarly societies, also known as not-for-profit-publishers, usually cost less than commercial publishers, but the prices of their scientific journals are still usually several thousand dollars a year. In general, this money is used to fund the activities of the scientific societies that run such journals, or is invested in providing further scholarly resources for scientists; thus, the money remains in and benefits the scientific sphere.\n\nDespite the transition to electronic publishing, the serials crisis persists.\n\nConcerns about cost and open access have led to the creation of free-access journals such as the Public Library of Science (PLoS) family and partly open or reduced-cost journals such as the \"Journal of High Energy Physics\". However, professional editors still have to be paid, and PLoS still relies heavily on donations from foundations to cover the majority of its operating costs; smaller journals do not often have access to such resources.\n\nBased on statistical arguments, it has been shown that electronic publishing online, and to some extent open access, both provide wider dissemination and increase the average number of citations an article receives.\n\nTraditionally, the author of an article was required to transfer the copyright to the journal publisher. Publishers claimed this was necessary in order to protect authors' rights, and to coordinate permissions for reprints or other use. However, many authors, especially those active in the open access movement, found this unsatisfactory, and have used their influence to effect a gradual move towards a license to publish instead. Under such a system, the publisher has permission to edit, print, and distribute the article commercially, but the authors retain the other rights themselves.\n\nEven if they retain the copyright to an article, most journals allow certain rights to their authors. These rights usually include the ability to reuse parts of the paper in the author's future work, and allow the author to distribute a limited number of copies. In the print format, such copies are called reprints; in the electronic format, they are called postprints. Some publishers, for example the American Physical Society, also grant the author the right to post and update the article on the author's or employer's website and on free e-print servers, to grant permission to others to use or reuse figures, and even to reprint the article as long as no fee is charged. The rise of open access journals, in which the author retains the copyright but must pay a publication charge, such as the Public Library of Science family of journals, is another recent response to copyright concerns.\n\n\n"}
{"id": "42382285", "url": "https://en.wikipedia.org/wiki?curid=42382285", "title": "Social journalism", "text": "Social journalism\n\nSocial journalism is a media model consisting of a hybrid of professional journalism, contributor and reader content. It is similar to open publishing platforms, like Twitter and WordPress.com, except that some or most content is also created and/or screened by professional journalists. Examples include Forbes.com, Medium, BuzzFeed, Soapbox and Gawker. The model, which in some instances has generated monthly audiences in the tens of millions, has been discussed as one way for professional journalism to thrive despite a marked decline in the audience for traditional journalism.\n\nWriting in Re/code, Jonathan Glick, CEO of Sulia, said the model of publishers as platforms (which he calls a \"platisher\") is \"on the rise\". Glick cites as examples Medium (from Twitter co-founders Evan Williams and Biz Stone), Vox Media, Sulia, Skift, First Look Media (backed by eBay founder Pierre Omidyar) and BuzzFeed. On March 12, 2014, Mark Little, the CEO of Storyful.com, now a division of News Corp., proposed \"10 Principles that Power Social Journalism,\" including \"UGC [User Generated Content] is governed by the same legal and ethical code as any other content\" and \"The currency of social journalism is authenticity not authority. We are not experts in every subject.\"\n\nIn an interview in \"The New York Times\", the editor of \"The Guardian\", Alan Rusbridger, said the Guardian was in the process of converting into a platform as well as a publisher. \"For years, news organizations had a quasi monopoly on information simply because we had the means of distribution. I think if as a journalist you are not intensely curious about what has been created by people who are not journalists, then you’re missing out on a lot,\" he said.\n\nOn April 1, 2014, in a column in GigaOM entitled \"Social journalism and open platforms are the new normal — now we have to make them work\" Mathew Ingram asked \"How can media entities take advantage of this phenomenon without losing their way in the process?\" and proceeded to review suggested rules for social journalism proposed by former FastCompany.com president Ed Sussman, an early adopter of the model. Ingram summarized Sussman's suggestions, including clear labeling types of contributors (e.g. staff, guest contributor, reader contribution); establishing guidelines, such as conflict of interest rules, that posters must consent to before posting; providing wiki-like tools for social improvements to content; elevating the best content with curators and algorithms; deleting weak or problematic content via curators or algorithms.\n\nSocial journalism has been attacked by media critic Michael Wolff in \"USA Today\" as the \"Forbes vanity model letting ‘contributors’ write whatever they want under your brand (‘as I wrote in Forbes …’) and not having to pay them anything — ultimately, of course, devaluing your authority.\" \n\nIn a March 20, 2014 op-ed for \"The New York Observer\", former FastCompany.com president Ed Sussman argued that social journalism does not devalue the authority of brands and that the success of Forbes.com in attracting a wide audience with its 1,000+ bloggers proved that the model could be successful for traditional media companies. Following revelations that some Forbes.com contributors used their columns to allegedly participate in a \"pump and dump\" scheme to promote, then sell stocks, Sussman followed up with \"The New Rules of Social Journalism: A Proposal\" in Pando Daily, on March 29, 2014. Sussman proposed various rules for elevating the quality and ethics of social journalism content.\n\nAn early, or perhaps the first \"social journalism\" platform at a major media company was FastCompany.com, in 2008. After the platform launched, in its first six months, FastCompany.com signed up 2,000 bloggers and 50,000 members. \"Fast Company is the first, but certainly not last, mainstream publication to integrate the majority of their site as a social community,\" wrote media analyst Jeremiah Owyang in 2008, then a senior social computing analyst for Forrester Research. After Ed Sussman left the website, the Fast Company print magazine editors reverted it to a standard journalism website.\n"}
{"id": "4937124", "url": "https://en.wikipedia.org/wiki?curid=4937124", "title": "Society for the History of Technology", "text": "Society for the History of Technology\n\nThe Society for the History of Technology, or SHOT, is the primary professional society for historians of technology. SHOT was founded in 1958 in the United States, and it has since become an international society with members \"from some thirty-five countries throughout the Americas, Europe, Asia, and Africa.\" SHOT owes its existence largely to the efforts of Professor Melvin Kranzberg (1917-1995). SHOT co-founders include John B. Rae, Carl W. Condit, Thomas P. Hughes, and Eugene S. Ferguson. SHOT's flagship publication is the journal \"Technology and Culture\", published by the Johns Hopkins University Press. Kranzberg served as editor of \"Technology and Culture\" until 1981, and was succeeded as editor by Robert C. Post until 1995, and John M. Staudenmaier from 1996 until 2010. The current editor of \"Technology and Culture\" is Suzanne Moon at the University of Oklahoma. SHOT is an affiliate of the American Council of Learned Societies and the American Historical Association and publishes a joint booklet series with the AHA, \"Historical Perspectives on Technology, Society, and Culture,\" under the co-editorship of Pamela O. Long, Robert C. Post, and Asif Azam Siddiqi. Pamela O. Long is the recipient of a MacArthur Foundation \"Genius Grant\" for 2014.\n\nThe history of technology was traditionally linked to economic history and history of science, but its interactions are now equally strong with environmental history, gender history, business history, and labor history. SHOT annually awards two book prizes, the Edelstein Prize and the Sally Hacker Prize, as well as the Kranzberg Dissertation Fellowship and the Brooke Hindle Postdoctoral Fellowship. Its highest award is the Leonardo da Vinci Medal. Recipients of the medal include Kranzberg, Ferguson, Post, Staudenmaier, Bart Hacker, and Brooke Hindle. In 1968 Kranzberg was also instrumental in the founding of a sister society, the International Committee for the History of Technology (ICOHTEC) in 1968. The two societies complement each other.\n\nThe Society for the History of Technology is dedicated to the historical study of technology and its relations with politics, economic, labor, business, the environment, public policy, science, and the arts. The society now numbers around 1500 members, and holds its meeting at a non-North-American venue every third year. SHOT also sponsors smaller conferences focused on specialized topics, often jointly with other scholarly societies and organizations.\n\n\n\n"}
{"id": "27488", "url": "https://en.wikipedia.org/wiki?curid=27488", "title": "Software documentation", "text": "Software documentation\n\nSoftware documentation is written text or illustration that accompanies computer software or is embedded in the source code. It either explains how it operates or how to use it, and may mean different things to people in different roles.\n\nDocumentation is an important part of software engineering. Types of documentation include:\n\nRequirements documentation is the description of what a particular software does or shall do. It is used throughout development to communicate how the software functions or how it is intended to operate. It is also used as an agreement or as the foundation for agreement on what the software will do. Requirements are produced and consumed by everyone involved in the production of software: end users, customers, product managers, project managers, sales, marketing, software architects, usability engineers, interaction designers, developers, and testers, to name a few. Thus, requirements documentation has many different purposes.\n\nRequirements comes in a variety of styles, notations and formality. Requirements can be goal-like (e.g., \"distributed work environment\"), close to design (e.g., \"builds can be started by right-clicking a configuration file and select the 'build' function\"), and anything in between. They can be specified as statements in natural language, as drawn figures, as detailed mathematical formulas, and as a combination of them all.\n\nThe variation and complexity of requirements documentation makes it a proven challenge. Requirements may be implicit and hard to uncover. It is difficult to know exactly how much and what kind of documentation is needed and how much can be left to the architecture and design documentation, and it is difficult to know how to document requirements considering the variety of people who shall read and use the documentation. Thus, requirements documentation is often incomplete (or non-existent). Without proper requirements documentation, software changes become more difficult — and therefore more error prone (decreased software quality) and time-consuming (expensive).\n\nThe need for requirements documentation is typically related to the complexity of the product, the impact of the product, and the life expectancy of the software. If the software is very complex or developed by many people (e.g., mobile phone software), requirements can help to better communicate what to achieve. If the software is safety-critical and can have negative impact on human life (e.g., nuclear power systems, medical equipment, mechanical equipment), more formal requirements documentation is often required. If the software is expected to live for only a month or two (e.g., very small mobile phone applications developed specifically for a certain campaign) very little requirements documentation may be needed. If the software is a first release that is later built upon, requirements documentation is very helpful when managing the change of the software and verifying that nothing has been broken in the software when it is modified.\n\nTraditionally, requirements are specified in requirements documents (e.g. using word processing applications and spreadsheet applications). To manage the increased complexity and changing nature of requirements documentation (and software documentation in general), database-centric systems and special-purpose requirements management tools are advocated.\n\nArchitecture documentation (also known as software architecture description) is a special breed of design document. In a way, architecture documents are third derivative from the code (design document being second derivative, and code documents being first). Very little in the architecture documents is specific to the code itself. These documents do not describe how to program a particular routine, or even why that particular routine exists in the form that it does, but instead merely lays out the general requirements that would motivate the existence of such a routine. A good architecture document is short on details but thick on explanation. It may suggest approaches for lower level design, but leave the actual exploration trade studies to other documents.\n\nAnother breed of design docs is the comparison document, or trade study. This would often take the form of a \"whitepaper\". It focuses on one specific aspect of the system and suggests alternate approaches. It could be at the user interface, code, design, or even architectural level. It will outline what the situation is, describe one or more alternatives, and enumerate the pros and cons of each. A good trade study document is heavy on research, expresses its idea clearly (without relying heavily on obtuse jargon to dazzle the reader), and most importantly is impartial. It should honestly and clearly explain the costs of whatever solution it offers as best. The objective of a trade study is to devise the best solution, rather than to push a particular point of view. It is perfectly acceptable to state no conclusion, or to conclude that none of the alternatives are sufficiently better than the baseline to warrant a change. It should be approached as a scientific endeavor, not as a marketing technique.\n\nA very important part of the design document in enterprise software development is the Database Design Document (DDD). It contains Conceptual, Logical, and Physical Design Elements. The DDD includes the formal information that the people who interact with the database need. The purpose of preparing it is to create a common source to be used by all players within the scene. The potential users are:\n\nWhen talking about Relational Database Systems, the document should include following parts:\n\nIt is very important to include all information that is to be used by all actors in the scene. It is also very important to update the documents as any change occurs in the database as well.\n\nIt is important for the code documents associated with the source code (which may include README files and API documentation) to be thorough, but not so verbose that it becomes overly time-consuming or difficult to maintain them. Various how-to and overview documentation guides are commonly found specific to the software application or software product being documented by API writers. This documentation may be used by developers, testers, and also the end-users using the software application. Today, a lot of high-end applications in the field of power, energy, transportation, networks, aerospace, safety, security, industry automation and a variety of other domains are seen. Technical documentation has become important within such organizations as the basic and advanced level of information may change over a period of time with architecture changes.\n\nCode documents are often organized into a \"reference guide\" style, allowing a programmer to quickly look up an arbitrary function or class.\n\nOften, tools such as Doxygen, NDoc, Visual Expert, Javadoc, EiffelStudio, Sandcastle, ROBODoc, POD, TwinText, or Universal Report can be used to auto-generate the code documents—that is, they extract the comments and software contracts, where available, from the source code and create reference manuals in such forms as text or HTML files.\n\nThe idea of auto-generating documentation is attractive to programmers for various reasons. For example, because it is extracted from the source code itself (for example, through comments), the programmer can write it while referring to the code, and use the same tools used to create the source code to make the documentation. This makes it much easier to keep the documentation up-to-date.\n\nOf course, a downside is that only programmers can edit this kind of documentation, and it depends on them to refresh the output (for example, by running a cron job to update the documents nightly). Some would characterize this as a pro rather than a con.\n\nRespected computer scientist Donald Knuth has noted that documentation can be a very difficult afterthought process and has advocated literate programming, written at the same time and location as the source code and extracted by automatic means. The programming languages Haskell and CoffeeScript have built-in support for a simple form of literate programming, but this support is not widely used.\n\nElucidative Programming is the result of practical applications of Literate Programming in real programming contexts. The Elucidative paradigm proposes that source code and documentation be stored separately.\n\nOften, software developers need to be able to create and access information that is not going to be part of the source file itself. Such annotations are usually part of several software development activities, such as code walks and porting, where third party source code is analysed in a functional way. Annotations can therefore help the developer during any stage of software development where a formal documentation system would hinder progress.\n\nUnlike code documents, user documents simply describe how a program is used.\n\nIn the case of a software library, the code documents and user documents could in some cases be effectively equivalent and worth conjoining, but for a general application this is not often true.\n\nTypically, the user documentation describes each feature of the program, and assists the user in realizing these features. A good user document can also go so far as to provide thorough troubleshooting assistance. It is very important for user documents to not be confusing, and for them to be up to date. User documents don't need to be organized in any particular way, but it is very important for them to have a thorough index. Consistency and simplicity are also very valuable. User documentation is considered to constitute a contract specifying what the software will do. API Writers are very well accomplished towards writing good user documents as they would be well aware of the software architecture and programming techniques used. See also technical writing.\n\nUser documentation can be produced in a variety of online and print formats. However, there are three broad ways in which user documentation can be organized. \n\nA common complaint among users regarding software documentation is that only one of these three approaches was taken to the near-exclusion of the other two. It is common to limit provided software documentation for personal computers to online help that give only reference information on commands or menu items. The job of tutoring new users or helping more experienced users get the most out of a program is left to private publishers, who are often given significant assistance by the software developer.\n\nLike other forms of technical documentation, good user documentation benefits from an organized process of development. In the case of user documentation, the process as it commonly occurs in industry consists of five steps:\n\n1. User analysis, the basic research phase of the process.<br>\n2. Planning, or the actual documentation phase.<br>\n3. Draft review, a self-explanatory phase where feedback is sought on the draft composed in the previous step.<br>\n4. Usability testing, whereby the usability of the document is tested empirically.<br>\n5. Editing, the final step in which the information collected in steps three and four is used to produce the final draft.\n\n\"The resistance to documentation among developers is well known and needs no emphasis.\" \nThis situation is particularly prevalent in agile software development because these methodologies try to avoid any unnecessary activities that do not directly bring value.\nSpecifically, the Agile Manifesto advocates valuing \"working software over comprehensive documentation\", which could be interpreted cynically as \"We want to spend all our time coding. Remember, real programmers don't write documentation.\"\n\nA survey among software engineering experts revealed, however, that documentation is by no means considered unnecessary in agile development.\nYet it is acknowledged that there are motivational problems in development, and that documentation methods tailored to agile development (e.g. through Reputation systems and Gamification) may be needed.\n\nFor many applications it is necessary to have some promotional materials to encourage casual observers to spend more time learning about the product. This form of documentation has three purposes:-\n\n\n\n"}
{"id": "44657216", "url": "https://en.wikipedia.org/wiki?curid=44657216", "title": "System for Cross-domain Identity Management", "text": "System for Cross-domain Identity Management\n\nSystem for Cross-domain Identity Management (SCIM) is a standard for automating the exchange of user identity information between identity domains, or IT systems.\n\nFor example, as a company hires and fires employees, they are added and removed from the company's electronic employee directory. SCIM could be used to automatically add/delete (or, provision/de-provision) accounts for those users in external systems such as Google Apps for Work, Office 365, or Salesforce.com. Then, a new user account would exist in the external systems for each new employee, and the user accounts for former employees might no longer exist in those systems.\n\nIn addition to simple user-record management (creating & deleting), SCIM can also be used to share information about user attributes, attribute schema, and group membership. Attributes could range from user contact information to group membership. Group membership or other attribute values are generally used to manage user permissions. Attribute values and group assignments can change, adding to the challenge of maintaining the relevant data across multiple identity domains.\n\nThe SCIM standard has grown in popularity and importance, as organizations use more SaaS tools. A large organization can have hundreds or thousands of hosted applications (internal and external) and related servers, databases and file shares that require user provisioning. Without a standard connection method, companies must write custom software connectors to join these systems and their IdM system.\n\nSCIM uses a standardised API through REST with data formatted in JSON or XML.\n\nThe first version, SCIM 1.0, was released in 2011 by a SCIM standard working group organized under the Open Web Foundation. In 2011, it was transferred to the IETF, and the current standard, SCIM 2.0 was released as IETF RFC in 2015.\n\nSCIM 2.0 was completed in September 2015 and is published as IETF RFCs 7643 and 7644. A use-case document is also available as RFC7642.\n\nThe standard has been implemented in various IdM software.\n\nThe standard was initially called Simple Cloud Identity Management (as it is still called this in some places), but the name was officially changed to \"System for Cross-domain Identity Management (SCIM)\" when the IETF adopted it.\n\nInteroperability was demonstrated in October, 2011, at the Cloud Identity Summit, an IAM industry conference. There, user accounts were provisioned and de-provisioned across separate systems using SCIM standards, by a collection of IdM software vendors: Okta, Ping Identity, SailPoint, Technology Nexus and UnboundID. In March 2012, at IETF 83 in Paris, Interoperability tests continued by the same vendors, joined by Salesforce.com, BCPSoft, WSO2, Gluu, and Courion (nine companies, total).\n\nSCIM is not the first standard for exchanging user data, but it builds on prior standards (e.g. SPML, PortableContacts, vCards, and LDAP directory services) in an attempt to be a simpler and more widely adopted solution for cloud services providers.\n\nGoogle, Salesforce.com, Cisco, SailPoint and Ping Identity have been considered as the initial driving forces to create this open standard.\n\n"}
{"id": "1544179", "url": "https://en.wikipedia.org/wiki?curid=1544179", "title": "Technogaianism", "text": "Technogaianism\n\nTechnogaianism (a portmanteau word combining \"techno-\" for technology and \"gaian\" for Gaia philosophy) is a bright green environmentalist stance of active support for the research, development and use of emerging and future technologies to help restore Earth's environment. Technogaians argue that developing safe, clean, alternative technology should be an important goal of environmentalists.\n\nThis point of view is different from the default position of radical environmentalists and a common opinion that all technology necessarily degrades the environment, and that environmental restoration can therefore occur only with reduced reliance on technology. Technogaians argue that technology gets cleaner and more efficient with time. They would also point to such things as hydrogen fuel cells to demonstrate that developments do not have to come at the environment's expense. More directly, they argue that such things as nanotechnology and biotechnology can directly reverse environmental degradation. Molecular nanotechnology, for example, could convert garbage in landfills into useful materials and products, while biotechnology could lead to novel microbes that devour hazardous waste.\n\nWhile many environmentalists still contend that \"most\" technology is detrimental to the environment, technogaians point out that it has been in humanity's best interests to exploit the environment mercilessly until fairly recently. This sort of behaviour follows accurately to current understandings of evolutionary systems, in that when new factors (such as foreign species or mutant subspecies) are introduced into an ecosystem, they tend to maximise their own resource consumption until either, \"a)\" they reach an equilibrium beyond which they cannot continue unmitigated growth, or \"b)\" they become extinct. In these models, it is \"completely impossible\" for such a factor to totally destroy its host environment, though they may precipitate major ecological transformation before their ultimate eradication. Technogaians believe humanity has currently reached just such a threshold, and that the only way for human civilization to continue advancing is to accept the tenets of technogaianism and limit future exploitive exhaustion of natural resources and minimize further unsustainable development or face the widespread, ongoing mass extinction of species. The destructive effects of modern civilization can be mitigated by technological solutions, such as using nuclear power. Furthermore, technogaians argue that only science and technology can help humanity be aware of, and possibly develop counter-measures for, risks to civilization, humans and planet Earth such as a possible impact event. \n\nSociologist James Hughes mentions Walter Truett Anderson, author of \"To Govern Evolution: Further Adventures of the Political Animal\", as an example of a technogaian political philosopher; argues that technogaianism applied to environmental management is found in the reconciliation ecology writings such as Michael Rosenzweig's \"Win-Win Ecology: How The Earth's Species Can Survive In The Midst of Human Enterprise\"; and considers Bruce Sterling's Viridian design movement to be an exemplary technogaian initiative.\n\nThe theories of English writer Fraser Clark may be broadly categorised as technogaian. Clark advocated \"balancing the hippie right brain with the techno left brain\". The idea of combining technology and ecology were extrapolated at length by a South African eco-anarchist project in the 1990s. The Kagenna Magazine project aimed to combine technology, art and ecology in an emerging movement that could restore the balance between human and nature.\n\nGeorge Dvorsky suggests the sentiment of technogaianism is to heal the Earth, use sustainable technology, and create ecologically diverse environments. Dvorsky argues that defensive counter measures could be designed to counter the harmful effects of asteroid impacts, earthquakes, and volcanic eruptions. Dvorksky also suggest that genetic engineering could be used to reduce the environmental impact humans have on the earth.\n\nTechnology facilities the sampling, testing and monitoring of various environments and ecosystems. NASA uses space-based observations to conduct research on solar activity, sea level rise, the temperature of the atmosphere and the oceans, the state of the ozone layer, air pollution, and changes in sea ice and land ice.\n\nClimate engineering is a technogaian method that uses two categories of technologies- carbon dioxide removal and solar radiation management. Carbon dioxide removal addresses a cause of climate change by removing one of the greenhouse gases from the atmosphere. Solar radiation management attempts to offset effects of greenhouse gases by causing the Earth to absorb less solar radiation.\n\nEarthquake engineering is a technogaian method concerned with protecting society and the natural and man-made environment from earthquakes by limiting the seismic risk to acceptable levels.\nAnother example of a technogaian practice is an artificial closed ecological system used to test if and how people could live and work in a closed biosphere, while carrying out scientific experiments. It is in some cases used to explore the possible use of closed biospheres in space colonization, and also allows the study and manipulation of a biosphere without harming Earth's. The most advanced technogaian proposal is the \"terraforming\" of a planet, moon, or other body by deliberately modifying its atmosphere, temperature, or ecology to be similar to those of Earth in order to make it habitable by humans.\n\nS. Matthew Liao, professor of philosophy and bioethics at New York University, claims that the human impact on the environment could be reduced by genetically engineering humans to have, a smaller stature, an intolerance to eating meat, and an increased ability to see in the dark, thereby using less lighting. Liao argues that human engineering is less risky than geoengineering.\n\nGenetically modified foods have reduced the amount of herbicide and insecticide needed for cultivation. The development of glyphosate-resistant (Roundup Ready) plants has changed the herbicide use profile away from more environmentally persistent herbicides with higher toxicity, such as atrazine, metribuzin and alachlor, and reduced the volume and danger of herbicide runoff.\n\nAn environmental benefit of Bt-cotton and maize is reduced use of chemical insecticides. A PG Economics study concluded that global pesticide use was reduced by 286,000 tons in 2006, decreasing the environmental impact of herbicides and pesticides by 15%. A survey of small Indian farms between 2002 and 2008 concluded that Bt cotton adoption had led to higher yields and lower pesticide use. Another study concluded insecticide use on cotton and corn during the years 1996 to 2005 fell by of active ingredient, which is roughly equal to the annual amount applied in the EU. A Bt cotton study in six northern Chinese provinces from 1990 to 2010 concluded that it halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders and extended environmental benefits to neighbouring crops of maize, peanuts and soybeans.\n\n"}
{"id": "58306941", "url": "https://en.wikipedia.org/wiki?curid=58306941", "title": "Technology readiness level", "text": "Technology readiness level\n\nTechnology readiness levels (TRL) are a method of estimating technology maturity of Critical Technology Elements (CTE) of a program during the acquisition process. They are determined during a Technology Readiness Assessment (TRA) that examines program concepts, technology requirements, and demonstrated technology capabilities. TRL are based on a scale from 1 to 9 with 9 being the most mature technology. The use of TRLs enables consistent, uniform discussions of technical maturity across different types of technology. TRL has been in widespread use at NASA since the 1980s where it was originally invented. In 1999 the US Department of Defense was advised by GAO to use the scale for procurement which it did from the early 2000s. By 2008 the scale was also in use at the European Space Agency (ESA) as it is evidenced by their handbook. The European Commission advised EU-funded research and innovation projects to adopt the scale in 2010 which they did from 2014 in its Horizon 2020 program. In 2013 TRL was further canonized by the ISO 16290:2013 standard. A comprehensive approach and discussion about TRLs has been published by the European Association of Research and Technology Organisations (EARTO). Extensive criticism of the adoption of TRL scale by the European Union was published in The Innovation Journal, in that \"concreteness and sophistication of the TRL scale gradually diminished as its usage spread outside its original context (space programs)\".\n\nTechnology Readiness Levels were originally conceived at NASA in 1974 and formally defined in 1989. The original definition included seven levels, but in the 1990s NASA adopted the current nine-level scale that subsequently gained widespread acceptance.\n\nOriginal NASA TRL Definitions (1989)\n\nThe TRL methodology was originated by Stan Sadin at NASA Headquarters in 1974. At that time, Ray Chase was the JPL Propulsion Division representative on the Jupiter Orbiter design team. At the suggestion of Stan Sadin, Mr Chase used this methodology to assess the technology readiness of the proposed JPL Jupiter Orbiter spacecraft design. Later Mr Chase spent a year at NASA Headquarters helping Mr Sadin institutionalize the TRL methodology. Mr Chase joined ANSER in 1978, where he used the TRL methodology to evaluate the technology readiness of proposed Air Force development programs. He published several articles during the 1980s and 90s on reusable launch vehicles utilizing the TRL methodology. These documented an expanded version of the methodology that included design tools, test facilities, and manufacturing readiness on the Air Force Have Not program. The Have Not program manager, Greg Jenkins, and Ray Chase published the expanded version of the TRL methodology, which included design and manufacturing. Leon McKinney and Mr Chase used the expanded version to assess the technology readiness of the ANSER team's Highly Reusable Space Transportation (\"HRST\") concept. ANSER also created an adapted version of the TRL methodology for proposed Homeland Security Agency programs.\n\nThe United States Air Force adopted the use of Technology Readiness Levels in the 1990s.\n\nIn 1995, John C. Mankins, NASA, wrote a paper that discussed NASA's use of TRLs and proposed expanded descriptions for each TRL. In 1999, the United States General Accounting Office produced an influential report that examined the differences in technology transition between the DOD and private industry. It concluded that the DOD takes greater risks and attempts to transition emerging technologies at lesser degrees of maturity than does private industry. The GAO concluded that use of immature technology increased overall program risk. The GAO recommended that the DOD make wider use of Technology Readiness Levels as a means of assessing technology maturity prior to transition. In 2001, the Deputy Under Secretary of Defense for Science and Technology issued a memorandum that endorsed use of TRLs in new major programs. Guidance for assessing technology maturity was incorporated into the \"Defense Acquisition Guidebook\". Subsequently, the DOD developed detailed guidance for using TRLs in the 2003 DOD Technology Readiness Assessment Deskbook.\n\nThe European Space Agency adopted the TRL scale in the mid-2000s. Its handbook closely follows the NASA definition of TRLs. The universal usage of TRL in EU policy was proposed in the final report of the first High Level Expert Group on Key Enabling Technologies, and it was indeed implemented in the subsequent EU framework program, called H2020, running from 2013 to 2020. This means not only space and weapons programs, but everything from nanotechnology to informatics and communication technology.\n\nA \"Technology Readiness Level Calculator\" was developed by the United States Air Force. This tool is a standard set of questions implemented in Microsoft Excel that produces a graphical display of the TRLs achieved. This tool is intended to provide a snapshot of technology maturity at a given point in time.\n\nThe \"Technology Program Management Model\" was developed by the United States Army. The TPMM is a TRL-gated high-fidelity activity model that provides a flexible management tool to assist Technology Managers in planning, managing, and assessing their technologies for successful technology transition. The model provides a core set of activities including systems engineering and program management tasks that are tailored to the technology development and management goals. This approach is comprehensive, yet it consolidates the complex activities that are relevant to the development and transition of a specific technology program into one integrated model.\n\nThe primary purpose of using technology readiness levels is to help management in making decisions concerning the development and transitioning of technology. It should be viewed as one of several tools that are needed to manage the progress of research and development activity within an organization.\n\nAmong the advantages of TRLs:\n\n\nSome of the characteristics of TRLs that limit their utility:\n\n\nCurrent TRL models tend to disregard negative and obsolescence factors. There have been suggestions made for incorporating such factors into assessments.\n\nFor complex technologies that incorporate various development stages, a more detailed scheme called the Technology Readiness Pathway Matrix has been developed going from basic units to applications in society. This tool aims to show that a readiness level of a technology is based on a less linear process but on a more complex pathway through its application in society.. \n\n\n\n"}
{"id": "58775", "url": "https://en.wikipedia.org/wiki?curid=58775", "title": "Timeline of electromagnetism and classical optics", "text": "Timeline of electromagnetism and classical optics\n\nTimeline of electromagnetism and classical optics lists, within the history of electromagnetism, the associated theories, technology, and events.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "52780566", "url": "https://en.wikipedia.org/wiki?curid=52780566", "title": "Tuniu", "text": "Tuniu\n\nTuniu Corporation () is a Chinese online travel agency. Products and services include packaged tours, accommodation reservation, airline and railway ticketing, car rentals, and corporate travel. The company listed on the Nasdaq Stock Exchange on May 9, 2014. The company headquarters are located in Nanjing with offices in Shanghai and Beijing.\n\nFounded in 2006 in Nanjing by current CEO Donald Dunde Yu and current COO Alex Haifeng Yan, the company was fully incorporated on June 1, 2008.\n\nOn May 9, 2014, Tuniu was listed on the Nasdaq Stock Exchange under TOUR, co-managed by Morgan Stanley & Co, Credit Suisse Securities LLC and China Renaissance Securities. Tuniu raised $72 million in its initial public offering, pricing 8 million shares at $9 per share. CEO Donald Dunde Yu rang the opening bell at the Nasdaq MarketSite in Times Square.\n\nIn April 2015, Tuniu was the subject of a boycott by seventeen Chinese travel agencies over a pricing dispute. The issue was settled a few days later following an investigation by the China National Tourism Administration, with partner relations returning to normal. Tuniu's share price fell 4.7% following news of the dispute.\n\nOn August 23, 2016, Tuniu’s Board of Directors authorized a share repurchase program to repurchase up to $150 million worth of shares. Tuniu’s share price had fallen below opening price.\n\nOn July 1, 2014, Ctrip CEO James Jianzhang Liang was appointed to Tuniu’s Board of Directors. On December 10, 2014, Tuniu and Ctrip signed a strategic collaboration agreement to share travel resources.\n\nOn December 15, 2014, Tuniu announced $148 million investment in aggregate from a group that included the investment arms of Hony Capital, JD.com, Ctrip Investment Holding Ltd, and the personal holding companies of Tuniu’s CEO and COO. Ctrip acquired $15 million Tuniu shares during their IPO, and currently owns over 3% of Tuniu’s outstanding shares.\n\nIn May, 2015, Tuniu announced the investment of $500 million from a group of investors led by JD.com. JD.com became the largest shareholder in Tuniu with 27.5% stake.\n\nOn March 9, 2015, Tuniu announced the acquisition of majority stakes in two Chinese travel agencies, Hangzhou-based Zhejiang Zhongshan International Services and Tianjin-based China Classical Holiday.\n\nOn January 21, 2016, Tuniu announced the completion of a US$500 million investment from HNA Tourism Group. Transaction purchase price was US$5.50 per Class A ordinary share. HNA Tourism Group bought 24.1% share of Tuniu.\n\nIn July 2016, Tuniu announced the signing of Taiwanese pop stars Jay Chou and Jimmy Lin as its celebrity brand ambassadors.\n\n"}
{"id": "58735847", "url": "https://en.wikipedia.org/wiki?curid=58735847", "title": "U-Report", "text": "U-Report\n\nU-Report is a free SMS social monitoring tool and real-time information system for community participation, designed to strengthen community-led development, citizen engagement, and positive change. SMS polls and alerts are sent out to U-reporters and real-time response information is collected. Results and ideas are shared back with the community. Issues polled include among others health, education, water, sanitation and hygiene, youth unemployment, HIV/ AIDS, disease outbreaks; social welfare sectors. The initiative is currently operational in 41 countries and covers more than 3 million people.\n\nUganda became the first country in which UNICEF launched the U-Report mobile initiative in May 2011. The country's population is one of the youngest in the world with over 55% of the population younger than 18 years and 78% - up to 30 years. Also there is a high level of using phones - 48%.\n\nThe success of U-Report in Uganda has prompted the launch of similar initiatives in Zambia in December 2012 and in Nigeria in June 2014.\n\nIn Zambia, U-report was used to accelerate HIV prevention among adolescents and young people. As a result of the U-Report, a significant increase in voluntary HIV testing was recorded - 40% compared to an average of 24% before.\n\nIn Nigeria, U-Report concentrates surveys on social and medical issues. The number of reporters in Nigeria has increased dramatically in subsequent years and Nigeria has become the first country out of 1 million reporters.\n\nIn July 2015, 15 countries initiated a project with a total number of more than a million reporters.\n\nUkraine became the first in Europe and the world's 18th largest U-Report country. The presentation took place on April 23, 2016 in Kyiv with the participation of 1,500 people and the well-known Ukrainian music band Pianoboy. During the year of activity in December 2016, there were over 25,000 u-reporters in Ukraine with 53% younger than 19. As for September 30 2018, there are 68,273 u-reporters in Ukraine.\n\nThe last country that joined the project was Malawi, March 28, 2018.\n\nUNICEF Innovation has been working with SMS systems since 2007, when an open source platform called RapidSMS was developed. RapidPro is an open-source platform which allows the implementation of an SMS application without the need of a programmer. It provides real-time information and data analytics. It was developed by the UNICEF global Innovation Centre in collaboration with Nyuruka, a Rwandan software development firm. An additional advantage is the ability to create fields required for a specific context, in each country, different fields for the presentation of identical data.\n\nTwo categories of information were identified based on data available from a sample of 6 countries (Mali, Burundi, Cameroon, Zimbabwe, Sierra Leone and Central African Republic): identification (age, gender, occupation) and location ( different administrative levels) and a set of best practices for naming variables.\n\nTelegram was chosen first in early 2016 because it was one of the first messaging applications to provide an open API that allowed organizations to develop and integrate chatbots with other platforms. Telegram, however, had low penetration in the countries where U-Report operates, so it was never used at scale. Later in 2016, U-Report tested the Facebook Messenger API for delivering U-Report surveys. By late 2017, the integration was still at an early stage, but the U-Report team viewed its partnership with Facebook as a critical investment that would result in cost savings to U-Report and U-Reporters because, unlike SMS, there is no per-message cost to use Facebook Messenger. \n\nIn December 2015, U-Report Indonesia was launched on Twitter. Data received can be disaggregated by age, gender, States, LGAs, Wards and settlements in real time.\n\nUNICEF examines the results of surveys and collaborates with organizations and government agencies to apply the results in practice: in programs, laws, decisions.\n\nOne example of U-Report in actionis in Liberia on sexual abuse in schools. To assess the magnitude of the situation, U-Report asked 13,000 users in Liberia whether teachers at their schools were exchanging grades for sex. An astonishing 86% of those polled said yes. Following U-Report’s discovery of a “Sex 4 Grades” epidemic, hotlines across the country were inundated by victims who now felt safe enough to reach out for support. Liberia’s Minister of Education and UNICEF are now collaborating on a plan to address the issue. \n\nUNICEF Pakistan has since launched an MHM innovation challenge to encourage U-Reporters to come up with their own solutions to break the taboo. U-Reporters responded. 60 submitted valid proposals of which 7 were selected as grant winners of up to PKR 250,000 to develop their ideas.  Winning ideas include a sanitary napkin vending machine, an artificial intelligence tool and an app that will be developed by young U-Reporters in partnership with Islamabad Engineering School.\n\nThe project has two categories of users involved in the activities: u-reporters and u-ambassadors. U-reporters - people who are interviewed and the U-ambassadors are the young people who distribute the idea of the project, organize the work with U-Reporters.\n\nDavid Beckham, UNICEF Goodwill Ambassador, supports the initiative. In September 2015, Beckham spoke with UN Secretary-General Ban Ki-moon and Anthony Lake, executive director of UNICEF, in the building of the General Assembly of the United Nations before the recently launched Youth Assembly, installed by Google, which displays messages from children from around the world.\n\nTaras Topola, the front of the group ANTITALY, is the official representative of U-Report in Ukraine. He constantly speaks at events, gives interviews. In 2017, Taras Topola made a tour with Ukraine in order to attract young people to U-report. \n\nGoodwill Ambassador to UNICEF, Hollywood actor Orlando Bloom supports the project at the global level. In 2017, he called on young people to join the project through video talk. And in 2018, in the framework of UNICEF, he came to Ukraine by visiting several kindergartens and schools in Slavyansk.\n\n\n"}
{"id": "55955", "url": "https://en.wikipedia.org/wiki?curid=55955", "title": "Version control", "text": "Version control\n\nA component of software configuration management, version control, also known as revision control or source control, is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the \"revision number\", \"revision level\", or simply \"revision\". For example, an initial set of files is \"revision 1\". When the first change is made, the resulting set is \"revision 2\", and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged.\n\nThe need for a logical way to organize and control revisions has existed for almost as long as writing has existed, but revision control became much more important, and complicated when the era of computing began. The numbering of book editions and of specification revisions are examples that date back to the print-only era. Today, the most capable (as well as complex) revision control systems are those used in software development, where a team of people may concurrently make changes to the same files.\n\nVersion control systems (VCS) most commonly run as stand-alone applications, but revision control is also embedded in various types of software such as word processors and spreadsheets, collaborative web docs and in various content management systems, e.g., Wikipedia's . Revision control allows for the ability to revert a document to a previous revision, which is critical for allowing editors to track each other's edits, correct mistakes, and defend against vandalism and spamming in wikis.\n\nIn computer software engineering, revision control is any kind of practice that tracks and provides control over changes to source code. Software developers sometimes use revision control software to maintain documentation and configuration files as well as source code.\n\nAs teams design, develop and deploy software, it is common for multiple versions of the same software to be deployed in different sites and for the software's developers to be working simultaneously on updates. Bugs or features of the software are often only present in certain versions (because of the fixing of some problems and the introduction of others as the program develops). Therefore, for the purposes of locating and fixing bugs, it is vitally important to be able to retrieve and run different versions of the software to determine in which version(s) the problem occurs. It may also be necessary to develop two versions of the software concurrently: for instance, where one version has bugs fixed, but no new features (branch), while the other version is where new features are worked on (trunk).\n\nAt the simplest level, developers could simply retain multiple copies of the different versions of the program, and label them appropriately. This simple approach has been used in many large software projects. While this method can work, it is inefficient as many near-identical copies of the program have to be maintained. This requires a lot of self-discipline on the part of developers and often leads to mistakes. Since the code base is the same, it also requires granting read-write-execute permission to a set of developers, and this adds the pressure of someone managing permissions so that the code base is not compromised, which adds more complexity. Consequently, systems to automate some or all of the revision control process have been developed. This ensures that the majority of management of version control steps is hidden behind the scenes.\n\nMoreover, in software development, legal and business practice and other environments, it has become increasingly common for a single document or snippet of code to be edited by a team, the members of which may be geographically dispersed and may pursue different and even contrary interests. Sophisticated revision control that tracks and accounts for ownership of changes to documents and code may be extremely helpful or even indispensable in such situations.\n\nRevision control may also track changes to configuration files, such as those typically stored in codice_1 or codice_2 on Unix systems. This gives system administrators another way to easily track changes made and a way to roll back to earlier versions should the need arise.\n\nRevision control manages changes to a set of data over time. These changes can be structured in various ways.\n\nOften the data is thought of as a collection of many individual items, such as files or documents, and changes to individual files are tracked. This accords with intuitions about separate files but causes problems when identity changes, such as during renaming, splitting or merging of files. Accordingly, some systems, such as git, instead consider changes to the data as a whole, which is less intuitive for simple changes but simplifies more complex changes.\n\nWhen data that is under revision control is modified, after being retrieved by \"checking out,\" this is not in general immediately reflected in the revision control system (in the \"repository\"), but must instead be \"checked in\" or \"committed.\" A copy outside revision control is known as a \"working copy\". As a simple example, when editing a computer file, the data stored in memory by the editing program is the working copy, which is committed by saving. Concretely, one may print out a document, edit it by hand, and only later manually input the changes into a computer and save it. For source code control, the working copy is instead a copy of all files in a particular revision, generally stored locally on the developer's computer; in this case saving the file only changes the working copy, and checking into the repository is a separate step.\n\nIf multiple people are working on a single data set or document, they are implicitly creating branches of the data (in their working copies), and thus issues of merging arise, as discussed below. For simple collaborative document editing, this can be prevented by using file locking or simply avoiding working on the same document that someone else is working on.\n\nRevision control systems are often centralized, with a single authoritative data store, the \"repository,\" and check-outs and check-ins done with reference to this central repository. Alternatively, in distributed revision control, no single repository is authoritative, and data can be checked out and checked into any repository. When checking into a different repository, this is interpreted as a merge or patch.\n\nIn terms of graph theory, revisions are generally thought of as a line of development (the \"trunk\") with branches off of this, forming a directed tree, visualized as one or more parallel lines of development (the \"mainlines\" of the branches) branching off a trunk. In reality the structure is more complicated, forming a directed acyclic graph, but for many purposes \"tree with merges\" is an adequate approximation.\n\nRevisions occur in sequence over time, and thus can be arranged in order, either by revision number or timestamp. Revisions are based on past revisions, though it is possible to largely or completely replace an earlier revision, such as \"delete all existing text, insert new text\". In the simplest case, with no branching or undoing, each revision is based on its immediate predecessor alone, and they form a simple line, with a single latest version, the \"HEAD\" revision or \"tip\". In graph theory terms, drawing each revision as a point and each \"derived revision\" relationship as an arrow (conventionally pointing from older to newer, in the same direction as time), this is a linear graph. If there is branching, so multiple future revisions are based on a past revision, or undoing, so a revision can depend on a revision older than its immediate predecessor, then the resulting graph is instead a directed tree (each node can have more than one child), and has multiple tips, corresponding to the revisions without children (\"latest revision on each branch\"). In principle the resulting tree need not have a preferred tip (\"main\" latest revision) – just various different revisions – but in practice one tip is generally identified as HEAD. When a new revision is based on HEAD, it is either identified as the new HEAD, or considered a new branch. The list of revisions from the start to HEAD (in graph theory terms, the unique path in the tree, which forms a linear graph as before) is the \"trunk\" or \"mainline.\" Conversely, when a revision can be based on more than one previous revision (when a node can have more than one \"parent\"), the resulting process is called a \"merge,\" and is one of the most complex aspects of revision control. This most often occurs when changes occur in multiple branches (most often two, but more are possible), which are then merged into a single branch incorporating both changes. If these changes overlap, it may be difficult or impossible to merge, and require manual intervention or rewriting.\n\nIn the presence of merges, the resulting graph is no longer a tree, as nodes can have multiple parents, but is instead a rooted directed acyclic graph (DAG). The graph is acyclic since parents are always backwards in time, and rooted because there is an oldest version. However, assuming that there is a trunk, merges from branches can be considered as \"external\" to the tree – the changes in the branch are packaged up as a \"patch,\" which is applied to HEAD (of the trunk), creating a new revision without any explicit reference to the branch, and preserving the tree structure. Thus, while the actual relations between versions form a DAG, this can be considered a tree plus merges, and the trunk itself is a line.\n\nIn distributed revision control, in the presence of multiple repositories these may be based on a single original version (a root of the tree), but there need not be an original root, and thus only a separate root (oldest revision) for each repository, for example, if two people starting working on a project separately. Similarly in the presence of multiple data sets (multiple projects) that exchange data or merge, there isn’t a single root, though for simplicity one may think of one project as primary and the other as secondary, merged into the first with or without its own revision history.\n\nEngineering revision control developed from formalized processes based on tracking revisions of early blueprints or bluelines. This system of control implicitly allowed returning to an earlier state of the design, for cases in which an engineering dead-end was reached in the development of the design. A revision table was used to keep track of the changes made. Additionally, the modified areas of the drawing were highlighted using revision clouds.\n\nVersion control is widespread in business and law. Indeed, \"contract redline\" and \"legal blackline\" are some of the earliest forms of revision control, and are still employed in business and law with varying degrees of sophistication. The most sophisticated techniques are beginning to be used for the electronic tracking of changes to CAD files (see product data management), supplanting the \"manual\" electronic implementation of traditional revision control.\n\nTraditional revision control systems use a centralized model where all the revision control functions take place on a shared server. If two developers try to change the same file at the same time, without some method of managing access the developers may end up overwriting each other's work. Centralized revision control systems solve this problem in one of two different \"source management models\": file locking and version merging.\n\nAn operation is \"atomic\" if the system is left in a consistent state even if the operation is interrupted. The \"commit\" operation is usually the most critical in this sense. Commits tell the revision control system to make a group of changes final, and available to all users. Not all revision control systems have atomic commits; notably, CVS lacks this feature.\n\nThe simplest method of preventing \"concurrent access\" problems involves locking files so that only one developer at a time has write access to the central \"repository\" copies of those files. Once one developer \"checks out\" a file, others can read that file, but no one else may change that file until that developer \"checks in\" the updated version (or cancels the checkout).\n\nFile locking has both merits and drawbacks. It can provide some protection against difficult merge conflicts when a user is making radical changes to many sections of a large file (or group of files). However, if the files are left exclusively locked for too long, other developers may be tempted to bypass the revision control software and change the files locally, leading to more serious problems.\n\nMost version control systems allow multiple developers to edit the same file at the same time. The first developer to \"check in\" changes to the central repository always succeeds. The system may provide facilities to merge further changes into the central repository, and preserve the changes from the first developer when other developers check in.\n\nMerging two files can be a very delicate operation, and usually possible only if the data structure is simple, as in text files. The result of a merge of two image files might not result in an image file at all. The second developer checking in the code will need to take care with the merge, to make sure that the changes are compatible and that the merge operation does not introduce its own logic errors within the files. These problems limit the availability of automatic or semi-automatic merge operations mainly to simple text-based documents, unless a specific merge plugin is available for the file types.\n\nThe concept of a \"reserved edit\" can provide an optional means to explicitly lock a file for exclusive write access, even when a merging capability exists.\n\nMost revision control tools will use only one of these similar terms (baseline, label, tag) to refer to the action of identifying a snapshot (\"label the project\") or the record of the snapshot (\"try it with baseline \"X\"\"). Typically only one of the terms \"baseline\", \"label\", or \"tag\" is used in documentation or discussion; they can be considered synonyms.\n\nIn most projects, some snapshots are more significant than others, such as those used to indicate published releases, branches, or milestones.\n\nWhen both the term \"baseline\" and either of \"label\" or \"tag\" are used together in the same context, \"label\" and \"tag\" usually refer to the mechanism within the tool of identifying or making the record of the snapshot, and \"baseline\" indicates the increased significance of any given label or tag.\n\nMost formal discussion of configuration management uses the term \"baseline\".\n\nDistributed revision control systems (DRCS) take a peer-to-peer approach, as opposed to the client-server approach of centralized systems. Rather than a single, central repository on which clients synchronize, each peer's working copy of the codebase is a bona-fide repository.\nDistributed revision control conducts synchronization by exchanging patches (change-sets) from peer to peer. This results in some important differences from a centralized system:\nRather, communication is only necessary when pushing or pulling changes to or from other peers.\n\nSome of the more advanced revision-control tools offer many other facilities, allowing deeper integration with other tools and software-engineering processes. Plugins are often available for IDEs such as Oracle JDeveloper, IntelliJ IDEA, Eclipse and Visual Studio. Delphi, NetBeans IDE, Xcode and GNU Emacs (via vc.el) come with integrated version control support.\n\nTerminology can vary from system to system, but some terms in common usage include:\n\n\n\n"}
{"id": "57534427", "url": "https://en.wikipedia.org/wiki?curid=57534427", "title": "XP-PEN", "text": "XP-PEN\n\nXP-Pen is founded in Japan in 2005, it specializes in graphics tablets, pen display monitors, light pads, stylus pens and digital graphical products. In 2008 they established an office in Taiwan. In 2013, XP-Pen Technology Co. was founded in the United States. In 2015 they opened their office in Shenzhen, China. \n\nIn December 2017, they were invited to DreamWorks campus in Glendale California. in October 2017, they exhibited in Stan Lee Comic Con during the Halloween weekend. In July 2017, they took part in Los Angeles' 25th Anime Expo.\n\nArtist series display\nXP-Pen supplies drivers for Windows 7, 8, 10 and Mac 10.8 and above.\n"}
