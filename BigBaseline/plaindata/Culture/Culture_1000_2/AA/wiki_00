{"id": "24145723", "url": "https://en.wikipedia.org/wiki?curid=24145723", "title": "2010 Arab Capital of Culture", "text": "2010 Arab Capital of Culture\n\nThe 2010 Arab Capital of Culture was chosen to be Doha, Qatar. The Arab Capital of Culture is an initiative undertaken by UNESCO, under the \"Cultural Capitals Program\" to promote and celebrate Arab culture and encourage cooperation in the Arab region. The Qatari government has begun preparations in many different fields including a US $82 million Cultural Village.\n\n\n"}
{"id": "24343762", "url": "https://en.wikipedia.org/wiki?curid=24343762", "title": "386 Generation", "text": "386 Generation\n\nThe 386 Generation (Korean: 386 세대; ---世代; \"sampallyuk sedae\") is the generation of South Koreans born in the 1960s who were very active politically as young adults, and instrumental in the democracy movement of the 1980s. \nThe term was coined in the early 1990s, in reference to what was then the latest computer model, Intel's 386, and referring to people then in their 30s, having attended university in the 1980s, and born in the 1960s. As the time flows, the people in 386 generation are in their 40s and the '486 Generation' is also used.\nThis was the first generation of South Koreans to grow up free from the poverty that had marked South Korea in the recent past. The broad political mood of the generation was far more left-leaning than that of their parents, or their eventual children. They played a pivotal role in the democratic protests which forced President Chun Doo-hwan to call democratic elections in 1987, marking the transition from military rule to democracy.\n\nMembers of the 386 generation now comprise much of the elite of South Korean society. Kim Dae-jung benefitted from widespread 386er support, but it is the election of Roh Moo-hyun who is the strongest demonstration of the more left-leaning politics of the generation.\n"}
{"id": "470031", "url": "https://en.wikipedia.org/wiki?curid=470031", "title": "Anecdote", "text": "Anecdote\n\nAn anecdote is a brief, revealing account of an individual person or an incident. Occasionally humorous, anecdotes differ from jokes because their primary purpose is not simply to provoke laughter but to reveal a truth more general than the brief tale itself, such as to characterize a person by delineating a specific quirk or trait, to communicate an abstract idea about a person, place, or thing through the concrete details of a short narrative. An anecdote is \"a story with a point.\"\n\nAnecdotes may be real or fictional; the anecdotal digression is a common feature of literary works, and even oral anecdotes typically involve subtle exaggeration and dramatic shape designed to entertain the listener. However, an anecdote is always presented as the recounting of a real incident, involving actual persons and usually in an identifiable place. In the words of Jurgen Heine, they exhibit \"a special realism\" and \"a claimed historical dimension.\"\n\nThe word \"anecdote\" (in Greek: ἀνέκδοτον \"unpublished\", literally \"not given out\") comes from Procopius of Caesarea, the biographer of Justinian I, who produced a work entitled (\"Anekdota\", variously translated as \"Unpublished Memoirs\" or \"Secret History\"), which is primarily a collection of short incidents from the private life of the Byzantine court. Gradually, the term \"anecdote\" came to be applied to any short tale utilized to emphasize or illustrate whatever point the author wished to make. In the context of Estonian, Lithuanian, Bulgarian and Russian humor, an anecdote refers to any short humorous story without the need of factual or biographical origins.\n\nAnecdotal evidence is an informal account of evidence in the form of an anecdote. The term is often used in contrast to scientific evidence, as evidence that cannot be investigated using the scientific method. The problem with arguing based on anecdotal evidence is that anecdotal evidence is not necessarily typical; only statistical evidence can determine how typical something is. Misuse of anecdotal evidence is an informal fallacy.\n\nWhen used in advertising or promotion of a product, service, or idea, anecdotal evidence is often called a testimonial. The term is also sometimes used in a legal context to describe certain kinds of testimony. Psychologists have found that people are more likely to remember notable examples than the typical example.\n\n"}
{"id": "25429108", "url": "https://en.wikipedia.org/wiki?curid=25429108", "title": "Angelic tongues", "text": "Angelic tongues\n\nAngelic tongues are the languages supposedly used by angels. It usually refers to sung praise in Second Temple period Jewish materials.\n\nSongs of the Sabbath Sacrifice is the principal source for angelic tongues at Qumran. The texts are fragmentary but appear to relate to praise tongues:\n\nIt is not clear whether the angelic tongues are coherent, intelligible to man. However, since Songs of the Sabbath Sacrifice is itself related to sung praise at the Qumran community, there is a parallel with coherent angelic praise tongues in Testament of Job.\n\nThe pseudepigraphical Testament of Job (ca.100 BCE–100CE) contains a conclusion which is believed to relate to the compiling of the hymnbook used by a Therapeutae community. Job gives one of his daughters \"a cord\" (a stringed instrument of some kind?) \nJob’s other daughters likewise took on “the dialect of archons”, “the dialect of those and the “dialect of the cherubim” (T. Job 49:1-50:3). The “cherubim” are also mentioned Songs of the Sabbath Sacrifice as blessing God (4Q403 1 2, 15, cf. 4Q405 20 2, 3). \n\nThere is parallel description of sung prophecy among the Therapeutae in Alexandria by Philo, but no mention there of angelic tongues.\n\nA possible reference to Jewish practices of angelic tongues is 1Co13:1 \"If I speak in the tongues of men and of angels, but have not love, I am a noisy gong or a clanging cymbal.\" The distinction \"of men\" and \"of angels\" may suggests that a distinction was known to the Corinthians. If a distinction is intended then 1Co14:10 \"There are doubtless many different languages in the world, and none is without meaning\" may imply that \"tongues of men\" were intelligible, whereas 1Co14:2 For one who speaks in a tongue speaks not to men but to God; for no one understands him, but he utters mysteries in the Spirit.\" refers to angelic tongues. The problem with this is that the \"angelic\" tongues documented at Qumran and among the Therapeutae appear to be inspired, but coherent and intelligible, sung praise. Against this is the view of Dunn that \"It is evident then that Paul thinks of glossolalia as language\". \n"}
{"id": "1802982", "url": "https://en.wikipedia.org/wiki?curid=1802982", "title": "Bobby soxer (music)", "text": "Bobby soxer (music)\n\nBobby soxer is a 1940s sociological coinage describing the often very zealous fans of traditional pop music, in particular the singer Frank Sinatra. Bobby soxers were usually teenage girls and young adult women aged 13 to 25. Fashionable adolescent girls wore poodle skirts and rolled down their socks to the ankle. In high schools and colleges, the gymnasium was often used as a dance floor; however, since street shoes and street detritus might damage the polished wood floors, the students were required to remove their shoes and flop dance in their bobby socks, hence the terms \"bobby soxer\" and \"sock hop\".\n\nAdopting (in her teenage years) an impressionable adolescent persona, former child actress Shirley Temple portrayed the type in the film \"The Bachelor and the Bobby-Soxer\" (1947). The 1959 Frankie Avalon song \"Bobby Sox to Stockings\" also referenced the phenomenon. Later, Bobby Sheen performed under the name Bob B. Soxx & the Blue Jeans.\n\n"}
{"id": "38440777", "url": "https://en.wikipedia.org/wiki?curid=38440777", "title": "Committee for Safety of Foreign Exchange Students", "text": "Committee for Safety of Foreign Exchange Students\n\nThe Committee for Safety of Foreign Exchange Students (CSFES) is a non-profit organization which Danielle Grijalva started in the United States of America in 2004. This committee was created to protect foreign children who could be abused both physically and mentally if not put into a safe home.\n\nGrijalva had worked as an area representative for CCI (Center for Cultural Interchange), a foreign-exchange program for student that is based in Illinois. As an area representative she had the task of ensuring the safe placement of exchange students travelling to the United States. As she gained experience as a representative, Ms. Grijalva learned that not all exchange students were placed with safe host-families.\n\nIn 2004, Danielle Grijalva resigned from CCI in protest when she found that another coordinator in the same agency had knowingly placed two boys from non-English speaking backgrounds in the home of a pederast who immediately began to groom them with gay pornography (which constitutes a crime in California), to provide them alcohol (which is against the law), and to deprive them of bathroom privacy. This was the eighth time that this man had been allowed to host boys.\n\nGrijalva was appalled when she found that her agency was totally disinterested in what had happened to the boys, but threatened her with a lawsuit if she continued her concerns. The agency then concocted the allegations that the boys were expelled from high school for downloading pornography from their host father's computer. The school principal contacted the Police Department and local Child Protective Services. Police, not the agency, removed the boys from the home. Their report claimed that when the officer went to the door, the man who answered stood there with his pants around his legs.\n\nWhen she expressed concerns about future students being placed in this man's home, the manager allegedly said, \"Now, Danielle, we cannot discriminate against homosexuality. Back off!\"\n\nThe president of the agency interfered with law enforcement stating that the boys concocted the allegations; because, they were accustomed to having a swimming pool in their back yard and wanted to be placed in a house that had one. The manager refused to change the placement and, hearing their allegations, notified police that the boys had attempted to steal from their host when they were caught looking for their passports and return tickets to go home. They, in common with most others in this situation, were instructed to sign a statement that the breakdown of the placement was due entirely to their own bad behavior, and they were told that they would be denied future access to the US if they refused to sign. This is sheer bullying; because, agencies do not have the power to prevent return visits. However, the boys did not know that they were being blackmailed and they signed. This of course prevents them from suing their abusers and the agencies.\n\nWhen in Grijalva wrote to inform the German agencies responsible for sending the boys to the US, she received an aggressive response. When she persisted and wrote again, she received a one-sentence reply: \"When we told you we didn't wish to hear from you, we meant it.\"\n\nGrijalva received so many complaints from parents and students that she set up a website which has attracted more and more horror stories. She established the International Committee for Safety of Foreign Exchange Students (CSFES), enlisting the support of others who were aware of the problems. CSFES became a registered charity in 2007, by which time its filing cabinets were full of complaints from parents along with newspaper reports of exchange students raped, molested, and exposed to every type of extortion. CSFES has no power other than its ability to support parents and students, to publicise the actions of unscrupulous agencies using media, to lobby politicians and to lodge complaints with government and exchange agency officials who invariably blame students.\n\nCSFES maintains that every exchange student deserves a safe exchange experience. To ensure this, Ms. Grijalva tries to educate various government agencies, the exchange organizations themselves and potential host-families.\n\nUnfortunately, not all exchange students end up in safe host-families. CSFES then steps in and acts as an advocate for exchange students in the US. Due to the efforts of CSFES, many exchange students who have felt alone in difficult circumstances have been able to achieve their goal of a good experience.\n\nSince the beginning, Ms. Grijalva has gained the attention of the US media.\n\nNot everyone has been happy about this attention. Exchange organizations have filed lawsuits against CSFES because of its work.\nMedia in non-US countries have contacted CSFES to get their take on the way the exchange industry works in the US. CSFES have become the go-to fount of information that has been needed.\n\nCSFES works with other non-profit organizations dedicated to the safety of exchange students:\n\n\nRepresentatives of CSFES operate in various countries around the world.\n\nIn 2013, CSFES joined the social media Facebook.\n"}
{"id": "5823", "url": "https://en.wikipedia.org/wiki?curid=5823", "title": "Confucius", "text": "Confucius\n\nConfucius ( ; 551–479 BC) was a Chinese teacher, editor, politician, and philosopher of the Spring and Autumn period of Chinese history.\n\nThe philosophy of Confucius, also known as Confucianism, emphasized personal and governmental morality, correctness of social relationships, justice and sincerity. His followers competed successfully with many other schools during the Hundred Schools of Thought era only to be suppressed in favor of the Legalists during the Qin dynasty. Following the victory of Han over Chu after the collapse of Qin, Confucius's thoughts received official sanction and were further developed into a system known in the West as Neo-Confucianism, and later New Confucianism (Modern Neo-Confucianism).\n\nConfucius is traditionally credited with having authored or edited many of the Chinese classic texts including all of the Five Classics, but modern scholars are cautious of attributing specific assertions to Confucius himself. Aphorisms concerning his teachings were compiled in the \"Analects\", but only many years after his death.\n\nConfucius's principles have commonality with Chinese tradition and belief. He championed strong family loyalty, ancestor veneration, and respect of elders by their children and of husbands by their wives, recommending family as a basis for ideal government. He espoused the well-known principle \"Do not do unto others what you do not want done to yourself\", the Golden Rule. He is also a traditional deity in Daoism.\n\nConfucius is widely considered as one of the most important and influential individuals in shaping human history. His teaching and philosophy greatly impacted people around the world and remains influential today.\n\nThe name \"Confucius\" is a Latinized form of the Mandarin Chinese \"Kǒng Fūzǐ\" (, meaning \"Master Kong\"), and was coined in the late 16th century by the early Jesuit missionaries to China. Confucius's clan name was \"Kǒng\" (; Old Chinese: ), and his given name was \"Qiū\" (; OC: ). His \"capping name\", given upon reaching adulthood and by which he would have been known to all but his older family members, was \"Zhòngní\" (; OC: *), the \"Zhòng\" indicating that he was the second son in his family.\nIt is thought that Confucius was born on September 28, 551BC, in the district of Zou () near present-day Qufu, China. The area was notionally controlled by the kings of Zhou but effectively independent under the local lords of Lu. His father Kong He () or Shuliang He (叔梁紇) was an elderly commandant of the local Lu garrison. His ancestry traced back through the dukes of Song to the Shang dynasty which had preceded the Zhou. Traditional accounts of Confucius's life relate that Kong He's grandfather had migrated the family from Song to Lu.\n\nKong He died when Confucius was three years old, and Confucius was raised by his mother Yan Zhengzai () in poverty. His mother would later die at less than 40 years of age. At age 19 he married Qiguan (亓官), and a year later the couple had their first child, Kong Li (孔鯉). Qiguan and Confucius would later have two daughters together, one of whom is thought to have died as a child.\n\nConfucius was educated at schools for commoners, where he studied and learned the Six Arts.\n\nConfucius was born into the class of \"shi\" (士), between the aristocracy and the common people. He is said to have worked in various government jobs during his early 20s, and as a bookkeeper and a caretaker of sheep and horses, using the proceeds to give his mother a proper burial. When his mother died, Confucius (aged 23) is said to have mourned for three years, as was the tradition.\n\nIn Confucius's time, the state of Lu was headed by a ruling ducal house. Under the duke were three aristocratic families, whose heads bore the title of viscount and held hereditary positions in the Lu bureaucracy. The Ji family held the position \"Minister over the Masses\", who was also the \"Prime Minister\"; the Meng family held the position \"Minister of Works\"; and the Shu family held the position \"Minister of War\". In the winter of 505 BC, Yang Hu—a retainer of the Ji family—rose up in rebellion and seized power from the Ji family. However, by the summer of 501 BC, the three hereditary families had succeeded in expelling Yang Hu from Lu. By then, Confucius had built up a considerable reputation through his teachings, while the families came to see the value of proper conduct and righteousness, so they could achieve loyalty to a legitimate government. Thus, that year (501 BC), Confucius came to be appointed to the minor position of governor of a town. Eventually, he rose to the position of Minister of Crime.\n\nConfucius desired to return the authority of the state to the duke by dismantling the fortifications of the city—strongholds belonging to the three families. This way, he could establish a centralized government. However, Confucius relied solely on diplomacy as he had no military authority himself. In 500 BC, Hou Fan—the governor of Hou—revolted against his lord of the Shu family. Although the Meng and Shu families unsuccessfully besieged Hou, a loyalist official rose up with the people of Hou and forced Hou Fan to flee to the Qi state. The situation may have been in favor for Confucius as this likely made it possible for Confucius and his disciples to convince the aristocratic families to dismantle the fortifications of their cities. Eventually, after a year and a half, Confucius and his disciples succeeded in convincing the Shu family to raze the walls of Hou, the Ji family in razing the walls of Bi, and the Meng family in razing the walls of Cheng. First, the Shu family led an army towards their city Hou and tore down its walls in 498 BC.\n\nSoon thereafter, Gongshan Furao or Buniu, a retainer of the Ji family, revolted and took control of the forces at Bi. He immediately launched an attack and entered the capital Lu. Earlier, Gongshan had approached Confucius to join him, which Confucius considered. Even though he disapproved the use of a violent revolution, the Ji family dominated the Lu state force for generations and had exiled the previous duke. Although he wanted the opportunity to put his principles into practice, Confucius gave up on this idea in the end. Creel (1949) states that, unlike the rebel Yang Hu before him, Gongshan may have sought to destroy the three hereditary families and restore the power of the duke. However, Dubs (1946) is of the view that Gongshan was encouraged by Viscount Ji Huan to invade the Lu capital in an attempt to avoid dismantling the Bi fortified walls. Whatever the situation may have been, Gongshan was considered an upright man who continued to defend the state of Lu, even after he was forced to flee.\n\nDuring the revolt by Gongshan, Zhong You () had managed to keep the duke and the three viscounts together at the court. Zhong You was one of the disciples of Confucius and Confucius had arranged for him to be given the position of governor by the Ji family. When Confucius heard of the raid, he requested that Viscount Ji Huan allow the duke and his court to retreat to a stronghold on his palace grounds. Thereafter, the heads of the three families and the duke retreated to the Ji's palace complex and ascended the Wuzi Terrace. Confucius ordered two officers to lead an assault against the rebels. At least one of the two officers was a retainer of the Ji family, but they were unable to refuse the orders while in the presence of the duke, viscounts, and court. The rebels were pursued and defeated at Gu. Immediately after the revolt was defeated, the Ji family razed the Bi city walls to the ground.\n\nThe attackers retreated after realizing that they would have to become rebels against the state and their lord. Through Confucius's actions, the Bi officials had inadvertently revolted against their own lord, thus forcing Viscount Ji Huan's hand in having to dismantle the walls of Bi (as it could have harbored such rebels) or confess to instigating the event by going against proper conduct and righteousness as an official. Dubs (1949) suggests that the incident brought to light Confucius's foresight, practical political ability, and insight into human character.\n\nWhen it was time to dismantle the city walls of the Meng family, the governor was reluctant to have his city walls torn down and convinced the head of the Meng family not to do so. The \"Zuo Zhuan\" recalls that the governor advised against razing the walls to the ground as he said that it made Cheng vulnerable to the Qi state and cause the destruction of the Meng family. Even though Viscount Meng Yi gave his word not to interfere with an attempt, he went back on his earlier promise to dismantle the walls.\n\nLater in 498 BC, Duke Ding personally went with an army to lay siege to Cheng in an attempt to raze its walls to the ground, but he did not succeed. Thus, Confucius could not achieve the idealistic reforms that he wanted including restoration of the legitimate rule of the duke. He had made powerful enemies within the state, especially with Viscount Ji Huan, due to his successes so far. According to accounts in the \"Zuo Zhuan\" and \"Shiji\", Confucius departed his homeland in 497 BC after his support for the failed attempt of dismantling the fortified city walls of the powerful Ji, Meng, and Shu families. He left the state of Lu without resigning, remaining in self-exile and unable to return as long as Viscount Ji Huan was alive.\n\nThe \"Shiji\" stated that the neighboring Qi state was worried that Lu was becoming too powerful while Confucius was involved in the government of the Lu state. According to this account, Qi decided to sabotage Lu's reforms by sending 100 good horses and 80 beautiful dancing girls to the duke of Lu. The duke indulged himself in pleasure and did not attend to official duties for three days. Confucius was disappointed and resolved to leave Lu and seek better opportunities, yet to leave at once would expose the misbehavior of the duke and therefore bring public humiliation to the ruler Confucius was serving. Confucius therefore waited for the duke to make a lesser mistake. Soon after, the duke neglected to send to Confucius a portion of the sacrificial meat that was his due according to custom, and Confucius seized upon this pretext to leave both his post and the Lu state.\n\nAfter Confucius's resignation, he began a long journey or set of journeys around the principality states of north-east and central China including Wey, Song, Zheng, Cao, Chu, Qi, Chen, and Cai (and a failed attempt to go to Jin). At the courts of these states, he expounded his political beliefs but did not see them implemented.\n\nAccording to the \"Zuo Zhuan\", Confucius returned home to his native Lu when he was 68, after he was invited to do so by Ji Kangzi, the chief minister of Lu. The \"Analects\" depict him spending his last years teaching 72 or 77 disciples and transmitting the old wisdom via a set of texts called the Five Classics.\n\nDuring his return, Confucius sometimes acted as an advisor to several government officials in Lu, including Ji Kangzi, on matters including governance and crime.\n\nBurdened by the loss of both his son and his favorite disciples, he died at the age of 71 or 72. He died from natural causes. Confucius was buried in Kong Lin cemetery which lies in the historical part of Qufu in the Shandong Province. The original tomb erected there in memory of Confucius on the bank of the Sishui River had the shape of an axe. In addition, it has a raised brick platform at the front of the memorial for offerings such as sandalwood incense and fruit.\n\nAlthough Confucianism is often followed in a religious manner by the Chinese, many argue that its values are secular and that it is, therefore, less a religion than a secular morality. Proponents argue, however, that despite the secular nature of Confucianism's teachings, it is based on a worldview that is religious. Confucianism discusses elements of the afterlife and views concerning Heaven, but it is relatively unconcerned with some spiritual matters often considered essential to religious thought, such as the nature of souls. However, Confucius is said to have believed in astrology, saying: \"Heaven sends down its good or evil symbols and wise men act accordingly\".\n\nIn the \"Analects\", Confucius presents himself as a \"transmitter who invented nothing\". He puts the greatest emphasis on the importance of study, and it is the Chinese character for study () that opens the text. Far from trying to build a systematic or formalist theory, he wanted his disciples to master and internalize older classics, so that their deep thought and thorough study would allow them to relate the moral problems of the present to past political events (as recorded in the \"Annals\") or the past expressions of commoners' feelings and noblemen's reflections (as in the poems of the \"Book of Odes\").\n\nOne of the deepest teachings of Confucius may have been the superiority of personal exemplification over explicit rules of behavior. His moral teachings emphasized self-cultivation, emulation of moral exemplars, and the attainment of skilled judgment rather than knowledge of rules. Confucian ethics may, therefore, be considered a type of virtue ethics. His teachings rarely rely on reasoned argument, and ethical ideals and methods are conveyed indirectly, through allusion, innuendo, and even tautology. His teachings require examination and context to be understood. A good example is found in this famous anecdote:\n\nBy not asking about the horses, Confucius demonstrates that the sage values human beings over property; readers are led to reflect on whether their response would follow Confucius's and to pursue self-improvement if it would not have. Confucius serves not as an all-powerful deity or a universally true set of abstract principles, but rather the ultimate model for others. For these reasons, according to many commentators, Confucius's teachings may be considered a Chinese example of humanism.\n\nOne of his teachings was a variant of the Golden Rule, sometimes called the \"Silver Rule\" owing to its negative form:\n\nOften overlooked in Confucian ethics are the virtues to the self: sincerity and the cultivation of knowledge. Virtuous action towards others begins with virtuous and sincere thought, which begins with knowledge. A virtuous disposition without knowledge is susceptible to corruption, and virtuous action without sincerity is not true righteousness. Cultivating knowledge and sincerity is also important for one's own sake; the superior person loves learning for the sake of learning and righteousness for the sake of righteousness.\n\nThe Confucian theory of ethics as exemplified in \"lǐ\" () is based on three important conceptual aspects of life: (a) ceremonies associated with sacrifice to ancestors and deities of various types, (b) social and political institutions, and (c) the etiquette of daily behavior. It was believed by some that \"lǐ\" originated from the heavens, but Confucius stressed the development of \"lǐ\" through the actions of sage leaders in human history. His discussions of \"lǐ\" seem to redefine the term to refer to all actions committed by a person to build the ideal society, rather than those simply conforming with canonical standards of ceremony.\n\nIn the early Confucian tradition, \"lǐ\" was doing the proper thing at the proper time, balancing between maintaining existing norms to perpetuate an ethical social fabric, and violating them in order to accomplish ethical good. Training in the \"lǐ\" of past sages cultivates in people virtues that include ethical judgment about when \"lǐ\" must be adapted in light of situational contexts.\n\nIn Confucianism, the concept of \"li\" is closely related to \"yì\" (), which is based upon the idea of reciprocity. \"Yì\" can be translated as righteousness, though it may simply mean what is ethically best to do in a certain context. The term contrasts with action done out of self-interest. While pursuing one's own self-interest is not necessarily bad, one would be a better, more righteous person if one's life was based upon following a path designed to enhance the greater good. Thus an outcome of \"yì\" is doing the right thing for the right reason.\n\nJust as action according to \"lǐ\" should be adapted to conform to the aspiration of adhering to \"yì\", so \"yì\" is linked to the core value of \"rén\" ().\"Rén\" consists of 5 basic virtues: seriousness, generosity, sincerity, diligence and kindness. \"Rén\" is the virtue of perfectly fulfilling one's responsibilities toward others, most often translated as \"benevolence\" or \"humaneness\"; translator Arthur Waley calls it \"Goodness\" (with a capital \"G\"), and other translations that have been put forth include \"authoritativeness\" and \"selflessness.\" Confucius's moral system was based upon empathy and understanding others, rather than divinely ordained rules. To develop one's spontaneous responses of \"rén\" so that these could guide action intuitively was even better than living by the rules of \"yì\". Confucius asserts that virtue is a mean between extremes. For example, the properly generous person gives the right amount—not too much and not too little.\n\nConfucius's political thought is based upon his ethical thought. He argued that the best government is one that rules through \"rites\" (\"lǐ\") and people's natural morality, and \"not\" by using bribery and coercion. He explained that this is one of the most important analects: \"If the people be led by laws, and uniformity sought to be given them by punishments, they will try to avoid the punishment, but have no sense of shame. If they be led by virtue, and uniformity sought to be given them by the rules of propriety, they will have the sense of the shame, and moreover will become good.\" (Translated by James Legge) in the Great Learning (). This \"sense of shame\" is an internalisation of duty, where the punishment precedes the evil action, instead of following it in the form of laws as in Legalism.\n\nConfucius looked nostalgically upon earlier days, and urged the Chinese, particularly those with political power, to model themselves on earlier examples. In times of division, chaos, and endless wars between feudal states, he wanted to restore the Mandate of Heaven () that could unify the \"world\" (, \"all under Heaven\") and bestow peace and prosperity on the people. Because his vision of personal and social perfections was framed as a revival of the ordered society of earlier times, Confucius is often considered a great proponent of conservatism, but a closer look at what he proposes often shows that he used (and perhaps twisted) past institutions and rites to push a new political agenda of his own: a revival of a unified royal state, whose rulers would succeed to power on the basis of their moral merits instead of lineage. These would be rulers devoted to their people, striving for personal and social perfection, and such a ruler would spread his own virtues to the people instead of imposing proper behavior with laws and rules.\n\nConfucius did not believe in the concept of \"democracy\", which is itself an Athenian concept unknown in ancient China, but could be interpreted by Confucius's principles recommending against individuals electing their own political leaders to govern them, or that anyone is capable of self-government. He expressed fears that the masses lacked the intellect to make decisions for themselves, and that, in his view, since not everyone is created equal, not everyone has a right of self-government.\n\nWhile he supported the idea of government ruling by a virtuous king, his ideas contained a number of elements to limit the power of rulers. He argued for representing truth in language, and honesty was of paramount importance. Even in facial expression, truth must always be represented. Confucius believed that if a ruler is to lead correctly, by action, that orders would be unnecessary in that others will follow the proper actions of their ruler. In discussing the relationship between a king and his subject (or a father and his son), he underlined the need to give due respect to superiors. This demanded that the subordinates must advise their superiors if the superiors are considered to be taking a course of action that is wrong. Confucius believed in ruling by example, if you lead correctly, orders by force or punishment are not necessary.\n\nConfucius's teachings were later turned into an elaborate set of rules and practices by his numerous disciples and followers, who organized his teachings into the Analects. Confucius's disciples and his only grandson, Zisi, continued his philosophical school after his death. These efforts spread Confucian ideals to students who then became officials in many of the royal courts in China, thereby giving Confucianism the first wide-scale test of its dogma.\n\nTwo of Confucius's most famous later followers emphasized radically different aspects of his teachings. In the centuries after his death, Mencius () and Xun Zi () both composed important teachings elaborating in different ways on the fundamental ideas associated with Confucius. Mencius (4th century BC) articulated the innate goodness in human beings as a source of the ethical intuitions that guide people towards \"rén\", \"yì\", and \"lǐ\", while Xun Zi (3rd century BC) underscored the realistic and materialistic aspects of Confucian thought, stressing that morality was inculcated in society through tradition and in individuals through training. In time, their writings, together with the \"Analects\" and other core texts came to constitute the philosophical corpus of Confucianism.\n\nThis realignment in Confucian thought was parallel to the development of Legalism, which saw filial piety as self-interest and not a useful tool for a ruler to create an effective state. A disagreement between these two political philosophies came to a head in 223 BC when the Qin state conquered all of China. Li Si, Prime Minister of the Qin dynasty, convinced Qin Shi Huang to abandon the Confucians' recommendation of awarding fiefs akin to the Zhou Dynasty before them which he saw as being against to the Legalist idea of centralizing the state around the ruler. When the Confucian advisers pressed their point, Li Si had many Confucian scholars killed and their books burned—considered a huge blow to the philosophy and Chinese scholarship.\n\nUnder the succeeding Han and Tang dynasties, Confucian ideas gained even more widespread prominence. Under Wudi, the works of Confucius were made the official imperial philosophy and required reading for civil service examinations in 140 BC which was continued nearly unbroken until the end of the 19th century. As Mohism lost support by the time of the Han, the main philosophical contenders were Legalism, which Confucian thought somewhat absorbed, the teachings of Laozi, whose focus on more spiritual ideas kept it from direct conflict with Confucianism, and the new Buddhist religion, which gained acceptance during the Southern and Northern Dynasties era. Both Confucian ideas and Confucian-trained officials were relied upon in the Ming Dynasty and even the Yuan Dynasty, although Kublai Khan distrusted handing over provincial control to them.\n\nDuring the Song dynasty, the scholar Zhu Xi (AD 1130–1200) added ideas from Daoism and Buddhism into Confucianism. In his life, Zhu Xi was largely ignored, but not long after his death, his ideas became the new orthodox view of what Confucian texts actually meant. Modern historians view Zhu Xi as having created something rather different and call his way of thinking \"Neo-Confucianism\". Neo-Confucianism held sway in China, Japan, Korea, and Vietnam until the 19th century.\nThe works of Confucius were first translated into European languages by Jesuit missionaries in the 16th century during the late Ming dynasty. The first known effort was by Michele Ruggieri, who returned to Italy in 1588 and carried on his translations while residing in Salerno. Matteo Ricci started to report on the thoughts of Confucius, and a team of Jesuits—Prospero Intorcetta, Philippe Couplet, and two others—published a translation of several Confucian works and an overview of Chinese history in Paris in 1687. François Noël, after failing to persuade ClementXI that Chinese veneration of ancestors and Confucius did not constitute idolatry, completed the Confucian canon at Prague in 1711, with more scholarly treatments of the other works and the first translation of the collected works of Mencius. It is thought that such works had considerable importance on European thinkers of the period, particularly among the Deists and other philosophical groups of the Enlightenment who were interested by the integration of the system of morality of Confucius into Western civilization.\n\nIn the modern era Confucian movements, such as New Confucianism, still exist, but during the Cultural Revolution, Confucianism was frequently attacked by leading figures in the Communist Party of China. This was partially a continuation of the condemnations of Confucianism by intellectuals and activists in the early 20th century as a cause of the ethnocentric close-mindedness and refusal of the Qing Dynasty to modernize that led to the tragedies that befell China in the 19th century.\n\nConfucius's works are studied by scholars in many other Asian countries, particularly those in the Chinese cultural sphere, such as Korea, Japan, and Vietnam. Many of those countries still hold the traditional memorial ceremony every year.\n\nThe Ahmadiyya Muslim Community believes Confucius was a Divine Prophet of God, as were Lao-Tzu and other eminent Chinese personages.\n\nIn modern times, Asteroid 7853, \"Confucius\", was named after the Chinese thinker.\n\nThere is not much known of Confucius's disciples, and a little over half of them had their surnames recorded in the \"Zuo Zhuan\". The \"Analects\" records 22 names that are most likely Confucius's disciples, while the \"Mencius\" records 24 names, although it is quite certain that there have been many more disciples whose name were not recorded. Most of Confucius's disciples were from the Lu state, while others were from neighboring states. For example, Zigong was from the Wey state, and Sima Niu was from the Song state. Confucius's favorite disciple was Yan Hui, most probably one of the most impoverished of them all. Sima Niu, in contrast to Yan Hui, was from a hereditary noble family hailing from the Song state. Under Confucius's teachings, the disciples became well-learned in the principles and methods of government. He often engaged in discussion and debate with his students and gave high importance to their studies in history, poetry, and ritual. Confucius advocated loyalty to principle rather than to individual acumen, in which reform was to be achieved by persuasion rather than violence. Even though Confucius denounced them for their practices, the aristocracy was likely attracted to the idea of having trustworthy officials who were studied in morals as the circumstances of the time made it desirable. In fact, the disciple Zilu even died defending his ruler in Wei.\n\nYang Hu, who was a subordinate of the Ji family, had dominated the Lu government from 505 to 502 and even attempted a coup, which narrowly failed. As a likely consequence, it was after that that the first disciples of Confucius were appointed to government positions. A few of Confucius's disciples went on to attain official positions of some importance, some of which were arranged by Confucius. By the time Confucius was 50 years old, the Ji family had consolidated their power in the Lu state over the ruling ducal house. Even though the Ji family had practices with which Confucius disagreed and disapproved, they nonetheless gave Confucius's disciples many opportunities for employment. Confucius continued to remind his disciples to stay true to their principles and renounced those who did not, all the while being openly critical of the Ji family.\n\nNo contemporary painting or sculpture of Confucius survives, and it was only during the Han Dynasty that he was portrayed visually. Carvings often depict his legendary meeting with Laozi. Since that time there have been many portraits of Confucius as the ideal philosopher. The oldest known portrait of Confucius has been unearthed in the tomb of the Han dynasty ruler Marquis of Haihun (died 59 BC). The picture was painted on the wooden frame to a polished bronze mirror.\n\nIn former times, it was customary to have a portrait in Confucius Temples; however, during the reign of Hongwu Emperor (Taizu) of the Ming dynasty, it was decided that the only proper portrait of Confucius should be in the temple in his home town, Qufu in Shandong. In other temples, Confucius is represented by a memorial tablet. In 2006, the China Confucius Foundation commissioned a standard portrait of Confucius based on the Tang dynasty portrait by Wu Daozi.\n\nSoon after Confucius's death, Qufu, his home town, became a place of devotion and remembrance. The Han dynasty \"Records of the Grand Historian\" records that it had already become a place of pilgrimage for ministers. It is still a major destination for cultural tourism, and many people visit his grave and the surrounding temples. In Sinic cultures, there are many temples where representations of the Buddha, Laozi, and Confucius are found together. There are also many temples dedicated to him, which have been used for Confucian ceremonies.\n\nFollowers of Confucianism have a tradition of holding spectacular memorial ceremonies of Confucius () every year, using ceremonies that supposedly derived from Zhou Li () as recorded by Confucius, on the date of Confucius's birth. In the twentieth century, this tradition was interrupted for several decades in mainland China, where the official stance of the Communist Party and the State was that Confucius and Confucianism represented reactionary feudalist beliefs which held that the subservience of the people to the aristocracy is a part of the natural order. All such ceremonies and rites were therefore banned. Only after the 1990s did the ceremony resume. As it is now considered a veneration of Chinese history and tradition, even Communist Party members may be found in attendance.\n\nIn Taiwan, where the Nationalist Party (Kuomintang) strongly promoted Confucian beliefs in ethics and behavior, the tradition of the memorial ceremony of Confucius (祭孔) is supported by the government and has continued without interruption. While not a national holiday, it does appear on all printed calendars, much as Father's Day or Christmas Day do in the Western world.\n\nIn South Korea, a grand-scale memorial ceremony called Seokjeon Daeje is held twice a year on Confucius's birthday and the anniversary of his death, at Confucian academies across the country and Sungkyunkwan in Seoul.\n\nConfucius's descendants were repeatedly identified and honored by successive imperial governments with titles of nobility and official posts. They were honored with the rank of a marquis thirty-five times since Gaozu of the Han dynasty, and they were promoted to the rank of duke forty-two times from the Tang dynasty to the Qing dynasty. Emperor Xuanzong of Tang first bestowed the title of \"Duke Wenxuan\" on Kong Suizhi of the 35th generation. In 1055, Emperor Renzong of Song first bestowed the title of \"Duke Yansheng\" on Kong Zongyuan of the 46th generation.\n\nDuring the Southern Song dynasty, the Duke Yansheng Kong Duanyou fled south with the Song Emperor to Quzhou in Zhejiang, while the newly established Jin dynasty (1115–1234) in the north appointed Kong Duanyou's brother Kong Duancao who remained in Qufu as Duke Yansheng. From that time up until the Yuan dynasty, there were two Duke Yanshengs, one in the north in Qufu and the other in the south at Quzhou. An invitation to come back to Qufu was extended to the southern Duke Yansheng Kong Zhu by the Yuan-dynasty Emperor Kublai Khan. The title was taken away from the southern branch after Kong Zhu rejected the invitation, so the northern branch of the family kept the title of Duke Yansheng. The southern branch remained in Quzhou where they live to this day. Confucius's descendants in Quzhou alone number 30,000. The Hanlin Academy rank of Wujing boshi 五經博士 was awarded to the southern branch at Quzhou by a Ming Emperor while the northern branch at Qufu held the title Duke Yansheng. The leader of the southern branch is 孔祥楷 Kong Xiangkai.\n\nIn 1351, during the reign of Emperor Toghon Temür of the Yuan dynasty, 93rd-generation descendant Kong Huan (孔浣)'s 2nd son Kong Shao (孔昭) moved from China to Korea during the Goryeo Dynasty, and was received courteously by Princess Noguk (the Mongolian-born wife of the future king Gongmin). After being naturalized as a Korean citizen, he changed the hanja of his name from \"昭\" to \"紹\" (both pronounced \"so\" in Korean), married a Korean woman and bore a son (Gong Yeo (), 1329–1397), therefore establishing the Changwon Gong clan (), whose ancestral seat was located in Changwon, South Gyeongsang Province.\nThe clan then received an aristocratic rank during the succeeding Joseon Dynasty. In 1794, during the reign of King Jeongjo, the clan then changed its name to Gokbu Gong clan () in honor of Confucius's birthplace Qufu (), Shandong Province.Famous descendants include actors such as Gong Yoo (real name Gong Ji-cheol (공지철)) & Gong Hyo-jin (공효진); and artists such as male idol group B1A4 member Gongchan (real name Gong Chan-sik (공찬식)), singer-songwriter Minzy (real name Gong Min-ji (공민지)), as well as her great-aunt traditional folk dancer (공옥진).\n\nDespite repeated dynastic change in China, the title of Duke Yansheng was bestowed upon successive generations of descendants until it was abolished by the Nationalist Government in 1935. The last holder of the title, Kung Te-cheng of the 77th generation, was appointed Sacrificial Official to Confucius. Kung Te-cheng died in October 2008, and his son, Kung Wei-yi, the 78th lineal descendant, had died in 1989. Kung Te-cheng's grandson, Kung Tsui-chang, the 79th lineal descendant, was born in 1975; his great-grandson, Kung Yu-jen, the 80th lineal descendant, was born in Taipei on January 1, 2006. Te-cheng's sister, Kong Demao, lives in mainland China and has written a book about her experiences growing up at the family estate in Qufu. Another sister, Kong Deqi, died as a young woman. Many descendants of Confucius still live in Qufu today.\n\nA descendant of Confucius, H. H. Kung was the Premier of the Republic of China. One of his sons, Kong Lingjie 孔令傑 married Debra Paget who gave birth to Gregory Kung (孔德基).\n\nConfucius's family, the Kongs, have the longest recorded extant pedigree in the world today. The father-to-son family tree, now in its 83rd generation, has been recorded since the death of Confucius. According to the Confucius Genealogy Compilation Committee, he has 2 million known and registered descendants, and there are an estimated 3 million in all. Of these, several tens of thousands live outside of China. In the 14th century, a Kong descendant went to Korea, where an estimated 34,000 descendants of Confucius live today. One of the main lineages fled from the Kong ancestral home in Qufu during the Chinese Civil War in the 1940s and eventually settled in Taiwan. There are also branches of the Kong family who have converted to Islam after marrying Muslim women, in Dachuan in Gansu province in the 1800s, and in 1715 in Xuanwei city in Yunnan province. Many of the Muslim Confucius descendants are descended from the marriage of Ma Jiaga (马甲尕), a Muslim woman, and Kong Yanrong (孔彦嵘), 59th generation descendant of Confucius in the year 1480 and are found among the Hui and Dongxiang peoples. The new genealogy includes the Muslims. Kong Dejun (孔德軍) is a prominent Islamic scholar and Arabist from Qinghai province and a 77th generation descendant of Confucius.\n\nBecause of the huge interest in the Confucius family tree, there was a project in China to test the DNA of known family members of the collateral branches in mainland China. Among other things, this would allow scientists to identify a common Y chromosome in male descendants of Confucius. If the descent were truly unbroken, father-to-son, since Confucius's lifetime, the males in the family would all have the same Y chromosome as their direct male ancestor, with slight mutations due to the passage of time. The aim of the genetic test was the help members of collateral branches in China who lost their genealogical records to prove their descent. However, in 2009, many of the collateral branches decided not to agree to DNA testing. Bryan Sykes, professor of genetics at Oxford University, understands this decision: \"The Confucius family tree has an enormous cultural significance,\" he said. \"It's not just a scientific question.\" The DNA testing was originally proposed to add new members, many of whose family record books were lost during 20th-century upheavals, to the Confucian family tree. The main branch of the family which fled to Taiwan was never involved in the proposed DNA test at all.\n\nIn 2013 a DNA test performed on multiple different families who claimed descent from Confucius found that they shared the same Y chromosome as reported by Fudan University.\n\nThe fifth and most recent edition of the Confucius genealogy was printed by the Confucius Genealogy Compilation Committee (CGCC). It was unveiled in a ceremony at Qufu on September 24, 2009. Women are now included for the first time.\n\nThere is also a \"Sacrificial Official to Mencius\" for a descendant of Mencius, a \"Sacrificial Official to Zengzi\" for a descendant of Zengzi, and a \"Sacrificial Official to Yan Hui\" for a descendant of Yan Hui.\n\nThe descendants of Confucius still use generation poems for their names given to them by the Ming- and Qing-dynasty Emperors along with the descendants of the other Four Sages 四氏, Mencius, Zengzi, and Yan Hui.\n\n\n"}
{"id": "33169034", "url": "https://en.wikipedia.org/wiki?curid=33169034", "title": "Convention for the Safeguarding of the Intangible Cultural Heritage", "text": "Convention for the Safeguarding of the Intangible Cultural Heritage\n\nThe Convention for the Safeguarding of the Intangible Cultural Heritage is a UNESCO treaty adopted by the UNESCO General Conference on 17 October 2003. The convention entered into force in 2006, after thirtieth instruments of ratification by UNESCO Member States. As of September 2018, 178 states have ratified, approved or accepted the convention.\n\nThe Convention contains following provisions:\n\nI. General Provisions\n\nII. Organs of the Convention\n\nIII. Safeguarding of the intangible cultural heritage at the national level\n\nIV. Safeguarding of the intangible cultural heritage at the international level\n\nV. International cooperation and assistance\n\nVI. Intangible Cultural Heritage Fund\n\nVII. Reports\n\nVIII. Transitional clause\n\nIX. Final clauses\n\nUnlike other UNESCO conventions, this convention begins with stating its purposes, which are;\nIntangible cultural heritage refers to \"traditions or living expressions inherited from our ancestors and passed on to our descendants, such as oral traditions, performing arts, social practices, rituals, festive events, knowledge and practices concerning nature and the universe or the knowledge and skills to produce traditional crafts\". The Convention defines it as follows:\n\nThe Convention works on both national and international levels.\nAt the national level, State Parties are supposed to ‘take necessary measures to ensure the safeguarding of the intangible cultural heritage present in its territory.” These measures include identification of the intangible cultural heritage that exists in its territory, adoption of appropriate policies, promotion of education and so on. Besides, in taking these measures, each state parties must “endeavor to ensure the widest possible participation of communities, groups, and, where appropriate, individuals that create, maintain and transmit such heritage, and to involve them actively in its management”.\n\nAt the international level, this Convention promotes international cooperation, which includes “the exchange of information and experience, joint initiatives, and the establishment of a mechanism of assistance” to other State Parties.\n\nLists\n\nThe Committee to the Convention publishes and keeps up to date two lists of intangible cultural heritage, which are\n\nIntangible Cultural Heritage Fund\n\nThe Convention establishes Intangible Cultural Heritage Fund, the use of which is decided by the Committee. The fund mainly consists of the contributions by State Parties and funds by the General Conference of UNESCO.\n\nOne of the first international occasions that mentioned the preservation of 'intangible heritage' was the World Conference on Cultural Policies in Mexico City in 1982. This Conference defined cultural heritage as including “both tangible and intangible works through which the creativity of people finds expression,” and asked UNESCO and Member States to take measures for protecting this kind of heritage.\n\nIn 1989, UNESCO adopted the Recommendation on the Safeguarding of Traditional Culture and Folklore as the first legal instrument towards the safeguarding of intangible cultural heritage. This Recommendation reflected the ideas of the earlier Conference in Mexico City. UNESCO conducted some promotional programs for raising awareness of this Recommendation, but was not very successful. However, in the late 90's, there was a conference held for the assessment of this Recommendation, which pointed out some problems to be considered in drafting the Convention. In this sense, this Recommendation served as an important step.\n\nIn 1997, UNESCO launched the program of Proclamation of the Masterpieces of the Oral and Intangible Heritage of Humanity, intending to raise awareness of the importance of intangible heritage. This program proclaimed a total of 90 masterpieces between 2001 and 2005, and caused the movement toward the Convention.\n\nAccording to the request of Member States, a preliminary study, undertaken by Director-General, on how could the safeguarding of intangible cultural heritage be conducted, recommended to create a new document that set an international standard.\n\nIn 2001, the General Conference adopted another instrument, Universal Declaration on Cultural Diversity, which also includes articles dealing with the preservation of ’heritage in all forms’. This declaration and its Action Plan presented basic idea of the coming convention and helped to develop it.\n\nAs a result of many meetings for two years, the draft Convention was brought into the General Conference and adopted in 2003.\n\n\n\n"}
{"id": "54004404", "url": "https://en.wikipedia.org/wiki?curid=54004404", "title": "Criminal tradition", "text": "Criminal tradition\n\nCriminal tradition - of the cultural transmission of criminal values. Criminal traditions are transmitted from the older generation to the younger generation, such as social customs are in other forms of society.\n\nStudies of the criminal tradition involved Clifford R. Shaw and Henry D. McKay. They put forward a theory of “cultural transmission”, focuses on the development in some urban neighborhoods of a criminal tradition that persists from one generation to another despite constant changes in population. This theory stresses the value systems of different areas\n\nAlso worth noting theory of “differential association,” in which Edwin H. Sutherland described the processes by which criminal values are taken over by the individual. Edwin H. Sutherland asserted that criminal behavior is learned and that it is learned in interaction with others who have already incorporated criminal values.\n\nResearch by Shaw and McKay on the concept of cultural transmission indicates that a criminal tradition or subculture does exist in areas of larger cities. According to their studies Criminal tradition arises and is maintained in areas of instability, and the values, norms, and behaviors of the participants in the criminal tradition are viewed as normal by these people.\n\nTraditions are not personified, so adolescents and boys are easier to obey than direct instructions from specific individuals. This is the power of tradition. The norms of the life of groups in which the will of their members manifest themselves take the form of tradition most often. In the tradition that there is no personification, adolescents find it easier to obey them than to a particular person. Against this background, the criminal traditions that are prevalent in youth criminal groups, especially in closed educational and correctional institutions are especially dangerous. In a criminal environment, there are two kinds of traditions:\n\n\nAt the same time, there is a transformation of existing criminal traditions and the emergence of new ones. The reason for this is the changes in the social, economic, legal and other spheres.\n\nOn the criminal tradition in different countries of the world, there is a huge amount of work. There is an impressive number of works on the Russian criminal tradition, written in different languages. \nWe can also highlight works by Jonny Steinberg on the numbers gangs of South Africa\n\nThere is also the view that it is impossible to consider all the traditions of the criminal environment as antisocial and harmful, including, for the reason that some of the traditions in the cells of the remand center, contribute to hygiene and the maintenance of sanitary norms.\n\nIn Russian criminal tradition adherents of the criminal (criminal) tradition are characterized by active participation in the life of the \"thieves' community\"; Living on tangible assets obtained by criminal means; Propaganda of \"thieves' customs and traditions, as well as criminal way of life; Compulsion to keep a word not only before the \"brother\", but also the criminal-criminal world; Organization of collection of \"obschekovyh\" funds and control over their use; Guardianship and assistance to detainees and convicts, the so-called \"vagabonds\" and \"honest prisoners\"; Compliance with the decisions of \"gatherings\"; Demanding of \"brotherhood\" and control over their compliance; Organization of counteraction to state bodies.\n\n"}
{"id": "46199685", "url": "https://en.wikipedia.org/wiki?curid=46199685", "title": "Daughter from California syndrome", "text": "Daughter from California syndrome\n\n\"Daughter from California\" syndrome is a phrase used in the medical profession to describe a situation in which a long-lost distant relative arrives at the hospital at which a dying elderly relative is being treated, and insists that the medical team pursue aggressive measures to prolong the patient's life, or otherwise challenges the care the patient is being given. In his 2015 book \"\", American doctor Angelo Volandes ascribes this to \"guilt and denial,\" \"not necessarily what is best for the patient.\"\n\nThe \"daughter from California\" is often described as angry, articulate and informed.\n\nMedical professionals say that because the \"daughter from California\" has been absent from the life and care of the elderly patient, he or she is frequently surprised by the scale of the patient's deterioration, and may have unrealistic expectations about what's medically feasible. He or she may feel guilty about having been absent, and may therefore feel motivated to reassert his or her role as an involved caregiver.\n\nThe phrase was first documented by David Molloy and other gerontologists in a 1991 case report published in the \"Journal of the American Geriatrics Society\", titled \"Decision Making in the Incompetent Elderly: 'The Daughter from California Syndrome'.\" In the paper, Molloy and colleagues presented strategies intended to help medical staff deal with the difficult family members of mentally incompetent patients.\n\nIn California, the \"daughter from California\" is known as the \"daughter from New York.\"\n"}
{"id": "354224", "url": "https://en.wikipedia.org/wiki?curid=354224", "title": "Discrimination based on skin color", "text": "Discrimination based on skin color\n\nDiscrimination based on skin color, also known as colorism or shadeism, is a form of prejudice or discrimination in which people are treated differently based on the social meanings attached to skin color.\n\nResearch has found extensive evidence of discrimination based on skin color in criminal justice, business, the economy, housing, health care, media, and politics in the United States and Europe. Lighter skin tones are seen as preferable in many countries in Africa, Asia and South America.\n\nSeveral meta-analyses find extensive evidence of ethnic and racial discrimination in hiring in the North American and European labor markets. A 2016 meta-analysis of 738 correspondence tests in 43 separate studies conducted in OECD countries between 1990 and 2015 finds that there is extensive racial discrimination used within both the European and North American hiring process. Equivalent minority candidates need to send around 50% more applications than majority candidates to be invited for an interview. Recent research in the U.S. shows that socioeconomic and health inequality among African Americans along the color continuum is often similar or even larger in magnitude than what obtains betweens whites and African Americans as a whole.\n\nIn East, South and Southeast Asia, a preference for lighter skin is prevalent, especially in countries such as China, Korea, India and Japan.\n\nThe history of skin whitening in East Asia dates far back to ancient times. In the ancient dynastic eras, to be light in an environment in which the sun was harsh implied wealth and nobility because those individuals were able to remain indoors while servants had to labor outside. Ancient Asian cultures also associated light skin with feminine beauty. \"Jade\" white skin in Korea is known to have been the ideal as far back as the Gojoseon era. Japan's Edo period saw the start of a trend of women whitening their faces with rice powder as a \"moral duty\". Chinese women valued a \"milk white\" complexion and swallowed powdered pearls towards that end. Four out of ten women surveyed in Hong Kong, Malaysia, the Philippines, and South Korea use a skin whitening cream. In many Asian cultures, colorism is taught to children in the form of fairy tales, just as the Grimms' fairy tales featured light-skinned princesses or maidens; Asian mythological protagonists are typically fair and depict virtue, purity, and goodness. A light complexion is equated with feminine beauty, racial superiority, and power, and continues to have strong influences on marital prospects, employment, status, and income.\n\nGlobalized East Asia still retains these biases, but they are compounded by the influence of Westernized beauty ideals and media that equate whiteness with modern and urban wealth and success. The legacies of European colonialism in India and Pakistan also influence the modern relations between light skin and power.\n\nIt was mistakenly thought by Western scholars that the Hindu goddess Kali represents demonic powers and ugliness and, as a dark-skinned goddess (whose name translates to \"she who is black\"), is therefore a demonstration of Indian colorism. This, however, was later understood to not be true, as Kali is actually traditionally viewed positively and seen as a symbol of sexuality, motherly love, violence, and power. There also remain plenty of examples of black being exemplified as holy as with Shiva and the most well known and popular avatars of the god Vishnu, Krishna and Rama. More recently, this was understood to have been a strategy by British colonial powers to subjugate Indian civilization.\n\nColorism in India has also been fueled due to the events under British colonial rule, where British officials consistently demeaned dark-skinned Indians and favored light-skinned Indians for jobs over dark-skinned Indians. As a result of hundreds of years of British colonial influence, remnants of the British tactics that exacerbated colorism still remain in Indian society. Other forms of colorism in India can be seen in the cosmetic industry, where \"fairness\" creams meant to lighten skin are popular, and in the Bollywood industry, where the majority of actors and actresses hired are light-skinned, and actresses are often photoshopped to look lighter.\n\nFor example, in the state of Maharashtra, a group of young tribal girls trained to be flight crew through a government scholarship program that aimed to empower women. The majority of girls were denied employment due to their darker skin tone. A few of those women obtained jobs, but only as out-of-sight ground crew.\n\nSkin-lightening creams are popular in Pakistan, especially among women. Many ads feature light-skin models in good light while portraying dark-skinned models poorly. Bollywood, which largely features light skinned Indian actors, is also influential among Pakistanis.\n\nFair skin is a beauty ideal in contemporary Sri Lankan society but has its roots in ancient Sri Lankan beauty ideals. Fairness products and other products that include whitening agents are commonly sold in Sri Lanka and are popular among females. Fair skinned actors and actresses feature prominently in Bollywood films and Korean dramas both of which are widely popular and influential in Sri Lanka.\n\nHiroshi Wagatsuma writes in \"Daedalus\" that Japanese culture has long associated skin color with other physical characteristics that signify degrees of spiritual refinement or of primitiveness.\nThe scholar repeats an old Japanese proverb: \"white skin makes up for seven defects.\" More specifically for a woman, very light skin allows people to overlook her lack of other desired physical characteristics. Skin color has and continues to influence attractiveness and socioeconomic status and capability.\n\nPeople in the western hemisphere have long characterised east Asians, specifically Chinese and Japanese people, as \"yellow\", but the Chinese and Japanese seldom describe their skin color in that way. The Japanese traditionally used the word \"shiroi\" – meaning \"white\" – to describe the lighter shades of skin in their society.\n\nThe court ladies of Japan during the Nara period from 710 to 793 AD applied a large amount of white powder to the face and added red rosy cheeks. Many references to plump women with white skin appear in both drawings and writings from 794–1186 AD. In literature, note for example \"The Tale of Genji\" (written 1000–1012) by Lady Murasaki.\n\nA survey concluded that three quarters of Malaysian men thought their partners would be more attractive if they had lighter skin complexions.\n\nIn certain Southeast Asian countries such as Malaysia, a common beauty ideal is the \"Eurasian look\" known locally in Malaysia as the \"pan-Asian look\" is an ideal that stems from the beauty ideal of fair skin, which Eurasians tend to naturally possess. The overuse of pan-Asian faces on billboards and on television screens has been a controversial issue in the country. The issue was highlighted in 2009 when Zainuddin Maidin, a Malaysian politician, called for the reduction of pan-Asian faces which he claimed dominate TV and billboards and instead increase the number of Malay, Chinese and Indian faces on local television. Despite the controversy surrounding the preference for Malaysians who are of mixed Asian (Malay, Chinese or Indian) and European descent who possess features such as fair skin, some experts in the industry have said the use of pan-Asian faces can be used to promote the racial diversity of Malaysians. They can also be used to promote a product towards a diverse racial demographic because of their mixed appearance, which the Minister of Information had suggested in 1993.\n\nResearch suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of racial minorities among crime suspects in Sweden, Italy, and England and Wales. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for racial minorities in Sweden, the Netherlands, Italy, Germany, Denmark and France.\n\nSeveral meta-analyses find extensive evidence of ethnic and racial discrimination in hiring in the North American and European labor markets. A 2016 meta-analysis of 738 correspondence tests in 43 separate studies conducted in OECD countries between 1990 and 2015 finds that there is extensive racial discrimination in hiring decisions in Europe and North America. Equivalent minority candidates need to send around 50% more applications to be invited for an interview than majority candidates.\n\nA 2014 meta-analysis found extensive evidence of racial and ethnic discrimination in the housing market of several European countries. There is extensive discrimination against immigrant groups in the French housing and labor markets, against Turkish immigrants in the German labor market, and against immigrants with non-Spanish names in the Spanish housing market.\n\nA 2017 experimental study found that the Dutch discriminate against non-Western immigrants in trust games.\n\nClare Anyiam-Osigwe's romantic drama \"No Shade\" (2018) confronts the issue of colorism in the UK dating scene.\n\nBrazil has the world's largest population of African descendants living outside Africa. Racially mixed individuals with lighter skin generally have higher rates of social mobility. There are a disproportionate number of mostly European descent elites than those of visible African descent. There are large health, education and income disparities between the races in Brazil. A recent study even finds that skin color is a stronger predictor of social inequality in Brazil than 'race' (i.e. 'race-color' categories used on the Brazilian census); and highlights that socially perceived skin color and 'race' are not the same thing. Even though browns and blacks comprise more than 50 percent of the population, they comprise less than 25 percent of elected politicians.\n\nA 2016 study, using twins as a control for neighborhood and family characteristics, found that the nonwhite twin is disadvantaged in the educational system. A 2015 study on racial bias in teacher evaluations in Brazil found that Brazilian math teachers gave better grading assessments of white students than equally proficient and equivalently well-behaved black students.\n\nA 2018 paper found that discriminatory hiring and retention policies accounted for 6-8% of the overall racial wage gap.\n\nA 2016 study found that Chilean schoolteachers had lower expectations of their dark-skinned students (\"morenos\") than their light-skinned students (\"blancos\").\n\nA 2017 study revealed a 45% gap in educational achievement between the darkest- and lightest-skinned Mexicans and that wealth in the country similarly correlated to skin color.\n\nEuropean colonialism created a system of racial hierarchy and race-based ideology, which had led to a structure of domination that privileged whites over blacks. Biological differences in skin color were used as a justification for the enslavement and oppression of Africans and Native Americans; developing a social hierarchy that placed whites at the top and blacks at the bottom, with the exception of \"white trash\", who were considered lower than blacks. Slaves with lighter complexion were allowed to engage in less strenuous tasks, like domestic duties, while the darker slaves participated in hard labor, which was more than likely outdoors. African American with a partial white heritage were seen to be smarter and superior to dark-skinned blacks, giving them broader opportunities for education and the acquisition of land and property. Colorism was a device used by the white colonist in order to create a division between the Africans and further the idea that being as close to white as possible was the ideal image. One of the first forms of colorism was the white slave owners deciding that only the light skinned slaves would work in the house while the darker ones were subjected to the harsh conditions of the fields. This led to a clear division between the slaves There were tests to determine who was light enough to work in the house and sometimes get special privileges. One of these tests was the brown paper bag test. If a person's skin was darker than a brown paper bag, they were deemed too dark to work in the house. The skin tests were not just used by white people trying to differentiate between black people, but also by the black people themselves. In addition to the bag test, the comb test and the door test were also used. The comb test was used to measure the kinkiness of the persons hair. The objective was for the comb to be able to pass through the hair without stopping. The door test was very popular at some African American clubs and churches. The people in charge would paint the door a certain shade of brown, similar to the bag test, and if you were darker than the door, you were not allowed admittance into the establishment. These tests were used to measure what level of \"blackness\" was and was not acceptable for the world. Because the lighter slaves were allowed to work in the house, they were more likely to be educated than the darker slaves. This birthed the stereotype that dark people were stupid and ignorant. Scholars predict that the preferred color of beauty will not be black or white, but mixed in the future. Scholars also predict the United States will adopt a \"multicultural matrix\" which will help bridge the racial gap in efforts of achieving racial harmony. The matrix has four components, the mixed race will help fix racial issues, it serves as a sign of racial progress, it suggest racism as a phenomenon and also suggest that focus on race is racist due to the lack of racial neutrality.\n\nA 2014 meta-analysis of racial discrimination in product markets found extensive evidence of minority applicants being quoted higher prices for products. A 1995 study found that car dealers \"quoted significantly lower prices to white males than to black or female test buyers using identical, scripted bargaining strategies.\" A 2013 study found that eBay sellers of iPods received 21 percent more offers if a white hand held the iPod in the photo than a black hand. A 2017 study found that minorities receive a lower boost to earnings from legal education than whites and were less likely to practice law. However, it is difficult to determine the extent to which this is the result of racial discrimination.\n\nAfrican-Americans have historically faced discrimination in terms of getting access to credit.\n\nResearch suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of racial minorities among crime suspects. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for racial minorities. A 2012 study found that \"(i) juries formed from all-white jury pools convict black defendants significantly (16 percentage points) more often than white defendants, and (ii) this gap in conviction rates is entirely eliminated when the jury pool includes at least one black member.\" Research has found evidence of in-group bias, where \"black (white) juveniles who are randomly assigned to black (white) judges are more likely to get incarcerated (as opposed to being placed on probation), and they receive longer sentences.\" In-group bias has also been observed when it comes to traffic citations, as black and white cops are more likely to cite out-groups.\n\nA 2014 study in the \"Journal of Political Economy\" found that 9% of the black-white gap in sentencing could not be accounted for. The elimination of unexplained sentencing disparities would reduce \"the level of black men in federal prison by 8,000–11,000 men [out of black male prison population of 95,000] and save $230–$320 million per year in direct costs.\" The majority of the unexplained sentencing disparity appears to occur at the point when prosecutors decide to bring charges carrying “mandatory minimum” sentences. A 2018 paper by Alma Cohen and Crystal Yang of Harvard Law School found that \"that Republican-appointed judges give substantially longer prison sentences to black offenders versus observably similar non-black offenders compared to Democratic-appointed judges within the same district court.\" A 2018 study in the \"Quarterly Journal of Economics\" found that bail judges in Miami and Philadelphia were racially biased against black defendants, as white defendants had higher rates of pretrial misconduct than black defendants.\n\nA 2018 study in the \"American Journal of Public Health\" found that black and Hispanic men were far more likely to be killed by police than white men. A 2016 paper by Roland G. Fryer, Jr., found that while there are no racial differences in lethal use of police force, blacks and Hispanics are significantly more likely to experience non-lethal use of force. Reports by the Department of Justice have also found that police officers in Baltimore, Maryland, and Ferguson, Missouri, systemically stop, search (in some cases strip-searching) and harass black residents. A January 2017 report by the DOJ also found that the Chicago Police Department had \"unconstitutionally engaged in a pattern of excessive and deadly force\" and that police \"have no regard for the sanctity of life when it comes to people of color.\" A 2018 study found that police officers more likely to use lethal force on blacks.\n\nIn criminal sentencing, medium to dark-skinned African Americans are likely to receive sentences 2.6 years longer than those of whites or light-skinned African Americans. When a white victim is involved, those with more \"black\" features are likely to receive a much more severe punishment.\n\nAccording to a 2011 \"ProPublica\" analysis, \"whites are nearly four times as likely as minorities to win a pardon, even when the type of crime and severity of sentence are taken into account.\"\n\nA 2013 report by the American Civil Liberties Union found that blacks were \"3.73 times more likely than whites to be arrested for marijuana possession,\" even though \"blacks and whites use drugs, including marijuana, at similar rates.\"\n\nA 2014 study on the application of the death penalty in Connecticut over the period 1973–2007 found \"that minority defendants who kill white victims are capitally charged at substantially higher rates than minority defendants who kill minorities... There is also strong and statistically significant evidence that minority defendants who kill whites are more likely to end up with capital sentences than comparable cases with white defendants.\"\n\nA 2016 analysis by the \"New York Times\" \"of tens of thousands of disciplinary cases against inmates in 2015, hundreds of pages of internal reports and three years of parole decisions found that racial disparities were embedded in the prison experience in New York.\" Blacks and Latinos were sent more frequently to solitary and held there for longer durations than whites. The New York Times analysis found that the disparities were the greatest for violations where the prison guards had lots of discretion, such as disobeying orders, but smaller for violations that required physical evidence, such as possessing contraband.\n\nA 2016 report by the \"Sarasota Herald-Tribune\" found that Florida judges sentence black defendants to far longer prison sentences than whites with the same background. For the same drug possession crimes, blacks were sentenced to double the time of whites. Blacks were given longer sentences in 60 percent of felony cases, 68 percent of the most serious first-degree crimes, 45 percent of burglary cases and 30 percent of battery cases. For third-degree felonies (the least serious types of felonies in Florida), white judges sentenced blacks to twenty percent more time than whites, whereas black judges gave more balanced sentences.\n\nA 2017 report by the Marshall Project found that killings of black men by whites were far more likely to be deemed \"justifiable\" than killings by any other combination of races.\n\nA 2017 report by the United States Sentencing Commission (USSC) found, \"after controlling for a wide variety of sentencing factors\" (such as age, education, citizenship, weapon possession and prior criminal history), that \"black male offenders received sentences on average 19.1 percent longer than similarly situated White male offenders.\"\n\nA 2018 study in the journal \"Proceedings of the National Academy of Sciences\" found that tall young black men are especially likely to receive unjustified attention by law enforcement. The authors furthermore found a \"causal link between perceptions of height and perceptions of threat for Black men, particularly for perceivers who endorse stereotypes that Black people are more threatening than White people.\"\n\nA 2018 study in the \"American Economic Journal: Applied Economics\" found that judges gave longer sentences, in particular to black defendants, after their favorite team lost a home game.\n\nAnalysis of more than 20 million traffic stops in North Carolina showed that blacks were more than twice as likely as whites to be pulled over by police for traffic stops, and that blacks were more likely to be searched following the stop. There were no significant difference in the likelihood that Hispanics would be pulled over, but Hispanics were much more likely to be searched following a traffic stop than whites. When the study controlled for searches in high-crime areas, it still found that police disproportionately targeted black individuals. These racial disparities were particularly pronounced for young men. The study found that whites who were searched were more likely to carry contraband than blacks and Hispanics.\n\nIn 1954, \"Brown vs. the Board of Education\" ruled that integrated, equal schools be accessible to all children unbiased to skin color. Currently in the United States, not all state funded schools are equally funded.  Schools are funded by the \"federal, state, and local governments\" while \"states play a large and increasing role in education funding.\" \"Property taxes support most of the funding that local government provides for education.\" Schools located in lower income areas receive a lower level of funding and schools located in higher income areas receiving greater funding for education all based on property taxes.  The U.S. Department of Education reports that \"many high-poverty schools receive less than their fair share of state and local funding, leaving students in high-poverty schools with fewer resources than schools attended by their wealthier peers.\" The U.S. Department of Education also reports this fact affects \"more than 40% of low-income schools.\" Children of color are much more likely to suffer from poverty than white children.\n\nA 2015 study using correspondence tests \"found that when considering requests from prospective students seeking mentoring in the future, faculty were significantly more responsive to White males than to all other categories of students, collectively, particularly in higher-paying disciplines and private institutions.\" Through affirmative action, elite colleges consider a broader range of experiences for minority applicants.\n\nThe phrase \"brown paper bag test,\" also known as a paper bag party, along with the \"ruler test\" refers to a ritual once practiced by certain African-American sororities and fraternities who would not let anyone into the group whose skin tone was darker than a paper bag. Spike Lee's film \"School Daze\" satirized this practice at historically black colleges and universities. Along with the \"paper bag test,\" guidelines for acceptance among the lighter ranks included the \"comb test\" and \"pencil test,\" which tested the coarseness of one's hair, and the \"flashlight test,\" which tested a person's profile to make sure their features measured up or were close enough to those of the Caucasian race.\n\nA 2016 study in the journal \"PNAS\" found that blacks and Hispanics were systemically underrepresented in education-programs for gifted children where teachers and parents referred students to those programs; when a universal screening program based on IQ was used to refer students, the disparity was reduced significantly.\n\nSkin color discrimination in education affects individuals in different ways depending on gender. This may be due to the disparity in standards of attractiveness, to which women are held much more closely than men. White women, previously thought to be in a group that did not experience discrimination based on skin color, have been shown to be affected by this inequality. A 2013 study used spectrophotometer readings to quantify skin color of respondents. White women experience discrimination in education, with those having darker skin graduating from college at lower rates than those with lighter skin. This precise and repeatable test of skin color revealed that white women experience skin color discrimination in education at levels consistent with African-Americans. White men are not affected in this way.\n\nA 1999 study found that doctors treat black and white patients differently, even when their medical files were statistically identical. When shown patient histories and asked to make judgments about heart disease, the doctors were much less likely to recommend cardiac catheterization (a helpful procedure) to black patients. A 2015 study found that pediatricians were more likely to undertreat appendicitis pain in black children than white children. A 2017 study found that medical staff treating anterior cruciate ligament (ACL) injuries perceived black collegiate athletes as having higher pain tolerance than white athletes.\n\nA 2018 ProPublica analysis found that African Americans and Native Americans were underrepresented in clinical trials for new drugs. Fewer than 5% of patients were African-American, even though they make up 13.4% of the total US population. African-Americans were even underrepresented in trials involving drugs intended for diseases that disproportionately affect African-Americans. As a result, African-Americans who had exhausted all other treatments have weaker access to experimental treatments.\n\nA 2014 meta-analysis found extensive evidence of racial discrimination in the American housing market. Minority applicants for housing needed to make many more enquiries to view properties. Geographical steering of African-Americans in US housing remained significant. A 2003 study finds \"evidence that agents interpret an initial housing request as an indication of a customer's preferences, but also are more likely to withhold a house from all customers when it is in an integrated suburban neighborhood (redlining). Moreover, agents' marketing efforts increase with asking price for white, but not for black, customers; blacks are more likely than whites to see houses in suburban, integrated areas (steering); and the houses agents show are more likely to deviate from the initial request when the customer is black than when the customer is white. These three findings are consistent with the possibility that agents act upon the belief that some types of transactions are relatively unlikely for black customers (statistical discrimination).\"\n\nA report by the federal Department of Housing and Urban Development where the department sent African-Americans and whites to look at apartments found that African-Americans were shown fewer apartments to rent and houses for sale. A 2017 study found that \"that applications [for Airbnb housing] from guests with distinctively African American names are 16 percent less likely to be accepted relative to identical guests with distinctively white names.\"\n\nA 2017 paper by Troesken and Walsh found that pre-20th century cities \"created and sustained residential segregation through private norms and vigilante activity.\" However, \"when these private arrangements began to break down during the early 1900s\" whites started \"lobbying municipal governments for segregation ordinances.\" As a result, cities passed ordinances which \"prohibited members of the majority racial group on a given city block from selling or renting property to members of another racial group\" between 1909 and 1917.\n\nA 2017 study by Federal Reserve Bank of Chicago economists found that the practice of redlining—the practice whereby banks discriminated against the inhabitants of certain neighborhoods—had a persistent adverse impact on the neighborhoods, with redlining affecting homeownership rates, home values and credit scores in 2010. Since many African-Americans could not access conventional home loans, they had to turn to predatory lenders (who charged high interest rates). Due to lower home ownership rates, slumlords were able to rent out apartments that would otherwise be owned.\n\nA 2017 study in \"Research & Politics\" found that white supporters of Donald Trump became less likely to approve of federal housing assistance when they were shown an image of a black man.\n\nA 2018 study in the \"American Sociological Review\" found that housing market professionals (real estate agents, housing developers, mortgage appraisers and home value appraisers) held derogatory racial views about black and Latino individuals and neighborhoods whereas white individuals and neighborhoods were beneficiaries of widely shared, positive racial beliefs.\n\nA 2018 experimental study by University of Illinois and Duke University economists found that real estate agents and housing providers systematically recommended homes in neighborhoods with higher poverty rates, greater pollution, higher crime rates, fewer college educated families, and fewer skilled workers to minority individuals who had all the same characteristics as white individuals except ethnic differences.\n\nA 2018 study in the \"American Political Science Review\" found that white voters in areas which experienced massive African-American population growth between 1940 and 1960 were more likely to vote for California Proposition 14 (1964) which sought to enshrine legal protections for landlords and property owners who discriminated against \"colored\" buyers and renters.\n\nA 2018 study in the \"Journal of Politics\" found extensive evidence of discrimination against blacks and Hispanics in the New York City rental market. A 2018 study in the journal \"Regional Science and Urban Economics\" found that there was discrimination against blacks and Arab males in the U.S. rental market.\n\nSeveral meta-analyses find extensive evidence of ethnic and racial discrimination in hiring in the American labor market. A 2017 meta-analysis found \"no change in the levels of discrimination against African Americans since 1989, although we do find some indication of declining discrimination against Latinos.\" A 2016 meta-analysis of 738 correspondence tests – tests where identical CVs for stereotypically black and white names were sent to employers – in 43 separate studies conducted in OECD countries between 1990 and 2015 finds that there is extensive racial discrimination in hiring decisions in Europe and North America. These correspondence tests showed that equivalent minority candidates need to send around 50% more applications to be invited for an interview than majority candidates. A study that examine the job applications of actual people provided with identical résumés and similar interview training showed that African-American applicants with no criminal record were offered jobs at a rate as low as white applicants who had criminal records.\n\nA 2017 study found that minorities receive a lower boost to earnings from legal education than whites and were less likely to practice law. However, it is difficult to determine the extent to which this is the result of racial discrimination. The differences in earnings premiums appears to have diminished in recent years.\n\nResearch suggests that light-skinned African American women have higher salaries and greater job satisfaction than dark-skinned women. Being \"too black\" has recently been acknowledged by the U.S. Federal courts in an employment discrimination case under Title VII of the Civil Rights Act of 1964. In \"Etienne v. Spanish Lake Truck & Casino Plaza, LLC\" the United States Court of Appeals for the Fifth Circuit, determined that an employee who was told on several occasions that her manager thought she was \"too black\" to do various tasks, found that the issue of the employee's skin color rather than race itself, played a key role in an employer's decision to keep the employee from advancing.\n\nA 2008 study found that black service providers receive lower tips than white service providers.\n\nA 2017 report by Travis L. Dixon (of the University of Illinois at Urbana-Champaign) found that major media outlets tended to portray black families as dysfunctional and dependent while white families were portrayed as stable. These portrayals may give the impression that poverty and welfare are primarily black issues. According to Dixon, this can reduce public support for social safety programs and lead to stricter welfare requirements.\n\nAfrican Americans possessing lighter skin complexion and \"European features,\" such as lighter eyes, and smaller noses and lips have more opportunities in the media industry. For example, film producers hire lighter-skinned African Americans more often, television producers choose lighter skinned cast members, and magazine editors choose African American models that resemble European features. In regards to the magazine industry, African American women are rarely showcased in the most popular magazines. Therefore, African American girls have difficultly identifying with the models showcased in these magazines, because they do not represent the type of women that they come into contact with in their own communities. Recent studies have indicated that the number of racially biased advertisements in magazines have increased over the years. A content analysis conducted by Scott and Neptune (1997) shows that less than one percent of advertisements in major magazines featured African American models. When African Americans did appear in advertisements they were mainly portrayed as athletes, entertainers or unskilled laborers. In addition, seventy percent of the advertisements that features animal print included African American women. Animal print reinforces the stereotypes that African Americans are animalistic in nature, sexually active, less educated, have lower income, and extremely concerned with personal appearances. Concerning African American males in the media, darker skinned men are more likely to be portrayed as violent or more threatening, influencing the public perception of African American men. Since dark-skinned males are more likely to be linked to crime and misconduct, many people develop preconceived notions about the characteristics of black men.\n\nColorism was and still is very much evident in the media. An example of this is shown in the minstrel shows that were popular during and after slavery. Minstrel shows were a very popular form of theater that involved white and black people in black face portraying black people while doing demeaning things. The actors painted their faces with black paint to and over lined their lips with bright red lipstick to exaggerate and make fun of black people. When minstrel shows died out and television became popular, black actors were rarely hired and when they were, they had very specific roles. These roles included being servants, slaves, idiots, and criminals. White people wanted to keep this narrative going that black people were forever in debt to them because they essentially rescued blacks from themselves and made them humans instead of savages. This is seen in the \"mammy\" role that black women often played. The highlights of this role included black women being the loyal servant to the master and taking care of and loving his kids more than her own. Even though black people were allowed to be on TV, they still couldn't be too black. They had to pass the color tests and if they were dark, they were usually playing a humiliating role. That trend is something that follows into present day especially for women. There is a huge absence of dark black women in the media and when they are shown, they are typically portraying the angry black woman stereotype but have a light skinned character to balance them out. Darker women are rarely the protagonist that isn't troubled by drugs, or caught up in the legal system. There is also a large absence of representation of dark women in the music industry. The object of affection in music videos are women who could have easily passed the paper bag test and many entertainers have gone on record saying that they don't girls who are too dark and that they only prefer light or white women.\n\nA 2011 study found that white state legislators of both political parties were less likely to respond to constituents with African-American names. A 2013 study found that in response to e-mail correspondence from a putatively black alias, \"nonblack legislators were markedly less likely to respond when their political incentives to do so were diminished, black legislators typically continued to respond even when doing so promised little political reward. Black legislators thus appear substantially more intrinsically motivated to advance blacks' interests.\"\n\nSome research suggests that white voters' voting behavior is motivated by racial threat. A 2016 study, for instance, found that white Chicago voters' turnout decreased when public housing was reconstructed and 25,000 African Americans displaced. This suggest that white voters' turnout decreased due to not living in proximity to African-Americans.\n\nVoter ID laws have brought on accusations of racial discrimination. In a 2014 review by the Government Accountability Office of the academic literature, three studies out of five found that voter ID laws reduced minority turnout whereas two studies found no significant impact. Disparate impact may also be reflected in access to information about voter ID laws. A 2015 experimental study found that election officials queried about voter ID laws are more likely to respond to emails from a non-Latino white name (70.5% response rate) than a Latino name (64.8% response rate), though response accuracy was similar across groups. Studies have also analyzed racial differences in ID requests rates. A 2012 study in the city of Boston found that black and Hispanic voters were more likely to be asked for ID during the 2008 election. According to exit polls, 23% of whites, 33% of blacks, and 38% of Hispanics were asked for ID, though this effect is partially attributed to black and Hispanics preferring non-peak voting hours when election officials inspected a greater portion of IDs. Precinct differences also confound the data as black and Hispanic voters tended to vote at black and Hispanic-majority precincts. A 2010 study of the 2006 midterm election in New Mexico found that Hispanics were more likely to incur ID requests while early voters, women, and non-Hispanics were less likely to incur requests. A 2009 study of the 2006 midterm election nationwide found that 47% of white voters reported being asked to show photo identification at the polls, compared with 54% of Hispanics and 55% of African Americans.\" Very few were however denied the vote as a result of voter identification requests. A 2015 study found that turnout among blacks in Georgia was generally higher since the state began enforcing its strict voter ID law. A 2016 study by University of California, San Diego researchers found that voter ID laws \"have a differentially negative impact on the turnout of Hispanics, Blacks, and mixed-race Americans in primaries and general elections.\"\n\nResearch by University of Oxford economist Evan Soltas and Stanford political scientist David Broockman suggests that voters act upon racially discriminatory tastes. A 2018 study in \"Public Opinion Quarterly\" found that whites, in particular those who had racial resentment, largely attributed Obama's success among African-Americans to his race, and not his characteristics as a candidate and the political preferences of African-Americans. A 2018 study in the journal \"American Politics Research\" found that white voters tended to misperceive political candidates from racial minorities as being more ideologically extreme than objective indicators would suggest; this adversely affected the electoral chances for those candidates. A 2018 study in the \"Journal of Politics\" found that \"when a white candidate makes vague statements, many [nonblack] voters project their own policy positions onto the candidate, increasing support for the candidate. But they are less likely to extend black candidates the same courtesy... In fact, black male candidates who make ambiguous statements are actually punished for doing so by racially prejudiced voters.\"\n\nA 2018 study found evidence of racial-motivated reasoning as voters assessed President Barack Obama's economic performance. The study found that \"Whites attributed more responsibility to Obama under negative economic conditions (i.e., blame) than positive economic conditions (i.e., credit)... Whites attributed equal responsibility to the President and governors for negative economic conditions, but gave more responsibility to governors than Obama for positive conditions. Whites also gave governors more responsibility for state improvements than they gave Obama for national ones.\"\n\nA 2018 study examining \"all 24 African American challengers (non-incumbents) from 2000 to 2014 to white challengers from the same party running in the same state for the same office around the same time\" found \"that white challengers are about three times more likely to win and receive about 13 percentage points more support among white voters. These estimates hold when controlling for a number of potential confounding factors and when employing several statistical matching estimators.\"\n\nStudies have shown that due to societal influences, people associate beauty with lighter skin. This is especially evident in children. This belief has led dark-skinned children to feel inadequate in who they are and inferior when compared to people with lighter skin. African American women believe they would haven better luck dating if they were of lighter skin especially when dating African American men.\n\nA 2018 study found evidence that nonblack voters in Heisman Trophy voting were biased against nonblack players.\n\n\n\n"}
{"id": "6852737", "url": "https://en.wikipedia.org/wiki?curid=6852737", "title": "E. San Juan Jr.", "text": "E. San Juan Jr.\n\nEpifanio San Juan Jr., also known as E. San Juan Jr. (born December 29, 1938)\n, at Sta. Cruz, Manila, Philippines), is a known Filipino American literary academic, Tagalog writer, Filipino poet, civic intellectual, activist, writer, essayist, video/film maker, editor, and poet whose works related to the Filipino Diaspora in English and Filipino writings have been translated into German, Russian, French, Italian, and Chinese. As an author of books on race and cultural studies, he was a \"major influence on the academic world\". He was the director of the Philippines Cultural Studies Center in Storrs, Connecticut in the United States. In 1999, San Juan received the Centennial Award for Achievement in Literature from the Cultural Center of the Philippines because of his contributions to Filipino and Filipino American Studies.\n\nSan Juan received his elementary education in the Philippines at the Bonifacio Elementary School. He took secondary education at Jose Abad Santos High. He graduated as a magna cum laude from the University of the Philippines in 1958. He received his masters degree in 1962. He obtained a PhD degree from Harvard University in 1965 with the help of a Rockefeller fellowship and Harvard teaching fellowship]. He was a fellow of the Rockefeller Study Center in Bellagio, Italy.\n\nHe became a professor of the English language, Comparative Literature, Ethnic Studies, American Studies and Cultural Studies in the United States, Europe, the Philippines, and Taiwan. From 1961 to 1963, San Juan was appointed as a fellow and English-language tutor at Harvard University. Among the other universities in the United States where he taught include the University of California at Davis, the University of Connecticut at Storrs, and the Brooklyn College of the City University of New York. In the Philippines, he taught in the University of the Philippines in 2008, and at the Ateneo de Manila. Other universities include the Bowling Green State University, Wesleyan University, the Universities of Leuven and Antwerp in Belgium, and the National Tsing Hua University in the Republic of China (Taiwan).\n\nFrom 1998 to June 15, 2001, San Juan was a professor and the chairman of the Department of Comparative American Cultures in Washington State University. He was the executive director of the so-called Working Papers Series when he published essays on Cultural Studies and Ethnic Studies. In 2009, he became a fellow at the W.E.B. Du Bois Institute for African and African American Research of Harvard University. He was also a Fulbright lecturer, fellow, and professor at the Center for the Humanities of Wesleyan University in Connecticut, the Institute for the Advanced Study of the Humanities at the University of Edinburgh, and at the Institute for the Study of Culture and Society in Ohio.\n\nIn 2009-2010 he was a fellow of the W.E.B. \nDu Bois Institute, Harvard University, visiting professor of American Studies in Leuven University, Belgium (2003)and professor of English Comparative Literature, University of the Philippines (2008). Currently (2012=2013) he is a fellow of the Harry Ransom Center, University of Texas,Austin; and director of the Philippines Cultural Studies Center, Storrs, CT, & Washington DC, USA. He was appointed professorial lecturer (2015-2016)in cultural studies, Polytechnic University of the Philippines, Manila, Philippines.\n\nApart from writing about the Filipino Diaspora, San Juan's works include essays on race, social class, subalternity, and the U.S. Empire. His works were first published in 1954 on the pages of The Collegian New Review. After winning awards, his poems were anthologized in \"Godkissing Carrion/Selected Poems: 1954-1964\" in 1964, in \"The Exorcism and Other Poems\" in 1967, and \"The Ashes of Pedro Abad Santos and Other Poems\" in 1985. His literary milieu extends to \"media pieces\" related to the current political landscape, the human rights abuses and extrajudicial killings in the Philippines, racial polity in the United States, social justice, global mechanism of racialization and its impact on immigrant workers of the global South, essays on Marxism, human liberation, and exposés related to the \"resurrection\" of the \"contours\" of the American empire.\n\nIn 1966, he made translations of Amado V. Hernandez's poetry resulting to the work entitled \"Rice Grains: Selected Poems of Amado V. Hernandez\". In 1975, he introduced the literary writings of Carlos Bulosan, a Filipino labor organizer and writer, resulting to the publication of \"Carlos Bulosan and the Imagination of the Class Struggle\", the first full-length critical assessment of Bulosan's works, which was followed twenty years later by \"On Becoming Filipino: Selected Writings by Carlos Bulosan\" and \"The Cry and the Dedication\" in 1995. He was also the author of the \"first collection in English translation\" of the essays written by Georg Lukács, a Hungarian philosopher and founder of the Western Marxist tradition.\n\nIn 2007, San Juan authored a book of poems, the \"Balikbayang Mahal: Passages of Exile\". His other works are \"Racism and Cultural Studies\", \"Working through the Contradictions\", \"In the Wake of Terror\", \"US Imperialism and Revolution in the Philippines\", \"Beyond Postcolonial Theory\" (1995), and \"Hegemony and Strategies of Transgression\" (1998).\n\nRecent books include \"In the Wake of Terror: Class, Race, Nation and Ethnicity in the Postmodern World\" (Lexington); \"Working Through the Contradictions\" (Bucknell University Press). \"Critique and Social Transformation\" (Edwin Mellen Press), \"From Globalization to National Liberation\" (University of the Philippines Press).\"Balikbayang Sinta: E San Juan Reader\" (Ateneo U Press), \"Critical Interventions: From Joyce and Ibsen to Kingston and C.S. Peirce\" (Saarbrücken: Lambert), and \"Rizal in Our Time: revised edition\" (Anvil). His new volumes of poetry include \"Balikbayang Mahal: Passages in Exile,\"\"Sutrang Kayumanggi,\" \"Mahal Magpakailanman,\" Diwata Babaylan,\" and \"Bukas Luwalhating Kay Ganda\"(all available in amazon.com). The UST Publishing House will issue in 2013 his collection \"Ulikba at iba pang bagong tula.\"\n\nDuring May 1964, he won the Spanish Siglo de Oro Prize after writing a literary review and criticism of the poetry of Gongora. In 1992, San Juan's \"Racial Formations/Critical Transformations: Articulations of Power in Ethnic and Racial Studies in the United States\" was awarded the Gustavus Myers Center's Outstanding Book Award for the Study of Human Rights in the United States. In 1993, the same work received the National Book Award in Cultural Studies from the Association for Asian American Studies. The book is regarded as a classic in Ethnic and Asian American Studies. In 1999, San Juan Jr. received the Centennial Award for Achievement in Literature from the Cultural Center of the Philippines. In 2001, San Juan's \"After Post-colonialism: Remapping Philippines-United States Confrontations\" won the Gustavus Myers Center for Human Rights's Outstanding Book Award on Human Rights (also known as the Myers Distinguished Book Award). In 2007, San Juan produced the books entitled \"In the Wake of Terror: Class, Race, Nation, Ethnicity in the Postmodern World\", \"Imperialism and Revolution in the Philippines\", \"Balikbayang Sinta: An E. San Juan Reader\", and \"From Globalization to National Liberation: Essays\". San Juan's also received awards from the Association for Asian American Studies, and the Society for the Study of Multi-Ethnic Literatures in the United States.\n\n\n 7. journals.ateneo.edu/index.php?id=112 Symposium on E. San Juan, Jr. (May 2016)\n"}
{"id": "9252", "url": "https://en.wikipedia.org/wiki?curid=9252", "title": "Education", "text": "Education\n\nEducation is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Education frequently takes place under the guidance of educators, but learners may also educate themselves. Education can take place in formal or informal settings and any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational. The methodology of teaching is called pedagogy.\n\nFormal education is commonly divided formally into such stages as preschool or kindergarten, primary school, secondary school and then college, university, or apprenticeship.\n\nA right to education has been recognized by some governments and the United Nations. In most regions, education is compulsory up to a certain age.\n\nEtymologically, the word \"education\" is derived from the Latin \"ēducātiō\" (\"A breeding, a bringing up, a rearing\") from \"ēducō\" (\"I educate, I train\") which is related to the homonym \"ēdūcō\" (\"I lead forth, I take out; I raise up, I erect\") from \"ē-\" (\"from, out of\") and \"dūcō\" (\"I lead, I conduct\").\n\nEducation began in prehistory, as adults trained the young in the knowledge and skills deemed necessary in their society. In pre-literate societies, this was achieved orally and through imitation. Story-telling passed knowledge, values, and skills from one generation to the next. As cultures began to extend their knowledge beyond skills that could be readily learned through imitation, formal education developed. Schools existed in Egypt at the time of the Middle Kingdom.\n\nPlato founded the Academy in Athens, the first institution of higher learning in Europe. The city of Alexandria in Egypt, established in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece. There, the great Library of Alexandria was built in the 3rd century BCE. European civilizations suffered a collapse of literacy and organization following the fall of Rome in CE 476.\n\nIn China, Confucius (551–479 BCE), of the State of Lu, was the country's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbours like Korea, Japan, and Vietnam. Confucius gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.\n\nThe Aztecs also had a well-developed theory about education, which has an equivalent word in Nahuatl called \"tlacahuapahualiztli.\" It means \"the art of raising or educating a person\" or \"the art of strengthening or bringing up men.\" This was a broad conceptualization of education, which prescribed that it begins at home, supported by formal schooling, and reinforced by community living. Historians cite that formal education was mandatory for everyone regardless of social class and gender. There was also the word \"neixtlamachiliztli\", which is \"the act of giving wisdom to the face.\" These concepts underscore a complex set of educational practices, which was oriented towards communicating to the next generation the experience and intellectual heritage of the past for the purpose of individual development and his integration into the community.\n\nAfter the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centres of advanced education. Some of these establishments ultimately evolved into medieval universities and forebears of many of Europe's modern universities. During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of inquiry, and produced a great variety of fine scholars and natural philosophers, including Thomas Aquinas of the University of Naples, Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation, and Saint Albert the Great, a pioneer of biological field research. Founded in 1088, the University of Bologne is considered the first, and the oldest continually operating university.\n\nElsewhere during the Middle Ages, Islamic science and mathematics flourished under the Islamic caliphate which was established across the Middle East, extending from the Iberian Peninsula in the west to the Indus in the east and to the Almoravid Dynasty and Mali Empire in the south.\n\nThe Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. The European Age of Empires saw European ideas of education in philosophy, religion, arts and sciences spread out across the globe. Missionaries and scholars also brought back new ideas from other civilizations – as with the Jesuit China missions who played a significant role in the transmission of knowledge, science, and culture between China and Europe, translating works from Europe like Euclid's Elements for Chinese scholars and the thoughts of Confucius for European audiences. The Enlightenment saw the emergence of a more secular educational outlook in Europe.\n\nIn most countries today, full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.\n\nFormal education occurs in a structured environment whose explicit purpose is teaching students. Usually, formal education takes place in a school environment with classrooms of multiple students learning together with a trained, certified teacher of the subject. Most school systems are designed around a set of values or ideals that govern all educational choices in that system. Such choices include curriculum, organizational models, design of the physical learning spaces (e.g. classrooms), student-teacher interactions, methods of assessment, class size, educational activities, and more.\n\nPreschools provide education from ages approximately three to seven, depending on the country when children enter primary education. These are also known as nursery schools and as kindergarten, except in the US, where kindergarten is a term often used to describe the earliest levels of primary education. Kindergarten \"provide[s] a child-centred, preschool curriculum for three- to seven-year-old children that aim[s] at unfolding the child's physical, intellectual, and moral nature with balanced emphasis on each of them.\"\n\nPrimary (or elementary) education consists of the first five to seven years of formal, structured education. In general, primary education consists of six to eight years of schooling starting at the age of five or six, although this varies between, and sometimes within, countries. Globally, around 89% of children aged six to twelve are enrolled in primary education, and this proportion is rising. Under the Education For All programs driven by UNESCO, most countries have committed to achieving universal enrollment in primary education by 2015, and in many countries, it is compulsory. The division between primary and secondary education is somewhat arbitrary, but it generally occurs at about eleven or twelve years of age. Some education systems have separate middle schools, with the transition to the final stage of secondary education taking place at around the age of fourteen. Schools that provide primary education, are mostly referred to as \"primary schools \"or \"elementary schools\". Primary schools are often subdivided into infant schools and junior school.\n\nIn India, for example, compulsory education spans over twelve years, with eight years of elementary education, five years of primary schooling and three years of upper primary schooling. Various states in the republic of India provide 12 years of compulsory school education based on a national curriculum framework designed by the National Council of Educational Research and Training.\n\nIn most contemporary educational systems of the world, secondary education comprises the formal education that occurs during adolescence. It is characterized by transition from the typically compulsory, comprehensive primary education for minors, to the optional, selective tertiary, \"postsecondary\", or \"higher\" education (e.g. university, vocational school) for adults. Depending on the system, schools for this period, or a part of it, may be called secondary or high schools, gymnasiums, lyceums, middle schools, colleges, or vocational schools. The exact meaning of any of these terms varies from one system to another. The exact boundary between primary and secondary education also varies from country to country and even within them but is generally around the seventh to the tenth year of schooling.\n\nSecondary education occurs mainly during the teenage years. In the United States, Canada, and Australia, primary and secondary education together are sometimes referred to as K-12 education, and in New Zealand Year 1–13 is used. The purpose of secondary education can be to give common knowledge, to prepare for higher education, or to train directly in a profession.\n\nSecondary education in the United States did not emerge until 1910, with the rise of large corporations and advancing technology in factories, which required skilled workers. In order to meet this new job demand, high schools were created, with a curriculum focused on practical job skills that would better prepare students for white collar or skilled blue collar work. This proved beneficial for both employers and employees, since the improved human capital lowered costs for the employer, while skilled employees received higher wages.\n\nSecondary education has a longer history in Europe, where grammar schools or academies date from as early as the 16th century, in the form of public schools, fee-paying schools, or charitable educational foundations, which themselves date even further back.\n\nCommunity colleges offer another option at this transitional stage of education. They provide nonresidential junior college courses to people living in a particular area.\n\nHigher education, also called tertiary, third stage, or postsecondary education, is the non-compulsory educational level that follows the completion of a school such as a high school or secondary school. Tertiary education is normally taken to include undergraduate and postgraduate education, as well as vocational education and training. Colleges and universities mainly provide tertiary education. Collectively, these are sometimes known as tertiary institutions. Individuals who complete tertiary education generally receive certificates, diplomas, or academic degrees.\n\nHigher education typically involves work towards a degree-level or foundation degree qualification. In most developed countries, a high proportion of the population (up to 50%) now enter higher education at some time in their lives. Higher education is therefore very important to national economies, both as a significant industry in its own right and as a source of trained and educated personnel for the rest of the economy.\n\nUniversity education includes teaching, research, and social services activities, and it includes both the undergraduate level (sometimes referred to as tertiary education) and the graduate (or postgraduate) level (sometimes referred to as graduate school). Some universities are composed of several colleges.\n\nOne type of university education is a liberal arts education, which can be defined as a \"college or university curriculum aimed at imparting broad general knowledge and developing general intellectual capacities, in contrast to a professional, vocational, or technical curriculum.\" Although what is known today as liberal arts education began in Europe, the term \"liberal arts college\" is more commonly associated with institutions in the United States such as Williams College or Barnard College.\n\nVocational education is a form of education focused on direct and practical training for a specific trade or craft. Vocational education may come in the form of an apprenticeship or internship as well as institutions teaching courses such as carpentry, agriculture, engineering, medicine, architecture and the arts.\n\nIn the past, those who were disabled were often not eligible for public education. Children with disabilities were repeatedly denied an education by physicians or special tutors. These early physicians (people like Itard, Seguin, Howe, Gallaudet) set the foundation for special education today. They focused on individualized instruction and functional skills. In its early years, special education was only provided to people with severe disabilities, but more recently it has been opened to anyone who has experienced difficulty learning.\n\nWhile considered \"alternative\" today, most alternative systems have existed since ancient times. After the public school system was widely developed beginning in the 19th century, some parents found reasons to be discontented with the new system. Alternative education developed in part as a reaction to perceived limitations and failings of traditional education. A broad range of educational approaches emerged, including alternative schools, self learning, homeschooling, and unschooling. Example alternative schools include Montessori schools, Waldorf schools (or Steiner schools), Friends schools, Sands School, Summerhill School, Walden's Path, The Peepal Grove School, Sudbury Valley School, Krishnamurti schools, and open classroom schools. Charter schools are another example of alternative education, which have in the recent years grown in numbers in the US and gained greater importance in its public education system.\n\nIn time, some ideas from these experiments and paradigm challenges may be adopted as the norm in education, just as Friedrich Fröbel's approach to early childhood education in 19th-century Germany has been incorporated into contemporary kindergarten classrooms. Other influential writers and thinkers have included the Swiss humanitarian Johann Heinrich Pestalozzi; the American transcendentalists Amos Bronson Alcott, Ralph Waldo Emerson, and Henry David Thoreau; the founders of progressive education, John Dewey and Francis Parker; and educational pioneers such as Maria Montessori and Rudolf Steiner, and more recently John Caldwell Holt, Paul Goodman, Frederick Mayer, George Dennison, and Ivan Illich.\n\nIndigenous education refers to the inclusion of indigenous knowledge, models, methods, and content within formal and non-formal educational systems. Often in a post-colonial context, the growing recognition and use of indigenous education methods can be a response to the erosion and loss of indigenous knowledge and language through the processes of colonialism. Furthermore, it can enable indigenous communities to \"reclaim and revalue their languages and cultures, and in so doing, improve the educational success of indigenous students.\"\n\nInformal learning is one of three forms of learning defined by the Organisation for Economic Co-operation and Development (OECD). Informal learning occurs in a variety of places, such as at home, work, and through daily interactions and shared relationships among members of society. For many learners, this includes language acquisition, cultural norms, and manners.\n\nIn informal learning, there is often a reference person, a peer or expert, to guide the learner. If learners have a personal interest in what they are informally being taught, learners tend to expand their existing knowledge and conceive new ideas about the topic being learned. For example, a museum is traditionally considered an informal learning environment, as there is room for free choice, a diverse and potentially non-standardized range of topics, flexible structures, socially rich interaction, and no externally imposed assessments.\n\nWhile informal learning often takes place outside educational establishments and does not follow a specified curriculum, it can also occur within educational settings and even during formal learning situations. Educators can structure their lessons to directly utilize their students informal learning skills within the education setting.\n\nIn the late 19th century, education through play began to be recognized as making an important contribution to child development. In the early 20th century, the concept was broadened to include young adults but the emphasis was on physical activities. L.P. Jacks, also an early proponent of lifelong learning, described education through recreation: \"A master in the art of living draws no sharp distinction between his work and his play, his labour and his leisure, his mind and his body, his education and his recreation. He hardly knows which is which. He simply pursues his vision of excellence through whatever he is doing and leaves others to determine whether he is working or playing. To himself, he always seems to be doing both. Enough for him that he does it well.\" Education through recreation is the opportunity to learn in a seamless fashion through all of life's activities. The concept has been revived by the University of Western Ontario to teach anatomy to medical students.\n\nAutodidacticism (also autodidactism) is a term used to describe self-directed learning. One may become an autodidact at nearly any point in one's life. Notable autodidacts include Abraham Lincoln (U.S. president), Srinivasa Ramanujan (mathematician), Michael Faraday (chemist and physicist), Charles Darwin (naturalist), Thomas Alva Edison (inventor), Tadao Ando (architect), George Bernard Shaw (playwright), Frank Zappa (composer, recording engineer, film director), and Leonardo da Vinci (engineer, scientist, mathematician).\n\nMany large university institutions are now starting to offer free or almost free full courses such as Harvard, MIT and Berkeley teaming up to form edX. Other universities offering open education are prestigious private universities such as Stanford, Princeton, Duke, Johns Hopkins, the University of Pennylvania, and Caltech, as well as notable public universities including Tsinghua, Peking, Edinburgh, University of Michigan, and University of Virginia.\n\nOpen education has been called the biggest change in the way people learn since the printing press. Despite favourable studies on effectiveness, many people may still desire to choose traditional campus education for social and cultural reasons.\n\nMany open universities are working to have the ability to offer students standardized testing and traditional degrees and credentials.\n\nThe conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the Open University in the United Kingdom. Presently, many of the major open education sources offer their own form of certificate. Due to the popularity of open education, these new kind of academic certificates are gaining more respect and equal \"academic value\" to traditional degrees.\n\nOut of 182 colleges surveyed in 2009 nearly half said tuition for online courses was higher than for campus-based ones.\n\nA recent meta-analysis found that online and blended educational approaches had better outcomes than methods that used solely face-to-face interaction.\n\nThe education sector or education system is a group of institutions (ministries of education, local educational authorities, teacher training institutions, schools, universities, etc.) whose primary purpose is to provide education to children and young people in educational settings. It involves a wide range of people (curriculum developers, inspectors, school principals, teachers, school nurses, students, etc.). These institutions can vary according to different contexts.\n\nSchools deliver education, with support from the rest of the education system through various elements such as education policies and guidelines – to which school policies can refer – curricula and learning materials, as well as pre- and in-service teacher training programmes. The school environment – both physical (infrastructures) and psychological (school climate) – is also guided by school policies that should ensure the well-being of students when they are in school. The Organisation for Economic Co-operation and Development has found that schools tend to perform best when principals have full authority and responsibility for ensuring that students are proficient in core subjects upon graduation. They must also seek feedback from students for quality-assurance and improvement. Governments should limit themselves to monitoring student proficiency.\n\nThe education sector is fully integrated into society, through interactions with a large number of stakeholders and other sectors. These include parents, local communities, religious leaders, NGOs, stakeholders involved in health, child protection, justice and law enforcement (police), media and political leadership.\n\nSeveral UN agencies have asserted that comprehensive sexuality education should be integrated into school curriculum.\n\nThe 2030 Agenda for Sustainable Development, adopted by the United Nations (UN) General Assembly in September 2015, calls for a new vision to address the environmental, social and economic concerns facing the world today. The Agenda includes 17 Sustainable Development Goals (SDGs), including SDG 4 on education.\n\nSince 1909, the ratio of children in the developing world attending school has increased. Before then, a small minority of boys attended school. By the start of the 21st century, the majority of all children in most regions of the world attended school.\n\nUniversal Primary Education is one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain. Securing charitable funding from prospective donors is one particularly persistent problem. Researchers at the Overseas Development Institute have indicated that the main obstacles to funding for education include conflicting donor priorities, an immature aid architecture, and a lack of evidence and advocacy for the issue. Additionally, Transparency International has identified corruption in the education sector as a major stumbling block to achieving Universal Primary Education in Africa. Furthermore, demand in the developing world for improved educational access is not as high as foreigners have expected. Indigenous governments are reluctant to take on the ongoing costs involved. There is also economic pressure from some parents, who prefer their children to earn money in the short term rather than work towards the long-term benefits of education.\n\nA study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole. Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:\n\nNearly every country now has Universal Primary Education.\n\nSimilarities – in systems or even in ideas – that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.\n\nThe Programme for International Student Assessment and the International Association for the Evaluation of Educational Achievement objectively monitor and compare the proficiency of students from a wide range of different nations.\n\nTechnology plays an increasingly significant role in improving access to education for people living in impoverished areas and developing countries. Charities like One Laptop per Child are dedicated to providing infrastructures through which the disadvantaged may access educational materials.\n\nThe OLPC foundation, a group out of MIT Media Lab and supported by several major corporations, has a stated mission to develop a $100 laptop for delivering educational software. The laptops were widely available as of 2008. They are sold at cost or given away based on donations.\n\nIn Africa, the New Partnership for Africa's Development (NEPAD) has launched an \"e-school program\" to provide all 600,000 primary and high schools with computer equipment, learning materials and internet access within 10 years. An International Development Agency project called nabuur.com, started with the support of former American President Bill Clinton, uses the Internet to allow co-operation by individuals on issues of social development.\n\nIndia is developing technologies that will bypass land-based telephone and Internet infrastructure to deliver distance learning directly to its students. In 2004, the Indian Space Research Organisation launched EDUSAT, a communications satellite providing access to educational materials that can reach more of the country's population at a greatly reduced cost.\n\nResearch into LCPS (low-cost private schools) found that over 5 years to July 2013, debate around LCPSs to achieving Education for All (EFA) objectives was polarized and finding growing coverage in international policy. The polarization was due to disputes around whether the schools are affordable for the poor, reach disadvantaged groups, provide quality education, support or undermine equality, and are financially sustainable.\nThe report examined the main challenges encountered by development organizations which support LCPSs. Surveys suggest these types of schools are expanding across Africa and Asia. This success is attributed to excess demand. These surveys found concern for:\n\nThe report showed some cases of successful voucher and subsidy programs; evaluations of international support to the sector are not widespread. Addressing regulatory ineffectiveness is a key challenge. Emerging approaches stress the importance of understanding the political economy of the market for LCPS, specifically how relationships of power and accountability between users, government, and private providers can produce better education outcomes for the poor.\n\nEducational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Although the terms \"educational psychology\" and \"school psychology\" are often used interchangeably, researchers and theorists are likely to be identified as , whereas practitioners in schools or school-related settings are identified as school psychologists. Educational psychology is concerned with the processes of educational attainment in the general population and in sub-populations such as gifted children and those with specific disabilities.\n\nEducational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. Educational psychology, in turn, informs a wide range of specialties within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks (Lucas, Blazek, & Raley, 2006).\n\nIntelligence is an important factor in how the individual responds to education. Those who have higher intelligence tend to perform better at school and go on to higher levels of education. This effect is also observable in the opposite direction, in that education increases measurable intelligence. Studies have shown that while educational attainment is important in predicting intelligence in later life, intelligence at 53 is more closely correlated to intelligence at 8 years old than to educational attainment.\n\nThere has been much interest in learning modalities and styles over the last two decades. The most commonly employed learning modalities are:\n\nOther commonly employed modalities include musical, interpersonal, verbal, logical, and intrapersonal.\n\nDunn and Dunn focused on identifying relevant stimuli that may influence learning and manipulating the school environment, at about the same time as Joseph Renzulli recommended varying teaching strategies. Howard Gardner identified a wide range of modalities in his Multiple Intelligences theories. The Myers-Briggs Type Indicator and Keirsey Temperament Sorter, based on the works of Jung, focus on understanding how people's personality affects the way they interact personally, and how this affects the way individuals respond to each other within the learning environment. The work of David Kolb and Anthony Gregorc's Type Delineator follows a similar but more simplified approach.\n\nSome theories propose that all individuals benefit from a variety of learning modalities, while others suggest that individuals may have preferred learning styles, learning more easily through visual or kinesthetic experiences. A consequence of the latter theory is that effective teaching should present a variety of teaching methods which cover all three learning modalities so that different students have equal opportunities to learn in a way that is effective for them. Guy Claxton has questioned the extent that learning styles such as Visual, Auditory and Kinesthetic(VAK) are helpful, particularly as they can have a tendency to label children and therefore restrict learning. Recent research has argued, \"there is no adequate evidence base to justify incorporating learning styles assessments into general educational practice.\"\n\nEducational neuroscience is an emerging scientific field that brings together researchers in cognitive neuroscience, developmental cognitive neuroscience, educational psychology, educational technology, education theory and other related disciplines to explore the interactions between biological processes and education. Researchers in educational neuroscience investigate the neural mechanisms of reading, numerical cognition, attention, and their attendant difficulties including dyslexia, dyscalculia, and ADHD as they relate to education. Several academic institutions around the world are beginning to devote resources to the establishment of educational neuroscience research.\n\nAs an academic field, philosophy of education is \"the philosophical study of education and its problems (...) its central subject matter is education, and its methods are those of philosophy\". \"The philosophy of education may be either the philosophy of the process of education or the philosophy of the discipline of education. That is, it may be part of the discipline in the sense of being concerned with the aims, forms, methods, or results of the process of educating or being educated; or it may be metadisciplinary in the sense of being concerned with the concepts, aims, and methods of the discipline.\" As such, it is both part of the field of education and a field of applied philosophy, drawing from fields of metaphysics, epistemology, axiology and the philosophical approaches (speculative, prescriptive or analytic) to address questions in and about pedagogy, education policy, and curriculum, as well as the process of learning, to name a few. For example, it might study what constitutes upbringing and education, the values and norms revealed through upbringing and educational practices, the limits and legitimization of education as an academic discipline, and the relation between education theory and practice.\n\nThere is no broad consensus as to what education's chief aim or aims are or should be. Some authors stress its value to the individual, emphasizing its potential for positively influencing students' personal development, promoting autonomy, forming a cultural identity or establishing a career or occupation. Other authors emphasize education's contributions to societal purposes, including good citizenship, shaping students into productive members of society, thereby promoting society's general economic development, and preserving cultural values.\n\nIn formal education, a curriculum is the set of courses and their content offered at a school or university. As an idea, curriculum stems from the Latin word for \"race course\", referring to the course of deeds and experiences through which children grow to become mature adults. A curriculum is prescriptive and is based on a more general syllabus which merely specifies what topics must be understood and to what level to achieve a particular grade or standard.\n\nAn academic discipline is a branch of knowledge which is formally taught, either at the university – or via some other such method. Each discipline usually has several sub-disciplines or branches, and distinguishing lines are often both arbitrary and ambiguous. Examples of broad areas of academic disciplines include the natural sciences, mathematics, computer science, social sciences, humanities and applied sciences.\n\nEducational institutions may incorporate fine arts as part of K-12 grade curricula or within majors at colleges and universities as electives. The various types of fine arts are music, dance, and theatre.\n\nThe Sudbury Valley School offers a model of education without a curricula.\n\nInstruction is the facilitation of another's learning. Instructors in primary and secondary institutions are often called teachers, and they direct the education of students and might draw on many subjects like reading, writing, mathematics, science and history. Instructors in post-secondary institutions might be called teachers, instructors, or professors, depending on the type of institution; and they primarily teach only their specific discipline. Studies from the United States suggest that the quality of teachers is the single most important factor affecting student performance, and that countries which score highly on international tests have multiple policies in place to ensure that the teachers they employ are as effective as possible. With the passing of NCLB in the United States (No Child Left Behind), teachers must be highly qualified. A popular way to gauge teaching performance is to use student evaluations of teachers (SETS), but these evaluations have been criticized for being counterproductive to learning and inaccurate due to student bias.\n\nCollege basketball coach John Wooden the Wizard of Westwood would teach through quick \"This not That\" technique. He would show (a) the correct way to perform an action, (b) the incorrect way the player performed it, and again (c) the correct way to perform an action. This helped him to be a responsive teacher and fix errors on the fly. Also, less communication from him meant more time that the player could practice.\n\nIt has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth. Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested by rich countries. However, technology transfer requires knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation. Therefore, a country's ability to learn from the leader is a function of its stock of \"human capital\". Recent study of the determinants of aggregate economic growth have stressed the importance of fundamental economic institutions and the role of cognitive skills.\n\nAt the level of the individual, there is a large literature, generally related to the work of Jacob Mincer, on how earnings are related to the schooling and other human capital. This work has motivated a large number of studies, but is also controversial. The chief controversies revolve around how to interpret the impact of schooling. Some students who have indicated a high potential for learning, by testing with a high intelligence quotient, may not achieve their full academic potential, due to financial difficulties.\n\nEconomists Samuel Bowles and Herbert Gintis argued in 1976 that there was a fundamental conflict in American schooling between the egalitarian goal of democratic participation and the inequalities implied by the continued profitability of capitalist production.\n\nMany countries are now drastically changing the way they educate their citizens. The world is changing at an ever quickening rate, which means that a lot of knowledge becomes obsolete and inaccurate more quickly. The emphasis is therefore shifting to teaching the skills of learning: to picking up new knowledge quickly and in as agile a way as possible. Finnish schools have even begun to move away from the regular subject-focused curricula, introducing instead developments like phenomenon-based learning, where students study concepts like climate change instead.\n\nEducation is also becoming a commodity no longer reserved for children. Adults need it too. Some governmental bodies, like the Finnish Innovation Fund Sitra in Finland, have even proposed compulsory lifelong education.\n\n\n"}
{"id": "30498454", "url": "https://en.wikipedia.org/wiki?curid=30498454", "title": "Elegant decay", "text": "Elegant decay\n\nElegant decay is the cultural agreement that some places, and structures, become gradually more elegant, notable, or beautiful as they decay, or fall into ruin, due to their historical, architectural, or cultural significance. Although such reverence is subjective, it is true that certain cities, regions or even countries are more susceptible to the general concept due to past opulence, or their lengthy or enduring history or culture.\n\nContrary to the general interpretation that places or structures are more noteworthy in their newness, or older structures hold more value and interest after being historically restored to their original state, the concept of elegant decay is that in the slow degradation of the structures an inherent elegance, or beauty, emerges due to past historic importance. In recent times, this concept has been exploited by those seeking to stimulate tourism.\n\nWith its rich history, and centuries of past prestige and prominence, Western Europe has many places where elegant decay is notable and celebrated. One of the most salient is Italy, due to its lengthy historic prominence in prehistoric, ancient, medieval, and modern times. The most celebrated of Italian cities in elegant decay is arguably Venice, largely due to the whole city slowly crumbling and sinking into its lagoon.\n\nAlthough Western Europe holds many instances of elegant decay today, the New World also has places of crumbled reverence as well. Most notable in the United States may be the city and region around New Orleans, Louisiana, with its historic fusion of cultures and important contributions to American expansion. In the concept of elegant decay, there sometimes appears to be an element of impending doom to such places and structures, as Venice slowly sinks, and New Orleans is under constant threat from hurricanes.\n\nElegant decay is different in concept and plan than ruin value, in which structures are planned to look beautiful as ruins from the early planning stages. The concept of elegant decay is related to the concept of patina.\n\n\n"}
{"id": "11657", "url": "https://en.wikipedia.org/wiki?curid=11657", "title": "Fashion", "text": "Fashion\n\nFashion is a popular style, especially in clothing, footwear, lifestyle, accessories, makeup, hairstyle and body. Fashion is a distinctive and often constant trend in the style in which people present themselves. A fashion can become the prevailing style in behaviour or manifest the newest creations of designers, technologists, engineers, and design managers.\n\nBecause the more technical term \"costume\" is regularly linked to the term \"fashion\", the use of the former has been relegated to special senses like fancy-dress or masquerade wear, while the word \"fashion\" often refers to clothing, including the study of clothing. Although aspects of fashion can be feminine or masculine, some trends are androgynous.\n\nHigh-flying trendsetters in fashion can aspire to the label \"haute couture\". But if one disapproves of a fashion, one can attempt to dismiss it as a mere fad.\n\nEarly Western travelers, traveling whether to India, Persia, Turkey or China, would frequently remark on the absence of change in fashion in those countries. The Japanese \"shōgun\"s secretary bragged (not completely accurately) to a Spanish visitor in 1609 that Japanese clothing had not changed in over a thousand years. However, there is considerable evidence in Ming China of rapidly changing fashions in Chinese clothing. Changes in costume often took place at times of economic or social change, as occurred in ancient Rome and the medieval Caliphate, followed by a long period without major changes. In 8th-century Moorish Spain, the musician Ziryab introduced to Córdoba sophisticated clothing-styles based on seasonal and daily fashions from his native Baghdad, modified by his own inspiration. Similar changes in fashion occurred in the 11th century in the Middle East following the arrival of the Turks, who introduced clothing styles from Central Asia and the Far East.\n\nThe beginning in Europe of continual and increasingly rapid change in clothing styles can be fairly reliably dated. Historians, including James Laver and Fernand Braudel, date the start of Western fashion in clothing to the middle of the 14th century, though they tend to rely heavily on contemporary imagery and illuminated manuscripts were not common before the fourteenth century. The most dramatic early change in fashion was a sudden drastic shortening and tightening of the male over-garment from calf-length to barely covering the buttocks, sometimes accompanied with stuffing in the chest to make it look bigger. This created the distinctive Western outline of a tailored top worn over leggings or trousers and one of the popular historical fashion was Often referred to as the \"Plume Boom,\" the popularity of wearing real feathers in hats reached its peak in the early 20th century. According to The Smithsonian, women adorned their hats with feathers, wings, and even full taxidermied birds. This caused a huge decline in the bird population, and even led to the exinction of some. Often referred to as the \"Plume Boom,\" the popularity of wearing real feathers in hats reached its peak in the early 20th century. According to The Smithsonian, women adorned their hats with feathers, wings, and even full taxidermied birds. This caused a huge decline in the bird population, and even led to the exinction of some. \n\nThe pace of change accelerated considerably in the following century, and women and men's fashion, especially in the dressing and adorning of the hair, became equally complex. Art historians are therefore able to use fashion with confidence and precision to date images, often to within five years, particularly in the case of images from the 15th century. Initially, changes in fashion led to a fragmentation across the upper classes of Europe of what had previously been a very similar style of dressing and the subsequent development of distinctive national styles. These national styles remained very different until a counter-movement in the 17th to 18th centuries imposed similar styles once again, mostly originating from Ancien Régime France. Though the rich usually led fashion, the increasing affluence of early modern Europe led to the bourgeoisie and even peasants following trends at a distance, but still uncomfortably close for the elites – a factor that Fernand Braudel regards as one of the main motors of changing fashion.\n\nIn the 16th century, national differences were at their most pronounced. Ten 16th century portraits of German or Italian gentlemen may show ten entirely different hats. Albrecht Dürer illustrated the differences in his actual (or composite) contrast of Nuremberg and Venetian fashions at the close of the 15th century (\"illustration, right\"). The \"Spanish style\" of the late 16th century began the move back to synchronicity among upper-class Europeans, and after a struggle in the mid-17th century, French styles decisively took over leadership, a process completed in the 18th century.\n\nThough different textile colors and patterns changed from year to year, the cut of a gentleman's coat and the length of his waistcoat, or the pattern to which a lady's dress was cut, changed more slowly. Men's fashions were largely derived from military models, and changes in a European male silhouette were galvanized in theaters of European war where gentleman officers had opportunities to make notes of foreign styles such as the \"Steinkirk\" cravat or necktie.\n\nThough there had been distribution of dressed dolls from France since the 16th century and Abraham Bosse had produced engravings of fashion in the 1620s, the pace of change picked up in the 1780s with increased publication of French engravings illustrating the latest Paris styles. By 1800, all Western Europeans were dressing alike (or thought they were); local variation became first a sign of provincial culture and later a badge of the conservative peasant.\n\nAlthough tailors and dressmakers were no doubt responsible for many innovations, and the textile industry certainly led many trends, the history of fashion design is normally understood to date from 1858 when the English-born Charles Frederick Worth opened the first true \"haute couture\" house in Paris. The Haute house was the name established by government for the fashion houses that met the standards of industry. These fashion houses have to adhere to standards such as keeping at least twenty employees engaged in making the clothes, showing two collections per year at fashion shows, and presenting a certain number of patterns to costumers and also old fashion is becoming trendy nowadays but with a trendy taste . Since then, the idea of the fashion designer as a celebrity in his or her own right has become increasingly dominant.\n\nThe idea of unisex dressing originated in the 1960s when designers such as Pierre Cardin and Rudi Gernreich created garments, such as stretch jersey tunics or leggings, meant to be worn by both males and females. The impact of unisex expands more broadly to encompass various themes in fashion including androgyny, mass-market retail, and conceptual clothing. The fashion trends of the 1970s, such as sheepskin jackets, flight jackets, duffel coats, and unstructured clothing influenced men to attend social gatherings without a tuxedo jacket and to accessorize in new ways. Some men's styles blended the sensuality and expressiveness despite the conservative trend, the growing gay-rights movement and an emphasis on youth allowed for a new freedom to experiment with style, fabrics such as wool crepe, which had previously been associated with women's attire was used by designers when creating male clothing.\n\nThe four major current fashion capitals are acknowledged to be Paris, Milan, New York City, and London, which are all headquarters to the greatest fashion companies and are renowned for their major influence on global fashion. Fashion weeks are held in these cities, where designers exhibit their new clothing collections to audiences. A succession of major designers such as Coco Chanel and Yves Saint-Laurent have kept Paris as the center most watched by the rest of the world, although \"haute couture\" is now subsidized by the sale of ready-to-wear collections and perfume using the same branding.\n\nModern Westerners have a wide number of choices available in the selection of their clothes. What a person chooses to wear can reflect his or her personality or interests. When people who have high cultural status start to wear new or different clothes, a fashion trend may start. People who like or respect these people become influenced by their personal style and begin wearing similarly styled clothes. Fashions may vary considerably within a society according to age, social class, generation, occupation, and geography and may also vary over time. If an older person dresses according to the fashion young people use, he or she may look ridiculous in the eyes of both young and older people. The terms \"fashionista\" and \"fashion victim\" refer to someone who slavishly follows current fashions.\n\nOne can regard the system of sporting various fashions as a fashion language incorporating various fashion statements using a grammar of fashion. (Compare some of the work of Roland Barthes.)\n\nIn recent years, Asian fashion has become increasingly significant in local and global markets. Countries such as China, Japan, India, and Pakistan have traditionally had large textile industries, which have often been drawn upon by Western designers, but now Asian clothing styles are also gaining influence based on their own ideas.\n\nThe notion of global fashion industry is a product of the modern age. Prior to the mid-19th century, most clothing was custom-made. It was handmade for individuals, either as home production or on order from dressmakers and tailors. By the beginning of the 20th century—with the rise of new technologies such as the sewing machine, the rise of global capitalism and the development of the factory system of production, and the proliferation of retail outlets such as department stores—clothing had increasingly come to be mass-produced in standard sizes and sold at fixed prices.\n\nAlthough the fashion industry developed first in Europe and America, , it is an international and highly globalized industry, with clothing often designed in one country, manufactured in another, and sold worldwide. For example, an American fashion company might source fabric in China and have the clothes manufactured in Vietnam, finished in Italy, and shipped to a warehouse in the United States for distribution to retail outlets internationally. The fashion industry has long been one of the largest employers in the United States, and it remains so in the 21st century. However, U.S. employment declined considerably as production increasingly moved overseas, especially to China. Because data on the fashion industry typically are reported for national economies and expressed in terms of the industry's many separate sectors, aggregate figures for world production of textiles and clothing are difficult to obtain. However, by any measure, the clothing industry accounts for a significant share of world economic output.\nThe fashion industry consists of four levels:\n\n\nThese levels consist of many separate but interdependent sectors. These sectors are Textile Design and Production, Fashion Design and Manufacturing, Fashion Retailing, Marketing and Merchandising, Fashion Shows, and Media and Marketing. Each sector is devoted to the goal of satisfying consumer demand for apparel under conditions that enable participants in the industry to operate at a profit.\n\nFashion trends are influenced by several factors including cinema, celebrities, climate, creative explorations, political, economical, social and technological. Examining these factors is called a PEST analysis. Fashion forecasters can use this information to help determine growth or decline of a particular trend. Fashion trends change daily, it can not stay unchanged\n\nNot only did political events make a huge impact on fashion trends but also the political figure played a critical role in forecasting the fashion trend. For example, First Lady Jacqueline Kennedy was a fashionable icon of the early 1960s who led formal dressing trend. By wearing a Chanel suit, a structural Givenchy shift dress or a soft color Cassini coat with huge buttons, it created her elegant look and led a delicate trend.\n\nFurthermore, political revolution also made much impact on the fashion trend. For example, during the 1960s the economy had become wealthier, divorce rate was increasing and government approved the birth control pill. This revolution inspired younger generation to rebellion. In 1964, the leg-baring miniskirt has become a major fashion trend of the 1960s. Given that fashion designers began to experiment with the shapes of garment, loose sleeveless, micro-minis, flared skirts, and trumpet sleeves. In this case, mini-skirt trend became an icon of the 1960s.\n\nMoreover, political movement built an impressive relationship with fashion trend. For instance, during Vietnam war, the youth of America made a movement that affected the whole country. In the 1960s, the fashion trend was full of fluorescent colors, prints patterns, bell-bottom jeans, fringed vests, and skirt became a protest outfit of the 1960s. This trend was called Hippie and it is still affecting current fashion trend.\n\nTechnology plays a large role in most aspects of today's society. Technological influences are growing more apparent in the fashion industry. Advances and new developments are shaping and creating current and future trends.\n\nDevelopments such as wearable technology have become an important trend in fashion and will continue with advances such as clothing constructed with solar panels that charge devices and smart fabrics that enhance wearer comfort by changing color or texture based on environmental changes.\n\nThe fashion industry is seeing how 3D printing technology has influenced designers such as Iris Van Herpen and Kimberly Ovitz. These designers have been heavily experimenting and developing 3D printed couture pieces. As the technology grows, the 3D printers will become more accessible to designers and eventually consumers, which could potentially shape the fashion industry entirely.\n\nInternet technology such as online retailers and social media platforms have given way for trends to be identified, marketed and sold immediately. Styles and trends are easily conveyed online to attract the trendsetters. Posts on Instagram or Facebook can easily increase awareness about new trends in fashion, which subsequently may create high demand for specific items or brands, new \"buy now button\" technology can link these styles with direct sales.\n\nMachine vision technology has been developed to track how fashions spread through society. The industry can now see the direct correlation on how fashion shows influence street-chic outfits. The effects can now be quantified and provide valuable feedback to fashion houses, designers and consumers regarding trends.\n\nMilitary technology has played an important role in the fashion industry. The camouflage pattern in clothing was developed to help military personnel be less visible to enemy forces. A trend emerged in the 1960s and camouflage fabric was introduced to street wear. The camouflage fabric trend disappeared and resurfaced several times since then. Camouflage started to appear in high fashion by the 1990s. Designers such as Valentino, Dior and Dolce & Gabbana combined camouflage into their runway and ready-to-wear collections.\n\nClothing is much more than fashion, it is a style of life an expression of ones self. Social media is one of the biggest platforms to exhibit and connect fashion trends with millions of people all at once worldwide. Fashion trend goes beyond the aesthetic values, it emerges as an expressive tool for designers to deliver their message about the society. With the influence of social media, celebrities and bloggers, their voice are easily being heard and have impact on fashion and trend at any time.\nFashion and music are inseparable. Prabal Gurung highlighted the importance of music to his shows, saying \"each season we want to tell a story for 10 minutes…. the perfect harmony between cloths and music allows this\". Music is a representation of fashion that expresses the abstract design concept into relatable harmony for viewers.\n\nFashion relates to social and cultural context of an environment. According to Matika, \"Elements of popular culture become fused when a person's trend is associated with a preference for a genre of music…like music, news or literature, fashion has been fused into everyday lives.\" Fashion is not only seen as pure aesthetic values; fashion is also a medium for performers to create an overall atmosphere and express their opinions altogether through music video. The latest music video ‘Formation’ by Beyoncé, according to Carlos, \"The pop star pays homage to her Creole root... tracing the roots of the Louisiana cultural nerve center from the post-abolition era to present day, Beyoncé catalogs the evolution of the city's vibrant style and its tumultuous history all at once. Atop a New Orleans police car in a red-and-white Gucci high-collar dress and combat boots, she sits among the ruins of Hurricane Katrina, immediately implanting herself in the biggest national debate on police brutality and race relations in modern day.\"\n\nRunway show is a reflection of fashion trend and a designer's thought. For designer like Vivienne Westwood, runway show is a platform for her voice on politics and current events. For her AW15 menswear show, according to Water, \"where models with severely bruised faces channeled eco-warriors on a mission to save the planet.\" Another recent example is a staged feminist protest march for Chanel's SS15 show, rioting models chanting words of empowerment with signs like \"Feminist but feminine\" and \"Ladies first.\" According to Water, \"The show tapped into Chanel's long history of championing female independence: founder Coco Chanel was a trailblazer for liberating the female body in the post-WWI era, introducing silhouettes that countered the restrictive corsets then in favour.\"\n\nAs we undergo a global economic downturn, the \"Spend now, think later\" belief is getting less relevant in our society. Today's consumer tends to be more mindful about consumption, looking for just enough and better, more durable options. People have also become more conscious of the impact their everyday consumption has on the environment and society. They're looking for ways to mediate their material desires with an aim to do more good in the world. A linear economy is slowly shifting to a circular one.\n\nIn today's linear economical system, manufacturers extract resources from the earth to make products that will soon be discarded in landfills, on the other hand, under the circular model, the production of goods operates like systems in nature, where the waste and demise of a substance becomes the food and source of growth for something new. Companies such as MUD Jeans, which is based in the Netherlands employs a leasing scheme for jeans. This Dutch company \"represents a new consuming philosophy that is about using instead of owning,\" according to MUD's website. The concept also protects the company from volatile cotton prices. Consumers pay €7.50 a month for a pair of jeans; after a year, they can return the jeans to Mud, trade them for a new pair and start another year-long lease, or keep them. MUD is responsible for any repairs during the lease period. Another ethical fashion company, Patagonia set up the first multi-seller branded store on EBay in order to facilitate secondhand sales; consumers who take the Common Threads pledge can sell in this store and have their gear listed on Patagonia.com's \"Used Gear\" section.\n\nConsumption as a share of gross domestic product in China has fallen for six decades, from 76 percent in 1952 to 28 percent in 2011. China plans to reduce tariffs on a number of consumer goods and expand its 72-hour transit visa plan to more cities in an effort to stimulate domestic consumption.\n\nThe announcement of import tax reductions follows changes in June 2015, when the government cut the tariffs on clothing, cosmetics and various other goods by half. Among the changes — easier tax refunds for overseas shoppers and accelerated openings of more duty-free shops in cities covered by the 72-hour visa scheme. The 72-hour visa was introduced in Beijing and Shanghai in January 2013 and has been extended to 18 Chinese cities.\n\nAccording to reports at the same time, Chinese consumer spending in other countries such as Japan has slowed even though the yen has dropped. There is clearly a trend in the next 5 years that the domestic fashion market will show an increase.\n\nConsumers all have different needs and demands that have to be suited. A person's needs change frequently. An important factor to take into consideration when thinking of consumers' needs is the key demographics of the customer. Gender, age, income, and even profession can help a company better understand the needs of their customers.\n\nFor example, a woman who is pregnant could be looking for diapers, baby strollers, and maternity clothes. Her needs would differ greatly from a woman with children that just went off to college or a teen entering high school.\n\nOften consumers need to be told what they want. Fashion companies have to do their research to ensure they know their customers' needs before developing solutions. Steve Jobs said, \"You’ve got to start with the customer experience and work backwards to the technology. You cannot start with the technology and try to figure out where you are going to sell it\".\n\nThe best way to understand the consumers' needs and therefore predict fashion trends is through market research. There are two research methods: primary and secondary. Secondary methods are taking other information that has already been collected, for example using a book or an article for research. Primary research is collecting data through surveys, interviews, observation, and/or focus groups.\n\nBenefits of primary research is specific information about a fashion brand's consumer is explored. Surveys are helpful tools; questions can be open-ended or closed-ended. A negative factor surveys and interviews present is that the answers can be biased, due to wording in the survey or on face-to-face interactions. Focus groups, about 8 to 12 people, can be beneficial because several points can be addressed in depth. However, there are drawbacks to this tactic, too. With such a small sample size, it is hard to know if the greater public would react the same way as the focus group. Observation can really help a company gain insight on what a consumer truly wants. There is less of a bias because consumers are just performing their daily tasks, not necessarily realizing they are being observed. For example, observing the public by taking street style photos of people, the consumer did not get dressed in the morning knowing that would have their photo taken necessarily. They just wear what they would normally wear. Through observation patterns can be seen, helping trend forecasters know what their target market needs and wants.\n\nKnowing the needs of the consumers will increase a fashion companies' sales and profits. Through research and studying the consumers' lives the needs of the customer can be obtained and help fashion brands know what trends the consumers are ready for.\n\nFashion breathes on media and medium .\nThe media plays a significant role when it comes to fashion. For instance, an important part of fashion is fashion journalism. Editorial critique, guidelines, and commentary can be found on television and in magazines, newspapers, fashion websites, social networks, and fashion blogs. In recent years, fashion blogging and YouTube videos have become a major outlet for spreading trends and fashion tips, creating an online culture of sharing one's style on a website or Instagram account. Through these media outlets readers and viewers all over the world can learn about fashion, making it very accessible.\n\nAt the beginning of the 20th century, fashion magazines began to include photographs of various fashion designs and became even more influential than in the past. In cities throughout the world these magazines were greatly sought after and had a profound effect on public taste in clothing. Talented illustrators drew exquisite fashion plates for the publications which covered the most recent developments in fashion and beauty. Perhaps the most famous of these magazines was \"La Gazette du Bon Ton\", which was founded in 1912 by Lucien Vogel and regularly published until 1925 (with the exception of the war years).\n\"Vogue\", founded in the United States in 1892, has been the longest-lasting and most successful of the hundreds of fashion magazines that have come and gone. Increasing affluence after World War II and, most importantly, the advent of cheap color printing in the 1960s, led to a huge boost in its sales and heavy coverage of fashion in mainstream women's magazines, followed by men's magazines in the 1990s. One such example of \"Vogue\"'s popularity is the younger version, \"Teen Vogue\", which covers clothing and trends that are targeted more toward the \"fashionista on a budget\". Haute couture designers followed the trend by starting ready-to-wear and perfume lines which are heavily advertised in the magazines and now dwarf their original couture businesses. A recent development within fashion print media is the rise of text-based and critical magazines which aim to prove that fashion is not superficial, by creating a dialogue between fashion academia and the industry. Examples of this trend are: \"Fashion Theory\" (1997) and \"Vestoj\" (2009). Television coverage began in the 1950s with small fashion features. In the 1960s and 1970s, fashion segments on various entertainment shows became more frequent, and by the 1980s, dedicated fashion shows such as \"Fashion Television\" started to appear. \"FashionTV\" was the pioneer in this undertaking and has since grown to become the leader in both Fashion Television and new media channels. The Fashion Industry is beginning to promote their styles through Bloggers on social media's. Vogue specified Chiara Ferragni as \"blogger of the moment\" due to the rises of followers through her Fashion Blog, that became popular.\n\nA few days after the 2010 Fall Fashion Week in New York City came to a close, \"The New Islander\"'s Fashion Editor, Genevieve Tax, criticized the fashion industry for running on a seasonal schedule of its own, largely at the expense of real-world consumers. \"Because designers release their fall collections in the spring and their spring collections in the fall, fashion magazines such as \"Vogue\" always and only look forward to the upcoming season, promoting parkas come September while issuing reviews on shorts in January\", she writes. \"Savvy shoppers, consequently, have been conditioned to be extremely, perhaps impractically, farsighted with their buying.\"\n\nThe fashion industry has been the subject of numerous films and television shows, including the reality show \"Project Runway\" and the drama series \"Ugly Betty\". Specific fashion brands have been featured in film, not only as product placement opportunities, but as bespoke items that have subsequently led to trends in fashion.\n\nVideos in general have been very useful in promoting the fashion industry. This is evident not only from television shows directly spotlighting the fashion industry, but also movies, events and music videos which showcase fashion statements as well as promote specific brands through product placements.\n\nFashion public relations involves being in touch with a company's audiences and creating strong relationships with them, reaching out to media and initiating messages that project positive images of the company. Social media plays an important role in modern-day fashion public relations; enabling practitioners to reach a wide range of consumers through various platforms.\n\nBuilding brand awareness and credibility is a key implication of good public relations. In some cases, great hype is built about new designers' collections before they are released into the market, due to the immense exposure generated by practitioners. Social media, such as blogs, micro blogs, podcasts, photo and video sharing sites have all become increasingly important to fashion public relations. The interactive nature of these platforms allows practitioners to engage and communicate with publics in real time, and tailor their clients' brand or campaign messages to the target audience. With blogging platforms such as Instagram, Tumblr, Wordpress, and other sharing sites, bloggers have emerged as expert fashion commentators, shaping brands and having a great impact on what is ‘on trend’. Women in the fashion public relations industry such as Sweaty Betty PR founder Roxy Jacenko and Oscar de la Renta's PR girl Erika Bearman, have acquired copious followers on their social media sites, by providing a brand identity and a behind the scenes look into the companies they work for.\n\nSocial media is changing the way practitioners deliver messages, as they are concerned with the media, and also customer relationship building. PR practitioners must provide effective communication among all platforms, in order to engage fashion publics in an industry socially connected via online shopping. Consumers have the ability to share their purchases on their personal social media pages (such as Facebook, Twitter, Instagram, etc.), and if practitioners deliver the brand message effectively and meet the needs of its publics, word-of-mouth publicity will be generated and potentially provide a wide reach for the designer and their products.\n\nAnthropology, the study of culture and human societies, studies fashion by asking why certain styles are deemed socially appropriate and others are not. A certain way is chosen and that becomes the fashion as defined by a certain people as a whole, so if a particular style has a meaning in an already occurring set of beliefs that style will become fashion. According to Ted Polhemus and Lynn Procter, fashion can be described as adornment, of which there are two types: fashion and anti-fashion. Through the capitalization and commoditisation of clothing, accessories, and shoes, etc., what once constituted anti-fashion becomes part of fashion as the lines between fashion and anti-fashion are blurred.\n\nThe definition of fashion and anti-fashion is as follows: Anti-fashion is fixed and changes little over time. Anti-fashion is different depending on the cultural or social group one is associated with or where one lives, but within that group or locality the style changes little. Fashion is the exact opposite of anti-fashion. Fashion changes very quickly and is not affiliated with one group or area of the world but is spread out throughout the world wherever people can communicate easily with each other. For example, Queen Elizabeth II's 1953 coronation gown is an example of anti-fashion because it is traditional and does not change over any period whereas a gown from fashion designer Dior's collection of 1953 is fashion because the style will change every season as Dior comes up with a new gown to replace the old one. In the Dior gown the length, cut, fabric, and embroidery of the gown change from season to season. Anti-fashion is concerned with maintaining the status quo while fashion is concerned with social mobility. Time is expressed in terms of continuity in anti-fashion and as change in fashion. Fashion has changing modes of adornment while anti-fashion has fixed modes of adornment. Indigenous and peasant modes of adornment are an example of anti-fashion. Change in fashion is part of the larger system and is structured to be a deliberate change in style.\n\nToday, people in rich countries are linked to people in poor countries through the commoditization and consumption of what is called fashion. People work long hours in one area of the globe to produce things that people in another part of the globe are anxious to consume. An example of this is the chain of production and consumption of Nike shoes, which are produced in Taiwan and then purchased in North America. At the production end, there is nation-building a hard working ideology that leads people to produce and entices people to consume with a vast amount of goods for the offering. Commodities are no longer just utilitarian but are fashionable, be they running shoes or sweat suits.\n\nThe change from anti-fashion to fashion because of the influence of western consumer-driven civilization can be seen in eastern Indonesia. The ikat textiles of the Ngada area of eastern Indonesia are changing because of modernization and development. Traditionally, in the Ngada area there was no idea similar to that of the Western idea of fashion, but anti-fashion in the form of traditional textiles and ways to adorn oneself were widely popular. Textiles in Indonesia have played many roles for the local people. Textiles defined a person's rank and status; certain textiles indicated being part of the ruling class. People expressed their ethnic identity and social hierarchy through textiles. Because some Indonesians bartered ikat textiles for food, the textiles constituted economic goods, and as some textile design motifs had spiritual religious meanings, textiles were also a way to communicate religious messages.\n\nIn eastern Indonesia, both the production and use of traditional textiles have been transformed as the production, use and value associated with textiles have changed due to modernization. In the past, women produced the textiles either for home consumption or to trade with others. Today, this has changed as most textiles are not being produced at home. Western goods are considered modern and are valued more than traditional goods, including the sarong, which retain a lingering association with colonialism. Now, sarongs are used only for rituals and ceremonial occasions, whereas western clothes are worn to church or government offices. Civil servants working in urban areas are more likely than peasants to make the distinction between western and traditional clothes. Following Indonesia's independence from the Dutch, people increasingly started buying factory made shirts and sarongs. In textile-producing areas the growing of cotton and production of naturally colored thread became obsolete. Traditional motifs on textiles are no longer considered the property of a certain social class or age group. Wives of government officials are promoting the use of traditional textiles in the form of western garments such as skirts, vests and blouses. This trend is also being followed by the general populace, and whoever can afford to hire a tailor is doing so to stitch traditional ikat textiles into western clothes. Thus, traditional textiles are now fashion goods and are no longer confined to the black, white and brown colour palette but come in array of colours. Traditional textiles are also being used in interior decorations and to make handbags, wallets and other accessories, which are considered fashionable by civil servants and their families. There is also a booming tourist trade in the eastern Indonesian city of Kupang where international as well as domestic tourists are eager to purchase traditionally printed western goods.\n\nThe use of traditional textiles for fashion is becoming big business in eastern Indonesia, but these traditional textiles are losing their ethnic identity markers and are being used as an item of fashion.\n\nIn the fashion industry, intellectual property is not enforced as it is within the film industry and music industry. Robert Glariston, an intellectual property expert, mentioned in a fashion seminar held in LA that \"Copyright law regarding clothing is a current hot-button issue in the industry. We often have to draw the line between designers being inspired by a design and those outright stealing it in different places.\" To take inspiration from others' designs contributes to the fashion industry's ability to establish clothing trends. For the past few years, WGSN has been a dominant source of fashion news and forecasts in encouraging fashion brands worldwide to be inspired by one another. Enticing consumers to buy clothing by establishing new trends is, some have argued, a key component of the industry's success. Intellectual property rules that interfere with this process of trend-making would, in this view, be counter-productive. On the other hand, it is often argued that the blatant theft of new ideas, unique designs, and design details by larger companies is what often contributes to the failure of many smaller or independent design companies.\n\nSince fakes are distinguishable by their poorer quality, there is still a demand for luxury goods, and as only a trademark or logo can be copyrighted, many fashion brands make this one of the most visible aspects of the garment or accessory. In handbags, especially, the designer's brand may be woven into the fabric (or the lining fabric) from which the bag is made, making the brand an intrinsic element of the bag.\n\nIn 2005, the World Intellectual Property Organization (WIPO) held a conference calling for stricter intellectual property enforcement within the fashion industry to better protect small and medium businesses and promote competitiveness within the textile and clothing industries.\n\nThere has been great debate about politics' place in fashion and traditionally, the fashion industry has maintained a rather apolitical stance. Considering the U.S.'s political climate in the surrounding months of the 2016 presidential election, during 2017 fashion weeks in London, Milan, New York, Paris and São Paulo amongst others, many designers took the opportunity to take political stances leveraging their platforms and influence to reach the masses.\n\nAiming to “amplify a greater message of unity, inclusion, diversity, and feminism in a fashion space”, Mara Hoffman invited the founders of the \"Women's March on Washington\" to open her show which featured modern silhouettes of utilitarian wear, described by critics as “Made for a modern warrior” and “Clothing for those who still have work to do”. Prabal Gurung debuted his collection of T-shirts featuring slogans such as “The Future is Female”, “We Will Not Be Silenced”, and “Nevertheless She Persisted”, with proceeds going to the ACLU, Planned Parenthood, and Gurung's own charity, “Shikshya Foundation Nepal”. Similarly, \"The Business of Fashion\" launched the \"#TiedTogether\" movement on Social Media, encouraging member of the industry from editors to models, to wear a white bandana advocating for “unity, solidarity, and inclusiveness during fashion week”.\n\nFashion may be used to promote a cause, such as to promote healthy behavior, to raise money for a cancer cure, or to raise money for local charities such as the Juvenile Protective Association or a children's hospice.\n\nOne fashion cause is trashion, which is using trash to make clothes, jewelery, and other fashion items in order to promote awareness of pollution. There are a number of modern trashion artists such as Marina DeBris, Ann Wizer, and Nancy Judd.\n\n\n"}
{"id": "9295770", "url": "https://en.wikipedia.org/wiki?curid=9295770", "title": "Festivals (book)", "text": "Festivals (book)\n\nFestivals is a 1973 anthology of festival-related folklore from around the world that have been compiled by Ruth Manning-Sanders. According to the book's dust jacket, \"This potpourri of festivals reveals fascinating customs and celebrations from many countries of the world. Each special day is preceded by background material on the origins of the holiday.\"\n\nSome of the special days covered are (using Manning-Sanders' words and spellings): New Year's Day, Saint Bride's Day, the Japanese Snow Festival, Saint Valentine's Day, Saint Patrick's Day, Shrove Tuesday, Mardi Gras, Easter Day, All Fools' Day, the Bright Weather Festival, Saint George's Day, May Day, the Padstow Hobby Horse, Independence Day, Michaelmas Day, Saint Crispin's Day, Hallow E'en, the Fifth of November, Hogmanay, and Christmas.\n\nFor many of the festivals, the book includes the writings of some famous authors or historical personalities. Among those included in the book are Robert Herrick, Fiona Macleod, Marco Polo, John Donne, Sir Charles Lyall, Norman Hunter, Chiang Yee, Flora Thompson, Laurie Lee, Laura Ingalls Wilder, Dylan Thomas, William Shakespeare, Richard Cobbold, P. L. Travers, Oliver Herford, Alison Uttley, Richard Crashaw, Jon and Rumer Godden, and Alfred Tennyson.\n\nEditor Manning-Sanders, a poet and author who was perhaps best known for her series of children's books in which she collected and retold fairy tales, contributes four pieces to this book:\n\n"}
{"id": "2148941", "url": "https://en.wikipedia.org/wiki?curid=2148941", "title": "Four Symbols (China)", "text": "Four Symbols (China)\n\nThe Four Symbols (, literally meaning \"four images\") are four mythological creatures in the Chinese constellations. They are the Azure Dragon of the East, the Vermilion Bird of the South, the White Tiger of the West, and the Black Turtle of the North. Each one of them represents a direction and a season, and each has its own individual characteristics and origins. Symbolically and as part of spiritual and religious belief, they have been culturally important across countries in the East Asian cultural sphere.\n\nThe Four Symbols were given human names after Daoism became popular. The Azure Dragon has the name Meng Zhang (), the Vermilion Bird was called Ling Guang (), the White Tiger Jian Bing (), and the Black Turtle Zhi Ming ().\n\nIn 1987, a tomb was found at Xishuipo (西水坡) in Puyang, Henan. There were some clam shells and bones forming the images of the Azure Dragon, the White Tiger, and the Big Dipper. It is believed that the tomb belongs to the Neolithic Age, dating to about 6,000 years ago.\n\nThe \"Rongcheng Shi\" manuscript recovered in 1994 gives five directions rather than four and places the animals quite differently: Yu the Great gave banners to his people marking the north with a bird, the south with a snake, the east with the sun, the west with the moon, and the center with a bear.\n\nThe colours of the animals also match the colours of soil in the corresponding areas of China: the bluish-grey water-logged soils of the east, the reddish iron-rich soils of the south, the whitish saline soils of the western deserts, the black organic-rich soils of the north and the yellow soils from the central loess plateau.\n\nThese mythological creatures have also been synthesized into the five principles system. The Azure Dragon of the East represents Wood, the Vermilion Bird of the South represents Fire, the White Tiger of the West represents Metal, and the Black Turtle (or Dark Warrior) of the North represents Water. In this system, the fifth principle Earth is represented by the Yellow Dragon of the Center.\n\nThe four beasts each represent a season. The Azure Dragon of the East represents Spring, the Vermilion Bird of the South represents Summer, the White Tiger of the West represents Autumn, and the Black Turtle of the North represents Winter.\n\n\n"}
{"id": "1189936", "url": "https://en.wikipedia.org/wiki?curid=1189936", "title": "French maid", "text": "French maid\n\nFrench maid is a strongly modified style of servant's dress that evolved from typical housemaid's black-and-white afternoon uniforms of 19th-century France (and their later use by stereotypical soubrette characters in burlesque dramas and bedroom farces). Some styles are conservative while others are revealing. The French maid costume is often used in cosplay, sexual roleplaying, and uniform fetishism. Depending on design details, some forms can be classified as lingerie.\n\nThough not strict to historically accurate uniforms, the French maid outfit has an easily recognizable pattern and black-and white theme that remains the template for other forms of the costume.\n\nThe typical French maid costume includes:\n\nOptional accessories depend on design and context:\n\nThe outfits are frequently worn to costume parties (fancy dress), and also used in drama/theater.\n\n"}
{"id": "48673469", "url": "https://en.wikipedia.org/wiki?curid=48673469", "title": "Gerald Prince", "text": "Gerald Prince\n\nGerald J. Prince (born November 7, 1942 in Alexandria, Egypt) is an American academic and literary theoretician. He is Professor of Romance Languages at the University of Pennsylvania, where he is also affiliated with department of Linguistics and the Program in Comparative Literature, and with the Annenberg School for Communication. He received his Ph.D. from Brown University (1968). He is a leading scholar of narrative poetics, and has helped to shape the discipline of narratology, developing key concepts such as the narratee, narrativity, the disnarrated, and narrative grammar. In addition to his theoretical work, he is a distinguished critic of contemporary French literature, and he is regarded as an authority on the French novel of the twentieth century. His writings in French and English have been translated into many other languages, and he has been a Visiting Professor at universities in France, Belgium, Italy, Australia, and Canada, as well as the United States. He is the General Editor of the \"Stages\" series at the University of Nebraska Press, and he serves on more than a dozen other editorial and advisory boards. In 2013 he received the Wayne C. Booth Lifetime Achievement Award from the International Society for the Study of Narrative, an organization that he presided in 2007.\n\n\n"}
{"id": "2499378", "url": "https://en.wikipedia.org/wiki?curid=2499378", "title": "Human dartboard", "text": "Human dartboard\n\nThe human dartboard is a contemporary sideshow act in which one person throws darts at another person. Usually the thrower is the target's assistant or a member of the audience. This act was popularized by the Jim Rose Circus, where it was performed by Jim, in the role of the target, and his wife Bebe, who threw the darts. Usually darts are thrown into the back of the target, and the spine is protected by a board or other long object stuck into the waistband.\n\n\n"}
{"id": "3559307", "url": "https://en.wikipedia.org/wiki?curid=3559307", "title": "Hybridity", "text": "Hybridity\n\nHybridity, in its most basic sense, refers to mixture. The term originates from biology and was subsequently employed in linguistics and in racial theory in the nineteenth century. Its contemporary uses are scattered across numerous academic disciplines and is salient in popular culture. Hybridity is used in discourses about race, postcolonialism, identity, anti-racism and multiculturalism, and globalization, developed from its roots as a biological term.\n\nHybridity is a cross between two separate races, plants or cultures. A hybrid is something that is mixed, and hybridity is simply mixture. Hybridity is not a new cultural or historical phenomenon. It has been a feature of all civilizations since time immemorial, from the Sumerians through the Egyptians, Greeks and Romans to the present. Both ancient and modern civilizations have, through trade and conquests, borrowed foreign ideas, philosophies, and sciences, thus producing hybrid cultures and societies. The term hybridity itself is not a modern coinage. It was common among the Greeks and Romans. In Latin hybrida or ibrida refers to \"the offspring of a tame sow and a wild boar,\" and by extension to the progeny of a Roman man and a non-Roman woman. The word hybridity was in use in English since the early 17th century and gained popular currency in the 19th century. Charles Darwin used this term in 1837 in reference to his experiments in cross-fertilization in plants. The concept of hybridity was fraught with negative connotations from its incipience. The Greeks and Romans borrowed extensively from other civilizations, the Egyptians and Persians in particular, and creating \"ipso facto\" hybridized cultures, but regarded unfavourably biological hybridity. Aristotle, Plato and Pericles were all opposed to racial mixing between Greeks and \"barbarians\" and viewed biological hybridity as a source of racial degeneration and social disorder. Similarly, within the Roman Empire, which is considered as one of the most multi-ethnic empires, cultural difference was usually integrated into the predominant culture, whereas biological hybridity was condemned. The Romans’ attitudes to racial mixing hardened from the 4th century AD when Rome embraced the Christian faith. This is manifest in the Codex Theodosianus (AD365) which prohibited marriages between Christians and non-Christians, the Jews in particular, and inflicted death penalty on those who did not obey this law. Contempt for biological hybridity did not end with the fall the Roman Empire, but continued throughout the Middle Ages and well into modern times, reaching a peak in the nineteenth century with the rise of Europe into an unrivalled imperial power. Hybridity and fear of racial degeneration caused by the mixing of Europeans and non-Europeans were major concerns in 19th century colonialist discourse prompted by racist pseudo-scientific discourses found in such works as Joseph Arthur de Gobineau's \"Essai sur l’inégalité des races\" and Joseph-Ernest Renan's \"L’Education culturelle et morale\".\n\nAs an explicative term, hybridity became a useful tool in forming a fearful discourse of racial mixing that arose toward the end of the 18th century. Pseudo-scientific models of anatomy and craniometry were used to argue that Africans, Asians, Native Americans, and Pacific Islanders were racially inferior to Europeans. The fear of miscegenation that followed responds to the concern that the offspring of racial interbreeding would result in the dilution of the European race. Hybrids were seen as an aberration, worse than the inferior races, a weak and diseased mutation. Hybridity as a concern for racial purity responds clearly to the zeitgeist of colonialism where, despite the backdrop of the humanitarian age of enlightenment, social hierarchy was beyond contention as was the position of Europeans at its summit. The social transformations that followed the ending of colonial mandates, rising immigration, and economic liberalization profoundly altered the use and understanding of the term hybridity.\n\n\"Hybrid talk\", the rhetoric of hybridity, is fundamentally associated with the emergence of post-colonial discourse and its critiques of cultural imperialism. It is the second stage in the history of hybridity, characterized by literature and theory that study the effects of mixture (hybridity) upon identity and culture. The principal theorists of hybridity are Homi Bhabha, Néstor García Canclini, Stuart Hall, Gayatri Spivak, and Paul Gilroy, whose works respond to the multi-cultural awareness that emerged in the early 1990s.\n\nIn the theoretic development of hybridity, the key text is \"The Location of Culture\" (1994), by Homi Bhabha, wherein the liminality of hybridity is presented as a paradigm of colonial anxiety. The principal proposition is the hybridity of colonial identity, which, as a cultural form, made the colonial masters ambivalent, and, as such, altered the authority of power; as such, Bhabha's arguments are important to the conceptual discussion of \"hybridity\". Hybridity demonstrates how cultures come to be represented by processes of iteration and translation through which their meanings are vicariously addressed to—through—an Other. This contrasts any \"essentialist claims for the inherent authenticity or purity of cultures which, when inscribed in the naturalistic sign of symbolic consciousness frequently become political arguments for the hierarchy and ascendary of powerful cultures.\" This also means that the colonial subject takes place, its subaltern position inscribed in that space of iteration. The colonial subject is located in a place of hybridity, its identity formed in a space of iteration and translation by the colonizer. Bhabha emphasizes that \"the discriminatory effects of the discourse of cultural colonialism, for instance, do not simply or singly refer to a ‘person’… or to a discrimination between mother culture and alien culture…the reference of discrimination is always to a process of splitting as the condition of subjection: a discrimination between the mother and its bastards, the self and its doubles, where the trace of what is disavowed is not repressed but repeated as something different—a mutation.\" Like mimicry, hybridity is a metonymy of presence. Hybridity opens up a space, figuratively speaking, where the construction of a political object that is new, neither the colonizer nor the Other, properly defies our political expectations. However, like Bhabha's concept of mimicry, hybridity is a doubling, dissembling image of being in at least two places at once. This turn in the effect of hybridity makes the presence of colonist authority no longer immediately visible.\n\nBhabha includes interpretations of hybridity in postcolonial discourse. One is that he sees hybridity as a strategic reversal of the process domination through disavowal. Hybridity reevaluates the assumption of colonial identity through the repetition of discriminatory identity effects. In this way, hybridity can unsettle the narcissist demands of colonial power, but reforms its identifications in strategies of subversion that turn the gaze of the discriminated back upon the colonist. Therefore, with this interpretation, hybridity represents that ambivalent ‘turn’ of the subject into the anxiety-causing object of \"paranoid classification—a disturbing questioning of the images and presences of authority\". The hybrid retains the actual semblance of the authoritative symbol but reforms its presence by denying it as the signifier of disfigurement—after the intervention of difference. In turn, mimicry is the effect of hybridity. First, the metonymy of presence supports the authoritarian voyeurism, but then as discrimination turns into the assertion of the hybrid, the sign of authority becomes a mask, a mockery.\n\nAlthough the original, theoretic development of hybridity addressed the narratives of cultural imperialism, Bhabha's work also comprehends the cultural politics of the condition of being \"a migrant\" in the contemporary metropolis. Yet hybridity no longer is solely associated with migrant populations and with border towns, it also applies contextually to the flow of cultures and their interactions.\n\nThat critique of cultural imperialist hybridity meant that the rhetoric of hybridity progressed to challenging essentialism, and is applied to sociological theories of identity, multiculturalism, and racism. Moreover, \"polyphony\" is another important element of hybridity theory, by Mikhail Bakhtin, which is applied to hybrid discourses presented in folklore and anthropology.\n\nThe development of hybridity theory as a discourse of anti-essentialism marked the height of the popularity of academic \"hybridity talk\". However the usage of hybridity in theory to eliminate essentialist thinking and practices (namely racism) failed as hybridity itself is prone to the same essentialist framework and thus requires definition and placement. A number of arguments have followed in which promoters and detractors argue the uses of hybridity theory. Much of this debate can be criticized as being excessively bogged down in theory and pertaining to some unhelpful quarrels on the direction hybridity should progress e.g. attached to racial theory, post-colonialism, cultural studies, or globalization. Sociologist Jan Nederveen Pieterse highlights these core arguments in a debate that promotes hybridity.\n\nSome on the left, such as cultural theorist John Hutnyk, have criticized hybridity as politically void. Others like Aijaz Ahmad, Arif Dirlik, and Benita Parry blame Homi Bhabha for recycling obscure psychoanalytic and postmodern theories of culture and identity. Ahmad criticizes Bhabha for establishing a postcolonial theory which overlooks the material colonial context and post-independence realities of the former colonies. He writes: \"Between postcoloniality as it exists in a former colony like India, and postcoloniality as the condition of discourse by such critics as Bhabha, there would appear to be a considerable gap\". Dirlik follows in a similar vein, stressing the postcolonial theorists’ propensity to flatten out cultural difference under the umbrella term of hybridity: \"Africa, Caribbean, South-Asian literatures come from different places and different histories, and not merely different from France, but different from each other. It is this real sort of difference that disappears in postcolonial studies\". In \"Signs of our Time\" Benita Parry discusses \"The Location of Culture\" and criticizes the \"linguistic turn\" in cultural studies, more particularly, Bhabha's dependence on fuzzy psychoanalytical and linguistic explanations of cultural identities, or what she calls the \"autarchy of the signifier\". \"In Postcolonial Studies: a materialist critique,\" she further rails against the \"linguistic turn\" and recommends a materialist postcolonial critique that addresses colonialism's epistemic violence within the wider context of the economic exploitation of the colonized masses by imperial capitalism.\n\nMore recently, Amar Acheraiou in \"Questioning Hybridity, Postcolonialism and Globalization\" challenges Bhabha's theory of hybridity on theoretical as well as ideological and historical grounds. He criticizes Bhabha for examining hybridity from a narrow, \"synchronic\" perspective confined to the 19th century, instead of adopting a \"diachronic\" view which renders better this concept's historical depth. He also reproaches this theorist for stripping the notion of hybridity off its constitutive racial connotations and considers this as an essentialist gesture. According to him, by clearing this concept of its negative biological associations, Bhabha evades the discussion of the problematic issue of race and racism, which should, paradoxically, be a central concern in hybridity theory. He further argues that Bhabha overlooks the fact that there are still today several places across the world where for many biologically hybrids, hybridity or \"the third space\" often proves \"the space of the impossible\" rather than a site of cultural and racial emancipation. The new theory of hybridity that Acheraiou develops in this book departs from the strictly \"cultural and spatial paradigm\" of postcolonial theory, or what he calls \"angelic hybridism.\" It is a broadly historical and multi-layered form of hybridity focused on the nebulous political, economic, and ideological power structures, emancipatory as well as oppressive, which have presided over the discourse and practice of hybridity since the dawn of civilization. He calls this alternative mode of rethinking postcoloniality \"a radical ethics of hybridity,\" which is \"global in scope and planetary in aspiration\". Furthermore, he stresses that this \"resistive planetary hybridity\" is not \"confined to the migrant, diasporic condition,\" and has \"as many centres of consciousness as geographical points of origin\".\n\nThe next phase in the use of the term has been to see hybridity as a cultural effect of globalization. For example, hybridity is presented by Kraidy as the ‘cultural logic’ of globalization as it \"entails that traces of other cultures exist in every culture, thus offering foreign media and marketers transcultural wedges for forging affective links between their commodities and local communities.\" Another promoter of hybridity as globalization is Jan Nederveen Pieterse, who asserts hybridity as the rhizome of culture. He argues that globalization as hybridization opposes views which see the process as homogenizing, modernizing, and westernizing, and that it broadens the empirical history of the concept. However neither of these scholars have reinvigorated the hybridity theory debate in terms of solving its inherent problematics. The term hybridity remains contested precisely because it has resisted the appropriations of numerous discourses despite the fact that it is radically malleable. For example, young Muslims in Indonesia are followers of Islam but have \"synthesized\" trends from global culture in ways that respect religious tradition. These include drinking non-alcoholic beer, using Koranic apps on their iPhones, and buying \"halal\" cosmetics. In anti-Western countries, youth who try to create cultural hybridity through clothing conflict with the traditional views of modesty in their religion. Conflict occurs across generations when older adults clash with youth over youth attempts to change traditions.\n\nLanguages are all hybrid, in varying degrees. For centuries people borrowed from foreign languages, creating thus hybrid linguistic idioms. They did so for commercial, aesthetic, ideological and technological reasons (to facilitate trade transactions, express philosophical or scientific ideas unavailable in their original idioms, enrich and adapt their languages to new realities, subvert a dominant colonial literary canon by deliberately introducing words from the colonized peoples' idiom). Trade and colonization have been the main vehicles of linguistic hybridization across history. Since the classical conquests, both the colonizers and colonized tapped into each other's languages. The Greeks soaked up many mathematical and astronomical concepts from the Egyptians. The Romans, too, absorbed much of Greek culture and ideas. They also drew abundantly from the \"barbarians\". In \"Taktika\", Arrian (92–175 AD), a Greek historian and philosopher of the Roman period, drew attention to the Romans' indebtedness to their colonial subjects, arguing that \"the Romans have many foreign (Iberian, Celtic) terms for formations, for they used Celtic cavalry\". In modern times, the French and British resorted to similar linguistic appropriations throughout their conquests. The French language, for example, contains over 200 Arabic and Berber words, most of which were taken up during France's colonization of Algeria. Similarly, hundreds of Indian words entered the English idiom from the seventeenth to the nineteenth century. According to \"The Oxford English Dictionary,\" 900 English words are of Indian origin. Linguistic hybridity was manifest in these colonial contexts, but was acknowledged by neither the colonizers nor the colonized. More still, while these linguistic borrowings had, de facto, rendered colonial languages hybrid and therefore impure, the myth of linguistic purity and superiority, inherited from the ancient Greeks’ \"linguistic racialism\", held firmly among the European colonizers. The Greek word ‘barbarian,’ which was used to refer to non-Greek languages’ inferiority, backwardness and inarticulacy, was adopted by the French since the 16th century. It was often applied to the Basque, Breton and Occitan languages and to their speakers. Abbé Grégoire recommended wiping out these \"crude idioms\" and forcing French on the Basques, Bretons and Occitans to \"spread enlightened ideas (...), well-being and political tranquillity\". According to him, this would \"banish superstition\" and \"simplify the mechanism of the political machine.\" It would, above all, \"mould the citizens into a national whole\" In Britain, this Aristotelian view of language was revitalized by authors like Jonathan Swift, Samuel Johnson, and Matthew Arnold, who cast respectively the Irish, Scots and Welsh as \"rude\" and \"backward\", attributing these peoples’ intellectual and economic \"backwardness\" to their \"inferior\" languages.\n\nLinguistic and cultural hybridity is a \"dual dynamics\" which operates \"passively\" as well as \"actively\". Mikhail Bakhtin distinguished two types of hybridity: \"organic\" or \"unconscious\" hybridity and \"intentional\" hybridity. He defines organic hybridity as an \"unintentional, unconscious hybridization\" and regards it as \"the most important mode in the historical life and evolution of all languages\". \"Intentional hybridization\" consists of juxtaposing deliberately different idioms, discourses, and perspectives within the same semiotic space without merging them. Bakhtin states that the language of the novel is \"a system of languages that mutually and ideologically interanimate each other\". He adds: \"the novelistic hybrid is an artistically organized system for bringing different languages in contact with one another, a system having as its goal the illumination of one language by another, the carving-out of a living image of another language\". Further down, yet, he cautions against drawing clear-cut boundaries between these two forms of hybridity, arguing that the \"centripetal\" forces inherent in \"organic hybridity\" are also present in \"intentional hybridity,\" in the same way as the \"centrifugal\" features of \"intentional hybridity\" may be at play in \"organic hybridity.\"\n\nLinguistic hybridity and the case of mixed languages challenge the Tree Model in linguistics. For example, \"Israeli\" (a term for Modern Hebrew) has been argued to be a Semito-European hybrid language that \"demonstrates that the reality of linguistic genesis is far more complex than a simple family tree system allows. 'Revived' languages are unlikely to have a single parent.\"\n\nPresently, human beings are immersed in a hybridised environment of reality and augmented reality on a daily basis, considering the proliferation of physical and digital media (i.e. print books vs. e-books, music downloads vs. physical formats). Many people attend performances intending to place a digital recording device between them and the performers, intentionally \"layering a digital reality on top of the real world.\" For artists working with and responding to new technologies, the hybridisation of physical and digital elements has become a reflexive reaction to this strange dichotomy. For example, in \"Rooms\" by Sara Ludy computer-generated effects process physical spaces into abstractions, making familiar environments and items such as carpets, doors and windows disorientating, set to the sound of an industrial hum. In effect, the distinction between real and virtual space in art is deconstructed.\n\n"}
{"id": "15079406", "url": "https://en.wikipedia.org/wiki?curid=15079406", "title": "Jack o' Lent", "text": "Jack o' Lent\n\nJack o' Lent was a tradition in England in the 15th, 16th and 17th centuries involving the abuse and burning of a straw effigy during the season of Lent, ending with its burning on Palm Sunday. \n\nThe effigy, made of straw or stuffed clothes, was abused and stoned on Ash Wednesday while being dragged about the parish. The figure was kept until Palm Sunday, when it was burnt. Its burning was often believed to be a symbolic revenge on Judas Iscariot, who had betrayed Christ. It is equally likely that the figure represented the hated winter and its destruction prepares the way for spring. Jack o' Lent is mentioned in Thomas Heywood's \"The Four Prentices of London\", Shakespeare's \"Merry Wives of Windsor,\" Anthony Burgess' \"Nothing Like the Sun\", Henry Fielding's \"Joseph Andrews\" as well as in the 1640s pamphlet, \"The Arraignment Conviction and Imprisonment of Christmas\".\n\n"}
{"id": "35079368", "url": "https://en.wikipedia.org/wiki?curid=35079368", "title": "Junior Ambassador", "text": "Junior Ambassador\n\nJunior Ambassador is an international cultural exchange and relations organization. Junior Ambassador manages international cultural exchange programs, which are certified by government organizations to train young global leaders(youth ambassadors).\n\nThe purpose of the program of Junior Ambassador is to train youth global cultural representatives, who could lead the generation of 21st Century cultural diplomacy and contribute to the happiness and development of mankind with much knowledge and love for mankind. The purpose of the Junior Ambassador program is to train youth ambassadors who would lead 21st century cultural diplomacy and contribute to the happiness and development of mankind. Since the 26th UNESCO General Conference has adopted ‘Seoul Agenda: Goals for Cultural Art Exchange’, which was mainly led by the government of Republic of Korea, practicing cultural diversity, intercultural dialogue, and sustainable development became significant issues. To practice UNESCO’s educational ideology, Junior Ambassador’s object is to train 21st Century style leaders, who comprehend about the movement of global cultural flow and lead exchange of cultural art.\n\nBritish Council, ATOUT France, ENIT Italia, Switzerland embassy and Switzerland tourism are participating as partners of the Junior Ambassador program. These global partners assist in designing the program, co-hosting cultural activities and issuing the letter of credence.\n\nJunior Ambassador manages many programs as below.\n\n"}
{"id": "4887671", "url": "https://en.wikipedia.org/wiki?curid=4887671", "title": "Kartvelian studies", "text": "Kartvelian studies\n\nThe Kartvelian studies () also referred as Kartvelology or Georgian studies is a field of humanities covering Kartvelian (Georgian) history, languages, religion and/or culture.\n\nIn a narrower sense, the term usually refers to the research activities conducted on these problems outside Georgia.\n\n\n\n\n"}
{"id": "4938890", "url": "https://en.wikipedia.org/wiki?curid=4938890", "title": "Lebeau, Louisiana", "text": "Lebeau, Louisiana\n\nLebeau (also spelled LeBeau) is an unincorporated community in St. Landry Parish, Louisiana, United States, in the central part of the state. Nearby communities include Palmetto, Ville Platte and Washington. The community is part of the Opelousas–Eunice Micropolitan Statistical Area.\n\n\nNorth Central High School (Hurricanes) - Grades 5-12. \n\nLebeau Zydeco Festival - An annual festival featuring performances from leading zydeco artist. As of 2018, the festival is hosting the 28th annual celebration. The festival is usually held on the first Saturday in July on the Immaculate Conception Catholic Church Grounds.\n\nSidney Simien - Internationally famed Zydeco musician, Rockin' Sidney, known for his hit single \"My Toot Toot\" stayed on The Country Top 40 for 18 weeks, Certified Platinum and won a Grammy award.\n"}
{"id": "17406217", "url": "https://en.wikipedia.org/wiki?curid=17406217", "title": "Lexis (Aristotle)", "text": "Lexis (Aristotle)\n\nIn philosophical discourse, lexis (from the Greek: λέξις \"word\") is a complete group of words in a language, vocabulary, the total set of all words in a language, and all words that have meaning or a function in grammar.\n\nAccording to Plato, lexis is the manner of speaking. Plato said that lexis can be divided into mimesis (imitation properly speaking) and diegesis (simple narrative). Gerard Genette states: \"Plato's theoretical division, opposing the two pure and heterogeneous modes of narrative and imitation, within poetic diction, elicits and establishes a practical classification of genres, which includes the two distinct modes...and a mixed mode, for example the Iliad\".\nIn the \"Iliad\", a Greek epic written by Homer, the mixed mode is very prevalent. According to Gerald Prince, diegesis in the \"Iliad\" is the fictional storytelling associated with the fictional world and the enacting/re-telling of the story. Mimesis in the\" Iliad\" is the imitation of everyday, yet fantastical life in the ancient Greek world. Diegesis and mimesis combined represent the fullest extent of lexis; both forms of speech, narrating and re-enacting.\nIn conclusion, lexis is the larger overview of literature. Within lexis the two areas of differentiation of mimesis (imitation) are diegesis (narrative) and the \"direct representation of the actors speaking to the public.\"\n\nAccording to Jose M. Gonzalez, \"Aristotle instructs us to view of his psychology, as mediating the rhetorical task and entrusted with turning the orator's subject matter into such opinion of the listeners and gain their pistis.\" Pistis is the Greek word for faith and is one of the rhetorical modes of persuasion.\nGonzalez also points out that, \"By invoking phantasia, lexis against the background Aristotle instructs us to view of his psychology, as mediating the rhetorical task and entrusted with turning the orator's subject matter into such opinion of the listeners and gain their pistis.\" Phantasia is a Greek word meaning the process by which all images are presented to us. Aristotle defines phantasia as \"our desire for the mind to mediate anything not actually present to the senses with a mental image.\" Aristotle instructs the reader to use his or her imagination to create the fantastic, unordinary images, all the while using narrative and re-enactment to create a play either written or produced.\n\nAlthough Aristotle at times seems to demean the art of diction or 'voice,' saying that it is not an \"elevated subject of inquiry,\" he does go into quite a bit of detail on its importance and its proper use in rhetorical speech. Often calling it \"style,\" he defines good style as follows: that it must be clear and avoid extremes of baseness and loftiness. Aristotle makes the cases for the importance of diction by saying that, \"it is not enough to know what we ought to say; we must also say it as we ought.\". In an oratorical speech, one must consider not only the facts, but also how to put the facts into words and which words and, also, the \"proper method of delivery.\" Aristotle goes on to say that only the facts in an argument should be important but that since the listeners can be swayed by diction, it must also be considered. \nVoice \n\nAt the time when Aristotle wrote his treatise on Rhetoric, orators had not paid much attention to voice. They thought it was a subject only of concern to actors and poets. In \"The Rhetoric\", Aristotles says, \"proper method of delivery…affects the success of a speech greatly; but hitherto the subject has been neglected.\" Aristotle defined voice as controlling one's voice, using rate, volume and pitch, to convey the appropriate emotions. The manner of voice in which an idea or speech is conveyed affects not only the emotions of the audience but, also, their ability to understand this concept.\nAlthough Aristotle gives this mention and explanation of voice, he does not go into specifics about how to produce appropriate voice or how to convey specific tones with one's voice. This may or may not be due to his mild disdain for the topic as a whole. Modern scholars have explored voice more extensively. According to Taylor Stoehr, \"voice is the pervasive reflection in written or spoken language, of an author's character, the marks by which we recognize his utterance as his.\". However, just as in Aristotle's time set of specific rules or guidelines has yet been laid out for the production or interpretation of voice. Due to the vast array of elements involved in the production of voice this task would be nearly, if not entirely, impossible. \n\nLanguage\n\nAs mentioned before, Aristotle thought the language of a speech should avoid being too lofty or too crude. The speaker must use the ordinary language of everyday life. However, because people best remember what is out of the ordinary, the speaker must use some language that gives the speech an air of importance.\n\nThe elevation of the language must correlate to the elevation of the subject or, in poetry, the character that is speaking. In poetry, language and linguistic devices that convey a sense of importance are more appropriate, and should be used more often because the events of poetry are more removed from ordinary life. They are less appropriate in rhetorical speech because the topics relate more directly to ordinary things and the people who are listening to the speech. Most of all, the speaker must \"give the impression of speaking naturally and not artificially.\" When one seems to speak with ease, the audience is more easily persuaded that the facts he is communicating are truthful. \n\nAlso, a speaker must avoid using very many \"strange words, compound words, and invented words.\". Aristotle considered this kind of language an excessive departure from the way people normally speak. However, one acceptable departure from plain language is the use of metaphor because metaphors are used by all people in everyday conversation.\n\nAccording to Aristotle, lexis, meaning the delivery of words, is the least important area of speech when in comparison to invention, arrangement and style. However, lexis is still closely looked at and broken down into two forms. The two types of lexis in rhetoric include: \"lexis graphike\" and \"lexis agonistike\" The separate terms that describe the two forms of lexis, graphike and agonistike, have been conformed by several Latin terms. Although the words directly relate to the type of lexis, the theories of Aristotle and Plato do not compare.\n\n\"Lexis graphike\" comes from the term \"zographia\", meaning realistic painting, and graphe, meaning writing. Plato believes that writing and painting are one of the same. His theory proves that both do not have the capability to defend themselves through an argument, question and answer, which conveys that these forms can not prove truth. Although for Aristotle, \"lexis graphik\"e is the most accurate delivery of language, which leads to his theory that proves that writing does not need to be questioned because it is already exact. \"Lexis agonistike\" however is from the term \"skiagraphia\", meaning a rough sketch or outline of painting. Aristotle once again opposes Plato by believing that \"lexis agonistike\" does not need questions asked, but only answers. The answer refers to the use of invention given to the actor because the writing portion is only outlined.\n\nTo further understand the separate types of lexis, each type can be broken down by how the writing is prepared and delivered. \"Lexis graphike\" is the most precise style of rhetoric and strongly appeals to intelligence. The delivery of \"lexis graphike\" is designed for a careful reading from either the book or paper as opposed to a performance that leaves room for improvisation. This type of lexis is a simple, straight forward recitation rather than an elaborate presentation. \"Lexis graphike\" is most accurately written and depends the least upon the person who is delivering the speech. \"Lexis agonistike\" contradicts \"lexis graphike\" because it is typically carelessly written and meant for a full performance. The lack of attention given to the written words allows the performer to improvise. This gives the presentation a style that reflects the entertainer rather the writer.\n"}
{"id": "17504356", "url": "https://en.wikipedia.org/wiki?curid=17504356", "title": "List of Slavic cultures", "text": "List of Slavic cultures\n\nThis is a list of the cultures of Slavic Europe.\n\n\n"}
{"id": "41923118", "url": "https://en.wikipedia.org/wiki?curid=41923118", "title": "List of urban legends", "text": "List of urban legends\n\nThis is a list of urban legends.\n\n\n\n"}
{"id": "545753", "url": "https://en.wikipedia.org/wiki?curid=545753", "title": "Magical Negro", "text": "Magical Negro\n\nIn the cinema of the United States, the Magical Negro is a supporting stock character who comes to the aid of white protagonists in a film. Magical Negro characters, who often possess special insight or mystical powers, have long been a tradition in American fiction.\n\nThe term \"Magical Negro\" was popularized in 2001 by film director Spike Lee, while discussing films with students during a tour of college campuses, in which he said he was dismayed at Hollywood's decision to continue employing this premise; he noted that the films \"The Green Mile\" and \"The Legend of Bagger Vance\" used the \"super-duper magical Negro\".\nCritics use the word \"Negro\" because it is considered archaic, and usually offensive, in modern English. This underlines their message that a \"magical black character\" who goes around selflessly helping white people is a throwback to stereotypes such as the \"Sambo\" or \"noble savage\".\n\nThe Magical Negro is a trope created by white people: the character is typically, but not always, \"in some way outwardly or inwardly disabled, either by discrimination, disability or social constraint\". The Negro is often a janitor or prisoner. The character often has no past but simply appears one day to help the white protagonist. He or she usually has some sort of magical power, \"rather vaguely defined but not the sort of thing one typically encounters.\" The character is patient and wise, often dispensing various words of wisdom, and is \"closer to the earth\". The character will also do almost anything, including sacrificing him or herself, to save the white protagonist, as exemplified in \"The Defiant Ones\", in which Sidney Poitier plays the prototypical Magical Negro.\n\nChristopher John Farley, referring to the magical Negro as \"Magical African American Friends\" (MAAFs), says they are rooted in screenwriter's ignorance of African Americans:\nMAAFs exist because most Hollywood screenwriters don't know much about black people other than what they hear on records by white hip-hop star Eminem. So instead of getting life histories or love interests, black characters get magical powers.\n\nThe Magical Negro stereotype serves as a plot device to help the white protagonist get out of trouble, typically through helping the white character recognize his own faults and overcome them and teaching them to be a better person. Although the character may have magical powers, the \"magic is ostensibly directed toward helping and enlightening a white male character\". An article in a 2009 edition of the journal \"Social Problems\" stated the Magical Negro was an expression of racial profiling within the United States:\nThese powers are used to save and transform disheveled, uncultured, lost, or broken whites (almost exclusively white men) into competent, successful, and content people within the context of the American myth of redemption and salvation. It is this feature of the Magical Negro that some people find most troubling. Although from a certain perspective the character may seem to be showing blacks in a positive light, the character is still ultimately subordinate to whites. He or she is also regarded as an exception, allowing white America to like individual black people but not black culture.\n\nIn 2001 Spike Lee used the term in a series of talks on college campuses to criticize the stereotypical, unreal roles created for black men in films that were recent at that time, naming \"The Family Man\" (2000), \"What Dreams May Come\" (1998), \"The Legend of Bagger Vance\" (2000) and \"The Green Mile\" (1999) as examples. Talking about the time and place in which Bagger Vance is set, he said:\n\nIn a book published in 2004, writer Krin Gabbard claimed that the Oda Mae Brown character in the 1990 movie \"Ghost\", played by Whoopi Goldberg, was an example of a Magical Negress.\n\nIn 2012, writer Kia Miakka Natisse, in \"The Grip\", opined about actor Morgan Freeman playing parts more or less conforming to the Magical Negro form. Natisse mentioned recent roles including the doctor fitting a dolphin with a prosthetic tail in \"Dolphin Tale\" despite the film's origin: the film is based on a true story. Natisse continued to gripe: \"[\"Red\"] an ailing CIA mentor—in both roles [Freeman] reprises the Magical Negro type, coming to save the day for his imperiled white counterparts. One could argue his gadget guru in \"The Dark Knight Rises\" fits under that same umbrella.\"\n\nComedian Dave Chappelle made several references to this trope in his mid-2000's series \"Chappelle's Show\", including a sketch entitled \"Black Pixies\". Chris Rock did likewise on his own show of the same period, with another, particularly critical of \"The Legend of Bagger Vance\", entitled \"Migger, the Magic Nigger\". More recently, Keegan-Michael Key and Jordan Peele, of \"MADtv\" and \"Key and Peele\" fame, followed suit in both shows with their own critical Magical Negro sketches.\n\nIn March 2007, American critic David Ehrenstein used the title \"Obama the 'Magic Negro'\" for an editorial he wrote for the \"Los Angeles Times\", in which he described Barack Obama's image in white American culture: \"He's there to assuage white 'guilt' (i.e., the \"minimal discomfort\" they feel) over the role of slavery and racial segregation in American history, while replacing stereotypes of a dangerous, highly sexualized black man with a benign figure for whom interracial sexual congress holds no interest ... The only mud that momentarily stuck was criticism (white and black alike) concerning Obama's alleged 'inauthenticity', as compared to such sterling examples of \"genuine\" blackness as Al Sharpton and Snoop Dogg. ... Obama's fame right now has little to do with his political record ... Like a comic-book superhero, Obama is there to help, out of the sheer goodness of a heart we need not know or understand. For as with all Magic Negroes, the less real he seems, the more desirable he becomes. If he were real, white America couldn't project all its fantasies of curative black benevolence on him.\"\n\nDiscussing the Ehrenstein editorial at length, Rush Limbaugh at one point sang the words, \"Barack the magic negro\" to the tune of song \"Puff, the Magic Dragon\". Shortly after that Paul Shanklin recorded a song about Barack the Magic Negro set to that same tune, which Limbaugh played numerous times throughout the 2008 presidential election season.\n\nIn Christmas 2008, Chip Saltsman, a Republican politician and chair of the Tennessee Republican Party, sent a 41-track CD containing the song to members of the Republican National Committee during the Republican National Committee chairmanship election. Saltsman's campaign imploded as a result of the controversy caused by the CD, and he withdrew from the race; \nMichael Steele, an African American, was elected.\n\nIn May 2015, theater and cultural critic Frank Rich, looking back at the coincidence of the 2015 Baltimore protests with the annual White House Correspondents Dinner in Washington, DC, wrote: \"What made this particular instance poignant was the presence in the ballroom of our first African-American president, the Magic Negro who was somehow expected to relieve a nation founded and built on slavery from the toxic burdens of centuries of history.\"\n\n"}
{"id": "19312", "url": "https://en.wikipedia.org/wiki?curid=19312", "title": "Meme", "text": "Meme\n\nA meme ( ) is an idea, behavior, or style that spreads from person to person within a culture—often with the aim of conveying a particular phenomenon, theme, or meaning represented by the meme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other imitable phenomena with a mimicked theme. Supporters of the concept regard memes as cultural analogues to genes in that they self-replicate, mutate, and respond to selective pressures.\n\nProponents theorize that memes are a viral phenomenon that may evolve by natural selection in a manner analogous to that of biological evolution. Memes do this through the processes of variation, mutation, competition, and inheritance, each of which influences a meme's reproductive success. Memes spread through the behavior that they generate in their hosts. Memes that propagate less prolifically may become extinct, while others may survive, spread, and (for better or for worse) mutate. Memes that replicate most effectively enjoy more success, and some may replicate effectively even when they prove to be detrimental to the welfare of their hosts.\n\nA field of study called memetics arose in the 1990s to explore the concepts and transmission of memes in terms of an evolutionary model. Criticism from a variety of fronts has challenged the notion that academic study can examine memes empirically. However, developments in neuroimaging may make empirical study possible. Some commentators in the social sciences question the idea that one can meaningfully categorize culture in terms of discrete units, and are especially critical of the biological nature of the theory's underpinnings. Others have argued that this use of the term is the result of a misunderstanding of the original proposal.\n\nThe word \"meme\" is a neologism coined by Richard Dawkins. It originated from Dawkins' 1976 book \"The Selfish Gene\". Dawkins's own position is somewhat ambiguous: he welcomed N. K. Humphrey's suggestion that \"memes should be considered as living structures, not just metaphorically\" and proposed to regard memes as \"physically residing in the brain\". Later, he argued that his original intentions, presumably before his approval of Humphrey's opinion, had been simpler.\n\nThe word \"meme\" is a shortening (modeled on \"gene\") of \"mimeme\" (from Ancient Greek \"mīmēma\", \"imitated thing\", from \"mimeisthai\", \"to imitate\", from \"mimos\", \"mime\") coined by British evolutionary biologist Richard Dawkins in \"The Selfish Gene\" (1976) as a concept for discussion of evolutionary principles in explaining the spread of ideas and cultural phenomena. Examples of memes given in the book included melodies, catchphrases, fashion, and the technology of building arches. Kenneth Pike coined the related terms emic and etic, generalizing the linguistic idea of phoneme, morpheme, grapheme, lexeme, and tagmeme (as set out by Leonard Bloomfield), characterizing them as insider view and outside view of behaviour and extending the concept into a tagmemic theory of human behaviour (culminating in \"Language in Relation to a Unified Theory of the Structure of Human Behaviour\", 1954).\n\nThe word \"meme\" originated with Richard Dawkins' 1976 book \"The Selfish Gene\". Dawkins cites as inspiration the work of geneticist L. L. Cavalli-Sforza, anthropologist F. T. Cloak and ethologist J. M. Cullen. Dawkins wrote that evolution depended not on the particular chemical basis of genetics, but only on the existence of a self-replicating unit of transmission—in the case of biological evolution, the gene. For Dawkins, the meme exemplified another self-replicating unit with potential significance in explaining human behavior and cultural evolution. Although Dawkins invented the term 'meme' and developed meme theory, the possibility that ideas were subject to the same pressures of evolution as were biological attributes was discussed in Darwin's time. T. H. Huxley claimed that 'The struggle for existence holds as much in the intellectual as in the physical world. A theory is a species of thinking, and its right to exist is coextensive with its power of resisting extinction by its rivals.'\nDawkins used the term to refer to any cultural entity that an observer might consider a replicator. He hypothesized that one could view many cultural entities as replicators, and pointed to melodies, fashions and learned skills as examples. Memes generally replicate through exposure to humans, who have evolved as efficient copiers of information and behavior. Because humans do not always copy memes perfectly, and because they may refine, combine or otherwise modify them with other memes to create new memes, they can change over time. Dawkins likened the process by which memes survive and change through the evolution of culture to the natural selection of genes in biological evolution.\n\nDawkins defined the \"meme\" as a unit of cultural transmission, or a unit of imitation and replication, but later definitions would vary. The lack of a consistent, rigorous, and precise understanding of what typically makes up one unit of cultural transmission remains a problem in debates about memetics. In contrast, the concept of genetics gained concrete evidence with the discovery of the biological functions of DNA. Meme transmission requires a physical medium, such as photons, sound waves, touch, taste, or smell because memes can be transmitted only through the senses.\n\nDawkins noted that in a society with culture a person need not have descendants to remain influential in the actions of individuals thousands of years after their death:\n\nBut if you contribute to the world's culture, if you have a good idea...it may live on, intact, long after your genes have dissolved in the common pool. Socrates may or may not have a gene or two alive in the world today, as G.C. Williams has remarked, but who cares? The meme-complexes of Socrates, Leonardo, Copernicus and Marconi are still going strong.\n\nAlthough Dawkins invented the term \"meme\", he has not claimed that the idea was entirely novel, and there have been other expressions for similar ideas in the past. In 1904, Richard Semon published \"Die Mneme\" (which appeared in English in 1924 as \"The Mneme\"). The term \"mneme\" was also used in Maurice Maeterlinck's \"The Life of the White Ant\" (1926), with some parallels to Dawkins's concept.\n\nMemes, analogously to genes, vary in their aptitude to replicate; successful memes remain and spread, whereas unfit ones stall and are forgotten. Thus memes that prove more effective at replicating and surviving are selected in the meme pool.\n\nMemes first need retention. The longer a meme stays in its hosts, the higher its chances of propagation are. When a host uses a meme, the meme's life is extended. The reuse of the neural space hosting a certain meme's copy to host different memes is the greatest threat to that meme's copy.\n\nA meme which increases the longevity of its hosts will generally survive longer. On the contrary, a meme which shortens the longevity of its hosts will tend to disappear faster. However, as hosts are mortal, retention is not sufficient to perpetuate a meme in the long term; memes also need transmission.\n\nLife-forms can transmit information both vertically (from parent to child, via replication of genes) and horizontally (through viruses and other means).\nMemes can replicate vertically or horizontally within a single biological generation. They may also lie dormant for long periods of time.\n\nMemes reproduce by copying from a nervous system to another one, either by communication or imitation. Imitation often involves the copying of an observed behavior of another individual. Communication may be direct or indirect, where memes transmit from one individual to another through a copy recorded in an inanimate source, such as a book or a musical score. Adam McNamara has suggested that memes can be thereby classified as either internal or external memes (i-memes or e-memes).\n\nSome commentators have likened the transmission of memes to the spread of contagions. Social contagions such as fads, hysteria, copycat crime, and copycat suicide exemplify memes seen as the contagious imitation of ideas. Observers distinguish the contagious imitation of memes from instinctively contagious phenomena such as yawning and laughing, which they consider innate (rather than socially learned) behaviors.\n\nAaron Lynch described seven general patterns of meme transmission, or \"thought contagion\":\n\n\nDawkins initially defined \"meme\" as a noun that \"conveys the idea of a unit of cultural transmission, or a unit of \"imitation\"\". John S. Wilkins retained the notion of meme as a kernel of cultural imitation while emphasizing the meme's evolutionary aspect, defining the meme as \"the least unit of sociocultural information relative to a selection process that has favorable or unfavorable selection bias that exceeds its endogenous tendency to change\". The meme as a unit provides a convenient means of discussing \"a piece of thought copied from person to person\", regardless of whether that thought contains others inside it, or forms part of a larger meme. A meme could consist of a single word, or a meme could consist of the entire speech in which that word first occurred. This forms an analogy to the idea of a gene as a single unit of self-replicating information found on the self-replicating chromosome.\n\nWhile the identification of memes as \"units\" conveys their nature to replicate as discrete, indivisible entities, it does not imply that thoughts somehow become quantized or that \"atomic\" ideas exist that cannot be dissected into smaller pieces. A meme has no given size. Susan Blackmore writes that melodies from Beethoven's symphonies are commonly used to illustrate the difficulty involved in delimiting memes as discrete units. She notes that while the first four notes of Beethoven's Fifth Symphony () form a meme widely replicated as an independent unit, one can regard the entire symphony as a single meme as well.\n\nThe inability to pin an idea or cultural feature to quantifiable key units is widely acknowledged as a problem for memetics. It has been argued however that the traces of memetic processing can be quantified utilizing neuroimaging techniques which measure changes in the connectivity profiles between brain regions.\" Blackmore meets such criticism by stating that memes compare with genes in this respect: that while a gene has no particular size, nor can we ascribe every phenotypic feature directly to a particular gene, it has value because it encapsulates that key unit of inherited expression subject to evolutionary pressures. To illustrate, she notes evolution selects for the gene for features such as eye color; it does not select for the individual nucleotide in a strand of DNA. Memes play a comparable role in understanding the evolution of imitated behaviors.\n\nThe 1981 book \"Genes, Mind, and Culture: The Coevolutionary Process\" by Charles J. Lumsden and E. O. Wilson proposed the theory that genes and culture co-evolve, and that the fundamental biological units of culture must correspond to neuronal networks that function as nodes of semantic memory. They coined their own word, \"culturgen\", which did not catch on. Coauthor Wilson later acknowledged the term \"meme\" as the best label for the fundamental unit of cultural inheritance in his 1998 book \"\", which elaborates upon the fundamental role of memes in unifying the natural and social sciences.\n\nDawkins noted the three conditions that must exist for evolution to occur:\nDawkins emphasizes that the process of evolution naturally occurs whenever these conditions co-exist, and that evolution does not apply only to organic elements such as genes. He regards memes as also having the properties necessary for evolution, and thus sees meme evolution as not simply analogous to genetic evolution, but as a real phenomenon subject to the laws of natural selection. Dawkins noted that as various ideas pass from one generation to the next, they may either enhance or detract from the survival of the people who obtain those ideas, or influence the survival of the ideas themselves. For example, a certain culture may develop unique designs and methods of tool-making that give it a competitive advantage over another culture. Each tool-design thus acts somewhat similarly to a biological gene in that some populations have it and others do not, and the meme's function directly affects the presence of the design in future generations. In keeping with the thesis that in evolution one can regard organisms simply as suitable \"hosts\" for reproducing genes, Dawkins argues that one can view people as \"hosts\" for replicating memes. Consequently, a successful meme may or may not need to provide any benefit to its host.\n\nUnlike genetic evolution, memetic evolution can show both Darwinian and Lamarckian traits. Cultural memes will have the characteristic of Lamarckian inheritance when a host aspires to replicate the given meme through inference rather than by exactly copying it. Take for example the case of the transmission of a simple skill such as hammering a nail, a skill that a learner imitates from watching a demonstration without necessarily imitating every discrete movement modeled by the teacher in the demonstration, stroke for stroke. Susan Blackmore distinguishes the difference between the two modes of inheritance in the evolution of memes, characterizing the Darwinian mode as \"copying the instructions\" and the Lamarckian as \"copying the product.\"\n\nClusters of memes, or \"memeplexes\" (also known as \"meme complexes\" or as \"memecomplexes\"), such as cultural or political doctrines and systems, may also play a part in the acceptance of new memes. Memeplexes comprise groups of memes that replicate together and coadapt. Memes that fit within a successful memeplex may gain acceptance by \"piggybacking\" on the success of the memeplex.\nAs an example, John D. Gottsch discusses the transmission, mutation and selection of religious memeplexes and the theistic memes contained. Theistic memes discussed include the \"prohibition of aberrant sexual practices such as incest, adultery, homosexuality, bestiality, castration, and religious prostitution\", which may have increased vertical transmission of the parent religious memeplex. Similar memes are thereby included in the majority of religious memeplexes, and harden over time; they become an \"inviolable canon\" or set of dogmas, eventually finding their way into secular law. This could also be referred to as the propagation of a taboo.\n\nThe discipline of memetics, which dates from the mid-1980s, provides an approach to evolutionary models of cultural information transfer based on the concept of the meme. Memeticists have proposed that just as memes function analogously to genes, memetics functions analogously to genetics. Memetics attempts to apply conventional scientific methods (such as those used in population genetics and epidemiology) to explain existing patterns and transmission of cultural ideas.\n\nPrincipal criticisms of memetics include the claim that memetics ignores established advances in other fields of cultural study, such as sociology, cultural anthropology, cognitive psychology, and social psychology. Questions remain whether or not the meme concept counts as a validly disprovable scientific theory. This view regards memetics as a theory in its infancy: a protoscience to proponents, or a pseudoscience to some detractors.\n\nAn objection to the study of the evolution of memes in genetic terms (although not to the existence of memes) involves a perceived gap in the gene/meme analogy: the cumulative evolution of genes depends on biological selection-pressures neither too great nor too small in relation to mutation-rates. There seems no reason to think that the same balance will exist in the selection pressures on memes.\n\nLuis Benitez-Bribiesca M.D., a critic of memetics, calls the theory a \"pseudoscientific dogma\" and \"a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution\". As a factual criticism, Benitez-Bribiesca points to the lack of a \"code script\" for memes (analogous to the DNA of genes), and to the excessive instability of the meme mutation mechanism (that of an idea going from one brain to another), which would lead to a low replication accuracy and a high mutation rate, rendering the evolutionary process chaotic.\n\nBritish political philosopher John Gray has characterized Dawkins' memetic theory of religion as \"nonsense\" and \"not even a theory... the latest in a succession of ill-judged Darwinian metaphors\", comparable to Intelligent Design in its value as a science.\n\nAnother critique comes from semiotic theorists such as Deacon and Kull. This view regards the concept of \"meme\" as a primitivized concept of \"sign\". The meme is thus described in memetics as a sign lacking a triadic nature. Semioticians can regard a meme as a \"degenerate\" sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.\n\nFracchia and Lewontin regard memetics as reductionist and inadequate. Evolutionary biologist Ernst Mayr disapproved of Dawkins' gene-based view and usage of the term \"meme\", asserting it to be an \"unnecessary synonym\" for \"concept\", reasoning that concepts are not restricted to an individual or a generation, may persist for long periods of time, and may evolve.\n\nOpinions differ as to how best to apply the concept of memes within a \"proper\" disciplinary framework. One view sees memes as providing a useful philosophical perspective with which to examine cultural evolution. Proponents of this view (such as Susan Blackmore and Daniel Dennett) argue that considering cultural developments from a meme's-eye view—\"as if\" memes themselves respond to pressure to maximise their own replication and survival—can lead to useful insights and yield valuable predictions into how culture develops over time. Others such as Bruce Edmonds and Robert Aunger have focused on the need to provide an empirical grounding for memetics to become a useful and respected scientific discipline.\n\nA third approach, described by Joseph Poulshock, as \"radical memetics\" seeks to place memes at the centre of a materialistic theory of mind and of personal identity.\n\nProminent researchers in evolutionary psychology and anthropology, including Scott Atran, Dan Sperber, Pascal Boyer, John Tooby and others, argue the possibility of incompatibility between modularity of mind and memetics. In their view, minds structure certain communicable aspects of the ideas produced, and these communicable aspects generally trigger or elicit ideas in other minds through inference (to relatively rich structures generated from often low-fidelity input) and not high-fidelity replication or imitation. Atran discusses communication involving religious beliefs as a case in point. In one set of experiments he asked religious people to write down on a piece of paper the meanings of the Ten Commandments. Despite the subjects' own expectations of consensus, interpretations of the commandments showed wide ranges of variation, with little evidence of consensus. In another experiment, subjects with autism and subjects without autism interpreted ideological and religious sayings (for example, \"Let a thousand flowers bloom\" or \"To everything there is a season\"). People with autism showed a significant tendency to closely paraphrase and repeat content from the original statement (for example: \"Don't cut flowers before they bloom\"). Controls tended to infer a wider range of cultural meanings with little replicated content (for example: \"Go with the flow\" or \"Everyone should have equal opportunity\"). Only the subjects with autism—who lack the degree of inferential capacity normally associated with aspects of theory of mind—came close to functioning as \"meme machines\".\n\nIn his book \"The Robot's Rebellion\", Stanovich uses the memes and memeplex concepts to describe a program of cognitive reform that he refers to as a \"rebellion\". Specifically, Stanovich argues that the use of memes as a descriptor for cultural units is beneficial because it serves to emphasize transmission and acquisition properties that parallel the study of epidemiology. These properties make salient the sometimes parasitic nature of acquired memes, and as a result individuals should be motivated to reflectively acquire memes using what he calls a \"Neurathian bootstrap\" process.\n\nAlthough social scientists such as Max Weber sought to understand and explain religion in terms of a cultural attribute, Richard Dawkins called for a re-analysis of religion in terms of the evolution of self-replicating ideas \"apart from\" any resulting biological advantages they might bestow.\nHe argued that the role of key replicator in cultural evolution belongs not to genes, but to memes replicating thought from person to person by means of imitation. These replicators respond to selective pressures that may or may not affect biological reproduction or survival.\n\nIn her book \"The Meme Machine\", Susan Blackmore regards religions as particularly tenacious memes. Many of the features common to the most widely practiced religions provide built-in advantages in an evolutionary context, she writes. For example, religions that preach of the value of faith over evidence from everyday experience or reason inoculate societies against many of the most basic tools people commonly use to evaluate their ideas. By linking altruism with religious affiliation, religious memes can proliferate more quickly because people perceive that they can reap societal as well as personal rewards. The longevity of religious memes improves with their documentation in revered religious texts.\n\nAaron Lynch attributed the robustness of religious memes in human culture to the fact that such memes incorporate multiple modes of meme transmission. Religious memes pass down the generations from parent to child and across a single generation through the meme-exchange of proselytism. Most people will hold the religion taught them by their parents throughout their life. Many religions feature adversarial elements, punishing apostasy, for instance, or demonizing infidels. In \"Thought Contagion\" Lynch identifies the memes of transmission in Christianity as especially powerful in scope. Believers view the conversion of non-believers both as a religious duty and as an act of altruism. The promise of heaven to believers and threat of hell to non-believers provide a strong incentive for members to retain their belief. Lynch asserts that belief in the Crucifixion of Jesus in Christianity amplifies each of its other replication advantages through the indebtedness believers have to their Savior for sacrifice on the cross. The image of the crucifixion recurs in religious sacraments, and the proliferation of symbols of the cross in homes and churches potently reinforces the wide array of Christian memes.\n\nAlthough religious memes have proliferated in human cultures, the modern scientific community has been relatively resistant to religious belief. Robertson (2007) reasoned that if evolution is accelerated in conditions of propagative difficulty, then we would expect to encounter variations of religious memes, established in general populations, addressed to scientific communities. Using a memetic approach, Robertson deconstructed two attempts to privilege religiously held spirituality in scientific discourse. Advantages of a memetic approach as compared to more traditional \"modernization\" and \"supply side\" theses in understanding the evolution and propagation of religion were explored.\n\nIn \"Cultural Software: A Theory of Ideology\", Jack Balkin argued that memetic processes can explain many of the most familiar features of ideological thought. His theory of \"cultural software\" maintained that memes form narratives, social networks, metaphoric and metonymic models, and a variety of different mental structures. Balkin maintains that the same structures used to generate ideas about free speech or free markets also serve to generate racistic beliefs. To Balkin, whether memes become harmful or maladaptive depends on the environmental context in which they exist rather than in any special source or manner to their origination. Balkin describes racist beliefs as \"fantasy\" memes that become harmful or unjust \"ideologies\" when diverse peoples come together, as through trade or competition.\n\nIn \"A Theory of Architecture\", Nikos Salingaros speaks of memes as \"freely propagating clusters of information\" which can be beneficial or harmful. He contrasts memes to patterns and true knowledge, characterizing memes as \"greatly simplified versions of patterns\" and as \"unreasoned matching to some visual or mnemonic prototype\". Taking reference to Dawkins, Salingaros emphasizes that they can be transmitted due to their own communicative properties, that \"the simpler they are, the faster they can proliferate\", and that the most successful memes \"come with a great psychological appeal\".\n\nArchitectural memes, according to Salingaros, can have destructive power. \"Images portrayed in architectural magazines representing buildings that could not possibly accommodate everyday uses become fixed in our memory, so we reproduce them unconsciously.\" He lists various architectural memes that circulated since the 1920s and which, in his view, have led to contemporary architecture becoming quite decoupled from human needs. They lack connection and meaning, thereby preventing \"the creation of true connections necessary to our understanding of the world\". He sees them as no different from antipatterns in software design—as solutions that are false but are re-utilized nonetheless.\n\nAn \"Internet meme\" is a concept that spreads rapidly from person to person via the Internet, largely through Internet-based E-mailing, blogs, forums, imageboards like 4chan, social networking sites like Facebook, Instagram, or Twitter, instant messaging, social news sites or thread sites like Reddit, and video hosting services like YouTube and Twitch.\n\nIn 2013, Richard Dawkins characterized an Internet meme as one deliberately altered by human creativity, distinguished from Dawkins's original idea involving mutation by random change and a form of Darwinian selection.\n\nOne technique of meme mapping represents the evolution and transmission of a meme across time and space. Such a meme map uses a figure-8 diagram (an analemma) to map the gestation (in the lower loop), birth (at the choke point), and development (in the upper loop) of the selected meme. Such meme maps are nonscalar, with time mapped onto the y-axis and space onto the x-axis transect. One can read the temporal progression of the mapped meme from south to north on such a meme map. Paull has published a worked example using the \"organics meme\" (as in organic agriculture).\n\n\n"}
{"id": "42372719", "url": "https://en.wikipedia.org/wiki?curid=42372719", "title": "Nader Kadhim", "text": "Nader Kadhim\n\nNader Hassan Ali Kadhim (; born 1973) is a Bahraini writer, academic and cultural critic.\n\nBorn in the village of Al Dair in 1973, Kadhim completed his primary, intermediate and secondary education in government schools. He then enrolled in University of Bahrain where he earned a B.A. in Arabic language and literature in 1995 and Master's degree in Modern Criticism in 2001. Between 1995 and 2003 he worked as a part-time lecturer and research assistant in University of Bahrain and then as teacher in government schools. In 2003, he earned his PhD in Arabic literature from the Cairo-based Institute of Arab Research and Studies of the Arab League. He currently works as Professor of Cultural Studies at University of Bahrain. He is also the editor of two literature magazines published by the same University.\n\nAlthough Kadhim was born to a Shia family, he sees it as no more than a historical coincidence which \"does not entail anything\". In an interview with \"Al-Waqt\" newspaper, he stated that joining college in 1991 was an important turn in his life which until then had been influenced by the Islamic revival in the early 1980s. Kadhim recalled that he took part in several religious activities in his childhood. He refused to be classified as Islamist, Shia Islamist or even Shia intellectual, but considered the Islamic faith and culture that he grew up with as a paving factor to his embracing of postmodernism ideas and distancing him from modernism. Since the early 1990s, Kadhim has shown increasing interest in the critical theory and cultural studies.\n\nIn February 2007, Kadhim was honored by \"Al-Wasat\" newspaper for his \"contributions to the intellectual and cultural\" debate in Bahrain. During the event, Mansoor al-Jamri, the editor-in-chief of \"Al-Wasat\" praised Kadhim for his anti-sectarian writings and said he was an exceptional writer. Several other academics and journalists praised him as well.\n\nKadhim is author to many literature and cultural criticism articles and studies published in Bahraini and Arabic media. Between 2003 and 2013, he has published nine books, two of them earning the \"Bahraini outstanding book\" award in 2003 and 2004.\n"}
{"id": "255991", "url": "https://en.wikipedia.org/wiki?curid=255991", "title": "Narrative", "text": "Narrative\n\nA narrative or story is a report of connected events, real or imaginary, presented in a sequence of written or spoken words, or still or moving images, or both. The word derives from the Latin verb \"narrare\", \"to tell\", which is derived from the adjective \"gnarus\", \"knowing\" or \"skilled\".\n\nNarrative can be organized in a number of thematic or formal categories: non-fiction (such as definitively including creative non-fiction, biography, journalism, transcript poetry, and historiography); fictionalization of historical events (such as anecdote, myth, legend, and historical fiction); and fiction proper (such as literature in prose and sometimes poetry, such as short stories, novels, and narrative poems and songs, and imaginary narratives as portrayed in other textual forms, games, or live or recorded performances).\n\nNarrative is found in all forms of human creativity, art, and entertainment, including speech, literature, theatre, music and song, comics, journalism, film, television and video, video games, radio, gameplay, unstructured recreation, and performance in general, as well as some painting, sculpture, drawing, photography, and other visual arts, as long as a sequence of events is presented. Several art movements, such as modern art, refuse the narrative in favor of the abstract and conceptual.\n\nOral storytelling is the earliest method for sharing narratives. During most people's childhoods, narratives are used to guide them on proper behavior, cultural history, formation of a communal identity, and values, as especially studied in anthropology today among traditional indigenous peoples.\n\nNarratives may also be nested within other narratives, such as narratives told by an unreliable narrator (a character) typically found in noir fiction genre. An important part of narration is the narrative mode, the set of methods used to communicate the narrative through a process narration (see also \"Narrative Aesthetics\" below).\n\nAlong with exposition, argumentation, and description, narration, broadly defined, is one of four rhetorical modes of discourse. More narrowly defined, it is the fiction-writing mode in which the narrator communicates directly to the reader.\n\nA narrative is a telling of some true or fictitious event or connected sequence of events, recounted by a narrator to a narratee (although there may be more than one of each). Narratives are to be distinguished from descriptions of qualities, states, or situations, and also from dramatic enactments of events (although a dramatic work may also include narrative speeches). A narrative consists of a set of events (the story) recounted in a process of narration (or discourse), in which the events are selected and arranged in a particular order (the plot). The category of narratives includes both the shortest accounts of events (for example, \"the cat sat on the mat\", or a brief news item) and the longest historical or biographical works, diaries, travelogues, and so forth, as well as novels, ballads, epics, short stories, and other fictional forms. In the study of fiction, it is usual to divide novels and shorter stories into first-person narratives and third-person narratives. As an adjective, \"narrative\" means \"characterized by or relating to storytelling\": thus narrative technique is the method of telling stories, and narrative poetry is the class of poems (including ballads, epics, and verse romances) that tell stories, as distinct from dramatic and lyric poetry. Some theorists of narratology have attempted to isolate the quality or set of properties that distinguishes narrative from non-narrative writings: this is called narrativity.\n\nOwen Flanagan of Duke University, a leading consciousness researcher, writes, \"Evidence strongly suggests that humans in all cultures come to cast their own identity in some sort of narrative form. We are inveterate storytellers.\" Stories are an important aspect of culture. Many works of art and most works of literature tell stories; indeed, most of the humanities involve stories.\nStories are of ancient origin, existing in ancient Egyptian, ancient Greek, Chinese and Indian cultures and their myths. Stories are also a ubiquitous component of human communication, used as parables and examples to illustrate points. Storytelling was probably one of the earliest forms of entertainment. As noted by Owen Flanagan, narrative may also refer to psychological processes in self-identity, memory and meaning-making.\n\nSemiotics begins with the individual building blocks of meaning called signs; and semantics, the way in which signs are combined into codes to transmit messages. This is part of a general communication system using both verbal and non-verbal elements, and creating a discourse with different\nmodalities and forms.\n\nIn \"On Realism in Art\" Roman Jakobson argues that literature exists as a separate entity. He and many other semioticians prefer the view that all texts, whether spoken or written, are the same, except that some authors encode their texts with distinctive \"literary\" qualities that distinguish them from other forms of discourse. Nevertheless, there is a clear trend to address literary narrative forms as separable from other forms. This is first seen in Russian Formalism through Victor Shklovsky's analysis of the relationship between composition and style, and in the work of Vladimir Propp, who analysed the plots used in traditional folk-tales and identified 31 distinct functional components. This trend (or these trends) continued in the work of the Prague School and of French scholars such as Claude Lévi-Strauss and Roland Barthes. It leads to a structural analysis of narrative and an increasingly influential body of modern work that raises important theoretical questions:\n\nIn literary theoretic approach, narrative is being narrowly defined as fiction-writing mode in which the narrator is communicating directly to the reader. Until the late 19th century, literary criticism as an academic exercise dealt solely with poetry (including epic poems like the \"Iliad\" and \"Paradise Lost,\" and poetic drama like Shakespeare). Most poems did not have a narrator distinct from the author.\n\nBut novels, lending a number of voices to several characters in addition to narrator's, created a possibility of narrator's views differing significantly from the author's views. With the rise of the novel in the 18th century, the concept of the narrator (as opposed to \"author\") made the question of narrator a prominent one for literary theory. It has been proposed that perspective and interpretive knowledge are the essential characteristics, while focalization and structure are lateral characteristics of the narrator.\n\nThe role of literary theory in narrative has been disputed; with some interpretations like Todorov's narrative model that views all narratives in a cyclical manner, and that each narrative is characterized by a three part structure that allows the narrative to progress. The beginning stage being an establishment of equilibrium--a state of non conflict, followed by a disruption to this state, caused by an external event, and lastly a restoration or a return to equilibrium--a conclusion that brings the narrative back to a similar space before the events of the narrative unfolded.\n\nOther critiques of literary theory in narrative challenge the very role of literariness in narrative, as well as the role of narrative in literature. Meaning, narratives and their associated aesthetics, emotions, and values have the ability to operate without the presence of literature and vice versa. According to Didier Costa, the structural model used by Todorov and others is unfairly biased towards a Western interpretation of narrative, and that a more comprehensive and transformative model must be created in order to properly analyze narrative discourse in literature. Framing also plays a pivotal role in narrative structure; an analysis of the historical and cultural contexts present during the development of a narrative is needed in order to more accurately represent the role of narratology in societies that relied heavily on oral narratives.\n\nA writer's choice in the narrator is crucial for the way a work of fiction is perceived by the reader. There is a distinction between first-person and third-person narrative, which Gérard Genette refers to as intradiegetic and extradiegetic narrative, respectively. Intradiagetic narrators are of two types: a homodiegetic narrator participates as a character in the story. Such a narrator cannot know more about other characters than what their actions reveal. A heterodiegetic narrator, in contrast, describes the experiences of the characters that appear in the story in which he or she does not participate.\n\nMost narrators present their story from one of the following perspectives (called narrative modes): first-person, or third-person limited or omniscient. Generally, a first-person narrator brings greater focus on the feelings, opinions, and perceptions of a particular character in a story, and on how the character views the world and the views of other characters. If the writer's intention is to get inside the world of a character, then it is a good choice, although a third-person limited narrator is an alternative that does not require the writer to reveal all that a first-person character would know. By contrast, a third-person omniscient narrator gives a panoramic view of the world of the story, looking into many characters and into the broader background of a story. A third-person omniscient narrator can be an animal or an object, or it can be a more abstract instance that does not refer to itself. For stories in which the context and the views of many characters are important, a third-person narrator is a better choice. However, a third-person narrator does not need to be an omnipresent guide, but instead may merely be the protagonist referring to himself in the third person (also known as third person limited narrator).\n\nA writer may choose to let several narrators tell the story from different points of view. Then it is up to the reader to decide which narrator seems most reliable for each part of the story. It may refer to the style of the writer in which he/she expresses the paragraph written. See for instance the works of Louise Erdrich. William Faulkner's \"As I Lay Dying\" is a prime example of the use of multiple narrators. Faulkner employs stream of consciousness to narrate the story from various perspectives.\n\nIn Indigenous American communities, narratives and storytelling are often told by a number of elders in the community. In this way, the stories are never static because they are shaped by the relationship between narrator and audience. Thus, each individual story may have countless variations. Narrators often incorporate minor changes in the story in order to tailor the story to different audiences.\n\nThe use of multiple narratives in a story is not simply a stylistic choice, but rather an interpretive one that offers insight into the development of a larger social identity and the impact that has on the overarching narrative, as explained by Lee Haring. Haring analyzes the use of framing in oral narratives, and how the usage of multiple perspectives provides the audience with a greater historical and cultural background of the narrative. She also argues that narratives (particularly myths and folktales) that implement multiple narrators deserves to be categorized as its own narrative genre, rather than simply a narrative device that is used solely to explain phenomena from different points of view.\n\nHaring provides an example from the Arabic folktales of \"A Thousand and One Nights\" to illustrate how framing was used to loosely connect each story to the next, where each story was enclosed within the larger narrative. Additionally, Haring draws comparisons between \"Thousand and One Nights\" and the oral storytelling observed in parts of rural Ireland, islands of the Southwest Indian Ocean, and African cultures such as Madagascar.\"I'll tell you what I'll do,\" said the smith. \"I'll fix your sword for you tomorrow, if you tell me a story while I'm doing it.\" The speaker was an Irish storyteller in 1935, framing one story in another (O'Sullivan 75, 264). The moment recalls the Thousand and One Nights , where the story of \"The Envier and the Envied\" is enclosed in the larger story told by the Second Kalandar (Burton 1 : 113-39), and many stories are enclosed in others.\"\n\nNarrative is a highly aesthetic art. Thoughtfully composed stories have a number of aesthetic elements. Such elements include the idea of narrative structure, with identifiable beginnings, middles and ends, or exposition-development-climax-denouement, with coherent plot lines; a strong focus on temporality including retention of the past, attention to present action and protention/future anticipation; a substantial focus on character and characterization, \"arguably the most important single component of the novel\" (David Lodge \"The Art of Fiction\" 67); different voices interacting, \"the sound of the human voice, or many voices, speaking in a variety of accents, rhythms and registers\" (Lodge \"The Art of Fiction\" 97; see also the theory of Mikhail Bakhtin for expansion of this idea); a narrator or narrator-like voice, which \"addresses\" and \"interacts with\" reading audiences (see Reader Response theory); communicates with a Wayne Booth-esque rhetorical thrust, a dialectic process of interpretation, which is at times beneath the surface, forming a plotted narrative, and at other times much more visible, \"arguing\" for and against various positions; relies substantially on the use of literary tropes (see Hayden White, \"Metahistory\" for expansion of this idea); is often intertextual with other literatures; and commonly demonstrates an effort toward \"bildungsroman\", a description of identity development with an effort to evince \"becoming\" in character and community. \n\nWithin philosophy of mind, the social sciences and various clinical fields including medicine, narrative can refer to aspects of human psychology. A personal narrative process is involved in a person's sense of personal or cultural identity, and in the creation and construction of memories; it is thought by some to be the fundamental nature of the self. The breakdown of a coherent or positive narrative has been implicated in the development of psychosis and mental disorder, and its repair said to play an important role in journeys of recovery. Narrative Therapy is a school of (family) psychotherapy.\n\nIllness narratives are a way for a person affected by an illness to make sense of his or her experiences. They typically follow one of several set patterns: \"restitution\", \"chaos\", or \"quest\" narratives. In the restitution narrative, the person sees the illness as a temporary detour. The primary goal is to return permanently to normal life and normal health. These may also be called cure narratives. In the chaos narrative, the person sees the illness as a permanent state that will inexorably get worse, with no redeeming virtues. This is typical of diseases like Alzheimer's disease: the patient gets worse and worse, and there is no hope of returning to normal life. The third major type, the quest narrative, positions the illness experience as an opportunity to transform oneself into a better person through overcoming adversity and re-learning what is most important in life; the physical outcome of the illness is less important than the spiritual and psychological transformation. This is typical of the triumphant view of cancer survivorship in the breast cancer culture.\n\nPersonality traits, more specifically the Big Five personality traits, appear to be associated with the type of language or patterns of word use found in an individual's self-narrative. In other words, language use in self-narratives accurately reflects human personality. The linguistic correlates of each Big Five trait are as follows:\n\n\nHuman beings often claim to understand events when they manage to formulate a coherent story or narrative explaining how they believe the event was generated. Narratives thus lie at foundations of our cognitive procedures and also provide an explanatory framework for the social sciences, particularly when it is difficult to assemble enough cases to permit statistical analysis. Narrative is often used in case study research in the social sciences. Here it has been found that the dense, contextual, and interpenetrating nature of social forces uncovered by detailed narratives is often more interesting and useful for both social theory and social policy than other forms of social inquiry.\n\nSociologists Jaber F. Gubrium and James A. Holstein have contributed to the formation of a constructionist approach to narrative in sociology. From their book The Self We Live By: Narrative Identity in a Postmodern World (2000), to more recent texts such as Analyzing Narrative Reality (2009)and Varieties of Narrative Analysis (2012), they have developed an analytic framework for researching stories and storytelling that is centered on the interplay of institutional discourses (big stories) on the one hand, and everyday accounts (little stories) on the other. The goal is the sociological understanding of formal and lived texts of experience, featuring the production, practices, and communication of accounts.\n\nIn order to avoid \"hardened stories,\" or \"narratives that become context-free, portable and ready to be used anywhere and anytime for illustrative purposes\" and are being used as conceptual metaphors as defined by linguist George Lakoff, an approach called narrative inquiry was proposed, resting on the epistemological assumption that human beings make sense of random or complex multicausal experience by the imposition of story structures.\" Human propensity to simplify data through a predilection for narratives over complex data sets typically leads to narrative fallacy. It is easier for the human mind to remember and make decisions on the basis of stories with meaning, than to remember strings of data. This is one reason why narratives are so powerful and why many of the classics in the humanities and social sciences are written in the narrative format. But humans read meaning into data and compose stories, even where this is unwarranted. In narrative inquiry, the way to avoid the narrative fallacy is no different from the way to avoid other error in scholarly research, i.e., by applying the usual methodical checks for validity and reliability in how data are collected, analyzed, and presented. Several criteria for assessing the validity of narrative research was proposed, including the objective aspect, the emotional aspect, the social/moral aspect, and the clarity of the story.\n\nIn mathematical sociology, the theory of comparative narratives was devised in order to describe and compare the structures (expressed as \"and\" in a directed graph where multiple causal links incident into a node are conjoined) of action-driven sequential events.\n\nNarratives so conceived comprise the following ingredients:\n\nThe structure (directed graph) is generated by letting the nodes stand for the states and the directed edges represent how the states are changed by specified actions. The action skeleton can then be abstracted, comprising a further digraph where the actions are depicted as nodes and edges take the form \"action a co-determined (in context of other actions) action b\".\n\nNarratives can be both abstracted and generalised by imposing an algebra upon their structures and thence defining homomorphism between the algebras. The insertion of action-driven causal links in a narrative can be achieved using the method of Bayesian narratives.\n\nDeveloped by Peter Abell, the theory of Bayesian Narratives conceives a narrative as a directed graph comprising multiple causal links (social interactions) of the general form: \"action a causes action b in a specified context\". In the absence of sufficient comparative cases to enable statistical treatment of the causal links, items of evidence in support and against a particular causal link are assembled and used to compute the Bayesian likelihood ratio of the link. Subjective causal statements of the form \"I/she did b because of a\" and subjective counterfactuals \"if it had not been for a I/she would not have done b\" are notable items of evidence.\n\nLinearity is one of several narrative qualities that can be found in a musical composition. As noted by American musicologist, Edward Cone, narrative terms are also present in the analytical language about music. The different components of a fugue — subject, answer, exposition, discussion and summary — can be cited as an example. However, there are several views on the concept of narrative in music and the role it plays.\nOne theory is that of Theodore Adorno, who has suggested that ‘music recites itself, is its own context, narrates without narrative’. Another, is that of Carolyn Abbate, who has suggested that ‘certain gestures experienced in music constitute a narrating voice’. Still others have argued that narrative is a semiotic enterprise that can enrich musical analysis.\nThe French musicologist Jean-Jacques Nattiez contends that ‘the narrative, strictly speaking, is not in the music, but in the plot imagined and constructed by the listeners’. He argues that discussing music in terms of narrativity is simply metaphorical and that the ‘imagined plot’ may be influenced by the work's title or other programmatic information provided by the composer. However, Abbate has revealed numerous examples of musical devices that function as narrative voices, by limiting music’s ability to narrate to rare ‘moments that can be identified by their bizarre and disruptive effect’. Various theorists share this view of narrative appearing in disruptive rather than normative moments in music.\nThe final word is yet to be said, regarding narratives in music, as there is still much to be determined.\n\nA narrative can take on the shape of a story, which gives listeners an entertaining and collaborative avenue for acquiring knowledge. Many cultures use storytelling as a way to record histories, myths, and values. These stories can be seen as living entities of narrative among cultural communities, as they carry the shared experience and history of the culture within them. Stories are often used within indigenous cultures in order to share knowledge to the younger generation. Due to indigenous narratives leaving room for open-ended interpretation, native stories often engage children in the storytelling process so that they can make their own meaning and explanations within the story. This promotes holistic thinking among native children, which works towards merging an individual and world identity. Such an identity upholds native epistemology and gives children a sense of belonging as their cultural identity develops through the sharing and passing on of stories.\n\nFor example, a number of indigenous stories are used to illustrate a value or lesson. In the Western Apache tribe, stories can be used to warn of the misfortune that befalls people when they do not follow acceptable behavior. One story speaks to the offense of a mother's meddling in her married son's life. In the story, the Western Apache tribe is under attack from a neighboring tribe, the Pimas. The Apache mother hears a scream. Thinking it is her son's wife screaming, she tries to intervene by yelling at him. This alerts the Pima tribe to her location, and she is promptly killed due to intervening in her son's life.\n\nIndigenous American cultures use storytelling to teach children the values and lessons of life. \nAlthough storytelling provides entertainment, its primary purpose is to educate. Alaskan Indigenous Natives state that narratives teach children where they fit in, what their society expects of them, how to create a peaceful living environment, and to be responsible, worthy members of their communities. In the Mexican culture, many adult figures tell their children stories in order to teach children values such as individuality, obedience, honesty, trust, and compassion. For example, one of the versions of La Llorona is used to teach children to make safe decisions at night and to maintain the morals of the community.\n\nNarratives are considered by the Canadian Métis community, to help children understand that the world around them is interconnected to their lives and communities. For example, the Métis community share the “Humorous Horse Story” to children, which portrays that horses stumble throughout life just like humans do. Navajo stories also use dead animals as metaphors by showing that all things have purpose. Lastly, elders from Alaskan Native communities claim that the use of animals as metaphors allow children to form their own perspectives while at the same time self-reflecting on their own lives.\n\nAmerican Indian elders also state that storytelling invites the listeners, especially children, to draw their own conclusions and perspectives while self-reflecting upon their lives. Furthermore, they insist that narratives help children grasp and obtain a wide range of perspectives that help them interpret their lives in the context of the story. American Indian community members emphasize to children that the method of obtaining knowledge can be found in stories passed down through each generation. Moreover, community members also let the children interpret and build a different perspective of each story.\n\nIn historiography, according to Lawrence Stone, narrative has traditionally been the main rhetorical device used by historians. In 1979, at a time when the new Social History was demanding a social-science model of analysis, Stone detected a move back toward the narrative. Stone defined narrative as organized chronologically; focused on a single coherent story; descriptive rather than analytical; concerned with people not abstract circumstances; and dealing with the particular and specific rather than the collective and statistical. He reported that, \"More and more of the 'new historians' are now trying to discover what was going on inside people's heads in the past, and what it was like to live in the past, questions which inevitably lead back to the use of narrative.\"\n\nSome philosophers identify narratives with a type of explanation. Mark Bevir argues, for example, that narratives explain actions by appealing to the beliefs and desires of actors and by locating webs of beliefs in the context of historical traditions. Narrative is an alternative form of explanation to that associated with natural science.\n\nHistorians committed to a social science approach, however, have criticized the narrowness of narrative and its preference for anecdote over analysis, and clever examples rather than statistical regularities.\n\nStorytelling rights may be broadly defined as the ethics of sharing narratives (including—but not limited to—firsthand, secondhand and imagined stories). In \"Storytelling Rights: The uses of oral and written texts by urban adolescents\", author Amy Shuman offers the following definition of storytelling rights: “the important and precarious relationship between narrative and event and, specifically, between the participants in an event and the reporters who claim the right to talk about what happened.\"\n\nThe ethics of retelling other people’s stories may be explored through a number of questions: whose story is being told and how, what is the story’s purpose or aim, what does the story promise (for instance: empathy, redemption, authenticity, clarification)--and at whose benefit? Storytelling rights also implicates questions of consent, empathy, and accurate representation. While storytelling—and retelling—can function as a powerful tool for agency and advocacy, it can also lead to misunderstanding and exploitation.\n\nStorytelling rights is notably important in the genre of personal experience narrative. Academic disciplines such as performance, folklore, literature, anthropology, Cultural Studies and other social sciences may involve the study of storytelling rights, often hinging on ethics.\n\n\n\n\n"}
{"id": "6337134", "url": "https://en.wikipedia.org/wiki?curid=6337134", "title": "New Sincerity", "text": "New Sincerity\n\nNew Sincerity (closely related to and sometimes described as synonymous with post-postmodernism) is a trend in music, aesthetics, literary fiction, film criticism, poetry, literary criticism and philosophy. It generally describes creative works that expand upon and break away from concepts of postmodernist irony and cynicism, representing a partial return to modernism. Its usage dates back to the mid-1980s; however, it was popularized in the 1990s by American author David Foster Wallace.\n\n\"New Sincerity\" was used as a collective name for a loose group of alternative rock bands, centered in Austin, Texas in the years from about 1985 to 1990, who were perceived as reacting to the ironic outlook of then-prominent music movements like punk rock and new wave. The use of \"New Sincerity\" in connection with these bands began with an off-handed comment by Austin punk rocker/author Jesse Sublett to his friend, local music writer Margaret Moser. According to author Barry Shank, Sublett said: \"All those new sincerity bands, they're crap.\" Sublett (at his own website) states that he was misquoted, and actually told Moser, \"It's all new sincerity to me... It's not my cup of tea.\" In any event, Moser began using the term in print, and it ended up becoming the catch phrase for these bands.\n\nNationally, the most successful \"New Sincerity\" band was The Reivers (originally called Zeitgeist), who released four well-received albums between 1985 and 1991. True Believers, led by Alejandro Escovedo and Jon Dee Graham, also received extensive critical praise and local acclaim in Austin, but the band had difficulty capturing its live sound on recordings, among other problems. Other important \"New Sincerity\" bands included Doctors Mob, Wild Seeds, and Glass Eye. Another significant \"New Sincerity\" figure was the eccentric, critically acclaimed songwriter Daniel Johnston.\n\nDespite extensive critical attention (including national coverage in \"Rolling Stone\" and a 1985 episode of the MTV program \"The Cutting Edge\"), none of the \"New Sincerity\" bands met with much commercial success, and the \"scene\" ended within a few years.\n\nOther music writers have used \"new sincerity\" to describe later performers such as Arcade Fire, Conor Oberst, Cat Power, Devendra Banhart, Joanna Newsom, Neutral Milk Hotel, Sufjan Stevens Idlewild, and Father John Misty, as well as Austin's Okkervil River Leatherbag, and Michael Waller.\n\nCritic Jim Collins introduced the concept of \"new sincerity\" to film criticism in his 1993 essay entitled “Genericity in the 90s: Eclectic Irony and the New Sincerity”. In this essay he contrasts films that treat genre conventions with \"eclectic irony\" and those that treat them seriously, with \"new sincerity\". Collins describes\nthe 'new sincerity' of films like \"Field of Dreams\" (1989), \"Dances With Wolves\" (1990), and \"Hook\" (1991), all of which depend not on hybridization, but on an \"ethnographic\" rewriting of the classic genre film that serves as their inspiration, all attempting, using one strategy or another, to recover a lost \"purity\", which apparently pre-existed even the Golden Age of film genre.\n\nOther critics have suggested \"new sincerity\" as a descriptive term for work by American filmmakers such as Wes Anderson, Paul Thomas Anderson, Todd Louiso, Sofia Coppola, Charlie Kaufman, Zach Braff, and Jared Hess, and filmmakers from other countries such as Michel Gondry, Lars von Trier, the Dogme 95 movement, Aki Kaurismäki, and Pedro Almodóvar. The \"aesthetics of new sincerity\" have also been connected to other art forms including \"reality television, Internet blogs, diary style 'chicklit' literature, [and] personal videos on You-Tube. . . . \"\n\nIn response to the hegemony of metafictional and self-conscious irony in contemporary fiction, writer David Foster Wallace predicted, in his 1993 essay \"E Unibus Pluram: Television and U.S. Fiction\", a new literary movement which would espouse something like the New Sincerity ethos:\"The next real literary “rebels” in this country might well emerge as some weird bunch of anti-rebels, born oglers who dare somehow to back away from ironic watching, who have the childish gall actually to endorse and instantiate single-entendre principles. Who treat of plain old untrendy human troubles and emotions in U.S. life with reverence and conviction. Who eschew self-consciousness and hip fatigue. These anti-rebels would be outdated, of course, before they even started. Dead on the page. Too sincere. Clearly repressed. Backward, quaint, naive, anachronistic. Maybe that’ll be the point. Maybe that’s why they’ll be the next real rebels. Real rebels, as far as I can see, risk disapproval. The old postmodern insurgents risked the gasp and squeal: shock, disgust, outrage, censorship, accusations of socialism, anarchism, nihilism. Today’s risks are different. The new rebels might be artists willing to risk the yawn, the rolled eyes, the cool smile, the nudged ribs, the parody of gifted ironists, the “Oh how banal”. To risk accusations of sentimentality, melodrama. Of overcredulity. Of softness. Of willingness to be suckered by a world of lurkers and starers who fear gaze and ridicule above imprisonment without law. Who knows.\"This was further examined on the blog \"Fiction Advocate by Mike Moats:\"\"The theory is this: \"Infinite Jest\" is Wallace’s attempt to both manifest and dramatize a revolutionary fiction style that he called for in his essay “E Unibus Pluram: Television and U.S. Fiction.” The style is one in which a new sincerity will overturn the ironic detachment that hollowed out contemporary fiction towards the end of the 20th century. Wallace was trying to write an antidote to the cynicism that had pervaded and saddened so much of American culture in his lifetime. He was trying to create an entertainment that would get us talking again.\"In his 2010 essay \"David Foster Wallace and the New Sincerity in American Fiction\", Adam Kelly argues that Wallace's fiction, and that of his generation, is marked by a revival and theoretical reconception of sincerity, challenging the emphasis on authenticity that dominated twentieth-century literature and conceptions of the self. Additionally, numerous authors have been described as contributors to the New Sincerity movement, including Jonathan Franzen, Zadie Smith, Dave Eggers, Stephen Graham Jones, and Michael Chabon.\n\n\"New sincerity\" has also sometimes been used to refer to a philosophical concept deriving from the basic tenets of performatism. It is also seen as one of the key characteristics of metamodernism. Related literature includes Wendy Steiner's \"The Trouble with Beauty\" and Elaine Scarry's \"On Beauty and Being Just\". Related movements may include post-postmodernism, New Puritans, Stuckism, the Kitsch movement and remodernism, as well as the Dogme 95 film movement led by Lars von Trier and others.\n\n\"The New Sincerity\" has been espoused since 2002 by radio host Jesse Thorn of PRI's \"The Sound of Young America\" (now \"Bullseye\"), self-described as \"the public radio program about things that are awesome\". Thorn characterizes New Sincerity as a cultural movement defined by dicta including \"Maximum Fun\" and \"Be More Awesome\". It celebrates outsized celebration of joy, and rejects irony, and particularly ironic appreciation of cultural products. Thorn has promoted this concept on his program and in interviews to the point that a scholarly work on Russian post-Soviet aesthetic theory included mention of Thorn as American popularizer of the term \"new sincerity\". A typical explication of Thorn's concept is this 2006 \"Manifesto for the New Sincerity\":\nWhat is The New Sincerity? Think of it as irony and sincerity combined like Voltron, to form a new movement of astonishing power. Or think of it as the absence of irony and sincerity, where less is (obviously) more. If those strain the brain, just think of Evel Knievel. Let's be frank. There's no way to appreciate Evel Knievel literally. Evel is the kind of man who defies even fiction, because the reality is too over the top. Here is a man in a red-white-and-blue leather jumpsuit, driving some kind of rocket car. A man who achieved fame and fortune jumping over things. Here is a real man who feels at home as Spidey on the cover of a comic book. Simply put, Evel Knievel boggles the mind. But by the same token, he isn't to be taken ironically, either. The fact of the matter is that Evel is, in a word, awesome. . . . Our greeting: a double thumbs-up. Our credo: \"Be More Awesome\". Our lifestyle: \"Maximum Fun\". Throw caution to the wind, friend, and live The New Sincerity.\n\nIn a September 2009 interview, Thorn commented that \"new sincerity\" had begun as \"a silly, philosophical movement that me and some friends made up in college\" and that \"everything that we said was a joke, but at the same time it wasn’t all a joke in the sense that we weren’t being arch or we weren’t being campy. While we were talking about ridiculous, funny things we were sincere about them.\"\n\nThorn's concept of \"new sincerity\" as a social response has gained popularity since his introduction of the term in 2002. Several point to the September 11, 2001 attacks and the subsequent wake of events that created this movement, in which there was a drastic shift in tone. The 1990s were considered a period of artistic works ripe with irony, and the attacks shocked a change in the American culture. Graydon Carter, editor of \"Vanity Fair\", published an editorial a few weeks after the attacks claiming that \"this was the end of the age of irony\". Jonathan D. Fitzgerald for \"The Atlantic\" suggests this new movement could also be attributed to broader periodic shifts that occur in culture.\n\nAs a result of this movement, several cultural works, including many identified above, were considered elements of \"new sincerity\", but this was also seen to be a mannerism adopted by the general public, to show appreciation for cultural works that they happened to enjoy. Andrew Watercutter of \"Wired\" saw this as having being able to enjoy one's guilty pleasures without having to feel guilty about enjoying it, and being able to share that appreciation with others. One such example of a \"new sincerity\" movement is the , generally adult and primarily male fans of the 2010 animated show \"\" which is produced by Hasbro to sell its toys to young girls. These fans have been called \"internet neo-sincerity at its best\", unabashedly enjoying the show and challenging the preconceived gender roles that such a show ordinarily carries.\n\nIn Russia, the term \"new sincerity\" (\"novaya iskrennost\") was used as early as the mid-1980s or early 1990s by dissident poet Dmitry Prigov and critic Mikhail Epstein, as a response to the dominant sense of absurdity in late Soviet and post-Soviet culture. In Epstein's words, \"Postconceptualism, or the New Sincerity, is an experiment in resuscitating \"fallen\", dead languages with a renewed pathos of love, sentimentality and enthusiasm.\n\nThis conception of \"new sincerity\" meant the avoidance of cynicism, but not necessarily of irony. In the words of Professor Alexei Yurchak of the University of California, Berkeley, it \"is a particular brand of irony, which is sympathetic and warm, and allows its authors to remain committed to the ideals that they discuss, while also being somewhat ironic about this commitment\".\n\nNowadays New Sincerity is being contraposed not to Soviet literature, but to postmodernism. Dmitry Vodennikov has been acclaimed as the leader of the new wave of Russian New Sincerity, as was Victor Pelevin.\n\nSince 2005, poets including Reb Livingston, Joseph Massey, Andrew Mister, and Anthony Robinson have collaborated in a blog-driven poetry movement, described by Massey as \"a ‘new sincerity’ brewing in American poetry—a contrast to the cold, irony-laden poetry dominating the journals and magazines and new books of poetry\". Other poets named as associated with this movement, or its tenets, have included David Berman, Catherine Wagner, Dean Young, Matt Hart, Miranda July, Tao Lin, Steve Roggenbuck, D.S. Chapman, Frederick Seidel, Arielle Greenberg, Karyna McGlynn, and Mira Gonzalez.\n"}
{"id": "3819214", "url": "https://en.wikipedia.org/wiki?curid=3819214", "title": "Nomadic pastoralism", "text": "Nomadic pastoralism\n\nNomadic pastoralism is a form of pastoralism when livestock are herded in order to find fresh pastures on which to graze. Strictly speaking, true nomads follow an irregular pattern of movement, in contrast with transhumance where seasonal pastures are fixed. However this distinction is often not observed and the term nomad used for both—in historical cases the regularity of movements is often unknown in any case. The herded livestock include cattle, yaks, sheep, goats, reindeer, horses, donkeys or camels, or mixtures of species. Nomadic pastoralism is commonly practised in regions with little arable land, typically in the developing world, especially in the steppe lands north of the agricultural zone of Eurasia. Of the estimated 30–40 million nomadic pastoralists worldwide, most are found in central Asia and the Sahel region of West Africa. Increasing numbers of stock may lead to overgrazing of the area and desertification if lands are not allowed to fully recover between one grazing period and the next. Increased enclosure and fencing of land has reduced the amount of land available for this practice. There is substantive uncertainty over the extent to which the various causes for degradation affect grassland. Different causes have been identified which include overgrazing, mining, agricultural reclamation, pests and rodents, soil properties, tectonic activity, and climate change. Simultaneously, it is maintained that some, such as overgrazing and overstocking, may be overstated while others, such as climate change, mining and agricultural reclamation, may be under reported. In this context, there is also uncertainty as to the long term effect of human behavior on the grassland as compared to non-biotic factors.\n\nNomadic pastoralism was a result of the Neolithic revolution. During the revolution, humans began domesticating animals and plants for food and started forming cities. Nomadism generally has existed in symbiosis with such settled cultures trading animal products (meat, hides, wool, cheeses and other animal products) for manufactured items not produced by the nomadic herders. Henri Fleisch tentatively suggested the Shepherd Neolithic industry of Lebanon may date to the Epipaleolithic and that it may have been used by one of the first cultures of nomadic shepherds in the Beqaa valley. Andrew Sherratt demonstrates that \"early farming populations used livestock mainly for meat, and that other applications were explored as agriculturalists adapted to new conditions, especially in the semi‐arid zone.\" \nIn the past it was asserted that pastoral nomads left no presence archaeologically or were impoverished, but this has now been challenged, and was clearly not so for many ancient Eurasian nomads, who have left very rich kurgan burial sites. Pastoral nomadic sites are identified based on their location outside the zone of agriculture, the absence of grains or grain-processing equipment, limited and characteristic architecture, a predominance of sheep and goat bones, and by ethnographic analogy to modern pastoral nomadic peoples Juris Zarins has proposed that pastoral nomadism began as a cultural lifestyle in the wake of the 6200 BC climatic crisis when Harifian pottery making hunter-gatherers in the Sinai fused with Pre-Pottery Neolithic B agriculturalists to produce the Munhata culture, a nomadic lifestyle based on animal domestication, developing into the Yarmoukian and thence into a circum-Arabian nomadic pastoral complex, and spreading Proto-Semitic languages. \n\nIn Bronze Age Central Asia, nomadic populations are associated with the earliest transmissions of millet and wheat grains through the region that eventually became central for the Silk Road. By the medieval period in Central Asia, nomadic communities exhibited isotopically diverse diets, suggesting a multitude of subsistence strategies.\n\nOften traditional nomadic groups settle into a regular seasonal pattern of transhumance. An example of a normal nomadic cycle in the northern hemisphere is:\n\nThe movements in this example are about 180 to 200 km. Camps are established in the same place each year; often semi-permanent shelters are built in at least one place on this migration route.\n\nIn sub-regions such as Chad, the nomadic pastoralist cycle is as follows:\n\nIn Chad, the sturdy villages are called hillé, the less sturdy villages are called dankhout and the tents ferik.\nDavid Christian made the following observations about pastoralism. The agriculturist lives from domesticated plants and the pastoralist lives from domesticated animals. Since animals are higher on the food chain, pastoralism supports a thinner population than agriculture. Pastoralism predominates where low rainfall makes farming impractical. Full pastoralism required the Secondary products revolution when animals began to be used for wool, milk, riding and traction as well as meat. Where grass is poor herds must be moved, which leads to nomadism. Some peoples are fully nomadic while others live in sheltered winter camps and lead their herds into the steppe in summer. Some nomads travel long distances, usually north in summer and south in winter. Near mountains, herds are led uphill in summer and downhill in winter (transhumance). Pastoralists often trade with or raid their agrarian neighbors.\n\nChristian distinguished ‘Inner Eurasia’, which was pastoral with a few hunter-gatherers in the far north, from ‘Outer Eurasia’, a crescent of agrarian civilizations from Europe through India to China. High civilization is based on agriculture where tax-paying peasants support landed aristocrats, kings, cities, literacy and scholars. Pastoral societies are less developed and as a result, according to Christian, more egalitarian. One tribe would often dominate its neighbors, but these ‘empires’ usually broke up after a hundred years or so. The heartland of pastoralism is the Eurasian steppe. In the center of Eurasia pastoralism extended south to Iran and surrounded agrarian oasis cities. When pastoral and agrarian societies went to war, horse-borne mobility counterbalanced greater numbers. Attempts by agrarian civilizations to conquer the steppe usually failed until the last few centuries. Pastoralists frequently raided and sometimes collected regular tribute from their farming neighbors. Especially in north China and Iran, they would sometimes conquer agricultural societies, but these dynasties were usually short-lived and broke up when the nomads became ‘civilized’ and lost their warlike virtues.\n\nNomadic pastoralism was historically widespread throughout less fertile regions of Earth. It is found in areas of low rainfall such as the Arabian Peninsula inhabited by Bedouins, as well as Northeast Africa inhabited by Somalis (where camel, sheep and goat nomadic pastoralism is especially common). Nomadic transhumance is also common in areas of harsh climate, such as Northern Europe and Russia inhabited by the indigenous Sami people, Nenets people and Chukchis. There are an estimated 30-40 million nomads in the world. Pastoral nomads and semi-nomadic pastoralists form a significant but declining minority in such countries as Saudi Arabia (probably less than 3%), Iran (4%), and Afghanistan (at most 10%). They comprise less than 2% of the population in the countries of North Africa except Libya and Mauritania.\n\nThe Eurasian steppe has been largely populated by nomads since early prehistoric times, with a succession of peoples known by the names given to them by surrounding literate societies, including the Scythians, Saka and Yuezhi.\n\nThe Mongols in what is now Mongolia, Russia and China, and the Tatars or Turkic people of Eastern Europe and Central Asia were nomadic people who practiced nomadic transhumance on harsh Asian steppes. Some remnants of these populations are nomadic to this day. In Mongolia, about 40% of the population continues to live a traditional nomadic lifestyle. In China, it is estimated that a little over five million herders are dispersed over the pastoral counties, and more than 11 million over the semi-pastoral counties. This brings the total of the (semi)nomadic herder population to over 16 million, in general living in remote, scattered and resource-poor communities.\n\nIn the Middle Hills and Himalaya of Nepal, people living above about 2,000 m practise transhumance and nomadic pastoralism because settled agriculture becomes less productive due to steep slopes, cooler temperatures and limited irrigation possibilities. Distances between summer and winter pasture may be short, for example in the vicinity of Pokhara where a valley at about 800 meters elevation is less than 20 km. from alpine pastures just below the Annapurna Himalaya, or distances may be 100 km or more. For example, in Rapti zone some 100 km west of Pokhara the Kham Magar move their herds between winter pastures just north of India and summer pastures on the southern slopes of Dhaulagiri Himalaya. In far western Nepal, ethnic Tibetans living in Dolpo and other valleys north among the high Himalaya moved their herds north to winter on the plains of the upper Brahmaputra basin in Tibet proper, until this practice was prohibited after China took over Tibet in 1950-51.\n\nThe nomadic Sami people, an indigenous people of northern Finland, Sweden, Norway, and the Kola Peninsula of Russia, practise a form of nomadic transhumance based on reindeer. In the 14th and 15th century, when reindeer population was sufficiently reduced that Sami could not subsist on hunting alone, some Sami, organized along family lines, became reindeer herders. Each family has traditional territories on which they herd, arriving at roughly the same time each season. Only a small fraction of Sami have subsisted on reindeer herding over the past century; as the most colorful part of the population, they are well known. But as elsewhere in Europe, transhumance is dying out.\n\nThe Mesta was an association of sheep owners, (Spanish nobility and religious orders) that had an important economic and political role in medieval Castile. To preserve the rights of way of its transhumant herds through \"cañadas\", the Mesta acted against small peasants.\n\nIn Chad, nomadic pastoralists include the Zaghawa, Kreda, and Mimi. Farther north in Egypt and western Libya, the Bedouins also practice pastoralism.\n\nSometimes nomadic pastoralists move their herds across international borders in search of new grazing terrain or for trade. This cross-border activity can occasionally lead to tensions with national governments as this activity is often informal and beyond their control and regulation. In East Africa, for example, over 95% of cross-border trade is through unofficial channels and the unofficial trade of live cattle, camels, sheep and goats from Ethiopia sold to Somalia, Kenya and Djibouti generates an estimated total value of between US$250 and US$300 million annually (100 times more than the official figure). This trade helps lower food prices, increase food security, relieve border tensions and promote regional integration. However, there are also risks as the unregulated and undocumented nature of this trade runs risks, such as allowing disease to spread more easily across national borders. Furthermore, governments are unhappy with lost tax revenue and foreign exchange revenues.\n\nThere have been initiatives seeking to promote cross-border trade and also document it, in order to both stimulate regional growth and food security, but also to allow the effective vaccination of livestock. Initiatives include Regional Resilience Enhancement Against Drought (RREAD), the Enhanced Livelihoods in Mandera Triangle/Enhanced Livelihoods in Southern Ethiopia (ELMT/ELSE) as part of the Regional Enhanced Livelihoods in Pastoral Areas (RELPA) programme in East Africa, and the Regional Livelihoods Advocacy Project (REGLAP) funded by the European Commission Humanitarian Aid Office (ECHO).\n\n"}
{"id": "52401447", "url": "https://en.wikipedia.org/wiki?curid=52401447", "title": "Nouvelles Mythologies", "text": "Nouvelles Mythologies\n\nNouvelles Mythologies is a collection of 57 texts written by authors, journalists and editorialists under the direction of Jérôme Garcin and published in 2007 at Éditions du Seuil to celebrate the 50th anniversary of the publication of the essay \"Mythologies\" by Roland Barthes.\n\n\n"}
{"id": "22810417", "url": "https://en.wikipedia.org/wiki?curid=22810417", "title": "Official culture", "text": "Official culture\n\nOfficial culture is the culture that receives social legitimation or institutional support in a given society. Official culture is usually identified with bourgeoisie culture. For revolutionary Guy Debord, official culture is a \"rigged game\", where conservative powers forbid subversive ideas to have direct access to the public discourse, and where such ideas are integrated only after being trivialized and sterilized.\n\nA widespread observation is that a great talent has a free spirit. For instance Pushkin, which some scholar regard as Russia's first great writer, attracted the mad irritation of the Russian officialdom and particularly of the Tsar, since he \n\n\n"}
{"id": "36253964", "url": "https://en.wikipedia.org/wiki?curid=36253964", "title": "Origin of speech", "text": "Origin of speech\n\nThe origin of speech refers to the more general problem of the origin of language in the context of the physiological\ndevelopment of the human speech organs such as the tongue, lips and vocal organs used to produce phonological units in all human languages.\n\nAlthough related to the more general problem of the origin of language, the evolution of distinctively human speech capacities has become a distinct and in many ways separate area of scientific research. The topic is a separate one because language is not necessarily spoken: it can equally be written or signed. Speech is in this sense optional, although it is the default modality for language.\n\nUncontroversially, monkeys, apes and humans, like many other animals, have evolved specialised mechanisms for producing \"sound\" for purposes of social communication. On the other hand, no monkey or ape uses its \"tongue\" for such purposes. Our species' unprecedented use of the tongue, lips and other moveable parts seems to place speech in a quite separate category, making its evolutionary emergence an intriguing theoretical challenge in the eyes of many scholars.\n\nThe term \"modality\" means the chosen representational format for encoding and transmitting information. A striking feature of language is that it is \"modality-independent.\" Should an impaired child be prevented from hearing or producing sound, its innate capacity to master a language may equally find expression in signing. Sign languages of the deaf are independently invented and have all the major properties of spoken language except for the modality of transmission. From this it appears that the language centres of the human brain must have evolved to function optimally irrespective of the selected modality.\n\nThis feature is extraordinary. Animal communication systems routinely combine visible with audible properties and effects, but not one is modality-independent. No vocally impaired whale, dolphin or songbird, for example, could express its song repertoire equally in visual display. Indeed, in the case of animal communication, message and modality are not capable of being disentangled. Whatever message is being conveyed stems from intrinsic properties of the signal.\n\nModality independence should not be confused with the ordinary phenomenon of multimodality. Monkeys and apes rely on a repertoire of species-specific \"gesture-calls\" — emotionally expressive vocalisations inseparable from the visual displays which accompany them. Humans also have species-specific gesture-calls — laughs, cries, sobs and so forth — together with involuntary gestures accompanying speech. Many animal displays are polymodal in that each appears designed to exploit multiple channels simultaneously.\n\nThe human linguistic property of \"modality independence\" is conceptually distinct from this. It allows the speaker to encode the informational content of a message in a single channel, while switching between channels as necessary. Modern city-dwellers switch effortlessly between the spoken word and writing in its various forms — handwriting, typing, e-mail and so forth. Whichever modality is chosen, it can reliably transmit the full message content without external assistance of any kind. When talking on the telephone, for example, any accompanying facial or manual gestures, however natural to the speaker, are not strictly necessary. When typing or manually signing, conversely, there's no need to add sounds. In many Australian Aboriginal cultures, a section of the population — perhaps women observing a ritual taboo — traditionally restrict themselves for extended periods to a silent (manually signed) version of their language. Then, when released from the taboo, these same individuals resume narrating stories by the fireside or in the dark, switching to pure sound without sacrifice of informational content.\n\nSpeaking is the default modality for language in all cultures. Humans' first recourse is to encode our thoughts in sound — a method which depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus.\n\nThe speech organs, everyone agrees, evolved in the first instance not for speech but for more basic bodily functions such as feeding and breathing. Nonhuman primates have broadly similar organs, but with different neural controls. Apes use their highly flexible, maneuverable tongues for eating but not for vocalizing. When an ape is not eating, fine motor control over its tongue is deactivated. \"Either\" it is performing gymnastics with its tongue \"or\" it is vocalising; it cannot perform both activities simultaneously. Since this applies to mammals in general, \"Homo sapiens\" is exceptional in harnessing mechanisms designed for respiration and ingestion to the radically different requirements of articulate speech.\n\nThe word \"language\" derives from the Latin \"lingua,\" \"tongue\". Phoneticians agree that the tongue is the most important speech articulator, followed by the lips. A natural language can be viewed as a particular way of using the tongue to express thought.\n\nThe human tongue has an unusual shape. In most mammals, it's a long, flat structure contained largely within the mouth. It is attached at the rear to the hyoid bone, situated below oral level in the pharynx. In humans, the tongue has an almost circular sagittal (midline) contour, much of it lying vertically down an extended pharynx, where it is attached to a hyoid bone in a lowered position. Partly as a result of this, the horizontal (inside-the-mouth) and vertical (down-the-throat) tubes forming the supralaryngeal vocal tract (SVT) are almost equal in length (whereas in other species, the vertical section is shorter). As we move our jaws up and down, the tongue can vary the cross-sectional area of each tube independently by about 10:1, altering formant frequencies accordingly. That the tubes are joined at a right angle permits pronunciation of the vowels [i], [u] and [a], which nonhuman primates cannot do. Even when not performed particularly accurately, in humans the articulatory gymnastics needed to distinguish these vowels yield consistent, distinctive acoustic results, illustrating the quantal nature of human speech sounds. It may not be coincidental that [i], [u] and [a] are the most common vowels in the world's languages.\n\nIn humans, the lips are important for the production of stops and fricatives, in addition to vowels. Nothing, however, suggests that the lips evolved for those reasons. During primate evolution, a shift from nocturnal to diurnal activity in tarsiers, monkeys and apes (the haplorhines) brought with it an increased reliance on vision at the expense of olfaction. As a result, the snout became reduced and the rhinarium or \"wet nose\" was lost. The muscles of the face and lips consequently became less constrained, enabling their co-option to serve purposes of facial expression. The lips also became thicker. \"Hence\", according to one major authority, \"the evolution of mobile, muscular lips, so important to human speech, was the exaptive result of the evolution of diurnality and visual communication in the common ancestor of haplorhines\". It is unclear whether our lips have undergone more recent adaptation to the specific requirements of speech.\n\nCompared with nonhuman primates, humans have significantly enhanced control of breathing, enabling exhalations to be extended and inhalations shortened as we speak. While we are speaking, intercostal and interior abdominal muscles are recruited to expand the thorax and draw air into the lungs, and subsequently to control the release of air as the lungs deflate. The muscles concerned are markedly more innervated in humans than in nonhuman primates. Evidence from fossil hominins suggests that the necessary enlargement of the vertebral canal, and therefore spinal cord dimensions, may not have occurred in \"Australopithecus\" or \"Homo erectus\" but was present in the Neanderthals and early modern humans.\n\nThe larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is \"descended\". Our species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls. Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays. Lions, jaguars, cheetahs and domestic cats also do this. However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.\nDespite all this, scholars remain divided as to how \"special\" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent. As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs — increasing the risk of choking while swallowing food — we are forced to ask what benefits might have outweighed those costs. The obvious benefit — so it is claimed — must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact \"not\" seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare. Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of \"Homo sapiens.\" A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was \"initially\" driven by selection pressures for speech.\n\nAt least one orangutan has demonstrated the ability to control the voice box.\n\nTo lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds \"deeper\" — giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself. Ohala points out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning. W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in our species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalising to protect their infants would also have benefited from this ability.\n\nMost specialists credit the Neanderthals with speech abilities not radically different from those of modern \"Homo sapiens\". An indirect line of argument is that their tool-making and hunting tactics would have been difficult to learn or execute without some kind of speech. A recent extraction of DNA from Neanderthal bones indicates that Neanderthals had the same version of the FOXP2 gene as modern humans. This gene, once mistakenly described as the \"grammar gene\", plays a role in controlling the orofacial movements which (in modern humans) are involved in speech.\n\nDuring the 1970s, it was widely believed that the Neanderthals lacked modern speech capacities. It was claimed that they possessed a hyoid bone so high up in the vocal tract as to preclude the possibility of producing certain vowel sounds.\n\nThe hyoid bone is present in many mammals. It allows a wide range of tongue, pharyngeal and laryngeal movements by bracing these structures alongside each other in order to produce variation. It is now realised that its lowered position is not unique to \"Homo sapiens\", while its relevance to vocal flexibility may have been overstated: although men have a lower larynx, they do not produce a wider range of sounds than women or two-year-old babies. There is no evidence that the larynx position of the Neanderthals impeded the range of vowel sounds they could produce. The discovery of a modern-looking hyoid bone of a Neanderthal man in the Kebara Cave in Israel led its discoverers to argue that the Neanderthals had a descended larynx, and thus human-like speech capabilities. However, other researchers have claimed that the morphology of the hyoid is not indicative of the larynx's position. It is necessary to take into consideration the skull base, the mandible and the cervical vertebrae and a cranial reference plane.\n\nThe morphology of the outer and middle ear of Middle Pleistocene hominins from Atapuerca SH in Spain, believed to be proto-Neanderthal, suggests they had an auditory sensitivity similar to modern humans and very different from chimpanzees. They were probably able to differentiate between many different speech sounds.\n\nThe hypoglossal nerve plays an important role in controlling movements of the tongue. In 1998, one research team used the size of the hypoglossal canal in the base of fossil skulls in an attempt to estimate the relative number of nerve fibres, claiming on this basis that Middle Pleistocene hominins and Neanderthals had more fine-tuned tongue control than either australopithecines or apes. Subsequently, however, it was demonstrated that hypoglossal canal size and nerve sizes are not correlated, and it is now accepted that such evidence is uninformative about the timing of human speech evolution.\n\nAccording to one influential school, the human vocal apparatus is intrinsically digital on the model of a keyboard or digital computer. If so, this is remarkable: nothing about a chimpanzee's vocal apparatus suggests a digital keyboard, notwithstanding the anatomical and physiological similarities. This poses the question as to when and how, during and the course of human evolution, the transition from analog to digital structure and function occurred.\n\nThe human supralaryngeal tract is said to be digital in the sense that it is an arrangement of moveable toggles or switches, each of which, at any one time, must be in one state or another. The vocal cords, for example, are either vibrating (producing a sound) or not vibrating (in silent mode). By virtue of simple physics, the corresponding distinctive feature — in this case, \"voicing\" — cannot be somewhere in between. The options are limited to \"off\" and \"on\". Equally digital is the feature known as \"nasalisation\". At any given moment the soft palate or velum either allows or doesn't allow sound to resonate in the nasal chamber. In the case of lip and tongue positions, more than two digital states may be allowed. (To experiment with this, click on Interactive Saggital Section).\n\nThe theory that speech sounds are composite entities constituted by complexes of binary phonetic features was first advanced in 1938 by the Russian linguist Roman Jakobson. A prominent early supporter of this approach was Noam Chomsky, who went on to extend it from phonology to language more generally, in particular to the study of syntax and semantics. In his 1965 book, \"Aspects of the Theory of Syntax,\" Chomsky treated semantic concepts as combinations of binary-digital atomic elements explicitly on the model of distinctive features theory. The lexical item \"bachelor\", on this basis, would be expressed as [+ Human], [+ Male], [- Married].\n\nSupporters of this approach view the vowels and consonants recognised by speakers of a particular language or dialect at a particular time as cultural entities of little scientific interest. From a natural science standpoint, the units which matter are those common to \"Homo sapiens\" by virtue of our biological nature. By combining the atomic elements or \"features\" with which all humans are innately equipped, anyone may in principle generate the entire range of vowels and consonants to be found in any of the world's languages, whether past, present or future. The distinctive features are in this sense atomic components of a universal language.\n\nIn recent years, the notion of an innate \"universal grammar\" underlying phonological variation has been called into question. The most comprehensive monograph ever written about speech sounds, \"Sounds of the World's Languages,\" by Peter Ladefoged and Ian Maddieson, found virtually no basis for the postulation of some small number of fixed, discrete, universal phonetic features. Examining 305 languages, for example, they encountered vowels that were positioned basically everywhere along the articulatory and acoustic continuum. Ladefoged concludes that phonological features are not determined by human nature: \"Phonological features are best regarded as artifacts that linguists have devised in order to describe linguistic systems.\" The controversy remains unresolved.\n\nSelf-organization characterizes systems where macroscopic structures are spontaneously formed out of local interactions between the many components of the system. In self-organized systems, global organizational properties are not to be found at the local level. In colloquial terms, self-organisation is roughly captured by the idea of \"bottom-up\" (as opposed to \"top-down\") organisation. Examples of self-organized systems range from ice crystals to galaxy spirals in the inorganic world, and from spots on the leopard skins to the architecture of termite nests or shape of a flock of starlings.\nAccording to many phoneticians, the sounds of language arrange and re-arrange themselves through self-organization Speech sounds have both perceptual (\"how you hear them\") and articulatory (\"how you produce them\") properties, all with continuous values. Speakers tend to minimise effort, favouring ease of articulation over clarity. Listeners do the opposite, favouring sounds which are easy to distinguish even if difficult to pronounce. Since speakers and listeners are constantly switching roles, the syllable systems actually found in the world's languages turn out to be a compromise between acoustic distinctiveness on the one hand, and articulatory ease on the other.\n\nHow, precisely, do systems of vowels, consonants and syllables arise? Agent-based computer models take the perspective of self-organisation at the level of the speech community or population. The two main paradigms here are (1) the iterated learning model and (2) the language game model. Iterated learning focuses on transmission from generation to generation, typically with just one agent in each generation.\nIn the language game model, a whole population of agents simultaneously produce, perceive and learn language, inventing novel forms when the need arises.\n\nSeveral models have shown how relatively simple peer-to-peer vocal interactions, such as imitation, can spontaneously self-organize a system of sounds shared by the whole population, and different in different populations. For example, models elaborated by Berrah et al., as well as de Boer, and recently reformulated using Bayesian theory, showed how a group of individuals playing imitation games can self-organize repertoires of vowel sounds which share substantial properties with human vowel systems. For example, in de Boer's model, initially vowels are generated randomly, but agents learn from each other as they interact repeatedly over time. Agent A chooses a vowel from its repertoire and produces it, inevitably with some noise. Agent B hears this vowel and chooses the closest equivalent from its own repertoire. To check whether this truly matches the original, B produces the vowel \"it thinks it has heard\", whereupon A refers once again to its own repertoire to find the closest equivalent. If this matches the one it initially selected, the game is successful, otherwise it has failed. \"Through repeated interactions,\" according to de Boer, \"vowel systems emerge that are very much like the ones found in human languages.\"\n\nIn a different model, the phonetician Björn Lindblom was able to predict, on self-organizational grounds, the favoured choices of vowel systems ranging from three to nine vowels on the basis of a principle of optimal perceptual differentiation.\n\nFurther models studied the role of self-organization in the origins of phonemic coding and combinatoriality, that is the existence of phonemes and their systematic reuse to build structured syllables. Pierre-Yves Oudeyer developed models which showed that basic neural equipment for adaptive holistic vocal imitation, coupling directly motor and perceptual representations in the brain, can generate spontaneously shared combinatorial systems of vocalizations, including phonotactic patterns, in a society of babbling individuals. These models also characterized how morphological and physiological innate constraints can interact with these self-organized mechanisms to account for both the formation of statistical regularities and diversity in vocalization systems.\n\nThe gestural theory states that speech was a relatively late development, evolving by degrees from a system that was originally gestural.\n\nTwo types of evidence support this theory:\n\nResearch has found strong support for the idea that spoken language and signing depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language. Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.\n\nHumans spontaneously use hand and facial gestures when formulating ideas to be conveyed in speech. There are also, of course, many sign languages in existence, commonly associated with deaf communities; as noted above, these are equal in complexity, sophistication, and expressive power, to any oral language. The main difference is that the \"phonemes\" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.\n\nCritics note that for mammals in general, sound turns out to be the best medium in which to encode information for transmission over distances at speed. Given the probability that this applied also to early humans, it's hard to see why they should have abandoned this efficient method in favour of more costly and cumbersome systems of visual gesturing — only to return to sound at a later stage.\n\nBy way of explanation, it has been proposed that at a relatively late stage in human evolution, our ancestors' hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language is said to have occurred only at that point. Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this puzzle — one setting out from considerations of signal reliability and trust — see \"from pantomime to speech\" below).\n\nLittle is known about the timing of language's emergence in the human species. Unlike writing, speech leaves no material trace, making it archaeologically invisible. Lacking direct linguistic evidence, specialists in human origins have resorted to the study of anatomical features and genes arguably associated with speech production. While such studies may provide information as to whether pre-modern \"Homo\" species had speech \"capacities\", it is still unknown whether they actually spoke. While they may have communicated vocally, the anatomical and genetic data lack the resolution necessary to differentiate proto-language from speech.\n\nUsing statistical methods to estimate the time required to achieve the current spread and diversity in modern languages today, Johanna Nichols — a linguist at the University of California, Berkeley — argued in 1998 that vocal languages must have begun diversifying in our species at least 100,000 years ago.\n\nMore recently — in 2012 — anthropologists Charles Perreault and Sarah Mathew used phonemic diversity to suggest a date consistent with this. \"Phonemic diversity\" denotes the number of perceptually distinct units of sound — consonants, vowels and tones — in a language. The current worldwide pattern of phonemic diversity potentially contains the statistical signal of the expansion of modern \"Homo sapiens\" out of Africa, beginning around 60-70 thousand years ago. Some scholars argue that phonemic diversity evolves slowly and can be used as a clock to calculate how long the oldest African languages would have to have been around in order to accumulate the number of phonemes they possess today. As human populations left Africa and expanded into the rest of the world, they underwent a series of bottlenecks — points at which only a very small population survived to colonise a new continent or region. Allegedly such population crash led to a corresponding reduction in genetic, phenotypic and phonemic diversity. African languages today have some of the largest phonemic inventories in the world, while the smallest inventories are found in South America and Oceania, some of the last regions of the globe to be colonized. For example, Rotokas, a language of New Guinea, and Pirahã, spoken in South America, both have just 11 phonemes, while !Xun, a language spoken in Southern Africa has 141 phonemes.\nThe authors use a natural experiment — the colonization of mainland Southeast Asia on the one hand, the long-isolated Andaman Islands on the other — to estimate the rate at which phonemic diversity increases through time. Using this rate, they estimate that the world's languages date back to the Middle Stone Age in Africa, sometime between 350 thousand and 150 thousand years ago. This corresponds to the speciation event which gave rise to \"Homo sapiens\".\n\nThese and similar studies have however been criticized by linguists who argue that they are based on a flawed analogy between genes and phonemes, since phonemes are frequently transferred laterally between languages unlike genes, and on a flawed sampling of the world's languages, since both Oceania and the Americas also contain languages with very high numbers of phonemes, and Africa contains languages with very few. They argue that the actual distribution of phonemic diversity in the world reflects recent language contact and not deep language history - since it is well demonstrated that languages can lose or gain many phonemes over very short periods. In other words, there is no valid linguistic reason to expect genetic founder effects to influence phonemic diversity.\n\nIn 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language:\n\nMost scholars today consider all such theories not so much wrong — they occasionally offer peripheral insights — as comically naïve and irrelevant. The problem with these theories is that they are so narrowly mechanistic. They assume that once our ancestors had stumbled upon the appropriate ingenious \"mechanism\" for linking sounds with meanings, language automatically evolved and changed.\n\nFrom the perspective of modern science, the main obstacle to the evolution of speech-like communication in nature is not a mechanistic one. Rather, it is that symbols — arbitrary associations of sounds with corresponding meanings — are unreliable and may well be false. As the saying goes, \"words are cheap\". The problem of reliability was not recognised at all by Darwin, Müller or the other early evolutionist theorists.\n\nAnimal vocal signals are for the most part intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. One can \"trust\" the signal not because the cat is inclined to be honest, but because it just can't fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason — because they are hard to fake. Primate social intelligence is \"Machiavellian\" — self-serving and unconstrained by moral scruples. Monkeys and apes often attempt to deceive one another, while at the same time constantly on guard against falling victim to deception themselves. Paradoxically, it is precisely primates' resistance to deception that blocks the evolution of their vocal communication systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.\n\nWords are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favour of hard-to-fake indices or cues. For language to work, then, listeners must be confident that those with whom they are on speaking terms are generally likely to be honest. A peculiar feature of language is \"displaced reference\", which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate \"here\" and \"now\". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot (see signalling theory).\n\nThe \"mother tongues\" hypothesis was proposed in 2004 as a possible solution to this problem. W. Tecumseh Fitch suggested that the Darwinian principle of \"kin selection\" — the convergence of genetic interests between relatives — might be part of the answer. Fitch suggests that spoken languages were originally \"mother tongues\". If speech evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable vocal signals — spoken words — to become accepted as trustworthy and so begin evolving for the first time.\n\nCritics of this theory point out that kin selection is not unique to humans. Ape mothers also share genes with their offspring, as do all animals, so why is it only humans who speak? Furthermore, it is difficult to believe that early humans restricted linguistic communication to genetic kin: the incest taboo must have forced men and women to interact and communicate with non-kin. So even if we accept Fitch's initial premises, the extension of the posited \"mother tongue\" networks from relatives to non-relatives remains unexplained.\n\nIb Ulbæk invokes another standard Darwinian principle — \"reciprocal altruism\" — to explain the unusually high levels of intentional honesty necessary for language to evolve. 'Reciprocal altruism' can be expressed as the principle that \"if you scratch my back, I'll scratch yours.\" In linguistic terms, it would mean that \"if you speak truthfully to me, I'll speak truthfully to you.\" Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, early society as a whole must have been subject to moral regulation.\n\nCritics point out that this theory fails to explain when, how, why or by whom \"obligatory reciprocal altruism\" could possibly have been enforced. Various proposals have been offered to remedy this defect. A further criticism is that language doesn't work on the basis of reciprocal altruism anyway. Humans in conversational groups don't withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting it to anyone who will listen without thought of return.\n\nGossip, according to Robin Dunbar, does for group-living humans what manual grooming does for other primates — it allows individuals to service their relationships and so maintain their alliances. As humans began living in larger and larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable. In response to this problem, humans invented \"a cheap and ultra-efficient form of grooming\" — \"vocal grooming\". To keep your allies happy, you now needed only to \"groom\" them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming (the production of pleasing sounds lacking syntax or combinatorial semantics) then evolved somehow into syntactical speech.\n\nCritics of this theory point out that the very efficiency of \"vocal grooming\" — that words are so cheap — would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming. A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming — the production of pleasing but meaningless sounds — to the cognitive complexities of syntactical speech.\n\nAccording to another school of thought, language evolved from mimesis — the \"acting out\" of scenarios using vocal and gestural pantomime. For as long as utterances needed to be emotionally expressive and convincing, it was not possible to complete the transition to purely conventional signs. On this assumption, pre-linguistic gestures and vocalisations would have been required not just to disambiguate intended meanings, but also to inspire confidence in their intrinsic reliability. If contractual commitments were necessary in order to inspire community-wide trust in communicative intentions, it would follow that these had to be in place before humans could shift at last to an ultra-efficient, high-speed — digital as opposed to analog — signalling format. Vocal distinctive features (sound contrasts) are ideal for this purpose. It is therefore suggested that the establishment of contractual understandings enabled the decisive transition from mimetic gesture to fully conventionalised, digitally encoded speech.\n\nThe ritual/speech coevolution theory was originally proposed by the distinguished social anthropologist Roy Rappaport before being elaborated by anthropologists such as Chris Knight, Jerome Lewis, Nick Enfield, Camilla Power and Ian Watts. Cognitive scientist and robotics engineer Luc Steels is another prominent supporter of this general approach, as is biological anthropologist/neuroscientist Terrence Deacon.\n\nThese scholars argue that there can be no such thing as a \"theory of the origins of language\". This is because language is not a separate adaptation but an internal aspect of something much wider — namely, human symbolic culture as a whole. Attempts to explain language independently of this wider context have spectacularly failed, say these scientists, because they are addressing a problem with no solution. Can we imagine a historian attempting to explain the emergence of credit cards independently of the wider system of which they are a part? Using a credit card makes sense only if you have a bank account institutionally recognised within a certain kind of advanced capitalist society — one where communications technology has already been invented and fraud can be detected and prevented. In much the same way, language would not work outside a specific array of social mechanisms and institutions. For example, it would not work for an ape communicating with other apes in the wild. Not even the cleverest ape could make language work under such conditions.\n\nSpeech consists of digital contrasts whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world — they are a theoretical impossibility. Being intrinsically unreliable, language works only if you can build up a reputation for trustworthiness within a certain kind of society — namely, one where symbolic cultural facts (sometimes called \"institutional facts\") can be established and maintained through collective social endorsement. In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective \"ritual\". Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human symbolic culture as a whole, with language an important but subsidiary component.\n\nCritics of the theory include Noam Chomsky, who terms it the \"non-existence\" hypothesis — a denial of the very existence of language as an object of study for natural science. Chomsky's own theory is that language emerged in an instant and in perfect form, prompting his critics in turn to retort that only something that doesn't exist — a theoretical construct or convenient scientific fiction — could possibly emerge in such a miraculous way. The controversy remains unresolved.\n\nThe essay \"The festal origin of human speech\", though published in the late nineteenth century, made little impact until the American philosopher Susanne Langer re-discovered and publicised it in 1941. \nThe theory sets out from the observation that primate vocal sounds are above all \"emotionally\" expressive. The emotions aroused are socially contagious. Because of this, an extended bout of screams, hoots or barks will tend to express not just the feelings of this or that individual but the mutually contagious ups and downs of everyone within earshot.\n\nTurning to the ancestors of \"Homo sapiens\", the \"festal origin\" theory suggests that in the \"play-excitement\" preceding or following a communal hunt or other group activity, everyone might have combined their voices in a comparable way, emphasising their mood of togetherness with such noises as rhythmic drumming and hand-clapping. Variably pitched voices would have formed conventional patterns, such that choral singing became an integral part of communal celebration.\n\nAlthough this was not yet speech, according to Langer, it developed the vocal capacities from which speech would later derive. There would be conventional modes of ululating, clapping or dancing appropriate to different festive occasions, each so intimately associated with \"that kind of occasion\" that it would tend to collectively uphold and embody the concept of it. Anyone hearing a snatch of sound from such a song would recall the associated occasion and mood. A melodic, rhythmic sequence of syllables conventionally associated with a certain type of celebration would become, in effect, its vocal mark. On that basis, certain familiar sound sequences would become \"symbolic\".\n\nIn support of all this, Langer cites ethnographic reports of tribal songs consisting entirely of \"rhythmic nonsense syllables\". She concedes that an English equivalent such as \"hey-nonny-nonny\", although perhaps suggestive of certain feelings or ideas, is neither noun, verb, adjective, nor any other syntactical part of speech. So long as articulate sound served only in the capacity of \"hey nonny-nonny\", \"hallelujah\" or \"alack-a-day\", it cannot yet have been speech. For that to arise, according to Langer, it was necessary for such sequences to be emitted increasingly \"out of context\" — outside the total situation that gave rise to them. Extending a set of associations from one cognitive context to another, completely different one, is the secret of \"metaphor\". Langer invokes an early version of what is nowadays termed \"grammaticalisation\" theory to show how, from, such a point of departure, syntactically complex speech might progressively have arisen.\n\nLanger acknowledges Emile Durkheim as having proposed a strikingly similar theory back in 1912. For recent thinking along broadly similar lines, see Steven Brown on \"musilanguage\", Chris Knight on \"ritual\" and \"play\", Jerome Lewis on \"mimicry\", Steven Mithen on \"Hmmmmm\" Bruce Richman on \"nonsense syllables\" and Alison Wray on \"holistic protolanguage\".\n\nThe term \"musilanguage\" (or \"hmmmmm\") refers to a pre-linguistic system of vocal communication from which (according to some scholars) \"both\" music \"and\" language later derived. The idea is that rhythmic, melodic, emotionally expressive vocal ritual helped bond coalitions and, over time, set up selection pressures for enhanced volitional control over the speech articulators. Patterns of synchronised choral chanting are imagined to have varied according to the occasion. For example, \"we're setting off to find honey\" might sound qualitatively different from \"we're setting off to hunt\" or \"we're grieving over our relative's death\". If social standing depended on maintaining a regular beat and harmonising one's own voice with that of everyone else, group members would have come under pressure to demonstrate their choral skills.\n\nArchaeologist Steven Mithen speculates that the Neanderthals possessed some such system, expressing themselves in a \"language\" known as \"Hmmmmm\", standing for Holistic, manipulative, multi-modal, musical and mimetic. In Bruce Richman's earlier version of essentially the same idea, frequent repetition of the same few songs by many voices made it easy for people to remember those sequences as whole units. Activities that a group of people were doing while they were vocalising together — activities that were important or striking or richly emotional — came to be associated with particular sound sequences, so that each time a fragment was heard, it evoked highly specific memories. The idea is that the earliest lexical items (words) started out as abbreviated fragments of what were originally communal songs.\n\nAs group members accumulated an expanding repertoire of songs for different occasions, interpersonal call-and-response patterns evolved along one trajectory to assume linguistic form. Meanwhile, along a divergent trajectory, polyphonic singing and other kinds of music became increasingly specialised and sophisticated.\n\nTo explain the establishment of syntactical speech, Richman cites English \"I wanna go home\". He imagines this to have been learned in the first instance not as a combinatorial sequence of free-standing words, but as a single stuck-together combination — the melodic sound people make to express \"feeling homesick\". Someone might sing \"I wanna go home\", prompting other voices to chime in with \"I need to go home\", \"I'd love to go home\", \"Let's go home\" and so forth. Note that one part of the song remains constant, while another is permitted to vary. If this theory is accepted, syntactically complex speech began evolving as each chanted mantra allowed for variation at a certain point, allowing for the insertion of an element from some other song. For example, while mourning during a funeral rite, someone might want to recall a memory of collecting honey with the deceased, signalling this at an appropriate moment with a fragment of the \"we're collecting honey\" song. Imagine that such practices became common. Meaning-laden utterances would now have become subject to a distinctively linguistic creative principle — that of recursive embedding.\n\nMany scholars associate the evolutionary emergence of speech with profound social, sexual, political and cultural developments. One view is that primate-style dominance needed to give way to a more cooperative and egalitarian lifestyle of the kind characteristic of modern hunter-gatherers.\n\nAccording to Michael Tomasello, the key cognitive capacity distinguishing \"Homo sapiens\" from our ape cousins is \"intersubjectivity\". This entails turn-taking and role-reversal: your partner strives to read your mind, you simultaneously strive to read theirs, and each of you makes a conscious effort to assist the other in the process. The outcome is that each partner forms a representation of the other's mind in which their own can be discerned by reflection.\n\nTomasello argues that this kind of bi-directional cognition is central to the very possibility of linguistic communication. Drawing on his research with both children and chimpanzees, he reports that human infants, from one year old onwards, begin viewing their own mind as if from the standpoint of others. He describes this as a cognitive revolution. Chimpanzees, as they grow up, never undergo such a revolution. The explanation, according to Tomasello, is that their evolved psychology is adapted to a deeply competitive way of life. Wild-living chimpanzees form despotic social hierarchies, most interactions involving calculations of dominance and submission. An adult chimp will strive to outwit its rivals by guessing at their intentions while blocking them from reciprocating. Since bi-directional intersubjective communication is impossible under such conditions, the cognitive capacities necessary for language don't evolve.\n\nIn the scenario favoured by David Erdal and Andrew Whiten, primate-style dominance provoked equal and opposite coalitionary resistance — \"counter-dominance.\" During the course of human evolution, increasingly effective strategies of rebellion against dominant individuals led to a compromise. While abandoning any attempt to dominate others, group members vigorously asserted their personal autonomy, maintaining their alliances to make potentially dominant individuals think twice. Within increasingly stable coalitions, according to this perspective, status began to be earned in novel ways, social rewards accruing to those perceived by their peers as especially cooperative and self-aware.\n\nWhile counter-dominance, according to this evolutionary narrative, culminates in a stalemate, anthropologist Christopher Boehm extends the logic a step further. Counter-dominance tips over at last into full-scale \"reverse dominance\". The rebellious coalition decisively overthrows the figure of the primate alpha-male. No dominance is allowed except that of the self-organised community as a whole.\n\nAs a result of this social and political change, hunter-gatherer egalitarianism is established. As children grow up, they are motivated by those around them to reverse perspective, engaging with other minds on the model of their own. Selection pressures favour such psychological innovations as imaginative empathy, joint attention, moral judgment, project-oriented collaboration and the ability to evaluate one's own behaviour from the standpoint of others. Underpinning enhanced probabilities of cultural transmission and cumulative cultural evolution, these developments culminate in the establishment of hunter-gatherer-style egalitarianism in association with intersubjective communication and cognition. It is in this social and political context that language evolves.\n\nAccording to Dean Falk's \"putting the baby down\" theory, vocal interactions between early hominin mothers and infants sparked a sequence of events that led, eventually, to our ancestors' earliest words. The basic idea is that evolving human mothers, unlike their monkey and ape counterparts, couldn't move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed reassurance that they were not being abandoned. Mothers responded by developing \"motherese\" — an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling and emotionally expressive contact calls. The argument is that language somehow developed out of all this.\n\nWhile this theory may explain a certain kind of infant-directed \"protolanguage\" — known today as \"motherese\" — it does little to solve the really difficult problem, which is the emergence among adults of syntactical speech. \n\nEvolutionary anthropologist Sarah Hrdy observes that only human mothers among great apes are willing to let another individual take hold of their own babies; further, we are routinely willing to let others babysit. She identifies lack of trust as the major factor preventing chimp, bonobo or gorilla mothers from doing the same: \"If ape mothers insist on carrying their babies everywhere ... it is because the available alternatives are not safe enough.\" The fundamental problem is that ape mothers (unlike monkey mothers who may often babysit) do not have female relatives nearby. The strong implication is that, in the course of \"Homo\" evolution, allocare could develop because \"Homo\" mothers did have female kin close by — in the first place, most reliably, their own mothers. Extending the Grandmother hypothesis, Hrdy argues that evolving \"Homo erectus\" females necessarily relied on female kin initially; this novel situation in ape evolution of mother, infant and mother's mother as allocarer provided the evolutionary ground for the emergence of intersubjectivity. She relates this onset of \"cooperative breeding in an ape\" to shifts in life history and slower child development, linked to the change in brain and body size from the 2 million year mark.\n\nPrimatologist Klaus Zuberbühler uses these ideas to help explain the emergence of vocal flexibility in the human species. Co-operative breeding would have compelled infants to struggle actively to gain the attention of caregivers, not all of whom would have been directly related. A basic primate repertoire of vocal signals may have been insufficient for this social challenge. Natural selection, according to this view, would have favoured babies with advanced vocal skills, beginning with babbling (which triggers positive responses in care-givers) and paving the way for the elaborate and unique speech abilities of modern humans.\n\nThese ideas might be linked to those of the renowned structural linguist Roman Jakobson, who claimed that \"the sucking activities of the child are accompanied by a slight nasal murmur, the only phonation to be produced when the lips are pressed to the mother's breast ... and the mouth is full\". He proposed that later in the infant's development, \"this phonatory reaction to nursing is reproduced as an anticipatory signal at the mere sight of food and finally as a manifestation of a desire to eat, or more generally, as an expression of discontent and impatient longing for missing food or absent nurser, and any ungranted wish.\" So, the action of opening and shutting the mouth, combined with the production of a nasal sound when the lips are closed, yielded the sound sequence \"Mama\", which may therefore count as the very first word. Peter MacNeilage sympathetically discusses this theory in his major book, The \"Origin of Speech\", linking it with Dean Falk's \"putting the baby down\" theory (see above). Needless to say, other scholars have suggested completely different candidates for \"Homo sapiens\"' very first word.\n\nWhile the biological language faculty is genetically inherited, actual languages or dialects are culturally transmitted, as are social norms, technological traditions and so forth. Biologists expect a robust co-evolutionary trajectory linking human genetic evolution with the evolution of culture. Individuals capable of rudimentary forms of protolanguage would have enjoyed enhanced access to cultural understandings, while these, conveyed in ways that young brains could readily learn, would in turn have become transmitted with increasing efficiency.\n\nIn some ways like beavers as they construct their dams, humans have always engaged in niche construction, creating novel environments to which they subsequently become adapted. Selection pressures associated with prior niches tend to become relaxed as humans depend increasingly on novel environments created continuously by their own productive activities. According to Steven Pinker, language is an adaptation to \"the cognitive niche\". Variations on the theme of ritual/speech co-evolution — according to which speech evolved for purposes of internal communication within a ritually constructed domain — have attempted to specify more precisely when, why and how this special niche was created by human collaborative activity.\n\n The Swiss scholar Ferdinand de Saussure founded linguistics as a twentieth-century professional discipline. Saussure regarded a language as a rule-governed system, much like a board game such as chess. In order to understand chess, he insisted, we must ignore such external factors as the weather prevailing during a particular session or the material composition of this or that piece. The game is autonomous with respect to its material embodiments. In the same way, when studying language, it's essential to focus on its internal structure as a social institution. External matters (\"e.g.\", the shape of the human tongue) are irrelevant from this standpoint. Saussure regarded 'speaking' \"(parole)\" as individual, ancillary and more or less accidental by comparison with \"language\" \"(langue)\", which he viewed as collective, systematic and essential.\n\nSaussure showed little interest in Darwin's theory of evolution by natural selection. Nor did he consider it worthwhile to speculate about how language might originally have evolved. Saussure's assumptions in fact cast doubt on the validity of narrowly conceived origins scenarios. His structuralist paradigm, when accepted in its original form, turns scholarly attention to a wider problem: how our species acquired the capacity to establish social institutions in general.\n\n In the United States, prior to and immediately following World War II, the dominant psychological paradigm was behaviourism. Within this conceptual framework, language was seen as a certain kind of behaviour — namely, verbal behaviour, to be studied much like any other kind of behaviour in the animal world. Rather as a laboratory rat learns how to find its way through an artificial maze, so a human child learns the verbal behaviour of the society into which it is born. The phonological, grammatical and other complexities of speech are in this sense \"external\" phenomena, inscribed into an initially unstructured brain. Language's emergence in \"Homo sapiens,\" from this perspective, presents no special theoretical challenge. Human behaviour, whether verbal or otherwise, illustrates the malleable nature of the mammalian — and especially the human — brain.\n\nNativism is the theory that humans are born with certain specialised cognitive modules enabling us to acquire highly complex bodies of knowledge such as the grammar of a language.\n\nFrom the mid-1950s onwards, Noam Chomsky, Jerry Fodor and others mounted what they conceptualised as a 'revolution' against behaviourism. Retrospectively, this became labelled 'the cognitive revolution'. Whereas behaviourism had denied the scientific validity of the concept of \"mind\", Chomsky replied that, in fact, the concept of \"body\" is more problematic. Behaviourists tended to view the child's brain as a \"tabula rasa\", initially lacking structure or cognitive content. According to B. F. Skinner, for example, richness of behavioural detail (whether verbal or non-verbal) emanated from the environment. Chomsky turned this idea on its head. The linguistic environment encountered by a young child, according to Chomsky's version of psychological nativism, is in fact hopelessly inadequate. No child could possibly acquire the complexities of grammar from such an impoverished source. Far from viewing language as wholly external, Chomsky re-conceptualised it as wholly internal. To explain how a child so rapidly and effortlessly acquires its natal language, he insisted, we must conclude that it comes into the world with the essentials of grammar already pre-installed. No other species, according to Chomsky, is genetically equipped with a language faculty — or indeed with anything remotely like one. The emergence of such a faculty in \"Homo sapiens\", from this standpoint, presents biological science with a major theoretical challenge.\n\nOne way to explain biological complexity is by reference to its inferred function. According to the influential philosopher John Austin, speech's primary function is action in the social world.\n\nSpeech acts, according to this body of theory, can be analysed on three different levels: locutionary, illocutionary and perlocutionary. An act is \"locutionary\" when viewed as the production of certain linguistic sounds — for example, practicing correct pronunciation in a foreign language. An act is \"illocutionary\" insofar as it constitutes an intervention in the world as jointly perceived or understood. Promising, marrying, divorcing, declaring, stating, authorising, announcing and so forth are all speech acts in this \"illocutionary\" sense. An act is \"perlocutionary\" when viewed in terms of its direct psychological effect on an audience. Frightening a baby by saying 'Boo!' would be an example of a \"perlocutionary\" act.\n\nFor Austin, \"doing things\" with words means, first and foremost, deploying \"illocutionary\" force. The secret of this is community participation or collusion. There must be a 'correct' (conventionally agreed) procedure, and all those concerned must accept that it has been properly followed. In the case of a priest declaring a couple to be man and wife, his words will have illocutionary force only if he is properly authorised and only if the ceremony is properly conducted, using words deemed appropriate to the occasion. Austin points out that should anyone attempt to baptize a penguin, the act would be null and void. For reasons which have nothing to do with physics, chemistry or biology, baptism is inappropriate to be applied to penguins, irrespective of the verbal formulation used.\n\nThis body of theory may have implications for speculative scenarios concerning the origins of speech. \"Doing things with words\" presupposes shared understandings and agreements pertaining not just to language but to social conduct more generally. Apes might produce sequences of structured sound, influencing one another in that way. To deploy \"illocutionary\" force, however, they would need to have entered a non-physical and non-biological realm — one of shared contractual and other intangibles. This novel cognitive domain consists of what philosophers term \"institutional facts\" — objective facts whose existence, paradoxically, depends on communal faith or belief. Few primatologists, evolutionary psychologists or anthropologists consider that nonhuman primates are capable of the necessary levels of joint attention, sustained commitment or collaboration in pursuit of future goals.\n\nBiosemiotics is a relatively new discipline, inspired in large part by the discovery of the genetic code in the early 1960s. Its basic assumption is that \"Homo sapiens\" is not alone in its reliance on codes and signs. Language and symbolic culture must have biological roots, hence semiotic principles must apply also in the animal world.\n\nThe discovery of the molecular structure of DNA apparently contradicted the idea that life could be explained, ultimately, in terms of the fundamental laws of physics. The letters of the genetic alphabet seemed to have \"meaning\", yet meaning is not a concept which has any place in physics. The natural science community initially solved this difficulty by invoking the concept of \"information\", treating information as independent of meaning. But a different solution to the puzzle was to recall that the laws of physics in themselves are never sufficient to explain natural phenomena. To explain, say, the unique physical and chemical characteristics of the planets in our solar system, scientists must work out how the laws of physics became constrained by particular sequences of events following the formation of the Sun.\n\nAccording to Howard Pattee — a pioneering figure in biosemiotics — the same principle applies to the evolution of life on earth, a process in which certain \"frozen accidents\" or \"natural constraints\" have from time to time drastically reduced the number of possible evolutionary outcomes. Codes, when they prove to be stable over evolutionary time, are constraints of this kind. The most fundamental such \"frozen accident\" was the emergence of DNA as a self-replicating molecule, but the history of life on earth has been characterised by a succession of comparably dramatic events, each of which can be conceptualised as the emergence of a new code. From this perspective, the evolutionary emergence of spoken language was one more event of essentially the same kind.\n\nIn 1975, the Israeli theoretical biologist Amotz Zahavi proposed a novel theory which, although controversial, has come to dominate Darwinian thinking on how signals evolve. Zahavi's \"handicap principle\" states that to be effective, signals must be reliable; to be reliable, the bodily investment in them must be so high as to make cheating unprofitable.\n\nParadoxically, if this logic is accepted, signals in nature evolve not to be efficient but, on the contrary, to be elaborate and wasteful of time and energy. A peacock's tail is the classic illustration. Zahavi's theory is that since peahens are on the look-out for male braggarts and cheats, they insist on a display of quality so costly that only a genuinely fit peacock could afford to pay. Needless to say, not all signals in the animal world are quite as elaborate as a peacock's tail. But if Zahavi is correct, all require some bodily investment — an expenditure of time and energy which \"handicaps\" the signaller in some way.\n\nAnimal vocalisations (according to Zahavi) are reliable because they are faithful reflections of the state of the signaller's body. To switch from an honest to a deceitful call, the animal would have to adopt a different bodily posture. Since every bodily action has its own optimal starting position, changing that position to produce a false message would interfere with the task of carrying out the action really intended. The gains made by cheating would not make up for the losses incurred by assuming an improper posture — and so the phony message turns out to be not worth its price. This may explain, in particular, why ape and monkey vocal signals have evolved to be so strikingly inflexible when compared with the varied speech sounds produced by the human tongue. The apparent inflexibility of chimpanzee vocalisations may strike the human observer as surprising until we realise that being inflexible is necessarily bound up with being perceptibly honest in the sense of \"hard-to-fake\".\n\nIf we accept this theory, the emergence of speech becomes theoretically impossible. Communication of this kind just cannot evolve. The problem is that words are cheap. Nothing about their acoustic features can reassure listeners that they are genuine and not fakes. Any strategy of reliance on someone else's tongue — perhaps the most flexible organ in the body — presupposes unprecedented levels of honesty and trust. To date, Darwinian thinkers have found it difficult to explain the requisite levels of community-wide cooperation and trust.\n\nAn influential standard textbook is \"Animal Signals,\" by John Maynard Smith and David Harper. These authors divide the costs of communication into two components, (1) the investment necessary to ensure transmission of a discernible signal; (2) the investment necessary to guarantee that each signal is reliable and not a fake. The authors point out that although costs in the second category may be relatively low, they are not zero. Even in relatively relaxed, cooperative social contexts — for example, when communication is occurring between genetic kin — some investment must be made to guarantee reliability. In short, the notion of super-efficient communication — eliminating all costs except those necessary for successful transmission — is biologically unrealistic. Yet speech comes precisely into this category.\n\nCognitive linguistics views linguistic structure as arising continuously out of usage. Speakers are forever discovering new ways to convey meanings by producing sounds, and in some cases these novel strategies become conventionalised. Between phonological structure and semantic structure there is no causal relationship. Instead, each novel pairing of sound and meaning involves an imaginative leap.\n\nIn their book, \"Metaphors We Live By,\" George Lakoff and Mark Johnson helped pioneer this approach, claiming that \"metaphor\" is what makes human thought special. All language, they argued, is permeated with metaphor, whose use in fact \"constitutes\" distinctively human — that is, distinctively abstract — thought. To conceptualise things which cannot be directly perceived — intangibles such as time, life, reason, mind, society or justice — we have no choice but to set out from more concrete and directly perceptible phenomena such as motion, location, distance, size and so forth. In all cultures across the world, according to Lakoff and Johnson, people resort to such familiar metaphors as \"ideas are locations, thinking is moving\" and \"mind is body\". For example, we might express the idea of \"arriving at a crucial point in our argument\" by proceeding as if literally travelling from one physical location to the next.\n\nMetaphors, by definition, are not literally true. Strictly speaking, they are fictions — from a pedantic standpoint, even falsehoods. But if we couldn't resort to metaphorical fictions, it's doubtful whether we could even form conceptual representations of such nebulous phenomena as \"ideas\", thoughts\", \"minds\", and so forth.\n\nThe bearing of these ideas on current thinking on speech origins remains unclear. One suggestion is that ape communication tends to resist metaphor for social reasons. Since they inhabit a Darwinian (as opposed to morally regulated) social world, these animals are under strong competitive pressure \"not\" to accept patent fictions as valid communicative currency. Ape vocal communication tends to be inflexible, marginalising the ultra-flexible tongue, precisely because listeners treat with suspicion any signal which might prove to be a fake. Such insistence on perceptible veracity is clearly incompatible with metaphoric usage. An implication is that neither articulate speech nor distinctively human abstract thought could have begun evolving until our ancestors had become more cooperative and trusting of one another's communicative intentions.\n\nWhen people converse with one another, according to the American philosopher John Searle, they're making moves, not in the real world which other species inhabit, but in a shared virtual realm peculiar to ourselves. Unlike the deployment of muscular effort to move a physical object, the deployment of illocutionary force requires no physical effort (except movement of the tongue/mouth to produce speech) and produces no effect which any measuring device could detect. Instead, our action takes place on a quite different level — that of \"social\" reality. This kind of reality is in one sense hallucinatory, being a product of collective intentionality. It consists, not of \"brute facts\" — facts which exist anyway, irrespective of anyone's belief — but of \"institutional facts\", which \"exist\" only if you believe in them. Government, marriage, citizenship and money are examples of \"institutional facts\". One can distinguish between \"brute\" facts and \"institutional\" ones by applying a simple test. Suppose no one believed in the fact — would it still be true? If the answer is \"yes\", it's \"brute\". If the answer is \"no\", it's \"institutional\".\n\nThe facts of language in general and of speech in particular are, from this perspective, \"institutional\" rather than \"brute\". The semantic meaning of a word, for example, is whatever its users imagine it to be. To \"do things with words\" is to operate in a virtual world which seems real because we share it in common. In this incorporeal world, the laws of physics, chemistry and biology do not apply. That explains why illocutionary force can be deployed without exerting muscular effort. Apes and monkeys inhabit the \"brute\" world. To make an impact, they must scream, bark, threaten, seduce or in other ways invest bodily effort. If they were invited to play chess, they would be unable to resist throwing their pieces at one another. Speech is not like that. A few movements of the tongue, under appropriate conditions, can be sufficient to open parliament, annul a marriage, confer a knighthood or declare war. To explain, on a Darwinian basis, how such apparent magic first began to work, we must ask how, when and why \"Homo sapiens\" succeeded in establishing the wider domain of institutional facts.\n\n\"Brute facts\", in the terminology of speech act philosopher John Searle, are facts which are true anyway, regardless of human belief. Suppose you don't believe in gravity: jump over a cliff and you'll still fall. Natural science is the study of facts of this kind. \"Institutional facts\" are fictions accorded factual status within human social institutions. Monetary and commercial facts are fictions of this kind. The complexities of today's global currency system are facts only while we believe in them: suspend the belief and the facts correspondingly dissolve. Yet although institutional facts rest on human belief, that doesn't make them mere distortions or hallucinations. Take my confidence that these two five-pound banknotes in my pocket are worth ten pounds. That's not merely my subjective belief: it's an objective, indisputable fact. But now imagine a collapse of public confidence in the currency system. Suddenly, the realities in my pocket dissolve.\n\nScholars who doubt the scientific validity of the notion of \"institutional facts\" include Noam Chomsky, for whom language is not social. In Chomsky's view, language is a natural object (a component of the individual brain) and its study, therefore, a branch of natural science. In explaining the origin of language, scholars in this intellectual camp invoke non-social developments — in Chomsky's case, a random genetic mutation. Chomsky argues that language might exist inside the brain of a single mutant gorilla even if no one else believed in it, even if no one else existed apart from the mutant — and even if the gorilla in question remained unaware of its existence, never actually speaking. In the opposite philosophical camp are those who, in the tradition of Ferdinand de Saussure, argue that if no one believed in words or rules, they simply would not exist. These scholars, correspondingly, regard language as essentially institutional, concluding that linguistics should be considered a topic within social science. In explaining the evolutionary emergence of language, scholars in this intellectual camp tend to invoke profound changes in social relationships.\n\nCriticism. Darwinian scientists today see little value in the traditional distinction between \"natural\" and \"social\" science. Darwinism in its modern form is the study of cooperation and competition in nature — a topic which is intrinsically social. Against this background, there is an increasing awareness among evolutionary linguists and Darwinian anthropologists that traditional inter-disciplinary barriers can have damaging consequences for investigations into the origins of speech.\n\n"}
{"id": "50400956", "url": "https://en.wikipedia.org/wiki?curid=50400956", "title": "Pandemonia", "text": "Pandemonia\n\nPandemonia is a character and persona created as conceptual art by an anonymous London-based artist that has appeared in the art and fashion world since 2009. Clad in a latex full-head mask with stylized hair and latex dresses, Pandemonia is seven feet tall and was described by Katia Ganfield of \"Vice\" as \"Roy Lichtenstein's blonde caricatures ... brought to life as a 7 ft Jeff Koons inflatable\". She is often accompanied by an inflatable white dog named Snowy.\n\nPandemonia told \"Stylist\":\nInitially a \"crasher\" at fashion and social events, Pandemonia eventually became a London Fashion Week VIP guest.\n\nPandemonia is a critical reflection and, as such, an intervention upon ideas of celebrity and femininity. She is a pointed manifestation of how these ideas intersect with mass media, social media, and the marketability of desire. The art of Pandemonia herself is that of a constructed figure placed in the landscape of media, fashion and art events that has instigated the media response by feeding back to the media its own language, imagery and ideals.\n\nThe growth of Pandemonia's celebrity is one of the themes in her art, which also explores archetypes of pop myth and reality.\n\nPandemonia’s art is not only cross-media (sculpture, digital art, photography, and performance), but also cross-generational as she ties the earliest moments of Pop Art to the most current worlds of celebrity, fashion and contemporary art, creating an arc and evolution which continues its ongoing exploration.\n\nIn 2016, Pandemonia was chosen by Camper as the protagonist and muse for its Kobarah shoe style, and has been featured in stores and billboards in several major cities including Paris, London, New York, and Tokyo.\n"}
{"id": "40889647", "url": "https://en.wikipedia.org/wiki?curid=40889647", "title": "Print disability", "text": "Print disability\n\nA print-disabled person is \"a person who cannot effectively read print because of a visual, physical, perceptual, developmental, cognitive, or learning disability\". A print disability prevents a person from gaining information from printed material in the standard way, and requires them to utilize alternative methods to access that information. Print disabilities include visual impairments, learning disabilities, or physical disabilities that impede the ability to manipulate a book in some way. The term was coined by George Kerscher, a pioneer in digital talking books.\n\nA conference organised by the World Intellectual Property Organization (WIPO) in Marrakesh, Morocco, in June 2013 adopted a special treaty called \"A Treaty to Facilitate Access to Published Works by Visually Impaired Persons and Persons with Print Disabilities\" (briefly Marrakesh VIP Treaty).\nThe Marrakesh Treaty represents an important change in how law makers balance the demands of copyright owners against the interests of people with disabilities in particular, and a potential point of inflection in global copyright politics more generally.\n"}
{"id": "1410488", "url": "https://en.wikipedia.org/wiki?curid=1410488", "title": "Quarrel of the Ancients and the Moderns", "text": "Quarrel of the Ancients and the Moderns\n\nThe quarrel of the Ancients and the Moderns () began overtly as a literary and artistic debate that heated up in the early 17th century and shook the Académie française.\n\nIt was an essential feature of the European Renaissance to praise recent discoveries and achievements as a means to assert the independence of modern culture from the institutions and wisdom inherited from Classical (Greek and Roman) authorities. From the first years of the sixteenth century, one of the major reasonings used to this end by the most eminent humanists (François Rabelais, Girolamo Cardano, Jean Bodin, Louis LeRoy, Tommaso Campanella, Francis Bacon, etc.) was that of the \"Three Greatest Inventions of Modern Times\" – the printing press, firearms, and the nautical compass – which together allowed the Moderns to communicate, exert power, and travel at distances never imagined by the Ancients. When the quarrel of the Ancients and the Moderns later arose in France, the \"Three Greatest Inventions of Modern Times\" would almost invariably be adduced as evidence of the Moderns' superiority.\n\nThe debate became known as 'a quarrel' after the frequently made pun on Charles Perrault's title \"Parallel of the Ancients and the Moderns\", the French word 'querelle' being substituted for 'parallele'.\n\nOn one side of the debate were the Ancients (\"Anciens\"), led by Boileau. The Ancients supported the merits of the ancient authors, and contended that a writer could do no better than imitate them. On the other side were the Moderns (\"Modernes\"), who opened fire first with Perrault's \"Le siècle de Louis le Grand\" (\"The Century of Louis the Great,\" 1687), in which he supported the merits of the authors of the century of Louis XIV and expressed the Moderns' stance in a nutshell:\n\nLearned Antiquity, through all its extent, Was never enlightened to equal our times.\nFontenelle quickly followed with his \"Digression sur les anciens et les modernes\" (1688), in which he took the Modern side, pressing the argument that modern scholarship allowed modern man to surpass the ancients in knowledge.\n\nIn the opening years of the next century, Marivaux was to show himself a Modern by establishing a new genre of theatre, unknown to the Ancients, the sentimental comedy (\"comédie larmoyante\"). In it the impending tragedy was resolved at the end, amid reconciliations and floods of tears. By constraining his choice of subjects to those drawn from the literature of Antiquity, Jean Racine showed himself one of the Ancients. He also restricted his tragedies by the classical unities, derived by the classicists from Aristotle's \"Poetics\": the unities of place, time, and action (one scene location, 24 hours, and consistent actions respectively).\n\nThe Quarrel of the Ancients and Moderns was a cover, often a witty one, for deeper opposed views. The very idea of Progress was under attack on the one side, and Authority on the other. The new antiquarian interests led to critical reassessment of the products of Antiquity that would eventually bring Scripture itself under the magnifying glass of some Moderns. The attack on authority in literary criticism had analogues in the rise of scientific inquiry, and the Moderns' challenge to authority in literature foreshadowed a later extension of challenging inquiry in systems of politics and religion.\n\nIn the 17th century, Sir William Temple argued against the Modern position in his essay \"On Ancient and Modern Learning\"; therein he repeated the commonplace, originally from Bernard of Chartres, that we see more only because we are dwarves standing on the shoulders of giants. Temple's essay prompted a small flurry of responses. Among others, two men who took the side opposing Temple were classicist and editor Richard Bentley and critic William Wotton.\n\nThe entire discussion in England was over by 1696, but it was revisited by Jonathan Swift, who saw in the opposing camps of Ancients and Moderns a shorthand of two general orientations or ways of life. He articulated his discussion most notably in his satire \"A Tale of a Tub\", composed between 1694 and 1697 and published in 1704 with the famous prolegomenon \"The Battle of the Books\", long after the initial salvoes were over in France. Swift's polarizing satire provided a framework for other satirists in his circle of the Scriblerians.\n\nTwo other distinguished 18th-century philosophers who wrote at length concerning the distinction between moderns and ancients are Giambattista Vico (cf. e.g. his \"De nostri temporis studiorum ratione\") and Gotthold Ephraim Lessing (for whom the moderns see 'more,' but the ancients see 'better').\n\nIn 19th-century England, highlighting the distinction between Hellenism (\"Athens\"/reason or \"sweetness and light\") and Hebraism (\"Jerusalem\"/faith), Matthew Arnold defended the ancients (most notably Plato and Aristotle) against the dominant progressive intellectual trends of his times. Arnold drew attention to the fact that the great divide between ancients and modernists pertained to the understanding of the relation between liberty/reason and authority.\n\nCountering the thrust of much of 20th-century intellectual history and literary criticism, Leo Strauss has contended that the debate between ancients and moderns (or the defenders of either camp) is ill understood when reduced to questions of progress or regress. Strauss himself revived the old \"querelle,\" siding with the ancients (against the modernist position advocated, e.g., by Strauss's friend Alexandre Kojève).\n\n\n\n"}
{"id": "36598455", "url": "https://en.wikipedia.org/wiki?curid=36598455", "title": "Racialized society", "text": "Racialized society\n\nA racialized society is a society where socioeconomic inequality, residential segregation and low intermarriage rates are the norm, where humans’ definitions of personal identity and choices of intimate relationships reveal racial distinctiveness.\n\nA racialized society is a society where perceived race matters profoundly for life experiences, opportunities, and interpersonal relationships.\n\nA racialized society can also be said to be \"a society that allocates differential economic, political, social, and even psychological rewards to groups along perceived racial lines; lines that are socially constructed.\"\n\nIt is argued that racial/ethnic identity are not separate or autonomous categories and what is called 'racial categories' in the United States are actually racialized ethnic categories.\n\nUnited States society is considered by some a racialized society in which divisions between the racial/ethnic groups are given. Critical race theory argues that racism is normal and is engrained [sic] in the fabric and system of the American society. There are ongoing racial disparities between races in the United States in employment, housing, religion, and race-conscious institutions. Some scholars argue a \"privileged/non-privileged dynamic\" exists. This means that cultural practice assigns value and assumed competence to people who have certain characteristics or features. The social psychological approach maintains that prejudice socialized early in life feeds racial stereotypes.\n\nIt is often said that social interaction is infused with a privileged / non-privileged dynamic which is defined by racial identity — is very complex issue. Racialization hurts both the privileged and the non-privileged, but hurts the non-privileged most.\n\nUntil the 1960s there was legal racial discrimination in the United States. The end of legal discrimination produced major improvements, but scarcely was successful in wiping the slate clean of the many legacies of more than three centuries of formalized state supported inequality. Even after the era of official social discrimination and segregation the lingering residual practices kept African Americans in lower-caste status. Racial problems were viewed as the nation's \"most important problem\" and many observers felt the United States was in a state of racial crisis. Racially related issues, such as welfare, crime, segregation, \"permissive judges\", affirmative action, group based rights, difference-blind treatment, and government regulation and state neutrality with respect to group, have been the subject of strenuous political debate and legislation in the past three decades. \nSignificant gaps between blacks and whites in most domains that measure the quality of life continue to exist. Effective standards designed to eliminate discrimination, often described as race-conscious remedies, have been intensely debated. Supporters argue that institutional racism is so deeply and subtly embedded in the fabric of American society that little would change if more proactive methods of eliminating discrimination had been used.\n\n"}
{"id": "30656844", "url": "https://en.wikipedia.org/wiki?curid=30656844", "title": "Sanankuya", "text": "Sanankuya\n\nSanankuya (also sanankou(n)ya, sinankun, senenkun, senankuya) refers to a social characteristic present especially among the Mande peoples of Mali, Guinea and Gambia as well as many West African societies in general, often described in English with terms such as \"cousinage\" or \"joking relationship\".\n\nIn addition to \"sanankuya\" relationships that are pre-established between certain ethnic or professional clans, a \"sanankuya\" relationship can also be established between any two willing participants who have \"broken the ice\". Those in a \"sanankuya\" relationship may treat one another as if cousins or close family members with whom familiar jokes or humorous insults are exchanged. It is considered an essential element of Mande/West African society/and was reputedly ordained as a civic duty in the \"Kurukan Fuga\", the oral Constitution of the Mali Empire, by Sundiata Keita in c. 1236. \n\nThe effects of the complex and longstanding custom on West African society may also be seen surviving in African-American culture in such cultural institutions as \"the Dozens\", as well as the custom of non-blood relatives according each other the status of familial relationships (\"play\" aunts, cousins, etc.).\n\nFor an example, the Traoré and Koné clans each maintain a \"sanankuya\" relationship with the others' members, and one of the biggest running jokes is that each clan will accuse the other of loving to eat beans the most.\n\n"}
{"id": "13251530", "url": "https://en.wikipedia.org/wiki?curid=13251530", "title": "Schleberoda", "text": "Schleberoda\n\nSchleberoda is a village and a former municipality in the Burgenlandkreis district, in Saxony-Anhalt, Germany. Since 1 July 2009, it is part of the town Freyburg.\n\nIt has been proposed by Germany for inscription in the List of World Heritage. The World Heritage nomination Naumburg Cathedral and the High Medieval Cultural Landscape of the Rivers Saale and Unstrut is representative for the processes that shaped the continent during the High Middle Ages between 1000 and 1300: Christianization, the so-called Landesausbau and the dynamics of cultural exchange and transfer characteristic for this very period.\n\nSchleberoda is located north of the Unstrut. It was founded as a result of intensive land development by clearance in the High Middle Ages such as other villages with ″root″ (Roda) in the name (e.g. Albersroda, Baumersorda, Schnellroda, Ebersroda and Schleberoda). \nThe closely bordering local nobles in the area separated their territories using stone landmarks, as is still visible at the historic edges of the nearby woods, authentic landmarks which have been unchanged since the High Middle Ages: The clearance work and founding of new villages ran southwards from the original parish of Mücheln and ended at the Neue Göhle forest, which belonged to the territory of the Landgraves of Thuringia and could therefore not be cleared.<br>\nThe forest Neue Göhle (Old Sorbian name gola = heath forest) has survived as a continuous, large forest complex. It was used for the production of pole wood in the context of winegrowing at a very early point in time, which is why it had been afforested in the form of coppice-with-standards. Stone landmarks, ditches, banks and paths are still visible at the historic edges of this woods separated the territories of the closely bordering local nobles in the area.\n\nSchleberoda has retained the structures and authentic form dating from the time of its foundation. It is an examples of the land development in the contact area between Germans and Slavs in the High Middle Ages. It has been able to retain its original ground plan as a radial round village dating from the High Middle Ages. Around the original village pond, serving for fire-fighting, and the bake house, replaced at the end of the 18th century, forward-gabled and side-gabled buildings are grouped, some made with packed loam, with high-quality portals dating from the 16th to 19th century. \nThe position of the former gate to the compact village settlement is still identifiable today. Including the outer boundary of the official village area is not only identifiable by a circumferential path and has survived in its original form to the north-west in the shape of an impenetrable hedge. The Romanesque choir tower of the village church with coupled abat sons has survived.\n\n\n"}
{"id": "12082329", "url": "https://en.wikipedia.org/wiki?curid=12082329", "title": "Science in newly industrialized countries", "text": "Science in newly industrialized countries\n\nScientific research is concentrated in the developed world, with only a marginal contribution from the rest of the world. Most Nobel Laureates are either from United States, Europe, or Japan. Many newly industrialized countries have been trying to establish scientific institutions, but with limited success. There is an insufficient dedicated, inspired and motivated labor pool for science and insufficient investment in science education.\n\nThe reason that there have been so few scientists, who have made their mark globally, from most NIC's (Newly Industrialized Countries) is partly historical and partly social A true scientist is nurtured from the school up wards to scientific establishments. Only, if there are inspired and dedicated school science teachers in abundance, there will be sufficient number of inspired students who would like to take science as a career option and who may one day become a successful scientist.\n\nA common thread can indeed be discerned in the state of science in many NICs. Thus although, most of the science establishments in the major NICs can be said to be doing fairly well, none of them have been as successful as the developed countries.\n\nAfter the Second World War, a small technical elite arose in developing countries such as India, Pakistan, Brazil, and Iraq who had been educated as scientists in the industrialized world. They spearheaded the development of science in these countries, presuming that by pushing for Manhattan project-type enterprises in nuclear power, electronics, pharmaceuticals, or space exploration they could leapfrog the dismally low level of development of science establishments in their countries. India, for example, started a nuclear energy program that mobilized thousands of technicians and cost hundreds of millions of dollars but had limited success. Though China, North Korea, India and Pakistan have been successful in deploying nuclear weapons and some of them e.g. China and India have launched fairly successful space programs, (for example, Chandrayaan I (\"Sanskrit\" चंद्रयान-1), which literally means \"Moon Craft,\" is an unmanned lunar mission by the Indian Space Research Organisation and it hopes to land a motorised rover on the moon in 2010 or 2011 as a part of its second Chandrayaan mission; Chang'e I, China's moon probing project is proceeding in full swing in a well-organized way), the fact remains that most of the scientists responsible for these deeds had received their terminal education from some institution or university in US or Europe. In addition there have been hardly any Nobel laureates in science who have conducted the path-breaking research in a native science establishment.\n\nBrazilian science effectively began in the 19th century, until then, Brazil was a poor colony, without universities, printing presses, libraries, museums, etc. This was perhaps a deliberate policy of the Portuguese colonial power, because they feared that the appearance of educated Brazilian classes would boost nationalism and aspirations toward political independence.\n\nThe first attempts of having a Brazilian science establishment were made around 1783, with the expedition of Portuguese naturalist Alexandre Rodrigues, who was sent by Portugal's prime minister, the Marquis of Pombal, to explore and identify Brazilian fauna, flora and geology. His collections, however, were lost to the French, when Napoleon invaded, and were transported to Paris by Étienne Geoffroy Saint-Hilaire. In 1772, the first learned society, the Sociedade Scientifica, was founded in Rio de Janeiro, but lasted only until 1794. Also, in 1797, the first botanic institute was founded in Salvador, Bahia. In the second and third decades of the twentieth century, the main universities in Brazil were organised from a set of existing medical, engineering and law schools. The University of Brazil dates from 1927, the University of São Paulo - today the largest in the Country - dates from 1934.\n\nToday, Brazil has a well-developed organization of science and technology. Basic research in science is largely carried out in public universities and research centers and institutes, and some in private institutions, particularly in non-profit non-governmental organizations. More than 90% of funding for basic research comes from governmental sources.\n\nApplied research, technology and engineering is also largely carried out in the university and research centers system, contrary-wise to more developed countries such as the United States, South Korea, Germany, Japan, etc. A significant trend is emerging lately. Companies such as Motorola, Samsung, Nokia and IBM have established large R&D&I centers in Brazil. One of the incentive factors for this, besides the relatively lower cost and high sophistication and skills of Brazilian technical manpower, has been the so-called Informatics Law, which exempts from certain taxes up to 5% of the gross revenue of high technology manufacturing companies in the fields of telecommunications, computers, digital electronics, etc. The Law has attracted annually more than 1,5 billion dollars of investment in Brazilian R&D&I. Multinational companies have also discovered that some products and technologies designed and developed by Brazilians are significantly competitive and are appreciated by other countries, such as automobiles, aircraft, software, fiber optics, electric appliances, and so on.\n\nThe challenges Brazilian science faces today are: to expand the system with quality, supporting the installed competence; transfer knowledge from the research sector to industry; embark on government action in strategic areas; enhance the assessment of existing programmes and commence innovative projects in areas of relevance for the Country. Furthermore, scientific dissemination plays a fundamental role in transforming the perception of the public at large of the importance of science in modern life. The government has undertaken to meet these challenges using institutional base and the operation of existing qualified scientists.\n\nA question that has been intriguing many historians studying China is the fact that China did not develop a scientific revolution and Chinese technology fell behind that of Europe. Many hypotheses have been proposed ranging from the cultural to the political and economic. has argued that China indeed had a scientific revolution in the 17th century and that we are still far from understanding the scientific revolutions of the West and China in all their political, economic and social ramifications. Some like John K. Fairbank are of the opinion that the Chinese political system was hostile to scientific progress.\n\nNeedham argued, and most scholars agreed, that cultural factors prevented these Chinese achievements from developing into what could be called \"science\". It was the religious and philosophical framework of the Chinese intellectuals which made them unable to believe in the ideas of laws of nature. More recent historians have questioned political and cultural explanations and have focused more on economic causes. Mark Elvin's high level equilibrium trap is one well-known example of this line of thought, as well as Kenneth Pomeranz' argument that resources from the New World made the crucial difference between European and Chinese development.\n\nThus, it was not that there was no order in nature for the Chinese, but rather that it was not an order ordained by a rational personal being, and hence there was no conviction that rational personal beings would be able to spell out in their lesser earthly languages the divine code of laws which he had decreed aforetime. The Taoists, indeed, would have scorned such an idea as being too naive for the subtlety and complexity of the universe as they intuited it. Similar grounds have been found for questioning much of the philosophy behind traditional Chinese medicine, which, derived mainly from Taoist philosophy, reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. Because its theory predates use of the scientific method, it has received various criticisms based on scientific thinking. Even though there are physically verifiable anatomical or histological bases for the existence of acupuncture points or meridians, for instance skin conductance measurements show increases at the predicted points.\n\nToday, science and technology establishment in the People's Republic of China is growing rapidly. Even as many Chinese scientists debate what institutional arrangements will be best for Chinese science, reforms of the Chinese Academy of Sciences continue. The average age of researchers at the Chinese Academy of Sciences has dropped by nearly ten years between 1991 and 2003. However, many of them are educated in the United States and other foreign countries.\n\nChinese university undergraduate and graduate enrollments more than doubled from 1995 to 2005. The universities now have more cited PRC papers than CAS in the Science Citation Index. Some Chinese scientists say CAS is still ahead on overall quality of scientific work but that lead will only last five to ten years.\n\nSeveral Chinese immigrants to the United States have also been awarded the Nobel Prize, including:, Samuel C. C. Ting, Chen Ning Yang, Tsung-Dao Lee, Yuan T. Lee, Daniel C. Tsui, and Gao Xingjian. Other overseas ethnic Chinese that have achieved success in sciences include Fields Medal recipient Shing-Tung Yau and Terence Tao, and Turing Award recipient Andrew Yao. Tsien Hsue-shen was a prominent scientist at NASA's Jet Propulsion Laboratory, while Chien-Shiung Wu contributed to the Manhattan Project (some argue she never received the Nobel Prize unlike her colleagues Tsung-Dao Lee and Chen Ning Yang due to sexism by the selection committee). Others include Charles K. Kao, a pioneer in fiber optics technology, and Dr. David Ho, one of the first scientists to propose that AIDS was caused by a virus, thus subsequently developing combination antiretroviral therapy to combat it. Dr. Ho was named TIME magazine's 1996 Man of the Year. In 2015, Tu Youyou, a pharmaceutical chemist, became the first native Chinese scientist, born and educated and carried out research exclusively in the People's Republic of China, to receive the Nobel Prize in natural sciences.\n\nThe earliest applications of science in India took place in the context of medicine, metallurgy, construction technology (such as ship building, manufacture of cement and paints) and in textile production and dyeing. But in the process of understanding chemical processes, led to some theories about physical processes and the forces of nature that are today studied as specific topics within the fields of chemistry and physics.\n\nMany mathematical concepts today were contributed by Indian mathematicians like Aryabhata.\n\nThere was really no place for scientists in the Indian caste system. Thus while there were/are castes for the learned brahmins, the warriors kshatriyas, the traders vaishyas and the menial workers shudras, maybe even the bureaucrats (the kayasths) there was/is hardly any formal place in the social hierarchy for a people who discover new knowledge or invent new devices based on the recently discovered knowledge, even though scientific temper has always been in India, in the form of logic, reasoning and method of acquiring knowledge. Its therefore no wonder that some Indians quickly learned to value science, especially those belonging to the privileged Brahmin caste during the British colonial rule that lasted over two centuries. Some Indians did succeed to achieve notable success and fame, examples include Satyendra Nath Bose, Meghnad Saha, Jagdish Chandra Bose and C. V. Raman even though they belonged to different castes. The science communication had begun with publication of a scientific journal, Asiatick Researches in 1788. Thereafter, the science communication in India has evolved in many facets. Following this, there has been a continuing development in the formation of scientific institutions and publication of scientific literature. Subsequently, scientific publications also started appearing in Indian languages by the end of eighteenth century. The publication of ancient scientific literature and textbooks at mass scale started in the beginning of nineteenth century. The scientific and technical terms, however, had been a great difficulty for a long time for popular science writing.\n\n\n"}
{"id": "8684290", "url": "https://en.wikipedia.org/wiki?curid=8684290", "title": "Shorter Jewish Encyclopedia", "text": "Shorter Jewish Encyclopedia\n\nThe Shorter Jewish Encyclopedia (\"SJE\" - , Kratkaya Yevreyskaya Entsiklopedia) was published in 11 volumes in Jerusalem from 1976 to 2005 in Russian by the Society for Research on Jewish Communities with the support of Hebrew University in Jerusalem. The \"SJE\" is the only comprehensive encyclopedia on Judaism published in Russian, and followed an almost 70-year gap following the publication of the \"Yevreyskaya Entsiklopedia\" (\"Encyclopedia Judaica\") of Brokhaus and Efron in Saint Petersburg in 1908.\nAlthough it was originally planned as an abridged translation of the English-language \"Encyclopedia Judaica\", it became clear as the work progressed that readers raised in the Soviet Union would not be familiar with the concepts lying at the foundation of the cultural and historical system known as Jewish civilization. Therefore, these concepts were elaborated on in greater detail in the \"SJE\", and terms were introduced which lacked equivalents in modern Russian. Most personal and geographic names (in Israel) from the Bible are given in the accepted Hebrew form. The conceptual foundation of the \"SJE\" is characterized by a thematic bipolarity: Eretz Israel, and in particular the State of Israel on the one hand, and Russian (i.e., Soviet) Jewry on the other, which nevertheless does not exclude the broad scope of different aspects of the lives and history of Jews in all the other countries of the diaspora.\n\nA group of editors worked on the \"SJE\" who prepared articles with the participation of invited specialists and also academic consultants, including the well-known Israeli academics and public figures Shraga Abramson, Mordechai Altschuler, Shlomo Pines, Hayim Tadmor, Chone Shmeruk, Hayyim Schirmann, Menachem Stern, Yaakov Tsur, Yaakov Landau, Israel Bartal, and Michael Liebman. Chairing the editorial board were Shmuel Ettinger and Haim Beinhart. Chief editors were Yitzhak Oren (Nadel), Michael Zand, Naftali Prat, and Ari Avner. The senior academic editors were Peretz Hein, Yosef Glozman, Amnon Ginzai, and Mark Kipnis. The managing editors were Ella Slivkina (vol. 1-10) and Marina Gutgarts.\n\nIn practical terms, the \"SJE\" was no longer \"shorter,\" and aside from the 11 volumes a \"Jewish calendar juxtaposed with a Gregorian calendar (1948-2048)\" was published as a pamphlet along with three supplements. Volume 10 also contains a \"thematic bibliographic index\" with 2,114 items. There are more than 5,300 vocabulary entries, and the total number of words exceeds six million. Volume 11, the final one, included an alphabetized index of subjects, including geographic and personal names and events with references to the volume and column where they are located. A system of citations indicating links between concepts serves as the only cross-reference in the entire corpus of the encyclopedia.\n\nIn 1996, the Society for Research on Jewish Communities undertook a reprinting of the first seven volumes of the \"SJE\", which were printed by the printing and publishing house Krasnyj Proletarij in Moscow.\n\nIn 2005, the \"Electronic Jewish Encyclopedia\" (EJE), (Elektronnaja Evrejskaja Entsiklopedia) was made available on the internet, presenting an expanded and more precise version of the \"SJE\". Work on the \"EJE\" continues today.\n"}
{"id": "36315801", "url": "https://en.wikipedia.org/wiki?curid=36315801", "title": "Social artistry", "text": "Social artistry\n\nSocial artistry is the attempt to address or recognize a particular social issue using art and creativity. Social artists are people who use creative skills to work with people or organizations in their community to affect change. While a traditional artist uses their creative skills to express their take on the world, a social artist puts their skills to use to help promote and improve communities. Thus, the main aim of a social artist is to improve society as a whole and to help other people find their own means of creative expression.\n\nSocial artists may address issues such as youth alienation or the breakdown of communities. Most commonly, social artists will address these problems by helping people express themselves and find their voice, or by bringing people together and using art to help them to foster an understanding of each other.\n\nSocial artistry can incorporate several different art forms including theatre, poetry, music and visual art. \n\nFindings from 2013 confirm the shift from individual expression to community engagement, or \"from autonomous to socially engaged.\" Lingo and Tepper cite several examples:\n\n"}
{"id": "5057528", "url": "https://en.wikipedia.org/wiki?curid=5057528", "title": "Social osmosis", "text": "Social osmosis\n\nSocial osmosis is the indirect infusion of social or cultural knowledge. Effectively, social content is diffused, and by happenstance authentic experience is displaced by degrees of mediated separation before a subject acquires knowledge of a social phenomenon.\n\nAn example of \"social osmosis\" would be knowing a show exists and possessing detailed information concerning aspects of the show, such as the characters' stage names and the names of the cast, \"without\" actively acquiring this knowledge by watching the show.\n\n"}
{"id": "57990980", "url": "https://en.wikipedia.org/wiki?curid=57990980", "title": "Symbiosis in fiction", "text": "Symbiosis in fiction\n\nSymbiosis (mutualism) appears in fiction, especially science fiction, as a plot device. It is distinguished from parasitism in fiction, a similar theme, by the mutual benefit to the organisms involved, whereas the parasite inflicts harm on its host.\n\nRelationships between species in early science fiction were often imaginatively parasitic, with the parasites draining the vital energy of their human hosts and taking over their minds, as in Arthur Conan Doyle's 1895 \"The Parasite\".\n\nAfter the Second World War, science fiction moved towards more mutualistic relationships, as in Ted White's 1970 \"By Furies Possessed\"; Brian Stableford argues that White was consciously opposing the xenophobia of Robert Heinlein's 1951 \"The Puppet Masters\" which involved a parasitic relationship close to demonic possession, with a more positive attitude towards aliens. Stableford notes, however, that Octavia Butler's 1984 \"Clay's Ark\" and other of her works such as \"Fledgling\", and Dan Simmons's 1989 \"Hyperion\" take an ambivalent position, in which the aliens may confer powers such as \"Hyperion\"'s ability to regenerate continually—but at a price, in its case an incremental loss of intelligence at each regeneration.\n\nThe Force in the \"Star Wars\" universe is described by the fictional seer Obi-Wan Kenobi as \"an energy field created by all living things\". In \"The Phantom Menace\", Qui-Gon Jinn says microscopic lifeforms called midi-chlorians, inside all living cells, allow characters with enough of these symbionts in their cells to feel and use the Force.\n\nIn Douglas Adams's humorous 1978 \"The Hitchhiker's Guide to the Galaxy\", the Babel fish lives in its human host's ear, feeding on the energy of its host's brain waves, in return translating any language to the host's benefit.\n\n"}
{"id": "31037220", "url": "https://en.wikipedia.org/wiki?curid=31037220", "title": "Vs. (magazine)", "text": "Vs. (magazine)\n\nVs. is an international fashion and lifestyle magazine published twice per year. Featuring sleek design, luxury fashion stories, editorial edge, and large format, glossy, print size, \"Vs.\" was founded by Jakob F. Stubkjær and Vibe Dabelsteen in 2005. \"Vs.\"<nowiki>'</nowiki>s offices are based in New York, United States. \n\n\"Vs.\" features some of the most prominent figures in fashion and mainstream culture from supermodels and celebrities to emerging talents within fashion, beauty, music, art, film and culture. \nEvery issue has four different covers. Fall/Winter 2010 featured Naomi Campbell, Eva Mendes, Christina Ricci and Rachel Weisz on the front cover, and the Spring/Summer 2011 issues featured Kylie Minogue, Sky Ferreira, Paris Hilton and Oh Land \n\nVsmagazinelive.com is the LIVE edition of the printed magazine. With all the notions of web exploited, that is the motion, the audio, the pace, the form. Vsmagazinelive.com features new content every day from historic film clips to exclusive Vs. fashion productions, daily style and fashion news, as well as trends, industry and cultural updates.\nHeadquarters in NYC\n\nThe following models and talent have featured on the front covers of \"Vs. Magazine\".\n"}
{"id": "12649093", "url": "https://en.wikipedia.org/wiki?curid=12649093", "title": "Wannarexia", "text": "Wannarexia\n\nWannarexia, or anorexic yearning,\nis a label applied to someone who claims to have anorexia nervosa, or wishes they did, but does not. These individuals are also called wannarexic, “wanna-be ana” or \"anorexic wannabe\". The neologism \"wannarexia\" is a portmanteau of the latter two terms. It may be used as a pejorative term.\n\nWannarexia is a cultural phenomenon and has no diagnostic criteria, although some wannarexics may be instead diagnosed with eating disorder not otherwise specified (EDNOS). Wannarexia is more commonly, but not always, found in teenage girls who want to be trendy, and is likely caused by a combination of cultural and media influences.\n\nDr. Richard Kreipe states that the distinction between anorexia and wannarexia is that anorexics aren't satisfied by their weight loss, while wannarexics are more likely to derive pleasure from weight loss. Many people who actually suffer from the eating disorder anorexia are angry, offended, or frustrated about wannarexia.\n\nWannarexics may be inspired or motivated by the pro-anorexia, or pro-ana, community that promotes or supports anorexia as a lifestyle choice rather than an eating disorder. Some participants in pro-ana web forums only want to associate with \"real anorexics\" and will shun wannarexics who only diet occasionally, and are not dedicated to the \"lifestyle\" full-time. Community websites for anorexics and bulimics have posted advice to wannarexics saying that they don't want their \"warped perspectives and dangerous behaviour to affect others.\"\n\nKelsey Osgood uses the label in her book \"How To Disappear Completely: On Modern Anorexia\" where she describes wannarexia as “a gateway drug for teenagers”.\n"}
