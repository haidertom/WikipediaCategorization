{"id": "14973425", "url": "https://en.wikipedia.org/wiki?curid=14973425", "title": "Ambilocal residence", "text": "Ambilocal residence\n\nAmbilocal residence (or ambilocality), also called bilocal residence (bilocality) is the societal postmarital residence in which couples, upon marriage, choose to live with or near either spouse's parents. This is contrasted with matrilocality and patrilocality, where the newlyweds are expected to live with either the wife's parents or the husband's parents respectively. \n\n"}
{"id": "26534334", "url": "https://en.wikipedia.org/wiki?curid=26534334", "title": "Anorexia nervosa", "text": "Anorexia nervosa\n\nAnorexia nervosa, often referred to simply as anorexia, is an eating disorder characterized by low weight, fear of gaining weight, and a strong desire to be thin, resulting in food restriction. Many people with anorexia see themselves as overweight even though they are in fact underweight. If asked they usually deny they have a problem with low weight. Often they weigh themselves frequently, eat only small amounts, and only eat certain foods. Some will exercise excessively, force themselves to vomit, or use laxatives to produce weight loss. Complications may include osteoporosis, infertility and heart damage, among others. Women will often stop having menstrual periods.\nThe cause is not known. There appear to be some genetic components with identical twins more often affected than non-identical twins. Cultural factors also appear to play a role with societies that value thinness having higher rates of disease. Additionally, it occurs more commonly among those involved in activities that value thinness such as high-level athletics, modelling, and dancing. Anorexia often begins following a major life-change or stress-inducing event. The diagnosis requires a significantly low weight. The severity of disease is based on body mass index (BMI) in adults with mild disease having a BMI of greater than 17, moderate a BMI of 16 to 17, severe a BMI of 15 to 16, and extreme a BMI less than 15. In children a BMI for age percentile of less than the 5th percentile is often used.\nTreatment of anorexia involves restoring a healthy weight, treating the underlying psychological problems, and addressing behaviors that promote the problem. While medications do not help with weight gain, they may be used to help with associated anxiety or depression. A number of types of therapy may be useful, such as cognitive behavioral therapy or an approach where parents assume responsibility for feeding their child known as Maudsley family therapy. Sometimes people require admission to hospital to restore weight. Evidence for benefit from nasogastric tube feeding, however, is unclear. Some people will just have a single episode and recover while others may have many episodes over years. Many complications improve or resolve with regaining of weight.\nGlobally, anorexia is estimated to affect 2.9 million people . It is estimated to occur in 0.9% to 4.3% of women and 0.2% to 0.3% of men in Western countries at some point in their life. About 0.4% of young women are affected in a given year and it is estimated to occur ten times less commonly in men. Rates in most of the developing world are unclear. Often it begins during the teen years or young adulthood. While anorexia became more commonly diagnosed during the 20th century it is unclear if this was due to an increase in its frequency or simply better diagnosis. In 2013 it directly resulted in about 600 deaths globally, up from 400 deaths in 1990. Eating disorders also increase a person's risk of death from a wide range of other causes, including suicide. About 5% of people with anorexia die from complications over a ten-year period, a nearly 6 times increased risk. The term anorexia nervosa was first used in 1873 by William Gull to describe this condition.\n\nAnorexia nervosa is an eating disorder characterized by attempts to lose weight, to the point of starvation. A person with anorexia nervosa may exhibit a number of signs and symptoms, the type and severity of which may vary and may be present but not readily apparent.\n\nAnorexia nervosa, and the associated malnutrition that results from self-imposed starvation, can cause complications in every major organ system in the body. Hypokalaemia, a drop in the level of potassium in the blood, is a sign of anorexia nervosa. A significant drop in potassium can cause abnormal heart rhythms, constipation, fatigue, muscle damage and paralysis.\n\nSymptoms may include:\n\nInteroception has an important role in homeostasis and regulation of emotions and motivation. Anorexia has been associated with disturbances to interoception. People with anorexia concentrate on distorted perceptions of their body exterior due to fear of looking overweight. Aside from outer appearance, they also report abnormal bodily functions such as indistinct feelings of fullness. This provides an example of miscommunication between body and brain. Further, people with anorexia experience abnormally intense cardiorespiratory sensations, particularly of the breath, most prevalent before they consume a meal. People with anorexia also report inability to distinguish emotions from bodily sensations in general, called alexithymia. In addition to metacognition, people with anorexia also have difficulty with social cognition including interpreting others’ emotions, and demonstrating empathy. Abnormal interoceptive awareness like these examples have been observed so frequently in anorexia that they have become key characteristics of the illness.\n\nOther psychological issues may factor into anorexia nervosa; some fulfill the criteria for a separate Axis I diagnosis or a personality disorder which is coded Axis II and thus are considered comorbid to the diagnosed eating disorder. Some people have a previous disorder which may increase their vulnerability to developing an eating disorder and some develop them afterwards. The presence of Axis I or Axis II psychiatric comorbidity has been shown to affect the severity and type of anorexia nervosa symptoms in both adolescents and adults.\n\nObsessive-compulsive disorder (OCD) and obsessive-compulsive personality disorder (OCPD) are highly comorbid with AN, particularly the restrictive subtype. Obsessive-compulsive personality disorder is linked with more severe symptomatology and worse prognosis. The causality between personality disorders and eating disorders has yet to be fully established. Other comorbid conditions include depression, alcoholism, borderline and other personality disorders, anxiety disorders, attention deficit hyperactivity disorder, and body dysmorphic disorder (BDD). Depression and anxiety are the most common comorbidities, and depression is associated with a worse outcome.\n\nAutism spectrum disorders occur more commonly among people with eating disorders than in the general population. Zucker \"et al.\" (2007) proposed that conditions on the autism spectrum make up the cognitive endophenotype underlying anorexia nervosa and appealed for increased interdisciplinary collaboration.\n\nThere is evidence for biological, psychological, developmental, and sociocultural risk factors, but the exact cause of eating disorders is unknown.\n\nAnorexia nervosa is highly heritable. Twin studies have shown a heritability rate of between 28 and 58%. First degree relative of those with anorexia have roughly 12 times the risk of developing anorexia. Association studies have been performed, studying 128 different polymorphisms related to 43 genes including genes involved in regulation of eating behavior, motivation and reward mechanics, personality traits and emotion. Consistent associations have been identified for polymorphisms associated with agouti-related peptide, brain derived neurotrophic factor, catechol-o-methyl transferase, SK3 and opioid receptor delta-1. Epigenetic modifications, such as DNA methylation, may contribute to the development or maintenance of anorexia nervosa, though clinical research in this area is in its infancy.\n\nObstetric complications: prenatal and perinatal complications may factor into the development of anorexia nervosa, such as maternal anemia, diabetes mellitus, preeclampsia, placental infarction, and neonatal cardiac abnormalities. Neonatal complications may also have an influence on harm avoidance, one of the personality traits associated with the development of AN.\n\nNeuroendocrine dysregulation: altered signalling of peptides that facilitate communication between the gut, brain and adipose tissue, such as ghrelin, leptin, neuropeptide Y and orexin, may contribute to the pathogenesis of anorexia nervosa by disrupting regulation of hunger and satiety.\n\nGastrointestinal diseases: people with gastrointestinal disorders may be more risk of developing disorders eating practices than the general population, principally restrictive eating disturbances. An association of anorexia nervosa with celiac disease has been found. The role that gastrointestinal symptoms play in the development of eating disorders seems rather complex. Some authors report that unresolved symptoms prior to gastrointestinal disease diagnosis may create a food aversion in these persons, causing alterations to their eating patterns. Other authors report that greater symptoms throughout their diagnosis led to greater risk. It has been documented that some people with celiac disease, irritable bowel syndrome or inflammatory bowel disease who are not conscious about the importance of strictly following their diet, choose to consume their trigger foods to promote weight loss. On the other hand, individuals with good dietary management may develop anxiety, food aversion and eating disorders because of concerns around cross contamination of their foods. Some authors suggest that medical professionals should evaluate the presence of an unrecognized celiac disease in all people with eating disorder, especially if they present any gastrointestinal symptom (such as decreased appetite, abdominal pain, bloating, distension, vomiting, diarrhea or constipation), weight loss, or growth failure; and also routinely ask celiac patients about weight or body shape concerns, dieting or vomiting for weight control, to evaluate the possible presence of eating disorders, especially in women.\n\nStudies have hypothesized the continuance of disordered eating patterns may be epiphenomena of starvation. The results of the Minnesota Starvation Experiment showed normal controls exhibit many of the behavioral patterns of anorexia nervosa (AN) when subjected to starvation. This may be due to the numerous changes in the neuroendocrine system, which results in a self-perpetuating cycle.\n\nAnother hypothesis is that anorexia nervosa is more likely to occur in populations in which obesity is more prevalent, and results from a sexually selected evolutionary drive to appear youthful in populations in which size becomes the primary indicator of age.\n\nAnorexia nervosa is more likely to occur in a person's pubertal years. Some explanatory hypotheses for the rising prevalence of eating disorders in adolescence are \"increase of adipose tissue in girls, hormonal changes of puberty, societal expectations of increased independence and autonomy that are particularly difficult for anorexic adolescents to meet; [and] increased influence of the peer group and its values.\" \n\nEarly theories of the cause of anorexia linked it to childhood sexual abuse or dysfunctional families; evidence is conflicting, and well-designed research is needed. The fear of food is known as \"sitiophobia\", \"cibophobia\", or \"sitophobia\" and is part of the differential diagnosis. Other psychological causes of anorexia include low self-esteem, feeling like there is lack of control, depression, anxiety, and loneliness.\n\nAnorexia nervosa has been increasingly diagnosed since 1950; the increase has been linked to vulnerability and internalization of body ideals. People in professions where there is a particular social pressure to be thin (such as models and dancers) were more likely to develop anorexia, and those with anorexia have much higher contact with cultural sources that promote weight loss. This trend can also be observed for people who partake in certain sports, such as jockeys and wrestlers. There is a higher incidence and prevalence of anorexia nervosa in sports with an emphasis on aesthetics, where low body fat is advantageous, and sports in which one has to make weight for competition. Family dynamics can play big part in the cause of anorexia. When there is a constant pressure from people to be thin, teasing and bullying can cause low self-esteem and other psychological symptoms.\n\nConstant exposure to media that presents body ideals may constitute a risk factor for body dissatisfaction and anorexia nervosa. The cultural ideal for body shape for men versus women continues to favor slender women and athletic, V-shaped muscular men. A 2002 review found that, of the magazines most popular among people aged 18 to 24 years, those read by men, unlike those read by women, were more likely to feature ads and articles on shape than on diet. Body dissatisfaction and internalization of body ideals are risk factors for anorexia nervosa that threaten the health of both male and female populations.\n\nWebsites that stress the importance of attainment of body ideals extol and promote anorexia nervosa through the use of religious metaphors, lifestyle descriptions, \"thinspiration\" or \"fitspiration\" (inspirational photo galleries and quotes that aim to serve as motivators for attainment of body ideals). Pro-anorexia websites reinforce internalization of body ideals and the importance of their attainment.\n\nThe media give men and women a false view of what people truly look like. In magazines, movies and even on billboards most of the actors/models are digitally altered in multiple ways. People then strive to look like these \"perfect\" role models when in reality they aren't any where near perfection themselves.\n\nEvidence from physiological, pharmacological and neuroimaging studies suggest serotonin may play a role in anorexia. While acutely ill, metabolic changes may produce a number of biological findings in people with anorexia that are not necessarily causative of the anorexic behavior. For example, abnormal hormonal responses to challenges with serotonergic agents have been observed during acute illness, but not recovery. Nevertheless, increased cerebrospinal fluid concentrations of 5-Hydroxyindoleacetic acid (a metabolite of serotonin), and changes in anorectic behavior in response to tryptophan depletion(a metabolic precursor to serotonin) support a role in anorexia. The binding potential of 5-HT receptors and 5-HT receptors have been reportedly decreased and increased respectively in a number of cortical regions. While these findings may be confounded by comorbid psychiatric disorders, taken as a whole they indicate serotonin in anorexia. These alterations in serotonin have been linked to traits characteristic of anorexia such as obsessiveness, anxiety, and appetite dysregulation.\n\nNeuroimaging studies investigating the functional connectivity between brain regions have observed a number of alterations in networks related to cognitive control, introspection, and sensory function. Alterations in networks related to the dorsal anterior cingulate cortex may be related to excessive cognitive control of eating related behaviors. Similarly, altered somatosensory integration and introspection may relate to abnormal body image. A review of functional neuroimaging studies reported reduced activations in \"bottom up\" limbic region and increased activations in \"top down\" cortical regions which may play a role in restrictive eating.\n\nCompared to controls, recovered anorexics show reduced activation in the reward system in response to food, and reduced correlation between self reported liking of a sugary drink and activity in the striatum and ACC. Increased binding potential of [11C]raclopride in the striatum, interpreted as reflecting decreased endogenous dopamine due to competitive displacement, has also been observed.\n\nStructural neuroimaging studies have found global reductions in both gray matter and white matter, as well as increased cerebrospinal fluid volumes. Regional decreases in the left hypothalamus, left inferior parietal lobe, right lentiform nucleus and right caudate have also been reported., in acutely ill patients. However, these alterations seem to be associated with acute malnutrition and largely reversible with weight restoration, at least in nonchronic cases in younger people. In contrast, some studies have reported increased orbitofrontal cortex volume in currently ill and  in recovered patients, although findings are inconsistent. Reduced white matter integrity in the fornix has also been reported.\n\nA diagnostic assessment includes the person's current circumstances, biographical history, current symptoms, and family history. The assessment also includes a mental state examination, which is an assessment of the person's current mood and thought content, focusing on views on weight and patterns of eating.\n\nAnorexia nervosa is classified under the Feeding and Eating Disorders in the latest revision of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM 5).\n\nRelative to the previous version of the DSM (DSM-IV-TR), the 2013 revision (DSM5) reflects changes in the criteria for anorexia nervosa, most notably that of the amenorrhea criterion being removed. Amenorrhea was removed for several reasons: it does not apply to males, it is not applicable for females before or after the age of menstruation or taking birth control pills, and some women who meet the other criteria for AN still report some menstrual activity.\n\nThere are two subtypes of AN:\n\nBody mass index (BMI) is used by the DSM-5 as an indicator of the level of severity of anorexia nervosa. The DSM-5 states these as follows:\n\n\nMedical tests to check for signs of physical deterioration in anorexia nervosa may be performed by a general physician or psychiatrist, including:\n\n\nA variety of medical and psychological conditions have been misdiagnosed as anorexia nervosa; in some cases the correct diagnosis was not made for more than ten years.\n\nThe distinction between the diagnoses of anorexia nervosa, bulimia nervosa and eating disorder not otherwise specified (EDNOS) is often difficult to make as there is considerable overlap between people diagnosed with these conditions. Seemingly minor changes in people's overall behavior or attitude can change a diagnosis from anorexia: binge-eating type to bulimia nervosa. A main factor differentiating binge-purge anorexia from bulimia is the gap in physical weight. Someone with bulimia nervosa is ordinarily at a healthy weight, or slightly overweight. Someone with binge-purge anorexia is commonly underweight. People with the binge-purging subtype of AN may be significantly underweight and typically do not binge-eat large amounts of food, yet they purge the small amount of food they eat. In contrast, those with bulimia nervosa tend to be at normal weight or overweight and binge large amounts of food. It is not unusual for a person with an eating disorder to \"move through\" various diagnoses as their behavior and beliefs change over time.\n\nThere is no conclusive evidence that any particular treatment for anorexia nervosa works better than others; however, there is enough evidence to suggest that early intervention and treatment are more effective. Treatment for anorexia nervosa tries to address three main areas.\nAlthough restoring the person's weight is the primary task at hand, optimal treatment also includes and monitors behavioral change in the individual as well. There is some evidence that hospitalisation might adversely affect long term outcome.\n\nPsychotherapy for individuals with AN is challenging as they may value being thin and may seek to maintain control and resist change. Some studies demonstrate that family based therapy in adolescents with AN is superior to individual therapy.\n\nTreatment of people with AN is difficult because they are afraid of gaining weight. Initially developing a desire to change may be important.\n\nDiet is the most essential factor to work on in people with anorexia nervosa, and must be tailored to each person's needs. Food variety is important when establishing meal plans as well as foods that are higher in energy density. People must consume adequate calories, starting slowly, and increasing at a measured pace. Evidence of a role for zinc supplementation during refeeding is unclear.\n\nFamily-based treatment (FBT) has been shown to be more successful than individual therapy for adolescents with AN. Various forms of family-based treatment have been proven to work in the treatment of adolescent AN including conjoint family therapy (CFT), in which the parents and child are seen together by the same therapist, and separated family therapy (SFT) in which the parents and child attend therapy separately with different therapists. Proponents of family therapy for adolescents with AN assert that it is important to include parents in the adolescent's treatment.\n\nA four- to five-year follow up study of the Maudsley family therapy, an evidence-based manualized model, showed full recovery at rates up to 90%. Although this model is recommended by the NIMH, critics claim that it has the potential to create power struggles in an intimate relationship and may disrupt equal partnerships.\n\nCognitive behavioral therapy (CBT) is useful in adolescents and adults with anorexia nervosa; acceptance and commitment therapy is a type of CBT, which has shown promise in the treatment of AN. Cognitive remediation therapy (CRT) is used in treating anorexia nervosa.\n\nPharmaceuticals have limited benefit for anorexia itself.\n\nAN has a high mortality and patients admitted in a severely ill state to medical units are at particularly high risk. Diagnosis can be challenging, risk assessment may not be performed accurately, consent and the need for compulsion may not be assessed appropriately, refeeding syndrome may be missed or poorly treated and the behavioural and family problems in AN may be missed or poorly managed. The MARSIPAN guidelines recommend that medical and psychiatric experts work together in managing severely ill people with AN.\n\nThe rate of refeeding can be difficult to establish, because the fear of refeeding syndrome (RFS) can lead to underfeeding. It is thought that RFS, with falling phosphate and potassium levels, is more likely to occur when BMI is very low, and when medical comorbidities such as infection or cardiac failure, are present. In those circumstances, it is recommended to start refeeding slowly but to build up rapidly as long as RFS does not occur. Recommendations on energy requirements vary, from 5–10 kcal/kg/day in the most medically compromised patients, who appear to have the highest risk of RFS to 1900 kcal/day\n\nAN has the highest mortality rate of any psychological disorder. The mortality rate is 11 to 12 times greater than in the general population, and the suicide risk is 56 times higher. Half of women with AN achieve a full recovery, while an additional 20–30% may partially recover. Not all people with anorexia recover completely: about 20% develop anorexia nervosa as a chronic disorder. If anorexia nervosa is not treated, serious complications such as heart conditions and kidney failure can arise and eventually lead to death. The average number of years from onset to remission of AN is seven for women and three for men. After ten to fifteen years, 70% of people no longer meet the diagnostic criteria, but many still continue to have eating-related problems.\n\nAlexithymia influences treatment outcome. Recovery is also viewed on a spectrum rather than black and white. According to the Morgan-Russell criteria, individuals can have a good, intermediate, or poor outcome. Even when a person is classified as having a \"good\" outcome, weight only has to be within 15% of average, and normal menstruation must be present in females. The good outcome also excludes psychological health. Recovery for people with anorexia nervosa is undeniably positive, but recovery does not mean a return to normal.\n\nAnorexia nervosa can have serious implications if its duration and severity are significant and if onset occurs before the completion of growth, pubertal maturation, or the attainment of peak bone mass. Complications specific to adolescents and children with anorexia nervosa can include the following:\nGrowth retardation may occur, as height gain may slow and can stop completely with severe weight loss or chronic malnutrition. In such cases, provided that growth potential is preserved, height increase can resume and reach full potential after normal intake is resumed. Height potential is normally preserved if the duration and severity of illness are not significant or if the illness is accompanied by delayed bone age (especially prior to a bone age of approximately 15 years), as hypogonadism may partially counteract the effects of undernutrition on height by allowing for a longer duration of growth compared to controls. Appropriate early treatment can preserve height potential, and may even help to increase it in some post-anorexic subjects, due to factors such as long-term reduced estrogen-producing adipose tissue levels compared to premorbid levels. In some cases, especially where onset is before puberty, complications such as stunted growth and pubertal delay are usually reversible.\n\nAnorexia nervosa causes alterations in the female reproductive system; significant weight loss, as well as psychological stress and intense exercise, typically results in a cessation of menstruation in women who are past puberty. In patients with anorexia nervosa, there is a reduction of the secretion of gonadotropin releasing hormone in the central nervous system, preventing ovulation. Anorexia nervosa can also result in pubertal delay or arrest. Both height gain and pubertal development are dependent on the release of growth hormone and gonadotrophins (LH and FSH) from the pituitary gland. Suppression of gonadotrophins in people with anorexia nervosa has been documented. Typically, growth hormone (GH) levels are high, but levels of IGF-1, the downstream hormone that should be released in response to GH are low; this indicates a state of “resistance” to GH due to chronic starvation. IGF-1 is necessary for bone formation, and decreased levels in anorexia nervosa contribute to a loss of bone density and potentially contribute to osteopenia or osteoporosis. Anorexia nervosa can also result in reduction of peak bone mass. Buildup of bone is greatest during adolescence, and if onset of anorexia nervosa occurs during this time and stalls puberty, low bone mass may be permanent.\n\nHepatic steatosis, or fatty infiltration of the liver, can also occur, and is an indicator of malnutrition in children. Neurological disorders that may occur as complications include seizures and tremors. Wernicke encephalopathy, which results from vitamin B1 deficiency, has been reported in patients who are extremely malnourished; symptoms include confusion, problems with the muscles responsible for eye movements and abnormalities in walking gait.\n\nThe most common gastrointestinal complications of anorexia nervosa are delayed stomach emptying and constipation, but also include elevated liver function tests, diarrhea, acute pancreatitis, heartburn, difficulty swallowing, and, rarely, superior mesenteric artery syndrome. Delayed stomach emptying, or gastroparesis, often develops following food restriction and weight loss; the most common symptom is bloating with gas and abdominal distension, and often occurs after eating. Other symptoms of gastroparesis include early satiety, fullness, nausea, and vomiting. The symptoms may inhibit efforts at eating and recovery, but can be managed by limiting high-fiber foods, using liquid nutritional supplements, or using metoclopramide to increase emptying of food from the stomach. Gastroparesis generally resolves when weight is regained.\n\nAnorexia nervosa increases the risk of sudden cardiac death, though the precise cause is unknown. Cardiac complications include structural and functional changes to the heart. Some of these cardiovascular changes are mild and are reversible with treatment, while others may be life-threatening. Cardiac complications can include arrhythmias, abnormally slow heart beat, low blood pressure, decreased size of the heart muscle, reduced heart volume, mitral valve prolapse, myocardial fibrosis, and pericardial effusion.\n\nAbnormalities in conduction and repolarization of the heart that can result from anorexia nervosa include QT prolongation, increased QT dispersion, conduction delays, and junctional escape rhythms. Electrolyte abnormalities, particularly hypokalemia and hypomagnesemia, can cause anomalies in the electrical activity of the heart, and result in life-threatening arrhythmias. Hypokalemia most commonly results in anorexic patients when restricting is accompanied by purging (induced vomiting or laxative use). Hypotension (low blood pressure) is common, and symptoms include fatigue and weakness. Orthostatic hypotension, a marked decrease in blood pressure when standing from a supine position, may also occur. Symptoms include lightheadedness upon standing, weakness, and cognitive impairment, and may result in fainting or near-fainting. Orthostasis in anorexia nervosa indicates worsening cardiac function and may indicate a need for hospitalization. Hypotension and orthostasis generally resolve upon recovery to a normal weight. The weight loss in anorexia nervosa also causes atrophy of cardiac muscle. This leads to decreased ability to pump blood, a reduction in the ability to sustain exercise, a diminished ability to increase blood pressure in response to exercise, and a subjective feeling of fatigue.\n\nSome individuals may also have a decrease in cardiac contractility. Cardiac complications can be life-threatening, but the heart muscle generally improves with weight gain, and the heart normalizes in size over weeks to months, with recovery. Atrophy of the heart muscle is a marker of the severity of the disease, and while it is reversible with treatment and refeeding, it is possible that it may cause permanent, microscopic changes to the heart muscle that increase the risk of sudden cardiac death. Individuals with anorexia nervosa may experience chest pain or palpitations; these can be a result of mitral valve prolapse. Mitral valve prolapse occurs because the size of the heart muscle decreases while the tissue of the mitral valve remains the same size. Studies have shown rates of mitral valve prolapse of around 20 percent in those with anorexia nervosa, while the rate in the general population is estimated at 2–4 percent. It has been suggested that there is an association between mitral valve prolapse and sudden cardiac death, but it has not been proven to be causative, either in patients with anorexia nervosa or in the general population.\n\nRelapse occurs in approximately a third of people in hospital, and is greatest in the first six to eighteen months after release from an institution.\n\nAnorexia is estimated to occur in 0.9% to 4.3% of women and 0.2% to 0.3% of men in Western countries at some point in their life. About 0.4% of young females are affected in a given year and it is estimated to occur three to ten times less commonly in males. Rates in most of the developing world are unclear. Often it begins during the teen years or young adulthood.\n\nThe lifetime rate of atypical anorexia nervosa, a form of ED-NOS in which not all of the diagnostic criteria for AN are met, is much higher, at 5–12%.\n\nWhile anorexia became more commonly diagnosed during the 20th century it is unclear if this was due to an increase in its frequency or simply better diagnosis. Most studies show that since at least 1970 the incidence of AN in adult women is fairly constant, while there is some indication that the incidence may have been increasing for girls aged between 14 and 20. According to researcher Ben Radford who wrote in \"Skeptical Inquirer\" \"I found many examples of flawed, misleading, and sometimes completely wrong information and data being copied and widely disseminated among eating disorder organizations and educators without anyone bothering to consult the original research to verify its accuracy\". Radford states that misleading statistics and data have been ignored by organizations like the National Eating Disorder Association who has not released data for \"incidence of anorexia from 1984–2017\" he states that each agency continues to report incorrect numbers assuming that someone else has checked the accuracy.\n\nEating disorders are less reported in preindustrial, non-westernized countries than in Western countries. In Africa, not including South Africa, the only data presenting information about eating disorders occurs in case reports and isolated studies, not studies investigating prevalence. Data shows in research that in westernized civilizations, ethnic minorities have very similar rates of eating disorders, contrary to the belief that eating disorders predominantly occur in Caucasian people.\n\nMen (and women) who might otherwise be diagnosed with anorexia may not meet the DSM IV criteria for BMI since they have muscle weight, but have very little fat. Men and women athletes are often overlooked as anorexic. Research emphasizes the importance to take athletes' diet, weight and symptoms into account when diagnosing anorexia, instead of just looking at weight and BMI. For athletes, ritualized activities such as weigh-ins place emphasis on weight, which may promote the development of eating disorders among them. While women use diet pills, which is an indicator of unhealthy behavior and an eating disorder, men use steroids, which contextualizes the beauty ideals for genders. In a Canadian study, 4% of boys in grade nine used anabolic steroids. Anorexic men are sometimes referred to as \"manorexic\".\n\nThe term \"anorexia nervosa\" was coined in 1873 by Sir William Gull, one of Queen Victoria's personal physicians. The history of anorexia nervosa begins with descriptions of religious fasting dating from the Hellenistic era and continuing into the medieval period. The medieval practice of self-starvation by women, including some young women, in the name of religious piety and purity also concerns anorexia nervosa; it is sometimes referred to as \"anorexia mirabilis.\"\n\nThe earliest medical descriptions of anorexic illnesses are generally credited to English physician Richard Morton in 1689. Case descriptions fitting anorexic illnesses continued throughout the 17th, 18th and 19th centuries.\n\nIn the late 19th century anorexia nervosa became widely accepted by the medical profession as a recognized condition. In 1873, Sir William Gull, one of Queen Victoria's personal physicians, published a seminal paper which coined the term \"anorexia nervosa\" and provided a number of detailed case descriptions and treatments. In the same year, French physician Ernest-Charles Lasègue similarly published details of a number of cases in a paper entitled \"De l'Anorexie hystérique\".\n\nAwareness of the condition was largely limited to the medical profession until the latter part of the 20th century, when German-American psychoanalyst Hilde Bruch published \"The Golden Cage: the Enigma of Anorexia Nervosa\" in 1978. Despite major advances in neuroscience, Bruch's theories tend to dominate popular thinking. A further important event was the death of the popular singer and drummer Karen Carpenter in 1983, which prompted widespread ongoing media coverage of eating disorders.\n\nThe term is of Greek origin: \"an-\" (ἀν-, prefix denoting negation) and \"orexis\" (ὄρεξις, \"appetite\"), translating literally to a nervous loss of appetite.\n\n\n"}
{"id": "489094", "url": "https://en.wikipedia.org/wiki?curid=489094", "title": "Area studies", "text": "Area studies\n\nArea studies (also regional studies) are interdisciplinary fields of research and scholarship pertaining to particular geographical, national/federal, or cultural regions. The term exists primarily as a general description for what are, in the practice of scholarship, many heterogeneous fields of research, encompassing both the social sciences and the humanities. Typical area study programs involve history, political science, sociology, cultural studies, languages, geography, literature, and related disciplines. In contrast to cultural studies, area studies often include diaspora and emigration from the area.\n\nInterdisciplinary area studies became increasingly common in the United States of America and in Western scholarship after World War II. Before that war American universities had just a few faculty who taught or conducted research on the non-Western world. Foreign-area studies were virtually nonexistent. After the war, liberals and conservatives alike were concerned about the U.S. ability to respond effectively to perceived external threats from the Soviet Union and China in the context of the emerging Cold War, as well as to the fall-out from the Decolonization of Africa and Asia.\n\nIn this context, the Ford Foundation, the Rockefeller Foundation, and the Carnegie Corporation of New York convened a series of meetings producing a broad consensus that to address this knowledge deficit, the U.S. must invest in international studies. Therefore, the foundations of the field are strongly rooted in America. Participants argued that a large brain trust of internationally oriented political scientists and economists was an urgent national priority. There was a central tension, however, between those who felt strongly that, instead of applying Western models, social scientists should develop culturally and historically contextualized knowledge of various parts of the world by working closely with humanists, and those who thought social scientists should seek to develop overarching macrohistorial theories that could draw connections between patterns of change and development across different geographies. The former became area-studies advocates, the latter proponents of modernization theory.\n\nThe Ford Foundation would eventually become the dominant player in shaping the area-studies program in the United States.\n\nIn 1950 the foundation established the prestigious Foreign Area Fellowship Program (FAFP), the first large-scale national competition in support of area-studies training in the United States. From 1953 to 1966 it contributed $270 million to 34 universities for area and language studies. Also during this period, it poured millions of dollars into the committees run jointly by the Social Science Research Council and the American Council of Learned Societies for field-development workshops, conferences, and publication programs. Eventually, the SSRC-ACLS joint committees would take over the administration of FAFP.\n\nOther large and important programs followed Ford's. Most notably, the National Defense Education Act of 1957, renamed the Higher Education Act in 1965, allocated funding for some 125 university-based area-studies units known as National Resource Center programs at U.S. universities, as well as for Foreign Language and Area Studies fellowships for graduate students.\n\nMeanwhile, area studies were also developed in the Soviet Union.\n\nSince their inception, area studies have been subject to criticism—including by area specialists themselves. Many of them alleged that because area studies were connected to the Cold War agendas of the CIA, the FBI, and other intelligence and military agencies, participating in such programs was tantamount to serving as an agent of the state. Some argue that there is the notion that U.S concerns and research priorities will define the intellectual terrain of area studies. Others insisted, however, that once they were established on university campuses, area studies began to encompass a much broader and deeper intellectual agenda than the one foreseen by government agencies, thus not American centric.\n\nArguably, one of the greatest threats to the area studies project was the rise of rational choice theory in political science and economics. To mock one of the most outspoken rational choice theory critics, Japan scholar Chalmers Johnson asked: Why do you need to know Japanese or anything about Japan's history and culture if the methods of rational choice will explain why Japanese politicians and bureaucrats do the things they do?\n\nFollowing the demise of the Soviet Union, philanthropic foundations and scientific bureaucracies moved to attenuate their support for area studies, emphasizing instead interregional themes like \"development and democracy\". When the Social Science Research Council and the American Council of Learned Societies, which had long served as the national nexus for raising and administering funds for area studies, underwent their first major restructuring in thirty years, closing down their area committees, scholars interpreted this as a massive signal about the changing research environment.\n\nFields are defined differently from university to university, and from department to department, but common area-studies fields include:\nDue to an increasing interest in studying translocal, transregional, transnational and transcontinental phenomena, a Potsdam-based research network has recently coined the term \"TransArea Studies\" (POINTS – Potsdam International Network for TransArea Studies).\n\nOther interdisciplinary research fields such as women's studies (also known as gender studies), disability studies, and ethnic studies (including African American studies, Asian American studies, Latino/a studies, and Native American studies) are not part of area studies but are sometimes included in discussion along with it.\n\nSome entire institutions of higher education (tertiary education) are devoted solely to area studies such as School of Oriental and African Studies, part of the University of London, or the Tokyo University of Foreign Studies in Japan. At the University of Oxford, the School of Interdisciplinary Area Studies (SIAS)School of Interdiscplinary Area Studies, Oxford and St Antony's College specialise in area studies, and hosts a number of post-graduate teaching programmes and research centres covering various regions of the world.\nJawaharlal Nehru University, New Delhi, is the only institution with immense contribution towards popularising area studies in India.\nAn institution which exclusively deals with Area Studies is the GIGA (German Institute of Global Area Studies) in Germany. Additionally, Lund University in Sweden offers the largest Asian Studies masters program in Northern Europe and is dedicated to promoting studies related to South Asia through its SASNet.\n\n\n\n"}
{"id": "11839182", "url": "https://en.wikipedia.org/wiki?curid=11839182", "title": "Avoidant/restrictive food intake disorder", "text": "Avoidant/restrictive food intake disorder\n\nAvoidant/restrictive food intake disorder (ARFID), previously known as selective eating disorder (SED), is a type of eating disorder, where certain foods are limited based on appearance, smell, taste, texture, brand, presentation, or a past negative experience with the food.\n\nPeople with ARFID have an inability to eat certain foods. \"Safe\" foods may be limited to certain food types and even specific brands. In some cases, individuals with the condition will exclude whole food groups, such as fruits or vegetables. Sometimes excluded foods can be refused based on color. Some may only like very hot or very cold foods, very crunchy or hard-to-chew foods, or very soft foods, or avoid sauces.\n\nMost people with ARFID will still maintain a healthy or typical body weight. There are no specific outward appearances associated with ARFID. Sufferers can experience physical gastrointestinal reactions to adverse foods such as retching, vomiting or gagging. Some studies have identified symptoms of social avoidance due to their eating habits. Most, however, would change their eating habits if they could.\n\nThe determination of the cause of ARFID has been difficult due to the lack of diagnostic criteria and concrete definition. However, many have proposed other conditions that co-occur with ARFID.\n\nThere are different kinds of 'sub-categories' identified for ARFID:\n\nSymptoms of ARFID are usually found with symptoms of other disorders. Some form of feeding disorder is found in 80% of children that also have a developmental disability. Children often exhibit symptoms of obsessive-compulsive disorder and autism. Although many people with ARFID have symptoms of these disorders, they usually do not qualify for a full diagnosis. Strict behavior patterns and difficulty adjusting to new things are common symptoms in patients that are on the autistic spectrum. A study done by Schreck at Pennsylvania State University compared the eating habits of children with ASD and typically developing children. After analyzing their eating patterns, they suggested that the children with some degree of ASD have a higher degree of selective eating. These children were found to have similar patterns of selective eating and favored more energy dense foods such as nuts and whole grains. Eating a diet of energy dense foods could put these children at a greater risk for health problems such as obesity and other chronic diseases due to the high fat and low fiber content of energy dense foods. Due to the tie to ASD, children are less likely to outgrow their selective eating behaviors and most likely should meet with a clinician to address their eating issues.\n\nSpecific food avoidances could be caused by food phobias that cause great anxiety when a person is presented with new or feared foods. Most eating disorders are related to a fear of gaining weight. Those who have ARFID do not have this fear, but the psychological symptoms and anxiety created is similar. Some people with ARFID have fears such as emetophobia (fear of vomiting) or a fear of choking, but this is not common.\n\nDiagnosis is often based on a diagnostic checklist to test whether an individual is exhibiting certain behaviors and characteristics. Clinicians will look at the variety of foods an individual consumes, as well as the portion size of accepted foods. They will also question how long the avoidance or refusal of particular foods has lasted, and if there are any associated medical concerns, such as malnutrition. Unlike most eating disorders, there may be a higher rate of ARFID in young boys than there is in young girls.\n\nThe fifth edition of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5) renamed \"Feeding Disorder of Infancy or Early Childhood\" to Avoidant/Restrictive Food Intake Disorder, and broadened the diagnostic criteria. Previously defined as a disorder exclusive to children and adolescents, the DSM-5 broadened the disorder to include adults who limit their eating and are affected by related physiological or psychological problems, but who do not fall under the definition of another eating disorder.\n\nThe DSM-5 defines the following diagnostic criteria:\nIn previous years, the DSM was not inclusive in recognizing all of the challenges associated with feeding and eating disorders in 3 main domains:\n\nChildren are often picky eaters, this does not necessarily mean they meet the criteria for an ARFID diagnosis. In addition, self-identification as having ARFID may contribute to the condition.\n\nWith time the symptoms of ARFID can lessen and can eventually disappear without treatment. However, in some cases treatment will be needed as the symptoms persist into adulthood. The most common type of treatment for ARFID is some form of cognitive-behavioral therapy. Working with a clinician can help to change behaviors more quickly than symptoms may typically disappear without treatment. \n\nThere are support groups for adults with ARFID.\n\nChildren can benefit from a four stage in-home treatment program based on the principles of systematic desensitization. The four stages of the treatment are record, reward, relax and review.\n\n\n"}
{"id": "13838324", "url": "https://en.wikipedia.org/wiki?curid=13838324", "title": "Bennett scale", "text": "Bennett scale\n\nThe Bennett scale, also called the Developmental Model of Intercultural Sensitivity (DMIS), was developed by Milton Bennett. The framework describes the different ways in which people can react to cultural differences.\n\nOrganized into six “stages” of increasing sensitivity to difference, the DMIS identifies the underlying cognitive orientations individuals use to understand cultural difference. Each position along the continuum represents increasingly complex perceptual organizations of cultural difference, which in turn allow increasingly sophisticated experiences of other cultures. By identifying the underlying experience of cultural difference, predictions about behavior and attitudes can be made and education can be tailored to facilitate development along the continuum. The first three stages are ethnocentric as one sees his own culture as central to reality. Climbing the scale, one develops a more and more ethnorelative point of view, meaning that one experiences one's own culture as in the context of other cultures.\nBy the fourth stage, ethnocentric views are replaced by ethnorelative views.\n\n\nIn his theory, Bennett describes what changes occur when evolving through each step of the scale. Summarized, they are the following: \n\n"}
{"id": "30797312", "url": "https://en.wikipedia.org/wiki?curid=30797312", "title": "Bibliography of tourism", "text": "Bibliography of tourism\n\nThis is a bibliography of works related the subject of tourism.\n\nTourism is travel for recreational, leisure or business purposes. The World Tourism Organization defines tourists as people \"traveling to and staying in places outside their usual environment for not more than one consecutive year for leisure, business and other purposes\".\n\n\n\n\n\n"}
{"id": "4097096", "url": "https://en.wikipedia.org/wiki?curid=4097096", "title": "Captivity (animal)", "text": "Captivity (animal)\n\nAnimals that are held by humans and prevented from escaping are said to be in captivity. The term is usually applied to wild animals that are held in confinement, but may also be used generally to describe the keeping of domesticated animals such as livestock or pets. This may include, for example, animals in farms, private homes, zoos and laboratories. Animal captivity may be categorized according to the particular motives, objectives and conditions of the confinement.\n\nThroughout history not only domestic animals as pets and livestock were kept in captivity and under human care, but also wild animals. Some were failed domestication attempts. Also, in past times, primarily the wealthy, aristocrats and kings collected wild animals for various reasons. Contrary to domestication, the ferociousness and natural behaviour of the wild animals were preserved and exhibited. Today's zoos claim other reasons for keeping animals under human care: conservation, education and science.\n\nCaptive animals, especially those not domesticated, sometimes develop abnormal behaviours.\n\nOne type of abnormal behaviour is \"stereotypical behaviors\", i.e. repetitive and apparently purposeless motor behaviors. Examples of stereotypical behaviours include pacing, self-injury, route tracing and excessive self-grooming. These behaviors are associated with stress and lack of stimulation. Many who keep animals in captivity attempt to prevent or decrease stereotypical behavior by introducing stimuli, a process known as environmental enrichment.\n\nA type of abnormal behavior shown in captive animals is self-injurious behavior (SIB). Self-injurious behavior indicates any activity that involves biting, scratching, hitting, hair plucking, or eye poke that may result in injuring oneself. Although its reported incidence is low, self-injurious behavior is observed across a range of primate species, especially when they experience social isolation in infancy. Self-bite involves biting one’s own body—typically the arms, legs, shoulders, or genitals. Threat bite involves biting one’s own body—typically the hand, wrist, or forearm—while staring at the observer, conspecific, or mirror in a threatening manner. Self-hit involves striking oneself on any part of the body. Eye poking is a behavior (widely observed in primates) that presses the knuckle or finger into the orbital space above the eye socket. Hair plucking is a jerking motion applied to one’s own hair with hands or teeth, resulting in its excessive removal.\n\nThe proximal causes of self-injurious behavior have been widely studied in captive primates; either social or nonsocial factors can trigger this type of behavior. Social factors include changes in group composition, stress, separation from the group, approaches by or aggression from members of other groups, conspecific male individuals nearby, separation from females, and removal from the group. Social isolation, particularly disruptions of early mother-rearing experiences, is an important risk factor. Studies have suggested that, although mother-reared rhesus macaques still exhibit some self-injurious behaviors, nursery-reared rhesus macaques are much more likely to self-abuse than mother-reared ones. Nonsocial factors include the presence of a small cut, a wound or irritant, cold weather, human contact, and frequent zoo visitors. For example, a study has shown that zoo visitor density positively correlates with the number of gorillas banging on the barrier, and that low zoo visitor density caused gorillas to behave in a more relaxed way. Captive animals often cannot escape the attention and disruption caused by the general public, and the stress resulting from this lack of environmental control may lead to an increased rate of self-injurious behaviors.\n\nOn top of self inflicted harm, some animals exhibit harm towards others and internal psychological harm. This can be exhibited in various forma, such as Orca whales, which never have killed a human in the wild, killing two of its own trainers. Psychological tics can also be identified, ranging from swaying to head bobbing to pacing. Continuous inbreeding is also bringing out mental disadvantages, such as crossed eyes and infertility.\n\nStudies suggest that many abnormal captive behaviors, including self-injurious behavior, can be successfully treated by pair housing. Pair housing provides a previously single-housed animal with a same-sex social partner; this method is especially effective with primates, which are widely known to be social animals. Social companionship provided by pair housing encourages social interaction, thus reducing abnormal and anxiety-related behavior in captive animals as well as increasing their locomotion.\n\nAnimal husbandry\nPet keeping\n\nAnimal welfare \n\nWild animal keeping\n\n"}
{"id": "2390192", "url": "https://en.wikipedia.org/wiki?curid=2390192", "title": "Childhood secret club", "text": "Childhood secret club\n\nA childhood secret club is an informal organization created by children.\n\nSome common features of a childhood secret club may include:\n\nNames. Unlike cliques, these associations often have names. These younger groups don't have the social competition that adolescent cliques do, or the level of bad behaviour that street gangs have. However, in rare instances there may be rivalry with other such groups around the same age, which can lead to physical fights between them.\n\nPeer leadership. Unlike group activities like Scouting, which are led by adults, these groups are led by children.\n\nPro forma secrecy. The secrecy may be more imaginary than real. For instance the name of the club (if it has one) and its membership are usually obvious to all. There may be a desire to create secret codes and plans, but they are rarely implemented. A ramshackle den, tree house, clubhouse, fort, or \"secret base\" may be built in nearby scrub-land or an abandoned building. Some children in a secret club may use a part of the grounds of the school they attend together as their \"base\" during periods of recess.\n\nSingle-sex membership. Such clubs are usually either all boys or all girls but not mixed but exceptions do occur. There may be a sense of competition between the genders, as well as independence from adult authority.\n\nCatch-phrase/greetings/secret words. Sometimes the clubs develop one or more secret catch-phrases or (sometimes nonsensical words) and greetings. This is illustrated in the 1982 film \"P'tang, Yang, Kipperbang\".\n\nMany schools have rules against secret clubs, and some jurisdictions even have laws prohibiting secret or invitation-only societies in public elementary or secondary schools.\n\nThe fact that interest in these clubs tends to be a passing phase at a certain age may result from the stages of children's cognitive development. After growing out of the \"egocentric\", or \"preoperational\" stage, reaching the \"age of reason\", one is able to understand other people's intentions. The next step is the ability to understand groups and the desire to belong to a club.\n\nWritten material about these secret clubs, such as the external links listed below, cites eight- and nine-year-olds most often. While slightly older children may also participate in secret clubs, they would be less expected to use fantasy elements in their activities.\n\nJuvenile comics and literature often feature such clubs as a plot device, often with spy or detective themes, and often far more organized than their real-life counterparts. A more realistic depiction is found in Zilpha Keatley Snyder's novel \"The Egypt Game\", in which six children gather in an abandoned storage yard to enact fantasy adventures and recreate actual ceremonies based on and adapted from their knowledge of ancient Egypt.\n\nMovies have featured such clubs, notably the early Ealing comedy-thriller \"Hue & Cry\". There are also juvenile non-fiction books that serve as \"how-to's\" for code-making and surveillance, most notably the Usborne \"Good Spy Guide\" series.\n\n\n"}
{"id": "590768", "url": "https://en.wikipedia.org/wiki?curid=590768", "title": "Classic", "text": "Classic\n\nA classic is an outstanding example of a particular style; something of lasting worth or with a timeless quality; of the first or highest quality, class, or rank – something that exemplifies its class. The word can be an adjective (a \"classic\" car) or a noun (a \"classic\" of English literature). It denotes a particular quality in art, architecture, literature, design, technology, or other cultural artifacts. In commerce, products are named 'classic' to denote a long-standing popular version or model, to distinguish it from a newer variety. \"Classic\" is used to describe many major, long-standing sporting events. Colloquially, an everyday occurrence (e.g. a joke or mishap) may be described in some dialects of English as 'an absolute classic'.\n\n\"Classic\" should not be confused with \"classical\", which refers specifically to certain cultural styles, especially in music and architecture: styles generally taking inspiration from the Classical tradition, hence classicism.\n\nThe classics are the literature of ancient Greece and Rome, known as classical antiquity, and once the principal subject studied in the humanities. Classics (without the definite article) can refer to the study of philosophy, literature, history and the arts of the ancient world, as in \"reading classics at Cambridge\". From that usage came the more general concept of 'classic'.\n\nThe Chinese classics occupy a similar position in Chinese culture, and various other cultures have their own classics.\n\nBooks, films and music particularly may become \"a classic\" but a painting would more likely be called a masterpiece. A classic is often something old that is still popular.\n\nThe first known use of \"classic\" in this sense — a work so excellent that it is on the level of the \"classics\" (Greek and Latin authors) — is by the 18th-century scholar Rev. John Bowle. He applied the term to \"Don Quixote\", of which Bowle prepared an innovative edition, such as he judged that a classic work needed.\n\nSome other examples would be the book \"The Adventures of Tom Sawyer\" by Mark Twain, the 1941 film \"Citizen Kane\", and the song Heartbreak Hotel by Elvis Presley. Lists of classics are long and wide-ranging, and would vary depending on personal opinion. Classic rock is a popular radio format, playing a repertoire of old but familiar recordings.\n\nA contemporary work may be hailed as an \"instant classic\", but the criteria for classic status tends to include the test of time. The term \"classic\" is in fact often generalized to refer to any work of a certain age, regardless of whether it is any good. A cult classic may be well known but is only favored by a minority.\n\nA well known and reliable procedure, such as a demonstration of well-established scientific principle, may be described as classic: e.g. the cartesian diver experiment.\n\nManufacturers frequently describe their products as classic, to distinguish the original from a new variety, or to imply qualities in the product – although the Ford Consul Classic, a car manufactured 1961–1963, has the \"classic\" tag for no apparent reason. The iPod classic was simply called the iPod until the sixth generation, when \"classic\" was added to the name because other designs were also available – an example of a retronym. \"Coca-Cola Classic\" is the name used for the relaunch of Coca-Cola after the failure of the New Coke recipe change. Similarly, the Classic (transit bus), a transit bus manufactured from 1982–97, succeeded an unpopular futuristic design.\n\nA classic can be something old that remains prized or valuable (but not an antique). Classic cars, for example, are recognised by various collectors' organisations such as the Classic Car Club of America, who regulate the qualifying attributes that constitute classic status.\n\nMany sporting events take the name \"classic\":\n\n\nIn Spanish-speaking countries, the term \"Clásico\" refers to a match between two football teams known as traditional rivals, e.g. El Clásico in Spain.\n\n"}
{"id": "4797772", "url": "https://en.wikipedia.org/wiki?curid=4797772", "title": "Creole Renaissance", "text": "Creole Renaissance\n\nThe Creole Renaissance is a movement which established Creole as legitimate literary language, started in large part by authors like Felix Morisseau-Leroy, who struggled successfully to make Haitian Creole the literary, educational, and official language of Haiti. This grew, in part, out of the Negritude and Haitian Indigenism movements and the Harlem Renaissance.\n\n"}
{"id": "55008943", "url": "https://en.wikipedia.org/wiki?curid=55008943", "title": "Critical university studies", "text": "Critical university studies\n\nCritical university studies (CUS) is a new field examining the role of higher education in contemporary society and its relation to culture, politics, and labor. Arising primarily from Cultural Studies, it takes a critical stance toward changes to the university since the 1970s, particularly the shift away from a strong public model of higher education to a neoliberal privatized model. Emerging largely in the United States, which has the most extensive system of higher education, the field has also seen significant work in the United Kingdom, as well as in other countries confronting neoliberalism. Key themes of CUS research are corporatization, academic labor, and student debt, among other issues.\n\nLike those doing research under the banner of Critical Legal Studies (CLS), scholars of Critical University Studies often have an activist bent. CLS and CUS both analyze powerful institutions in order to draw attention to structural inequalities and embedded practices of exploitation and marginalization. In addition, both fields seek to move beyond abstract theorizing, targeting institutional practices and making proposals for policy changes.\n\nIn contrast to CLS, which has roots in elite institutions like Harvard and Yale, CUS largely comes out of public colleges and universities. While CLS has tended to seek remedies in the legal system, CUS has gravitated toward student and labor union movements. Moreover, CUS has emphasized investigative reportage and exposés of current institutional policies and practices alongside academic work. Rather than a uniform group, CUS includes a range of scholars, critics, and activists, among them tenured professors, graduate students, and adjunct instructors.\n\nThe term “Critical University Studies” was first defined in print by Jeffrey J. Williams in a 2012 article in \"The Chronicle of Higher Education\". The piece, “An Emerging Field Deconstructs Academe,” describes the “new wave of criticism of higher education” that came to the fore in the 1990s and has gained momentum in the ensuing decades. This new work has primarily come from literary and cultural critics, as well as those in education, history, sociology, and labor studies. As Williams notes, criticism of higher education has a strong tradition, and scholars like Heather Steffen have traced CUS's lineage at least to the early 20th century, for example to Thorstein Veblen’s \"The Higher Learning in America: A Memorandum on the Conduct of Universities by Business Men\" (1918) and Upton Sinclair’s \"The Goose-Step: A Study of American Education\" (1923), which criticize the influence of business principles and Gilded Age wealth on the emerging university system.\n\nIn addition, the 1960s saw a great deal of criticism of social institutions, and much focused on university campuses. Students for a Democratic Society (SDS) started with a strong statement about higher education, and anti-war and civil rights protests were a major presence on US campuses. The Critical Pedagogy movement, inspired by Paolo Freire and furthered by Henry Giroux and others, arose from this moment. The feminist movement also played a role in criticism of the university during the ‘60s and ‘70s, with activists like Adrienne Rich calling for “a women-centered university.”\n\nIn 1980, the Bayh-Dole Act granted universities the right to patent their inventions, thereby encouraging them to conduct research with business aims in mind. In the late 1990s and early 2000s, critics began to address the new direction of higher education, often coming from the graduate student unionization movement. Some met in conferences such as “Reworking/Rethinking the University” at the University of Minnesota (2008–11), or came out of groups such as Edu-factory, which was inspired by the Italian autonomist movement.\n\nThis first wave of CUS publications addressed the corporatization of higher education, along with the exploitation of academic labor and the rise of student debt. Key texts from this period include Sheila Slaughter and Larry Leslie's \"Academic Capitalism: Politics, Policies, and the Entrepreneurial University\" (1997), David Noble's \"Digital Diploma Mills: The Automation of Higher Education\" (2001), Marc Bousquet's “The Waste Product of Graduate Education” (2002) and \"How the University Works: Higher Education and the Low-Wage Nation\" (2008). In addition, \"Steal This University: The Rise of the Corporate University and the Academic Labor Movement\", edited by Benjamin Johnson et al. (2003), Stefano Harney and Fred Moten's “The University and the Undercommons\" (2004), Williams’ “The Post-Welfare State University” (2006) and “Student Debt and the Spirit of Indenture” (2008), and Christopher Newfield's \"Unmaking the Public University: The Forty-Year Assault on the Middle Class\" (2008).\n\nMore recently, a second wave of CUS scholars have widened the field's scope to address issues including universities’ reliance on proprietorial technology, the dominance of entrepreneurial values, and globalization. Notable texts in this vein include Newfield's \"The Great Mistake: How We Wrecked Public Universities and How We Can Fix Them\" (2016), Benjamin Ginsberg's \"The Fall of the Faculty: The Rise of the All-Administrative University and Why It Matters\" (2011), Robert Samuels’ \"Why Public Higher Education Should Be Free: How to Decrease Cost and Increase Quality at American Universities\" (2013). In addition, Jacob Rooksby's \"The Branding of the American Mind\" (2016), Avery Wiscomb's “The Entrepreneurship Racket” (2016), and Heather Steffen's \"Inventing Our University: Student-Faculty Collaboration in Critical University Studies” (2017).\n\nIn addition, after Britain adopted neoliberal policies and raised tuitions from minor fees to major levels, critics such as Stefan Collini, in “Browne’s Gamble” (2010), and Andrew McGettigan, in \"The Great University Gamble: Money, Markets, and the Future of Higher Education\" (2013), focused on the United Kingdom. Meng-Hsuan Chou, Isaac Kamola, and Tamson Pietsch's edited volume \"The Transnational Politics of Higher Education\" (2016) looks at globalization.\n\nAlthough continually responding to new trends in higher education, Critical University Studies has so far concerned itself with several key themes:\n\n\nCritical University Studies research has contributed to student and faculty movements across US campuses, including the aforementioned New Faculty Majority, the graduate student union movement, Oregon's CORE faculty-student advocacy group, and the Occupy Student Debt Campaign (on which cultural critic Andrew Ross has worked).\n\nCUS scholars often publish outside of traditional academic outlets, through blogs like Michael Meranze and Christopher Newfield's Remaking the University, as well as through contributions to new media like Jacobin, the Los Angeles Review of Books (LARB), Salon and HuffPost. Through such non-traditional outlets, as well as through book publications and academic journal articles, CUS has helped to bring issues like student debt into the mainstream political conversation.\n\nAlongside CUS criticism and activism in the 1990s, U.S. campuses saw a rise in unionization efforts. This culminated in a 2000 decision by the National Labor Relations Board (NLRB) that graduate employees were protected by the National Labor Relations Act and could unionize. Union efforts proliferated following this decision, with organizations like the United Automobile Workers (UAW) and the Service Employees International Union (SEIU) allying with graduate students and adjunct instructors in the fight for employee status and collective bargaining rights. Despite setbacks, including a 2004 reversal of the 2000 ruling, student and adjunct unions have made significant headway. In August 2016, the NLRB reversed itself again, ruling that graduate research and teaching assistants at private universities do have the right to unionize (UAW vs. Columbia).\n\nThe journal \"Workplace: A Journal for Academic Labor\", co-founded by Bousquet, stemmed from early efforts in the field and continues to publish open-access issues around themes of academic labor and higher education activism. In addition, 2015 saw the formation of \"Academic Labor: Research and Artistry\", a journal devoted to issues of tenure and contingency in the university.\nIn book publishing, The Johns Hopkins University Press initiated a book series on Critical University Studies, edited by Jeffrey J. Williams and Christopher Newfield, and In the UK, Palgrave also launched a series on CUS, edited by John Smyth.\n\nOn the whole, the stance of CUS against the status quo of US universities puts it at odds with individuals and administrators who are in favor of the continued privatization, corporatization, and globalization of the university.\n\nScholars like Williams, Steffen, and others continue to call for the incorporation of CUS into the undergraduate curriculum, encouraging students to think critically about the institutions in which they find themselves.\n\n"}
{"id": "13775689", "url": "https://en.wikipedia.org/wiki?curid=13775689", "title": "Cultural emphasis", "text": "Cultural emphasis\n\nCultural emphasis is an important aspect of a culture which is often reflected though language and, more specifically, vocabulary (Ottenheimer, 2006, p. 266). This means that the vocabulary people use in a culture indicates what is important to that group of people. If there are a lot of words to describe a certain topic in a specific culture, then there is a good chance that that topic is considered important to that culture.\n\nThe idea of cultural emphasis is rooted form the work of Franz Boas, who is considered to be one of the founders of American Anthropology (Ottenheimer, 2006, p. 15). Franz Boas developed and taught concepts such as cultural relativism and the \"cultural unconscious\", which allowed anthropologists who studied under him, like Edward Sapir and Ruth Benedict, to further study and develop ideas on language and culture (Hart, 2005, p. 179).\n\nOne way in which cultural emphasis is exemplified is a populace talks about the weather. For example, in a place where it is cold and it snows a lot, a large collection of words to describe the snow would be expected.\n\nIn a place where it is hot, a cornucopia of associated terms would be expected.\n\nA concentration of related terms for similar phenomena suggests the importance in distinguishing between them. Furthermore, if you are not from the area, or that culture, you might not have experienced or know the difference between, for example, a dry heat or a humid heat, when the difference may have huge implications for the outcome of a particular action.\n\n\n"}
{"id": "35343748", "url": "https://en.wikipedia.org/wiki?curid=35343748", "title": "Cultural group selection", "text": "Cultural group selection\n\nCultural group selection is an explanatory model within cultural evolution of how cultural traits evolve according to the competitive advantage they bestow upon a group. This multidisciplinary approach to the question of human culture engages research from the fields of anthropology, behavioural economics, evolutionary biology, evolutionary game theory, sociology, and psychology.\n\nWhile cultural norms are often beneficial to the individuals who hold them, they need not be. Norms can spread by cultural group selection when they are practiced within successful groups, and norms are more likely to spread from groups that are successful. But, for cultural group selection to occur, there must exist, between groups, cultural differences that when transmitted across time affect the persistence or proliferation of the groups. Cultural norms that provide these advantages will, in turn, lead to the displacement, absorption or even extinction of other, less successful cultural groups. However, game theoretic models suggest that if individuals are able to migrate between groups (which is common in small-scale societies), differences between groups should be difficult to maintain. Research in psychology reveals that humans have a particular set of traits, which include imitation, conformity, and in-group bias, that are capable of supporting the maintenance of these group differences over extended periods of time.\n\nCultural group selection gives a compelling explanation for how large-scale complex societies have formed. While altruistic behaviour such as kin selection and reciprocity can explain the behaviour of small social groups common in many species, it is unable to explain the large complex societies of unrelated, anonymous individuals that we see in the human species. However, one of the major distinctions between humans and other species is our reliance on social learning in acquiring behaviours. These instincts allow for the acquisition and persistence of culture. Through cultural group selection, culturally specific cooperative behaviour can evolve to support large societies. For example, in a study that spanned a variety of cultures, testing behaviour in Ultimatum, Dictator, and Third-party punishment games, it was found that standards of fairness and inclination to punish were correlated with both participation in world religions and market integration. This indicates how many of the behaviours necessary for complex societies are the result of cultural exposure rather than any evolution of our psychology.\n\nFor cultural knowledge and behaviour to persist across multiple generations, humans need to have the capacity to acquire, retain, and transmit cultural information. While many species engage in social learning, humans consistently rely upon it for behavioural cues and information about the environment. In a study comparing human children and young chimpanzees, it was shown that, when given a demonstration on how to retrieve a reward from a box, chimps copy relevant behaviour, while ignoring irrelevant behaviour, to solve the task. Meanwhile, human children will faithfully imitate both relevant and irrelevant behaviour to solve the same task. While this may seem like a negative quality, it is what allows for reliable, high-fidelity transmission of cultural information, and produces stable behavioural equilibria within cultural groups.\n\nMichael Tomasello suggests the following three adaptations are necessary for human culture:\n\nAt around 9–12 months infants begin engaging in joint attention. This involves following the gaze of an adult or using them as social reference points. Put simply, they become aware of the adult's attention and behaviour towards objects in the environment. In this sense, the child is beginning to understand people as goal-oriented intentional agents. This is vitally important for learning through imitation and, eventually, language acquisition.\n\nBy about 1 year of age, children begin to learn by imitation. At this point, children are capable of discriminating intentional actions from unintentional ones, and will attempt to accurately copy those intentional actions to accomplish tasks they've seen adults do. Because of imitative learning, children will copy those intentional acts which have no perceivable effect on the outcome, as well as strange or unnatural actions when easier methods are available. For example, an Andrew Meltzoff study found that 14-month-old children will, after seeing an adult do it, bend at the waist and press a panel with their head to turn on a light, instead of using their hands. According to Tomasello, imitative learning is necessary for learning the symbolic conventions of language.\n\nThrough imitatively learning, the child comprehends that linguistic symbols are intended to focus attention to some specific aspect of the shared experience. In doing this, the child must be able to take the perspective of the speaker. Due to the intersubjectivity of linguistic symbols, language allows one to communicate various perspectives and shift attention to one aspect of the world over another. In learning a language, a child is inheriting a vast set of linguistic symbols that have been passed down many generations. What is inherited then is the methods of shifting attention and perspective that were historically of importance to the people of that culture.\n\nWithout between-group variation, cultural group selection could not occur, as there would be no group differentiation to select for. While processes such as cultural drift, epidemics, and natural disasters increase between-group variation, migration and genetic mixing decrease between-group variation \"and\" increase within-group variation. Variation is only maintained when cultural groups have mechanisms that prevent the norms of outside groups from invading the cultural group. These ‘mechanisms’ are those uniquely human psychological traits and behaviours that encourage imitation, conformity, and in-group biases.\n\nAccording to Joseph Henrich, between-group variation is maintained by the following four mechanisms:\n\nConformist transmission refers to the psychological bias to preferentially imitate high frequency behaviors in the cultural group. This homogenizes the social group and reinforces widely held cultural norms. This explains why individuals within a social group hold the same beliefs and why these beliefs persist over time. While individuals will rely on copying high frequency behaviors under various conditions, this reliance increases when an individual is exposed to ambiguous environmental or social information. Conformist transmission can maintain between-group variation by reducing within-group variation, but it also facilitates the rapid spread of novel ideas, which increases between-group variation. Taken together, reduced within-group variation and increased between-group variation lead to the cultural divergence between groups that is the driving force of cultural group selection.\n\nPrestige-biased transmission is the tendency the copy those members of the group that are more successful. Preferentially copying successful members of the group allows individuals to avoid costly trial-and-error learning by imitating the better-than-average skills of the more prestigious cultural models. Individual can determine the rank of potential models by how much deference they are shown by the rest of the group. Deference is shown to high-prestige individuals to gain the opportunity to copy their successful models. We can see evidence for this bias in how new technologies, or economic practices spread to different groups according to how quick \"opinion leaders\" adopt them.\n\nMeanwhile, self-similarity transmission is the tendency to copy those individuals who are similar in language, appearance, social standing and other behavioral and cultural traits. In the context of prestige-biased transmission, self-similarity means that individuals will preferentially imitate those high-prestige individuals who are similar to them. From the perspective of an imitator, this trait is adaptive. By only imitating those high-prestige individuals who are similar, the imitator avoids adopting traits or behaviors that are not compatible with his or her knowledge or social environment.\n\nThese two social biases act together in reducing within-group variation. Additionally, prestige-biased transmission increases between-group variation by contributing to the spread of novel ideas.\n\nNon-conformists threaten to increase within-group variation by introducing deviant behaviours to the group and must receive costly punishment to maintain a homogenous social group. As a consequence of being punished, non-conformists will be less successful than other members of the group. Prestige-biased transmission would suggest that non-conformist behaviors would, therefore, not spread through the population. Papers on the topic suggest that this kind of punishment is prevalent across many different societies.\n\nNormative conformity is the act of changing one’s visible behaviour, simply to appear to match the majority, and without actually internalizing the groups opinions. This differs from conformist transmission since normative conformity does not consider frequency of a behaviour as an indicator of worth. The Asch conformity experiments are a perfect example of how robust this effect is and its replication across many cultures shows that this behaviour is very common. Henrich suggests that normative conformity may have evolved to respond to the spread of punishing behaviour toward non-conformists. By appearing similar to the group, one can gain the advantages of in-group membership, while also avoiding punishment. A curious byproduct of normative conformity is that it can contribute to the conformity transmission of norms that the transmitter does not hold, because they were mistakenly attributed by the imitator.\n\nAs Donald T. Campbell says, for cultural group selection to occur, there must be cultural differences between groups which affect their persistence or proliferation. This means groups are selected \"for\" or \"against\" according to their respective \"gains\" or \"losses\" relative to other groups.\n\nJoseph Henrich describes the three mechanisms through which this process occurs:\n\nDemographic swamping occurs when one or more cultural groups reproduces individuals faster than other groups in the region because of stable, culturally transmitted ideas or practices. This is the slowest kind of cultural groups selection as it depends on natural selection of between-group cultural variation operating on a scale of millennia. It has been suggested that this is how early agriculturalist displaced hunter-gatherer societies.\n\nDirect intergroup competition is the process by which cultural groups compete with each other over resources by engaging in warfare and raiding. The cultural practices and behaviour that gives an advantage to one group over another will proliferate at the expense of those who cannot compete. There are many possible traits that could contribute to a group's success, such as technological development, social and political organization, economic development, nationalism, etc. According to Joseph Soltis, it would take 500–1000 years for group selection to happen this way.\n\nIn prestige-biased group selection, when individuals have opportunities to copy people from nearby groups, they will preferentially imitate the members of groups that are more cooperative than their own. Since cooperative groups have a higher average payoff than non-cooperative groups, members of cooperative groups will be considered more prestigious and worthy of imitation.\n\n"}
{"id": "7745490", "url": "https://en.wikipedia.org/wiki?curid=7745490", "title": "Cultural universal", "text": "Cultural universal\n\nA cultural universal (also called an anthropological universal or human universal), as discussed by Emile Durkheim, George Murdock, Claude Lévi-Strauss, Donald Brown and others, is an element, pattern, trait, or institution that is common to all human cultures worldwide. Taken together, the whole body of cultural universals is known as the human condition. Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Some anthropological and sociological theorists that take a cultural relativist perspective may deny the existence of cultural universals: the extent to which these universals are \"cultural\" in the narrow sense, or in fact biologically inherited behavior is an issue of \"nature versus nurture\".\n\nThe emergence of these universals dates to the Upper Paleolithic, with the first evidence of full behavioral modernity. \n\nIn his book \"Human Universals\" (1991), Donald Brown defines human universals as comprising \"those features of culture, society, language, behavior, and psyche for which there are no known exception\", providing a list of hundreds of items he suggests as universal. Among the cultural universals listed by Donald Brown are:\n\n\n\n\n\nThe observation of the same or similar behavior in different cultures does not prove that they are the results of a common underlying psychological mechanism. One possibility is that they may have been invented independently due to a common practical problem.\n\nSince any cultures that have been studied by anthropologists have had contact with at least the anthropologists that studied it, and anthropological research ethics slows the studies down so that other groups unbound by such ethics, often at least locally represented by people of the same skin color as the supposedly isolated tribe but significantly culturally globalized, reach the tribe before the anthropologists do, no truly uncontacted culture has ever been scientifically studied. This allows outside influence to be an explanation for cultural universals as well. This does not preclude multiple independent inventions of civilization and is therefore not the same thing as hyperdiffusionism, it merely means that cultural universals are not proof of innateness.\n\nAnthropology's application of one ordinary evidence standard for data that agrees with its theories of cultural universals and one extraordinary standard of evidence for data that disagree with it raises issues with population level publication bias. While it is possible with strong enough evidence for the validity of one case study that disagree to pass peer review, those that agree will statistically do so more often than those that disagree even if the level of evidence is the same. Since multiple field studies regarding the same culture are combined when statistical prevalences of behaviors are assessed, the result may cause appearances of behaviors having a supposedly universal statistical demographic even if it does not. Some anthropologists suggest that this may give an appearance of statistics such as more men than women hunting large prey being universal without actually being universal, for example that field studies in which more men than women hunted passed peer review while field studies for the same culture in which the genders were equal or inverted for hunting were not published. \n\n\n"}
{"id": "43051637", "url": "https://en.wikipedia.org/wiki?curid=43051637", "title": "Czech Vašek", "text": "Czech Vašek\n\nCzech Vašek (literal. \"Czech Václav\") is a historical figure representing the national character of the Czech people used in time of national competition with German nationalism at the end of the 19th century.\n\nSuch figures differ from those that serve as personifications of the nation itself, as Čechie did the Czech nation and Marianne the French. He is usually depicted in a folk costume combining hat from Pilsen region with clothes from different regions.\n\nThey were used as a negative caricature of Czechs by Germans, symbolising them as street musicians. But also positively by Czech themselves.\n\nCzech Vašek is considered a counterpart to Deutscher Michel, a figure representing the national character of the German people.\n\n"}
{"id": "27798585", "url": "https://en.wikipedia.org/wiki?curid=27798585", "title": "Diedrich Diederichsen", "text": "Diedrich Diederichsen\n\nDiedrich Diederichsen (born August 15, 1957) is a German author, music journalist and cultural critic. He is one of Germany′s most renowned intellectual writers at the crossroads of the arts, politics, and pop culture.\n\nDiedrich Diederichsen was born and grew up in Hamburg where he worked as a music journalist and editor of the German \"Sounds\" magazine in the heyday of punk and new wave from 1979 to 1983. Until the 1990s he was then the editor-in-chief of the influential subculture magazine \"Spex\" in Cologne. Diederichsen worked as visiting professor in Frankfurt am Main, Stuttgart, Pasadena, Offenbach am Main, Gießen, Weimar, Bremen, Vienna, St. Louis, Cologne, Los Angeles and Gainesville. After teaching at the Merz Academy in Stuttgart for several years, he became Professor for Theory, Practice and Communication of Contemporary Art at the Academy of Fine Arts Vienna in 2006.\n\nDiedrich Diederichsen is a prolific writer whose articles and texts are printed in a variety of periodicals and publications. Newspapers and magazines with contributions by Diedrich Diederichsen include \"Texte zur Kunst\", \"Die Zeit\", \"die tageszeitung\", \"Der Tagesspiegel\", \"Süddeutsche Zeitung\", \"Theater heute\", \"Artscribe\", \"Artforum\", and \"Frieze\".\n\nWith his thinking and his attitude Diedrich Diederichsen reflects a multitude of influences. His writing shows traces of Post-structuralism, Marxism, Cultural studies, New Journalism, Beat literature, Psychoanalysis, and Situationism. A main topic of his writing is the tension between subjectivity, identity politics, and culture industry in Post-Fordist society. In his writings he frequently refers to personal experiences and the links between zeitgeist, biography, and history.\n\n\n"}
{"id": "47192473", "url": "https://en.wikipedia.org/wiki?curid=47192473", "title": "Digital heritage", "text": "Digital heritage\n\nDigital heritage is the use of digital media in the service of preserving cultural or natural heritage.\n\nThe Charter on the Preservation of Digital Heritage of UNESCO defines digital heritage as embracing \"cultural, educational, scientific and administrative resources, as well as technical, legal, medical and other kinds of information created digitally, or converted into digital form from existing analogue resources\".\n\nA particular branch of digital heritage, known as \"virtual heritage\", is formed by the use of information technology with the aim of recreating the experience of existing cultural heritage, as in (approximations of) virtual reality.\n"}
{"id": "6638857", "url": "https://en.wikipedia.org/wiki?curid=6638857", "title": "Donkey rides", "text": "Donkey rides\n\nDonkey rides are a traditional feature of seaside resorts in the United Kingdom. Children are allowed to ride donkeys on a sandy beach for a fee in summer months while on holiday, normally led in groups at walking pace. Typically, the donkeys used to have their names on their harnesses so they could be identified by children and parents alike.\n\nDonkey rides have been available since 1886 in Weston Super Mare and since 1895 in Bridlington.\n\nThe tradition started in Victorian times, but is now much less popular. It is probable that the donkeys offered to ride on were originally working draught animals in the cockle industries around the coast.\n\n"}
{"id": "549109", "url": "https://en.wikipedia.org/wiki?curid=549109", "title": "Eighth Wonder of the World", "text": "Eighth Wonder of the World\n\nEighth Wonder of the World is an unofficial title sometimes given to new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World.\n\n\n\n\n\n\n"}
{"id": "9928", "url": "https://en.wikipedia.org/wiki?curid=9928", "title": "Ethnology", "text": "Ethnology\n\nEthnology (from the Greek \"ἔθνος\", \"ethnos\" meaning \"nation\") is the branch of anthropology that compares and analyzes the characteristics of different peoples and the relationships between them (cf. cultural, social, or sociocultural anthropology).\n\nCompared to ethnography, the study of single groups through direct contact with the culture, ethnology takes the research that ethnographers have compiled and then compares and contrasts different cultures.\nThe term \"ethnologia\" (\"ethnology\") is credited to Adam Franz Kollár (1718-1783) who used and defined it in his \"Historiae ivrisqve pvblici Regni Vngariae amoenitates\" published in Vienna in 1783. as: “the science of nations and peoples, or, that study of learned men in which they inquire into the origins, languages, customs, and institutions of various nations, and finally into the fatherland and ancient seats, in order to be able better to judge the nations and peoples in their own times.” \n\nKollár's interest in linguistic and cultural diversity was aroused by the situation in his native multi-ethnic and multilingual Kingdom of Hungary and his roots among its Slovaks, and by the shifts that began to emerge after the gradual retreat of the Ottoman Empire in the more distant Balkans.\n\nAmong the goals of ethnology have been the reconstruction of human history, and the formulation of cultural invariants, such as the incest taboo and culture change, and the formulation of generalizations about \"human nature\", a concept which has been criticized since the 19th century by various philosophers (Hegel, Marx, structuralism, etc.). In some parts of the world, ethnology has developed along independent paths of investigation and pedagogical doctrine, with \"cultural anthropology\" becoming dominant especially in the United States, and \"social anthropology\" in Great Britain. The distinction between the three terms is increasingly blurry. Ethnology has been considered an academic field since the late 18th century, especially in Europe and is sometimes conceived of as any comparative study of human groups.\nThe 15th-century exploration of America by European explorers had an important role in formulating new notions of the Occident (the Western world), such as the notion of the \"Other\". This term was used in conjunction with \"savages\", which was either seen as a brutal barbarian, or alternatively, as the \"noble savage\". Thus, civilization was opposed in a dualist manner to barbary, a classic opposition constitutive of the even more commonly shared ethnocentrism. The progress of ethnology, for example with Claude Lévi-Strauss's structural anthropology, led to the criticism of conceptions of a linear progress, or the pseudo-opposition between \"societies with histories\" and \"societies without histories\", judged too dependent on a limited view of history as constituted by accumulative growth.\n\nLévi-Strauss often referred to Montaigne's essay on cannibalism as an early example of ethnology. Lévi-Strauss aimed, through a structural method, at discovering universal invariants in human society, chief among which he believed to be the incest taboo. However, the claims of such cultural universalism have been criticized by various 19th- and 20th-century social thinkers, including Marx, Nietzsche, Foucault, Derrida, Althusser, and Deleuze.\n\nThe French school of ethnology was particularly significant for the development of the discipline, since the early 1950s. Important figures in this movement have included Lévi-Strauss, Paul Rivet, Marcel Griaule, Germaine Dieterlen, and Jean Rouch.\n\n\n\n\n\n"}
{"id": "44806362", "url": "https://en.wikipedia.org/wiki?curid=44806362", "title": "Frankfurt Christmas Market", "text": "Frankfurt Christmas Market\n\nThe Frankfurt Christmas Market (in German: \"Frankfurter Weinachtmarkt\") is an annual outdoor Christmas market in central Frankfurt, Hessen, Germany.\n\nThe Christmas market opens in late November and continues until just before Christmas (normally 22 December) during Advent. It occupies a large area in central Frankfurt, including Friedrich-Stoltze-Platz, Hauptwache (to the north), Mainkai, Paulsplatz, and Römerberg (to the south).\n\nThe market is one of the oldest such \"Christkindchesmarkets\" in Germany. Its origins date back to 1393.\n\nThere are a number of affiliated markets, including in Birmingham, England.\n\n"}
{"id": "6845279", "url": "https://en.wikipedia.org/wiki?curid=6845279", "title": "Generationism", "text": "Generationism\n\nGenerationism is the belief that a specific generation has inherent traits that make it inferior or superior to another generation. The term is usually applied to claims of superiority in the expressed values, valuations, lifestyles and general beliefs of one generation compared to those of another, where objectively verifiable criteria substantiating the claim of superiority in themselves are lacking. \n\nGenerationism is most commonly used as an accusation against the belief that the contemporary generation in itself is inherently superior to previous generations, for example through the common practice of pejoratively referring to ancient cultures as \"primitive\" – not to be confused with the positive label of primitivism – although an older generation's opposition to the values and lifestyles of a younger generation may also popularly be referred to as generationist.\n\nGenerationism as a recurring sociological phenomenon has been studied in detail by the Swedish philosophers Alexander Bard and Jan Söderqvist in their work \"The Global Empire\" (published in Swedish in 2003). Bard and Söderqvist regard generationism as a close but far less frequently analyzed relative of racism which they propose should be studied in more detail to strengthen the general argumentation for relativism and pragmatism in contemporary philosophy.\n\n"}
{"id": "11020549", "url": "https://en.wikipedia.org/wiki?curid=11020549", "title": "Germanische Altertumskunde Online", "text": "Germanische Altertumskunde Online\n\nGermanische Altertumskunde Online, formerly called Reallexikon der Germanischen Altertumskunde, is a German encyclopedia of the study of Germanic history and cultures, as well as the cultures that were in close contact with them.\n\nThe first edition of the \"Reallexikon der Germanischen Altertumskunde\" appeared in four volumes between 1911 and 1919, edited by Johannes Hoops. The second edition, under the auspices of the Göttingen Academy of Sciences and Humanities, was edited by Heinrich Beck (from vol 1, 1968/72), Heiko Steuer (from vol. 8, 1991/94), Rosemarie Müller (from 1992), and Dieter Geuenich (from vol. 13, 1999), and was published by Walter de Gruyter between 1969 and 2008.\n\nIn 2010, the most recent version was published, now renamed \"Germanische Altertumskunde Online\". Edited by Heinrich Beck, Sebastian Brather, Dieter Geuenich, Wilhelm Heizmann, Steffen Patzold, and Heiko Steuer, it is published online by De Gruyter, accessible via subscription.\n\n"}
{"id": "14669242", "url": "https://en.wikipedia.org/wiki?curid=14669242", "title": "Grisi siknis", "text": "Grisi siknis\n\nGrisi siknis (in Miskito language, from the English, means \"crazy sickness\") is a contagious, culture-bound syndrome that occurs predominantly among the Miskito People of eastern Central America and affects mainly young women. It is also known as \"grisi munaia\", \"Chipil siknis\" and \"Nil siknis\". More recently cases occurring amongst people of Spanish descent have also been reported.\n\nAccording to Dr. Phil Dennis of Texas Tech University, \"grisi siknis\" is typically characterized by longer periods of anxiety, nausea, dizziness, irrational anger and fear, interlaced with short periods of rapid frenzy, in which the victim \"lose[s] consciousness, believe[s] that devils beat them and have sexual relations with them\" and runs away.\n\nOften the outbreak of the syndrome is violent in nature, with victims grabbing weapons, attacking unseen enemies and/or hurting themselves.\n\nThe causes of grisi siknis are unknown, says the American Psychology Association, but the prevailing Western theory calls this syndrome a \"psychological disorder due to stress, upheaval and despair\".\n\nTraditional Miskito belief, says Dennis, holds that grisi siknis is the result of evil spirits or black sorcerers. While Western medicine typically has no effect on those afflicted with the disease, the remedies of Miskito herbalists or witch doctors are often successful in curing grisi siknis.\n\nSymptoms of grisi siknis vary, but there is a distinct set of central characteristics. Most of the victims are young girls from 15 to 18 years old. The attacks are prefaced by headaches, dizziness, anxiety, nausea, irrational anger and/or fear. During the attack, the \"victim loses consciousness\" and falls to the ground, subsequently running away, which running Dennis calls \"perhaps the most distinctive defining characteristic of grisi siknis behavior\". The victim may view other people as devils, feel no pain for bodily injuries and have absolute amnesia regarding their physical circumstances. Some grab machetes or broken bottles to wave off unseen assailants. Other victims are reported to have performed superhuman feats, vomited strange objects such as spiders, hair and coins and spoken in tongues. In some cases the semi-conscious victim will speak the names of the next to be infected, although it is not always accurate. It is still highly contagious. During attacks, victims report mental visions in which devils or evil spirits come for them, and have sex with them. These visions also include anything from horrifying nightmares to pleasant experiences, and many anthropologists claim these are sexual experiences. This is a disputed fact, as not all cases involve sexual encounters. Attacks occur anywhere from multiple times a day to rarely when one is infected with grisi siknis. A person typically remains with the disease for several months to a year without medical attention, although some cases have been documented to recur for much longer.\n\nJoseph Westermeyer, Head of Psychiatry at the University of Oklahoma, states that a culturally bound syndrome is defined as \"certain trance-like disturbances [that] occur with unusual frequencies in certain societies\". These syndromes, he says, actually occur in a wide variety of cultures separated by great distance that have similar symptoms including \"fear, anxiety, amnesia, aimless escape, psychophysiological symptoms, social withdrawal, behavioral deviance and nondirected violence\". However, these symptoms are not confined to culturally bound syndromes, as others, such as \"personality disorder, neurasthenia, crisis/judgment disorders, organic brain syndromes, drug-induced delirium, major depression, mania, schizophreniform and schizophrenia\" might constitute the true psychiatric diagnosis.\" Because of these cross-cultural symptoms, it is argued that these syndromes are not necessarily unique \". and that applying the term ‘culturally bound’ to them hampers science’s ability to explore them on the same level as other psychological problems. Outbreaks of these symptoms are sporadic and epidemic.\" Some culturally bound syndromes, in addition to grisi siknis, include:\n\nAlthough grisi siknis behaves like a virus, researchers have not been able to trace anything irregular in the blood samples of victims.\n\nAccording to the American Psychiatry Association, pibloktoq, \"frenzy\" witchcraft, chakore and amok, are all, like grisi siknis, classified as \"running\" syndromes, in that they contain \"sudden high-level activity, [a] trance-like state, potentially dangerous behavior in the form of running [and] exhaustion, sleep [and/or] amnesia\". It is generally applied to purposeless roving.\n\nIn addition, Dr. Richard Castillo, as quoted by Dr. C. George Boeree, believes that amok (with very similar symptoms to grisi siknis), pibloktoq, latah, \"falling out\", \"indisposition\", and the \"fits\" are all related to impulse control disorders, and thus are associated with trichotillomania, compulsive gambling, pyromania, and kleptomania in Western medicine.\n\nAccording to the American Psychiatric Association, a Western medical condition similar in many aspects to culturally bound syndromes, particularly the \"running\" syndromes, of which grisi siknis is part, is dissociative (or psychogenic) fugue. In any fugal state, a person appears normal, but has amnesia or identity forgetfulness. Dissociative fugue is distinguished by impulsive travel and amnesia, identity uncertainty, stress, and impediment to normal social function, all of which must not be influenced by substance intake. It is most often related to intense emotional stress and occurs randomly. However, some argue that \"running\" syndromes are really not dissociative fugue, and have no proper Western medical classification. Others contend that associating culturally bound symptoms with known ailments severely limits the discovery of new psychiatric disorders in folk culture. Edgardo Ruiz, PhD at the University of Pittsburgh argues that grisi siknis does not correlate with Western scientific cultural perspectives, and the cross-cultural translation of symptoms is an inaccurate device wherewith to understand the disease.\n\nMiskito tradition, according to Dennis, holds that grisi siknis is caused by possession by evil spirits. This belief stems from the combination of traditional Native American animism. and Miskito Christian idea of the devil. When epidemic outbreaks of the disease occur, the Miskito hold that it is the result of an imbalance with spirits, says Nicola Ross, a reporter for \"The Walrus\" magazine, which predicament they believe to be caused by a dilman or evil sorcerer.\n\nThere is no definitively known cause of grisi siknis, although there are some theories which attempt to explain its origin. Although it has not discovered organic cause, says Dennis, grisi siknis still \"follow[s] the classic model for contagious disease\". Dennis claims that grisi siknis is the source of the emotionally volatile Miskito culture, saying \"it is clear that grisi siknis is related to emotional upset, worry, fear and general anxiety\", while microorganisms, if involved, are intermediate. Dr. Ronald C. Simons, professor emeritus of psychiatry and anthropology at Michigan State University, as quoted by Nicola Ross in \"The Walrus\" magazine, upholds this argument, proposing that grisi siknis is caused by poverty and stress among the Miskito. Culturally bound syndromes, Simons says, are often strongly influenced by behavior and experience and have become a local way of expressing misfortune. Dr. Wolfgang Jilek, of the University of Columbia’s psychiatry department, also quoted by Ross in \"The Walrus\", calls culturally bound syndromes \"real\" despite a general lack of evidence for organic causes. They are primarily the result of trauma and stress, Jilek claims, that end in mental dissociation problems. Susan Kellogg, Associate Professor and Chair of the History Department at the University of Houston, says that grisi siknis is the result of the cultural \"physical and emotional stresses\" that Miskito women endure. Shlomo Ariel, Co-Director of the Integrative Psychotherapy Center in Ramat Gan, Israel, says that such disorders are the product of the culture, delineates acceptable coping mechanisms for dealing with external or internal changes. In a typical homeostatic function, Ariel says, \"emotional or behavioral disorders in the individual are defined as such by the culture\", which culture subsequently imposes treatment in order to restore equilibrium. Grisi siknis can be considered a ritualized behavior associated with the adolescent to adult transition among the Miskito, says Mark Jamieson, professor of social anthropology at the University of Manchester. Girls in Miskito culture, claims Jamieson, are faced with the culturally inconsistent task of attracting a husband sexually while remaining safe and pure to maintain societal status quo. The contradictory familial pressures to both protect and marry off the daughter adds to this. Thus, says Shlomo, \"the syndrome may be viewed as a safety valve\" to maintain equilibrium between these conflicting pressures. Miskito girls express transitional sexuality through the syndrome while maintaining social purity, with the culture holding the victims blameless for their actions while attacked by the disease.\n\nGrisi siknis is generally only cured by traditional Miskito healing methods, according to \"The Journal of the American Botanical Council\". In treating the ailment, the Miskito typically follow a hierarchy of remedies, turning first to home-based remedies, second to modern health facilities and finally to curandero or witch doctors, the latter particularly, if evil spirits are believed to be involved. These healers use an assortment of vapor baths, anointing, teas and potions, all of which are organically derived. According to Dennis, the Miskito healers use a variety of undisclosed steamed herbal remedies that are generally more successful than any Western medicine. However, the cures can be counteracted, Dennis says, by exposure to dead people, pregnant women and various meats.\n\nCases of grisi siknis were registered in Nicaragua in March 2009 in Puerto Cabezas and Siuna where many students of the National Institute of Technology and other schools, suffered attacks. Most of the victims were girls. The Miskito people argued that it was due to the action of black sorcerers to oblige the people to pay the expensive cures. Some scholars in the country conclude that it is due to the extreme poverty that the Miskito people endure and that was worsened by Hurricane Felix of September 2007.\n\n"}
{"id": "638740", "url": "https://en.wikipedia.org/wiki?curid=638740", "title": "Institute for Dialectology, Onomastics and Folklore Research in Umeå", "text": "Institute for Dialectology, Onomastics and Folklore Research in Umeå\n\nDAUM, the Institute for Dialectology, Onomastics and Folklore Research in Umeå (), is a Swedish governmental archive bureau which collects, preserves, works up and provides information about dialects, place names, folklore culture and local history. DAUM is part of the Swedish Institute for Language and Folklore.\n\n"}
{"id": "29908016", "url": "https://en.wikipedia.org/wiki?curid=29908016", "title": "Interbellum Generation", "text": "Interbellum Generation\n\nInterbellum Generation is a term (derived from the Latin \"inter\" \"between\" and \"bellum\" \"war\") that is sometimes used to denote persons born in the United States during the first decade of the 20th century, often expressed specifically as the years 1901 to 1913. The name comes from the fact that those born during this time were too young to have served in the military during World War I, and were generally too old to serve as enlisted personnel in World War II, although many of them could indeed be found in the armed forces in some capacity during the later conflict.\n\nMembers of this generation came of age either during the Roaring Twenties or the initial phase of the Great Depression, prior to the election of Franklin D. Roosevelt and the promulgation of the New Deal. This fact contributed to the core of this generation holding lifelong left-liberal views in politics, especially on economic issues, although a few prominent dissenters (such as Barry Goldwater) do stand out. Most of their children belong to the Silent Generation.\n\nThe four Presidents of the United States of the Interbellum Generation were Lyndon B. Johnson (born in 1908), Ronald Reagan (born in 1911) and Richard M. Nixon and Gerald Ford (both born in 1913). (However, Reagan, Nixon and Ford served in World War II with members of the Greatest Generation.)\n\nLiving members of this generation include Richard Arvine Overton, the oldest living World War II veteran. For deceased celebrities born in this era, see G.I. Generation.\n"}
{"id": "7160538", "url": "https://en.wikipedia.org/wiki?curid=7160538", "title": "Interdiscourse", "text": "Interdiscourse\n\nInterdiscourse is the implicit or explicit relations that a discourse has to other discourses. Interdiscursivity is the aspect of a discourse that relates it to other discourses. Norman Fairclough prefers the concept \"orders of discourse\". Interdiscursivity is often mostly an analytic concept, e.g. in Foucault and Fairclough. Interdiscursivity has close affinity to recontextualisation because interdiscourse often implies that elements are imported from another discourse.\n\nThe meaning of interdiscourse varies. It denotes at least three levels:\n\n\nAn example (where 1. corresponds to a., etc.) illustrates the three levels: A minister of environment speaks in the parliament about a proposal.\n\n\nThe example illustrates that 2. and 3. are specific cases of 1, in the sense a-c all relate to another discourse. To avoid this, level 1. might be defined as relations to other discourses \"within\" the same discursive formation and type of discourse. Consequently, the definition of the levels depends on the definition of discursive formation and types of discourse, and the three levels may collapse to the extent that these concepts are not conceived. In short, the stratification of interdiscourse depends on the stratification of discourse.\n\nLevel 2. and 3. may be conceived as particularly salient. This is explained in Marc Angenot and Bruce by reference to Bakhtin: In Bakhtin's dialogism, the utterance is the natural meaningful and finalised unit of speech, which others are supposed to respond to, that is, others \"interpret the utterance by situating it in a discursive context\". But, an utterance may be interpreted (contextualised) in various ways, and interdiscourse and interdiscursivity denote how certain such interpretations (and relations to other discourses) are socially more privileged than others. Since interdiscourse privileges certain interpretations, it has a close affinity to the concepts of ideology, hegemony and power (sociology). For Bakhtin/Voloshinov, signs are a reality that refracts another reality, that is, signs are ideological. Therefore, the embedding of a discourse in an interdiscourse is an ideological interpretation of the discourse.\n\nIn Michel Foucault, interdiscursivity is not bound to ideology, but is a more open concept for analysing relations between discursive formations, that is, mapping their interdiscursive configuration. Such analyses constitute a part of his discourse analysis.\n\nThe cogent and restrictive character of the interdiscourse is reflected in the concept of the \"primacy of interdiscourse\". The interdiscourse is the sayable, the opposite of what cannot be enunciated (l'inénoncable). But, the interdiscourse has also primacy in the sense that it \"defines the relations between discursive entities (or formations) that are constitutive of the discursive entities\". What is acceptable discourse, is in many respects a matter of interdiscursivity at level 2 and 3, because the interdiscursive import and export relations constitute a worksharing between the discursive entities, and this frames what is acceptable discourse within each discursive entity: Generally, a discourse has little authority over what other discourses are assumed to speak of and will therefore accept imported form and content from the other discourses. On the other hand, when a discourse exports content to other discourses, there are expectations as to the exported form and content. Thus, the interdiscursive system shapes the discourses.\n\n"}
{"id": "28769250", "url": "https://en.wikipedia.org/wiki?curid=28769250", "title": "International Day For Monuments and Sites", "text": "International Day For Monuments and Sites\n\nThe International Day for Monuments and Sites also known as World Heritage Day is an international observance held on 18 April each year around the world with different types of activities, including visits to monuments and heritage sites, conferences, round tables and newspaper articles. Each year has a theme, for example sustainable tourism in 2017.\n\nThe International Day for Monuments and Sites was proposed by the International Council on Monuments and Sites (ICOMOS) on 18 April 1982 and approved by the General Assembly of UNESCO in 1983. The aim is to promote awareness about the diversity of cultural heritage of humanity, their vulnerability and the efforts required for their protection and conservation.\n"}
{"id": "43629007", "url": "https://en.wikipedia.org/wiki?curid=43629007", "title": "Jihad Cool", "text": "Jihad Cool\n\nJihad Cool is term used by American security experts concerning the re-branding of militant jihadism into something fashionable, or \"cool\", to younger people through social media, magazines, rap videos, clothing, toys, propaganda videos, and other means. It is a sub-culture mainly applied to individuals in developed nations who are recruited to travel to conflict zones on Jihad. For example, Jihadi rap videos make participants look \"more MTV than Mosque\", according to NPR, which was the first to report on the phenomenon in 2010.\n\n"}
{"id": "967342", "url": "https://en.wikipedia.org/wiki?curid=967342", "title": "Liberty Generation", "text": "Liberty Generation\n\nThe Liberty Generation is an identified cohort within the Strauss–Howe generational theory that describes the generation of people who were born from approximately 1724–1741. According to the authors, notable members include King George III, Czarina Catherine the Great, George Washington, Giacomo Casanova and Daniel Boone.\n"}
{"id": "7513910", "url": "https://en.wikipedia.org/wiki?curid=7513910", "title": "List of films and television series featuring Robin Hood", "text": "List of films and television series featuring Robin Hood\n\nThe following are some of the notable adaptations of the Robin Hood story in film and television.\n\n\n\n\n\n\n\n\n\n\n\nSince Robin Hood is a character in the public domain, there is no restriction on his use. Thus, he has often appeared in other series that do not necessarily revolve around the legend, particularly in animated science fiction and fantasy setting.\n\n\n\n\n\n\n\n\n\n"}
{"id": "54257719", "url": "https://en.wikipedia.org/wiki?curid=54257719", "title": "Lurish dances", "text": "Lurish dances\n\nThe mosaic dance of Iran is as vibrant as the country's cultural diversity, and the Iranian dances are the colorful tapestries of practices which permeate the country, as well as the Iranian diaspora. The Lurish dances include a range of folk dances popular among different groups of Lur people which have formed, developed and transferred during the subsequent generations. They usually include common Iranian dances; Collective dances and quadrille, the circular arrangement (circle dance), and the colorful clothing.\n\nDue to discovered objects and archeological excavations from Lurish-inhabited areas like Kozagaran, Tepe Giyan, Tepe Musiyan, CheghaMish and Kul-e Farah, it is evident that dance history predates the Arian migrations inward the Iranian plateau. Motifs depicting three dancing persons on the pottery discovered and dance portrays on the bronze flags of Luristan of the first millennium BC, musicians and musical instruments depicted on the petroglyphs in Kul-e Farah of Izeh and Cheghamish are the most prominent archaeological evidences showing the value and history of dance in these areas.\n\nThere are many dance styles common in Lurish-inhabited areas. The most prevalent Lurish dance styles are handkerchief dance, Chupi dances (SanguinSama; slow rhythm along with strike and fiddle, Se-Pa (three steps) dance that is performed faster than SanguinSama, and Du-Pa (Two steps) dance that is the fastest and the most exciting performance), and the stick dance (Çubâzi or Tarka-bâzi)(like Jitterbug wood) which is a martial show. Harvesting ceremony dances, PaPoshtePa dances, Suwaruna and the mourning dances are of less popular dances. Mourning dances are performed while loved persons are died as Chamariuna among Feylis and also among Great Lur tribes (Bakhtiries and the Southern Lurs).\n\nHandkerchief dance also known as scarf dance (Lurish:Dasmâl-bâzi) is a vibrant circle dance style. This dance style is more common among Southern Lurs and Bakhtiari people. This style is usually performed by both male and female persons and includes different performing styles like Ashtafi-manganâ. Men or women may dance and wave colorful scarves, called dastmâl. A dasmâl is held in each hand. In case of the philosophy, it is believed that these dances have originated in the actions the women do in their daily livelihood. As the dance rhythm increases slowly, each person may chooses patterns to perform.\n\nČu-bâzi or Chubazi (twig dance) is a special dance double that is performed in the celebrations and joys by Bakhtiari and the southern Lurs and is inspired by the heroic battles. It is usually dance by two men, although sometimes is performed by the women too. One dancer (the defender) is equipped with a long heavy stick up to four or five feet long. The other (the attacker) whom holds a shorter flexible stick (tarka), has to attack the opponent only below the level of the knee. While the dance is accompanied by drum and karnâ (a horn music instrument), the defender is only allowed to escape the strike by leaping up in the air. The contributors change in turn till the protagaonists are specified. The dancers are surrounded by female and male enthusiastic audiences whom cheer the winners by screams and whistles.\n\nIn Lurish dialects, mass dance styles that are mainly based on leg movements are said Čupi (چووپی, Choupi). Mixed male and female dancers hold hands to form rings, hearing the vibrant musics by musicians, they perform different styles of čupi. The leader performer (Sarčupi or Čupikeš) is a skillful dancer whose duty is to lead others and warm up them in accordance with dance music rhythms. Other performers should coordinate the movements and body rhythms with the leader. The dance usually starts with Sangin-Samâ rhythm which is performed slowly, following Sepâ (Three steps) the next rhythm with faster performing and soars when vibrant Dopâ (two steps) is performing. After a while, the dance concludes to Sangin-Samâ movements in a reversed trend.\n\nSangin-samâ (also Sangin-sepâ or Samâ) is usually the first phase of Čupi mass dance style and includes gentle and slow movements. Performers, initially put their right foot in the form of a cross with left foot, then take it and take two steps forward. Then they put the left foot with the tip of the toe and finally put it aside right foot. The hands bend from elbow and small fingers of dancers, in line with the shoulders, are tied among each other. It seems that each of the gestures and dance components of Sama, has a philosophy retrieved from thousands years of coexistence with humane and natural elements behind it.\n\nIn Čupi dance continuum, the second phase is Sepâ (three steps) which is performed faster and more dynamic than Sangin-Samâ in accordance with speeding up the music rhythm. Dancers start the dance with the right foot and take three steps forward. In Lurish dances, like most Iranian group dances, the start of the move is with the right foot. After removing the three steps, the dancers first bend the right leg and then the left foot. The dancers' hands in this dance are tied to each other and their shoulders rely on each other. The literal meaning of dance in Lurish (Dasgerta), is the bond between hands when dancing.\n"}
{"id": "175592", "url": "https://en.wikipedia.org/wiki?curid=175592", "title": "Metaphorical language", "text": "Metaphorical language\n\nMetaphorical language is the use of a complex system of metaphors to create a sub-language within a common language which provides the basic terms (verbs, prepositions, conjunctions) to express metaphors.\n\nPeople who over-expose themselves to environments atypical from their usual habits tend to develop understanding of new words and apply new meanings to existing words. Such sources could include: absorbing a native language spoken in a different dialects or countries, a language other than their own and fictional language from books, music, films and videogames.\n\nPeople who are classified with Autistic Spectrum Disorder tend to communicate metaphorically rather than with common phrases. It is currently not known why they can do this, although they also express other abnormalities from level 1 and 2 psychedelic experiences.\n\nMetaphorical language is a common feature of religious discussion, (for example midrash or medieval Roman Catholic \"common places\" or modern biblespeak) wherein fluency in a religious text is often a prerequisite to participating fully in a conversation. Not just conceptual metaphors (part of every language) that express belief in analogy between generic concepts, but extremely specific metaphors involving proper names or use of concrete nouns to express generics or processes.\n\nThe Tao te ching is considered by many to be almost entirely metaphorical. For example, change is usually expressed with the “water” character, not the “change” character. \n\nTo the outsider, such terms in such combinations will likely seem esoteric or otherwise unintelligible. Only by learning the underlying patterns of events that are considered important in the religion or ethical or political system, would one be able to comprehend what was said. The religious text thus acts as a code book. Since many religious authorities believe in the self-evident truth of their doctrines, a mere exposure to the truth in the book would tend to convert outsiders trying to learn the language. However, use of such language is not confined to religious groups\n\nUse of metaphorical language was historically common among secret society devotees, for instance the Chinese Tongs that resisted Imperial rulers, and even some modern troll organizations.\n\n"}
{"id": "28523058", "url": "https://en.wikipedia.org/wiki?curid=28523058", "title": "Mormon studies", "text": "Mormon studies\n\nMormon studies is the interdisciplinary academic study of the beliefs, practices, history and culture of those known by the term \"Mormon\" and denominations belonging to the Latter Day Saint movement whose members do not generally go by the term \"Mormon\". The Latter Day Saint movement includes not only The Church of Jesus Christ of Latter-day Saints (LDS Church) but also the Community of Christ (CoC) and other groups, as well as those falling under the umbrella of Mormon fundamentalism.\n\nBefore 1903, writings about Mormons were mostly orthodox documentary histories or anti-Mormon material. The first dissertations on Mormons, published in the 1900s, had a naturalistic style that approached Mormon history from economic, psychological, and philosophical theories. While their position within Mormon studies is debated, Mormon apologetics have a tradition dating back to Parley P. Pratt's response to an anti-Mormon book in 1838.\n\nThe amount of scholarship in Mormon studies increased after World War II. From 1972–1982, while Leonard Arrington was a Church Historian in the history department, the LDS Church Archives were open to Mormon and non-Mormon researchers. Researchers wrote detached accounts for Mormon intellectuals in the \"New Mormon history\" style. Many new publications started to publish history in this style, including , \"BYU Studies Quarterly\", and \"Exponent II\". Some general authorities in the church did not like the New Mormon history style, and Arrington and his remaining staff were transferred to Brigham Young University (BYU) in 1982, where they worked in the Joseph Fielding Smith Institute for Church History. The institute continued to support scholarship in Mormon history until 2005, when the institute closed and employees transferred to the LDS Church Office Building. In the late 1980s and 1990s, several other incidents made BYU faculty reluctant to voice unorthodox ideas about church history. Around 1990, BYU professors were asked not to contribute to \"Dialogue\" or \"Sunstone\". Two historians were excommunicated in 1993, probably for their published unorthodox views. \"BYU Studies\" and other LDS church-sponsored publishers published more \"faithful\" scholarship at this time. Presses outside of Utah started to publish more books in Mormon studies. \n\nMormon scholars engaging in Mormon studies still feel they must be careful about what they write, especially if they work with material from the Church History Library archives. Non-Mormon scholars are often suspicious of Mormon scholars' work. This is gradually changing as Mormon scholars find employment outside of church-sponsored institutions. Universities without affiliation to the LDS Church have endowed chairs for Mormon studies. Emerging trends in \"Newer\" Mormon History are an increase in interdisciplinary work and women's history. The LDS Church History Department hired a women's history specialist in 2011 and has recently published books focusing on women's history. Blogs focusing on Mormon history have helped make Mormon history more accessible and provided a safe space for unorthodox ideas, although they may be superficial at times. \n\nBefore World War II, church histories were mostly either orthodox Mormon or anti-Mormon and written by faithful Mormons or hostile non-Mormons, respectively. A few writers in the first era of church history (1830–1905) wrote about Mormons as a curiosity and focused on their peculiar ways.\n\nNon-Mormons wrote for a non-Mormon public about how \"primitive and dangerous\" Mormons were in \"extreme terms.\" Eber D. Howe published \"Mormonism Unvailed, or a Faithful Account of that Singular Imposition and Delusion\" in 1834, which claimed that Sidney Rigdon was the original author of the Book of Mormon and that Joseph Smith was a \"vile wretch.\" Howe included affidavits from people who knew Joseph Smith collected by ex-Mormon Philastus Hurlbut. The book influenced future anti-Mormon literature. (by La Roy Sunderland, John Bennett, and John A. Clark). Origen Bacheler examined the Book of Mormon itself in \"Mormonism Exposed Internally and Externally\", arguing that the book was inconsistent with the Bible and was written by Joseph Smith himself.\n\nIn the 1960s, ex-Mormons Jerald and Sandra Tanner continued that anti-Mormon tradition by reprinting anti-Mormon works in the public domain as well as important but unflattering documents from LDS history through Utah Light House Ministry. They published their own criticisms of the LDS church as well, which, unlike early anti-Mormon works, cite historical documents. Ed Decker, an excommunicated Mormon, made two anti-Mormon films: \"The God Makers\" (1982) and \"The God Makers II\" (1993). The films described Mormons as being a cult, abusing women and children, manipulating news outlets, and practicing Satanism. \"The God Makers II\" received criticism from other anti-Mormons, including Jerald and Sandra Tanner, who stated it contained inaccuracies.\n\nOfficial recorders have existed since Joseph Smith organized the Church of Christ on April 6, 1830. Church records continue to the present and are kept in the LDS church archives. The first official church history was published in 1842, when Smith and his associates began writing \"History of Joseph Smith\" as an official diary of Joseph Smith. This history was published in Times and Seasons in Nauvoo, and then in Deseret News and Latter-day Saints' Millennial Star up until 1863. \"History of Joseph Smith\" was followed by \"History of Brigham Young\", which was also published in Deseret News and Millennial Star over the next two years. Church Historians and their assistants edited the material, which was published in official publications. Andrew Jenson made sizable contributions to documentary church history with the Latter-day Saint Biographical Encyclopedia (1901–36), Encyclopedic History of the Church (1941), and an unpublished \"Journal History of the Church\" containing over 1,500 scrapbooks filled with published and unpublished records of daily activities in the church. Jenson made a special report on the Mountain Meadows Massacre, and parts of the report were not openly used until \"Massacre at Mountain Meadows\" (2008) by Richard E. Turley, Ronald W. Walker, and Glen M. Leonard.\n\nThe first historian to attempt to summarize Mormon history on a large scale was Edward Tullidge, who wrote \"Life of Brigham Young: or Utah and Her Founders\" (1876), \"History of Salt Lake City\" (1886), and History of Northern Utah and Southern Idaho (1889). Hubert How Bancroft wrote \"History of Utah\" (1889) with the help of the Historian's Office. Bancroft's history of Utah portrayed Mormons favorably. Critics say that he wasn't objective since he allowed LDS Church authorities to read the book before publication. Perhaps his favorable treatment was how he obtained access to the church records. Expanding on Bancroft's history, Orson F. Whitney wrote \"History of Utah\" (1898–1904) in four volumes. Joseph Fielding Smith wrote \"Essentials of Church History\" in 1922. Most of these accounts combined various testimonies into a single narrative without questioning the validity of the eyewitnesses or other observers, especially those of church authorities.\n\nMormons wrote accounts for other Mormons, often published in church-sponsored venues like \"The Juvenile Instructor\" and in church-published lesson manuals. These writings were written for a Mormon audience in order to support their existing beliefs. Brigham H. Roberts was an associate editor of the \"Salt Lake Herald\" and while on a mission to England, was the editor of the \"Millenial Star\". Upon returning to Utah, he became a General Authority. After an invitation from \"Americana\", Brigham H. Roberts wrote a chapter each month from 1909 to 1915 in what later became the \"Comprehensive History of the Church of Jesus Christ of Latter-day Saints: Century One\". The history had some of the first historical analysis of events in church history. It was serialized in Americana 1909–1915. \n\nFrom 1830-1930, women were victims or symbols in historical accounts. Church historians mentioned their suffering, but rarely mentioned them by name. Anti-polygamy tracts also described Mormon women in general terms, describing them as deluded or miserable. In an effort to combat the way anti-polygamists portrayed Mormon women, Edward Tullidge and Eliza R. Snow compiled \"The Women of Mormondom\" (1877), a book that portrayed Mormon women as hardworking and independent in a combined history, biography, and theology. \"Heroines of Mormondom\" (1884) highlighted faithful Mormon women's lives. Women wrote short biographies of other women and recorded them in \"Women's Exponent\" and through publications from the Daughters of the Utah Pioneers.\n\nEarly academic writers on Mormon topics had a \"naturalistic\" approach to history, using theory from economics, psychology, and philosophy to guide their study. Richard Ely contributed to the professionalization of Mormon studies with his early dissertation \"Economic Aspects of Mormonism\" (1903). In the work, he praised Mormon irrigation and communalism as a good model of economic development. He influenced Leonard Arrington's interest in economics and Mormons. Andrew Love Neff wrote \"The Mormon Migration to Utah,\" which he finished in 1918 but had started over ten years earlier. He was interested in how Mormons helped colonize the West. Mormon Ephraim Edward Ericksen wrote \"The Psychological and Ethical Aspects of Mormonism\" (1922) while studying at the University of Chicago. His dissertation, influenced by functionalist theory, argued that Mormonism was a product of conflicts with non-Mormons and harsh environments. Lowry Nelson, also a Mormon, studied at the University of Wisconsin in the 1920s. He worked in agriculture and was dean of BYU's College of Applied Science and director of the Utah Agriculture Experiment stations. He wrote articles about how the Mormon village was designed to promote unity and sociability, which allowed Mormon settlers to colonize the Great Basin Desert. He left Utah in 1937. Nels Anderson studied at the University of Chicago, and studied hobos in Utah, where he converted to Mormonism. His book \"Desert Saints\" (1944) recounted the history of saints in the St. George, Utah area. Other scholars publishing on Mormonism from this time period include I. Woodbridge Riley, Walter F. Prince, Franklin D. Daines, Hamilton Gardner, Joseph Geddes, Feramorz Fox, Arden Beal Olsen, William McNiff, Kimball Young, Austin Fife and Alta Fife.\n\nIn the 1950s after World War II, an increasing number of Mormons studied history professionally and wrote dissertations about Mormon history. Non-Mormon sociologist Thomas F. O'Dea wrote a dissertation entitled \"Mormon Values: The Significance of a Religious Outlook for Social Action\" after living in a rural Mormon farming village in New Mexico for six months and subsequently teaching at Utah State University. This study of Mormon culture \"stunned Mormon readers with its objectivity and sympathetic insight,\" according to Mormon scholar Richard Bushman. Bernard DeVoto, Dale L. Morgan, Fawn McKay Brodie, Stuart Ferguson, and Juanita Brooks did not have graduate degrees in history, but made significant contributions to the foundations of Mormonism's \"New History\" movement. Brodie wrote \"No Man Knows My History\" (1945), which contemporary reviews praised as definitive and scholarly. Other LDS scholars, notably Hugh Nibley, criticized Brodie's biography. In 1950, Juanita Brooks, an educated Mormon housewife, published a scholarly book on the Mountain Meadows Massacre, which is well-researched and balanced even by contemporary standards. Brooks's Mormon neighbors did not like \"the frankness\" of her book.\n\nMormon scholars are divided on whether apologetics should be considered part of Mormon studies or not. Brian D. Birch argues that it should be a part of Mormon studies, as long as apologetic authors concede that their arguments are objective and subject to academic debate. Apologists write defensively, and view their polemical responses to criticism as a battle for their faith. Parley P. Pratt responded to \"Mormonism Unveiled\" in detail in his 1838 pamphlet \"Mormonism Unveiled: Zion's Watchman Unmasked and Its Editor Mr. L.R. Sunderland Exposed, Truth Vindicated, the Devil Mad, and Priestcraft in Danger!\" Pratt argued against Sunderland's character, quoting Hurlbut, who stated that Sunderland has a \"notorious character.\"\n\nHugh Nibley's \"No Ma'am, That's Not History\" set a standard for apologetics to use academic language, and criticized Brodie's use of sources in her controversial biography of Joseph Smith, \"No Man Knows My History\". The Foundation for Ancient Research and Mormon Studies (FARMS) aimed to support the historical authenticity of the Book of Mormon and respond to criticism, and used Nibley's style to counter research that contradicted the Book of Mormon's ancient origins. FARMS collaborated with Deseret Book to publish the complete works of Hugh Nibley starting in 1984. In 1997, LDS church president Gordon B. Hinckley invited FARMS to be officially affiliated at BYU, and in 2006 it was subsumed by the Neal A. Maxwell Institute of Religious Scholarship. In 2012, Daniel C. Petersen, the editor of \"FARMS Review\", started publishing a new journal called \"Interpreter\". The Foundation for Apologetic Information and Research (FAIR), a group including both laypeople and academics, attempts to answer criticisms of the Mormon faith. in 2013, it changed its name to FairMormon. \n\nSome other Mormon \"insiders\" countered the Book of Mormon's ancient origins through the Smith-Pettit Foundation in Salt Lake City and George Smith's Signature Books publishing company. Signature Books published \"New Approaches to the Book of Mormon\" by Brent Metcalfe and \"American Apocrypha\" by Dan Vogel and Metcalfe. These insider views of the Book of Mormon's origins were diverse. \"American Apocrypha\" described the Book of Mormon as a work of fiction reflecting its environment. Ostler argued that the Book of Mormon was partially inspired. FARMS's responses were at times patronizing, and even descending into veiled name-calling in William Hamblin's 1994 critique of a Metcalfe essay.\n\nIn the 1990s and 2000s, Evangelicals Carl Mosser and Paul Owen encouraged other Evangelicals to respond to Mormon apologetics. Evangelical Craig L. Blomberg discussed whether or not Mormons were Christian with Mormon Stephen E. Robinson in \"How Wide the Divide? A Mormon and Evangelical in Conversation\". Richard Bushman encouraged fellow Mormon historians to be less defensive and more open to criticism, and also to do research on Mormon history from a consciously Mormon point of view.\n\nIn the 1960s, a new generation of Mormon scholars emerged. The publication of , the newly-established Mormon History Association, and the professionalization of LDS and RLDS history departments provided spaces for historians to do new research in Mormon topics. RLDS scholars founded the John Whitmer Historical Association in 1972. In 1974, Claudia Bushman and Laurel Thatcher Ulrich founded the magazine \"Exponent II\". The first issue of \"BYU Studies\" was published in 1959.\nAlso in 1972, the LDS Church hired Leonard Arrington as their historian. During Arrington's time as historian, Mormon and non-Mormon historians were allowed to access the LDS Church Archives. Much of the research in the 1970s used these newly-available sources to examine church history, sometimes in great detail. Leonard Arrington influenced important scholars of Mormon history, including Richard Jensen, William Hartley, and Ronald Walker. In 1969, Jewish historian Moses Rischin named the increasing amount of Mormon scholarship \"the New Mormon history.\" The \"New Mormon History\" movement included non-Mormons Thomas F. O'Dea, P.A.M. Taylor, Mario De Pillis, Lawrence Foster, Community of Christ member Robert Flanders, and Mormon scholar Klaus Hansen. \n\nMaureen Ursenbach Beecher was a leading researcher in women's studies. In the 1970s women's biographies were published, but not integrated into larger narratives. Other women hired by the Church Historical Department included Jill Mulvay Derr, Carol Cornwall Madsen, and Edyth Romney. Journals dedicated special issues to Mormon women, and the increased interest in Mormon women led to more publications focused on them. Scholars published biographies of Emma Smith, Eliza Snow, Emmeline B. Wells, and Amy Brown Lyman. \n\nSome writers looked at Mormon women's history with the goal of restructuring historical narratives. Mormon feminist articles on Mormon history started with the special Summer 1971 issue of \"Dialogue\" on women's issues and continued in publications like \"Exponent II\" (starting in 1974), and \"Mormon Sisters: Women in Early Utah\" (1976), edited by Claudia Bushman. Beecher and Laurel Thatcher Ulrich edited another volume about Mormon women's history in \"Sisters in Sprit: Mormon Women in Historical and Cultural Perspective\" (1987). \"Women and Authority: Re-emerging Mormon Feminism\" (1992) was another milestone in feminist publications, and it encouraged Mormon women to be empowered by their history and \"reclaim lost opportunities.\"\n\nMost New Mormon historians were LDS. Their audience was Mormon intellectuals and non-Mormons. They maintained their respect for the Mormon faith, admitted to flaws in people and policies, and avoided taking a defensive stance, a tone which non-Mormon historian Jan Shipps wrote \"made them seem more secular than they actually were.\" Mormon history by non-Mormons at this time had a similar detached tone. New Mormon historians often published with the University of Illinois Press in order to publish for an academic audience independent of the church. Charles S. Peterson argued in \"The Great Basin Kingdom Revisited\" that Arrington took an exceptionalist view of Mormon history, which he then taught to other New Mormon Historians. This exceptionalist view was that they could believe in both secular history and orthodox Mormon views of the restoration.\n\nThe LDS church stopped funding so much research and limited access to the church archives. Apostle Ezra Taft Benson warned employees in the Church Educational System against New Mormon History in a 1976 speech. He said that writing history in a neutral style undermined \"prophetic history.\" Boyd K. Packer's 1981 article, \"The Mantle is Far, Far Greater than the Intellect\" was published in \"BYU Studies.\" He wrote that contemporary historians were too eager to focus on the faults of church leaders and dismiss spiritual inspiration. In 1982, historians from Arrington's department were transferred to Brigham Young University, where they were assigned to teach in the history department and worked in the Joseph Fielding Smith Institute for Church History.\nAfter Arrington's death in 1999, Ronald K. Esplin and Jill Mulvay Derr led the Joseph Fielding Smith Institute for Church History at BYU. Carol Cornwall Madsen led research in the Women's History Initiative at the institute, where she wrote an important biographical study of Emmeline B. Wells. In 2001, Richard Bushman retired from full-time teaching at Columbia University and was a research director at the Smith Institute. Dean C. Jessee started editing Joseph Smith's papers in \"The Personal Writings of Joseph Smith\". The Smith Institute closed in 2005, and institute staff along with the Smith papers project moved to the Church Office Building. The Joseph Smith Papers project, started by the LDS church in 2001, aimed to publish Joseph Smith's papers with rigorous accuracy, and was validated by the National Historic Public Records Commission. \n\nJan Shipps asserts that this reluctance to support new Mormon history was a response to the Mark Hofmann document forgeries. Also, some church authorities disliked the books and articles produced by the history department, which noted flaws as well as strengths of people in church history. Shipps states that the increase in new converts to the LDS Church led General Authorities to emphasize the need for \"palatable\" versions of church history in museums and historic sites rather than in-depth articles in church-sponsored publications. Mormon sociologist Armand Mauss argued that Mormonism was a struggle between remaining distinctive and assimilating to accepted American cultural practices; scholar Ronald Helfrich speculates that the change in General Authority's reception to Arrington's research was because they feared assimilating too much. General interest in Mormon studies continued during the 1980s, with over 2,000 books, articles, and other material published on Mormon history during that decade. \n\n\"BYU Studies\" and Deseret Books published more \"New Mormon Faith Historians\" after General Authority pushback against New Mormon History. One of these New Mormon Faith Historians was Louis Midgely, who argued that from a relativist, postmodern theory, the Mormon view that the LDS Church had divine origins was just as valuable and valid as others. New Mormon Faith Historians said that the New Mormon scholars left faith out of their analyses. Many were members of FARMS, and often saw writers of New Mormon History as the same as other anti-Mormons, even though most writers of New Mormon History were Mormon. The difference between the New Mormon Faith Historians and New Mormon scholars was hard to define. \n\nAlong with Arrington's transfer and a subsequent increase in restrictions in the LDS Church Archives, several other incidents led to an intellectual chilling of Mormon history by Mormons in the 1990s. In 1992, Arrington wrote that \"the church cannot afford to place its official stamp of approval on any 'private' interpretation of its past,\" and this kind of history must be not sponsored by the LDS Church. In September 1993, the LDS church excommunicated the September Six, which included two historians: Lavina Fielding Anderson and D. Michael Quinn. These excommunications served as a warning to other Mormon historians. Quinn's excommunication was perhaps tied to his idea that Mormon women had been given the priesthood in 1843, which he published in an essay in \"Women and Authority: Re-emerging Mormon Feminism\". In 2003, he was scheduled to give a speech at a conference at Yale which was co-sponsored by BYU, and BYU stated they would withdraw their funding if Quinn presented his paper. That same year, Quinn applied to work as a professor at the University of Utah and Arizona State University. He was not hired as a professor, possibly because of fears that LDS people in power would retaliate against the university. In 1986, administrators were asked not to contribute to \"Dialogue\" or present at the Sunstone symposium; around 1990, BYU professors were asked not to contribute to \"Dialogue\" or \"Sunstone\". Eugene England, one of the founders of \"Dialogue\" and then a professor at BYU, spoke out against these prohibitions. He was asked not to write for the \"Encyclopedia of Mormonism\" in 1990, and in 1998 he was asked to retire from BYU without justification. England saw this as stemming from his publicly anti-war stance, and for his attention to Mormon racism and sexism. He viewed his differences as a potential source of learning for himself and others. After retiring from BYU, he started one of the first Mormon studies programs at Utah Valley State College. According to a 1997 report by the American Association of University Professors on academic freedom at BYU, Alan Wilkins was questioned about his motives for contributing to \"Dialogue\" and \"Sunstone\" in a tenure review. The report also mentioned other incidents where BYU administration criticized speakers and articles for criticism of the church, among other complaints. \n\nIn 1997, Joanna Brooks argued that the goal of Mormon studies was to critically examine Mormonism, not to determine religious truths. She postulated that Mormon studies done as a type of cultural studies will help scholars in the field feel less defensive and more productive. Outside of Brigham Young University and Utah, the University of North Carolina Press, Knopf, and the University of Oklahoma Press published books on Mormonism. In the 2000s, Jan Shipps was a large influence on news articles about Mormons; often she is the only expert cited for an entire article. In 2005, the National Endowment for the Humanities held a seminar at Brigham Young University on the bicentennial of Joseph Smith's birth. Terryl Givens, a comparative literature scholar, analyzed discourse about the Book of Mormon in \"By the Hand of Mormon: The American Scripture that Launched a New World Religion\" in 2002. \n\nMormon women's history has not been well-integrated in general histories. Arrington and Davis Bitton discussed women's issues in two chapters on marriage and sisterhood in \"The Mormon Experience\" (1992). \"The Story of the Latter-day Saints\" (1992) by James Allen and Glen Leonard mentioned women in the context of auxiliaries like Relief Society and Primary, plural marriage, suffrage, and the ERA. \"The Encyclopedia of Latter-day Saint History\" (2000) contained 435 entries about men, but only 64 about women, with three-quarters of the women receiving less than a page of description. Church publication \"Our Heritage\" (1996) only mentioned a few women. Women's history remained in a \"separate sphere.\" \"Daughters in My Kingdom\" (2011), an official history of the Relief Society, is mostly used in women's meetings. Outside of Mormon history specialists, Mormon women are rarely mentioned.\n\nNon-Mormon scholars are still often suspicious of LDS scholars' work. That is gradually changing as non-Mormon scholars increase and universities not affiliated with the LDS Church have endowed chairs for Mormon studies. Kathleen Flake is the first Richard L. Bushman Chair of Mormon Studies at University of Virginia, and Patrick Mason is the Howard W. Hunter Chair of Mormon Studies at Claremont Graduate University in California.\nThe Church History Library still restricts access to certain documents for most scholars. Scholars may self-censor their research for fear of losing access to documents from the Church History Library. Previous excommunications of Mormon historians give Mormon researchers the sense that they are being watched. Scholars from various disciplines see the \"New Mormon History\" movement as ending, bring replaced by \"Post New Mormon History\" or \"Newer Mormon History.\" This emerging movement is interdisciplinary and endeavors to place Mormon studies in a broader historical context, further eroding boundaries between disciplines. Mormon women's history has not been well-integrated with other Mormon studies topics. Contemporary historians like R. Marie Griffith, Grant Wacker, and Robert Orsi encourage the use of interdisciplinary tools in Mormon studies.\n\nIncluded in these interdisciplinary tools are oral histories. In 1972, an the Charles Redd Center for Western Studies was established at BYU, where Jessie L. Embry directed an extensive oral history project. The Church History Department started their own oral history project in 2009. Claudia L. Bushman and her students started the Claremont Oral History collection in 2009, and papers using the oral history data were published in \"Mormon Women Have Their Say: Essays from the Claremont Oral History Collection\".\n\nThe LDS Church History Department hired a specialist in women's history in 2011, Kate Holbrook. She co-authored \"The First Fifty Years of Relief Society: Key Documents in Latter-Day Saint Women's History\" with Jill Mulvay Derr, Carol Cornwall Madsen, and Matthew J. Grow. Laurel Thatcher Ulrich said the book was \"the most important work to emerge from a Mormon Press in the last 50 years.\" Jenny Reeder, specializing in 19th century women's history, was hired in 2013. Brittany Chapman Nash and Lisa Tait also specialize in women's history and work in the LDS Church history department. Nash works in public services and helps researchers to be aware of women's sources the archive offers. She co-authored \"Women of Faith in the Latter Days\" with Richard Turley. Tait works on the web team, helping to add a \"Women of Conviction\" section to church history website. In 2017, Reeder and Holbrook edited a compilation of women's speeches called \"At the Pulpit: 185 Years of Discourses by Latter-day Saint Women.\" \n\nThe Mormon blogosphere influences Mormon studies. In 2011, Patrick Mason surveyed 113 Mormon blog readers who were also graduate students. Most respondents viewed blogs as a way to democratize Mormon studies. Since blogs are independent from Church institutions, many felt that blogs were a safe space to test more unorthodox ideas. A few observed that men's voices are more prominent in the blogging community, though a few prominent blogs have all-women authors. Other respondents felt that blogs made Mormon studies \"more of an echo chamber,\" and were \"superficial,\" and \"glorified navel-gazing.\" One of the most popular blogs, By Common Consent, had over two million page visitors in 2011. It and other blogs are influential on Mormon studies.\n\nArchives with significant Mormon collections include the L. Tom Perry Special Collections at BYU, the Church Archives in Salt Lake, the J. Willard Marriott Library at the University of Utah in Salt Lake, Utah State University Libraries, and the Beinecke Rare Book and Manuscript Library at Yale in New Haven, Connecticut.\n\nAwards for writing or service in the field of Mormon studies are presented annually by scholarly societies. The Mormon History Association (MHA) and the John Whitmer Historical Association (JWHA) each present annual awards for various categories within Mormon history, such as books, biographies, documentary history, journal articles, and lifetime achievement. The MHA also gives awards for theses and student papers. The Utah State Historical Society (USHS), which frequently engages Mormon history, also presents awards for books, articles, and student papers. Literary awards are presented by the Association for Mormon Letters, often awarding Mormon publications in biography, criticism, and special categories. \"\" honors the best contributions to its journal and \"\" awards the best article submitted by a woman.\n\nUniversities also present awards. The University of Utah gives the Juanita Brooks Prize in Mormon Studies and offers a Mormon Studies Fellowship. Utah State University's Evans Biography Awards focus on biographies significant to \"Mormon Country\". Student writing competitions are held by Utah State University, the MHA, and the JWHA. BYU Religious Education presents annual awards to its faculty for teaching, research, and service, as well as books in the categories of church history or ancient scripture.\n\nSeveral universities have programs in the study of Mormonism, with professors named to oversee coursework, research, and events on Mormon studies. While independent academic programs have emerged in recent years, devotional religious education programs have existed far longer. Additional colleges have also taught courses on Mormonism without having institutionally sponsored programs, but they are not included in the list below.\n\n\n\n\n\n\n\nThe following primarily publish books on Mormon studies:\n\nSeveral publishers within the devotional religious market also occasionally publish in Mormon studies, including the LDS publishers Cedar Fort, Inc., Covenant Communications, and Deseret Book (which is owned by the LDS Church), as well as Herald House (which is owned by the Community of Christ).\n\nIn addition, certain general book publishers or university presses have also published significant Mormon studies. These include:\n\n\n\n\n\n\n\n\n\n"}
{"id": "27016573", "url": "https://en.wikipedia.org/wiki?curid=27016573", "title": "Nature–culture divide", "text": "Nature–culture divide\n\nThe nature–culture divide refers to a theoretical foundation of contemporary anthropology. Early anthropologists sought theoretical insight from the perceived tensions between nature and culture. Later, the argument became framed by the question of whether the two entities function separately from one another, or if they were in a continuous biotic relationship with each other.\n\nIn eastern society nature and culture are conceptualized as dichotomous (separate and distinct domains of reference). Some consider culture to be \"man's secret adaptive weapon\" in the sense that it is the core means of survival. \nIt has been observed that the terms \"nature\" and \"culture\" that can not necessarily be translated into non-western languages, for example, the Native American John Mohawk describing \"nature\" as \"anything that supports life\".\n\nIt has been suggested that small scale-societies can have a more symbiotic relationship with nature. But less symbiotic relations with nature are limiting small-scale communities' access to water and food resources. It was also argued that the contemporary Man-Nature divide manifests itself in different aspects of alienation and conflicts. \nGreenwood and Stini argue that agriculture is only monetarily cost-efficient because it takes much more to produce than one can get out of eating their own crops, e.g. \"high culture cannot come at low energy costs\".\n\nDuring the 1960s and 1970s Sherry Ortner showed the parallel between the divide and gender roles with women as nature and men as culture.\n\nThe nature-culture divide is deeply intertwined with the social versus biological debate, since it both are implications of each other. As viewed in earlier forms of Anthropology, it is believed that genetic determinism de-emphasizes the importance of culture, making it obsolete. However, more modern views show that culture is valued more than nature because everyday aspects of culture have a wider impact on how the humans see the world, rather than just our genetic makeup. Older anthropological theories have separated the two, such as Franz Boas, who claimed that social organization and behavior is purely the transmission of social norms and not necessarily the passing of hereditable traits. Instead of using such a contrasting approach, more modern anthropologists see Neo-Darwinism as an outline for culture, therefore nature is essentially guiding how culture develops. When looking at adaptations. anthropologists such as Daniel Nettle believe that behavior associated with cultural groups is a development of genetic difference between groups. Essentially, he states that animals choose their mates based on their environment, which is shaped by directly by culture. More importantly, the adaptations seen in nature are a result of evoked nature, which is defined as cultural characteristics which shape the environment and that then queue changes in phenotypes for future generations. To put simply, cultures that promote more effective resource allocation and chance for survival are more likely to be successful and produce more developed societies and cultures that feed off of each other.\n\nOn the other hand, transmitted culture can be used to bridge the gap between the two even more, for it uses a trial and error based approach that shows how humans are constantly learning, and that they use social learning to influence individual choices. This is seen best about how the more superficial aspects of culture still are intertwined with nature and generic variation. For example, there are beauty standards intertwined into culture because they are associated with better survival rates, yet they also serve personal interests which allows for individual breeding pairs to understand how they fit into society. Additionally, cultural lags dissolve because it is not sustainable for reproduction, and cultural norms that benefit biology continue to persevere. By learning from each other, nature becomes more intertwined with culture since they reinforce each other.\n\nSince nature and culture are now viewed as more intertwined than ever before, which makes the divide between the two seems obsolete. Similarly, the social scientists have been reluctant to use biological explanations as explantations for cultural divisions because it is difficult to construct what what ‘biological’ explanations entail. According to social scientists like Emile Durkheim, anthropologists and especially sociologists have tended to characterize biological explanations in only a physiological and cognitive sense within individuals, not in a group setting. On the other hand, there is a heavier focus on the social determinism as seen in human behavior instead. Furthermore, even as divide between nature and culture has been narrowed there is a reluctance to define biological determinism on a large scale.\n\n\n"}
{"id": "36253964", "url": "https://en.wikipedia.org/wiki?curid=36253964", "title": "Origin of speech", "text": "Origin of speech\n\nThe origin of speech refers to the more general problem of the origin of language in the context of the physiological\ndevelopment of the human speech organs such as the tongue, lips and vocal organs used to produce phonological units in all human languages.\n\nAlthough related to the more general problem of the origin of language, the evolution of distinctively human speech capacities has become a distinct and in many ways separate area of scientific research. The topic is a separate one because language is not necessarily spoken: it can equally be written or signed. Speech is in this sense optional, although it is the default modality for language.\n\nUncontroversially, monkeys, apes and humans, like many other animals, have evolved specialised mechanisms for producing \"sound\" for purposes of social communication. On the other hand, no monkey or ape uses its \"tongue\" for such purposes. Our species' unprecedented use of the tongue, lips and other moveable parts seems to place speech in a quite separate category, making its evolutionary emergence an intriguing theoretical challenge in the eyes of many scholars.\n\nThe term \"modality\" means the chosen representational format for encoding and transmitting information. A striking feature of language is that it is \"modality-independent.\" Should an impaired child be prevented from hearing or producing sound, its innate capacity to master a language may equally find expression in signing. Sign languages of the deaf are independently invented and have all the major properties of spoken language except for the modality of transmission. From this it appears that the language centres of the human brain must have evolved to function optimally irrespective of the selected modality.\n\nThis feature is extraordinary. Animal communication systems routinely combine visible with audible properties and effects, but not one is modality-independent. No vocally impaired whale, dolphin or songbird, for example, could express its song repertoire equally in visual display. Indeed, in the case of animal communication, message and modality are not capable of being disentangled. Whatever message is being conveyed stems from intrinsic properties of the signal.\n\nModality independence should not be confused with the ordinary phenomenon of multimodality. Monkeys and apes rely on a repertoire of species-specific \"gesture-calls\" — emotionally expressive vocalisations inseparable from the visual displays which accompany them. Humans also have species-specific gesture-calls — laughs, cries, sobs and so forth — together with involuntary gestures accompanying speech. Many animal displays are polymodal in that each appears designed to exploit multiple channels simultaneously.\n\nThe human linguistic property of \"modality independence\" is conceptually distinct from this. It allows the speaker to encode the informational content of a message in a single channel, while switching between channels as necessary. Modern city-dwellers switch effortlessly between the spoken word and writing in its various forms — handwriting, typing, e-mail and so forth. Whichever modality is chosen, it can reliably transmit the full message content without external assistance of any kind. When talking on the telephone, for example, any accompanying facial or manual gestures, however natural to the speaker, are not strictly necessary. When typing or manually signing, conversely, there's no need to add sounds. In many Australian Aboriginal cultures, a section of the population — perhaps women observing a ritual taboo — traditionally restrict themselves for extended periods to a silent (manually signed) version of their language. Then, when released from the taboo, these same individuals resume narrating stories by the fireside or in the dark, switching to pure sound without sacrifice of informational content.\n\nSpeaking is the default modality for language in all cultures. Humans' first recourse is to encode our thoughts in sound — a method which depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus.\n\nThe speech organs, everyone agrees, evolved in the first instance not for speech but for more basic bodily functions such as feeding and breathing. Nonhuman primates have broadly similar organs, but with different neural controls. Apes use their highly flexible, maneuverable tongues for eating but not for vocalizing. When an ape is not eating, fine motor control over its tongue is deactivated. \"Either\" it is performing gymnastics with its tongue \"or\" it is vocalising; it cannot perform both activities simultaneously. Since this applies to mammals in general, \"Homo sapiens\" is exceptional in harnessing mechanisms designed for respiration and ingestion to the radically different requirements of articulate speech.\n\nThe word \"language\" derives from the Latin \"lingua,\" \"tongue\". Phoneticians agree that the tongue is the most important speech articulator, followed by the lips. A natural language can be viewed as a particular way of using the tongue to express thought.\n\nThe human tongue has an unusual shape. In most mammals, it's a long, flat structure contained largely within the mouth. It is attached at the rear to the hyoid bone, situated below oral level in the pharynx. In humans, the tongue has an almost circular sagittal (midline) contour, much of it lying vertically down an extended pharynx, where it is attached to a hyoid bone in a lowered position. Partly as a result of this, the horizontal (inside-the-mouth) and vertical (down-the-throat) tubes forming the supralaryngeal vocal tract (SVT) are almost equal in length (whereas in other species, the vertical section is shorter). As we move our jaws up and down, the tongue can vary the cross-sectional area of each tube independently by about 10:1, altering formant frequencies accordingly. That the tubes are joined at a right angle permits pronunciation of the vowels [i], [u] and [a], which nonhuman primates cannot do. Even when not performed particularly accurately, in humans the articulatory gymnastics needed to distinguish these vowels yield consistent, distinctive acoustic results, illustrating the quantal nature of human speech sounds. It may not be coincidental that [i], [u] and [a] are the most common vowels in the world's languages.\n\nIn humans, the lips are important for the production of stops and fricatives, in addition to vowels. Nothing, however, suggests that the lips evolved for those reasons. During primate evolution, a shift from nocturnal to diurnal activity in tarsiers, monkeys and apes (the haplorhines) brought with it an increased reliance on vision at the expense of olfaction. As a result, the snout became reduced and the rhinarium or \"wet nose\" was lost. The muscles of the face and lips consequently became less constrained, enabling their co-option to serve purposes of facial expression. The lips also became thicker. \"Hence\", according to one major authority, \"the evolution of mobile, muscular lips, so important to human speech, was the exaptive result of the evolution of diurnality and visual communication in the common ancestor of haplorhines\". It is unclear whether our lips have undergone more recent adaptation to the specific requirements of speech.\n\nCompared with nonhuman primates, humans have significantly enhanced control of breathing, enabling exhalations to be extended and inhalations shortened as we speak. While we are speaking, intercostal and interior abdominal muscles are recruited to expand the thorax and draw air into the lungs, and subsequently to control the release of air as the lungs deflate. The muscles concerned are markedly more innervated in humans than in nonhuman primates. Evidence from fossil hominins suggests that the necessary enlargement of the vertebral canal, and therefore spinal cord dimensions, may not have occurred in \"Australopithecus\" or \"Homo erectus\" but was present in the Neanderthals and early modern humans.\n\nThe larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is \"descended\". Our species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls. Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays. Lions, jaguars, cheetahs and domestic cats also do this. However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.\nDespite all this, scholars remain divided as to how \"special\" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent. As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs — increasing the risk of choking while swallowing food — we are forced to ask what benefits might have outweighed those costs. The obvious benefit — so it is claimed — must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact \"not\" seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare. Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of \"Homo sapiens.\" A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was \"initially\" driven by selection pressures for speech.\n\nAt least one orangutan has demonstrated the ability to control the voice box.\n\nTo lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds \"deeper\" — giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself. Ohala points out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning. W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in our species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalising to protect their infants would also have benefited from this ability.\n\nMost specialists credit the Neanderthals with speech abilities not radically different from those of modern \"Homo sapiens\". An indirect line of argument is that their tool-making and hunting tactics would have been difficult to learn or execute without some kind of speech. A recent extraction of DNA from Neanderthal bones indicates that Neanderthals had the same version of the FOXP2 gene as modern humans. This gene, once mistakenly described as the \"grammar gene\", plays a role in controlling the orofacial movements which (in modern humans) are involved in speech.\n\nDuring the 1970s, it was widely believed that the Neanderthals lacked modern speech capacities. It was claimed that they possessed a hyoid bone so high up in the vocal tract as to preclude the possibility of producing certain vowel sounds.\n\nThe hyoid bone is present in many mammals. It allows a wide range of tongue, pharyngeal and laryngeal movements by bracing these structures alongside each other in order to produce variation. It is now realised that its lowered position is not unique to \"Homo sapiens\", while its relevance to vocal flexibility may have been overstated: although men have a lower larynx, they do not produce a wider range of sounds than women or two-year-old babies. There is no evidence that the larynx position of the Neanderthals impeded the range of vowel sounds they could produce. The discovery of a modern-looking hyoid bone of a Neanderthal man in the Kebara Cave in Israel led its discoverers to argue that the Neanderthals had a descended larynx, and thus human-like speech capabilities. However, other researchers have claimed that the morphology of the hyoid is not indicative of the larynx's position. It is necessary to take into consideration the skull base, the mandible and the cervical vertebrae and a cranial reference plane.\n\nThe morphology of the outer and middle ear of Middle Pleistocene hominins from Atapuerca SH in Spain, believed to be proto-Neanderthal, suggests they had an auditory sensitivity similar to modern humans and very different from chimpanzees. They were probably able to differentiate between many different speech sounds.\n\nThe hypoglossal nerve plays an important role in controlling movements of the tongue. In 1998, one research team used the size of the hypoglossal canal in the base of fossil skulls in an attempt to estimate the relative number of nerve fibres, claiming on this basis that Middle Pleistocene hominins and Neanderthals had more fine-tuned tongue control than either australopithecines or apes. Subsequently, however, it was demonstrated that hypoglossal canal size and nerve sizes are not correlated, and it is now accepted that such evidence is uninformative about the timing of human speech evolution.\n\nAccording to one influential school, the human vocal apparatus is intrinsically digital on the model of a keyboard or digital computer. If so, this is remarkable: nothing about a chimpanzee's vocal apparatus suggests a digital keyboard, notwithstanding the anatomical and physiological similarities. This poses the question as to when and how, during and the course of human evolution, the transition from analog to digital structure and function occurred.\n\nThe human supralaryngeal tract is said to be digital in the sense that it is an arrangement of moveable toggles or switches, each of which, at any one time, must be in one state or another. The vocal cords, for example, are either vibrating (producing a sound) or not vibrating (in silent mode). By virtue of simple physics, the corresponding distinctive feature — in this case, \"voicing\" — cannot be somewhere in between. The options are limited to \"off\" and \"on\". Equally digital is the feature known as \"nasalisation\". At any given moment the soft palate or velum either allows or doesn't allow sound to resonate in the nasal chamber. In the case of lip and tongue positions, more than two digital states may be allowed. (To experiment with this, click on Interactive Saggital Section).\n\nThe theory that speech sounds are composite entities constituted by complexes of binary phonetic features was first advanced in 1938 by the Russian linguist Roman Jakobson. A prominent early supporter of this approach was Noam Chomsky, who went on to extend it from phonology to language more generally, in particular to the study of syntax and semantics. In his 1965 book, \"Aspects of the Theory of Syntax,\" Chomsky treated semantic concepts as combinations of binary-digital atomic elements explicitly on the model of distinctive features theory. The lexical item \"bachelor\", on this basis, would be expressed as [+ Human], [+ Male], [- Married].\n\nSupporters of this approach view the vowels and consonants recognised by speakers of a particular language or dialect at a particular time as cultural entities of little scientific interest. From a natural science standpoint, the units which matter are those common to \"Homo sapiens\" by virtue of our biological nature. By combining the atomic elements or \"features\" with which all humans are innately equipped, anyone may in principle generate the entire range of vowels and consonants to be found in any of the world's languages, whether past, present or future. The distinctive features are in this sense atomic components of a universal language.\n\nIn recent years, the notion of an innate \"universal grammar\" underlying phonological variation has been called into question. The most comprehensive monograph ever written about speech sounds, \"Sounds of the World's Languages,\" by Peter Ladefoged and Ian Maddieson, found virtually no basis for the postulation of some small number of fixed, discrete, universal phonetic features. Examining 305 languages, for example, they encountered vowels that were positioned basically everywhere along the articulatory and acoustic continuum. Ladefoged concludes that phonological features are not determined by human nature: \"Phonological features are best regarded as artifacts that linguists have devised in order to describe linguistic systems.\" The controversy remains unresolved.\n\nSelf-organization characterizes systems where macroscopic structures are spontaneously formed out of local interactions between the many components of the system. In self-organized systems, global organizational properties are not to be found at the local level. In colloquial terms, self-organisation is roughly captured by the idea of \"bottom-up\" (as opposed to \"top-down\") organisation. Examples of self-organized systems range from ice crystals to galaxy spirals in the inorganic world, and from spots on the leopard skins to the architecture of termite nests or shape of a flock of starlings.\nAccording to many phoneticians, the sounds of language arrange and re-arrange themselves through self-organization Speech sounds have both perceptual (\"how you hear them\") and articulatory (\"how you produce them\") properties, all with continuous values. Speakers tend to minimise effort, favouring ease of articulation over clarity. Listeners do the opposite, favouring sounds which are easy to distinguish even if difficult to pronounce. Since speakers and listeners are constantly switching roles, the syllable systems actually found in the world's languages turn out to be a compromise between acoustic distinctiveness on the one hand, and articulatory ease on the other.\n\nHow, precisely, do systems of vowels, consonants and syllables arise? Agent-based computer models take the perspective of self-organisation at the level of the speech community or population. The two main paradigms here are (1) the iterated learning model and (2) the language game model. Iterated learning focuses on transmission from generation to generation, typically with just one agent in each generation.\nIn the language game model, a whole population of agents simultaneously produce, perceive and learn language, inventing novel forms when the need arises.\n\nSeveral models have shown how relatively simple peer-to-peer vocal interactions, such as imitation, can spontaneously self-organize a system of sounds shared by the whole population, and different in different populations. For example, models elaborated by Berrah et al., as well as de Boer, and recently reformulated using Bayesian theory, showed how a group of individuals playing imitation games can self-organize repertoires of vowel sounds which share substantial properties with human vowel systems. For example, in de Boer's model, initially vowels are generated randomly, but agents learn from each other as they interact repeatedly over time. Agent A chooses a vowel from its repertoire and produces it, inevitably with some noise. Agent B hears this vowel and chooses the closest equivalent from its own repertoire. To check whether this truly matches the original, B produces the vowel \"it thinks it has heard\", whereupon A refers once again to its own repertoire to find the closest equivalent. If this matches the one it initially selected, the game is successful, otherwise it has failed. \"Through repeated interactions,\" according to de Boer, \"vowel systems emerge that are very much like the ones found in human languages.\"\n\nIn a different model, the phonetician Björn Lindblom was able to predict, on self-organizational grounds, the favoured choices of vowel systems ranging from three to nine vowels on the basis of a principle of optimal perceptual differentiation.\n\nFurther models studied the role of self-organization in the origins of phonemic coding and combinatoriality, that is the existence of phonemes and their systematic reuse to build structured syllables. Pierre-Yves Oudeyer developed models which showed that basic neural equipment for adaptive holistic vocal imitation, coupling directly motor and perceptual representations in the brain, can generate spontaneously shared combinatorial systems of vocalizations, including phonotactic patterns, in a society of babbling individuals. These models also characterized how morphological and physiological innate constraints can interact with these self-organized mechanisms to account for both the formation of statistical regularities and diversity in vocalization systems.\n\nThe gestural theory states that speech was a relatively late development, evolving by degrees from a system that was originally gestural.\n\nTwo types of evidence support this theory:\n\nResearch has found strong support for the idea that spoken language and signing depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language. Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.\n\nHumans spontaneously use hand and facial gestures when formulating ideas to be conveyed in speech. There are also, of course, many sign languages in existence, commonly associated with deaf communities; as noted above, these are equal in complexity, sophistication, and expressive power, to any oral language. The main difference is that the \"phonemes\" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.\n\nCritics note that for mammals in general, sound turns out to be the best medium in which to encode information for transmission over distances at speed. Given the probability that this applied also to early humans, it's hard to see why they should have abandoned this efficient method in favour of more costly and cumbersome systems of visual gesturing — only to return to sound at a later stage.\n\nBy way of explanation, it has been proposed that at a relatively late stage in human evolution, our ancestors' hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language is said to have occurred only at that point. Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this puzzle — one setting out from considerations of signal reliability and trust — see \"from pantomime to speech\" below).\n\nLittle is known about the timing of language's emergence in the human species. Unlike writing, speech leaves no material trace, making it archaeologically invisible. Lacking direct linguistic evidence, specialists in human origins have resorted to the study of anatomical features and genes arguably associated with speech production. While such studies may provide information as to whether pre-modern \"Homo\" species had speech \"capacities\", it is still unknown whether they actually spoke. While they may have communicated vocally, the anatomical and genetic data lack the resolution necessary to differentiate proto-language from speech.\n\nUsing statistical methods to estimate the time required to achieve the current spread and diversity in modern languages today, Johanna Nichols — a linguist at the University of California, Berkeley — argued in 1998 that vocal languages must have begun diversifying in our species at least 100,000 years ago.\n\nMore recently — in 2012 — anthropologists Charles Perreault and Sarah Mathew used phonemic diversity to suggest a date consistent with this. \"Phonemic diversity\" denotes the number of perceptually distinct units of sound — consonants, vowels and tones — in a language. The current worldwide pattern of phonemic diversity potentially contains the statistical signal of the expansion of modern \"Homo sapiens\" out of Africa, beginning around 60-70 thousand years ago. Some scholars argue that phonemic diversity evolves slowly and can be used as a clock to calculate how long the oldest African languages would have to have been around in order to accumulate the number of phonemes they possess today. As human populations left Africa and expanded into the rest of the world, they underwent a series of bottlenecks — points at which only a very small population survived to colonise a new continent or region. Allegedly such population crash led to a corresponding reduction in genetic, phenotypic and phonemic diversity. African languages today have some of the largest phonemic inventories in the world, while the smallest inventories are found in South America and Oceania, some of the last regions of the globe to be colonized. For example, Rotokas, a language of New Guinea, and Pirahã, spoken in South America, both have just 11 phonemes, while !Xun, a language spoken in Southern Africa has 141 phonemes.\nThe authors use a natural experiment — the colonization of mainland Southeast Asia on the one hand, the long-isolated Andaman Islands on the other — to estimate the rate at which phonemic diversity increases through time. Using this rate, they estimate that the world's languages date back to the Middle Stone Age in Africa, sometime between 350 thousand and 150 thousand years ago. This corresponds to the speciation event which gave rise to \"Homo sapiens\".\n\nThese and similar studies have however been criticized by linguists who argue that they are based on a flawed analogy between genes and phonemes, since phonemes are frequently transferred laterally between languages unlike genes, and on a flawed sampling of the world's languages, since both Oceania and the Americas also contain languages with very high numbers of phonemes, and Africa contains languages with very few. They argue that the actual distribution of phonemic diversity in the world reflects recent language contact and not deep language history - since it is well demonstrated that languages can lose or gain many phonemes over very short periods. In other words, there is no valid linguistic reason to expect genetic founder effects to influence phonemic diversity.\n\nIn 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language:\n\nMost scholars today consider all such theories not so much wrong — they occasionally offer peripheral insights — as comically naïve and irrelevant. The problem with these theories is that they are so narrowly mechanistic. They assume that once our ancestors had stumbled upon the appropriate ingenious \"mechanism\" for linking sounds with meanings, language automatically evolved and changed.\n\nFrom the perspective of modern science, the main obstacle to the evolution of speech-like communication in nature is not a mechanistic one. Rather, it is that symbols — arbitrary associations of sounds with corresponding meanings — are unreliable and may well be false. As the saying goes, \"words are cheap\". The problem of reliability was not recognised at all by Darwin, Müller or the other early evolutionist theorists.\n\nAnimal vocal signals are for the most part intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. One can \"trust\" the signal not because the cat is inclined to be honest, but because it just can't fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason — because they are hard to fake. Primate social intelligence is \"Machiavellian\" — self-serving and unconstrained by moral scruples. Monkeys and apes often attempt to deceive one another, while at the same time constantly on guard against falling victim to deception themselves. Paradoxically, it is precisely primates' resistance to deception that blocks the evolution of their vocal communication systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.\n\nWords are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favour of hard-to-fake indices or cues. For language to work, then, listeners must be confident that those with whom they are on speaking terms are generally likely to be honest. A peculiar feature of language is \"displaced reference\", which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate \"here\" and \"now\". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot (see signalling theory).\n\nThe \"mother tongues\" hypothesis was proposed in 2004 as a possible solution to this problem. W. Tecumseh Fitch suggested that the Darwinian principle of \"kin selection\" — the convergence of genetic interests between relatives — might be part of the answer. Fitch suggests that spoken languages were originally \"mother tongues\". If speech evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable vocal signals — spoken words — to become accepted as trustworthy and so begin evolving for the first time.\n\nCritics of this theory point out that kin selection is not unique to humans. Ape mothers also share genes with their offspring, as do all animals, so why is it only humans who speak? Furthermore, it is difficult to believe that early humans restricted linguistic communication to genetic kin: the incest taboo must have forced men and women to interact and communicate with non-kin. So even if we accept Fitch's initial premises, the extension of the posited \"mother tongue\" networks from relatives to non-relatives remains unexplained.\n\nIb Ulbæk invokes another standard Darwinian principle — \"reciprocal altruism\" — to explain the unusually high levels of intentional honesty necessary for language to evolve. 'Reciprocal altruism' can be expressed as the principle that \"if you scratch my back, I'll scratch yours.\" In linguistic terms, it would mean that \"if you speak truthfully to me, I'll speak truthfully to you.\" Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, early society as a whole must have been subject to moral regulation.\n\nCritics point out that this theory fails to explain when, how, why or by whom \"obligatory reciprocal altruism\" could possibly have been enforced. Various proposals have been offered to remedy this defect. A further criticism is that language doesn't work on the basis of reciprocal altruism anyway. Humans in conversational groups don't withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting it to anyone who will listen without thought of return.\n\nGossip, according to Robin Dunbar, does for group-living humans what manual grooming does for other primates — it allows individuals to service their relationships and so maintain their alliances. As humans began living in larger and larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable. In response to this problem, humans invented \"a cheap and ultra-efficient form of grooming\" — \"vocal grooming\". To keep your allies happy, you now needed only to \"groom\" them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming (the production of pleasing sounds lacking syntax or combinatorial semantics) then evolved somehow into syntactical speech.\n\nCritics of this theory point out that the very efficiency of \"vocal grooming\" — that words are so cheap — would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming. A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming — the production of pleasing but meaningless sounds — to the cognitive complexities of syntactical speech.\n\nAccording to another school of thought, language evolved from mimesis — the \"acting out\" of scenarios using vocal and gestural pantomime. For as long as utterances needed to be emotionally expressive and convincing, it was not possible to complete the transition to purely conventional signs. On this assumption, pre-linguistic gestures and vocalisations would have been required not just to disambiguate intended meanings, but also to inspire confidence in their intrinsic reliability. If contractual commitments were necessary in order to inspire community-wide trust in communicative intentions, it would follow that these had to be in place before humans could shift at last to an ultra-efficient, high-speed — digital as opposed to analog — signalling format. Vocal distinctive features (sound contrasts) are ideal for this purpose. It is therefore suggested that the establishment of contractual understandings enabled the decisive transition from mimetic gesture to fully conventionalised, digitally encoded speech.\n\nThe ritual/speech coevolution theory was originally proposed by the distinguished social anthropologist Roy Rappaport before being elaborated by anthropologists such as Chris Knight, Jerome Lewis, Nick Enfield, Camilla Power and Ian Watts. Cognitive scientist and robotics engineer Luc Steels is another prominent supporter of this general approach, as is biological anthropologist/neuroscientist Terrence Deacon.\n\nThese scholars argue that there can be no such thing as a \"theory of the origins of language\". This is because language is not a separate adaptation but an internal aspect of something much wider — namely, human symbolic culture as a whole. Attempts to explain language independently of this wider context have spectacularly failed, say these scientists, because they are addressing a problem with no solution. Can we imagine a historian attempting to explain the emergence of credit cards independently of the wider system of which they are a part? Using a credit card makes sense only if you have a bank account institutionally recognised within a certain kind of advanced capitalist society — one where communications technology has already been invented and fraud can be detected and prevented. In much the same way, language would not work outside a specific array of social mechanisms and institutions. For example, it would not work for an ape communicating with other apes in the wild. Not even the cleverest ape could make language work under such conditions.\n\nSpeech consists of digital contrasts whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world — they are a theoretical impossibility. Being intrinsically unreliable, language works only if you can build up a reputation for trustworthiness within a certain kind of society — namely, one where symbolic cultural facts (sometimes called \"institutional facts\") can be established and maintained through collective social endorsement. In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective \"ritual\". Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human symbolic culture as a whole, with language an important but subsidiary component.\n\nCritics of the theory include Noam Chomsky, who terms it the \"non-existence\" hypothesis — a denial of the very existence of language as an object of study for natural science. Chomsky's own theory is that language emerged in an instant and in perfect form, prompting his critics in turn to retort that only something that doesn't exist — a theoretical construct or convenient scientific fiction — could possibly emerge in such a miraculous way. The controversy remains unresolved.\n\nThe essay \"The festal origin of human speech\", though published in the late nineteenth century, made little impact until the American philosopher Susanne Langer re-discovered and publicised it in 1941. \nThe theory sets out from the observation that primate vocal sounds are above all \"emotionally\" expressive. The emotions aroused are socially contagious. Because of this, an extended bout of screams, hoots or barks will tend to express not just the feelings of this or that individual but the mutually contagious ups and downs of everyone within earshot.\n\nTurning to the ancestors of \"Homo sapiens\", the \"festal origin\" theory suggests that in the \"play-excitement\" preceding or following a communal hunt or other group activity, everyone might have combined their voices in a comparable way, emphasising their mood of togetherness with such noises as rhythmic drumming and hand-clapping. Variably pitched voices would have formed conventional patterns, such that choral singing became an integral part of communal celebration.\n\nAlthough this was not yet speech, according to Langer, it developed the vocal capacities from which speech would later derive. There would be conventional modes of ululating, clapping or dancing appropriate to different festive occasions, each so intimately associated with \"that kind of occasion\" that it would tend to collectively uphold and embody the concept of it. Anyone hearing a snatch of sound from such a song would recall the associated occasion and mood. A melodic, rhythmic sequence of syllables conventionally associated with a certain type of celebration would become, in effect, its vocal mark. On that basis, certain familiar sound sequences would become \"symbolic\".\n\nIn support of all this, Langer cites ethnographic reports of tribal songs consisting entirely of \"rhythmic nonsense syllables\". She concedes that an English equivalent such as \"hey-nonny-nonny\", although perhaps suggestive of certain feelings or ideas, is neither noun, verb, adjective, nor any other syntactical part of speech. So long as articulate sound served only in the capacity of \"hey nonny-nonny\", \"hallelujah\" or \"alack-a-day\", it cannot yet have been speech. For that to arise, according to Langer, it was necessary for such sequences to be emitted increasingly \"out of context\" — outside the total situation that gave rise to them. Extending a set of associations from one cognitive context to another, completely different one, is the secret of \"metaphor\". Langer invokes an early version of what is nowadays termed \"grammaticalisation\" theory to show how, from, such a point of departure, syntactically complex speech might progressively have arisen.\n\nLanger acknowledges Emile Durkheim as having proposed a strikingly similar theory back in 1912. For recent thinking along broadly similar lines, see Steven Brown on \"musilanguage\", Chris Knight on \"ritual\" and \"play\", Jerome Lewis on \"mimicry\", Steven Mithen on \"Hmmmmm\" Bruce Richman on \"nonsense syllables\" and Alison Wray on \"holistic protolanguage\".\n\nThe term \"musilanguage\" (or \"hmmmmm\") refers to a pre-linguistic system of vocal communication from which (according to some scholars) \"both\" music \"and\" language later derived. The idea is that rhythmic, melodic, emotionally expressive vocal ritual helped bond coalitions and, over time, set up selection pressures for enhanced volitional control over the speech articulators. Patterns of synchronised choral chanting are imagined to have varied according to the occasion. For example, \"we're setting off to find honey\" might sound qualitatively different from \"we're setting off to hunt\" or \"we're grieving over our relative's death\". If social standing depended on maintaining a regular beat and harmonising one's own voice with that of everyone else, group members would have come under pressure to demonstrate their choral skills.\n\nArchaeologist Steven Mithen speculates that the Neanderthals possessed some such system, expressing themselves in a \"language\" known as \"Hmmmmm\", standing for Holistic, manipulative, multi-modal, musical and mimetic. In Bruce Richman's earlier version of essentially the same idea, frequent repetition of the same few songs by many voices made it easy for people to remember those sequences as whole units. Activities that a group of people were doing while they were vocalising together — activities that were important or striking or richly emotional — came to be associated with particular sound sequences, so that each time a fragment was heard, it evoked highly specific memories. The idea is that the earliest lexical items (words) started out as abbreviated fragments of what were originally communal songs.\n\nAs group members accumulated an expanding repertoire of songs for different occasions, interpersonal call-and-response patterns evolved along one trajectory to assume linguistic form. Meanwhile, along a divergent trajectory, polyphonic singing and other kinds of music became increasingly specialised and sophisticated.\n\nTo explain the establishment of syntactical speech, Richman cites English \"I wanna go home\". He imagines this to have been learned in the first instance not as a combinatorial sequence of free-standing words, but as a single stuck-together combination — the melodic sound people make to express \"feeling homesick\". Someone might sing \"I wanna go home\", prompting other voices to chime in with \"I need to go home\", \"I'd love to go home\", \"Let's go home\" and so forth. Note that one part of the song remains constant, while another is permitted to vary. If this theory is accepted, syntactically complex speech began evolving as each chanted mantra allowed for variation at a certain point, allowing for the insertion of an element from some other song. For example, while mourning during a funeral rite, someone might want to recall a memory of collecting honey with the deceased, signalling this at an appropriate moment with a fragment of the \"we're collecting honey\" song. Imagine that such practices became common. Meaning-laden utterances would now have become subject to a distinctively linguistic creative principle — that of recursive embedding.\n\nMany scholars associate the evolutionary emergence of speech with profound social, sexual, political and cultural developments. One view is that primate-style dominance needed to give way to a more cooperative and egalitarian lifestyle of the kind characteristic of modern hunter-gatherers.\n\nAccording to Michael Tomasello, the key cognitive capacity distinguishing \"Homo sapiens\" from our ape cousins is \"intersubjectivity\". This entails turn-taking and role-reversal: your partner strives to read your mind, you simultaneously strive to read theirs, and each of you makes a conscious effort to assist the other in the process. The outcome is that each partner forms a representation of the other's mind in which their own can be discerned by reflection.\n\nTomasello argues that this kind of bi-directional cognition is central to the very possibility of linguistic communication. Drawing on his research with both children and chimpanzees, he reports that human infants, from one year old onwards, begin viewing their own mind as if from the standpoint of others. He describes this as a cognitive revolution. Chimpanzees, as they grow up, never undergo such a revolution. The explanation, according to Tomasello, is that their evolved psychology is adapted to a deeply competitive way of life. Wild-living chimpanzees form despotic social hierarchies, most interactions involving calculations of dominance and submission. An adult chimp will strive to outwit its rivals by guessing at their intentions while blocking them from reciprocating. Since bi-directional intersubjective communication is impossible under such conditions, the cognitive capacities necessary for language don't evolve.\n\nIn the scenario favoured by David Erdal and Andrew Whiten, primate-style dominance provoked equal and opposite coalitionary resistance — \"counter-dominance.\" During the course of human evolution, increasingly effective strategies of rebellion against dominant individuals led to a compromise. While abandoning any attempt to dominate others, group members vigorously asserted their personal autonomy, maintaining their alliances to make potentially dominant individuals think twice. Within increasingly stable coalitions, according to this perspective, status began to be earned in novel ways, social rewards accruing to those perceived by their peers as especially cooperative and self-aware.\n\nWhile counter-dominance, according to this evolutionary narrative, culminates in a stalemate, anthropologist Christopher Boehm extends the logic a step further. Counter-dominance tips over at last into full-scale \"reverse dominance\". The rebellious coalition decisively overthrows the figure of the primate alpha-male. No dominance is allowed except that of the self-organised community as a whole.\n\nAs a result of this social and political change, hunter-gatherer egalitarianism is established. As children grow up, they are motivated by those around them to reverse perspective, engaging with other minds on the model of their own. Selection pressures favour such psychological innovations as imaginative empathy, joint attention, moral judgment, project-oriented collaboration and the ability to evaluate one's own behaviour from the standpoint of others. Underpinning enhanced probabilities of cultural transmission and cumulative cultural evolution, these developments culminate in the establishment of hunter-gatherer-style egalitarianism in association with intersubjective communication and cognition. It is in this social and political context that language evolves.\n\nAccording to Dean Falk's \"putting the baby down\" theory, vocal interactions between early hominin mothers and infants sparked a sequence of events that led, eventually, to our ancestors' earliest words. The basic idea is that evolving human mothers, unlike their monkey and ape counterparts, couldn't move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed reassurance that they were not being abandoned. Mothers responded by developing \"motherese\" — an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling and emotionally expressive contact calls. The argument is that language somehow developed out of all this.\n\nWhile this theory may explain a certain kind of infant-directed \"protolanguage\" — known today as \"motherese\" — it does little to solve the really difficult problem, which is the emergence among adults of syntactical speech. \n\nEvolutionary anthropologist Sarah Hrdy observes that only human mothers among great apes are willing to let another individual take hold of their own babies; further, we are routinely willing to let others babysit. She identifies lack of trust as the major factor preventing chimp, bonobo or gorilla mothers from doing the same: \"If ape mothers insist on carrying their babies everywhere ... it is because the available alternatives are not safe enough.\" The fundamental problem is that ape mothers (unlike monkey mothers who may often babysit) do not have female relatives nearby. The strong implication is that, in the course of \"Homo\" evolution, allocare could develop because \"Homo\" mothers did have female kin close by — in the first place, most reliably, their own mothers. Extending the Grandmother hypothesis, Hrdy argues that evolving \"Homo erectus\" females necessarily relied on female kin initially; this novel situation in ape evolution of mother, infant and mother's mother as allocarer provided the evolutionary ground for the emergence of intersubjectivity. She relates this onset of \"cooperative breeding in an ape\" to shifts in life history and slower child development, linked to the change in brain and body size from the 2 million year mark.\n\nPrimatologist Klaus Zuberbühler uses these ideas to help explain the emergence of vocal flexibility in the human species. Co-operative breeding would have compelled infants to struggle actively to gain the attention of caregivers, not all of whom would have been directly related. A basic primate repertoire of vocal signals may have been insufficient for this social challenge. Natural selection, according to this view, would have favoured babies with advanced vocal skills, beginning with babbling (which triggers positive responses in care-givers) and paving the way for the elaborate and unique speech abilities of modern humans.\n\nThese ideas might be linked to those of the renowned structural linguist Roman Jakobson, who claimed that \"the sucking activities of the child are accompanied by a slight nasal murmur, the only phonation to be produced when the lips are pressed to the mother's breast ... and the mouth is full\". He proposed that later in the infant's development, \"this phonatory reaction to nursing is reproduced as an anticipatory signal at the mere sight of food and finally as a manifestation of a desire to eat, or more generally, as an expression of discontent and impatient longing for missing food or absent nurser, and any ungranted wish.\" So, the action of opening and shutting the mouth, combined with the production of a nasal sound when the lips are closed, yielded the sound sequence \"Mama\", which may therefore count as the very first word. Peter MacNeilage sympathetically discusses this theory in his major book, The \"Origin of Speech\", linking it with Dean Falk's \"putting the baby down\" theory (see above). Needless to say, other scholars have suggested completely different candidates for \"Homo sapiens\"' very first word.\n\nWhile the biological language faculty is genetically inherited, actual languages or dialects are culturally transmitted, as are social norms, technological traditions and so forth. Biologists expect a robust co-evolutionary trajectory linking human genetic evolution with the evolution of culture. Individuals capable of rudimentary forms of protolanguage would have enjoyed enhanced access to cultural understandings, while these, conveyed in ways that young brains could readily learn, would in turn have become transmitted with increasing efficiency.\n\nIn some ways like beavers as they construct their dams, humans have always engaged in niche construction, creating novel environments to which they subsequently become adapted. Selection pressures associated with prior niches tend to become relaxed as humans depend increasingly on novel environments created continuously by their own productive activities. According to Steven Pinker, language is an adaptation to \"the cognitive niche\". Variations on the theme of ritual/speech co-evolution — according to which speech evolved for purposes of internal communication within a ritually constructed domain — have attempted to specify more precisely when, why and how this special niche was created by human collaborative activity.\n\n The Swiss scholar Ferdinand de Saussure founded linguistics as a twentieth-century professional discipline. Saussure regarded a language as a rule-governed system, much like a board game such as chess. In order to understand chess, he insisted, we must ignore such external factors as the weather prevailing during a particular session or the material composition of this or that piece. The game is autonomous with respect to its material embodiments. In the same way, when studying language, it's essential to focus on its internal structure as a social institution. External matters (\"e.g.\", the shape of the human tongue) are irrelevant from this standpoint. Saussure regarded 'speaking' \"(parole)\" as individual, ancillary and more or less accidental by comparison with \"language\" \"(langue)\", which he viewed as collective, systematic and essential.\n\nSaussure showed little interest in Darwin's theory of evolution by natural selection. Nor did he consider it worthwhile to speculate about how language might originally have evolved. Saussure's assumptions in fact cast doubt on the validity of narrowly conceived origins scenarios. His structuralist paradigm, when accepted in its original form, turns scholarly attention to a wider problem: how our species acquired the capacity to establish social institutions in general.\n\n In the United States, prior to and immediately following World War II, the dominant psychological paradigm was behaviourism. Within this conceptual framework, language was seen as a certain kind of behaviour — namely, verbal behaviour, to be studied much like any other kind of behaviour in the animal world. Rather as a laboratory rat learns how to find its way through an artificial maze, so a human child learns the verbal behaviour of the society into which it is born. The phonological, grammatical and other complexities of speech are in this sense \"external\" phenomena, inscribed into an initially unstructured brain. Language's emergence in \"Homo sapiens,\" from this perspective, presents no special theoretical challenge. Human behaviour, whether verbal or otherwise, illustrates the malleable nature of the mammalian — and especially the human — brain.\n\nNativism is the theory that humans are born with certain specialised cognitive modules enabling us to acquire highly complex bodies of knowledge such as the grammar of a language.\n\nFrom the mid-1950s onwards, Noam Chomsky, Jerry Fodor and others mounted what they conceptualised as a 'revolution' against behaviourism. Retrospectively, this became labelled 'the cognitive revolution'. Whereas behaviourism had denied the scientific validity of the concept of \"mind\", Chomsky replied that, in fact, the concept of \"body\" is more problematic. Behaviourists tended to view the child's brain as a \"tabula rasa\", initially lacking structure or cognitive content. According to B. F. Skinner, for example, richness of behavioural detail (whether verbal or non-verbal) emanated from the environment. Chomsky turned this idea on its head. The linguistic environment encountered by a young child, according to Chomsky's version of psychological nativism, is in fact hopelessly inadequate. No child could possibly acquire the complexities of grammar from such an impoverished source. Far from viewing language as wholly external, Chomsky re-conceptualised it as wholly internal. To explain how a child so rapidly and effortlessly acquires its natal language, he insisted, we must conclude that it comes into the world with the essentials of grammar already pre-installed. No other species, according to Chomsky, is genetically equipped with a language faculty — or indeed with anything remotely like one. The emergence of such a faculty in \"Homo sapiens\", from this standpoint, presents biological science with a major theoretical challenge.\n\nOne way to explain biological complexity is by reference to its inferred function. According to the influential philosopher John Austin, speech's primary function is action in the social world.\n\nSpeech acts, according to this body of theory, can be analysed on three different levels: locutionary, illocutionary and perlocutionary. An act is \"locutionary\" when viewed as the production of certain linguistic sounds — for example, practicing correct pronunciation in a foreign language. An act is \"illocutionary\" insofar as it constitutes an intervention in the world as jointly perceived or understood. Promising, marrying, divorcing, declaring, stating, authorising, announcing and so forth are all speech acts in this \"illocutionary\" sense. An act is \"perlocutionary\" when viewed in terms of its direct psychological effect on an audience. Frightening a baby by saying 'Boo!' would be an example of a \"perlocutionary\" act.\n\nFor Austin, \"doing things\" with words means, first and foremost, deploying \"illocutionary\" force. The secret of this is community participation or collusion. There must be a 'correct' (conventionally agreed) procedure, and all those concerned must accept that it has been properly followed. In the case of a priest declaring a couple to be man and wife, his words will have illocutionary force only if he is properly authorised and only if the ceremony is properly conducted, using words deemed appropriate to the occasion. Austin points out that should anyone attempt to baptize a penguin, the act would be null and void. For reasons which have nothing to do with physics, chemistry or biology, baptism is inappropriate to be applied to penguins, irrespective of the verbal formulation used.\n\nThis body of theory may have implications for speculative scenarios concerning the origins of speech. \"Doing things with words\" presupposes shared understandings and agreements pertaining not just to language but to social conduct more generally. Apes might produce sequences of structured sound, influencing one another in that way. To deploy \"illocutionary\" force, however, they would need to have entered a non-physical and non-biological realm — one of shared contractual and other intangibles. This novel cognitive domain consists of what philosophers term \"institutional facts\" — objective facts whose existence, paradoxically, depends on communal faith or belief. Few primatologists, evolutionary psychologists or anthropologists consider that nonhuman primates are capable of the necessary levels of joint attention, sustained commitment or collaboration in pursuit of future goals.\n\nBiosemiotics is a relatively new discipline, inspired in large part by the discovery of the genetic code in the early 1960s. Its basic assumption is that \"Homo sapiens\" is not alone in its reliance on codes and signs. Language and symbolic culture must have biological roots, hence semiotic principles must apply also in the animal world.\n\nThe discovery of the molecular structure of DNA apparently contradicted the idea that life could be explained, ultimately, in terms of the fundamental laws of physics. The letters of the genetic alphabet seemed to have \"meaning\", yet meaning is not a concept which has any place in physics. The natural science community initially solved this difficulty by invoking the concept of \"information\", treating information as independent of meaning. But a different solution to the puzzle was to recall that the laws of physics in themselves are never sufficient to explain natural phenomena. To explain, say, the unique physical and chemical characteristics of the planets in our solar system, scientists must work out how the laws of physics became constrained by particular sequences of events following the formation of the Sun.\n\nAccording to Howard Pattee — a pioneering figure in biosemiotics — the same principle applies to the evolution of life on earth, a process in which certain \"frozen accidents\" or \"natural constraints\" have from time to time drastically reduced the number of possible evolutionary outcomes. Codes, when they prove to be stable over evolutionary time, are constraints of this kind. The most fundamental such \"frozen accident\" was the emergence of DNA as a self-replicating molecule, but the history of life on earth has been characterised by a succession of comparably dramatic events, each of which can be conceptualised as the emergence of a new code. From this perspective, the evolutionary emergence of spoken language was one more event of essentially the same kind.\n\nIn 1975, the Israeli theoretical biologist Amotz Zahavi proposed a novel theory which, although controversial, has come to dominate Darwinian thinking on how signals evolve. Zahavi's \"handicap principle\" states that to be effective, signals must be reliable; to be reliable, the bodily investment in them must be so high as to make cheating unprofitable.\n\nParadoxically, if this logic is accepted, signals in nature evolve not to be efficient but, on the contrary, to be elaborate and wasteful of time and energy. A peacock's tail is the classic illustration. Zahavi's theory is that since peahens are on the look-out for male braggarts and cheats, they insist on a display of quality so costly that only a genuinely fit peacock could afford to pay. Needless to say, not all signals in the animal world are quite as elaborate as a peacock's tail. But if Zahavi is correct, all require some bodily investment — an expenditure of time and energy which \"handicaps\" the signaller in some way.\n\nAnimal vocalisations (according to Zahavi) are reliable because they are faithful reflections of the state of the signaller's body. To switch from an honest to a deceitful call, the animal would have to adopt a different bodily posture. Since every bodily action has its own optimal starting position, changing that position to produce a false message would interfere with the task of carrying out the action really intended. The gains made by cheating would not make up for the losses incurred by assuming an improper posture — and so the phony message turns out to be not worth its price. This may explain, in particular, why ape and monkey vocal signals have evolved to be so strikingly inflexible when compared with the varied speech sounds produced by the human tongue. The apparent inflexibility of chimpanzee vocalisations may strike the human observer as surprising until we realise that being inflexible is necessarily bound up with being perceptibly honest in the sense of \"hard-to-fake\".\n\nIf we accept this theory, the emergence of speech becomes theoretically impossible. Communication of this kind just cannot evolve. The problem is that words are cheap. Nothing about their acoustic features can reassure listeners that they are genuine and not fakes. Any strategy of reliance on someone else's tongue — perhaps the most flexible organ in the body — presupposes unprecedented levels of honesty and trust. To date, Darwinian thinkers have found it difficult to explain the requisite levels of community-wide cooperation and trust.\n\nAn influential standard textbook is \"Animal Signals,\" by John Maynard Smith and David Harper. These authors divide the costs of communication into two components, (1) the investment necessary to ensure transmission of a discernible signal; (2) the investment necessary to guarantee that each signal is reliable and not a fake. The authors point out that although costs in the second category may be relatively low, they are not zero. Even in relatively relaxed, cooperative social contexts — for example, when communication is occurring between genetic kin — some investment must be made to guarantee reliability. In short, the notion of super-efficient communication — eliminating all costs except those necessary for successful transmission — is biologically unrealistic. Yet speech comes precisely into this category.\n\nCognitive linguistics views linguistic structure as arising continuously out of usage. Speakers are forever discovering new ways to convey meanings by producing sounds, and in some cases these novel strategies become conventionalised. Between phonological structure and semantic structure there is no causal relationship. Instead, each novel pairing of sound and meaning involves an imaginative leap.\n\nIn their book, \"Metaphors We Live By,\" George Lakoff and Mark Johnson helped pioneer this approach, claiming that \"metaphor\" is what makes human thought special. All language, they argued, is permeated with metaphor, whose use in fact \"constitutes\" distinctively human — that is, distinctively abstract — thought. To conceptualise things which cannot be directly perceived — intangibles such as time, life, reason, mind, society or justice — we have no choice but to set out from more concrete and directly perceptible phenomena such as motion, location, distance, size and so forth. In all cultures across the world, according to Lakoff and Johnson, people resort to such familiar metaphors as \"ideas are locations, thinking is moving\" and \"mind is body\". For example, we might express the idea of \"arriving at a crucial point in our argument\" by proceeding as if literally travelling from one physical location to the next.\n\nMetaphors, by definition, are not literally true. Strictly speaking, they are fictions — from a pedantic standpoint, even falsehoods. But if we couldn't resort to metaphorical fictions, it's doubtful whether we could even form conceptual representations of such nebulous phenomena as \"ideas\", thoughts\", \"minds\", and so forth.\n\nThe bearing of these ideas on current thinking on speech origins remains unclear. One suggestion is that ape communication tends to resist metaphor for social reasons. Since they inhabit a Darwinian (as opposed to morally regulated) social world, these animals are under strong competitive pressure \"not\" to accept patent fictions as valid communicative currency. Ape vocal communication tends to be inflexible, marginalising the ultra-flexible tongue, precisely because listeners treat with suspicion any signal which might prove to be a fake. Such insistence on perceptible veracity is clearly incompatible with metaphoric usage. An implication is that neither articulate speech nor distinctively human abstract thought could have begun evolving until our ancestors had become more cooperative and trusting of one another's communicative intentions.\n\nWhen people converse with one another, according to the American philosopher John Searle, they're making moves, not in the real world which other species inhabit, but in a shared virtual realm peculiar to ourselves. Unlike the deployment of muscular effort to move a physical object, the deployment of illocutionary force requires no physical effort (except movement of the tongue/mouth to produce speech) and produces no effect which any measuring device could detect. Instead, our action takes place on a quite different level — that of \"social\" reality. This kind of reality is in one sense hallucinatory, being a product of collective intentionality. It consists, not of \"brute facts\" — facts which exist anyway, irrespective of anyone's belief — but of \"institutional facts\", which \"exist\" only if you believe in them. Government, marriage, citizenship and money are examples of \"institutional facts\". One can distinguish between \"brute\" facts and \"institutional\" ones by applying a simple test. Suppose no one believed in the fact — would it still be true? If the answer is \"yes\", it's \"brute\". If the answer is \"no\", it's \"institutional\".\n\nThe facts of language in general and of speech in particular are, from this perspective, \"institutional\" rather than \"brute\". The semantic meaning of a word, for example, is whatever its users imagine it to be. To \"do things with words\" is to operate in a virtual world which seems real because we share it in common. In this incorporeal world, the laws of physics, chemistry and biology do not apply. That explains why illocutionary force can be deployed without exerting muscular effort. Apes and monkeys inhabit the \"brute\" world. To make an impact, they must scream, bark, threaten, seduce or in other ways invest bodily effort. If they were invited to play chess, they would be unable to resist throwing their pieces at one another. Speech is not like that. A few movements of the tongue, under appropriate conditions, can be sufficient to open parliament, annul a marriage, confer a knighthood or declare war. To explain, on a Darwinian basis, how such apparent magic first began to work, we must ask how, when and why \"Homo sapiens\" succeeded in establishing the wider domain of institutional facts.\n\n\"Brute facts\", in the terminology of speech act philosopher John Searle, are facts which are true anyway, regardless of human belief. Suppose you don't believe in gravity: jump over a cliff and you'll still fall. Natural science is the study of facts of this kind. \"Institutional facts\" are fictions accorded factual status within human social institutions. Monetary and commercial facts are fictions of this kind. The complexities of today's global currency system are facts only while we believe in them: suspend the belief and the facts correspondingly dissolve. Yet although institutional facts rest on human belief, that doesn't make them mere distortions or hallucinations. Take my confidence that these two five-pound banknotes in my pocket are worth ten pounds. That's not merely my subjective belief: it's an objective, indisputable fact. But now imagine a collapse of public confidence in the currency system. Suddenly, the realities in my pocket dissolve.\n\nScholars who doubt the scientific validity of the notion of \"institutional facts\" include Noam Chomsky, for whom language is not social. In Chomsky's view, language is a natural object (a component of the individual brain) and its study, therefore, a branch of natural science. In explaining the origin of language, scholars in this intellectual camp invoke non-social developments — in Chomsky's case, a random genetic mutation. Chomsky argues that language might exist inside the brain of a single mutant gorilla even if no one else believed in it, even if no one else existed apart from the mutant — and even if the gorilla in question remained unaware of its existence, never actually speaking. In the opposite philosophical camp are those who, in the tradition of Ferdinand de Saussure, argue that if no one believed in words or rules, they simply would not exist. These scholars, correspondingly, regard language as essentially institutional, concluding that linguistics should be considered a topic within social science. In explaining the evolutionary emergence of language, scholars in this intellectual camp tend to invoke profound changes in social relationships.\n\nCriticism. Darwinian scientists today see little value in the traditional distinction between \"natural\" and \"social\" science. Darwinism in its modern form is the study of cooperation and competition in nature — a topic which is intrinsically social. Against this background, there is an increasing awareness among evolutionary linguists and Darwinian anthropologists that traditional inter-disciplinary barriers can have damaging consequences for investigations into the origins of speech.\n\n"}
{"id": "40142750", "url": "https://en.wikipedia.org/wiki?curid=40142750", "title": "Peng's Coefficient", "text": "Peng's Coefficient\n\nPeng’s Coefficient is an economic term which refers to the proportion of an individual’s spending on culture- and spirit-related products or services, such as books, movie, opera, concert, travelling, training and so forth, to her/his total expenditure. Peng’s Coefficient is inversely proportional to Engel’s Coefficient, because the more proportion people spend on food, the less proportion on culture and spirit.\nThe concept was named after its creator Peng Bing from China.\nEquation: P=S/T\n"}
{"id": "3050350", "url": "https://en.wikipedia.org/wiki?curid=3050350", "title": "Plural society", "text": "Plural society\n\nA plural society is defined by Fredrik Barth as a society combining ethnic contrasts: the economic interdependence of those groups, and their ecological specialization (i.e., use of different environmental resources by each ethnic group). The ecological interdependence, or the lack of competition, between ethnic groups may be based on the different activities in the same region or on long–term occupation of different regions in the\nDefined by J S Furnivall as a medley of peoples - European, Chinese, Indian and native, who do mix but do not combine. Each group holds by its own religion, its own culture and language, its own ideas and ways. As individuals they meet, but only in the marketplace in buying and selling. There is a plural society, with different sections of the community living side by side, within the same political unit.\n\nDuring research about plural societies, Asim Ejaz, Student of M.phil Political Science in Islamia university bahawalpur, Pakistan, presented his analytical summary about the book of Arend Lijphart, \"democracy in plural societies\" that it is so much difficult to achieve and stable democratic government in plural society. As Aristotle says about stable governing system that, “a state aims at being, as far as it can be, a society composed of equal & peers”. For the stability of democratic regimes, there must be social homogeneity and political consensus among the deep social divisions, and, there must be termination of political differences. There are these criteria, because, the factors that help in producing instability and breakdown of democracies.\nArend Lijphart, therefore, used a particular form of democracy, “Consociational Democracy”, which is, according to him, may be difficult but it is not at all impossible to achieve and maintain stable democratic government in plural societies.\n\nConsociational democracy can be characterized by the cooperative attitude and behavior of the leaders of the different segments of the population. In other meanings, there will be elite cooperation. This model of Consociational democracy is both, normative and an empirical. In Austria, Belgium, Netherland and Switzerland, there are sharp political divisions, but due to Consoiciational democracy, there is existence of political stability. In Austria, political stability can be observed in the forms of Catholic-Socialist elite cooperation and grand coalition.\n\nIn non-Western countries, as Arend Lijphart highlights twin problems, and there are, sharp cleavages of various kind and political stability. For the successful democratic regimes in the third world, due to plural societies, Consociational democracy is based, also on normative model. A Plural society is a society, divided by segmental cleavages, and, political stability is characterized by system maintenance, legitimacy, civil order and effectiveness. Without these four elements, which are also interdependent, political stability cannot exist. According to Gabriel Almond, there are four types of political systems;\n\n1) Anglo-American Political System\n2) Continental European Political System\n3) Pre-Industrial Political System\n4) Totalitarian Political System.\n\nHe says that Anglo-American and Continental European Political systems show democratic regimes. The Anglo-American political system is a homogenous and secular political system, while the Continental European political system is characterized by a fragmentation of political culture due to plural societies within European countries.\n\nAccording to Gabriel Almond, Separation of power doctrine is also concerned with political stability. He extends the idea of “separation of power” from three formal branches of government, executives and legislature, to informal political subcultures like parties, interest groups and the media of communication. He much more emphasizes on input structures than the output structures.\n\nDuverger and Neumann argue that there is a close relationship between the number of parties and democratic stability, but a two party system not only seems to correspond to the nature of things because it can moderate better than multiparty systems. In other words, a two party system is the best aggregation. In Switzerland, there is a multiparty system, while in Austria, there is a two party system.\n\nArend Lijphart says that there are deep divisions between different segments of the population and absence of a unifying consensus in most of the Asian, African and South American countries like Guyana, Surinam and Trinidad. According to Cliffard Geertz, Communal attachment is called “primordial loyalties”, which may be based on language, religion, custom, region, race or assumed blood ties. Each communal group hold its assumed ties, therefore there is political instability and breakdown of democracy up until now.\n\nHe argues that due to political development, western countries have created homogeneity among their plural societies, as like idealize British society. But Gabriel Almond says that, in Continental European political system, there is no secularism and political homogeneity, but there is cultural homogeneity. He argues that, non-western countries would become more comprehensive and less remote they use this continental type, which is based on a multi-racial (multi-national) society and lacking in strong consensus.\n\nFurnivall says that democracy is achieved by the European countries with the help of Consociationalism, and, there is fulfillment of the requirements and demands of the divided societies through appropriate processes. On the other hand, in non-western countries, there is lack of strength in social will and social unity due to divided society, and, it is dangerous for both, the democracy and a considerable degree of political unity.\n\nAs Arend Lijhpart argues that there is constitutionalization for the segments of plural societies, and its better solution is consociational or semi-consociational democratic system. This system provides the facility of Mutual Veto regarding the decision making process on the specific issues within the country, to all the segments of society with equality. but Arend Lijhpart highlights Malaysia and Lebanon for its perfect example. In Lebanon, there are Shia's and Sunni's in the Muslim Segment while Christians are in the minority. Similarly in Malaysia, there are Chinese with local Communities.\n\n\n"}
{"id": "31729317", "url": "https://en.wikipedia.org/wiki?curid=31729317", "title": "Pluriculturalism", "text": "Pluriculturalism\n\nPluriculturalism is an approach to the self and others as complex rich beings which act and react from the perspective of multiple identifications. In this case, identity or identities are the by-products of experiences in different cultures. As an effect, multiple identifications create a unique personality instead of or more than a static identity. It is based on multiple-identity, wherein people have multiple identities who belong to multiple groups with different degrees of identification. The term pluricultural competence is a consequence of the idea of plurilingualism. There is a distinction between pluriculturalism and multiculturalism.\n\nSpain has been referred to as a pluricultural country, due to its nationalisms and regionalisms.\n\n"}
{"id": "47077127", "url": "https://en.wikipedia.org/wiki?curid=47077127", "title": "Project 1975", "text": "Project 1975\n\nProject 1975 started in 2010 as a two-year project based in the Netherlands with the intent to explore the relationships between contemporary art and postcolonialism. With this project Stedelijk Museum Bureau Amsterdam (SMBA) explored the role of art and visual culture in the context of colonial practices.\nThe project consisted of multiple exhibitions, seminars, reading groups, articles, and a blog. \n\"1975\" in the title refers to the year that Suriname gained independence (the independent Republic of Surinam was founded in 1975) and the Netherlands thus became to some extent \"postcolonial\"..\n\nThe project broadened SMBA’s focus, traditionally on artists based in Amsterdam, to include artists and people who were new to the city but wanted to contribute to the artistic and cultural environment. Consequently, themes that had not previously been addressed in art institutions in Amsterdam found a place to be discussed at SMBA. Many questions were raised in this project. Artists and critics responded to questions such as \"Do colonial mindsets persist in art and in its institutions?\".\n\nExhibitions that took place in the context of Project 1975 were: \"See Reason\", \"Identity Bluffs\", \"The Marx Lounge\", \"Mounira Al Solh & Bassam Ramlawi\", \"Informality, Art, Economy & Precarity\", \" Vincent Vulsma - A Sign of Autumn \", \"The Jinn - Tala Madani\", \"Any other Business – Nicoline van Harskamp\", \"Bart Groenendaal, Stefan Ruitenbeek, Quinsy Gario\", \"The Memories are Present\", \"Time, Trade & Travel\" and finally \"Hollandaise - a Journey into an Iconic Fabric\".\n\nThe project was finalized with a publication: \"Project 1975. Contemporary Art and the Postcolonial Unconscious \" which includes (visual) documentation of the project, interviews between the curators and artists and essays.\n\nThe Memories are Present and the video programme \"Really Exotic\" ran from 16 June to 12 August 2012 and was curated by Kerstin Winking (SMBA). The participating artists in the exhibition were Artun Alaska Arasli, and Christoph Westermeier. The focus in this exhibition was on the museum as a bearer and communicator of knowledge and interrogated institutional divisions. Central attention was paid to the ways in which different institutions collect, categorise and display objects. The exhibition challenged the binary oppositions still prevalent in many institutions between “art objects” and “ethnographic objects”. \nThe video programme \"Really Exotic\" focused on the notion of the exotic experience. Presented were works that could convey or tell the viewer more about this type of experience. The videoprogram was organised by Joram\nKraaijeveld and Kerstin Winking (SMBA) in collaboration with Marthe Singelenberg\n(Filmtheater Kriterion).\n\nThe exhibition Time, Trade & Travel took place from 25 August to 21 October 2012 and was organized in collaboration with the Nubuke Foundation, Accra, Ghana. Participating artists were: Bernard Akoi-Jackson, Dorothy Akpene Amenuke, Serge Clottey, Zachary Formwalt, Iris Kensmil, Aukje Koks, Navid Nuur, Jeremiah Quarshie, kąrî’kạchä seidóu, Katarina Zdjelar. It was the result of an active exchange of knowledge between artists and curators from SMBA and Nubiuke Foundation, Accra. The curators from both institutions as well as the participating artists visited each other in their work and cultural environment. The title of this exhibition refers to the complicated aspects of international trade and traffic with their capitalist forces and influence on life and art. The participating artists in this exhibition set out to discover historical encounters between Africans and Europeans, the subsequent trade and cultual relationships that evolved from these contacts and the extent to which these cultural and economic relationships are still of influence today.\nThe exhibition travelled on to the Nubuke Foundation in Accra, Ghana.\n\nThe exhibition was curated by: Jelle Bouwhuis and Kerstin Winking (Stedelijk Museum Bureau Amsterdam), Kofi Setordji and Odile Tevie (Nubuke Foundation). Time, Trade & Travel was made possible in part by contributions from the Mondrian Fund, the Amsterdam Fund for the Arts, HIVOS and SNS REAAL Fund.\n\nThe exhibition Hollandaise: a journey into an iconic fabric took place at SMBA from 30 November 2012 to 6 January 2013. The exhibition travelled on to Raw Material Company, Dakar (Senegal), where it was on view from 10 April – 1 June 2013.\n\nThe curator for this exhibition was Koyo Kouoh. Participating artists in this exhibition were Godfried Donkor, Abdoulaye Konaté, Wendelien van Oldenborgh, Willem de Rooij, Billie Zangewa.\n\nFor this exhibition Kouoh commissioned five artists, all working in the medium of textile, in Europe and Africa to make new works on the subject of “Hollandaise” or “Dutch wax”, the colourful wax print fabrics that are most often regarded as typically African. In the context of this exhibition a catalogue, edited by Kouoh, and an SMBA Newsletter were published.\n\n"}
{"id": "1490408", "url": "https://en.wikipedia.org/wiki?curid=1490408", "title": "Scrupulosity", "text": "Scrupulosity\n\nScrupulosity is characterized by pathological guilt about moral or religious issues. It is personally distressing, objectively dysfunctional, and often accompanied by significant impairment in social functioning. It is typically conceptualized as a moral or religious form of obsessive–compulsive disorder (OCD), although this categorization is empirically disputable.\n\nThe term is derived from the Latin \"scrupulum\", a sharp stone, implying a stabbing pain on the conscience. Scrupulosity was formerly called \"scruples\" in religious contexts, but the word \"scruples\" now commonly refers to a troubling of the conscience rather than to the disorder.\n\nAs a personality trait, scrupulosity is a recognized diagnostic criterion for obsessive–compulsive personality disorder. It is sometimes called \"scrupulousness\", but that word properly applies to the positive trait of having scruples.\n\nScrupulosity is a modern-day psychological problem that echoes a traditional use of the term \"scruples\" in a religious context, e.g. by Roman Catholics, to mean obsessive concern with one's own sins and compulsive performance of religious devotion. This use of the term dates to the 12th century. Several historical and religious figures suffered from doubts of sin, and expressed their pains. Ignatius of Loyola, founder of the Jesuits, wrote \"After I have trodden upon a cross formed by two straws ... there comes to me from without a thought that I have sinned ... this is probably a scruple and temptation suggested by the enemy.\" Alphonsus Liguori, the Redemptorists' founder, wrote of it as \"groundless fear of sinning that arises from 'erroneous ideas'\". Although the condition was lifelong for Loyola and Liguori, Thérèse of Lisieux stated that she recovered from her condition after 18 months, writing \"One would have to pass through this martyrdom to understand it well, and for me to express what I experienced for a year and a half would be impossible.\" Martin Luther also suffered from obsessive doubts; in his mind, his omitting the word \"enim\" (\"for\") during the Eucharist was as horrible as laziness, divorce, or murdering one's parent.\n\nAlthough historical religious figures such as Loyola, Luther and John Bunyan are commonly cited as examples of scrupulosity in modern self-help books, some of these retrospective diagnoses may be deeply ahistorical: these figures' obsession with salvation may have been excessive by modern standards, but that does not mean that it was pathological.\n\nScrupulosity's first known public description as a disorder was in 1691, by John Moore, who called it \"religious melancholy\" and said it made people \"fear, that what they do, is so defective and unfit to be presented unto God, that he will not accept it\". Loyola, Liguori, the French confessor R.P. Duguet, and other religious authorities and figures attempted to develop solutions and coping mechanisms; the monthly newsletter \"Scrupulous Anonymous\", published by the followers of Liguori, has been used as an adjunct to therapy. In the 19th century, Christian spiritual advisors in the U.S. and Britain became worried that scrupulosity was not only a sin in itself, but also led to sin, by attacking the virtues of faith, hope, and charity. Studies in the mid-20th century reported that scrupulosity was a major problem among American Catholics, with up to 25 per cent of high school students affected; commentators at the time asserted that this was an increase over previous levels.\n\nStarting in the 20th century, individuals with scrupulosity in the U.S. and Britain increasingly began looking to psychiatrists, rather than to religious advisors, for help with the condition.\n\nIn scrupulosity, a person's obsessions focus on moral or religious fears, such as the fear of being an evil person or the fear of divine retribution for sin. Although it can affect nonreligious people, it is usually related to religious beliefs. In the strict sense, not all obsessive–compulsive behaviors related to religion are instances of scrupulosity: strictly speaking, for example, scrupulosity is not present in people who repeat religious requirements merely to be sure that they were done properly.\n\nTreatment is similar to that for other forms of obsessive–compulsive disorder. Exposure and response prevention (ERP), a form of behavior therapy, is widely used for OCD in general and may be promising for scrupulosity in particular. ERP is based on the idea that deliberate repeated exposure to obsessional stimuli lessens anxiety, and that avoiding rituals lowers the urge to behave compulsively. For example, with ERP a person obsessed by blasphemous thoughts while reading the Bible would practice reading the Bible. However, ERP is considerably harder to implement than with other disorders, because scrupulosity often involves spiritual issues that are not specific situations and objects. For example, ERP is not appropriate for a man obsessed by feelings that God has rejected and is punishing him. Cognitive therapy may be appropriate when ERP is not feasible. Other therapy strategies include noting contradictions between the compulsive behaviors and moral or religious teachings, and informing individuals that for centuries religious figures have suggested strategies similar to ERP. Religious counseling may be an additional way to readjust beliefs associated with the disorder, though it may also stimulate greater anxiety.\n\nLittle evidence is available on the use of medications to treat scrupulosity. Although serotonergic medications are often used to treat OCD, studies of pharmacologic treatment of scrupulosity in particular have produced so few results that even tentative recommendations cannot be made.\n\nTreatment of scrupulosity in children has not been investigated to the extent it has been studied in adults, and one of the factors that makes the treatment difficult is the fine line the therapist must walk between engaging and offending the client.\n\nThe prevalence of scrupulosity is speculative. Available data do not permit reliable estimates, and available analyses mostly disregard associations with age or with gender, and have not reliably addressed associations with geography or ethnicity. Available data suggest that the prevalence of obsessive–compulsive disorder does not differ by culture, except where prevalence rates differ for all psychiatric disorders. No association between OCD and depth of religious beliefs has been demonstrated, although data are scarce. There are large regional differences in the percentage of OCD patients who have religious obsessions or compulsions, ranging from 0–7% in countries like the U.K. and Singapore, to 40–60% in traditional Muslim and orthodox Jewish populations.\n\n"}
{"id": "8684290", "url": "https://en.wikipedia.org/wiki?curid=8684290", "title": "Shorter Jewish Encyclopedia", "text": "Shorter Jewish Encyclopedia\n\nThe Shorter Jewish Encyclopedia (\"SJE\" - , Kratkaya Yevreyskaya Entsiklopedia) was published in 11 volumes in Jerusalem from 1976 to 2005 in Russian by the Society for Research on Jewish Communities with the support of Hebrew University in Jerusalem. The \"SJE\" is the only comprehensive encyclopedia on Judaism published in Russian, and followed an almost 70-year gap following the publication of the \"Yevreyskaya Entsiklopedia\" (\"Encyclopedia Judaica\") of Brokhaus and Efron in Saint Petersburg in 1908.\nAlthough it was originally planned as an abridged translation of the English-language \"Encyclopedia Judaica\", it became clear as the work progressed that readers raised in the Soviet Union would not be familiar with the concepts lying at the foundation of the cultural and historical system known as Jewish civilization. Therefore, these concepts were elaborated on in greater detail in the \"SJE\", and terms were introduced which lacked equivalents in modern Russian. Most personal and geographic names (in Israel) from the Bible are given in the accepted Hebrew form. The conceptual foundation of the \"SJE\" is characterized by a thematic bipolarity: Eretz Israel, and in particular the State of Israel on the one hand, and Russian (i.e., Soviet) Jewry on the other, which nevertheless does not exclude the broad scope of different aspects of the lives and history of Jews in all the other countries of the diaspora.\n\nA group of editors worked on the \"SJE\" who prepared articles with the participation of invited specialists and also academic consultants, including the well-known Israeli academics and public figures Shraga Abramson, Mordechai Altschuler, Shlomo Pines, Hayim Tadmor, Chone Shmeruk, Hayyim Schirmann, Menachem Stern, Yaakov Tsur, Yaakov Landau, Israel Bartal, and Michael Liebman. Chairing the editorial board were Shmuel Ettinger and Haim Beinhart. Chief editors were Yitzhak Oren (Nadel), Michael Zand, Naftali Prat, and Ari Avner. The senior academic editors were Peretz Hein, Yosef Glozman, Amnon Ginzai, and Mark Kipnis. The managing editors were Ella Slivkina (vol. 1-10) and Marina Gutgarts.\n\nIn practical terms, the \"SJE\" was no longer \"shorter,\" and aside from the 11 volumes a \"Jewish calendar juxtaposed with a Gregorian calendar (1948-2048)\" was published as a pamphlet along with three supplements. Volume 10 also contains a \"thematic bibliographic index\" with 2,114 items. There are more than 5,300 vocabulary entries, and the total number of words exceeds six million. Volume 11, the final one, included an alphabetized index of subjects, including geographic and personal names and events with references to the volume and column where they are located. A system of citations indicating links between concepts serves as the only cross-reference in the entire corpus of the encyclopedia.\n\nIn 1996, the Society for Research on Jewish Communities undertook a reprinting of the first seven volumes of the \"SJE\", which were printed by the printing and publishing house Krasnyj Proletarij in Moscow.\n\nIn 2005, the \"Electronic Jewish Encyclopedia\" (EJE), (Elektronnaja Evrejskaja Entsiklopedia) was made available on the internet, presenting an expanded and more precise version of the \"SJE\". Work on the \"EJE\" continues today.\n"}
{"id": "35836108", "url": "https://en.wikipedia.org/wiki?curid=35836108", "title": "Social networking pedagogy", "text": "Social networking pedagogy\n\nSocial Networking Pedagogy is a philosophy of education described by curriculum theorists Thomas Patrick Huston and Hallie DeCatherine Jones as a form of learner-driven participatory culture where the user’s experience is reciprocal, consisting of a range of input and output experiences mediated by social networking technologies. The range of individual and unique differentiated user experiences form a composite impression on the individual which functions as an epistemic form of how we come to know and experience the world and its social, cultural, political and economic realities. \n\nIn educational settings, social networking pedagogical approaches place social networking technologies at their core, thus presenting expansive opportunities for education to extend the curriculum beyond the traditional walls of educational institutions by reaching into the daily-lived experiences of individuals. The archival features and lack of temporal constraints associated with Internet technologies sustain continually changing currents that educators can connect to student-driven interests by engaging them with trending topics. Through Social Network Pedagogy, educators can develop a teaching praxis where the classroom and social space blend and individuals’ creative contributions are acknowledged for their cultural value. \n\nJones and Huston began theorizing the development of Social Networking Pedagogy during their tenure as doctoral students at the Indiana University School of Education where they were colleagues in the Department of Curriculum and Instruction. They point to the fields of cultural studies and critical pedagogy, especially the contributions of David Trend with his book Cultural Pedagogy\" Art, Education, Politics and critical theorist Henry Giroux.\n"}
{"id": "17569583", "url": "https://en.wikipedia.org/wiki?curid=17569583", "title": "Sociology of literature", "text": "Sociology of literature\n\nThe sociology of literature is a subfield of the sociology of culture. It studies the social production of literature and its social implications. A notable example is Pierre Bourdieu's 1992 \"Les Règles de L'Art: Genèse et Structure du Champ Littéraire\", translated by Susan Emanuel as \"Rules of Art: Genesis and Structure of the Literary Field\" (1996).\n\nNone of the 'founding fathers' of sociology produced a detailed study of literature, but they did develop ideas that were subsequently applied to literature by others. Karl Marx's theory of ideology has been directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Max Weber's theory of modernity as cultural rationalisation, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Theodor Adorno and Jürgen Habermas. Emile Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's work is clearly indebted to Marx, Weber and Durkheim\n\nAn important first step in the sociology of literature was taken by Georg Lukács's \"The Theory of the Novel\", first published in German in 1916, in the \"Zeitschrift fur Aesthetik und Allgemeine Kunstwissenschaft\". In 1920 it was republished in book form and this version strongly influenced the Frankfurt School. A second edition, published in 1962, was similarly influential on French structuralism. \"The Theory of the Novel\" argued that, whilst the classical epic poem had given form to a totality of life pregiven in reality by the social integration of classical civilisation, the modern novel had become 'the epic of an age in which the extensive totality of life is no longer directly given'. The novel form is therefore organised around the problematic hero in pursuit of problematic values within a problematic world. \n\nLukács's second distinctive contribution to the sociology of literature was \"The Historical Novel\", written in German but first published in Russian in 1937, which appeared in English translation in 1962. Here, Lukács argued that the early 19th century historical novel's central achievement was to represent realistically the differences between pre-capitalist past and capitalist present. This was not a matter of individual talent, but of collective historical experience, because the French Revolution and the revolutionary and Napoleonic wars had made history for the first time a mass experience. He went on to argue that the success of the 1848 revolutions led to the decline of the historical novel into 'decorative monumentalization' and the 'making private of history'. The key figures in the historical novel were thus those of the early 19th century, especially Sir Walter Scott.\n\nLukács was an important influence on Lucien Goldmann's \"Towards a Sociology of the Novel\", Alan Swingewood's discussion of the sociology of the novel in Part 3 of Laurenson and Swingewood's \"The Sociology of Literature\" and Franco Moretti's \"Signs Taken for Wonders\".\n\nFounded in 1923, the Institute for Social Research at the University of Frankfurt developed a distinctive kind of 'critical sociology' indebted to Marx, Weber and Freud. Leading Frankfurt School critics who worked on literature included Adorno, Walter Benjamin and Leo Löwenthal. Adorno's \"Notes to Literature\", Benjamin's \"The Origin of German Tragic Drama\" and Löwentahl's \"Literature and the Image of Man\" were each influential studies in the sociology of literature. Löwenthal continued this work at the University of California, Berkeley, during the 1950s.\n\nAdorno's \"Notes to Literature\" is a collection of essays, the most influential of which is probably 'On Lyric Poetry and Society'. It argued that poetic thought is a reaction against the commodification and reification of modern life, citing Goethe and Baudelaire as examples. Benjamin's \"The Origin of German Tragic Drama\" argued that the extreme 'sovereign violence' of the 16th and 17th century German 'Trauerspiel' (literally mourning play, less literally tragedy) playwrights expressed the historical realities of princely power far better than had classical tragedy. \n\nHabermas succeeded Adorno to the Chair of Sociology and Philosophy at Frankfurt. Habermas's first major work, \"Strukturwandel der Öffentlichkeit\" was published in German in 1962, and in English translation as \"The Structural Transformation of the Public Sphere\" in 1989. It attempted to explain the socio-historical emergence of middle-class public opinion in the seventeenth and eighteenth centuries. Developing a new kind of institutional sociology of literature, it argued that the public sphere had been organised around literary salons in France, learned and literary societies in Germany, and coffee houses in England. These institutions sustained the early novel, newspaper and periodical press.\n\nPeter Bürger was Professor of French and Comparative Literature at the University of Bremen. His \"Theorie der Avantgarde\" was published in German in 1974 and in English translation in 1984. Like Habermas, Bürger was interested in the institutional sociology of literature and art. He postulated a historical typology of aesthetic social relations, measured along three main axes, the function of the artwork, its mode of production and its mode of reception. This gave him three main kinds of art, sacral, courtly and bourgeois. Bourgeois art, he argued, had as its function individual self-understanding and was produced and received individually. It became a celebration in form of the liberation of art from religion, the court and, eventually, even the bourgeoisie. Modernist art was thus an autonomous social 'institution', the preserve of an increasingly autonomous intellectual class. The 'historical avant-garde' of the inter-war years developed as a movement within and against modernism, he concluded, as an ultimately unsuccessful revolt against precisely this autonomy.\n\nHabermas adopts a very similar approach in his own account of the avant-garde.\n\nRobert Escarpit was Professor of Comparative Literature at the University of Bordeaux and founder of the Centre for the Sociology of Literary Facts. His works included \"The Sociology of Literature\", published in French in 1958 and in English translation in 1971, and \"The Book Revolution\", published in French in 1965 and in English in 1966. In Durkheimian fashion, Escarpit aimed to concern himself only with the externally defined 'social facts' of literature, especially those registered in the book trade. His focus fell on the 'community of writers', understood in aggregate as 'generations' and 'teams'. He extended the definition of literature to include all 'non-functional' writing and also insisted that literary success resulted from 'a convergence of intentions between author and reader'.\n\nAnalogously empirical studies of the sociology of the book trade were carried out by Lewis Coser in the United States and Peter H. Mann in Britain.\n\nLucien Febvre and Henri-Jean Martin's \"L'Apparition du livre\", first published in French 1958 and in English translation as \"The Coming of the Book\" in 1976, is strictly speaking a work of social history (Febvre was a leading figure in the \"Annales\" School of historiography). But it is deeply sociological in character - \"Annales\" history was determinedly social scientific - and provides a systematic account of the long-run development of the European book trade (the period covered is 1450–1800).\n\nLucien Goldmann was Director of Studies at the School for Advanced Studies in the Social Sciences in Paris and founding Director of the Centre for the Sociology of Literature at the Free University of Brussels. Like Escarpit, Goldmann was influenced by Durkheim: hence, his definition of the subject matter of sociology as the 'study of the facts of consciousness'. But he was also interested in developing a sociology of the text. The central task of the literary sociologist, he argued, was to bring out the objective meaning of the literary work by placing it in its historical context, studied as a whole. \n\nGoldmann defined the creating subject as transindividual, that is, as an instance of Durkheim's 'collective consciousness'. Following Marx and Lukács, however, Goldmann also assumed that group consciousness was normally class consciousnesses. The mediating agency between a social class and the work of literature then became the 'world vision', which binds the individual members of a social class together. \"Le Dieu caché\", his study of Blaise Pascal and Jean Racine, was published in French in 1955 and in English translation as \"The Hidden God\" in 1964. It identified 'structural homologies' between the Jansenist 'tragic vision', the textual structures of Pascal's \"Pensées\" and Racine's plays, and the social position of the seventeenth-century 'noblesse de robe'. Goldmann's structuralism was 'genetic' because it sought to trace the genesis of literary structures in extra-literary phenomena.\n\nIn 1964 Goldmann published \"Pour une Sociologie du Roman\" translated by Alan Sheridan as \"Towards a Sociology of the Novel\" in 1974. Like Lukács, Goldmann sees the novel as revolving around the problematic hero's search for authentic values in a degraded society. But Goldmann also postulates a 'rigorous homology' between the literary form of the novel and the economic form of the commodity. The early novel, he argues, is concerned with individual biography and the problematic hero, but, as competitive capitalism evolves into monopoly capitalism, the problematic hero progressively disappears. The period between the First and Second World Wars witnesses a temporary experiment with the community as collective hero: Goldmann's example is André Malraux. But the main line of development is characterised by the effort to write the novel of 'the absence of subjects'. Here, Goldmann's example is the \"nouveau roman\" of Alain Robbe-Grillet and Nathalie Sarraute. \n\nAndrew Milner's \"John Milton and the English Revolution\" (1981) is essentially an application of Goldmann's genetic structuralism to the study of seventeenth-century English literature.\n\nGoldmann's sociology of literature remains significant in itself and as a source of inspiration, both positive and negative, to the kind of 'sociocriticism' developed by Edmond Cros, Pierre Zima and their co-workers in France and Canada.\n\nMarx used the term ideology to denote the inner connectedness of culture, including literature, and class. The philosopher Louis Althusser elaborated on this notion in the early 1970s, arguing that ideology functions so as to constitute biological individuals as social 'subjects' by representing their imaginary relation to their real conditions of existence. \n\nFor Althusser himself art was not ideology. But his theory was applied to literature by Macherey in France, Eagleton in Britain and Jameson in the United States. The central novelty of Eagleton's \"Criticism and Ideology\" was its argument that literature could be understood as 'producing' ideology, in the sense of performing it. Jameson's \"The Political Unconscious\" argued that literary analysis can be focussed on three distinct levels, 'text', 'ideologeme' and 'ideology of form', each of which has its socio-historical corollary, in the equivalent 'semantic horizon' of political history, society and mode of production. The version of ideology Jameson applies to all three levels is essentially Althusserian. The novelty of his position, however, was to argue for a 'double hermeneutic' simultaneously concerned with ideology and utopia.\n\nMacherey, Eagleton and Jameson were literary critics by profession, but their applications of ideology-critique to literature are sociological in character, insofar as they seek to explain literary phenomena in extra-literary terms.\n\nBourdieu was Professor of Sociology at the Collège de France and Director of the Centre de Sociologie Européenne. His first major contribution to the sociology of literature (and other arts) was \"La Distinction\", published in French in 1979 and in English translation in 1984. It is based on detailed sociological surveys and ethnographic observation of the social distribution of cultural preferences. Bourdieu identified three main zones of taste, 'legitimate', 'middle-brow' and 'popular', which he found to be dominant respectively in the educated sections of the dominant class, the middle classes and the working classes. He described legitimate taste as centred on an 'aesthetic disposition' to assert the primacy of form over function. The 'popular aesthetic', by contrast, is based on continuity between art and life and 'a deep-rooted demand for participation'. Hence, its hostility to representations of objects that in real life are either ugly or immoral. Artistic and social 'distinction' are inextricably interrelated, he concluded, because the 'pure gaze' implies a break with ordinary attitudes towards the world and, as such, is a 'social break'.\n\n\"The Rules of Art\" is more specifically focussed on literature, especially the significance of Gustave Flaubert for the making of modern French literature. Bourdieu postulated a model of 'the field of cultural production' as structured externally in relation to the 'field of power' and internally in relation to two 'principles of hierarchization', the heteronomous and the autonomous. The modern literary and artistic field is a site of contestation between the heteronomous principle, subordinating art to economy, and the autonomous, resisting such subordination. In Bourdieu's map of the French literary field in the late nineteenth century, the most autonomous genre, that is, the least economically profitable - poetry - is to the left, whilst the most heteronomous, the most economically profitable - drama - is to the right, with the novel located somewhere in between. Additionally, higher social status audiences govern the upper end of the field and lower status audiences the lower end. Flaubert's distinctive achievement in \"L'Éducation sentimentale\" was, in Bourdieu's account, to have understood and defined the rules of modern autonomous art.\n\nOne of the earliest English-language contributions to the sociology of literature is \"The Rise of the Novel\" (1957) by Ian Watt, Professor of English at Stanford University. For Watt, the novel's 'novelty' was its 'formal realism', the idea 'that the novel is a full and authentic report of human experience'. His paradigmatic instances are Daniel Defoe, Samuel Richardson and Henry Fielding. Watt argued that the novel's concern with realistically described relations between ordinary individuals, ran parallel to the more general development of philosophical realism, middle-class economic individualism and Puritan individualism. He also argued that the form addressed the interests and capacities of the new middle-class reading public and the new book trade evolving in response to them. As tradesmen themselves, Defoe and Richardson had only to 'consult their own standards' to know that their work would appeal to a large audience.\n\nRaymond Williams was Professor of Drama at Cambridge University and one of the founders of contemporary cultural studies. He described his own distinctive approach as a 'cultural materialism', by which he meant a theory of culture 'as a (social and material) productive process' and of the arts 'as social uses of material means of production'. This is a clearly sociological, as distinct from literary-critical, perspective: hence, its most general exposition in the United States as \"The Sociology of Culture\" and in Britain as \"Culture\", a 1981 title in Fontana's New Sociology series. Although Williams's interests ranged widely across the whole field of literary and cultural studies, his major work was concentrated on literature and drama. He was thus a sociologist of culture, specialising in the sociology of literature. \n\nIn \"The Long Revolution\" (1961), Williams developed pioneering accounts of the sociology of the book trade, the sociology of authorship and the sociology of the novel. In \"The English Novel from Dickens to Lawrence\" (1970), he argued that the modern novel articulated a distinctively modern 'structure of feeling', the key problem of which was the 'knowable community'. In \"The Country and the City\" (1973) he developed a social history of English country-house poetry, aimed at demystifying the idealisations of rural life contained in the literature: 'It is what the poems are: not country life but social compliment; the familiar hyperboles of the aristocracy and its attendants'. His \"Marxism and Literature\" (1977) - simultaneously a critique of both Marxism and 'Literature' - is an extensive formal elaboration of Williams's own theoretical system. \n\nAlan Sinfield's \"Faultlines: Cultural Materialism and the Politics of Dissident Reading\" (1992) and \"Literature, Politics and Culture in Postwar Britain\" (1997) are both clearly indebted to Williams. So, too, is Andrew Milner's \"Literature, Culture and Society\" (2005).\n\nFranco Moretti was, by turn, Professor of English Literature at the University of Salerno, of Comparative Literature at Verona University and of English and Comparative Literature at Stanford University. His first book, \"Signs Taken for Wonders\" (1983) was subtitled \"Essays in the Sociology of Literary Forms\" and was essentially qualitative in method. His later work, however, became progressively more quantitative. \n\nApplying Immanuel Wallerstein's world-systems theory to literature, Moretti argued, in \"Atlas of the European Novel\" (1998), that the nineteenth-century literary economy had comprised 'three Europes', with France and Britain at the core, most countries in the periphery and a variable semiperiphery located in between. Measured by the volume of translations in national bibliographies, he found that French novelists were more successful in the Catholic South and British in the Protestant North, but that the whole continent nonetheless read the leading figures from both. London and Paris 'rule the entire continent for over a century', he concluded, publishing half or more of all European novels. \n\nMoretti's theses prompted much subsequent controversy, collected together in Christopher Prendergast's edited collection \"Debating World Literature\" (2004). Moretti himself expanded on the argument in his \"Distant Reading\" (2013).\n\nBuilding on earlier work in the production of culture, reception aesthetics and cultural capital, the sociology of literature has recently concentrated on readers' construction of meaning. New developments include studying the relationship between literature and group identities; concerning institutional and reader-response analysis; reintroducing the role of intentions of the author in literature; reconsidering the role of ethics and morality in literature and developing a clearer understanding of how literature is and is not like other media.\n\nThe sociology of literature has also recently taken an interest in the global inequality between First-World and Third-World authors, where the latter tend to be strongly dependent on the editorial decisions of publishers in Paris, London or New York and are often excluded from participation in the global literary market.\n\nThe journal New Literary History devoted a special issue to new approaches to the sociology of literature in Spring 2010.\n\n"}
{"id": "937531", "url": "https://en.wikipedia.org/wiki?curid=937531", "title": "Souvenir", "text": "Souvenir\n\nA souvenir (from French, for \"a remembrance or memory\"), memento, keepsake, or token of remembrance is an object a person acquires for the memories the owner associates with it. A souvenir can be any object that can be collected or purchased and transported home by the traveler as a memento of a visit. While there is no set minimum or maximum cost that one is required to adhere to when purchasing a souvenir, etiquette would suggest to keep it within a monetary amount that the receiver would not feel uncomfortable with when presented the souvenir. The object itself may have intrinsic value, or be a symbol of experience. Without the owner's input, the symbolic meaning is invisible and cannot be articulated.\n\nThe tourism industry designates tourism souvenirs as commemorative merchandise associated with a location, often including geographic information and usually produced in a manner that promotes souvenir collecting.\nThroughout the world, the souvenir trade is an important part of the tourism industry serving a dual role, first to help improve the local economy, and second to allow visitors to take with them a memento of their visit, ultimately to encourage an opportunity for a return visit, or to promote the locale to other tourists as a form of word-of-mouth marketing. Perhaps the most collected souvenirs by tourists are photographs as a medium to document specific events and places for future reference.\n\nSouvenirs as objects include mass-produced merchandise such as clothing: T-shirts and hats; collectables: postcards, refrigerator magnets, miniature figures; household items: mugs, bowls, plates, ashtrays, egg timers, spoons, fudge, notepads, plus many others.\n\nSouvenirs also include non-mass-produced items like folk art, local artisan handicrafts, objects that represent the traditions and culture of the area, non-commercial, natural objects like sand from a beach, and anything else that a person attaches nostalgic value to and collects among his personal belongings.\n\nA more grisly form of souvenir in the First World War was displayed by a Pathan soldier to an English Territorial. After carefully studying the Tommy's acquisitions (a fragment of shell, a spike and badge from a German helmet), he produced a cord with the ears of enemy soldiers he claimed to have killed. He was keeping them to take back to India for his wife.\n\nSimilar to souvenirs, memorabilia (Latin for \"memorable (things)\", plural of \"memorābile\") are objects treasured for their memories or historical interest; however, unlike souvenirs, memorabilia can be valued for a connection to an event or a particular professional field, company or brand.\n\nExamples include sporting events, historical events, culture, and entertainment. Such items include: clothing; game equipment; publicity photographs and posters; magic memorabilia; other entertainment-related merchandise & memorabilia; movie memorabilia; airline and other transportation-related memorabilia; and pins, among others.\n\nOften memorabilia items are kept in protective covers or display cases to safeguard and preserve their condition.\n\nThe largest collection of Superman memorabilia belongs to Herbert Chavez (Philippines). \n\nIn Japan, souvenirs are known as , and are frequently selected from \"meibutsu\", or products associated with a particular region. Bringing back \"omiyage\" from trips to co-workers and families is a social obligation, and can be considered a form of apology for the traveller's absence. \"Omiyage\" sales are big business at Japanese tourist sites.\nUnlike souvenirs, however, \"omiyage\" are frequently special food products, packaged into several small portions to be easily distributed to all the members of a family or a workplace.\n\nTravelers may buy souvenirs as gifts for those who did not make the trip.\n\nIn the Philippines a similar tradition of bringing souvenirs as a gift to family members, friends, and coworkers is called pasalubong.\n"}
{"id": "44183472", "url": "https://en.wikipedia.org/wiki?curid=44183472", "title": "Spatial turn", "text": "Spatial turn\n\nSpatial turn is a term used to describe an intellectual movement that places emphasis on place and space in social science and the humanities. It is closely linked with quantitative studies of history, literature, cartography, and other studies of society. The movement has been influential in providing mass amounts of data for study of cultures, regions, and specific locations.\n\nAcademics such as Ernst Cassirer and Lewis Mumford helped to define a sense of \"community\" and \"commons\" in their studies, forming the first part of a \"spatial turn.\" The turn developed more comprehensively in the later twentieth century in French academic theories, such as those of Michel Foucault. \n\nTechnologies have also played an important role in \"turns.\" The introduction of Geographic Information Systems (GIS) has also been instrumental in quantifying data in the humanities for study by its place.\n"}
{"id": "1515577", "url": "https://en.wikipedia.org/wiki?curid=1515577", "title": "Three Treasures (Taoism)", "text": "Three Treasures (Taoism)\n\nThe Three Treasures or Three Jewels () are basic virtues in Taoism. Although the \"Tao Te Ching\" originally used \"sanbao\" to mean \"compassion\", \"frugality\", and \"humility\", the term was later used to translate the Three Jewels (Buddha, Dharma, and Sangha) in Chinese Buddhism, and to mean the Three Treasures (jing, qi, and shen) in Traditional Chinese Medicine.\n\n\"Sanbao\" \"three treasures\" first occurs in \"Tao Te Ching\" chapter 67, which Lin Yutang (1948:292) says contains Laozi's \"most beautiful teachings\":\n天下皆謂我道大，似不肖。夫唯大，故似不肖。若肖久矣。其細也夫！我有三寶，持而保之。一曰慈，二曰儉，三曰不敢為天下先。慈故能勇；儉故能廣；不敢為天下先，故能成器長。今舍慈且勇；舍儉且廣；舍後且先；死矣！夫慈以戰則勝，以守則固。天將救之，以慈衛之。\nEvery one under heaven says that our Way is greatly like folly. But it is just because it is great, that it seems like folly. As for things that do not seem like folly — well, there can be no question about \"their\" smallness!<br>\nHere are my three treasures. Guard and keep them! The first is pity; the second, frugality; the third, refusal to be 'foremost of all things under heaven'. <br>\nFor only he that pities is truly able to be brave; <br>\nOnly he that is frugal is able to be profuse. <br>\nOnly he that refuses to be foremost of all things <br>\nIs truly able to become chief of all Ministers. <br>\nAt present your bravery is not based on pity, nor your profusion on frugality, nor your vanguard on your rear; and this is death. But pity cannot fight without conquering or guard without saving. Heaven arms with pity those whom it would not see destroyed. (tr. Waley 1958:225)\n\nArthur Waley describes these Three Treasures as, \"The three rules that formed the practical, political side of the author's teaching (1) abstention from aggressive war and capital punishment, (2) absolute simplicity of living, (3) refusal to assert active authority.\"\n\nThe first of the Three Treasures is \"ci\" (), which is also a Classical Chinese term for \"mother\" (with \"tender love, nurturing \" semantic associations). \"Tao Te Ching\" chapters 18 and 19 parallel \"ci\" (\"parental love\") with \"xiao\" (孝 \"filial love; filial piety\"). Wing-tsit Chan (1963:219) believes \"the first is the most important\" of the Three Treasures, and compares \"ci\" with Confucianist \"ren\" (仁 \"humaneness; benevolence\"), which the \"Tao Te Ching\" (e.g., chapters 5 and 38) mocks.\n\nThe second is \"jian\" (), a practice that the \"Tao Te Ching\" (e.g., chapter 59) praises. Ellen M. Chen (1989:209) believes \"jian\" is \"organically connected\" with the Taoist metaphor \"pu\" (樸 \"uncarved wood; simplicity\"), and \"stands for the economy of nature that does not waste anything. When applied to the moral life it stands for the simplicity of desire.\"\n\nThe third treasure is a six-character phrase instead of a single word: \"Bugan wei tianxia xian\" 不敢為天下先 \"not dare to be first/ahead in the world\". \nChen notes that\nThe third treasure, daring not be at the world's front, is the Taoist way to avoid premature death. To be at the world's front is to expose oneself, to render oneself vulnerable to the world's destructive forces, while to remain behind and to be humble is to allow oneself time to fully ripen and bear fruit. This is a treasure whose secret spring is the fear of losing one's life before one's time. This fear of death, out of a love for life, is indeed the key to Taoist wisdom. (1989:209) \n\nIn the Mawangdui Silk Texts version of the \"Tao Te Ching\", this traditional \"Three Treasures\" chapter 67 is chapter 32, following the traditional last chapter (81, 31). Based upon this early silk manuscript, Robert G. Henricks (1989:160) concludes that \"Chapters 67, 68, and 69 should be read together as a unit.\" Besides some graphic variants and phonetic loan characters, like \"ci\" (兹 \"mat, this\") for \"ci\" (慈 \"compassion, love\", clarified with the \"heart radical\" 心), the most significant difference with the received text is the addition of \"heng\" (恆, \"constantly, always\") with \"I constantly have three …\" (我恆有三) instead of \"I have three …\" (我有三).\n\nThe language of the \"Tao Te Ching\" is notoriously difficult to translate, as illustrated by the diverse English renditions of \"Three Treasures\" below.\n\nA consensus translation of the Three Treasures could be: compassion or love, frugality or simplicity, and humility or modesty.\n\nIn addition to these Taoist \"Three Treasures\", Chinese \"sanbao\" can also refer to the Three Treasures in Traditional Chinese Medicine or the Three Jewels in Buddhism. Victor H. Mair (1990:110) notes that Chinese Buddhists chose the Taoist term \"sanbao\" to translate Sanskrit \"triratna\" or \"ratnatraya\" (\"three jewels\"), and \"It is not at all strange that the Taoists would take over this widespread ancient Indian expression and use it for their own purposes.\"\n\nErik Zürcher, who studied influences of Buddhist doctrinal terms in Daoism, noted (1980:115) two later meanings of \"sanbao\": \"Dao\" 道 \"the Way\", \"jing\" 經 \"the Scriptures\", and \"shi\" 師 \"the Master\" seems to be patterned after Buddhist usage; \"Tianbao jun\" 天寶君 \"Lord of Celestial Treasure\", \"Lingbao jun\" 靈寶君 \"Lord of Numinous Treasure\", and \"Shenbao jun\" 神寶君 \"Lord of Divine Treasure\" are the \"Sanyuan\" 三元 \"Three Primes\" of the Lingbao School.\n\n\n"}
{"id": "1192640", "url": "https://en.wikipedia.org/wiki?curid=1192640", "title": "Tripartite language", "text": "Tripartite language\n\nA tripartite language, also called an ergative–accusative language, is one that treats the agent of a transitive verb, the patient of a transitive verb, and the single argument of an intransitive verb each in different ways. This contrasts with nominative–accusative and ergative–absolutive languages. If the language has morphological case, the arguments are marked in this way:\n\n\nIn this Nez Perce intransitive sentence, the intransitive argument has no suffix and the verb carries the third person agreement prefix \"hi-\":\nIn a transitive sentence with two third person arguments, the agent is marked with \"-n(i)m\", the patient with \"-ne\", and the verb with the third person on third person transitive agreement marker \"pée-\":\n\nThe Ainu language of northern Japan also shows tripartite marking in its pronominal prefixes, with the first person \"Ku=\" being the ergative form, \"=an\" being the intransitive form and \"=en=\" being the accusative form. Ainu also shows the passive voice formation typical of nominative-accusative languages and the antipassive of ergative-absolutive languages. Like Nez Percé, the use of both the passive and antipassive is a trait of a tripartite language.\n\nTripartite languages are rare. Besides Native American Nez Perce, they include the Vakh dialects of the Khanty language, Wangkumara, Semelai, and, in its singular pronouns, Kalaw Lagaw Ya. Yazghulami is tripartite, but only in the past tense. Several constructed languages, especially engineered languages, use a tripartite case system or tripartite adposition system, notably the fictional Na'vi language featured in the 2009 movie Avatar.\n\n\n"}
{"id": "33896833", "url": "https://en.wikipedia.org/wiki?curid=33896833", "title": "Urban acupuncture", "text": "Urban acupuncture\n\nUrban acupuncture is a socio-environmental theory that combines contemporary urban design with traditional Chinese acupuncture, using small-scale interventions to transform the larger urban context. Sites are selected through analysis of aggregate social, economic and ecological factors, and are developed through a dialogue between designers and the community. Just as the practice of acupuncture is aimed at relieving stress in the human body, the goal of urban acupuncture is to relieve stress in the built environment. In Taipei, there was an urban acupuncture workshop that aimed to \"produce small-scale but socially catalytic interventions\" into the city's fabric.\n\nOriginally coined by Barcelonan architect and urbanist, Manuel de Sola Morales, the term has been recently championed and developed further by Finnish architect and social theorist Marco Casagrande, this school of thought eschews massive urban renewal projects in favour of a more localised and community approach that, in an era of constrained budgets and limited resources, could democratically and cheaply offer a respite to urban dwellers.\nCasagrande views cities as complex energy organisms in which different overlapping layers of energy flows are determining the actions of the citizens as well as the development of the city. By mixing environmentalism and urban design Casagrande is developing methods of punctual manipulation of the urban energy flows in order to create an ecologically sustainable urban development towards the so-called 3rd Generation City (postindustrial city). The theory is developed in the Tamkang University of Taiwan and at independent multidisciplinary research center Ruin Academy. With focus on environmentalism and urban design, Casagrande defines urban acupuncture as a design tool where punctual manipulations contribute to creating sustainable urban development, such as the community gardens and urban farms in Taipei.\n\nCasagrande describes urban acupuncture as:\n\n\"[a] cross-over architectural manipulation of the collective sensuous intellect of a city. City is viewed as multi-dimensional sensitive energy-organism, a living environment. Urban acupuncture aims into a touch with this nature.\" and \"Sensitivity to understand the energy flows of the collective chi beneath the visual city and reacting on the hot-spots of this chi. Architecture is in the position to produce the acupuncture needles for the urban chi.\" and \"A weed will root into the smallest crack in the asphalt and eventually break the city. Urban acupuncture is the weed and the acupuncture point is the crack. The possibility of the impact is total, connecting human nature as part of nature.\"\n\nCasagrande utilized the tenets of acupuncture: treat the points of blockage and let relief ripple throughout the body. More immediate and sensitive to community needs than traditional institutional forms of large scale urban renewal interventions would not only respond to localized needs, but do so with a knowledge of how citywide systems operated and converged at that single node. Release pressure at strategic points, release pressure for the whole city.\n\nIn theory, urban acupuncture opens the door for uncontrolled creativity and freedom. Citizens are enabled to join the creative participatory planning process, feel free to use city space for any purpose and develop their environment according to their will. This \"new\" post-industrialized city Casagrande, dubbed the Third Generation City, is driven by people who are concerned about the destruction that the modern machine is causing to nature including human nature. They want sustainable co-operation with the rest of the nature. In a larger context, a site of urban acupuncture can be viewed as communicating to the city outside like a natural sign of life in a city programmed to subsume it.\n\nUrban acupuncture bears some similarities to the new urbanist concept of Tactical Urbanism. The idea focuses on local resources rather than capital-intensive municipal programs and promotes the idea of citizens installing and caring for interventions. These small changes, proponents claim, will boost community morale and catalyze revitalization. Boiled down to a simple statement, \"urban acupuncture\" means focusing on small, subtle, bottom-up interventions that harness and direct community energy in positive ways to heal urban blight and improve the cityscape. It is meant as an alternative to large, top-down, mega-interventions that typically require heavy investments of municipal funds (which many cities at the moment simply don't have) and the navigation of yards of bureaucratic red tape. The micro-scale interventions targeted by \"urban acupuncture\" appeal to both citizen-activists and cash-strapped communities. In Mexico urban acupuncture converts temporary housing, like sheds in the slums, to simple homes that allow for \"add-ons\" later, based on need and affordability. This strategy transforms the slum zone, without relocating families that have been living together for generations. In South Africa Urban Acupuncture is viewed as a possibility to provide a means for people to unlock their creativity and the advantages thereof, for example, innovation and entrepreneurship concentrating on parts of the city, i.e. communities thereby providing opportunities to those areas which do not have the sort of infrastructure that is found in mainstream cities. This approach can provide a more realistic and less costly method for city planners and citizens as an effective way to make minor improvements in the communities in order to achieve a greater good in the cities.\n\nJaime Lerner, the former mayor of Curitiba, suggests urban acupuncture as the future solution for contemporary urban issues; by focusing on very narrow pressure points in cities, we can initiate positive ripple effects for the greater society. Urban acupuncture reclaims the ownership of land to the public and emphasizes the importance of community development through small interventions in design of cities. It involves pinpointed interventions that can be accomplished quickly to release energy and create a positive ripple effect.\nHe described in 2007:\n\"I believe that some medicinal \"magic\" can and should be applied to cities, as many are sick and some nearly terminal. As with the medicine needed in the interaction between doctor and patient, in urban planning it is also necessary to make the city react; to poke an area in such a way that it is able to help heal, improve, and create positive chain reactions. It is indispensable in revitalizing interventions to make the organism work in a different way.\"\n\nTaiwanese architect and academic Ti-Nan Chi is looking with micro urbanism at the vulnerable and insignificant side of contemporary cities around the world identified as micro-zones, points for recovery in which micro-projects have been carefully proposed to involve the public on different levels, aiming to resolve conflicts among property owners, villagers, and the general public. A loosely affiliated team of architects Wang Shu, Marco Casagrande, Hsieh Ying-chun and Roan Ching-yueh (sometimes called WEAK! Architecture) are describing the unofficial Instant City, or Instant Taipei, as architecture that uses the Official City as a \"growing platform and energy source, where to attach itself like a parasite and from where to leach the electricity and water… [The Instant City's] illegal urban farms or night markets is so widespread and deep rooted in the Taiwanese culture and cityscape that we could almost speak of another city on top of the \"official\" Taipei, a parallel city – or a para-city.\" WEAK! is calling urban acupuncture depending on the context as Illegal Architecture, Orchid Architecture, the People's Architecture, or Weak Architecture. The theory of urban acupuncture suggests that scores of small-scale, less costly and localized projects is what cities need in order to recover and renew themselves.\n\nThe winner of the Deutsche Bank Urban Age Award (DBUAA) Cape Town 2012 is the neighbourhood project \"Mothers Unite\" demonstrating the power of urban acupuncture. Mothers Unite was founded in 2007 and provides a safe haven from the gangsterism, drugs and violence that are part and parcel of street and home life in the area. Built with donated shipping containers, the village is made up of a library, kitchen, office, sheltered area, playground and food garden. Professor Edgar Pieterse:\n\n\"Durable urban change is often about carefully targeted micro interventions that can change the energies and dynamics of a surrounding neighbourhood. When one engages with the physical manifestations that the tenacity, blood, sweat, and tears of the protagonists of Mothers Unite and Masiphumelele Library (another of the eight finalists) have created, the power of urban acupuncture is apparent. These projects serve as reminders that through vision, commitment over the long haul, and principle-based partnerships, just about any problem can be confronted and addressed.\"\n\nIn ecological restoration of industrial cities Urban Acupuncture can take form as spontaneous and often illegal urban farms and community gardens punctuating the more mechanical city and tuning it towards a more sustainable co-existence with the natural environment. \nUrban Acupuncture areas can receive, treat and recycle the waste from the surrounding city acting as eco-valleys within the urban fabric. In River Urbanism the Urban Acupuncture areas can include underground stormwater reservoirs and act as flood relief for the surrounding city as a sponge and they can act as biological filters purifying water originated from polluted rivers. Urban acupuncture is a point by point manipulation of the urban energy to create a sustainable town or city, which Marco Casagrande has dubbed '3rd Generation Cities'.\n\nAmerican artist Gordon Matta-Clark is credited with developing a system for identifying pockets of disrepair in the built environment—the first step in the framework of urban acupuncture. \nArtist Miru Kim explores industrial ruins and structures making her look at the city as one living organism. She claims to feel not only the skin of the city, but also to penetrate the inner layers of its intestines and veins, which swarm with minuscule life forms. Referring to a public environmental art work, Cicada, Casagrande explains:\n\n\"Cicada is urban acupuncture for Taipei city penetrating the hard surfaces of industrial laziness in order to reach the original ground and get in touch with the collective Chi, the local knowledge that binds the people of Taipei basin with nature. The cocoon of Cicada is an accidental mediator between the modern man and reality. There is no other reality than nature.\"\n\nEnvironmental art as urban acupuncture is an artistic way of injecting a healthy dose of natural elements and human scale into the mechanized urban grid.\n\n\n\n\n"}
{"id": "12649093", "url": "https://en.wikipedia.org/wiki?curid=12649093", "title": "Wannarexia", "text": "Wannarexia\n\nWannarexia, or anorexic yearning,\nis a label applied to someone who claims to have anorexia nervosa, or wishes they did, but does not. These individuals are also called wannarexic, “wanna-be ana” or \"anorexic wannabe\". The neologism \"wannarexia\" is a portmanteau of the latter two terms. It may be used as a pejorative term.\n\nWannarexia is a cultural phenomenon and has no diagnostic criteria, although some wannarexics may be instead diagnosed with eating disorder not otherwise specified (EDNOS). Wannarexia is more commonly, but not always, found in teenage girls who want to be trendy, and is likely caused by a combination of cultural and media influences.\n\nDr. Richard Kreipe states that the distinction between anorexia and wannarexia is that anorexics aren't satisfied by their weight loss, while wannarexics are more likely to derive pleasure from weight loss. Many people who actually suffer from the eating disorder anorexia are angry, offended, or frustrated about wannarexia.\n\nWannarexics may be inspired or motivated by the pro-anorexia, or pro-ana, community that promotes or supports anorexia as a lifestyle choice rather than an eating disorder. Some participants in pro-ana web forums only want to associate with \"real anorexics\" and will shun wannarexics who only diet occasionally, and are not dedicated to the \"lifestyle\" full-time. Community websites for anorexics and bulimics have posted advice to wannarexics saying that they don't want their \"warped perspectives and dangerous behaviour to affect others.\"\n\nKelsey Osgood uses the label in her book \"How To Disappear Completely: On Modern Anorexia\" where she describes wannarexia as “a gateway drug for teenagers”.\n"}
