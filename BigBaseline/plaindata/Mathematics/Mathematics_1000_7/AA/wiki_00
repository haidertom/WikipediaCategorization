{"id": "1118832", "url": "https://en.wikipedia.org/wiki?curid=1118832", "title": "Arbitrarily large", "text": "Arbitrarily large\n\nIn mathematics, the phrases arbitrarily large, arbitrarily small, and arbitrarily long are used in statements such as:\n\nwhich is shorthand for:\n\n\"Arbitrarily large\" is not equivalent to \"sufficiently large\". For instance, while it is true that prime numbers can be arbitrarily large since there are an infinite number of them, it is not true that all sufficiently large numbers are prime. \"Arbitrarily large\" does not mean \"infinitely large\" because although prime numbers can be arbitrarily large, an infinitely large prime does not exist since all prime numbers (as well as all other integers) are finite.\n\nIn some cases, phrases such as \"P(\"x\") is true for arbitrarily large \"x\"\" are used primarily for emphasis, as in \"P(\"x\") is true for all \"x\", no matter how large \"x\" is.\" In these cases, the phrase \"arbitrarily large\" does not have the meaning indicated above but is in fact logically synonymous with \"all.\"\n\nTo say that there are \"arbitrarily long arithmetic progressions of prime numbers\" does not mean that there exists any infinitely long arithmetic progression of prime numbers (there is not), nor that there exists any particular arithmetic progression of prime numbers that is in some sense \"arbitrarily long\", but rather that no matter how large a number \"n\" is, there exists some arithmetic progression of prime numbers of length at least \"n\".\n\nThe statement \"ƒ(\"x\") is non-negative for arbitrarily large \"x\".\" could be rewritten as:\n\nUsing \"sufficiently large\" instead yields:\n\n"}
{"id": "339496", "url": "https://en.wikipedia.org/wiki?curid=339496", "title": "As I was going to St Ives", "text": "As I was going to St Ives\n\n\"As I was going to St Ives\" is a traditional English-language nursery rhyme in the form of a riddle. Its Roud Folk Song Index number is 19772.\n\nThe most common modern version is:\n\nThe following version is found in a manuscript (Harley MS 7316) dating from approximately 1730: \n\nA version very similar to that accepted today was published in the \"Weekly Magazine\" of August 4, 1779:\n\nThe suggestion of polygamy (implicit in the line \"I met a man with seven wives\") is generally absent from the earliest publications, but is present by 1837.\n\nThere are a number of places called St Ives in England and elsewhere. It is generally thought\nthat the rhyme refers to St Ives, Cornwall, when it was a busy fishing port and had many cats to stop the rats and mice destroying the fishing gear, although some people argue it was St Ives, Cambridgeshire as this is an ancient market town and therefore an equally plausible destination.\n\nAll potential answers to this riddle are based on its ambiguity because the riddle only tells us the group has been \"met\" on the journey to St. Ives and gives no further information about its intentions, only those of the narrator. In modern usage, 'to meet someone on the road' may include the sense of 'passed' or 'overtook'; while the older usage may have referred exclusively to those going in opposite directions. As such, the 'correct' answer could be stated as \"at least one, the narrator plus anyone who happens to be travelling in the same direction as him or her\".\n\nIf the group that the narrator meets is assumed \"not\" to be travelling to St. Ives the answer could be \"one\" person going to St. Ives: the narrator. This is the most common assumption, as the purpose of the riddle was most likely to trick the listener into making long winded calculations only to be surprised by the simplicity of the answer.\n\nIf one disregards the 'trick' answer and assumes the narrator overtook the group as they were \"also\" travelling to St. Ives, the most common mathematical answer is 2802: 1 man, 7 wives, 49 sacks, 343 cats, and 2401 kits, plus the narrator (the sum of a geometric series, plus one).\n\nAfter the riddle was published in the August 4, 1779 issue of \"The Weekly Magazine\", as described above, a subsequent edition of that journal contained the following solution, submitted by reader \"Philo-Rhithmus\" of Edinburgh:\n\nA similar problem is found in the Rhind Mathematical Papyrus (Problem 79), dated to around 1650 BC.\nThe papyrus is translated as follows:\nThe problem appears to be an illustration of an algorithm for multiplying numbers. The sequence 7, 7, 7, 7, 7 appears in the right-hand column, and the terms 2,801, 2×2,801, 4×2,801 appear in the left; the sum on the left is 7×2,801 = 19,607, the same as the sum of the terms on the right. The equality of the two geometric sequences can be stated as the equation (2+2+2)(7+7+7+7+7) = 7+7+7+7+7, which relies on the coincidence 2+2+2=7.\n\nNote that the author of the papyrus listed a wrong value for the fourth power of 7; it should be 2,401, not 2,301. However, the sum of the powers (19,607) is correct.\n\nThe problem has been paraphrased by modern commentators as a story problem involving houses, cats, mice, and grain, although in the Rhind Mathematical Papyrus there is no discussion beyond the bare outline stated above. The hekat was of a cubic cubit (approximately ).\n\n"}
{"id": "10993199", "url": "https://en.wikipedia.org/wiki?curid=10993199", "title": "Cebeci–Smith model", "text": "Cebeci–Smith model\n\nThe Cebeci–Smith model is a 0-equation eddy viscosity model used in computational fluid dynamics analysis of turbulent boundary layer flows. The model gives eddy viscosity, formula_1, as a function of the local boundary layer velocity profile. The model is suitable for high-speed flows with thin attached boundary-layers, typically present in aerospace applications. Like the Baldwin-Lomax model, this model is not suitable for cases with large separated regions and significant curvature/rotation effects. Unlike the Baldwin-Lomax model, this model requires the determination of a boundary layer edge.\n\nThe model was developed by Tuncer Cebeci and Apollo M. O. Smith, in 1967.\n\nIn a two-layer model, the boundary layer is considered to comprise two layers: inner (close to the surface) and outer. The eddy viscosity is calculated separately for each layer and combined using:\n\nwhere formula_3 is the smallest distance from the surface where formula_4 is equal to formula_5.\n\nThe inner-region eddy viscosity is given by:\n\nwhere\n\nwith the von Karman constant formula_8 usually being taken as 0.4, and with\n\nThe eddy viscosity in the outer region is given by:\n\nwhere formula_11, formula_12 is the displacement thickness, given by\n\nand \"F\" is the Klebanoff intermittency function given by\n\n\n"}
{"id": "6497220", "url": "https://en.wikipedia.org/wiki?curid=6497220", "title": "Computational complexity of mathematical operations", "text": "Computational complexity of mathematical operations\n\nThe following tables list the computational complexity of various algorithms for common mathematical operations.\n\nHere, complexity refers to the time complexity of performing computations on a multitape Turing machine. See big O notation for an explanation of the notation used.\n\nNote: Due to the variety of multiplication algorithms, \"M\"(\"n\") below stands in for the complexity of the chosen multiplication algorithm.\n\nMany of the methods in this section are given in Borwein & Borwein.\n\nThe elementary functions are constructed by composing arithmetic operations, the exponential function (exp), the natural logarithm (log), trigonometric functions (sin, cos), and their inverses. The complexity of an elementary function is equivalent to that of its inverse, since all elementary functions are analytic and hence invertible by means of Newton's method. In particular, if either exp or log in the complex domain can be computed with some complexity, then that complexity is attainable for all other elementary functions.\n\nBelow, the size \"n\" refers to the number of digits of precision at which the function is to be evaluated.\n\nIt is not known whether O(\"M\"(\"n\") log \"n\") is the optimal complexity for elementary functions. The best known lower bound is the trivial bound Ω(\"M\"(\"n\")).\n\nThis table gives the complexity of computing approximations to the given constants to \"n\" correct digits.\nAlgorithms for number theoretical calculations are studied in computational number theory.\n\nThe following complexity figures assume that arithmetic with individual elements has complexity \"O\"(1), as is the case with fixed-precision floating-point arithmetic or operations on a finite field.\n\nIn 2005, Henry Cohn, Robert Kleinberg, Balázs Szegedy, and Chris Umans showed that either of two different conjectures would imply that the exponent of matrix multiplication is 2.\n\nBecause of the possibility of blockwise inverting a matrix, where an inversion of an matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of operations, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.\n\n"}
{"id": "17910805", "url": "https://en.wikipedia.org/wiki?curid=17910805", "title": "Counting board", "text": "Counting board\n\nThe counting board is the precursor of the abacus, and the earliest known form of a counting device (excluding fingers and other very simple methods). Counting boards were made of stone or wood, and the counting was done on the board with beads, or pebbles etc. Not many boards survive because of the perishable materials used in their construction. \n\nThe oldest known counting board, the Salamis Tablet (\"c.\" 300 BC) was discovered on the Greek island of Salamis in 1899. It is thought to have been used by the Babylonians in about 300 BC and is more of a gaming board than a calculating device. It is marble, about 150 x 75 x 4.5 cm, and is in the Greek National museum in Athens. It has carved Greek letters and parallel grooves. \n\nThe German mathematicican Adam Ries described the use of counting boards in \"Rechenbuch auf Linien und Ziphren in allerlei Handthierung / geschäfften und Kaufmanschafft\". In the novel \"Wolf Hall\", Hilary Mantel refers to Thomas Cromwell using a counting board in 16th-century England.\n\n"}
{"id": "6166234", "url": "https://en.wikipedia.org/wiki?curid=6166234", "title": "Cryptomorphism", "text": "Cryptomorphism\n\nIn mathematics, two objects, especially systems of axioms or semantics for them, are called cryptomorphic if they are equivalent but not obviously equivalent. This word is a play on the many morphisms in mathematics, but \"cryptomorphism\" is only very distantly related to \"isomorphism\", \"homomorphism\", or \"morphisms\". The equivalence may possibly be in some informal sense, or may be formalized in terms of a bijection or equivalence of categories between the mathematical objects defined by the two cryptomorphic axiom systems.\n\nThe word was coined by Garrett Birkhoff before 1967, for use in the third edition of his book \"Lattice Theory\". Birkhoff did not give it a formal definition, though others working in the field have made some attempts since.\n\nIts informal sense was popularized (and greatly expanded in scope) by Gian-Carlo Rota in the context of matroid theory: there are dozens of equivalent axiomatic approaches to matroids, but two different systems of axioms often look very different. \n\nIn his 1997 book \"Indiscrete Thoughts\", Rota describes the situation as follows:\n\nThough there are many cryptomorphic concepts in mathematics outside of matroid theory and universal algebra, the word has not caught on among mathematicians generally. It is, however, in fairly wide use among researchers in matroid theory.\n\n\n"}
{"id": "324872", "url": "https://en.wikipedia.org/wiki?curid=324872", "title": "Ethnomathematics", "text": "Ethnomathematics\n\nIn mathematics education, ethnomathematics is the study of the relationship between mathematics and culture. Often associated with \"cultures without written expression\", it may also be defined as \"the mathematics which is practised among identifiable cultural groups\". It refers to a broad cluster of ideas ranging from distinct numerical and mathematical systems to multicultural mathematics education. The goal of ethnomathematics is to contribute both to the understanding of culture and the understanding of mathematics, and mainly to lead to an appreciation of the connections between the two.\n\nThe term \"ethnomathematics\" was introduced by the Brazilian educator and mathematician Ubiratàn D'Ambrosio in 1977 during a presentation for the American Association for the Advancement of Science. Since D'Ambrosio put forth the term, people - D'Ambrosio included - have struggled with its meaning (\"An etymological abuse leads me to use the words, respectively, \"ethno\" and \"mathema\" for their categories of analysis and \"tics\" from (from techne)\".).\n\nThe following is a sampling of some of the definitions of ethnomathematics proposed between 1985 and 2006:\n\n\nSome of the systems for representing numbers in previous and present cultures are well known. Roman numerals use a few letters of the alphabet to represent numbers up to the thousands, but are not intended for arbitrarily large numbers and can only represent positive integers. Arabic numerals are a family of systems, originating in India and passing to medieval Islamic civilization, then to Europe, and now standard in global culture—and having undergone many curious changes with time and geography—can represent arbitrarily large numbers and have been adapted to negative numbers, fractions, and real numbers.\n\nLess well known systems include some that are written and can be read today, such as the Hebrew and Greek method of using the letters of the alphabet, in order, for digits 1–9, tens 10–90, and hundreds 100–900.\n\nA completely different system is that of the quipu, which recorded numbers on knotted strings.\n\nEthnomathematicians are interested in the ways in which numeration systems grew up, as well as their similarities and differences and the reasons for them. The great variety in ways of representing numbers is especially intriguing.\n\nThis means the ways in which number words are formed.\n\nFor instance, in English, there are four different systems. The units words (one to nine) and ten are special. The next two are reduced forms of Anglo-Saxon \"one left over\" and \"two left over\" (i.e., after counting to ten). Multiples of ten from \"twenty\" to \"ninety\" are formed from the units words, one through nine, by a single pattern. Thirteen to nineteen, and in a slightly different way twenty-one through ninety-nine (excluding the tens words), are compounded from tens and units words. Larger numbers are also formed on a base of ten and its powers (\"hundred\" and \"thousand\"). One may suspect this is based on an ancient tradition of finger counting. Residues of ancient counting by 20s and 12s are the words \"score\", \"dozen\", and \"gross\". (Larger number words like \"million\" are not part of the original English system; they are scholarly creations based ultimately on Latin.)\n\nThe German language counts similarly to English, but the unit is placed first in numbers over 20. For example, \"26\" is \"sechsundzwanzig\", literally \"six and twenty\". This system was formerly common in English, as seen in an artifact from the English nursery rhyme \"Sing a Song of Sixpence\": \"Sing a song of sixpence, / a pocket full of rye. / Four and twenty blackbirds, / baked in a pie.\" It persists in some children's songs such as \"One and Twenty.\"\n\nIn the French language as used in France, one sees some differences. \"Soixante-dix\" (literally, \"sixty-ten\") is used for \"seventy\". The words \"quatre-vingt\" (literally, \"four-twenty\", or 80) and \"quatre-vingt-dix\" (literally, \"four-twenty ten\" 90) are based on 20 (\"vingt\") instead of 10. Swiss French and Belgian French do not use these forms, preferring more standard Latinate forms: \"septante\" for 70, \"octante\" for 80 and \"nonante\" for 90; In Swiss, they even replaced 80 with \"huitante\" (See the article ), which can date back to 12th century\n\nIn ancient Mesopotamia, the base for constructing numbers was 60, with 10 used as an intermediate base for numbers below 60.\n\nMany West African languages base their number words on a combination of 5 and 20, derived from thinking of a complete hand or a complete set of digits comprising both fingers and toes. In fact, in some languages, the words for 5 and 20 refer to these body parts (e.g., a word for 20 that means \"man complete\"). The words for numbers below 20 are based on 5 and higher numbers combine the lower numbers with multiples and powers of 20. Of course, this description of hundreds of languages is badly oversimplified; better information and references can be found in Zaslavsky (1973).\n\nMany systems of finger counting have been, and still are, used in various parts of the world. Most are not as obvious as holding up a number of fingers. The position of fingers may be most important. One continuing use for finger counting is for people who speak different languages to communicate prices in the marketplace.\n\nIn contrast to finger counting, the Yuki people (indigenous Americans from Northern California) keep count by using the four spaces between their fingers rather than the fingers themselves. This is known as an octal (base-8) counting system.\n\nThis area of ethnomathematics mainly focuses on addressing Eurocentrism by countering the common belief that most worthwhile mathematics known and used today was developed in the Western world.\nThe area stresses that \"the history of mathematics has been oversimplified\",\nand seeks to explore the emergence of mathematics from various ages and civilizations throughout human history.\n\nD'Ambrosio's 1980 review of the evolution of mathematics, his 1985 appeal to include ethnomathematics in the history of mathematics and his 2002 paper about the historiographical approaches to non-Western mathematics are excellent examples. Additionally, Frankenstein and Powell's 1989 attempt to redefine mathematics from a non-eurocentric viewpoint and Anderson's 1990 concepts of world mathematics are strong contributions to this area. Detailed examinations of the history of the mathematical developments of non-European civilizations, such as the mathematics of ancient Japan, Iraq, Egypt, and of Islamic, Hebrew, and Incan civilizations, have also been presented.\n\nThe core of any debate about the cultural nature of mathematics will ultimately lead to an examination of the nature of mathematics itself. One of the oldest and most controversial topics in this area is whether mathematics is internal or external, tracing back to the arguments of Plato, an externalist, and Aristotle, an internalist. On the one hand, Internalists such as Bishop, Stigler and Baranes, believe mathematics to be a cultural product. On the other hand, externalists, like Barrow, Chevallard and Penrose, see mathematics as culture-free, and tend to be major critics of ethnomathematics. With disputes about the nature of mathematics, come questions about the nature of ethnomathematics, and the question of whether ethnomathematics is part of mathematics or not. Barton, who has offered the core of research about ethnomathematics and philosophy, asks whether \"ethnomathematics is a precursor, parallel body of knowledge or precolonized body of knowledge\" to mathematics and if it is even possible for us to identify all types of mathematics based on a Western-epistemological foundation.\n\nThe contributions in this area try to illuminate how mathematics has affected the nonacademic areas of society. One of the most controversial and provocative political components of ethnomathematics is its racial implications. Ethnomathematicians purport that the prefix \"ethno\" should not be taken as relating to race, but rather, the cultural traditions of groups of people. However, in places like South Africa concepts of culture, ethnicity and race are not only intertwined but carry strong, divisive negative connotations. So, although it may be made explicit that ethnomathematics is not a \"racist doctrine\" it is vulnerable to association with racism.\n\nAnother major facet of this area addresses the relationship between gender and mathematics. This looks at topics such as discrepancies between male and female math performance in educations and career-orientation, societal causes, women's contributions to mathematics research and development, etc.\n\nGerdes' writings about how mathematics can be used in the school systems of Mozambique and South Africa, and D'Ambrosio's 1990 discussion of the role mathematics plays in building a democratic and just society are examples of the impact mathematics can have on developing the identity of a society. In 1990, Bishop also writes about the powerful and dominating influence of Western mathematics. More specific examples of the political impact of mathematics are seen in Knijik's 1993 study of how Brazilian sugar cane farmers could be politically and economically armed with mathematics knowledge, and Osmond's analysis of an employer's perceived value of mathematics (2000).\n\nThe focus of this area is to introduce the mathematical ideas of people who have generally been excluded from discussions of formal, academic mathematics. The research of the mathematics of these cultures indicates two, slightly contradictory viewpoints. The first supports the objectivity of mathematics and that it is something discovered not constructed. The studies reveal that all cultures have basic counting, sorting and deciphering methods, and that these have arisen independently in different places around the world. This can be used to argue that these mathematical concepts are being discovered rather than created. However, others emphasize that the usefulness of mathematics is what tends to conceal its cultural constructs. Naturally, it is not surprising that extremely practical concepts such as numbers and counting have arisen in all cultures. The universality of these concepts, however, seems harder to sustain as more and more research reveals practices which are typically mathematical, such as counting, ordering, sorting, measuring and weighing, done in radically different ways (see Section 2.1: Numerals and Naming Systems).\n\nOne of the challenges faced by researchers in this area is the fact that they are limited by their own mathematical and cultural frameworks. The discussions of the mathematical ideas of other cultures recast these into a Western framework in order to identify and understand them. This raises the questions of how many mathematical ideas evade notice simply because they lack similar Western mathematical counterparts, and of how to draw the line classifying mathematical from non-mathematical ideas.\n\nThe majority of research in this area has been about the intuitive mathematical thinking of small-scale, traditional, indigenous cultures including: Aboriginal Australians, the indigenous people of Liberia, Native Americans in North America, Pacific Islanders, Brazilian construction foremen, and tribes in Africa.\n\nAn enormous variety of games that can be analyzed mathematically have been played around the world and through history. The interest of the ethnomathematician usually centers on the ways in which the game represents informal mathematical thought as part of ordinary society, but sometimes has extended to mathematical analyses of games. It does not include the careful analysis of good play—but it may include the social or mathematical aspects of such analysis.\n\nA mathematical game that is well known in European culture is tic-tac-toe (noughts-and-crosses). This is a geometrical game played on a 3-by-3 square; the goal is to form a straight line of three of the same symbol. There are many broadly similar games from all parts of England, to name only one country where they are found.\n\nAnother kind of geometrical game involves objects that move or jump over each other within a specific shape (a \"board\"). There may be captures. The goal may be to eliminate the opponent's pieces, or simply to form a certain configuration, e.g., to arrange the objects according to a rule. One such game is Nine Men's Morris; it has innumerable relatives where the board or setup or moves may vary, sometimes drastically. This kind of game is well suited to play out of doors with stones on the dirt, though now it may use plastic pieces on a paper or wooden board.\n\nA mathematical game found in West Africa is to draw a certain figure by a line that never ends until it closes the figure by reaching the starting point (in mathematical terminology, this is a Eulerian path on a graph). Children use sticks to draw these in the dirt or sand, and of course the game can be played with pen and paper.\n\nThe games of checkers, chess, oware (and other mancala games), and Go may also be viewed as subjects for ethnomathematics.\n\nOne way mathematics appears in art is through symmetries. Woven designs in cloth or carpets (to name two) commonly have some kind of symmetrical arrangement. A rectangular carpet often has rectangular symmetry in the overall pattern. A woven cloth may exhibit one of the seventeen kinds of plane symmetry groups; see Crowe (2004) for an illustrated mathematical study of African weaving patterns. Several types of patterns discovered by ethnomathematical communities are related to technologies; see Berczi (2002) about illustrated mathematical study of patterns and symmetry in Eurasia. Following the analysis of Indonesian folk weaving patterns and Batak traditional architectural ornaments, the geometry of Indonesian traditional motifs of batik is analyzed by Hokky Situngkir that eventually made a new genre of fractal batik designs as generative art; see Situngkir and Surya (2007) for implementations.\n\nEthnomathematics and mathematics education addresses first, how cultural values can affect teaching, learning and curriculum, and second, how mathematics education can then affect the political and social dynamics of a culture. One of the stances taken by many educators is that it is crucial to acknowledge the cultural context of mathematics students by teaching culturally based mathematics that students can relate to. Can teaching math through cultural relevance and personal experiences help the learners know more about reality, culture, society and themselves? Robert (2006)\n\nAnother approach suggested by mathematics educators is exposing students to the mathematics of a variety of different cultural contexts, often referred to as multicultural math. This can be used both to increase the social awareness of students and offer alternative methods of approaching conventional mathematics operations, like multiplication.(Andrew, 2005)\n\nVarious mathematics educators have explored ways of bringing together culture and mathematics in the classroom, such as: Barber and Estrin (1995) and Bradley (1984) on Native American education, Gerdes (1988b and 2001) with suggestions for using African art and games, Malloy (1997) about African American students and Flores (1997), who developed instructional strategies for Hispanic students.\n\nSome critics claim that mathematics education in some countries, including the United States, unduly emphasizes ethnomathematics in order to promote multiculturalism while spending too little time on core mathematical content, and that this often results in pseudoscience being taught. An example of this criticism is an article by Marianne M. Jennings. Another example is Richard Askey, who accuses \"Focus on Algebra\", the same Addison-Wesley textbook criticized by Marianne M. Jennings, of teaching pseudoscience, claiming for South Sea islanders mystic knowledge of astronomy more advanced than scientific knowledge.\n\n\n"}
{"id": "13505846", "url": "https://en.wikipedia.org/wiki?curid=13505846", "title": "European Congress of Mathematics", "text": "European Congress of Mathematics\n\nThe European Congress of Mathematics (ECM) is an international congress of the mathematics community, held every four years. Its objectives are \"to present various new aspects of pure and applied mathematics to a wide audience, to be a forum for discussion of the relationship between mathematics and society in Europe, and to enhance cooperation among mathematicians from all European countries.\"\n\nThe Congress is held under the auspices of the European Mathematical Society (EMS), and was one of its earliest initiatives. The EMS Prizes are awarded at the beginning of the Congress.\n\n\nThe 8th European Congress of Mathematics will be held in Slovenia in 2020.\n\n"}
{"id": "191933", "url": "https://en.wikipedia.org/wiki?curid=191933", "title": "Exponential growth", "text": "Exponential growth\n\nExponential growth is exhibited when the rate of change—the change per instant or unit of time—of the value of a mathematical function is proportional to the function's current value, resulting in its value at any time being an exponential function of time, i.e., a function in which the time value is the exponent.\nExponential decay occurs in the same way when the growth rate is negative. In the case of a discrete domain of definition with equal intervals, it is also called geometric growth or geometric decay, the function values forming a geometric progression. In either exponential growth or exponential decay, the ratio of the rate of change of the quantity to its current size remains constant over time.\n\nThe formula for exponential growth of a variable \"x\" at the growth rate \"r\", as time \"t\" goes on in discrete intervals (that is, at integer times 0, 1, 2, 3, ...), is\n\nwhere \"x\" is the value of \"x\" at time 0. This formula is transparent when the exponents are converted to multiplication. For instance, with a starting value of 50 and a growth rate of per interval, the passage of one interval would give ; two intervals would give ; and three intervals would give . In this way, each increase in the exponent by a full interval can be seen to increase the previous total by another five percent. (The order of multiplication does not change the result based on the associative property of multiplication.)\n\nSince the time variable, which is the input to this function, occurs as the exponent, this is an exponential function. This contrasts with growth based on a power function, where the time variable is the base value raised to a fixed exponent, such as cubic growth (or in general terms denoted as polynomial growth).\n\n\nA quantity \"x\" depends exponentially on time \"t\" if\n\nwhere the constant \"a\" is the initial value of \"x\",\n\nthe constant \"b\" is a positive growth factor, and \"τ\" is the time constant—the time required for \"x\" to increase by one factor of \"b\":\n\nIf and , then \"x\" has exponential growth. If and , or and 0 < , then \"x\" has exponential decay.\n\nExample: \"If a species of bacteria doubles every ten minutes, starting out with only one bacterium, how many bacteria would be present after one hour?\" The question implies \"a\" = 1, \"b\" = 2 and \"τ\" = 10 min.\n\nAfter one hour, or six ten-minute intervals, there would be sixty-four bacteria.\n\nMany pairs (\"b\", \"τ\") of a dimensionless non-negative number \"b\" and an amount of time \"τ\" (a physical quantity which can be expressed as the product of a number of units and a unit of time) represent the same growth rate, with \"τ\" proportional to log \"b\". For any fixed \"b\" not equal to 1 (e.g. \"e\" or 2), the growth rate is given by the non-zero time \"τ\". For any non-zero time \"τ\" the growth rate is given by the dimensionless positive number \"b\".\n\nThus the law of exponential growth can be written in different but mathematically equivalent forms, by using a different base. The most common forms are the following:\n\nwhere \"x\" expresses the initial quantity \"x\"(0).\n\nParameters (negative in the case of exponential decay):\nThe quantities \"k\", \"τ\", and \"T\", and for a given \"p\" also \"r\", have a one-to-one connection given by the following equation (which can be derived by taking the natural logarithm of the above):\n\nwhere \"k\" = 0 corresponds to \"r\" = 0 and to \"τ\" and \"T\" being infinite.\n\nIf \"p\" is the unit of time the quotient \"t\"/\"p\" is simply the number of units of time. Using the notation \"t\" for the (dimensionless) number of units of time rather than the time itself, \"t\"/\"p\" can be replaced by \"t\", but for uniformity this has been avoided here. In this case the division by \"p\" in the last formula is not a numerical division either, but converts a dimensionless number to the correct quantity including unit.\n\nA popular approximated method for calculating the doubling time from the growth rate is the rule of 70,\ni.e. formula_9.\n\nIf a variable \"x\" exhibits exponential growth according to formula_10, then the log (to any base) of \"x\" grows linearly over time, as can be seen by taking logarithms of both sides of the exponential growth equation:\n\nThis allows an exponentially growing variable to be modeled with a log-linear model. For example, if one wishes to empirically estimate the growth rate from intertemporal data on \"x\", one can linearly regress log \"x\" on \"t\".\n\nThe exponential function formula_12 satisfies the linear differential equation:\n\nsaying that the change per instant of time of \"x\" at time \"t\" is proportional to the value of \"x\"(\"t\"), and \"x\"(\"t\") has the initial value\n\nThe differential equation is solved by direct integration:\n\nso that\n\nIn the above differential equation, if , then the quantity experiences exponential decay.\n\nFor a nonlinear variation of this growth model see logistic function.\n\nThe difference equation\n\nhas solution\n\nshowing that \"x\" experiences exponential growth.\n\nIn the long run, exponential growth of any kind will overtake linear growth of any kind (the basis of the Malthusian catastrophe) as well as any polynomial growth, i.e., for all α:\n\nThere is a whole hierarchy of conceivable growth rates that are slower than exponential and faster than linear (in the long run). See Degree of a polynomial#The degree computed from the function values.\n\nGrowth rates may also be faster than exponential. In the most extreme case, when growth increases without bound in finite time, it is called hyperbolic growth. In between exponential and hyperbolic growth lie more classes of growth behavior, like the hyperoperations beginning at tetration, and formula_20, the diagonal of the Ackermann function.\n\nExponential growth models of physical phenomena only apply within limited regions, as unbounded growth is not physically realistic. Although growth may initially be exponential, the modelled phenomena will eventually enter a region in which previously ignored negative feedback factors become significant (leading to a logistic growth model) or other underlying assumptions of the exponential growth model, such as continuity or instantaneous feedback, break down.\n\nAccording to an old legend, vizier Sissa Ben Dahir presented an Indian King Sharim with a beautiful, hand-made chessboard. The king asked what he would like in return for his gift and the courtier surprised the king by asking for one grain of rice on the first square, two grains on the second, four grains on the third etc. The king readily agreed and asked for the rice to be brought. All went well at first, but the requirement for 2 grains on the \"n\"th square demanded over a million grains on the 21st square, more than a million million ( trillion) on the 41st and there simply was not enough rice in the whole world for the final squares. (From Swirski, 2006)\n\nThe second half of the chessboard is the time when an exponentially growing influence is having a significant economic impact on an organization's overall business strategy.\n\nFrench children are told a story in which they imagine having a pond with water lily leaves floating on the surface. The lily population doubles in size every day and, if left unchecked, it will smother the pond in thirty days killing all the other living things in the water. Day after day, the plant's growth is small and so it is decided that it shall be cut down when the water lilies cover half of the pond. The children are then asked on what day will half of the pond be covered in water lilies. The solution is simple when one considers that the water lilies must double to completely cover the pond on the thirtieth day. Therefore, the water lilies will cover half of the pond on the twenty-ninth day. There is only one day to save the pond. (From Meadows \"et al\". 1972)\n\n\n"}
{"id": "2445607", "url": "https://en.wikipedia.org/wiki?curid=2445607", "title": "Formulario mathematico", "text": "Formulario mathematico\n\nFormulario Mathematico (Latino sine flexione: \"Formulation of mathematics\") is a book by Giuseppe Peano which expresses fundamental theorems of mathematics in a symbolic language developed by Peano. The author was assisted by Giovanni Vailati, Mario Pieri, Alessandro Padoa, Giovanni Vacca, Vincenzo Vivanti, Gino Fano and Cesare Burali-Forti.\n\nThe \"Formulario\" was first published in 1895. The fifth and last edition was published in 1908. \n\nKennedy wrote \"the development and use of mathematical logic is the guiding motif of the project\". He also explains the variety of Peano's publication under the title:\n\nPeano believed that students needed only precise statement of their lessons. He wrote:\nSuch a dismissal of the oral tradition in lectures at universities was the undoing of Peano's own teaching career.\n\n"}
{"id": "43839113", "url": "https://en.wikipedia.org/wiki?curid=43839113", "title": "Glossary of Principia Mathematica", "text": "Glossary of Principia Mathematica\n\nThis is a list of the notation used in Alfred North Whitehead and Bertrand Russell's \"Principia Mathematica\" (1910–13).\n\nThe second (but not the first) edition of volume I has a list of notation used at the end.\n\nThis is a glossary of some of the technical terms in \"Principia Mathematica\" that are no longer widely used or whose meaning has changed.\n\n\n\n"}
{"id": "293606", "url": "https://en.wikipedia.org/wiki?curid=293606", "title": "Graph paper", "text": "Graph paper\n\nGraph paper, coordinate paper, grid paper, or squared paper is writing paper that is printed with fine lines making up a regular grid. The lines are often used as guides for plotting mathematical functions or experimental data and drawing two-dimensional graphs. It is commonly found in mathematics and engineering education settings and in laboratory notebooks. Graph paper is available either as loose leaf paper or bound in notebooks.\n\nThe first commercially published \"coordinate paper\" is usually attributed to Dr. Buxton of England, who patented paper, printed with a rectangular coordinate grid, in 1794. A century later, E. H. Moore, a distinguished mathematician at the University of Chicago, advocated usage of paper with \"squared lines\" by students of high schools and universities. The 1906 edition of \"Algebra for Beginners\" by H. S. Hall and S. R. Knight included a strong statement that \"the squared paper should be of good quality and accurately ruled to inches and tenths of an inch. Experience shows that anything on a smaller scale (such as 'millimeter' paper) is practically worthless in the hands of beginners.\"\n\nThe term \"graph paper\" did not catch on quickly in American usage. \"A School Arithmetic\" (1919) by H. S. Hall and F. H. Stevens had a chapter on graphing with \"squared paper\". \"Analytic Geometry\" (1937) by W. A. Wilson and J. A. Tracey used the phrase \"coordinate paper\". The term \"squared paper\" remained in British usage for longer; for example it was used in \"Public School Arithmetic\" (1961) by W. M. Baker and A. A. Bourne published in London.\n\n\nIn general, graphs showing grids are sometimes called Cartesian graphs because the square can be used to map measurements onto a Cartesian (x vs. y) coordinate system. It is also available without lines but with dots at the positions where the lines would intersect.\n\n\n"}
{"id": "39516424", "url": "https://en.wikipedia.org/wiki?curid=39516424", "title": "Grey box model", "text": "Grey box model\n\nIn mathematics, statistics, and computational modelling, a grey box model combines a partial theoretical structure with data to complete the model. The theoretical structure may vary from information on the smoothness of results, to models that need only parameter values from data or existing literature. Thus, almost all models are grey box models as opposed to black box where no model form is assumed or white box models that are purely theoretical. Some models assume a special form such as a linear regression or neural network. These have special analysis methods. In particular linear regression techniques are much more efficient than most non-linear techniques. The model can be deterministic or stochastic (i.e. containing random components) depending on its planned use.\n\nThe general case is a non-linear model with a partial theoretical structure and some unknown parts derived from data. Models with unlike theoretical structures need to be evaluated individually, possibly using simulated annealing or genetic algorithms.\n\nWithin a particular model structure, parameters or variable parameter relations may need to be found. For a particular structure it is arbitrarily assumed that the data consists of sets of feed vectors f, product vectors p, and operating condition vectors c. Typically c will contain values extracted from f, as well as other values. In many cases a model can be converted to a function of the form:\nwhere the vector function m gives the errors between the data p, and the model predictions. The vector q gives some variable parameters that are the model's unknown parts.\n\nThe parameters q vary with the operating conditions c in a manner to be determined. This relation can be specified as q = Ac where A is a matrix of unknown coefficients, and c as in linear regression includes a constant term and possibly transformed values of the original operating conditions to obtain non-linear relations between the original operating conditions and q. It is then a matter of selecting which terms in A are non-zero and assigning their values. The model completion becomes an optimisation problem to determine the non-zero values in A that minimizes the error terms m(f,p,Ac) over the data.\n\nOnce a selection of non-zero values is made, the remaining coefficients in A can be determined by minimizing \"m\"(\"f\",\"p\",\"Ac\") over the data with respect to the nonzero values in A, typically by non-linear least squares. Selection of the nonzero terms can be done by optimization methods such as simulated annealing and evolutionary algorithms. Also the non-linear least squares can provide accuracy estimates for the elements of A that can be used to determine if they are significantly different from zero, thus providing a method of term selection.\n\nIt is sometimes possible to calculate values of q for each data set, directly or by non-linear least squares. Then the more efficient linear regression can be used to predict q using c thus selecting the non-zero values in A and estimating their values. Once the non-zero values are located non-linear least squares can be used on the original model m(f,p,Ac) to refine these values .\n\nA third method is model inversion, which converts the non-linear m(f,p,Ac) into an approximate linear form in the elements of A, that can be examined using efficient term selection and evaluation of the linear regression. For the simple case of a single q value (q = ac) and an estimate q* of q. Putting dq = ac − q* gives\n\nso that a is now in a linear position with all other terms known, and thus can be analyzed by linear regression techniques. For more than one parameter the method extends in a direct manner. After checking that the model has been improved this process can be repeated until convergence. This approach has the advantages that it does not need the parameters q to be able to be determined from an individual data set and the linear regression is on the original error terms\n\nWhere sufficient data is available, division of the data into a separate model construction set and one or two evaluation sets is recommended. This can be repeated using multiple selections of the construction set and the resulting models averaged or used to evaluate prediction differences.\n\nA statistical test such as chi-squared on the residuals is not particularly useful. The chi squared test requires known standard deviations which are seldom available, and failed tests give no indication of how to improve the model\n\nAn attempt to predict the residuals m(, ) with the operating conditions c using linear regression will show if the residuals can be predicted. Residuals that cannot be predicted offer little prospect of improving the model using the current operating conditions. Terms that do predict the residuals are prospective terms to incorporate into the model to improve its performance.\n\nThe model inversion technique above can be used as a method of determining whether a model can be improved. In this case selection of nonzero terms is not so important and linear prediction can be done using the significant eigenvectors of the regression matrix. The values in A determined in this manner need to be substituted into the nonlinear model to assess improvements in the model errors. The absence of a significant improvement indicates the available data is not able to improve the current model form using the defined parameters. Extra parameters can be inserted into the model to make this test more comprehensive.\n"}
{"id": "9226345", "url": "https://en.wikipedia.org/wiki?curid=9226345", "title": "Hat operator", "text": "Hat operator\n\nThe hat operator is a mathematical notation with various uses in different branches of science and mathematics.\n\nIn statistics, the hat matrix \"H\" projects the observed values y of response variable to the predicted values ŷ: \n\nIn screw theory, one use of the hat operator is to represent the cross product operation. Since the cross product is a linear transformation, it can be represented as a matrix. The hat operator takes a vector and transforms it into its equivalent matrix. \n\nFor example, in three dimensions,\n\nIn statistics, the hat is used to denote an estimator or an estimated value, as opposed to its theoretical counterpart. For example, in the context of errors and residuals, the \"hat\" over the letter ε indicates an observable estimate (the residuals) of an unobservable quantity called ε (the statistical errors).\n\n"}
{"id": "17201962", "url": "https://en.wikipedia.org/wiki?curid=17201962", "title": "Helen Abbot Merrill", "text": "Helen Abbot Merrill\n\nHelen Abbot Merrill (1864 – 1949) was an American mathematician, educator and textbook author.\n\nBorn March 30, 1876 to a New Jersey insurance claims adjuster and a housewife, and raised in Massachusetts, her family tree included colonial settlers. Young Helen's formal education started at a high school in Massachusetts, and after graduating she went to Wellesley College, where she intended to major in Greek and Latin. Unusually, the mathematics faculty at the college consisted mostly of women, including Ellen Hayes, and before completing her first years, Helen Merrill had decided to major in mathematics instead of languages. In 1893 she began teaching at Wellesley while also studying and guest lecturing abroad. In 1903 she earned a PhD in mathematics at Yale under the direction of James Pierpont. In 1920 she was appointed vice-president of the Mathematical Association of America. Upon her retirement from Wellesley, she was given the title Professor Emerita.\n\nAt Wellesley, Merrill wrote two textbooks with Clara Eliza Smith, \"Selected Topics in Higher Algebra\" (Norwood, 1914) and \"A First Course in Higher Algebra\" (Macmillan, 1917).\nShe also wrote as a popularizer a book titled \"Mathematical Excursions\" in 1933.\n\n\n"}
{"id": "1930406", "url": "https://en.wikipedia.org/wiki?curid=1930406", "title": "Impredicativity", "text": "Impredicativity\n\nSomething that is impredicative, in mathematics, logic and philosophy of mathematics, is a self-referencing definition. Roughly speaking, a definition is impredicative if it invokes (mentions or quantifies over) the set being defined, or (more commonly) another set that contains the thing being defined. There is no generally accepted precise definition of what it means to be predicative or impredicative. Authors have given different but related definitions.\n\nThe opposite of impredicativity is predicativity, which essentially entails building stratified (or ramified) theories where quantification over lower levels results in variables of some new type, distinguished from the lower types that the variable ranges over. A prototypical example is intuitionistic type theory, which retains ramification so as to discard impredicativity.\n\nRussell's paradox is a famous example of an impredicative construction—namely the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be asked whether it contains itself or not — if it does then by definition it should not, and if it does not then by definition it should.\n\nThe greatest lower bound of a set , , also has an impredicative definition: if and only if for all elements of , is less than or equal to , and any less than or equal to all elements of is less than or equal to . This definition quantifies over the set (potentially infinite, depending on the order in question) whose members are the lower bounds of , one of which being the glb itself. Hence predicativism would reject this definition.\n\nThe terms \"predicative\" and \"impredicative\" were introduced by , though the meaning has changed a little since then. \n\nSolomon Feferman provides a historical review of predicativity, connecting it to current outstanding research problems.\n\nThe vicious circle principle was suggested by Henri Poincaré (1905-6, 1908) and Bertrand Russell in the wake of the paradoxes as a requirement on legitimate set specifications. Sets that do not meet the requirement are called \"impredicative\".\n\nThe first modern paradox appeared with Cesare Burali-Forti's 1897 \"A question on transfinite numbers\" and would become known as the Burali-Forti paradox. Cantor had apparently discovered the same paradox in his (Cantor's) \"naive\" set theory and this become known as Cantor's paradox. Russell's awareness of the problem originated in June 1901 with his reading of Frege's treatise of mathematical logic, his 1879 \"Begriffsschrift\"; the offending sentence in Frege is the following:\nIn other words, given the function is the variable and is the invariant part. So why not substitute the value for itself? Russell promptly wrote Frege a letter pointing out that:\nFrege promptly wrote back to Russell acknowledging the problem:\nWhile the problem had adverse personal consequences for both men (both had works at the printers that had to be emended), van Heijenoort observes that \"The paradox shook the logicians' world, and the rumbles are still felt today. ... Russell's paradox, which uses the bare notions of set and element, falls squarely in the field of logic. The paradox was first published by Russell in \"The principles of mathematics\" (1903) and is discussed there in great detail ...\". Russell, after six years of false starts, would eventually answer the matter with his 1908 theory of types by \"propounding his \"axiom of reducibility\". It says that any function is coextensive with what he calls a \"predicative\" function: a function in which the types of apparent variables run no higher than the types of the arguments\". But this \"axiom\" was met with resistance from all quarters.\n\nThe rejection of impredicatively defined mathematical objects (while accepting the natural numbers as classically understood) leads to the position in the philosophy of mathematics known as predicativism, advocated by Henri Poincaré and Hermann Weyl in his \"Das Kontinuum\". Poincaré and Weyl argued that impredicative definitions are problematic only when one or more underlying sets are infinite.\n\nErnst Zermelo in his 1908 \"A new proof of the possibility of a well-ordering\" presents an entire section \"b. \"Objection concerning nonpredicative definition\"\" where he argued against \"Poincaré (1906, p. 307) [who states that] a definition is 'predicative' and logically admissible only if it \"excludes\" all objects that are dependent upon the notion defined, that is, that can in any way be determined by it\". He gives two examples of impredicative definitions – (i) the notion of Dedekind chains and (ii) \"in analysis wherever the maximum or minimum of a previously defined \"completed\" set of numbers is used for further inferences. This happens, for example, in the well-known Cauchy proof of the fundamental theorem of algebra, and up to now it has not occurred to anyone to regard this as something illogical\". He ends his section with the following observation: \"A definition may very well rely upon notions that are equivalent to the one being defined; indeed, in every definition \"definiens\" and \"definiendum\" are equivalent notions, and the strict observance of Poincaré's demand would make every definition, hence all of science, impossible\".\n\nZermelo's example of minimum and maximum of a previously defined \"completed\" set of numbers reappears in Kleene 1952:42-42 where Kleene uses the example of Least upper bound in his discussion of impredicative definitions; Kleene does not resolve this problem. In the next paragraphs he discusses Weyl's attempt in his 1918 \"Das Kontinuum\" (\"The Continuum\") to eliminate impredicative definitions and his failure to retain the \"theorem that an arbitrary non-empty set of real numbers having an upper bound has a least upper bound (cf. also Weyl 1919)\".\n\nRamsey argued that \"impredicative\" definitions can be harmless: for instance, the definition of \"tallest person in the room\" is impredicative, since it depends on a set of things of which it is an element, namely the set of all persons in the room. Concerning mathematics, an example of an impredicative definition is the smallest number in a set, which is formally defined as: if and only if for all elements of , is less than or equal to , and is in .\n\nBurgess (2005) discusses predicative and impredicative theories at some length, in the context of Frege's logic, Peano arithmetic, second order arithmetic, and axiomatic set theory.\n\n\n"}
{"id": "48606512", "url": "https://en.wikipedia.org/wiki?curid=48606512", "title": "Lagrange stability", "text": "Lagrange stability\n\nLagrange stability is a concept in the stability theory of dynamical systems, named after Joseph-Louis Lagrange.\n\nFor any point in the state space, formula_1 in a real continuous dynamical system formula_2, where formula_3 is formula_4, the motion formula_5 is said to be \"positively Lagrange stable\" if the positive semi-orbit formula_6 is compact. If the negative semi-orbit formula_7 is compact, then the motion is said to be \"negatively Lagrange stable\". The motion through formula_8 is said to be \"Lagrange stable\" if it is both positively and negatively Lagrange stable. If the state space formula_9 is the Euclidean space formula_10, then the above definitions are equivalent to formula_11 and formula_12 being bounded, respectively.\n\nA dynamical system is said to be positively-/negatively-/Lagrange stable if \"for each\" formula_1, the motion formula_5 is positively-/negativey-/Lagrange stable, respectively.\n\n"}
{"id": "1595681", "url": "https://en.wikipedia.org/wiki?curid=1595681", "title": "Lie theory", "text": "Lie theory\n\nIn mathematics, the researcher Sophus Lie ( ) initiated lines of study involving integration of differential equations, transformation groups, and contact of spheres that have come to be called Lie theory. For instance, the latter subject is Lie sphere geometry. This article addresses his approach to transformation groups, which is one of the areas of mathematics, and was worked out by Wilhelm Killing and Élie Cartan.\n\nThe foundation of Lie theory is the exponential map relating Lie algebras to Lie groups which is called the Lie group–Lie algebra correspondence. The subject is part of differential geometry since Lie groups are differentiable manifolds. Lie groups evolve out of the identity (1) and the tangent vectors to one-parameter subgroups generate the Lie algebra. The structure of a Lie group is implicit in its algebra, and the structure of the Lie algebra is expressed by root systems and root data.\n\nLie theory has been particularly useful in mathematical physics since it describes important physical groups such as the Galilean group, the Lorentz group and the Poincaré group.\n\nThe one-parameter groups are the first instance of Lie theory. The compact case arises through Euler's formula in the complex plane. Other one-parameter groups occur in the split-complex number plane as the unit hyperbola\nand in the dual number plane as the line formula_2\nIn these cases the Lie algebra parameters have names: angle, hyperbolic angle, and slope. Using the appropriate \"angle\", and a radial vector, any one of these planes can be given a polar decomposition. Any one of these decompositions, or Lie algebra renderings, may be necessary for rendering the Lie subalgebra of a 2 × 2 real matrix.\n\nThere is a classical 3-parameter Lie group and algebra pair: the quaternions of unit length which can be identified with the 3-sphere. Its Lie algebra is the subspace of quaternion vectors. Since the commutator ij − ji = 2k, the Lie bracket in this algebra is twice the cross product of ordinary vector analysis.\n\nAnother elementary 3-parameter example is given by the Heisenberg group and its Lie algebra.\nStandard treatments of Lie theory often begin with the classical groups.\n\nEarly expressions of Lie theory are found in books composed by Sophus Lie with Friedrich Engel and Georg Scheffers from 1888 to 1896.\n\nIn Lie's early work, the idea was to construct a theory of \"continuous groups\", to complement the theory of discrete groups that had developed in the theory of modular forms, in the hands of Felix Klein and Henri Poincaré. The initial application that Lie had in mind was to the theory of differential equations. On the model of Galois theory and polynomial equations, the driving conception was of a theory capable of unifying, by the study of symmetry, the whole area of ordinary differential equations.\n\nAccording to historian Thomas W. Hawkins, it was Élie Cartan that made Lie theory what it is:\n\nLie theory is frequently built upon a study of the classical linear algebraic groups. Special branches include Weyl groups, Coxeter groups, and buildings. The classical subject has been extended to Groups of Lie type.\n\nIn 1900 David Hilbert challenged Lie theorists with his Fifth Problem presented at the International Congress of Mathematicians in Paris.\n\n\n\n"}
{"id": "40400729", "url": "https://en.wikipedia.org/wiki?curid=40400729", "title": "Limiting case (mathematics)", "text": "Limiting case (mathematics)\n\nIn mathematics, a limiting case of a mathematical object is a special case that arises when one or more components of the object take on their most extreme possible values. For example:\n\n\nA limiting case is sometimes a degenerate case in which some qualitative properties differ from the corresponding properties of the generic case. For example:\n\n"}
{"id": "44051448", "url": "https://en.wikipedia.org/wiki?curid=44051448", "title": "List of Martin Gardner Mathematical Games columns", "text": "List of Martin Gardner Mathematical Games columns\n\nOver a period of 24 years (January 1957 – December 1980), Martin Gardner wrote 288 consecutive \"Mathematical Games\" columns for \"Scientific American\" magazine. Subsequently, he alternated with other authors, producing 9 more columns under that title (February 1981 – June 1986), for a total of 297. These are listed in chronological order below.\n\nThe subjects of twelve of the columns provided the cover art for the magazine in the month they were published. These columns are indicated in the table below by the word [cover] and a link to the associated cover.\n\nGardner wrote 5 other articles for \"Scientific American\". His flexagon article in December 1956 was in all but name the first article in the series of \"Mathematical Games\" columns and led directly to the series which began the following month. These five articles are listed below.\n\n"}
{"id": "632487", "url": "https://en.wikipedia.org/wiki?curid=632487", "title": "List of algorithm general topics", "text": "List of algorithm general topics\n\nThis is a list of algorithm general topics. \n\n\n"}
{"id": "33900798", "url": "https://en.wikipedia.org/wiki?curid=33900798", "title": "List of formulas in elementary geometry", "text": "List of formulas in elementary geometry\n\nThis is a short list of some common mathematical shapes and figures and the formulas that describe them.\n"}
{"id": "9212913", "url": "https://en.wikipedia.org/wiki?curid=9212913", "title": "List of impossible puzzles", "text": "List of impossible puzzles\n\nThis is a list of puzzles that cannot be solved.\n\n\n"}
{"id": "241820", "url": "https://en.wikipedia.org/wiki?curid=241820", "title": "List of mathematical shapes", "text": "List of mathematical shapes\n\nFollowing is a list of some mathematically well-defined shapes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"See the list of algebraic surfaces.\"\n\n\n\nThis table shows a summary of regular polytope counts by dimension.\n\nThere are no nonconvex Euclidean regular tessellations in any number of dimensions.\n\nThe elements of a polytope can be considered according to either their own dimensionality or how many dimensions \"down\" they are from the body.\n\n\nFor example, in a polyhedron (3-dimensional polytope), a face is a facet, an edge is a ridge, and a vertex is a peak.\n\n\nThe classical convex polytopes may be considered tessellations, or tilings, of spherical space. Tessellations of euclidean and hyperbolic space may also be considered regular polytopes. Note that an 'n'-dimensional polytope actually tessellates a space of one dimension less. For example, the (three-dimensional) platonic solids tessellate the 'two'-dimensional 'surface' of the sphere.\n\n\nThere is only one polytope in 1 dimension, whose boundaries are the two endpoints of a line segment, represented by the empty Schläfli symbol {}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolygons named for their number of sides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971799", "url": "https://en.wikipedia.org/wiki?curid=5971799", "title": "List of mathematicians (C)", "text": "List of mathematicians (C)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971806", "url": "https://en.wikipedia.org/wiki?curid=5971806", "title": "List of mathematicians (G)", "text": "List of mathematicians (G)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971828", "url": "https://en.wikipedia.org/wiki?curid=5971828", "title": "List of mathematicians (R)", "text": "List of mathematicians (R)\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "462534", "url": "https://en.wikipedia.org/wiki?curid=462534", "title": "Lotka–Volterra equations", "text": "Lotka–Volterra equations\n\nThe Lotka–Volterra equations, also known as the predator–prey equations, are a pair of first-order nonlinear differential equations, frequently used to describe the dynamics of biological systems in which two species interact, one as a predator and the other as prey. The populations change through time according to the pair of equations:\nwhere\n\nThe Lotka–Volterra system of equations is an example of a Kolmogorov model, which is a more general framework that can model the dynamics of ecological systems with predator–prey interactions, competition, disease, and mutualism.\n\nThe Lotka–Volterra predator–prey model was initially proposed by Alfred J. Lotka in the theory of autocatalytic chemical reactions in 1910. This was effectively the , originally derived by Pierre François Verhulst. In 1920 Lotka extended the model, via Andrey Kolmogorov, to \"organic systems\" using a plant species and a herbivorous animal species as an example and in 1925 he utilised the equations to analyse predator–prey interactions in his book on biomathematics. The same set of equations was published in 1926 by Vito Volterra, a mathematician and physicist, who had become interested in mathematical biology. Volterra's enquiry was inspired through his interactions with the marine biologist Umberto D'Ancona, who was courting his daughter at the time and later was to become his son-in-law. D'Ancona studied the fish catches in the Adriatic Sea and had noticed that the percentage of predatory fish caught had increased during the years of World War I (1914–18). This puzzled him, as the fishing effort had been very much reduced during the war years. Volterra developed his model independently from Lotka and used it to explain d'Ancona's observation.\n\nThe model was later extended to include density-dependent prey growth and a functional response of the form developed by C. S. Holling; a model that has become known as the Rosenzweig–McArthur model. Both the Lotka–Volterra and Rosenzweig–MacArthur models have been used to explain the dynamics of natural populations of predators and prey, such as the lynx and snowshoe hare data of the Hudson's Bay Company and the moose and wolf populations in Isle Royale National Park.\n\nIn the late 1980s, an alternative to the Lotka–Volterra predator–prey model (and its common-prey-dependent generalizations) emerged, the ratio dependent or Arditi–Ginzburg model. The validity of prey- or ratio-dependent models has been much debated.\n\nThe Lotka–Volterra equations have a long history of use in economic theory; their initial application is commonly credited to Richard Goodwin in 1965 or 1967.\n\nThe Lotka–Volterra model makes a number of assumptions, not necessarily realizable in nature, about the environment and evolution of the predator and prey populations:\n\nAs differential equations are used, the solution is deterministic and continuous. This, in turn, implies that the generations of both the predator and prey are continually overlapping.\nWhen multiplied out, the prey equation becomes\n\nThe prey are assumed to have an unlimited food supply and to reproduce exponentially, unless subject to predation; this exponential growth is represented in the equation above by the term \"αx\". The rate of predation upon the prey is assumed to be proportional to the rate at which the predators and the prey meet, this is represented above by \"βxy\". If either or is zero, then there can be no predation.\n\nWith these two terms the equation above can be interpreted as follows: the rate of change of the prey's population is given by its own growth rate minus the rate at which it is preyed upon.\n\nThe predator equation becomes\n\nIn this equation, \"δxy\" represents the growth of the predator population. (Note the similarity to the predation rate; however, a different constant is used, as the rate at which the predator population grows is not necessarily equal to the rate at which it consumes the prey). \"γy\" represents the loss rate of the predators due to either natural death or emigration, it leads to an exponential decay in the absence of prey.\n\nHence the equation expresses that the rate of change of the predator's population depends upon the rate at which it consumes prey, minus its intrinsic death rate.\n\nThe equations have periodic solutions and do not have a simple expression in terms of the usual trigonometric functions, although they are quite tractable.\n\nIf none of the non-negative parameters vanishes, three can be absorbed into the normalization of variables to leave only one parameter: since the first equation is homogeneous in , and the second one in , the parameters \"β\"/\"α\" and \"δ\"/\"γ\" are absorbable in the normalizations of and respectively, and into the normalization of , so that only remains arbitrary. It is the only parameter affecting the nature of the solutions.\n\nA linearization of the equations yields a solution similar to simple harmonic motion with the population of predators trailing that of prey by 90° in the cycle.\n\nSuppose there are two species of animals, a baboon (prey) and a cheetah (predator). If the initial conditions are 80 baboons and 40 cheetahs, one can plot the progression of the two species over time. The choice of time interval is arbitrary.\nOne may also plot solutions parametrically as orbits in phase space, without representing time, but with one axis representing the number of prey and the other axis representing the number of predators for all times.\n\nThis corresponds to eliminating time from the two differential equations above to produce a single differential equation\nrelating the variables \"x\" and \"y\". The solutions of this equation are closed curves. It is amenable to separation of variables: integrating\nyields the implicit relationship \nwhere \"V\" is a constant quantity depending on the initial conditions and conserved on each curve.\n\nAn aside: These graphs illustrate a serious potential problem with this \"as a biological model\": For this specific choice of parameters, in each cycle, the baboon population is reduced to extremely low numbers, yet recovers (while the cheetah population remains sizeable at the lowest baboon density). In real-life situations, however, chance fluctuations of the discrete numbers of individuals, as well as the family structure and life-cycle of baboons, might cause the baboons to actually go extinct, and, by consequence, the cheetahs as well. This modelling problem has been called the \"atto-fox problem\", an atto-<nowiki>fox</nowiki> being a notional 10 of a fox.\n\nA less extreme example covers: \nIn the model system, the predators thrive when there are plentiful prey but, ultimately, outstrip their food supply and decline. As the predator population is low, the prey population will increase again. These dynamics continue in a cycle of growth and decline.\n\nPopulation equilibrium occurs in the model when neither of the population levels is changing, i.e. when both of the derivatives are equal to 0:\n\nThe above system of equations yields two solutions:\n\nand\n\nHence, there are two equilibria.\n\nThe first solution effectively represents the extinction of both species. If both populations are at 0, then they will continue to be so indefinitely. The second solution represents a fixed point at which both populations sustain their current, non-zero numbers, and, in the simplified model, do so indefinitely. The levels of population at which this equilibrium is achieved depend on the chosen values of the parameters \"α\", \"β\", \"γ\", and \"δ\".\n\nThe stability of the fixed point at the origin can be determined by performing a linearization using partial derivatives.\n\nThe Jacobian matrix of the predator–prey model is\n\nand is known as community matrix.\n\nWhen evaluated at the steady state of (0, 0), the Jacobian matrix \"J\" becomes\n\nThe eigenvalues of this matrix are\n\nIn the model \"α\" and \"γ\" are always greater than zero, and as such the sign of the eigenvalues above will always differ. Hence the fixed point at the origin is a saddle point.\n\nThe stability of this fixed point is of significance. If it were stable, non-zero populations might be attracted towards it, and as such the dynamics of the system might lead towards the extinction of both species for many cases of initial population levels. However, as the fixed point at the origin is a saddle point, and hence unstable, it follows that the extinction of both species is difficult in the model. (In fact, this could only occur if the prey were artificially completely eradicated, causing the predators to die of starvation. If the predators were eradicated, the prey population would grow without bound in this simple model.) The populations of prey and predator can get infinitesimally close to zero and still recover.\n\nEvaluating \"J\" at the second fixed point leads to\n\nThe eigenvalues of this matrix are\n\nAs the eigenvalues are both purely imaginary and conjugate to each others, this fixed point is elliptic, so the solutions are periodic, oscillating on a small ellipse around the fixed point, with a period formula_18.\n\nAs illustrated in the circulating oscillations in the figure above, the level curves are closed orbits surrounding the fixed point: the levels of the predator and prey populations cycle and oscillate without damping around the fixed point with period formula_19.\n\nThe value of the constant of motion \"V\", or, equivalently, \"K\" = exp(\"V\"), formula_20, can be found for the closed orbits near the fixed point.\n\nIncreasing \"K\" moves a closed orbit closer to the fixed point. The largest value of the constant \"K\" is obtained by solving the optimization problem\nThe maximal value of \"K\" is thus attained at the stationary (fixed) point formula_22 and amounts to \nwhere \"e\" is Euler's number.\n\n\n\n"}
{"id": "1024328", "url": "https://en.wikipedia.org/wiki?curid=1024328", "title": "Lwów School of Mathematics", "text": "Lwów School of Mathematics\n\nThe Lwów school of mathematics () was a group of Polish mathematicians who worked between the two World Wars in Lwów, Poland (since 1945 Lviv, Ukraine). The mathematicians often met at the famous Scottish Café to discuss mathematical problems, and published in the journal \"Studia Mathematica\", founded in 1929. The school was renowned for its productivity and its extensive contributions to subjects such as point-set topology, set theory and functional analysis. The biographies and contributions of these mathematicians were documented in 1980 by their contemporary Kazimierz Kuratowski in his book \"A Half Century of Polish Mathematics: Remembrances and Reflections\".\n\nMany of the mathematicians, especially those of Jewish background, fled this southeastern part of Poland in 1941 when it became clear that it would be invaded by Germany. Few of the mathematicians survived World War II, but after the war a group including some of the original community carried on their work in western Poland's Wrocław, the successor city to prewar Lwów; see Polish population transfers (1944–1946). A number of the prewar mathematicians, prominent among them Stanisław Ulam, became famous for work done in the West.\n\nNotable members of the Lwów school of mathematics included:\n\n"}
{"id": "82285", "url": "https://en.wikipedia.org/wiki?curid=82285", "title": "Mathematical proof", "text": "Mathematical proof\n\nIn mathematics, a proof is an inferential argument for a mathematical statement. In the argument, other previously established statements, such as theorems, can be used. In principle, a proof can be traced back to self-evident or assumed statements, known as axioms, along with accepted rules of inference. Axioms may be treated as conditions that must be met before the statement applies. Proofs are examples of exhaustive deductive reasoning or inductive reasoning and are distinguished from empirical arguments or non-exhaustive inductive reasoning (or \"reasonable expectation\"). A proof must demonstrate that a statement is always true (occasionally by listing \"all\" possible cases and showing that it holds in each), rather than enumerate many confirmatory cases. An unproved proposition that is believed to be true is known as a conjecture.\n\nProofs employ logic but usually include some amount of natural language which usually admits some ambiguity. In fact, the vast majority of proofs in written mathematics can be considered as applications of rigorous informal logic. Purely formal proofs, written in symbolic language instead of natural language, are considered in proof theory. The distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics (in both senses of that term). The philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language.\n\nThe word \"proof\" comes from the Latin \"probare\" meaning \"to test\". Related modern words are the English \"probe\", \"probation\", and \"probability\", the Spanish \"probar\" (to smell or taste, or (lesser use) touch or test), Italian \"provare\" (to try), and the German \"probieren\" (to try). The early use of \"probity\" was in the presentation of legal evidence. A person of authority, such as a nobleman, was said to have probity, whereby the evidence was by his relative authority, which outweighed empirical testimony.\n\nPlausibility arguments using heuristic devices such as pictures and analogies preceded strict mathematical proof. It is likely that the idea of demonstrating a conclusion first arose in connection with geometry, which originally meant the same as \"land measurement\". The development of mathematical proof is primarily the product of ancient Greek mathematics, and one of the greatest achievements thereof. Thales (624–546 BCE) and Hippocrates of Chios (c470-410 BCE) proved some theorems in geometry. Eudoxus (408–355 BCE) and Theaetetus (417–369 BCE) formulated theorems but did not prove them. Aristotle (384–322 BCE) said definitions should describe the concept being defined in terms of other concepts already known. Mathematical proofs were revolutionized by Euclid (300 BCE), who introduced the axiomatic method still in use today, starting with undefined terms and axioms (propositions regarding the undefined terms assumed to be self-evidently true from the Greek \"axios\" meaning \"something worthy\"), and used these to prove theorems using deductive logic. His book, the \"Elements\", was read by anyone who was considered educated in the West until the middle of the 20th century. In addition to theorems of geometry, such as the Pythagorean theorem, the \"Elements\" also covers number theory, including a proof that the square root of two is irrational and that there are infinitely many prime numbers.\n\nFurther advances took place in medieval Islamic mathematics. While earlier Greek proofs were largely geometric demonstrations, the development of arithmetic and algebra by Islamic mathematicians allowed more general proofs that no longer depended on geometry. In the 10th century CE, the Iraqi mathematician Al-Hashimi provided general proofs for numbers (rather than geometric demonstrations) as he considered multiplication, division, etc. for \"lines.\" He used this method to provide a proof of the existence of irrational numbers. An inductive proof for arithmetic sequences was introduced in the \"Al-Fakhri\" (1000) by Al-Karaji, who used it to prove the binomial theorem and properties of Pascal's triangle. Alhazen also developed the method of proof by contradiction, as the first attempt at proving the Euclidean parallel postulate.\n\nModern proof theory treats proofs as inductively defined data structures. There is no longer an assumption that axioms are \"true\" in any sense; this allows for parallel mathematical theories built on alternate sets of axioms (see Axiomatic set theory and Non-Euclidean geometry for examples).\n\nAs practiced, a proof is expressed in natural language and is a rigorous argument intended to convince the audience of the truth of a statement. The standard of rigor is not absolute and has varied throughout history. A proof can be presented differently depending on the intended audience. In order to gain acceptance, a proof has to meet communal statements of rigor; an argument considered vague or incomplete may be rejected.\n\nThe concept of a proof is formalized in the field of mathematical logic. A formal proof is written in a formal language instead of a natural language. A formal proof is defined as sequence of formulas in a formal language, in which each formula is a logical consequence of preceding formulas. Having a definition of formal proof makes the concept of proof amenable to study. Indeed, the field of proof theory studies formal proofs and their properties, for example, the property that a statement has a formal proof. An application of proof theory is to show that certain undecidable statements are not provable.\n\nThe definition of a formal proof is intended to capture the concept of proofs as written in the practice of mathematics. The soundness of this definition amounts to the belief that a published proof can, in principle, be converted into a formal proof. However, outside the field of automated proof assistants, this is rarely done in practice. A classic question in philosophy asks whether mathematical proofs are analytic or synthetic. Kant, who introduced the analytic-synthetic distinction, believed mathematical proofs are synthetic. \n\nProofs may be viewed as aesthetic objects, admired for their mathematical beauty. The mathematician Paul Erdős was known for describing proofs he found particularly elegant as coming from \"The Book\", a hypothetical tome containing the most beautiful method(s) of proving each theorem. The book \"Proofs from THE BOOK\", published in 2003, is devoted to presenting 32 proofs its editors find particularly pleasing.\n\nIn direct proof, the conclusion is established by logically combining the axioms, definitions, and earlier theorems. For example, direct proof can be used to establish that the sum of two even integers is always even:\n\nThis proof uses the definition of even integers, the integer properties of closure under addition and multiplication, and distributivity.\n\nDespite its name, mathematical induction is a method of deduction, not a form of inductive reasoning. In proof by mathematical induction, a single \"base case\" is proved, and an \"induction rule\" is proved that establishes that any arbitrary case implies the next case. Since in principle the induction rule can be applied repeatedly starting from the proved base case, we see that all (usually infinitely many) cases are provable. This avoids having to prove each case individually. A variant of mathematical induction is proof by infinite descent, which can be used, for example, to prove the irrationality of the square root of two.\n\nA common application of proof by mathematical induction is to prove that a property known to hold for one number holds for all natural numbers:\nLet } be the set of natural numbers, and be a mathematical statement involving the natural number belonging to such that\n\nFor example, we can prove by induction that all positive integers of the form are odd. Let represent \" is odd\":\n\nThe shorter phrase \"proof by induction\" is often used instead of \"proof by mathematical induction\".\n\nProof by contraposition infers the conclusion \"if \"p\" then \"q\"\" from the premise \"if \"not q\" then \"not p\"\". The statement \"if \"not q\" then \"not p\"\" is called the contrapositive of the statement \"if \"p\" then \"q\"\". For example, contraposition can be used to establish that, given an integer formula_1, if formula_2 is even, then formula_1 is even:\n\nIn proof by contradiction (also known as \"reductio ad absurdum\", Latin for \"by reduction to the absurd\"), it is shown that if some statement were true, a logical contradiction occurs, hence the statement must be false. A famous example of proof by contradiction shows that formula_10 is an irrational number:\n\nProof by construction, or proof by example, is the construction of a concrete example with a property to show that something having that property exists. Joseph Liouville, for instance, proved the existence of transcendental numbers by constructing an explicit example. It can also be used to construct a counterexample to disprove a proposition that all elements have a certain property.\n\nIn proof by exhaustion, the conclusion is established by dividing it into a finite number of cases and proving each one separately. The number of cases sometimes can become very large. For example, the first proof of the four color theorem was a proof by exhaustion with 1,936 cases. This proof was controversial because the majority of the cases were checked by a computer program, not by hand. The shortest known proof of the four color theorem still has over 600 cases.\n\nA probabilistic proof is one in which an example is shown to exist, with certainty, by using methods of probability theory. Probabilistic proof, like proof by construction, is one of many ways to show existence theorems.\n\nThis is not to be confused with an argument that a theorem is 'probably' true, a 'plausibility argument'. The work on the Collatz conjecture shows how far plausibility is from genuine proof.\n\nWhile most mathematicians do not think that probabilistic evidence ever counts as a genuine mathematical proof, a few mathematicians and philosophers have argued that at least some types of probabilistic evidence (such as Rabin's probabilistic algorithm for testing primality) are as good as genuine mathematical proofs. \n\nA combinatorial proof establishes the equivalence of different expressions by showing that they count the same object in different ways. Often a bijection between two sets is used to show that the expressions for their two sizes are equal. Alternatively, a double counting argument provides two different expressions for the size of a single set, again showing that the two expressions are equal.\n\nA nonconstructive proof establishes that a mathematical object with a certain property exists without explaining how such an object can be found. Often, this takes the form of a proof by contradiction in which the nonexistence of the object is proved to be impossible. In contrast, a constructive proof establishes that a particular object exists by providing a method of finding it. A famous example of a\nnonconstructive proof shows that there exist two irrational numbers \"a\" and \"b\" such that formula_15 is a rational number:\n\nThe expression \"statistical proof\" may be used technically or colloquially in areas of pure mathematics, such as involving cryptography, chaotic series, and probabilistic or analytic number theory. It is less commonly used to refer to a mathematical proof in the branch of mathematics known as mathematical statistics. See also \"Statistical proof using data\" section below.\n\nUntil the twentieth century it was assumed that any proof could, in principle, be checked by a competent mathematician to confirm its validity. However, computers are now used both to prove theorems and to carry out calculations that are too long for any human or team of humans to check; the first proof of the four color theorem is an example of a computer-assisted proof. Some mathematicians are concerned that the possibility of an error in a computer program or a run-time error in its calculations calls the validity of such computer-assisted proofs into question. In practice, the chances of an error invalidating a computer-assisted proof can be reduced by incorporating redundancy and self-checks into calculations, and by developing multiple independent approaches and programs. Errors can never be completely ruled out in case of verification of a proof by humans either, especially if the proof contains natural language and requires deep mathematical insight.\n\nA statement that is neither provable nor disprovable from a set of axioms is called undecidable (from those axioms). One example is the parallel postulate, which is neither provable nor refutable from the remaining axioms of Euclidean geometry.\n\nMathematicians have shown there are many statements that are neither provable nor disprovable in Zermelo-Fraenkel set theory with the axiom of choice (ZFC), the standard system of set theory in mathematics (assuming that ZFC is consistent); see list of statements undecidable in ZFC.\n\nGödel's (first) incompleteness theorem shows that many axiom systems of mathematical interest will have undecidable statements.\n\nWhile early mathematicians such as Eudoxus of Cnidus did not use proofs, from Euclid to the foundational mathematics developments of the late 19th and 20th centuries, proofs were an essential part of mathematics. With the increase in computing power in the 1960s, significant work began to be done investigating mathematical objects outside of the proof-theorem framework, in experimental mathematics. Early pioneers of these methods intended the work ultimately to be embedded in a classical proof-theorem framework, e.g. the early development of fractal geometry, which was ultimately so embedded.\n\nAlthough not a formal proof, a visual demonstration of a mathematical theorem is sometimes called a \"proof without words\". The left-hand picture below is an example of a historic visual proof of the Pythagorean theorem in the case of the (3,4,5) triangle.\nSome illusory visual proofs, such as the missing square puzzle, can be constructed in a way which appear to prove a supposed mathematical fact but only do so under the presence of tiny errors (for example, supposedly straight lines which actually bend slightly) which are unnoticeable until the entire picture is closely examined, with lengths and angles precisely measured or calculated.\n\nAn elementary proof is a proof which only uses basic techniques. More specifically, the term is used in number theory to refer to proofs that make no use of complex analysis. For some time it was thought that certain theorems, like the prime number theorem, could only be proved using \"higher\" mathematics. However, over time, many of these results have been reproved using only elementary techniques.\n\nA particular way of organising a proof using two parallel columns is often used in elementary geometry classes in the United States. The proof is written as a series of lines in two columns. In each line, the left-hand column contains a proposition, while the right-hand column contains a brief explanation of how the corresponding proposition in the left-hand column is either an axiom, a hypothesis, or can be logically derived from previous propositions. The left-hand column is typically headed \"Statements\" and the right-hand column is typically headed \"Reasons\".\n\nThe expression \"mathematical proof\" is used by lay people to refer to using mathematical methods or arguing with mathematical objects, such as numbers, to demonstrate something about everyday life, or when data used in an argument is numerical. It is sometimes also used to mean a \"statistical proof\" (below), especially when used to argue from data.\n\n\"Statistical proof\" from data refers to the application of statistics, data analysis, or Bayesian analysis to infer propositions regarding the probability of data. While \"using\" mathematical proof to establish theorems in statistics, it is usually not a mathematical proof in that the \"assumptions\" from which probability statements are derived require empirical evidence from outside mathematics to verify. In physics, in addition to statistical methods, \"statistical proof\" can refer to the specialized \"mathematical methods of physics\" applied to analyze data in a particle physics experiment or observational study in physical cosmology. \"Statistical proof\" may also refer to raw data or a convincing diagram involving data, such as scatter plots, when the data or diagram is adequately convincing without further analysis.\n\nProofs using inductive logic, while considered mathematical in nature, seek to establish propositions with a degree of certainty, which acts in a similar manner to probability, and may be less than full certainty. Inductive logic should not be confused with mathematical induction.\n\nBayesian analysis uses Bayes' theorem to update a person's assessment of likelihoods of hypotheses when new evidence or information is acquired.\n\nPsychologism views mathematical proofs as psychological or mental objects. Mathematician philosophers, such as Leibniz, Frege, and Carnap have variously criticized this view and attempted to develop a semantics for what they considered to be the language of thought, whereby standards of mathematical proof might be applied to empirical science.\n\nPhilosopher-mathematicians such as Spinoza have attempted to formulate philosophical arguments in an axiomatic manner, whereby mathematical proof standards could be applied to argumentation in general philosophy. Other mathematician-philosophers have tried to use standards of mathematical proof and reason, without empiricism, to arrive at statements outside of mathematics, but having the certainty of propositions deduced in a mathematical proof, such as Descartes' \"cogito\" argument.\n\nSometimes, the abbreviation \"Q.E.D.\" is written to indicate the end of a proof. This abbreviation stands for \"Quod Erat Demonstrandum\", which is Latin for \"that which was to be demonstrated\". A more common alternative is to use a square or a rectangle, such as □ or ∎, known as a \"tombstone\" or \"halmos\" after its eponym Paul Halmos. Often, \"which was to be shown\" is verbally stated when writing \"QED\", \"□\", or \"∎\" during an oral presentation.\n\n\n"}
{"id": "326471", "url": "https://en.wikipedia.org/wiki?curid=326471", "title": "Mathematics education", "text": "Mathematics education\n\nIn contemporary education, mathematics education is the practice of teaching and learning mathematics, along with the associated scholarly research.\n\nResearchers in mathematics education are primarily concerned with the tools, methods and approaches that facilitate practice or the study of practice; however, mathematics education research, known on the continent of Europe as the didactics or pedagogy of mathematics, has developed into an extensive field of study, with its own concepts, theories, methods, national and international organisations, conferences and literature. This article describes some of the history, influences and recent controversies.\n\nElementary mathematics was part of the education system in most ancient civilisations, including Ancient Greece, the Roman Empire, Vedic society and ancient Egypt. In most cases, a formal education was only available to male children with a sufficiently high status, wealth or caste.\nIn Plato's division of the liberal arts into the trivium and the quadrivium, the quadrivium included the mathematical fields of arithmetic and geometry. This structure was continued in the structure of classical education that was developed in medieval Europe. Teaching of geometry was almost universally based on Euclid's \"Elements\". Apprentices to trades such as masons, merchants and money-lenders could expect to learn such practical mathematics as was relevant to their profession.\n\nIn the Renaissance, the academic status of mathematics declined, because it was strongly associated with trade and commerce, and considered somewhat un-Christian. Although it continued to be taught in European universities, it was seen as subservient to the study of Natural, Metaphysical and Moral Philosophy. The first modern arithmetic curriculum (starting with addition, then subtraction, multiplication, and division) arose at reckoning schools in Italy in the 1300s. Spreading along trade routes, these methods were designed to be used in commerce. They contrasted with Platonic math taught at universities, which was more philosophical and concerned numbers as concepts rather than calculating methods. They also contrasted with mathematical methods learned by artisan apprentices, which were specific to the tasks and tools at hand. For example, the division of a board into thirds can be accomplished with a piece of string, instead of measuring the length and using the arithmetic operation of division.\n\nThe first mathematics textbooks to be written in English and French were published by Robert Recorde, beginning with \"The Grounde of Artes\" in 1540. However, there are many different writings on mathematics and mathematics methodology that date back to 1800 BCE. These were mostly located in Mesopotamia where the Sumerians were practicing multiplication and division. There are also artifacts demonstrating their own methodology for solving equations like the quadratic equation. After the Sumerians some of the most famous ancient works on mathematics come from Egypt in the form of the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus. The more famous Rhind Papyrus has been dated to approximately 1650 BCE but it is thought to be a copy of an even older scroll. This papyrus was essentially an early textbook for Egyptian students.\n\nThe social status of mathematical study was improving by the seventeenth century, with the University of Aberdeen creating a Mathematics Chair in 1613, followed by the Chair in Geometry being set up in University of Oxford in 1619 and the Lucasian Chair of Mathematics being established by the University of Cambridge in 1662. However, it was uncommon for mathematics to be taught outside of the universities. Isaac Newton, for example, received no formal mathematics teaching until he joined Trinity College, Cambridge in 1661.\n\nIn the 18th and 19th centuries, the Industrial Revolution led to an enormous increase in urban populations. Basic numeracy skills, such as the ability to tell the time, count money and carry out simple arithmetic, became essential in this new urban lifestyle. Within the new public education systems, mathematics became a central part of the curriculum from an early age.\n\nBy the twentieth century, mathematics was part of the core curriculum in all developed countries.\n\nDuring the twentieth century, mathematics education was established as an independent field of research. Here are some of the main events in this development:\n\n\nIn the 20th century, the cultural impact of the \"electronic age\" (McLuhan) was also taken up by educational theory and the teaching of mathematics. While previous approach focused on \"working with specialized 'problems' in arithmetic\", the emerging structural approach to knowledge had \"small children meditating about number theory and 'sets'.\"\n\nAt different times and in different cultures and countries, mathematics education has attempted to achieve a variety of different objectives. These objectives have included:\n\n\nThe method or methods used in any particular context are largely determined by the objectives that the relevant educational system is trying to achieve. Methods of teaching mathematics include the following:\n\n\n\nDifferent levels of mathematics are taught at different ages and in somewhat different sequences in different countries. Sometimes a class may be taught at an earlier age than typical as a special or honors class.\n\nElementary mathematics in most countries is taught in a similar fashion, though there are differences. In the United States fractions are typically taught starting from 1st grade, whereas in other countries they are usually taught later, since the metric system does not require young children to be familiar with them. Most countries tend to cover fewer topics in greater depth than in the United States. K-12 topics include elementary arithmetic (addition, subtraction, multiplication, and division), and pre-algebra.\n\nIn most of the U.S., algebra, geometry and analysis (pre-calculus and calculus) are taught as separate courses in different years of high school. Mathematics in most other countries (and in a few U.S. states) is integrated, with topics from all branches of mathematics studied every year. Students in many countries choose an option or pre-defined course of study rather than choosing courses \"à la carte\" as in the United States. Students in science-oriented curricula typically study differential calculus and trigonometry at age 16–17 and integral calculus, complex numbers, analytic geometry, exponential and logarithmic functions, and infinite series in their final year of secondary school. Probability and statistics may be taught in secondary education classes.\n\nScience and engineering students in colleges and universities may be required to take multivariable calculus, differential equations, linear algebra. Applied mathematics is also used in specific majors; for example, civil engineers may be required to study fluid mechanics, while \"math for computer science\" might include graph theory, permutation, probability, and proofs. Mathematics students would continue to study potentially any area.\n\nThroughout most of history, standards for mathematics education were set locally, by individual schools or teachers, depending on the levels of achievement that were relevant to, realistic for, and considered socially appropriate for their pupils.\n\nIn modern times, there has been a move towards regional or national standards, usually under the umbrella of a wider standard school curriculum. In England, for example, standards for mathematics education are set as part of the National Curriculum for England, while Scotland maintains its own educational system. In the USA, the National Governors Association Center for Best Practices and the Council of Chief State School Officers have published the national mathematics Common Core State Standards Initiative.\n\nMa (2000) summarised the research of others who found, based on nationwide data, that students with higher scores on standardised mathematics tests had taken more mathematics courses in high school. This led some states to require three years of mathematics instead of two. But because this requirement was often met by taking another lower level mathematics course, the additional courses had a “diluted” effect in raising achievement levels.\n\nIn North America, the National Council of Teachers of Mathematics has published the Principles and Standards for School Mathematics, which boosted the trend towards reform mathematics. In 2006, they released \"Curriculum Focal Points\", which recommend the most important mathematical topics for each grade level through grade 8. However, these standards are enforced as American states and Canadian provinces choose. A US state's adoption of the Common Core State Standards in mathematics is at the discretion of the state, and is not mandated by the Federal Government. \"States routinely review their academic standards and may choose to change or add onto the standards to best meet the needs of their students.\" The National Council of Teachers of Mathematics has state affiliates that have different education standards at the state level. For example, Missouri has the Missouri Council of Teachers of Mathematics (MCTM) which has its own pillars and standards of education listed on its website. The MCTM also offers membership opportunities to teachers and future teachers so they can stay up to date on the changes in math educational standards. \n\n\"Robust, useful theories of classroom teaching do not yet exist\". However, there are useful theories on how children learn mathematics and much research has been conducted in recent decades to explore how these theories can be applied to teaching. The following results are examples of some of the current findings in the field of mathematics education:\n\n\n\n\n\n\n\nAs with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods in order to test their effects. They depend on large samples to obtain statistically significant results.\n\nQualitative research, such as case studies, action research, discourse analysis, and clinical interviews, depend on small but focused samples in an attempt to understand student learning and to look at how and why a given method gives the results it does. Such studies cannot conclusively establish that one method is better than another, as randomized trials can, but unless it is understood \"why\" treatment X is better than treatment Y, application of results of quantitative studies will often lead to \"lethal mutations\" of the finding in actual classrooms. Exploratory qualitative research is also useful for suggesting new hypotheses, which can eventually be tested by randomized experiments. Both qualitative and quantitative studies therefore are considered essential in education—just as in the other social sciences. Many studies are “mixed”, simultaneously combining aspects of both quantitative and qualitative research, as appropriate.\n\nThere has been some controversy over the relative strengths of different types of research. Because randomized trials provide clear, objective evidence on “what works”, policy makers often take only those studies into consideration. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes. In other disciplines concerned with human subjects, like biomedicine, psychology, and policy evaluation, controlled, randomized experiments remain the preferred method of evaluating treatments. Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods. On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective, or the difficulty of assuring rigid control of the independent variable in fluid, real school settings.\n\nIn the United States, the National Mathematics Advisory Panel (NMAP) published a report in 2008 based on studies, some of which used randomized assignment of treatments to experimental units, such as classrooms or students. The NMAP report's preference for randomized experiments received criticism from some scholars. In 2010, the What Works Clearinghouse (essentially the research arm for the Department of Education) responded to ongoing controversy by extending its research base to include non-experimental studies, including regression discontinuity designs and single-case studies.\n\nThe following are some of the people who have had a significant influence on the teaching of mathematics at various periods in history:\n\n\nThe following people all taught mathematics at some stage in their lives, although they are better known for other things:\n\n\n\n\n\n\n\n"}
{"id": "8385078", "url": "https://en.wikipedia.org/wiki?curid=8385078", "title": "Melnikov distance", "text": "Melnikov distance\n\nOne of the main tools for determining the existence of (or non-existence of) chaos in a perturbed Hamiltonian system is Melnikov theory. In this theory, the distance between the stable and unstable manifolds of the perturbed system is calculated up to the first-order term. Consider a smooth dynamical system formula_1, with formula_2 and formula_3 periodic with period formula_4. Suppose for formula_5 the system has a hyperbolic fixed point x and a homoclinic orbit formula_6 corresponding to this fixed point. Then for sufficiently small formula_7 there exists a \"T\"-periodic hyperbolic solution. The stable and unstable manifolds of this periodic solution intersect transversally. The distance between these manifolds measured along a direction that is perpendicular to the unperturbed homoclinc orbit formula_6 is called the Melnikov distance. If formula_9\ndenotes this distance, then formula_10. The function formula_11 is called the Melnikov function.\n\nMelnikov’s distance can be used to predict chaotic vibrations . In this method, critical amplitude is found by setting the distance between homoclinic orbits and stable manifolds equal to zero. The Melnikov’s method provides necessary but not sufficient condition for chaos.\n"}
{"id": "35200984", "url": "https://en.wikipedia.org/wiki?curid=35200984", "title": "Minimal K-type", "text": "Minimal K-type\n\nIn mathematics, a minimal K-type is a representation of a maximal compact subgroup \"K\" of a semisimple Lie group \"G\" that is in some sense the smallest representation of \"K\" occurring in a Harish-Chandra module of \"G\". Minimal K-types were introduced by as part of an algebraic description of the Langlands classification.\n"}
{"id": "604268", "url": "https://en.wikipedia.org/wiki?curid=604268", "title": "Multi-index notation", "text": "Multi-index notation\n\nMulti-index notation is a mathematical notation that simplifies formulas used in multivariable calculus, partial differential equations and the theory of distributions, by generalising the concept of an integer index to an ordered tuple of indices.\n\nAn \"n\"-dimensional multi-index is an \"n\"-tuple\n\nof non-negative integers (i.e. an element of the \"n\"-dimensional set of natural numbers, denoted formula_2).\n\nFor multi-indices formula_3 and formula_4 one defines:\n\n\n\n\n\n\n\nwhere formula_11.\n\n\n\nwhere formula_14 (see also 4-gradient).\n\nThe multi-index notation allows the extension of many formulae from elementary calculus to the corresponding multi-variable case. Below are some examples. In all the following, formula_15 (or formula_16), formula_17, and formula_18 (or formula_19).\n\n\nThis formula is used for the definition of distributions and weak derivatives.\n\nIf formula_21 are multi-indices and formula_22, then\n\nThe proof follows from the power rule for the ordinary derivative; if \"α\" and \"β\" are in {0, 1, 2, . . .}, then\n\nSuppose formula_25, formula_26, and formula_22. Then we have that\n\nFor each \"i\" in {1, . . ., \"n\"}, the function formula_29 only depends on formula_30. In the above, each partial differentiation formula_31 therefore reduces to the corresponding ordinary differentiation formula_32. Hence, from equation (1), it follows that formula_33 vanishes if \"α\" > \"β\" for at least one \"i\" in {1, . . ., \"n\"}. If this is not the case, i.e., if \"α\" ≤ \"β\" as multi-indices, then\n\nfor each formula_35 and the theorem follows. formula_36\n\n\n"}
{"id": "1297317", "url": "https://en.wikipedia.org/wiki?curid=1297317", "title": "No free lunch theorem", "text": "No free lunch theorem\n\nIn mathematical folklore, the \"no free lunch\" (NFL) theorem (sometimes pluralized) of David Wolpert and William Macready appears in the 1997 \"No Free Lunch Theorems for Optimization\". Wolpert had previously derived no free lunch theorems for machine learning (statistical inference).\n\nIn 2005, Wolpert and Macready themselves indicated that the first theorem in their paper \"state[s] that any two optimization algorithms are equivalent when their performance is averaged across all possible problems\". The 1997 theorems of Wolpert and Macready are mathematically technical.\n\nThe folkloric \"no free lunch\" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them. Various investigators have extended the work of Wolpert and Macready substantively. See No free lunch in search and optimization for treatment of the research area.\n\nWhile some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.\n\nTo find the highest point on Earth, Alice uses a steepest-ascent local search which restarts when a local peak is fully climbed. Bob uses a steepest-descent local search which restarts when it bottoms out at a local trough. In our Earth, Alice will find the highest point much faster than Bob will. However, in some ensembles, each world such as ours can be paired 1-to-1 with a hypothetical \"Bobworld\" which is identical to ours, except that the elevation of peak of Mount Everest and of the lowest point in the Marianas Trench are swapped, as if a tall pole has been stuck in the Trench. In Bobworld, Bob's strategy of descending outperforms Alice's strategy of climbing; in fact, given further 1-to-1 pairing assumptions that are reasonable in a universe of bounded size, neither Alice's strategy nor Bob's strategy performs better on average, in the absence of some systematic natural bias towards worlds like Earth and away from worlds like Bobworld.\n\nWolpert and Macready give two NFL theorems that are closely related to the folkloric theorem. In their paper, they state:\n\nThe first theorem hypothesizes objective functions that do not change while optimization is in progress, and the second hypothesizes objective functions that may change.\n\nwhere formula_1 denotes the ordered set of size formula_2 of the cost values formula_3 associated to input values formula_4, formula_5 is the function being optimized and formula_6 is the conditional probability of obtaining a given sequence of cost values from algorithm formula_7 run formula_2 times on function formula_9.\n\nThe theorem can be equivalently formulated as follows:\n\nHere, \"blind search\" means that at each step of the algorithm, the element formula_10 is chosen at random with uniform probability distribution from the elements of formula_11 that have not been chosen previously.\n\nIn essence, this says that when all functions \"f\" are equally likely, the probability of observing an arbitrary sequence of \"m\" values in the course of optimization does not depend upon the algorithm. In the analytic framework of Wolpert and Macready, performance is a function of the sequence of observed values (and not e.g. of wall-clock time), so it follows easily that all algorithms have identically distributed performance when objective functions are drawn uniformly at random, and also that all algorithms have identical mean performance. But identical mean performance of all algorithms does not imply Theorem 1, and thus the folkloric theorem is not equivalent to the original theorem.\n\nTheorem 2 establishes a similar, but \"more subtle\", NFL result for time-varying objective functions.\n\nThe NFL theorems were explicitly \"not\" motivated by the question of what can be inferred (in the case of NFL for machine learning) or found (in the case of NFL for search) when the \"environment is uniform random\". Rather uniform randomness was used as a tool, to compare the number of environments for which algorithm A outperforms algorithm B to the number of environments for which B outperforms A. NFL tells us that (appropriately weighted) there are just as many environments in both of those sets.\n\nThis is true for many definitions of what precisely an \"environment\" is. In particular, there are just as many prior distributions (appropriately weighted) in which learning algorithm A beats B (on average) as vice versa. This statement about \"sets of priors\" is what is most important about NFL, not the fact that any two algorithms perform equally for the single, specific prior distribution that assigns equal probability to all environments.\n\nWhile the NFL is important to understand the fundamental limitation for a set of problems, it does not state anything about each particular instance of a problem that can arise in practice. That is, the NFL states what the NFL states in the mathematical statements and it is nothing more than that. For example, it applies to the situations where the algorithm is fixed first and a nature can choose a worst problem instance to each fixed algorithm. Therefore, if we have a \"good\" problem in practice or if we can choose a \"good\" learning algorithm for a given particular problem instance, then the NFL does not mention any limitation about this particular problem instance. See for example. To understand the results of the NFL along with \"seemingly\" contradicting results from other papers, it is important to actually understand the mathematical logic of the NFL instead of intuitive notation of the NFL. All results including the NFL and are indeed consistent.\n\nTo illustrate one of the counter-intuitive implications of NFL, suppose we fix two supervised learning algorithms, C and D. We then sample a target function f to produce a set of input-output pairs, \"d\". How should we choose whether to train C or D on \"d\", in order to make predictions for what output would be associated with a point lying outside of \"d?\"\n\nIt is common in almost of all science and statistics to answer this question - to choose between C and D - by running cross-validation on \"d\" with those two algorithms. In other words, to decide whether to generalize from \"d\" with either C or D\",\" we see which of them has better out-of-sample performance when tested within \"d\".\n\nNote that since C and D are fixed, this use of cross-validation to choose between them is itself an algorithm, i.e., a way of generalizing from an arbitrary dataset. Call this algorithm A. (Arguably, A is a simplified model of the scientific method itself.)\n\nNote as well though that we could also use \"anti\"-cross-validation to make our choice. In other words, we could choose between C and D based on which has \"worse\" out-of-sample performance within \"d\". Again, since C and D are fixed, this use of anti-cross-validation is itself an algorithm. Call that algorithm B.\n\nNFL tells us (loosely speaking) that B must beat A on just as many target functions (and associated datasets \"d\") as A beats B. In this very specific sense, the scientific method will lose to the \"anti\" scientific method just as readily as it wins.\n\nHowever, note that NFL only applies if the target function is chosen from a uniform distribution of all possible functions. If this is not the case, and certain target functions are more likely to be chosen than others, then A may perform better than B overall. The contribution of NFL is that it tells us choosing an appropriate algorithm requires making assumptions about the kinds of target functions the algorithm is being used for. With no assumptions, no \"meta-algorithm\", such as the scientific method, performs better than random choice.\n\nWhile some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research. If Occam's razor is correct, for example if sequences of lower Kolmogorov complexity are more probable than sequences of higher complexity, then (as is observed in real life) some algorithms, such as cross-validation, perform better on average on practical problems (when compared with random choice or with anti-cross-validation).\n\n"}
{"id": "43672852", "url": "https://en.wikipedia.org/wiki?curid=43672852", "title": "Normal form (dynamical systems)", "text": "Normal form (dynamical systems)\n\nIn mathematics, the normal form of a dynamical system is a simplified form that can be useful in determining the system's behavior.\n\nNormal forms are often used for determining local bifurcations in a system. All systems exhibiting a certain type of bifurcation are locally (around the equilibrium) topologically equivalent to the normal form of the bifurcation. For example, the normal form of a saddle-node bifurcation is formula_1 where formula_2 is the bifurcation parameter. The transcritical bifurcation formula_3 near formula_4 can be converted to the normal form formula_5 with the transformation formula_6.\n\n"}
{"id": "4965178", "url": "https://en.wikipedia.org/wiki?curid=4965178", "title": "Quasi-triangular quasi-Hopf algebra", "text": "Quasi-triangular quasi-Hopf algebra\n\nA quasi-triangular quasi-Hopf algebra is a specialized form of a quasi-Hopf algebra defined by the Ukrainian mathematician Vladimir Drinfeld in 1989. It is also a generalized form of a quasi-triangular Hopf algebra.\n\nA quasi-triangular quasi-Hopf algebra is a set formula_1 where formula_2 is a quasi-Hopf algebra and formula_3 known as the R-matrix, is an invertible element such that \n\nso that formula_7 is the switch map and\n\nwhere formula_10 and formula_11.\n\nThe quasi-Hopf algebra becomes \"triangular\" if in addition, formula_12.\n\nThe twisting of formula_13 by formula_14 is the same as for a quasi-Hopf algebra, with the additional definition of the twisted \"R\"-matrix\n\nA quasi-triangular (resp. triangular) quasi-Hopf algebra with formula_15 is a quasi-triangular (resp. triangular) Hopf algebra as the latter two conditions in the definition reduce the conditions of quasi-triangularity of a Hopf algebra.\n\nSimilarly to the twisting properties of the quasi-Hopf algebra, the property of being quasi-triangular or triangular quasi-Hopf algebra is preserved by twisting.\n\n\n"}
{"id": "58854243", "url": "https://en.wikipedia.org/wiki?curid=58854243", "title": "Quota rule", "text": "Quota rule\n\nIn mathematics and political science, the quota rule states that when proportionally apportioning seats, the number of representative seats that should be alloted to a given party should be no more than the upper or lower roundings (called upper and lower quotas) of the proportional representation; i.e if a party deserves 10.56 seats out of 15, the quota rule states that when the seats are alloted, the party may get 10 or 11 seats, but not lower or higher. The use of this rule is important when allocating seats in places such as the U.S. House of Representatives and any place that uses proportional representation.\n\nThe rule can be rigorously defined as follows: Let formula_1 be the population of the party, formula_2 be the total population, and formula_3 be the number of available seats. The quota for that party is then\n\nWhen the quota results in a fractional value, the lower quota is the quota rounded down and the upper quota is the quota rounded up. Hence, the quota rule states that the only allocation for that party should be either the lower or upper quota. If at any time an allocation gives a party a greater or lesser number of seats than the upper or lower quota, that allocation (and by extension, the method used to allocate it) is said to be in violation of the quota rule.\n\nThe Balinski–Young theorem proved in 1980 that if an apportionment method satisfies the quota rule, it must fail to satisfy some other apportionment paradox. For instance, although Hamilton's method satisfies the quota rule, it violates the Alabama paradox and the population paradox. The theorem itself is broken up into several different proofs that cover a wide number of circumstances.\n\nSpecifically, there are two main statements that apply to the quota rule:\n\nDifferent methods for allocating seats may or may not satisfy the quota rule. For example, Hamilton's method proportions seats equally until a fractional value is reached; the surplus seats are then given to the state with the largest fractional parts until there are no more surplus seats. Hamilton's method will always pass the quota rule, as there is no period in which a state can be allotted more than the upper quota or less than the lower quota. \n\nJefferson's method, which was one of the first used by the United States, sometimes allocated more than is allowed, violating the quota rule. This violation led to a growing problem where larger states receive more representatives than smaller states, which was not corrected until Webster's method was implemented in 1842; Webster's method also violates the quota rule, although much more rarely.\n\n"}
{"id": "8965926", "url": "https://en.wikipedia.org/wiki?curid=8965926", "title": "Sand table", "text": "Sand table\n\nA sand table uses constrained sand for modelling or educational purposes. The original version of a sand table may be the abax used by early Greek students. In the modern era, one common use for a sand table is to make terrain models for military planning and wargaming.\n\nAn abax was a table covered with sand commonly used by students, particularly in Greece, to perform studies such as writing, geometry, and calculations.\n\nAn abax was the predecessor to the abacus. Objects, such as stones, were added for counting and then columns for place-valued arithmetic. The demarcation between an abax and an abacus seems to be poorly defined in history; moreover, modern definitions of the word \"abacus\" universally describe it as a frame with rods and beads and, in general, do not include the definition of \"sand table\".\n\nThe sand table may well have been the predecessor to some board games. (\"The word abax, or abacus, is used both for the reckoning-board with its counters and the play-board with its pieces, ...\"). \"Abax\" is from the old Greek for \"sand table\".\n\nAn Arabic word for sand (or dust) is \"ghubar\" (or \"gubar\"), and Western numerals (the decimal digits 0–9) are derived from the style of digits written on \"ghubar\" tables in North-West Africa and Iberia, also described as the 'West Arabic' or 'gubar' style.\n\nSand tables have been used for military planning and wargaming for many years as a field expedient, small-scale map, and in training for military actions. In 1890 a Sand table room was built at the Royal Military College of Canada for use in teaching cadets military tactics; this replaced the old sand table room in a pre-college building, in which the weight of the sand had damaged the floor. The use of sand tables increasingly fell out of favour with improved maps, aerial and satellite photography, and later, with digital terrain simulations. More modern sand tables have incorporated Augmented Reality, such as the Augmented Reality Sandtable (ARES) developed by the Army Research Laboratory. Today, virtual and conventional sand tables are used in operations training.\n\nIn 1991, \"Special Forces teams discovered an elaborate sand-table model of the Iraqi military plan for the defense of Kuwait City. Four huge red arrows from the sea pointed at the coastline of Kuwait City and the huge defensive effort positioned there. Small fences of concertina wire marked the shoreline and models of artillery pieces lined the shore area. Throughout the city were plastic models of other artillery and air defense positions, while thin, red-painted strips of board designated supply routes and main highways.\"\n\nIn 2006, Google Earth users looking at satellite photography of China found a several \"kilometre\" large \"sand table\" scale model, strikingly reminiscent of a mountainous region (Aksai Chin) which China occupies militarily in a disputed zone with India, 2400 km from the model's location. Speculation has been rife that the terrain is used for military exercises of familiarisation.\nA sand table is a device useful for teaching in the early grades and for special needs children.\n\n\n\n\n"}
{"id": "362651", "url": "https://en.wikipedia.org/wiki?curid=362651", "title": "Scottish Café", "text": "Scottish Café\n\nThe Scottish Café () was the café in Lwów, Poland (now Lviv, Ukraine) where, in the 1930s and 1940s, mathematicians from the Lwów School collaboratively discussed research problems, particularly in functional analysis and topology.\n\nStanislaw Ulam recounts that the tables of the café had marble tops, so they could write in pencil, directly on the table, during their discussions. To keep the results from being lost, and after becoming annoyed with their writing directly on the table tops, Stefan Banach's wife provided the mathematicians with a large notebook, which was used for writing the problems and answers and eventually became known as the \"Scottish Book\". The book—a collection of solved, unsolved, and even probably unsolvable problems—could be borrowed by any of the guests of the café. Solving any of the problems was rewarded with prizes, with the most difficult and challenging problems having expensive prizes (during the Great Depression and on the eve of World War II), such as a bottle of fine brandy.\n\nFor problem 153, which was later recognized as being closely related to Stefan Banach's \"basis problem\", Stanisław Mazur offered the prize of a live goose. This problem was solved only in 1972 by Per Enflo, who was presented with the live goose in a ceremony that was broadcast throughout Poland.\n\nThe café building now houses the Szkotcka Restaurant & Bar (named for the original Scottish Café) and the Atlas Deluxe hotel at the street address of 27 Taras Shevchenko Prospekt.\n\nThe following mathematicians were associated with the Lwów School of Mathematics or contributed to \"The Scottish Book\":\n\n\n"}
{"id": "45456706", "url": "https://en.wikipedia.org/wiki?curid=45456706", "title": "Smooth maximum", "text": "Smooth maximum\n\nIn mathematics, a smooth maximum of an indexed family \"x\", ..., \"x\" of numbers is a smooth approximation to the maximum function formula_1 meaning a parametric family of functions formula_2 such that for every , the function is smooth, and the family converges to the maximum function as . The concept of smooth minimum is similarly defined. In many cases, a single family approximates both: maximum as the parameter goes to positive infinity, minimum as the parameter goes to negative infinity; in symbols, as and as . The term can also be used loosely for a specific smooth function that behaves similarly to a maximum, without necessarily being part of a parametrized family.\n\nFor large positive values of the parameter formula_3, the following formulation is one smooth, differentiable approximation of the maximum function. For negative values of the parameter that are large in absolute value, it approximates the minimum.\n\nformula_5 has the following properties:\n\nThe gradient of formula_11 is closely related to softmax and is given by\n\nThis makes the softmax function useful for optimization techniques that use gradient descent.\n\nAnother smooth maximum is LogSumExp:\n\nThis can also be normalized if the formula_14 are all non-negative, yielding a function with domain formula_15 and range formula_16:\n\nThe formula_18 term corrects for the fact that formula_19 by canceling out all but one zero exponential, and formula_20 if all formula_14 are zero.\n\n\nM. Lange, D. Zühlke, O. Holz, and T. Villmann, \"Applications of lp-norms and their smooth approximations for gradient based learning vector quantization,\" \"in Proc. ESANN\", Apr. 2014, pp. 271-276."}
{"id": "246160", "url": "https://en.wikipedia.org/wiki?curid=246160", "title": "Summation", "text": "Summation\n\nIn mathematics, summation (denoted with an enlarged capital Greek sigma symbol formula_1) is the addition of a sequence of numbers; the result is their \"sum\" or \"total\". If numbers are added sequentially from left to right, any intermediate result is a partial sum, prefix sum, or running total of the summation.\n\nThe numbers to be summed (called \"addends\", or sometimes \"summands\") may be integers, rational numbers, real numbers, or complex numbers. Besides numbers, other types of values can be added as well: vectors, matrices, polynomials and, in general, elements of any additive group (or even monoid).\n\nFor finite sequences of such elements, summation always produces a well-defined sum. The summation of an infinite sequence of values is called a series. A value of such a series may often be defined by means of a limit (although sometimes the value may be infinite, and often no value results at all). Another notion involving limits of finite sums is integration.\n\nThe summation of the sequence <nowiki>[</nowiki>1, 2, 4, 2<nowiki>]</nowiki> is an expression whose value is the sum of each of the members of the sequence. In the example, = 9. Because addition is associative, the sum does not depend on how the additions are grouped, for instance and both have the value 9; therefore, parentheses are usually omitted in repeated additions. Addition is also commutative, so permuting the terms of a finite sequence does not change its sum. For infinite summations this property may fail. See Absolute convergence for conditions under which it still holds.\n\nThere is no special notation for the summation of such explicit sequences, as the corresponding repeated addition expression will do. There is only a slight difficulty if the sequence has fewer than two elements: the summation of a sequence of one term involves no plus sign (it is indistinguishable from the term itself) and the summation of the empty sequence cannot even be written down (but one can write its value \"0\" in its place). If, however, the terms of the sequence are given by a regular pattern, possibly of variable length, then a summation operator may be useful or even essential.\n\nFor the summation of the sequence of consecutive integers from 1 to 100, one could use an addition expression involving an ellipsis to indicate the missing terms: . In this case, the reader can easily guess the pattern. However, for more complicated patterns, one needs to be precise about the rule used to find successive terms, which can be achieved by using the summation operator \"Σ\". Using this sigma notation the above summation is written as:\n\nThe value of this summation is 5050. It can be found without performing 99 additions, since it can be shown (for instance by mathematical induction) that\n\nfor all natural numbers \"n\". More generally, formulae exist for many summations of terms following a regular pattern.\n\nThe term \"indefinite summation\" refers to the search for an inverse image of a given infinite sequence \"s\" of values for the forward difference operator, in other words for a sequence, called antidifference of \"s\", whose finite differences are given by \"s\". By contrast, summation as discussed in this article is called \"definite summation\".\n\nWhen it is necessary to clarify that numbers are added with their signs, the term algebraic sum is used. For example, in electric circuit theory Kirchhoff's circuit laws consider the algebraic sum of currents in a network of conductors meeting at a point, assigning opposite signs to currents flowing in and out of the node.\n\nMathematical notation uses a symbol that compactly represents summation of many similar terms: the \"summation symbol\", formula_1, an enlarged form of the upright capital Greek letter Sigma. This is defined as:\n\nwhere \"i\" represents the index of summation; \"a\" is an indexed variable representing each successive term in the series; \"m\" is the lower bound of summation, and \"n\" is the upper bound of summation. The \"i = m\" under the summation symbol means that the index \"i\" starts out equal to \"m\". The index, \"i\", is incremented by 1 for each successive term, stopping when \"i\" = \"n\".\n\nHere is an example showing the summation of squares:\n\nInformal writing sometimes omits the definition of the index and bounds of summation when these are clear from context, as in:\n\nOne often sees generalizations of this notation in which an arbitrary logical condition is supplied, and the sum is intended to be taken over all values satisfying the condition. Here are some common examples:\nis the sum of formula_9 over all (integers) formula_10 in the specified range,\nis the sum of formula_12 over all elements formula_13 in the set formula_14, and\nis the sum of formula_16 over all positive integers formula_17 dividing formula_18.\n\nThere are also ways to generalize the use of many sigma signs. For example,\nis the same as\n\nA similar notation is applied when it comes to denoting the product of a sequence, which is similar to its summation, but which uses the multiplication operation instead of addition (and gives 1 for an empty sequence instead of 0). The same basic structure is used, with formula_21, an enlarged form of the Greek capital letter Pi, replacing the formula_1.\n\nIt is possible to sum fewer than 2 numbers:\n\nThese degenerate cases are usually only used when the summation notation gives a degenerate result in a special case.\nFor example, if formula_25 in the definition above, then there is only one term in the sum; if formula_26, then there is none.\n\nSummation may be defined recursively as follows \n\nIn the notation of measure and integration theory, a sum can be expressed as a definite integral,\n\nwhere formula_31 is the subset of the integers from formula_32 to formula_33, and where formula_34 is the counting measure.\n\nGiven a function that is defined over the integers in the interval , one has\n\nThis is the analogue in calculus of finite differences of the fundamental theorem of calculus, which states\nwhere \nis the derivative of .\n\nAn example of application of the above equation is \nUsing binomial theorem, this may be rewritten \n\nThe above formula is more commonly used for inverting of the difference operator formula_40 defined by\nwhere is a function defined on the nonnegative integers.\nThus, given such a function , the problem is to compute the antidifference of , that is, a function formula_42 such that formula_43, that is,formula_44\nThis function is defined up to the addition of a constant, and may be chosen as\n\nThere is not always a closed-form expression for such a summation, but Faulhaber's formula provides a closed form in the case of formula_46 and, by linearity for every polynomial function of .\n\nMany such approximations can be obtained by the following connection between sums and integrals, which holds for any:\n\nincreasing function \"f\":\n\ndecreasing function \"f\":\n\nFor more general approximations, see the Euler–Maclaurin formula.\n\nFor summations in which the summand is given (or can be interpolated) by an integrable function of the index, the summation can be interpreted as a Riemann sum occurring in the definition of the corresponding definite integral. One can therefore expect that for instance\n\nsince the right hand side is by definition the limit for formula_50 of the left hand side. However, for a given summation \"n\" is fixed, and little can be said about the error in the above approximation without additional assumptions about \"f\": it is clear that for wildly oscillating functions the Riemann sum can be arbitrarily far from the Riemann integral.\n\nThe formulae below involve finite sums; for infinite summations or finite summations of expressions involving trigonometric functions or other transcendental functions, see list of mathematical series.\n\nMore generally,\nwhere formula_72 denotes a Bernoulli number (that is Faulhaber's formula).\n\nIn the following summations, is supposed to be different of 1.\n\nThere exist very many summation identities involving binomial coefficients (a whole chapter of \"Concrete Mathematics\" is devoted to just the basic techniques). Some of the most basic ones are the following.\n\nIn the following summations, formula_83 is the number of -permutations of.\n\nThe following are useful approximations (using theta notation):\n\n\n"}
{"id": "394508", "url": "https://en.wikipedia.org/wiki?curid=394508", "title": "Séminaire de Géométrie Algébrique du Bois Marie", "text": "Séminaire de Géométrie Algébrique du Bois Marie\n\nIn mathematics, the Séminaire de Géométrie Algébrique du Bois Marie (SGA) was an influential seminar run by Alexander Grothendieck. It was a unique phenomenon of research and publication outside of the main mathematical journals that ran from 1960 to 1969 at the IHÉS near Paris. (The name came from the small wood on the estate in Bures-sur-Yvette where the IHÉS was located from 1962.) The seminar notes were eventually published in twelve volumes, all except one in the Springer Lecture Notes in Mathematics series.\n\nThe material has a reputation of being hard to read for a number of reasons. More elementary or foundational parts were relegated to the EGA series of Grothendieck and Jean Dieudonné, causing long strings of logical dependencies in the statements. The style is very abstract and makes heavy use of category theory. Moreover, an attempt was made to achieve maximally general statements, while assuming that the reader is aware of the motivations and concrete examples.\n\nThe original notes to SGA were published in fascicles by the IHÉS, most of which went through two or three revisions. These were published as the seminar proceeded, beginning in the early 60's and continuing through most of the decade. They can still be found in large math libraries, but distribution was limited. In the late 60's and early 70's, the original seminar notes were comprehensively revised and rewritten to take into account later developments. In addition, a new volume, SGA 4½, was compiled by Pierre Deligne and published in 1977; it contains simplified and new results by Deligne within the scope of SGA4 as well as some material from SGA5, which had not yet appeared at that time. The revised notes, except for SGA2, were published by Springer in its \"Lecture Notes in Mathematics\" series.\n\nAfter a dispute with Springer, Grothendieck refused the permission for reprints of the series. While these later revisions were more widely distributed than the original fascicles, they are still uncommon outside of libraries.\n\nReferences to SGA typically mean the later, revised editions and not the original fascicles; some of the originals were labelled by capital letters, thus for example S.G.A.D. = SGA3 and S.G.A.A. = SGA4.\n\nThe volumes of the SGA series are the following:\n\n\nIn the 1990s it became obvious that the lack of availability of the SGA was becoming more and more of a problem to researchers and graduate students in algebraic geometry: not only are the copies in book form too few for the growing number of researchers, but they are also difficult to read because of the way they are typeset (on an electric typewriter, with mathematical formulae written by hand). Thus, under the impetus of various mathematicians from several countries, a project was formed of re-publishing SGA in a more widely available electronic format and using LaTeX for typesetting; also, various notes are to be added to correct for minor mistakes or obscurities. The result should be published by the Société Mathématique de France. Legal permission to reprint the works was obtained from every author except Alexander Grothendieck himself, who could not be contacted; it was decided to proceed without his explicit agreement on the grounds that his refusal for the SGA to be re-published by Springer-Verlag was an objection against Springer and not one of principle.\n\nAs a first step, the entire work was scanned and made available on-line (see the links section below) by Frank Calegari, Jim Borger and William Stein. The job of typesetting the text anew and proofreading it was then distributed among dozens of volunteers (most of them junior French mathematicians, because of the required fluency in French and knowledge of algebraic geometry), starting with SGA1 in late 2001.\n\nThe coordinating editor for the work on SGA1 was Bas Edixhoven from University of Leiden (at the time University of Rennes): the first version was available on the arXiv.org e-print archive on June 20, 2002, and the proof-read version was uploaded on January 4, 2004, and later published in book form by the Société Mathématique de France. Work on SGA2 was started in 2004 with Yves Laszlo as coordinating editor. The LaTeX source file is available on the arXiv.org e-print archive; SGA2 appeared in print in late 2005 by the Société Mathématique de France (see https://web.archive.org/web/20091130171320/http://smf.emath.fr/Publications/DocumentsMathematiques/).\n\nLaszlo has also edited SGA4 and recently Philippe Gille and Patrick Polo have uploaded TeXed version of SGA3. In January 2010, however, Grothendieck requested that work cease on republishing SGA. In late 2014 work on republishing SGA resumed and it was restored to the Grothendieck circle site.\n\n\n\n\n\n\n\n"}
{"id": "1387689", "url": "https://en.wikipedia.org/wiki?curid=1387689", "title": "Table of Lie groups", "text": "Table of Lie groups\n\nThis article gives a table of some common Lie groups and their associated Lie algebras.\n\nThe following are noted: the topological properties of the group (dimension; connectedness; compactness; the nature of the fundamental group; and whether or not they are simply connected) as well as on their algebraic properties (abelian; simple; semisimple).\n\nFor more examples of Lie groups and other related topics see the list of simple Lie groups; the Bianchi classification of groups of up to three dimensions; and the list of Lie group topics.\n\nColumn legend\n\nTable legend:\n\nThe dimensions given are dimensions over C. Note that every complex Lie group/algebra can also be viewed as a real Lie group/algebra of twice the dimension.\n\nThe dimensions given are dimensions over C. Note that every complex Lie algebra can also be viewed as a real Lie algebra of twice the dimension.\n"}
{"id": "819251", "url": "https://en.wikipedia.org/wiki?curid=819251", "title": "Tombstone (typography)", "text": "Tombstone (typography)\n\nThe tombstone, Halmos, end of proof, or Q.E.D. mark \"∎\" is used in mathematics to denote the end of a proof, in place of the traditional abbreviation \"Q.E.D.\" for the Latin phrase \"quod erat demonstrandum\", \"which was to be shown\". In magazines, it is one of the various symbols used to indicate the end of an article.\n\nIn Unicode, it is represented as character . Its graphic form varies. It may be a hollow or filled rectangle or square.\n\nIn AMS-LaTeX, the symbol is automatically appended at the end of a proof environment \\begin{proof} ... \\end{proof}. It can also be obtained from the commands \\qedsymbol or \\qed (the latter causes the symbol to be right aligned).\n\nIt is sometimes called a halmos after the mathematician Paul Halmos, who first used it in mathematical context. He got the idea of using it from seeing it was being used to indicate the end of articles in magazines. In his memoir \"I Want to Be a Mathematician\", he wrote the following:\n"}
{"id": "26837834", "url": "https://en.wikipedia.org/wiki?curid=26837834", "title": "University of Chicago School Mathematics Project", "text": "University of Chicago School Mathematics Project\n\nThe University of Chicago School Mathematics Project (UCSMP) is a multi-faceted project of the University of Chicago in the United States, intended to improve competency in mathematics in the United States by elevating educational standards for children in elementary and secondary schools.\n\nThe UCSMP supports educators by supplying training materials to them and offering a comprehensive mathematics curriculum at all levels of primary and secondary education. It seeks to bring international strengths into the United States, translating non-English math textbooks for English students and sponsoring international conferences on the subject of math education. Launched in 1983 with the aid of a six-year grant from Amoco, the UCSMP is used throughout the United States.\n\nUCSMP developed \"Everyday Mathematics\", a pre-K and elementary school mathematics curriculum.\n\n\n\n"}
{"id": "26639238", "url": "https://en.wikipedia.org/wiki?curid=26639238", "title": "Venvaroha", "text": "Venvaroha\n\nVeṇvāroha is a work in Sanskrit composed by Mādhava (c.1350 – c.1425) of Sangamagrāma the founder of the Kerala school of astronomy and mathematics. It is a work in 74 verses describing methods for the computation of the true positions of the Moon at intervals of about half an hour for various days in an anomalistic cycle. This work is an elaboration of an earlier and shorter work of Mādhava himself titled \"Sphutacandrāpti\". \"Veṇvāroha\" is the most popular astronomical work of Mādhava. It is dated 1403 CE. Acyuta Piṣārati (1550–1621), another prominent mathematician/astronomer of the Kerala school, has composed a Malayalam commentary on \"Veṇvāroha\". This astronomical treatise is of a type generally described as Karaṇa texts in India. Such works are characterized by the fact that they are compilations of computational methods of practical astronomy. The title \"Veṇvāroha\" literally means Bamboo Climbing and it is indicative of the computational procedure expounded in the text. The computational scheme is like climbing a bamboo tree, going up and up step by step at measured equal heights.\n\nThe novelty and ingenuity of the method attracted the attention of several of the followers of Mādhava and they composed similar texts thereby creating a genre of works in Indian mathematical tradition collectively referred to as ‘veṇvāroha texts’. These include \"Drik-veṇvārohakriya\" of unknown authorship of epoch 1695 and \"Veṇvārohastaka\" of Putuman Somāyaji.\n\nIn the technical terminology of astronomy, the ingenuity introduced by Mādhava in \"Veṇvāroha\" can be explained thus: Mādhava has endeavored to compute the true longitude of the Moon by making use of the true motions rather than the epicyclic astronomy of the Aryabhata tradition. He made use of the anomalistic revolutions for computing the true positions of the Moon using the successive true daily velocity specified in \"Candravākyas\" (Table of Moon-mnemonics) for easy memorization and use.\n\nVeṇvāroha has been studied from a modern perspective and the process is explained using the properties of periodic functions.\n\n\n"}
