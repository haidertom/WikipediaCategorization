{"id": "6921017", "url": "https://en.wikipedia.org/wiki?curid=6921017", "title": "Algebraic Geometry (book)", "text": "Algebraic Geometry (book)\n\nAlgebraic Geometry is an influential, algebraic geometry textbook written by Robin Hartshorne and published by Springer-Verlag in 1977.\n\nIt was the first extended treatment of scheme theory written as a text intended to be accessible to graduate students.\n\nThe first chapter, titled \"Varieties\", deals with the classical algebraic geometry of varieties over algebraically closed fields. This chapter uses many classical results in commutative algebra, including Hilbert's Nullstellensatz, with the books by Atiyah–Macdonald, Matsumura, and Zariski–Samuel as usual references. The second and the third chapters, \"Schemes\" and \"Cohomology\", form the technical heart of the book. The last two chapters, \"Curves\" and \"Surfaces\", respectively explore the geometry of 1-dimensional and 2-dimensional objects, using the tools developed in the Chapters 2 and 3.\n\n"}
{"id": "1309", "url": "https://en.wikipedia.org/wiki?curid=1309", "title": "Almost all", "text": "Almost all\n\nIn mathematics, the term \"almost all\" means \"all but a negligible amount\". More precisely, if X is a set, \"almost all elements of X\" means \"all elements of X but those in a negligible subset of X\". The meaning of \"negligible\" depends on the mathematical context; for instance, it can mean finite, countable, or null.\n\nIn contrast, \"almost no\" means \"a negligible amount\"; that is, \"almost no elements of X\" means \"the elements of some negligible subset of X\".\n\nThroughout mathematics, \"almost all\" is sometimes used to mean \"all (elements of an infinite set) but finitely many\". This use occurs in philosophy as well. Similarly, \"almost all\" can mean \"all (elements of an uncountable set) but countably many\".\n\nExamples:\n\nWhen speaking about the reals, sometimes \"almost all\" means \"all reals but a null set\". Similarly, if S is some set of reals, \"almost all numbers in S\" can mean \"all numbers in S but those in a null set\". The real line can be thought of as a one-dimensional Euclidean space. In the more general case of an n-dimensional space (where n is a positive integer), these definitions can be generalised to \"all points but those in a null set\" or \"all points in S but those in a null set\" (this time, S is a set of points in the space). Even more generally, \"almost all\" is sometimes used in the sense of \"almost everywhere\" in measure theory, or in the closely related sense of \"almost surely\" in probability theory.\n\nExamples:\n\nIn number theory, \"almost all positive integers\" can mean \"the positive integers in a set whose natural density is 1\". That is, if A is a set of positive integers, and if the proportion of positive integers below n that are in A (out of all positive integers below n) tends to 1 as n tends to infinity, then almost all positive integers are in A. More generally, let S be an infinite set of positive integers, such as the set of even positive numbers or of primes. If A is a subset of S, and if the proportion of elements of S below n that are in A (out of all elements of S below n) tends to 1 as n tends to infinity, then it can be said that almost all elements of S are in A.\n\nExamples:\n\nIn graph theory, if A is a set of (finite labelled) graphs, it can be said to contain almost all graphs if the proportion of graphs with n vertices that are in A tends to 1 as n tends to infinity. However, it is sometimes easier to work with probabilities, so the definition is reformulated as follows. The proportion of graphs with n vertices that are in A equals the probability that a random graph with n vertices (chosen with the uniform distribution) is in A, and choosing a graph in this way has the same outcome as generating a graph by flipping a coin for each pair of vertices to decide whether to connect them. Therefore, equivalently to the preceding definition, A contains almost all graphs if the probability that a coin flip-generated graph with n vertices is in A tends to 1 as n tends to infinity. Sometimes the latter definition is modified so that the graph is chosen randomly in some other way, where not all graphs with n vertices have the same probability, and those modified definitions are not always equivalent to the main one.\n\nThe use of the term \"almost all\" in graph theory is not standard; the term \"asymptotically almost surely\" is more commonly used for this concept.\n\nExample:\n\nIn topology and especially dynamical systems theory (including applications in economics), \"almost all\" of a topological space's points can mean \"all of the space's points but those in a meagre set\". Some use a more limited definition, where a subset only contains almost all of the space's points if it contains some open dense set.\n\nExample:\n\nIn abstract algebra and mathematical logic, if U is an on a set X, \"almost all elements of X\" sometimes means \"the elements of some \"element\" of U\". For any partition of X into two disjoint sets, one of them necessarily contains almost all elements of X. It is possible to think of the elements of a filter on X as containing almost all elements of X even if it isn't an ultrafilter.\n"}
{"id": "294085", "url": "https://en.wikipedia.org/wiki?curid=294085", "title": "Arithmetization of analysis", "text": "Arithmetization of analysis\n\nThe arithmetization of analysis was a research program in the foundations of mathematics carried out in the second half of the 19th century. \n\nKronecker originally introduced the term \"arithmetization of analysis\", by which he meant its constructivization in the context of the natural numbers (see quotation at bottom of page). The meaning of the term later shifted to signify the set-theoretic construction of the real line. Its main proponent was Weierstrass, who argued the geometric foundations of calculus were not solid enough for rigorous work. \n\nThe highlights of this research program are:\n\nAn important spinoff of the arithmetization of analysis is set theory. Naive set theory was created by Cantor and others after arithmetization was completed as a way to study the singularities of functions appearing in calculus. \n\nThe arithmetization of analysis had several important consequences:\n\n\n"}
{"id": "58053955", "url": "https://en.wikipedia.org/wiki?curid=58053955", "title": "Augmented Reality Sandtable", "text": "Augmented Reality Sandtable\n\nThe Augmented Reality Sandtable (ARES) is an interactive, digital sand table that uses augmented reality (AR) technology to create a 3D battlespace map. It was developed by the Human Research and Engineering Directorate (HRED) at the Army Research Laboratory (ARL) to combine the positive aspects of traditional military sand tables with the latest digital technologies to better support soldier training and offer new possibilities of learning. It uses a projector to display a topographical map on top of the sand in a regular sandbox as well as a motion sensor that keeps track of changes in the layout of the sand to appropriately adjust the computer-generated terrain display.\n\nAn ARL study conducted in 2017 with 52 active duty military personnel (36 males and 16 females) found that the participants who used ARES spent less time setting up the table compared to participants who used a traditional sand table. In addition, ARES demonstrated a lower perceived workload score, as measured using the NASA Task Load Index (NASA-TLX) ratings, compared to the traditional sand table. However, there was no significant difference in post-knowledge test scores in recreating the visual map.\n\nThe ARES project was one of the 25 ARL initiatives in development from 1995 to 2015 that focused on visualizing spatial data on virtual or sand table interfaces. It was developed by HRED’s Simulation and Training Technology Center (STTC) with Charles Aumburn as the principal investigator. Collaborations involved with ARES included Dignitas Technologies, Design Interactive (DI), the University of Central Florida’s Institute for Simulation and Training, and the U.S. Military Academy at West Point.\n\nARES was largely designed to be a tangible user interface (TUI), in which digital information can be manipulated using physical objects such as a person’s hand. It was constructed using commercial off-the-shelf components, including a projector, a laptop, an LCD monitor, Microsoft’s Xbox Kinect sensor, and government-developed ARES software. With the projector and Kinect sensor both facing down on the surface of the sandbox, the projector provides a digital overlay over the sand and the Kinect sensor scans the surface of the map to detect any user gestures inside the boundaries of the sandbox.\n\nDuring development, researchers explored the possibility of incorporating ideas such as multi-touch surfaces, 3D holographic displays, and virtual environments. However, budget restrictions limited the implementation of such ideas.\n\nOn September 2014 during the Modern Day Marine exhibition in Quantico, Virginia, researchers from ARL showcased ARES for the first time.\n\nAccording to a 2015 technical report by ARL scientists, ARES is reported to have the following capabilities.\n\n"}
{"id": "10283346", "url": "https://en.wikipedia.org/wiki?curid=10283346", "title": "Backtesting", "text": "Backtesting\n\nBacktesting is a term used in modeling to refer to testing a predictive model on historical data. Backtesting is a type of retrodiction, and a special type of cross-validation applied to previous time period(s).\n\nIn a trading strategy, investment strategy, or risk modeling, backtesting seeks to estimate the performance of a strategy or model if it had been employed during a past period. This requires simulating past conditions with sufficient detail, making one limitation of backtesting the need for detailed historical data. A second limitation is the inability to model strategies that would affect historic prices. Finally, backtesting, like other modeling, is limited by potential overfitting. That is, it is often possible to find a strategy that would have worked well in the past, but will not work well in the future. Despite these limitations, backtesting provides information not available when models and strategies are tested on synthetic data.\n\nBacktesting has historically only been performed by large institutions and professional money managers due to the expense of obtaining and using detailed datasets. However, backtrading is increasingly used on a wider basis, and independent web-based backtesting platforms have emerged. Although the technique is widely used, it is prone to weaknesses. Basel financial regulations require large financial institutions to backtest certain risk models.\n\nIn oceanography and meteorology, \"backtesting\" is also known as \"hindcasting\": a hindcast is a way of testing a mathematical model; researchers enter known or closely estimated inputs for past events into the model to see how well the output matches the known results.\n\nHindcasting usually refers to a numerical-model integration of a historical period where no observations have been assimilated. This distinguishes a hindcast run from a reanalysis. Oceanographic observations of salinity and temperature as well as observations of surface-wave parameters such as the significant wave height are much scarcer than meteorological observations, making hindcasting more common in oceanography than in meteorology. Also, since surface waves represent a forced system where the wind is the only generating force, wave hindcasting is often considered adequate for generating a reasonable representation of the wave climate with little need for a full reanalysis. Hydrologists use hindcasting for model stream flows.\n\nAn example of hindcasting would be entering climate forcings (events that force change) into a climate model. If the hindcast showed reasonably-accurate climate response, the model would be considered successful.\n\nThe ECMWF re-analysis is an example of a combined atmospheric reanalysis coupled with a wave-model integration where no wave parameters were assimilated, making the wave part a hindcast run.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n"}
{"id": "10993126", "url": "https://en.wikipedia.org/wiki?curid=10993126", "title": "Baldwin–Lomax model", "text": "Baldwin–Lomax model\n\nThe Baldwin–Lomax model is a 0-equation turbulence model used in computational fluid dynamics analysis of turbulent boundary layer flows.\n\n"}
{"id": "53394437", "url": "https://en.wikipedia.org/wiki?curid=53394437", "title": "Bayesian Program Synthesis", "text": "Bayesian Program Synthesis\n\nIn machine learning, Bayesian Program Synthesis (BPS), Bayesian Programs write (synthesize) new Bayesian programs. This is in contrast to the field of probabilistic programs where humans write new probabilistic (Bayesian) programs.\n\nBayesian probabilities is a strategy to learn distributions over Bayesian programs.\n\nBayesian Program Synthesis can be compared to the work on Bayesian Program Learning by Lake, Salakhutdinov, and Tenenbaum's, where probabilistic program components were hand-written, pre-trained on data, and then hand assembled in order to recognize handwritten characters.\n\nBayesian Program Synthesis (BPS) has been described as a framework related to and utilizing probabilistic programming. In BPS, probabilistic programs are generated that are themselves priors over a space of probabilistic programs. This strategy allows more automatic synthesis of new programs via inference and is achieved by the composition of modular component programs. \n\nThe modularity in BPS allows inference to work on and test smaller probabilistic programs before being integrated into a larger model.\n\nBayesian methods and models are frequently used to incorporate prior knowledge. When good prior knowledge can be incorporated into a Bayesian model, effective inference can often be performed with much less data.\n\nThis framework can be also be contrasted with the family of automated program synthesis fields, including program synthesis, programming by example, and programming by demonstration. The goal in such fields is to find the best program that satisfies some constraint. In program synthesis, for instance, verification of logical constraints reduce the state space of possible programs, allowing more efficient search to find an optimal program. Bayesian Program Synthesis differs both in that the constraints are probabilistic and the output is itself a distribution over programs that can be further refined.\n\n\n"}
{"id": "199226", "url": "https://en.wikipedia.org/wiki?curid=199226", "title": "Bernoulli family", "text": "Bernoulli family\n\nThe Bernoulli family () of Basel is a patrician family, notable for having produced eight mathematically gifted academics who, between them, contributed to the foundations of applied mathematics and physics during the early modern period. Originally from Antwerp, Belgium, they moved to Basel and gained Swiss citizenship in 1620.\nThe family was related by marriage to the prominent French academic dynasty, Curie family through Johann Bernoulli (1667–1748).\nWhile their origin in Antwerp is certain, proposed connections with the Dutch family \"Bornouilla, Bernoullie\" or the Castilian family \"de Bernuy\" (\"Bernoille, Bernouille\") are uncertain.\n\nThe first known member of the family was Leon Bernoulli (d. 1561), a doctor in Antwerp, at that time, part of the Spanish Netherlands. His son, Jacob, emigrated to Frankfurt am Main in 1570 to escape from the Spanish persecution of the Protestants. \nJacob's grandson, a spice trader, also named Jacob, moved to Basel, Switzerland in 1620, and was granted Basel citizenship in 1622. His son, Niklaus (Nicolaus, 1623–1708), Leon's great-great-grandson, married Margarethe Schönauer. \nNiklaus had four sons, of which Johann and Hieronyus became the progenitors of the \"greater\" and the \"lesser\" branch of the family, respectively.\nThe four sons of Niklaus were:\n\nIn addition to those mentioned above, the Bernoulli family produced many notable artists and scientists, in particular, a number of famous mathematicians in the 18th century:\n\nThe surname survives in Switzerland, with ten entries in the white pages for the city of Basel as of 2018.\n\n\n"}
{"id": "26866141", "url": "https://en.wikipedia.org/wiki?curid=26866141", "title": "Black–Karasinski model", "text": "Black–Karasinski model\n\nIn financial mathematics, the Black–Karasinski model is a mathematical model of the term structure of interest rates; see short rate model. It is a one-factor model as it describes interest rate movements as driven by a single source of randomness.\nIt belongs to the class of no-arbitrage models, i.e. it can fit today's zero-coupon bond prices, and in its most general form, today's prices for a set of caps, floors or European swaptions. The model was introduced by Fischer Black and Piotr Karasinski in 1991.\n\nThe main state variable of the model is the short rate, which is assumed to follow the stochastic differential equation (under the risk-neutral measure):\n\nwhere \"dW\" is a standard Brownian motion. The model implies a log-normal distribution for the short rate and therefore the expected value of the money-market account is infinite for any maturity.\n\nIn the original article by Fischer Black and Piotr Karasinski the model was implemented using a binomial tree with variable spacing, but a trinomial tree implementation is more common in practice, typically a lognormal application of the Hull-White Lattice.\n\nThe model is used mainly for the pricing of exotic interest rate derivatives such as American and Bermudan bond options and swaptions, once its parameters have been calibrated to the current term structure of interest rates and to the prices or implied volatilities of caps, floors or European swaptions. Numerical methods (usually trees) are used in the calibration stage as well as for pricing.\n\n\n"}
{"id": "27440323", "url": "https://en.wikipedia.org/wiki?curid=27440323", "title": "Cartan's lemma", "text": "Cartan's lemma\n\nIn mathematics, Cartan's lemma refers to a number of results named after either Élie Cartan or his son Henri Cartan:\n\n\n"}
{"id": "45715603", "url": "https://en.wikipedia.org/wiki?curid=45715603", "title": "Compound of cubes", "text": "Compound of cubes\n\nThere is one regular compound of cubes:\n\nThere are 3 uniform compounds of cubes:\n\nThere is one uniform dual:\n"}
{"id": "305456", "url": "https://en.wikipedia.org/wiki?curid=305456", "title": "Conway chained arrow notation", "text": "Conway chained arrow notation\n\nConway chained arrow notation, created by mathematician John Horton Conway, is a means of expressing certain extremely large numbers. It is simply a finite sequence of positive integers separated by rightward arrows, e.g. 2 → 3 → 4 → 5 → 6.\n\nAs with most combinatorial notations, the definition is recursive. In this case the notation eventually resolves to being the leftmost number raised to some (usually enormous) integer power.\n\nA \"Conway chain\" is defined as follows:\n\nAny chain represents an integer, according to the four rules below. Two chains are said to be equivalent if they represent the same integer.\n\nIf formula_2 and formula_3 are positive integers, and formula_4 is a subchain, then:\n\nNote that the last rule can be restated recursively to avoid the ellipses:\n\n\nOne must be careful to treat an arrow chain \"as a whole\". Arrow chains do not describe the iterated application of a binary operator. Whereas chains of other infixed symbols (e.g. 3 + 4 + 5 + 6 + 7) can often be considered in fragments (e.g. (3 + 4) + 5 + (6 + 7)) without a change of meaning (see associativity), or at least can be evaluated step by step in a prescribed order, e.g. 3 from right to left, that is not so with Conway's arrow.\n\nFor example:\n\nThe fourth rule is the core: A chain of 3 or more elements ending with 2 or higher becomes a chain of the same length with a (usually vastly) increased penultimate element. But its \"ultimate\" element is decremented, eventually permitting the third rule to shorten the chain. After, to paraphrase Knuth, \"much detail\", the chain is reduced to two elements and the second rule terminates the recursion.\n\nExamples get quite complicated quickly. Here are some small examples:\n\n\"n\"\n\n\"p→q\"\n\n1→(\"any arrowed expression\")\n\n4→3→2\n\n2→2→4\n\n2→4→3\n\n2→3→2→2\n\n3→2→2→2\n\nThe simplest cases with four terms (containing no integers less than 2) are:\n\nWe can see a pattern here. If, for any chain \"X\", we let formula_36 then formula_37 (see\nfunctional powers).\n\nApplying this with formula_38, then formula_39 and formula_40\n\nThus, for example, formula_41.\n\nMoving on:\n\nAgain we can generalize. When we write formula_47 we have formula_48, that is, formula_49. In the case above, formula_50 and formula_51, so formula_52\n\nThe Ackermann function may be expressed using Conway chained arrow notation:\n\nhence\n\n(\"n\" = 1 and \"n\" = 2 would correspond with \"A\"(\"m\", −2) = −1 and \"A\"(\"m\", −1) = 1, which could logically be added).\n\nGraham's number formula_53 itself cannot be expressed concisely in Conway chained arrow notation, but by defining the intermediate function formula_54, we have:\nformula_55 (see functional powers), and\nformula_56\n\nProof: Applying in order the definition, rule 3, and rule 4, we have:\n\nformula_57\nformula_62\n\nformula_63\nformula_66\n\nformula_67\nformula_74\n\nSince \"f\" is strictly increasing,\nwhich is the given inequality.\n\nWith chain arrows it is very easy to specify a much larger number. For example, note that\nformula_80\nwhich is much greater than Graham's number, because the number formula_81 = f(1) is much greater than 65.\n\nConway and Guy created a simple, single-argument function that diagonalizes over the entire notation, defined as:\n\nformula_82\n\nmeaning the sequence is:\n\ncg(1) = 1\n\ncg(2) = 2 → 2 = 2 = 4\n\ncg(3) = 3 → 3 → 3 = 3↑↑↑3\n\ncg(4) = 4 → 4 → 4 → 4\n\ncg(5) = 5 → 5 → 5 → 5 → 5\n\nThis function, as one might expect, grows extraordinarily fast.\n\nPeter Hurford has defined an extension to this notation:\n\nformula_83\n\nAll normal rules are unchanged otherwise.\n\nformula_84 is already equal to the aforementioned cg(a), and the function formula_85 is much faster growing than Conway and Guy's cg(n).\n\nNote that expressions like formula_86 are illegal if b and d are different numbers; one chain must only have one type of right-arrow.\n\nHowever, if we modify this slightly such that:\n\nformula_87\n\nthen not only does formula_86 become legal, but the notation as a whole becomes much stronger.\n\n\n"}
{"id": "29018709", "url": "https://en.wikipedia.org/wiki?curid=29018709", "title": "Correlation coefficient", "text": "Correlation coefficient\n\nA correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables. The variables may be two columns of a given data set of observations, often called a sample, or two components of a multivariate random variable with a known distribution.\n\nSeveral types of correlation coefficient exist, each with their own definition and own range of usability and characteristics. They all assume values in the range from −1 to +1, where +1 indicates the strongest possible agreement and −1 the strongest possible disagreement. As tools of analysis, correlation coefficients present certain problems, including the propensity of some types to be distorted by outliers and the possibility of incorrectly being used to infer a causal relationship between the variables.\n\nThe Pearson product-moment correlation coefficient, also known as \"r\", \"R\", or Pearson's \"r\", is a measure of the strength and direction of the linear relationship between two variables that is defined as the covariance of the variables divided by the product of their standard deviations. This is the best known and most commonly used type of correlation coefficient; when the term \"correlation coefficient\" is used without further qualification, it usually refers to the Pearson product-moment correlation coefficient\n\nIntraclass correlation (ICC) is a descriptive statistic that can be used when quantitative measurements are made on units that are organized into groups; it describes how strongly units in the same group resemble each other.\n\nRank correlation is a measure of the relationship between the rankings of two variables or two rankings of the same variable:\n\n"}
{"id": "2625993", "url": "https://en.wikipedia.org/wiki?curid=2625993", "title": "Erdős–Bacon number", "text": "Erdős–Bacon number\n\nA person's Erdős–Bacon number is the sum of one's Erdős number—which measures the \"collaborative distance\" in authoring academic papers between that person and Hungarian mathematician Paul Erdős—and one's Bacon number—which represents the number of links, through roles in films, by which the individual is separated from American actor Kevin Bacon. The lower the number, the closer a person is to Erdős and Bacon, which reflects a small world phenomenon in academia and entertainment.\n\nThe combined Erdős/Bacon [sic] number was introduced by mathematicians Tim Hsu and David Grabiner sometime before late-January 1999, when they pointed out that Daniel Kleitman has a combined number of 3: a Bacon number of 2 and Erdős number of 1.\n\nTo have a defined Erdős–Bacon number, it is necessary to have both appeared in a film and co-authored an academic paper, although this in and of itself is not sufficient.\nMathematician Daniel Kleitman has the Erdős–Bacon number of 3; it is the lowest among scientists: he is a co-author of Erdős on multiple papers, and has a Bacon number of 2, via Minnie Driver in \"Good Will Hunting\".\n\nMathematician Ken Ono has an Erdős–Bacon number of 4; 2 for Erdős and 2 for Bacon.\n\nMathematician Doron Zeilberger has an Erdős-Bacon number of 5. Computer scientist Tom Porter also has an Erdős-Bacon number of 5; 3 for Erdős in two ways and 2 for Bacon.\n\nAstronomer Carl Sagan has an Erdős number of 4 (via Steven J. Ostro) and a Bacon number of 2 (Sagan and Bacon having appeared with Johnny Carson on episodes of \"The Tonight Show\"), for a total of 6. Physicist Richard Feynman has an Erdős number of 3, and a Bacon number of 3, having appeared in the film \"Anti-Clock\" alongside Tony Tang. Geneticist Jonathan Pritchard appeared in the 1998 movie \"Without Limits\" which gives him a Bacon number of 2. Pritchard has an Erdős number of 4 thus giving him an Erdős–Bacon number of 6. Theoretical physicist Stephen Hawking has an Erdős–Bacon number of 6: his Bacon number of 2 (via his appearance alongside John Cleese in \"Monty Python Live (Mostly)\" who acted alongside Kevin Bacon in \"The Big Picture\") is lower than his Erdős number of 4.\n\nCanadian actor Albert M. Chan has an Erdős-Bacon number of 4. He co-authored a peer-reviewed paper on Orthogonal frequency-division multiplexing, giving him an Erdős number of 3, and was cast alongside Kevin Bacon in \"Patriots Day\", giving him a Bacon number of 1.\n\nDanica McKellar, who played Winnie Cooper in \"The Wonder Years\", has an Erdős–Bacon number of 6, having coauthored a mathematics paper published while an undergraduate at the University of California, Los Angeles. Her paper gives her an Erdős number of 4, and she has a Bacon number of 2, having worked with Margaret Easley.\n\nAmerican actress Natalie Portman has an Erdős–Bacon number of 7. She collaborated (using her birth name, Natalie Hershlag) with Abigail A. Baird, who has a collaboration path leading to Joseph Gillis, who has an Erdős number of 1. Portman appeared in \"A Powerful Noise Live\" (2009) with Sarah Michelle Gellar, who appeared in \"The Air I Breathe\" (2007) with Bacon, giving Portman a Bacon number of 2 and an Erdős number of 5.\n\nBritish actor Colin Firth has an Erdős–Bacon number of 7. Firth is credited as co-author of a neuroscience paper, \"Political Orientations Are Correlated with Brain Structure in Young Adults\", after he suggested on BBC Radio 4 that such a study could be done. Another author of that paper, Geraint Rees, has an Erdős number of 5, which gives Firth an Erdős number of 6. Firth's Bacon number of 1 is due to his appearance in \"Where the Truth Lies\".\n\nKristen Stewart has an Erdős–Bacon number of 7; she is credited as a co-author on an artificial intelligence paper that was written after a technique was used for her short film \"Come Swim\", giving her an Erdős number of 5, and she co-starred with Michael Sheen in \"Twilight\", who co-starred with Bacon in \"Frost/Nixon\", giving her a Bacon number of 2.\n\nAmerican scholar and actor Michael M. Chemers has an Erdös-Bacon number of 6. He co-authored a 2018 paper about \"Game of Thrones\" with mathematician Andrew Beveridge, who has an Erdös number of 2, giving him an Erdös number of 3, and co-starred in two films (\"When Tyrants Kiss,\" 2004; \"Before the Thunder,\" 2018) both of which give him a Bacon number of 3 through a number of co-stars.\n\nNotes:\n"}
{"id": "523076", "url": "https://en.wikipedia.org/wiki?curid=523076", "title": "Excitable medium", "text": "Excitable medium\n\nAn excitable medium is a nonlinear dynamical system which has the capacity to propagate a wave of some description, and which cannot support the passing of another wave until a certain amount of time has passed (known as the refractory time).\n\nA forest is an example of an excitable medium: if a wildfire burns through the forest, no fire can return to a burnt spot until the vegetation has gone through its refractory period and regrown. In chemistry, oscillating reactions are excitable media, for example the Belousov–Zhabotinsky reaction and the Briggs–Rauscher reaction. Pathological activities in the heart and brain can be modelled as excitable media. A group of spectators at a sporting event are an excitable medium, as can be observed in a Mexican wave (so-called from its initial appearance in the 1986 World Cup in Mexico).\n\nExcitable media can be modelled using both partial differential equations and cellular automata.\n\nCellular automata provide a simple model to aid in the understanding of excitable media. Perhaps the simplest such model is in. See Greenberg-Hastings cellular automaton for this model. \nEach cell of the automaton is made to represent some section of the medium being modelled (for example, a patch of trees in a forest, or a segment of heart tissue). Each cell can be in one of the three following states:\n\n\nAs in all cellular automata, the state of a particular cell in the next time step depends on the state of the cells around it—its neighbours—at the current time. In the forest fire example the simple rules given in Greenberg-Hastings cellular automaton might be modified as follows:\n\n\nThis function can be refined according to the particular medium. For example, the effect of wind can be added to the model of the forest fire.\n\nIt is most common for a one-dimensional medium to form a closed circuit, i.e. a ring. For example, the Mexican wave can be modeled as a ring going around the stadium. If the wave moves in one direction it will eventually return to where it started. If, upon a wave's return to the origin, the original spot has gone through its refractory period, then the wave will propagate along the ring again (and will do so indefinitely). If, however, the origin is still refractory upon the wave's return, the wave will be stopped.\n\nIn the Mexican wave, for example, if for some reason, the originators of the wave are still standing upon its return it will not continue. If the originators have sat back down then the wave can, in theory, continue.\n\nSeveral forms of waves can be observed in a two-dimensional medium.\n\nA \"spreading wave\" will originate at a single point in the medium and spread outwards. For example, a forest fire could start from a lightning strike at the centre of a forest and spread outwards.\n\nA \"spiral wave\" will again originate at a single point, but will spread in a spiral circuit. Spiral waves are believed to underlie phenomena such as tachycardia and fibrillation.\n\nSpiral waves constitute one of the mechanisms of fibrillation when they organize in long-lasting reentrant activities named rotors.\n\n\n"}
{"id": "34824761", "url": "https://en.wikipedia.org/wiki?curid=34824761", "title": "Fraïssé's theorem", "text": "Fraïssé's theorem\n\nIn mathematics, Fraïssé's theorem, named after Roland Fraïssé, states that a class \"K\" of finite relational structures is the age of a countable homogeneous relational structure if and only if it satisfies the following four conditions:\n\n\nIf these conditions hold, then the countable homogeneous structure whose age is \"K\" is unique up to isomorphism.\n\nFraïssé proved the theorem in the 1950s. \n\nFor a proof and more details see Section 1.2 and Appendix A of this thesis. \n"}
{"id": "6134187", "url": "https://en.wikipedia.org/wiki?curid=6134187", "title": "History of mathematical notation", "text": "History of mathematical notation\n\nThe history of mathematical notation includes the commencement, progress, and cultural diffusion of mathematical symbols and the conflict of the methods of notation confronted in a notation's move to popularity or inconspicuousness. Mathematical notation comprises the symbols used to write mathematical equations and formulas. Notation generally implies a set of well-defined representations of quantities and symbols operators. The history includes Hindu–Arabic numerals, letters from the Roman, Greek, Hebrew, and German alphabets, and a host of symbols invented by mathematicians over the past several centuries.\n\nThe development of mathematical notation can be divided in stages. The \"\"rhetorical\" stage is where calculations are performed by words and no symbols are used. The \"syncopated\"\" stage is where frequently used operations and quantities are represented by symbolic syntactical abbreviations. From ancient times through the post-classical age, bursts of mathematical creativity were often followed by centuries of stagnation. As the early modern age opened and the worldwide spread of knowledge began, written examples of mathematical developments came to light. The \"symbolic\" stage is where comprehensive systems of notation supersede rhetoric. Beginning in Italy in the 16th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This symbolic system was in use by medieval Indian mathematicians and in Europe since the middle of the 17th century, and has continued to develop in the contemporary era.\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, the focus here, the investigation into the mathematical methods and notation of the past.\n\nAlthough the history commences with that of the Ionian schools, there is no doubt that those Ancient Greeks who paid attention to it were largely indebted to the previous investigations of the Ancient Egyptians and Ancient Phoenicians. Numerical notation's distinctive feature, i.e. symbols having local as well as intrinsic values (arithmetic), implies a state of civilization at the period of its invention. Our knowledge of the mathematical attainments of these early peoples, to which this section is devoted, is imperfect and the following brief notes be regarded as a summary of the conclusions which seem most probable, and the history of mathematics begins with the symbolic sections.\n\nMany areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world; algebra started with methods of solving problems in arithmetic.\n\nThere can be no doubt that most early peoples which have left records knew something of numeration and mechanics, and that a few were also acquainted with the elements of land-surveying. In particular, the Egyptians paid attention to geometry and numbers, and the Phoenicians to practical arithmetic, book-keeping, navigation, and land-surveying. The results attained by these people seem to have been accessible, under certain conditions, to travelers. It is probable that the knowledge of the Egyptians and Phoenicians was largely the result of observation and measurement, and represented the accumulated experience of many ages.\n\nWritten mathematics began with numbers expressed as tally marks, with each tally representing a single unit. The numerical symbols consisted probably of strokes or notches cut in wood or stone, and intelligible alike to all nations. For example, one notch in a bone represented one animal, or person, or anything else. The peoples with whom the Greeks of Asia Minor (amongst whom notation in western history begins) were likely to have come into frequent contact were those inhabiting the eastern littoral of the Mediterranean: and Greek tradition uniformly assigned the special development of geometry to the Egyptians, and that of the science of numbers either to the Egyptians or to the Phoenicians.\n\nThe Ancient Egyptians had a symbolic notation which was the numeration by Hieroglyphics. The Egyptian mathematics had a symbol for one, ten, one-hundred, one-thousand, ten-thousand, one-hundred-thousand, and one-million. Smaller digits were placed on the left of the number, as they are in Hindu–Arabic numerals. Later, the Egyptians used hieratic instead of hieroglyphic script to show numbers. Hieratic was more like cursive and replaced several groups of symbols with individual ones. For example, the four vertical lines used to represent four were replaced by a single horizontal line. This is found in the Rhind Mathematical Papyrus (c. 2000–1800 BC) and the Moscow Mathematical Papyrus (c. 1890 BC). The system the Egyptians used was discovered and modified by many other civilizations in the Mediterranean. The Egyptians also had symbols for basic operations: legs going forward represented addition, and legs walking backward to represent subtraction.\n\nThe Mesopotamians had symbols for each power of ten. Later, they wrote their numbers in almost exactly the same way done in modern times. Instead of having symbols for each power of ten, they would just put the coefficient of that number. Each digit was at separated by only a space, but by the time of Alexander the Great, they had created a symbol that represented zero and was a placeholder. The Mesopotamians also used a sexagesimal system, that is base sixty. It is this system that is used in modern times when measuring time and angles. Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework. The earliest evidence of written mathematics dates back to the ancient Sumerians and the system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\n\nThe majority of Mesopotamian clay tablets date from 1800 to 1600 BC, and cover topics which include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear and quadratic equations. The Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of minutes and seconds of arc to denote fractions of a degree. Babylonian advances in mathematics were facilitated by the fact that 60 has many divisors: the reciprocal of any integer which is a multiple of divisors of 60 has a finite expansion in base 60. (In decimal arithmetic, only reciprocals of multiples of 2 and 5 have finite decimal expansions.) Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. They lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.\n\nThe history of mathematics cannot with certainty be traced back to any school or period before that of the Ionian Greeks, but the subsequent history may be divided into periods, the distinctions between which are tolerably well marked. Greek mathematics, which originated with the study of geometry, tended from its commencement to be deductive and scientific. Since the fourth century AD, Pythagoras has commonly been given credit for discovering the Pythagorean theorem, a theorem in geometry that states that in a right-angled triangle the area of the square on the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares of the other two sides. The ancient mathematical texts are available with the prior mentioned Ancient Egyptians notation and with Plimpton 322 (Babylonian mathematics c. 1900 BC). The study of mathematics as a subject in its own right begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\".\n\nPlato's influence has been especially strong in mathematics and the sciences. He helped to distinguish between pure and applied mathematics by widening the gap between \"arithmetic\", now called number theory and \"logistic\", now called arithmetic. Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Aristotle is credited with what later would be called the law of excluded middle.\n\n\"Abstract Mathematics\" is what treats of magnitude or quantity, absolutely and generally conferred, without regard to any species of particular magnitude, such as Arithmetic and Geometry, In this sense, abstract mathematics is opposed to mixed mathematics; wherein simple and abstract properties, and the relations of quantities primitively considered in mathematics, are applied to sensible objects, and by that means become intermixed with physical considerations; Such are Hydrostatics, Optics, Navigation, &c.\n\nArchimedes is generally considered to be the greatest mathematician of antiquity and one of the greatest of all time. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers.\nIn the historical development of geometry, the steps in the abstraction of geometry were made by the ancient Greeks. Euclid's Elements being the earliest extant documentation of the axioms of plane geometry— though Proclus tells of an earlier axiomatisation by Hippocrates of Chios. Euclid's \"Elements\" (c. 300 BC) is one of the oldest extant Greek mathematical treatises and consisted of 13 books written in Alexandria; collecting theorems proven by other mathematicians, supplemented by some original work. The document is a successful collection of definitions, postulates (axioms), propositions (theorems and constructions), and mathematical proofs of the propositions. Euclid's first theorem is a lemma that possesses properties of prime numbers. The influential thirteen books cover Euclidean geometry, geometric algebra, and the ancient Greek version of algebraic systems and elementary number theory. It was ubiquitous in the Quadrivium and is instrumental in the development of logic, mathematics, and science.\n\nDiophantus of Alexandria was author of a series of books called \"Arithmetica\", many of which are now lost. These texts deal with solving algebraic equations. Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nThe Greeks employed Attic numeration, which was based on the system of the Egyptians and was later adapted and used by the Romans. Greek numerals one through four were vertical lines, as in the hieroglyphics. The symbol for five was the Greek letter Π (pi), which is the letter of the Greek word for five, \"pente\". Numbers six through nine were \"pente\" with vertical lines next to it. Ten was represented by the letter (Δ) of the word for ten, \"deka\", one hundred by the letter from the word for hundred, etc.\n\nThe Ionian numeration used their entire alphabet including three archaic letters. The numeral notation of the Greeks, though far less convenient than that now in use, was formed on a perfectly regular and scientific plan, and could be used with tolerable effect as an instrument of calculation, to which purpose the Roman system was totally inapplicable. The Greeks divided the twenty-four letters of their alphabet into three classes, and, by adding another symbol to each class, they had characters to represent the units, tens, and hundreds. (Jean Baptiste Joseph Delambre's Astronomie Ancienne, t. ii.)\nThis system appeared in the third century BC, before the letters digamma (Ϝ), koppa (Ϟ), and sampi (Ϡ) became obsolete. When lowercase letters became differentiated from upper case letters, the lower case letters were used as the symbols for notation. Multiples of one thousand were written as the nine numbers with a stroke in front of them: thus one thousand was \",α\", two-thousand was \",β\", etc. M (for μὐριοι, as in \"myriad\") was used to multiply numbers by ten thousand. For example, the number 88,888,888 would be written as M,ηωπη*ηωπη\n\nGreek mathematical reasoning was almost entirely geometric (albeit often used to reason about non-geometric subjects such as number theory), and hence the Greeks had no interest in algebraic symbols. The great exception was Diophantus of Alexandria, the great algebraist. His \"Arithmetica\" was one of the texts to use symbols in equations. It was not completely symbolic, but was much more so than previous books. An unknown number was called s. The square of s was formula_1; the cube was formula_2; the fourth power was formula_3; and the fifth power was formula_4.\n\nThe Chinese used numerals that look much like the tally system. Numbers one through four were horizontal lines. Five was an X between two horizontal lines; it looked almost exactly the same as the Roman numeral for ten. Nowadays, the huāmǎ system is only used for displaying prices in Chinese markets or on traditional handwritten invoices.\n\nIn the history of the Chinese, there were those who were familiar with the sciences of arithmetic, geometry, mechanics, optics, navigation, and astronomy. Mathematics in China emerged independently by the 11th century BC. It is almost certain that the Chinese were acquainted with several geometrical or rather architectural implements; with mechanical machines; that they knew of the characteristic property of the magnetic needle; and were aware that astronomical events occurred in cycles. Chinese of that time had made attempts to classify or extend the rules of arithmetic or geometry which they knew, and to explain the causes of the phenomena with which they were acquainted beforehand. The Chinese independently developed very large and negative numbers, decimals, a place value decimal system, a binary system, algebra, geometry, and trigonometry.\nChinese mathematics made early contributions, including a place value system. The geometrical theorem known to the ancient Chinese were acquainted was applicable in certain cases (namely the ratio of sides). It is that geometrical theorems which can be demonstrated in the quasi-experimental way of superposition were also known to them. In arithmetic their knowledge seems to have been confined to the art of calculation by means of the swan-pan, and the power of expressing the results in writing. Our knowledge of the early attainments of the Chinese, slight though it is, is more complete than in the case of most of their contemporaries. It is thus instructive, and serves to illustrate the fact, that it can be known a nation may possess considerable skill in the applied arts with but our knowledge of the later mathematics on which those arts are founded can be scarce. Knowledge of Chinese mathematics before 254 BC is somewhat fragmentary, and even after this date the manuscript traditions are obscure. Dates centuries before the classical period are generally considered conjectural by Chinese scholars unless accompanied by verified archaeological evidence.\n\nAs in other early societies the focus was on astronomy in order to perfect the agricultural calendar, and other practical tasks, and not on establishing formal systems.The Chinese Board of Mathematics duties were confined to the annual preparation of an almanac, the dates and predictions in which it regulated. Ancient Chinese mathematicians did not develop an axiomatic approach, but made advances in algorithm development and algebra. The achievement of Chinese algebra reached its zenith in the 13th century, when Zhu Shijie invented method of four unknowns.\n\nAs a result of obvious linguistic and geographic barriers, as well as content, Chinese mathematics and that of the mathematics of the ancient Mediterranean world are presumed to have developed more or less independently up to the time when \"The Nine Chapters on the Mathematical Art\" reached its final form, while the \"Writings on Reckoning\" and \"Huainanzi\" are roughly contemporary with classical Greek mathematics. Some exchange of ideas across Asia through known cultural exchanges from at least Roman times is likely. Frequently, elements of the mathematics of early societies correspond to rudimentary results found later in branches of modern mathematics such as geometry or number theory. The Pythagorean theorem for example, has been attested to the time of the Duke of Zhou. Knowledge of Pascal's triangle has also been shown to have existed in China centuries before Pascal, such as by Shen Kuo.\nThe state of trigonometry in China slowly began to change and advance during the Song Dynasty (960–1279), where Chinese mathematicians began to express greater emphasis for the need of spherical trigonometry in calendarical science and astronomical calculations. The polymath Chinese scientist, mathematician and official Shen Kuo (1031–1095) used trigonometric functions to solve mathematical problems of chords and arcs. Sal Restivo writes that Shen's work in the lengths of arcs of circles provided the basis for spherical trigonometry developed in the 13th century by the mathematician and astronomer Guo Shoujing (1231–1316). As the historians L. Gauchet and Joseph Needham state, Guo Shoujing used spherical trigonometry in his calculations to improve the calendar system and Chinese astronomy. The mathematical science of the Chinese would incorporate the work and teaching of Arab missionaries with knowledge of spherical trigonometry who had come to China in the course of the thirteenth century.\n\nAlthough the origin of our present system of numerical notation is ancient, there is no doubt that it was in use among the Hindus over two thousand years ago. The algebraic notation of the Indian mathematician, Brahmagupta, was syncopated. Addition was indicated by placing the numbers side by side, subtraction by placing a dot over the subtrahend (the number to be subtracted), and division by placing the divisor below the dividend, similar to our notation but without the bar. Multiplication, evolution, and unknown quantities were represented by abbreviations of appropriate terms. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, likely evolved over the course of the first millennium AD in India and was transmitted to the west via Islamic mathematics.\n\nDespite their name, Arabic numerals actually started in India. The reason for this misnomer is Europeans saw the numerals used in an Arabic book, \"Concerning the Hindu Art of Reckoning\", by Mohommed ibn-Musa al-Khwarizmi. Al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. Al-Khwarizmi did not claim the numerals as Arabic, but over several Latin translations, the fact that the numerals were Indian in origin was lost. The word \"algorithm\" is derived from the Latinization of Al-Khwārizmī's name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\").\n\nIslamic mathematics developed and expanded the mathematics known to Central Asian civilizations. Al-Khwārizmī gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and Al-Khwārizmī was to teach algebra in an elementary form and for its own sake. Al-Khwārizmī also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" Al-Khwārizmī also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nAl-Karaji, in his treatise \"al-Fakhri\", extends the methodology to incorporate integer powers and integer roots of unknown quantities. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham would develop analytic geometry. Al-Haytham derived the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. Al-Haytham performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. In the late 11th century, Omar Khayyam would develop algebraic geometry, wrote \"Discussions of the Difficulties in Euclid\", and wrote on the general geometric solution to cubic equations. Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals.\n\nMany Greek and Arabic texts on mathematics were then translated into Latin, which led to further development of mathematics in medieval Europe. In the 12th century, scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's and the complete text of Euclid's \"Elements\". One of the European books that advocated using the numerals was \"Liber Abaci\", by Leonardo of Pisa, better known as Fibonacci. \"Liber Abaci\" is better known for the mathematical problem Fibonacci wrote in it about a population of rabbits. The growth of the population ended up being a Fibonacci sequence, where a term is the sum of the two preceding terms.\n\nAbū al-Hasan ibn Alī al-Qalasādī (1412–1482) was the last major medieval Arab algebraist, who improved on the algebraic notation earlier used by Ibn al-Yāsamīn in the 12th century and, in the Maghreb, by Ibn al-Banna in the 13th century. In contrast to the syncopated notations of their predecessors, Diophantus and Brahmagupta, which lacked symbols for mathematical operations, al-Qalasadi's algebraic notation was the first to have symbols for these functions and was thus \"the first steps toward the introduction of algebraic symbolism.\" He represented mathematical symbols using characters from the Arabic alphabet.\n\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. The two widely used arithmetic symbols are addition and subtraction, + and −. The plus sign was used by 1360 by Nicole Oresme in his work \"Algorismus proportionum\". It is thought an abbreviation for \"et\", meaning \"and\" in Latin, in much the same way the ampersand sign also began as \"et\". Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of the distance covered by a body undergoing uniformly accelerated motion, asserting that the area under the line depicting the constant acceleration and represented the total distance traveled. The minus sign was used in 1489 by Johannes Widmann in \"Mercantile Arithmetic\" or \"Behende und hüpsche Rechenung auff allen Kauffmanschafft,\". Widmann used the minus symbol with the plus symbol, to indicate deficit and surplus, respectively. In \"Summa de arithmetica, geometria, proportioni e proportionalità\", Luca Pacioli used symbols for plus and minus symbols and contained algebra.\n\nIn the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots. In 1533, Regiomontanus's table of sines and cosines were published. Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. The radical symbol for square root was introduced by Christoph Rudolff. Michael Stifel's important work \"Arithmetica integra\" contained important innovations in mathematical notation. In 1556, Niccolò Tartaglia used parentheses for precedence grouping. In 1557 Robert Recorde published The Whetstone of Witte which used the equal sign (=) as well as plus and minus signs for the English reader. In 1564, Gerolamo Cardano analyzed games of chance beginning the early stages of probability theory. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations. Simon Stevin's book \"De Thiende\" ('the art of tenths'), published in Dutch in 1585, contained a systematic treatment of decimal notation, which influenced all later work on the real number system. The New algebra (1591) of François Viète introduced the modern notational manipulation of algebraic expressions. For navigation and accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus coin the word \"trigonometry\", publishing his \"Trigonometria\" in 1595.\n\nJohn Napier is best known as the inventor of logarithms and made common the use of the decimal point in arithmetic and mathematics. After Napier, Edmund Gunter created the logarithmic scales (lines, or rules) upon which slide rules are based, it was William Oughtred who used two such scales sliding by one another to perform direct multiplication and division; and he is credited as the inventor of the slide rule in 1622. In 1631 Oughtred introduced the multiplication sign (×) his proportionality sign, and abbreviations \"sin\" and \"cos\" for the sine and cosine functions. Albert Girard also used the abbreviations 'sin', 'cos' and 'tan' for the trigonometric functions in his treatise.\n\nJohannes Kepler was one of the pioneers of the mathematical applications of infinitesimals. René Descartes is credited as the father of analytical geometry, the bridge between algebra and geometry, crucial to the discovery of infinitesimal calculus and analysis. In the 17th century, Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Blaise Pascal influenced mathematics throughout his life. His \"Traité du triangle arithmétique\" (\"Treatise on the Arithmetical Triangle\") of 1653 described a convenient tabular presentation for binomial coefficients. Pierre de Fermat and Blaise Pascal would investigate probability. John Wallis introduced the infinity symbol. He similarly used this notation for infinitesimals. In 1657, Christiaan Huygens published the treatise on probability, \"On Reasoning in Games of Chance\". \n\nJohann Rahn introduced the division symbol (obelus) and the therefore sign in 1659. William Jones used π in \"Synopsis palmariorum mathesios\" in 1706 because it is the letter of the Greek word perimetron (περιμετρον), which means perimeter in Greek. This usage was popularized in 1737 by Euler. In 1734, Pierre Bouguer used double horizontal bar below the inequality sign.\n\nThe study of linear algebra emerged from the study of determinants, which were used to solve systems of linear equations. Calculus had two main systems of notation, each created by one of the creators: that developed by Isaac Newton and the notation developed by Gottfried Leibniz. Leibniz's is the notation used most often today. Newton's was simply a dot or dash placed above the function. In modern usage, this notation generally denotes derivatives of physical quantities with respect to time, and is used frequently in the science of mechanics. Leibniz, on the other hand, used the letter \"d\" as a prefix to indicate differentiation, and introduced the notation representing derivatives as if they were a special type of fraction. This notation makes explicit the variable with respect to which the derivative of the function is taken. Leibniz also created the integral symbol. The symbol is an elongated S, representing the Latin word \"Summa\", meaning \"sum\". When finding areas under curves, integration is often illustrated by dividing the area into infinitely many tall, thin rectangles, whose areas are added. Thus, the integral symbol is an elongated s, for sum.\n\nLetters of the alphabet in this time were to be used as symbols of quantity; and although much diversity existed with respect to the choice of letters, there were to be several universally recognized rules in the following history. Here thus in the history of equations the first letters of the alphabet were indicatively known as coefficients, the last letters the s (an \"incerti ordinis\"). In algebraic geometry, again, a similar rule was to be observed, the last letters of the alphabet there denoting the variable or current coordinates. Certain letters, such as formula_5, formula_6, etc., were by universal consent appropriated as symbols of the frequently occurring numbers 3.14159 ..., and 2.7182818 ..., etc., and their use in any other acceptation was to be avoided as much as possible. Letters, too, were to be employed as symbols of operation, and with them other previously mentioned arbitrary operation characters. The letters formula_7, elongated formula_8 were to be appropriated as operative symbols in the differential calculus and integral calculus, formula_9 and ∑ in the calculus of differences. In functional notation, a letter, as a symbol of operation, is combined with another which is regarded as a symbol of quantity.\n\nBeginning in 1718, Thomas Twinin used the division slash (solidus), deriving it from the earlier Arabic horizontal fraction bar. Pierre-Simon, marquis de Laplace developed the widely used Laplacian differential operator. In 1750, Gabriel Cramer developed \"Cramer's Rule\" for solving linear systems.\n\nLeonhard Euler was one of the most prolific mathematicians in history, and also a prolific inventor of canonical notation. His contributions include his use of \"e\" to represent the base of natural logarithms. It is not known exactly why formula_6 was chosen, but it was probably because the four letters of the alphabet were already commonly used to represent variables and other constants. Euler used formula_5 to represent pi consistently. The use of formula_5 was suggested by William Jones, who used it as shorthand for perimeter. Euler used formula_13 to represent the square root of negative one, although he earlier used it as an \"infinite number.\" For summation, Euler used sigma, Σ. For functions, Euler used the notation formula_14 to represent a function of formula_15. In 1730, Euler wrote the gamma function. In 1736, Euler produces his paper on the Seven Bridges of Königsberg initiating the study of graph theory.\n\nThe mathematician, William Emerson would develop the proportionality sign. Much later in the abstract expressions of the value of various proportional phenomena, the parts-per notation would become useful as a set of pseudo units to describe small values of miscellaneous dimensionless quantities. Marquis de Condorcet, in 1768, advanced the partial differential sign. In 1771, Alexandre-Théophile Vandermonde deduced the importance of topological features when discussing the properties of knots related to the geometry of position. Between 1772 and 1788, Joseph-Louis Lagrange re-formulated the formulas and calculations of Classical \"Newtonian\" mechanics, called Lagrangian mechanics. The prime symbol for derivatives was also made by Lagrange.\n\nAt the turn of the 19th century, Carl Friedrich Gauss developed the identity sign for congruence relation and, in Quadratic reciprocity, the integral part. Gauss contributed functions of complex variables, in geometry, and on the convergence of series. He gave the satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law. Gauss developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy. He would also develop the product sign. Also in this time, Niels Henrik Abel and Évariste Galois conducted their work on the solvability of equations, linking group theory and field theory.\n\nAfter the 1800s, Christian Kramp would promote factorial notation during his research in generalized factorial function which applied to non-integers. Joseph Diaz Gergonne introduced the set inclusion signs. Peter Gustav Lejeune Dirichlet developed Dirichlet \"L\"-functions to give the proof of Dirichlet's theorem on arithmetic progressions and began analytic number theory. In 1828, Gauss proved his Theorema Egregium (\"remarkable theorem\" in Latin), establishing property of surfaces. In the 1830s, George Green developed Green's function. In 1829. Carl Gustav Jacob Jacobi publishes Fundamenta nova theoriae functionum ellipticarum with his elliptic theta functions. By 1841, Karl Weierstrass, the \"father of modern analysis\", elaborated on the concept of absolute value and the determinant of a matrix.\n\nMatrix notation would be more fully developed by Arthur Cayley in his three papers, on subjects which had been suggested by reading the Mécanique analytique of Lagrange and some of the works of Laplace. Cayley defined matrix multiplication and matrix inverses. Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\nWilliam Rowan Hamilton would introduce the nabla symbol for vector differentials. This was previously used by Hamilton as a general-purpose operator sign. Hamilton reformulated Newtonian mechanics, now called Hamiltonian mechanics. This work has proven central to the modern study of classical field theories such as electromagnetism. This was also important to the development of quantum mechanics. In mathematics, he is perhaps best known as the inventor of quaternion notation and biquaternions. Hamilton also introduced the word \"tensor\" in 1846. James Cockle would develop the tessarines and, in 1849, coquaternions. In 1848, James Joseph Sylvester introduced into matrix algebra the term matrix.\n\nIn 1864 James Clerk Maxwell reduced all of the then current knowledge of electromagnetism into a linked set of differential equations with 20 equations in 20 variables, contained in \"A Dynamical Theory of the Electromagnetic Field\". (See Maxwell's equations.) The method of calculation which it is necessary to employ was given by Lagrange, and afterwards developed, with some modifications, by Hamilton's equations. It is usually referred to as Hamilton's principle; when the equations in the original form are used they are known as Lagrange's equations. In 1871 Richard Dedekind called a set of real or complex numbers which is closed under the four arithmetic operations a field. In 1873 Maxwell presented \"A Treatise on Electricity and Magnetism\".\n\nIn 1878, William Kingdon Clifford published his Elements of Dynamic. Clifford developed split-biquaternions, which he called \"algebraic motors\". Clifford obviated quaternion study by separating the dot product and cross product of two vectors from the complete quaternion notation. This approach made vector calculus available to engineers and others working in three dimensions and skeptical of the lead–lag effect in the fourth dimension. The common vector notations are used when working with vectors which are spatial or more abstract members of vector spaces, while angle notation (or phasor notation) is a notation used in electronics.\n\nIn 1881, Leopold Kronecker defined what he called a \"domain of rationality\", which is a field extension of the field of rational numbers in modern terms. In 1882, wrote the book titled \"Linear Algebra\". Lord Kelvin's aetheric atom theory (1860s) led Peter Guthrie Tait, in 1885, to publish a topological table of knots with up to ten crossings known as the Tait conjectures. In 1893, Heinrich M. Weber gave the clear definition of an abstract field. Tensor calculus was developed by Gregorio Ricci-Curbastro between 1887–96, presented in 1892 under the title \"absolute differential calculus\", and the contemporary usage of \"tensor\" was stated by Woldemar Voigt in 1898. In 1895, Henri Poincaré published \"Analysis Situs\". In 1897, Charles Proteus Steinmetz would publish , with the assistance of Ernst J. Berg.\n\nIn 1895 Giuseppe Peano issued his \"Formulario mathematico\", an effort to digest mathematics into terse text based on special symbols. He would provide a definition of a vector space and linear map. He would also introduce the intersection sign, the union sign, the membership sign (is an element of), and existential quantifier (there exists). Peano would pass to Bertrand Russell his work in 1900 at a Paris conference; it so impressed Russell that Russell too was taken with the drive to render mathematics more concisely. The result was Principia Mathematica written with Alfred North Whitehead. This treatise marks a watershed in modern literature where symbol became dominant. Ricci-Curbastro and Tullio Levi-Civita popularized the tensor index notation around 1900.\n\nAt the beginning of this period, Felix Klein's \"Erlangen program\" identified the underlying theme of various geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra. Georg Cantor would introduce the aleph symbol for cardinal numbers of transfinite sets. His notation for the cardinal numbers was the Hebrew letter formula_16 (aleph) with a natural number subscript; for the ordinals he employed the Greek letter ω (omega). This notation is still in use today in ordinal notation of a finite sequence of symbols from a finite alphabet which names an ordinal number according to some scheme which gives meaning to the language. His theory created a great deal of controversy. Cantor would, in his study of Fourier series, consider point sets in Euclidean space.\n\nAfter the turn of the 20th century, Josiah Willard Gibbs would in physical chemistry introduce middle dot for dot product and the multiplication sign for cross products. He would also supply notation for the scalar and vector products, which was introduced in \"Vector Analysis\". In 1904, Ernst Zermelo promotes axiom of choice and his proof of the well-ordering theorem. Bertrand Russell would shortly afterward introduce logical disjunction (OR) in 1906. Also in 1906, Poincaré would publish \"On the Dynamics of the Electron\" and Maurice Fréchet introduced metric space. Later, Gerhard Kowalewski and Cuthbert Edmund Cullis would successively introduce matrices notation, parenthetical matrix and box matrix notation respectively. After 1907, mathematicians studied knots from the point of view of the knot group and invariants from homology theory. In 1908, Joseph Wedderburn's structure theorems were formulated for finite-dimensional algebras over a field. Also in 1908, Ernst Zermelo proposed \"definite\" property and the first axiomatic set theory, Zermelo set theory. In 1910 Ernst Steinitz published the influential paper \"Algebraic Theory of Fields\". In 1911, Steinmetz would publish \"Theory and Calculation of Transient Electric Phenomena and Oscillations\".\nAlbert Einstein, in 1916, introduced the Einstein notation which summed over a set of indexed terms in a formula, thus exerting notational brevity. Arnold Sommerfeld would create the contour integral sign in 1917. Also in 1917, Dimitry Mirimanoff proposes axiom of regularity. In 1919, Theodor Kaluza would solve general relativity equations using five dimensions, the results would have electromagnetic equations emerge. This would be published in 1921 in \"Zum Unitätsproblem der Physik\". In 1922, Abraham Fraenkel and Thoralf Skolem independently proposed replacing the axiom schema of specification with the axiom schema of replacement. Also in 1922, Zermelo–Fraenkel set theory was developed. In 1923, Steinmetz would publish \"Four Lectures on Relativity and Space\". Around 1924, Jan Arnoldus Schouten would develop the modern notation and formalism for the Ricci calculus framework during the absolute differential calculus applications to general relativity and differential geometry in the early twentieth century. In 1925, Enrico Fermi would describe a system comprising many identical particles that obey the Pauli exclusion principle, afterwards developing a diffusion equation (Fermi age equation). In 1926, Oskar Klein would develop the Kaluza–Klein theory. In 1928, Emil Artin abstracted ring theory with Artinian rings. In 1933, Andrey Kolmogorov introduces the \"Kolmogorov axioms\". In 1937, Bruno de Finetti deduced the \"operational subjective\" concept.\n\nMathematical abstraction began as a process of extracting the underlying essence of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena. Two abstract areas of modern mathematics are category theory and model theory. Bertrand Russell, said, \"Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say\". Though, one can substituted mathematics for real world objects, and wander off through equation after equation, and can build a concept structure which has no relation to reality.\n\nSymbolic logic studies the purely formal properties of strings of symbols. The interest in this area springs from two sources. First, the notation used in symbolic logic can be seen as representing the words used in philosophical logic. Second, the rules for manipulating symbols found in symbolic logic can be implemented on a computing machine. Symbolic logic is usually divided into two subfields, propositional logic and predicate logic. Other logics of interest include temporal logic, modal logic and fuzzy logic. The area of symbolic logic called propositional logic, also called \"propositional calculus\", studies the properties of sentences formed from constants and logical operators. The corresponding logical operations are known, respectively, as conjunction, disjunction, material conditional, biconditional, and negation. These operators are denoted as keywords and by symbolic notation.\n\nSome of the introduced mathematical logic notation during this time included the set of symbols used in Boolean algebra. This was created by George Boole in 1854. Boole himself did not see logic as a branch of mathematics, but it has come to be encompassed anyway. Symbols found in Boolean algebra include formula_17 (AND), formula_18 (OR), and formula_19 (\"not\"). With these symbols, and letters to represent different truth values, one can make logical statements such as formula_20, that is \"(\"a\" is true OR \"a\" is \"not\" true) is true\", meaning it is true that \"a\" is either true or not true (i.e. false). Boolean algebra has many practical uses as it is, but it also was the start of what would be a large set of symbols to be used in logic. Predicate logic, originally called \"predicate calculus\", expands on propositional logic by the introduction of variables and by sentences containing variables, called predicates. In addition, predicate logic allows quantifiers. With these logic symbols and additional quantifiers from predicate logic, valid proofs can be made that are irrationally artificial, but syntactical.\n\nWhile proving his incompleteness theorems, Kurt Gödel created an alternative to the symbols normally used in logic. He used Gödel numbers, which were numbers that represented operations with set numbers, and variables with the prime numbers greater than 10. With Gödel numbers, logic statements can be broken down into a number sequence. Gödel then took this one step farther, taking the \"n\" prime numbers and putting them to the power of the numbers in the sequence. These numbers were then multiplied together to get the final product, giving every logic statement its own number.\n\nAbstraction of notation is an ongoing process and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. Various set notations would be developed for fundamental object sets. Around 1924, David Hilbert and Richard Courant published \"Methods of mathematical physics. Partial differential equations\". In 1926, Oskar Klein and Walter Gordon proposed the Klein–Gordon equation to describe relativistic particles. The first formulation of a quantum theory describing radiation and matter interaction is due to Paul Adrien Maurice Dirac, who, during 1920, was first able to compute the coefficient of spontaneous emission of an atom. In 1928, the relativistic Dirac equation was formulated by Dirac to explain the behavior of the relativistically moving electron. Dirac described the quantification of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, and Werner Heisenberg, and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles.\n\nIn 1931, Alexandru Proca developed the Proca equation (Euler–Lagrange equation) for the vector meson theory of nuclear forces and the relativistic quantum field equations. John Archibald Wheeler in 1937 develops S-matrix. Studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.\n\nIn the 1930s, the double-struck capital Z for integer number sets was created by Edmund Landau. Nicolas Bourbaki created the double-struck capital Q for rational number sets. In 1935, Gerhard Gentzen made universal quantifiers. In 1936, Tarski's undefinability theorem is stated by Alfred Tarski and proved. In 1938, Gödel proposes the constructible universe in the paper \"The Consistency of the Axiom of Choice and of the Generalized Continuum-Hypothesis\". André Weil and Nicolas Bourbaki would develop the empty set sign in 1939. That same year, Nathan Jacobson would coin the double-struck capital C for complex number sets.\n\nAround the 1930s, Voigt notation would be developed for multilinear algebra as a way to represent a symmetric tensor by reducing its order. Schönflies notation became one of two conventions used to describe point groups (the other being Hermann–Mauguin notation). Also in this time, van der Waerden notation became popular for the usage of two-component spinors (Weyl spinors) in four spacetime dimensions. Arend Heyting would introduce Heyting algebra and Heyting arithmetic.\n\nThe arrow, e.g., →, was developed for function notation in 1936 by Øystein Ore to denote images of specific elements. Later, in 1940, it took its present form, e.g., \"f: X → Y\", through the work of Witold Hurewicz. Werner Heisenberg, in 1941, proposed the S-matrix theory of particle interactions.\nBra–ket notation (Dirac notation) is a standard notation for describing quantum states, composed of angle brackets and vertical bars. It can also be used to denote abstract vectors and linear functionals. It is so called because the inner product (or dot product on a complex vector space) of two states is denoted by a bra|ket consisting of a left part, ⟨\"φ\"|, and a right part, |\"ψ\"⟩. The notation was introduced in 1939 by Paul Dirac, though the notation has precursors in Grassmann's use of the notation [\"φ\"|\"ψ\"] for his inner products nearly 100 years previously.\n\nBra–ket notation is widespread in quantum mechanics: almost every phenomenon that is explained using quantum mechanics—including a large portion of modern physics—is usually explained with the help of bra–ket notation. The notation establishes an encoded abstract representation-independence, producing a versatile specific representation (e.g., \"x\", or \"p\", or eigenfunction base) without much , or excessive reliance on, the nature of the linear spaces involved. The overlap expression ⟨\"φ\"|\"ψ\"⟩ is typically interpreted as the probability amplitude for the state \"ψ\" to collapse into the state \"ϕ\". The Feynman slash notation (Dirac slash notation) was developed by Richard Feynman for the study of Dirac fields in quantum field theory.\n\nIn 1948, Valentine Bargmann and Eugene Wigner proposed the relativistic Bargmann–Wigner equations to describe free particles and the equations are in the form of multi-component spinor field wavefunctions. In 1950, William Vallance Douglas Hodge presented \"The topological invariants of algebraic varieties\" at the Proceedings of the International Congress of Mathematicians. Between 1954 and 1957, Eugenio Calabi worked on the Calabi conjecture for Kähler metrics and the development of Calabi–Yau manifolds. In 1957, Tullio Regge formulated the mathematical property of potential scattering in the Schrödinger equation. Stanley Mandelstam, along with Regge, did the initial development of the Regge theory of strong interaction phenomenology. In 1958, Murray Gell-Mann and Richard Feynman, along with George Sudarshan and Robert Marshak, deduced the chiral structures of the weak interaction in physics. Geoffrey Chew, along with others, would promote matrix notation for the strong interaction, and the associated bootstrap principle, in 1960. In the 1960s, set-builder notation was developed for describing a set by stating the properties that its members must satisfy. Also in the 1960s, tensors are abstracted within category theory by means of the concept of monoidal category. Later, multi-index notation eliminates conventional notions used in multivariable calculus, partial differential equations, and the theory of distributions, by abstracting the concept of an integer index to an ordered tuple of indices.\n\nIn the modern mathematics of special relativity, electromagnetism and wave theory, the d'Alembert operator is the Laplace operator of Minkowski space. The Levi-Civita symbol is used in tensor calculus.\n\nAfter the full Lorentz covariance formulations that were finite at any order in a perturbation series of quantum electrodynamics, Sin-Itiro Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel prize in physics in 1965. Their contributions, and those of Freeman Dyson, were about covariant and gauge invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Quantum electrodynamics has served as the model and template for subsequent quantum field theories. Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force. In the late 1960s, the particle zoo was composed of the then known elementary particles before the discovery of quarks.\n\nA step towards the Standard Model was Sheldon Glashow's discovery, in 1960, of a way to combine the electromagnetic and weak interactions. In 1967, Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form. The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions - i.e. the quarks and leptons. Also in 1967, Bryce DeWitt published his equation under the name \"\"Einstein–Schrödinger equation\" (later renamed the \"Wheeler–DeWitt equation\"\"). In 1969, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind described space and time in terms of strings. In 1970, Pierre Ramond develop two-dimensional supersymmetries. Michio Kaku and Keiji Kikkawa would afterwards formulate string variations. In 1972, Michael Artin, Alexandre Grothendieck, Jean-Louis Verdier propose the Grothendieck universe.\n\nAfter the neutral weak currents caused by boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74. With the establishment of quantum chromodynamics, a finalized a set of fundamental and exchange particles, which allowed for the establishment of a \"standard model\" based on the mathematics of gauge invariance, which successfully described all forces except for gravity, and which remains generally accepted within the domain to which it is designed to be applied. In the late 1970s, William Thurston introduced hyperbolic geometry into the study of knots with the hyperbolization theorem. The orbifold notation system, invented by Thurston, has been developed for representing types of symmetry groups in two-dimensional spaces of constant curvature. In 1978, Shing-Tung Yau deduced that the Calabi conjecture have Ricci flat metrics. In 1979, Daniel Friedan showed that the equations of motions of string theory are abstractions of Einstein equations of General Relativity.\n\nThe first superstring revolution is composed of mathematical equations developed between 1984 and 1986. In 1984, Vaughan Jones deduced the Jones polynomial and subsequent contributions from Edward Witten, Maxim Kontsevich, and others, revealed deep connections between knot theory and mathematical methods in statistical mechanics and quantum field theory. According to string theory, all particles in the \"particle zoo\" have a common ancestor, namely a vibrating string. In 1985, Philip Candelas, Gary Horowitz, Andrew Strominger, and Edward Witten would publish \"Vacuum configurations for superstrings\" Later, the tetrad formalism (tetrad index notation) would be introduced as an approach to general relativity that replaces the choice of a coordinate basis by the less restrictive choice of a local basis for the tangent bundle.\n\nIn the 1990s, Roger Penrose would propose Penrose graphical notation (tensor diagram notation) as a, usually handwritten, visual depiction of multilinear functions or tensors. Penrose would also introduce abstract index notation. In 1995, Edward Witten suggested M-theory and subsequently used it to explain some observed dualities, initiating the second superstring revolution.\nJohn Conway would further various notations, including the Conway chained arrow notation, the Conway notation of knot theory, and the Conway polyhedron notation. The Coxeter notation system classifies symmetry groups, describing the angles between with fundamental reflections of a Coxeter group. It uses a bracketed notation, with modifiers to indicate certain subgroups. The notation is named after H. S. M. Coxeter and Norman Johnson more comprehensively defined it.\n\nCombinatorial LCF notation has been developed for the representation of cubic graphs that are Hamiltonian. The cycle notation is the convention for writing down a permutation in terms of its constituent cycles. This is also called circular notation and the permutation called a \"cyclic\" or \"circular\" permutation.\n\nIn 1931, IBM produces the IBM 601 Multiplying Punch; it is an electromechanical machine that could read two numbers, up to 8 digits long, from a card and punch their product onto the same card. In 1934, Wallace Eckert used a rigged IBM 601 Multiplying Punch to automate the integration of differential equations. In 1936, Alan Turing publishes \"On Computable Numbers, With an Application to the Entscheidungsproblem\". John von Neumann, pioneer of the digital computer and of computer science, in 1945, writes the incomplete \"First Draft of a Report on the EDVAC\". In 1962, Kenneth E. Iverson developed an integral part notation that became known as Iverson Notation for manipulating arrays that he taught to his students, and described in his book \"A Programming Language\". In 1970, E.F. Codd proposed relational algebra as a relational model of data for database query languages. In 1971, Stephen Cook publishes \"The complexity of theorem proving procedures\" In the 1970s within computer architecture, Quote notation was developed for a representing number system of rational numbers. Also in this decade, the Z notation (just like the APL language, long before it) uses many non-ASCII symbols, the specification includes suggestions for rendering the Z notation symbols in ASCII and in LaTeX. There are presently various C mathematical functions (Math.h) and numerical libraries. They are libraries used in software development for performing numerical calculations. These calculations can be handled by symbolic executions; analyzing a program to determine what inputs cause each part of a program to execute. Mathematica and SymPy are examples of computational software programs based on symbolic mathematics.\n\nIn the history of mathematical notation, ideographic symbol notation has come full circle with the rise of computer visualization systems. The notations can be applied to abstract visualizations, such as for rendering some projections of a Calabi-Yau manifold. Examples of abstract visualization which properly belong to the mathematical imagination can be found in computer graphics. The need for such models abounds, for example, when the measures for the subject of study are actually random variables and not really ordinary mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "383424", "url": "https://en.wikipedia.org/wiki?curid=383424", "title": "Italian school of algebraic geometry", "text": "Italian school of algebraic geometry\n\nIn relation with the history of mathematics, the Italian school of algebraic geometry refers to the work over half a century or more (flourishing roughly 1885–1935) done internationally in birational geometry, particularly on algebraic surfaces. There were in the region of 30 to 40 leading mathematicians who made major contributions, about half of those being in fact Italian. The leadership fell to the group in Rome of Guido Castelnuovo, Federigo Enriques and Francesco Severi, who were involved in some of the deepest discoveries, as well as setting the style.\n\nThe emphasis on algebraic surfaces—algebraic varieties of dimension two—followed on from an essentially complete geometric theory of algebraic curves (dimension 1). The position in around 1870 was that the curve theory had incorporated with Brill–Noether theory the Riemann–Roch theorem in all its refinements (via the detailed geometry of the theta-divisor).\n\nThe classification of algebraic surfaces was a bold and successful attempt to repeat the division of curves by their genus \"g\". It corresponds to the rough classification into the three types: \"g\" = 0 (projective line); \"g\" = 1 (elliptic curve); and \"g\" > 1 (Riemann surfaces with independent holomorphic differentials). In the case of surfaces, the Enriques classification was into five similar big classes, with three of those being analogues of the curve cases, and two more (elliptic fibrations, and K3 surfaces, as they would now be called) being with the case of two-dimension abelian varieties in the 'middle' territory. This was an essentially sound, breakthrough set of insights, recovered in modern complex manifold language by Kunihiko Kodaira in the 1950s, and refined to include mod p phenomena by Zariski, the Shafarevich school and others by around 1960. The form of the Riemann–Roch theorem on a surface was also worked out.\n\nSome proofs produced by the school are not considered satisfactory because of foundational difficulties. These included frequent use of birational models in dimension three of surfaces that can have non-singular models only when embedded in higher-dimensional projective space. In order to avoid these issues, a sophisticated theory of handling a linear system of divisors was developed (in effect, a line bundle theory for hyperplane sections of putative embeddings in projective space). Many modern techniques were found, in embryonic form, and in some cases the articulation of these ideas exceeded the available technical language.\n\nAccording to Guerraggio & Nastasi (page 9, 2005) Luigi Cremona is \"considered the founder of the Italian school of algebraic geometry\". Later they explain that in Turin the collaboration of Enrico D'Ovidio and Corrado Segre \"would bring, either by their own efforts or those of their students, Italian algebraic geometry to full maturity\". A one-time student of Segre, H.F. Baker wrote (1926, page 269), [Corrado Segre] \"may probably be said to be the father of that wonderful Italian school which has achieved so much in the birational theory of algebraical loci.\" On this topic, Brigaglia & Ciliberto (2004) say \"Segre had headed and maintained the school of geometry that Luigi Cremona had established in 1860.\" Reference to the Mathematics Genealogy Project shows that, in terms of \"Italian doctorates\", the real productivity of the school began with Guido Castelnuovo and Federigo Enriques. In the USA Oscar Zariski inspired many Ph.D.s.\n\nThe roll of honour of the school includes the following other Italians: Giacomo Albanese, Eugenio Bertini, Luigi Campedelli, Oscar Chisini, Michele De Franchis, Pasquale del Pezzo, Beniamino Segre, Francesco Severi, Guido Zappa (with contributions also from Gino Fano, Carlo Rosati, Giuseppe Torelli, Giuseppe Veronese).\n\nElsewhere it involved H. F. Baker and Patrick du Val (UK), Arthur Byron Coble (USA), Georges Humbert and Charles Émile Picard (France), Lucien Godeaux (Belgium), Hermann Schubert and Max Noether, and later Erich Kähler (Germany), H. G. Zeuthen (Denmark).\n\nThese figures were all involved in algebraic geometry, rather than the pursuit of projective geometry as synthetic geometry, which during the period under discussion was a huge (in volume terms) but secondary subject (when judged by its importance as research).\n\nThe new algebraic geometry that would succeed the Italian school was distinguished also by the intensive use of algebraic topology. The founder of that tendency was Henri Poincaré; during the 1930s it was developed by Lefschetz, Hodge and Todd. The modern synthesis brought together their work, that of the Cartan school, and of W.L. Chow and Kunihiko Kodaira, with the traditional material.\n\nIn the earlier years of the Italian school under Castelnuovo, the standards of rigor were as high as most areas of mathematics. Under Enriques it gradually became acceptable to use somewhat more informal arguments instead of complete rigorous proofs, such as the \"principle of continuity\" saying that what is true up to the limit is true at the limit, a claim that had neither a rigorous proof nor even a precise statement. At first this did not matter too much, as Enriques's intuition was so good that essentially all the results he claimed were in fact correct, and using this more informal style of argument allowed him to produce spectacular results about algebraic surfaces. \nUnfortunately, from about 1930 onwards under Severi's leadership the standards of accuracy declined further, to the point where some of the claimed results were not just inadequately proved, but were hopelessly wrong. \nFor example, in 1934 Severi claimed that the space of rational equivalence classes of cycles on an algebraic surface is finite-dimensional, but showed that this is false for surfaces of positive geometric genus, and in 1946 Severi published a paper claiming to prove that a degree-6 surface in 3-dimensional projective space has at most 52 nodes, but the Barth sextic has 65 nodes.\nSeveri did not accept that his arguments were inadequate, leading to some acrimonious disputes as to the status of some results.\n\nBy about 1950 it had become too difficult to tell which of the results claimed were correct, and the informal intuitive school of algebraic geometry simply collapsed due to its inadequate foundations.\nFrom about 1950 to 1980 there was considerable effort to salvage as much as possible from the wreckage, and convert it into the rigorous algebraic style of algebraic geometry set up by Weil and Zariski. In particular in the 1960s Kodaira and Shafarevich and his students rewrote the Enriques classification of algebraic surfaces in a more rigorous style, and also extended it to all compact complex surfaces, while in the 1970s Fulton and MacPherson put the classical calculations of intersection theory on rigorous foundations.\n\n\n"}
{"id": "37302995", "url": "https://en.wikipedia.org/wiki?curid=37302995", "title": "Lehrbuch der Topologie", "text": "Lehrbuch der Topologie\n\nIn mathematics, Lehrbuch der Topologie (German for \"textbook of topology\") is a book by Herbert Seifert and William Threlfall, first published in 1934 and published in an English translation in 1980. It was one of the earliest textbooks on algebraic topology, and was the standard reference on this topic for many years.\n\nAlbert W. Tucker wrote a review.\n\n"}
{"id": "3314988", "url": "https://en.wikipedia.org/wiki?curid=3314988", "title": "List of cohomology theories", "text": "List of cohomology theories\n\nThis is a list of some of the ordinary and generalized (or extraordinary) homology and cohomology theories in algebraic topology that are defined on the categories of CW complexes or spectra. For other sorts of homology theories see the links at the end of this article.\n\n\nIf \"X\" is a spectrum, then it defines generalized homology and cohomology theories on the category of spectra as follows.\n\nThese are the theories satisfying the \"dimension axiom\" of the Eilenberg–Steenrod axioms that the homology of a point vanishes in dimension other than 0. They are determined by an abelian coefficient group \"G\", and denoted by H(\"X\", \"G\") (where\n\"G\" is sometimes omitted, especially if it is Z). Usually \"G\" is the integers, the rationals, the reals, the complex numbers, or the integers mod a prime \"p\".\n\nThe cohomology functors of ordinary cohomology theories are represented by Eilenberg–MacLane spaces.\n\nOn simplicial complexes, these theories coincide with singular homology and cohomology.\n\nSpectrum: H (Eilenberg–MacLane spectrum of the integers.)\n\nCoefficient ring: π(H) = Z if \"n\" = 0, 0 otherwise.\n\nThe original homology theory.\n\nSpectrum: HQ (Eilenberg–Mac Lane spectrum of the rationals.)\n\nCoefficient ring: π(HQ) = Q if \"n\" = 0, 0 otherwise.\n\nThese are the easiest of all homology theories.\nThe homology groups HQ(\"X\") are often denoted by H(\"X\", \"Q\").\nThe homology groups H(\"X\", Q), H(\"X\", R), H(\"X\", C) with rational, real, and complex coefficients are all similar, and are used mainly when torsion is not of interest (or too complicated to work out). The Hodge decomposition writes the complex cohomology of a complex projective variety as a sum of sheaf cohomology groups.\n\nSpectrum: HZ (Eilenberg–Maclane spectrum of the integers mod \"p\".)\n\nCoefficient ring: π(HZ) = Z (Integers mod \"p\") if \"n\" = 0, 0 otherwise.\n\nThe simpler K-theories of a space are often related to vector bundles over the space, and different sorts of K-theories correspond to different structures that can be put on a vector bundle.\n\nSpectrum: KO\n\nCoefficient ring: The coefficient groups π(KO) have period 8 in \"i\", given by the sequence Z, Z, Z,0, Z, 0, 0, 0, repeated. As a ring, it is generated by a class \"η\" in degree 1, a class \"x\" in degree 4, and an invertible class \"v\" in degree 8, subject to the relations that 2\"η\" = \"η\" = \"ηx\" = 0, and \"x\" = 4\"v\".\n\nKO(\"X\") is the ring of stable equivalence classes of real vector bundles over \"X\". Bott periodicity implies that the K-groups have period 8.\n\nSpectrum: KU (even terms BU or Z × BU, odd terms \"U\").\n\nCoefficient ring: The coefficient ring \"K\"(point) is the ring of Laurent polynomials in a generator of degree 2.\n\n\"K\"(\"X\") is the ring of stable equivalence classes of complex vector bundles over \"X\". Bott periodicity implies that the K-groups have period 2.\n\nSpectrum: KSp\n\nCoefficient ring: The coefficient groups π(KSp) have period 8 in \"i\", given by the sequence Z, 0, 0, 0,Z, Z, Z,0, repeated.\n\nKSp(\"X\") is the ring of stable equivalence classes of quaternionic vector bundles over \"X\". Bott periodicity implies that the K-groups have period 8.\n\nSpectrum: KG\n\n\"G\" is some abelian group; for example the localization Z at the prime \"p\". Other K-theories can also be given coefficients.\n\nSpectrum: KSC\n\nCoefficient ring: \"to be written...\"\n\nThe coefficient groups formula_1(KSC) have period 4 in \"i\", given by the sequence Z, Z, 0, Z, repeated. Introduced by Donald W. Anderson in his unpublished 1964 University of California, Berkeley Ph.D. dissertation, \"A new cohomology theory\".\n\nSpectrum: ku for connective K-theory, ko for connective real K-theory.\n\nCoefficient ring: For ku, the coefficient ring is the ring of polynomials over \"Z\" on a single class \"v\" in dimension 2. For ko, the coefficient ring is the quotient of a polynomial ring on three generators, \"η\" in dimension 1, \"x\" in dimension 4, and \"v\" in dimension 8, the periodicity generator, modulo the relations that 2\"η\" = 0, \"x\" = 4\"v\", \"η\" = 0, and \"ηx\" = 0.\n\nRoughly speaking, this is K-theory with the negative dimensional parts killed off.\n\nThis is a cohomology theory defined for spaces with involution, from which many of the other K-theories can be derived.\n\nCobordism studies manifolds, where a manifold is regarded as \"trivial\" if it is the boundary of another compact manifold. The cobordism classes of manifolds form a ring that is usually the coefficient ring of some generalized cohomology theory. There are many such theories, corresponding roughly to the different structures that one can put on a manifold.\n\nThe functors of cobordism theories are often represented by Thom spaces of certain groups.\n\nSpectrum: S (sphere spectrum).\n\nCoefficient ring: The coefficient groups π(\"S\") are the stable homotopy groups of spheres, which are notoriously hard to compute or understand for \"n\" > 0. (For \"n\" < 0 they vanish, and for \"n\" = 0 the group is Z.)\n\nStable homotopy is closely related to cobordism of framed manifolds (manifolds with a trivialization of the normal bundle).\n\nSpectrum: MO (Thom spectrum of orthogonal group)\n\nCoefficient ring: π(MO) is the ring of cobordism classes of unoriented manifolds, and is a polynomial ring over the field with 2 elements on generators of degree \"i\" for every \"i\" not of the form 2−1. That is: formula_2 where formula_3 can be represented by the classes of formula_4 while for odd indices one can use appropriate Dold manifolds.\n\nUnoriented bordism is 2-torsion, since \"2M\" is the boundary of formula_5.\n\nMO is a rather weak cobordism theory, as the spectrum MO is isomorphic to\nH(π(MO)) (\"homology with coefficients in π(MO)\") – MO is a product of Eilenberg–MacLane spectra. In other words, the corresponding homology and cohomology theories are no more powerful than homology and cohomology with coefficients in Z/2Z. This was the first cobordism theory to be described completely.\n\nSpectrum: MU (Thom spectrum of unitary group)\n\nCoefficient ring: π(\"MU\") is the polynomial ring on generators of degree 2, 4, 6, 8, ...\nand is naturally isomorphic to Lazard's universal ring, and is the cobordism ring of stably almost complex manifolds.\n\nSpectrum: MSO (Thom spectrum of special orthogonal group)\n\nCoefficient ring: The oriented cobordism class of a manifold is completely determined by its characteristic numbers: its Stiefel–Whitney numbers and Pontryagin numbers, but the overall coefficient ring, denoted formula_6 is quite complicated.\nRationally, and at 2 (corresponding to Pontryagin and Stiefel–Whitney classes, respectively), MSO is a product of Eilenberg–MacLane spectra – formula_7 and formula_8 – but at odd primes it is not, and the structure is complicated to describe. The ring has been completely described integrally, due to work of John Milnor, Boris Averbuch, Vladimir Rokhlin, and C. T. C. Wall.\n\nSpectrum: MSU (Thom spectrum of special unitary group)\n\nCoefficient ring:\n\nSpectrum: MSpin (Thom spectrum of spin group)\n\nCoefficient ring: See .\n\nSpectrum: MSp (Thom spectrum of symplectic group)\n\nCoefficient ring:\n\nSpectrum: MPL, MSPL, MTop, MSTop\n\nCoefficient ring:\n\nThe definition is similar to cobordism, except that one uses piecewise linear or topological instead of smooth manifolds, either oriented or unoriented.\nThe coefficient rings are complicated.\n\nSpectrum: BP\n\nCoefficient ring: π(BP) is a polynomial algebra over \"Z\" on generators \"v\" of dimension 2(\"p\" − 1) for \"n\" ≥ 1.\n\nBrown–Peterson cohomology BP is a summand of MU, which is complex cobordism MU localized at a prime \"p\". In fact MU is a sum of suspensions of BP.\n\nSpectrum: K(\"n\") (They also depend on a prime \"p\".)\n\nCoefficient ring: F[\"v\", \"v\"], where \"v\" has degree 2(\"p\" -1).\n\nThese theories have period 2(\"p\" − 1). They are named after Jack Morava.\n\nSpectrum \"E\"(\"n\")\n\nCoefficient ring Z[\"v\", ..., \"v\", 1/\"v\"] where \"v\" has degree 2(2−1)\n\nSpectrum:\n\nCoefficient ring:\n\nSpectrum: Ell\n\nSpectra: tmf, TMF (previously called eo.)\n\nThe coefficient ring π(tmf) is called the ring of topological modular forms. TMF is tmf with the 24th power of the modular form Δ inverted, and has period 24=576. At the prime \"p\" = 2, the completion of tmf is the spectrum eo, and the K(2)-localization of tmf is the Hopkins-Miller Higher Real K-theory spectrum EO.\n\n"}
{"id": "234998", "url": "https://en.wikipedia.org/wiki?curid=234998", "title": "List of integrals of hyperbolic functions", "text": "List of integrals of hyperbolic functions\n\nThe following is a list of integrals (anti-derivative functions) of hyperbolic functions. For a complete list of integral functions, see list of integrals.\n\nIn all formulas the constant \"a\" is assumed to be nonzero, and \"C\"\ndenotes the constant of integration.\n\nformula_1\n\nformula_2\n\nformula_3\n\nformula_5\n\nformula_9\n\nformula_10\n\nformula_11\n\nformula_12\n\nformula_13\n\nformula_14\n\nformula_16\n\nformula_18\n\nformula_19\n\nformula_20\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nformula_26\n\nformula_27\n\nformula_28\n\nformula_29\n\nformula_30\n\nformula_36\n\nformula_37\n\nformula_38\n\nformula_39\n"}
{"id": "2498388", "url": "https://en.wikipedia.org/wiki?curid=2498388", "title": "List of logic symbols", "text": "List of logic symbols\n\nIn logic, a set of symbols is commonly used to express logical representation. The following table lists many common symbols together with their name, pronunciation, and the related field of mathematics. Additionally, the third column contains an informal definition, the fourth column gives a short example, the fifth and sixth give the unicode location and name for use in HTML documents. The last column provides the LaTeX symbol.\n\nThese symbols are sorted by their Unicode value:\n\n\nNote that the following operators are rarely supported by natively installed fonts. If you wish to use these in a web page, you should always embed the necessary fonts so the page viewer can see the web page without having the necessary fonts installed in their computer.\n\n in Poland, the universal quantifier is sometimes written formula_4 and the existential quantifier as formula_5.\nThe same applies for Germany.\nThe ⇒ symbol is often used in text to mean \"result\" or \"conclusion\", as in \"We examined whether to sell the product ⇒ We will not sell it\". Also, the → symbol is often used to denote \"changed to\" as in the sentence \"The interest rate changed. March 20% → April 21%\".\n\n\n\n"}
{"id": "346167", "url": "https://en.wikipedia.org/wiki?curid=346167", "title": "List of mathematical logic topics", "text": "List of mathematical logic topics\n\nThis is a list of mathematical logic topics, by Wikipedia page.\n\nFor traditional syllogistic logic, see the list of topics in logic. See also the list of computability and complexity topics for more theory of algorithms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "43302756", "url": "https://en.wikipedia.org/wiki?curid=43302756", "title": "List of mathematical symbols by subject", "text": "List of mathematical symbols by subject\n\nThis list of mathematical symbols by subject shows a selection of the most common symbols that are used in modern mathematical notation within formulas, grouped by mathematical topic. As it is virtually impossible to list all the symbols ever used in mathematics, only those symbols which occur often in mathematics or mathematics education are included. Many of the characters are standardized, for example in DIN 1302 \"General mathematical symbols\" or DIN EN ISO 80000-2 \"Quantities and units – Part 2: Mathematical signs for science and technology\".\n\nThe following list is largely limited to non-alphanumeric characters. It is divided by areas of mathematics and grouped within sub-regions. Some symbols have a different meaning depending on the context and appear accordingly several times in the list. Further information on the symbols and their meaning can be found in the respective linked articles.\n\nThe following information is provided for each mathematical symbol:\n\n\n\"Note:\" The symbols formula_1 and formula_2 are used inconsistently and often do not exclude the equality of the two quantities.\n\n\"Note:\" the power function is not represented by its own icon, but by the positioning of the exponent as a superscript.\n\n\"See also:\" mathematical constant for symbols of additional mathematical constants.\n\n\n\n\"Note: This article is a translation of the German Wikipedia article .\"\n"}
{"id": "5971835", "url": "https://en.wikipedia.org/wiki?curid=5971835", "title": "List of mathematicians (U)", "text": "List of mathematicians (U)\n\n\n"}
{"id": "534959", "url": "https://en.wikipedia.org/wiki?curid=534959", "title": "List of multivariable calculus topics", "text": "List of multivariable calculus topics\n\nThis is a list of multivariable calculus topics. See also multivariable calculus, vector calculus, list of real analysis topics, list of calculus topics. \n\n"}
{"id": "13186787", "url": "https://en.wikipedia.org/wiki?curid=13186787", "title": "List of price index formulas", "text": "List of price index formulas\n\nA number of different formulae, more than hundred, have been proposed as means of calculating price indexes. While price index formulae all use price and possibly quantity data, they aggregate these in different ways. A price index aggregates various combinations of base period prices (formula_1), later period prices (formula_2), base period quantities (formula_3), and later period quantities (formula_4). Price index numbers are usually defined either in terms of (actual or hypothetical) expenditures (expenditure = price * quantity) or as different weighted averages of price relatives (formula_5). These tell the relative change of the price in question. Two of the most commonly used price index formulae were defined by German economists and statisticians Étienne Laspeyres and Hermann Paasche, both around 1875 when investigating price changes in Germany.\n\nDeveloped in 1871 by Laspeyres, the formula:\n\ncompares the total cost of the same basket of goods formula_3 at the old and new prices.\n\nDeveloped in 1874 by Paasche, the formula:\n\ncompares the total cost of a new basket of goods formula_4 at the old and new prices.\n\nThe geometric means index:\n\nincorporates quantity information through the share of expenditure in the base period.\n\nUnweighted, or \"elementary\", price indices only compare prices of a single type of good between two periods. They do not make any use of quantities or expenditure weights. They are called \"elementary\" because they are often used at the lower levels of aggregation for more comprehensive price indices. In such a case, they are not indices but merely an intermediate stage in the calculation of an index. At these lower levels, it is argued that weighting is not necessary since only one type of good is being aggregated. However this implicitly assumes that only one type of the good is available (e.g. only one brand and one package size of frozen peas) and that it has not changed in quality etc between time periods.\n\nDeveloped in 1764 by Carli, an Italian economist, this formula is the arithmetic mean of the price relative between a period \"t\" and a base period \"0\".\n\nOn 17 August 2012 the BBC Radio 4 program \"More or Less\" noted that the Carli index, used in part in the British Retail Price Index measure, has a built-in bias towards recording inflation even when over successive periods there is no increase in prices overall.\n\nIn 1738 French economist Dutot proposed using an index calculated by dividing the average price in period \"t\" by the average price in period \"0\".\n\nIn 1863, English economist Jevons proposed taking the geometric average of the price relative of period \"t\" and base period \"0\". When used as an elementary aggregate, the Jevons index is considered a constant elasticity of substitution index since it allows for product substitution between time periods.\n\nThis is the formula that was used for the old Financial Times stock market index (the predecessor of the FTSE 100 Index). It was inadequate for that purpose. In particular, if the price of any of the constituents were to fall to zero, the whole index would fall to zero. That is an extreme case; in general the formula will understate the total cost of a basket of goods (or of any subset of that basket) unless their prices all change at the same rate. Also, as the index is unweighted, large price changes in selected constituents can transmit to the index to an extent not representing their importance in the average portfolio.\n\nThe harmonic average counterpart to the Carli index. The index was proposed by Jevons in 1865 and by Coggeshall in 1887.\n\nIs the geometric mean of the Carli and the harmonic price indexes. In 1922 Fisher wrote that this and the Jevons were the two best unweighted indexes based on Fisher's test approach to index number theory.\n\nThe ratio of harmonic means or \"Harmonic means\" price index is the harmonic average counterpart to the Dutot index.\n\nThe Marshall-Edgeworth index, credited to Marshall (1887) and Edgeworth (1925), is a weighted relative of current period to base period sets of prices. This index uses the arithmetic average of the current and based period quantities for weighting. It is considered a pseudo-superlative formula and is symmetric. The use of the Marshall-Edgeworth index can be problematic in cases such as a comparison of the price level of a large country to a small one. In such instances, the set of quantities of the large country will overwhelm those of the small one.\n\nSuperlative indices treat prices and quantities equally across periods. They are symmetrical and provide close approximations of cost of living indices and other theoretical indices used to provide guidelines for constructing price indices. All superlative indices produce similar results and are generally the favored formulas for calculating price indices. A superlative index is defined technically as \"an index that is exact for a flexible functional form that can provide a second-order approximation to other twice-differentiable functions around the same point.\"\n\nThe change in a Fisher index from one period to the next is the geometric mean of the changes in Laspeyres's and Paasche's indexes between those periods, and these are chained together to make comparisons over many periods:\n\nThis is also called Fisher's \"ideal\" price index.\n\nThe Törnqvist or Törnqvist-Theil index is the geometric average of the n price relatives of the current to base period prices (for n goods) weighted by the arithmetic average of the value shares for the two periods.\n\nThe Walsh price index is the weighted sum of the current period prices divided by the weighted sum of the base period prices with the geometric average of both period quantities serving as the weighting mechanism:\n\n"}
{"id": "1606990", "url": "https://en.wikipedia.org/wiki?curid=1606990", "title": "Logical harmony", "text": "Logical harmony\n\nLogical harmony, a name coined by Michael Dummett, is a supposed constraint on the rules of inference that can be used in a given logical system.\n\nThe logician Gerhard Gentzen proposed that the meanings of logical connectives could be given by the rules for introducing them into discourse. For example, if one believes that \"the sky is blue\" and one also believes that \"grass is green\", then one can introduce the connective \"and\" as follows: \"The sky is blue AND grass is green.\" Gentzen's idea was that having rules like this is what gives meaning to one's words, or at least to certain words. The idea has also been associated with Wittgenstein's dictum that in many cases we can say, \"meaning is use\". Most contemporary logicians prefer to think that the introduction rules and the elimination rules for an expression are equally important. In this case, \"and\" is characterized by the following rules:\nAn apparent problem with this was pointed out by Arthur Prior: Why can't we have an expression (call it \"tonk\") whose introduction rule is that of OR (from \"p\" to \"p tonk q\") but whose elimination rule is that of AND (from \"p tonk q\" to \"q\")? This lets us deduce anything at all from any starting point. Prior suggested that this meant that inferential rules could \"not\" determine meaning. He was answered by Nuel Belnap, that even though introduction and elimination rules can constitute meaning, not just any pair of such rules will determine a meaningful expression—they must meet certain constraints, such as not allowing us to deduce any new truths in the old vocabulary. These constraints are what Dummett was referring to.\n\nHarmony, then, refers to certain constraints that a proof theory must let hold between introduction and elimination rules for it to be meaningful, or in other words, for its inference rules to be meaning-constituting.\n\nThe application of harmony to logic may be considered a special case; it makes sense to talk of harmony with respect to not only inferential systems, but also conceptual systems in human cognition, and to type systems in programming languages.\n\nSemantics of this form has not provided a very great challenge to that sketched in Tarski's semantic theory of truth, but many philosophers interested in reconstituting the semantics of logic in a way that respects Ludwig Wittgenstein's \"meaning is use\" have felt that harmony holds the key.\n\n\n"}
{"id": "31501520", "url": "https://en.wikipedia.org/wiki?curid=31501520", "title": "Makespan", "text": "Makespan\n\nIn operations research, the makespan of a project is the distance in time that elapses from the start of work to the end. This type of multi-mode resource constrained project scheduling problem (MRCPSP) seeks to create the shortest logical project schedule, by efficiently using project resources, adding the lowest number of additional resources as possible to achieve the minimum makespan. The term commonly appears in the context of scheduling. There is a complex project that is composed of several sub-tasks. We would like to assign tasks to workers, such that the project finishes in the shortest possible time.\n\nAs an example, suppose the \"project\" is to feed the goats. There are three goats to feed, one child can only feed one goat at a time, and there are two children that can feed them: Shmuel feeds each goat in 10 minutes and Shifra feeds each goat in 12 minutes. Several schedules are possible:\nSo in this case, the second schedule attains the shortest makespan, which is 20.\n\n\n\"A solution procedure for preemptive multi-mode project scheduling problem with mode changeability to resumption\", Afshar-Nadjafi, B, \"in Applied Computing and Informatics (2014)\"\n"}
{"id": "47304362", "url": "https://en.wikipedia.org/wiki?curid=47304362", "title": "Mathematical sculpture", "text": "Mathematical sculpture\n\nA mathematical sculpture is a sculpture which uses mathematics as an essential conception. Helaman Ferguson, George W. Hart, Bathsheba Grossman, Peter Forakis and Jacobus Verhoeff are well-known mathematical sculptors.\n"}
{"id": "3667560", "url": "https://en.wikipedia.org/wiki?curid=3667560", "title": "Millennium Mathematics Project", "text": "Millennium Mathematics Project\n\nThe Millennium Mathematics Project (MMP) was set up within the University of Cambridge in England as a joint project between the Faculties of Mathematics and Education in 1999. The MMP aims to support maths education for pupils of all abilities from ages 5 to 19 and promote the development of mathematical skills and understanding, particularly through enrichment and extension activities beyond the school curriculum, and to enhance the mathematical understanding of the general public. The project is currently directed by John Barrow.\n\nThe MMP includes a range of complementary programmes:\n\n\nThe project has also developed a Hands On Maths Roadshow presenting creative methods of exploring mathematics, and in 2004 took on the running of Simon Singh's Enigma schools workshops, exploring maths through cryptography and codebreaking. Both are taken to primary and secondary schools and public venues such as shopping centres across the UK and Ireland. James Grime is the Enigma Project Officer and gives talks in schools and to the general public about the history and mathematics of code breaking - including the demonstration of a genuine World War II Enigma Machine.\n\nIn November 2005, the MMP won the Queen's Anniversary Prize for Higher and Further Education.\n"}
{"id": "39688248", "url": "https://en.wikipedia.org/wiki?curid=39688248", "title": "N-topological space", "text": "N-topological space\n\nIn mathematics, an \"N\"-topological space is a set equipped with \"N\" arbitrary topologies. If \"τ\", \"τ\", ..., \"τ\" are \"N\" topologies defined on a nonempty set X, then the \"N\"-topological space is denoted by (\"X\",\"τ\",\"τ\"...,\"τ\").\nFor \"N\" = 1, the structure is simply a topological space.\nFor \"N\" = 2, the structure becomes a bitopological space introduced by J. C. Kelly.\n\nLet \"X\" = {\"x\", \"x\", ..., \"x\"} be any finite set. Suppose \"A\" = {\"x\", \"x\", ..., \"x\"}. Then the collection \"τ\" = {\"φ\", \"A\", \"A\", ..., \"A\" = \"X\"} will be a topology on \"X\". If \"τ\", \"τ\", ..., \"τ\" be \"m\" such topologies (chain topologies) defined on \"X\", then the structure (\"X\", \"τ\", \"τ\", ..., \"τ\") is an \"m\"-topological space.\n"}
{"id": "1297317", "url": "https://en.wikipedia.org/wiki?curid=1297317", "title": "No free lunch theorem", "text": "No free lunch theorem\n\nIn mathematical folklore, the \"no free lunch\" (NFL) theorem (sometimes pluralized) of David Wolpert and William Macready appears in the 1997 \"No Free Lunch Theorems for Optimization\". Wolpert had previously derived no free lunch theorems for machine learning (statistical inference).\n\nIn 2005, Wolpert and Macready themselves indicated that the first theorem in their paper \"state[s] that any two optimization algorithms are equivalent when their performance is averaged across all possible problems\". The 1997 theorems of Wolpert and Macready are mathematically technical.\n\nThe folkloric \"no free lunch\" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them. Various investigators have extended the work of Wolpert and Macready substantively. See No free lunch in search and optimization for treatment of the research area.\n\nWhile some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.\n\nTo find the highest point on Earth, Alice uses a steepest-ascent local search which restarts when a local peak is fully climbed. Bob uses a steepest-descent local search which restarts when it bottoms out at a local trough. In our Earth, Alice will find the highest point much faster than Bob will. However, in some ensembles, each world such as ours can be paired 1-to-1 with a hypothetical \"Bobworld\" which is identical to ours, except that the elevation of peak of Mount Everest and of the lowest point in the Marianas Trench are swapped, as if a tall pole has been stuck in the Trench. In Bobworld, Bob's strategy of descending outperforms Alice's strategy of climbing; in fact, given further 1-to-1 pairing assumptions that are reasonable in a universe of bounded size, neither Alice's strategy nor Bob's strategy performs better on average, in the absence of some systematic natural bias towards worlds like Earth and away from worlds like Bobworld.\n\nWolpert and Macready give two NFL theorems that are closely related to the folkloric theorem. In their paper, they state:\n\nThe first theorem hypothesizes objective functions that do not change while optimization is in progress, and the second hypothesizes objective functions that may change.\n\nwhere formula_1 denotes the ordered set of size formula_2 of the cost values formula_3 associated to input values formula_4, formula_5 is the function being optimized and formula_6 is the conditional probability of obtaining a given sequence of cost values from algorithm formula_7 run formula_2 times on function formula_9.\n\nThe theorem can be equivalently formulated as follows:\n\nHere, \"blind search\" means that at each step of the algorithm, the element formula_10 is chosen at random with uniform probability distribution from the elements of formula_11 that have not been chosen previously.\n\nIn essence, this says that when all functions \"f\" are equally likely, the probability of observing an arbitrary sequence of \"m\" values in the course of optimization does not depend upon the algorithm. In the analytic framework of Wolpert and Macready, performance is a function of the sequence of observed values (and not e.g. of wall-clock time), so it follows easily that all algorithms have identically distributed performance when objective functions are drawn uniformly at random, and also that all algorithms have identical mean performance. But identical mean performance of all algorithms does not imply Theorem 1, and thus the folkloric theorem is not equivalent to the original theorem.\n\nTheorem 2 establishes a similar, but \"more subtle\", NFL result for time-varying objective functions.\n\nThe NFL theorems were explicitly \"not\" motivated by the question of what can be inferred (in the case of NFL for machine learning) or found (in the case of NFL for search) when the \"environment is uniform random\". Rather uniform randomness was used as a tool, to compare the number of environments for which algorithm A outperforms algorithm B to the number of environments for which B outperforms A. NFL tells us that (appropriately weighted) there are just as many environments in both of those sets.\n\nThis is true for many definitions of what precisely an \"environment\" is. In particular, there are just as many prior distributions (appropriately weighted) in which learning algorithm A beats B (on average) as vice versa. This statement about \"sets of priors\" is what is most important about NFL, not the fact that any two algorithms perform equally for the single, specific prior distribution that assigns equal probability to all environments.\n\nWhile the NFL is important to understand the fundamental limitation for a set of problems, it does not state anything about each particular instance of a problem that can arise in practice. That is, the NFL states what the NFL states in the mathematical statements and it is nothing more than that. For example, it applies to the situations where the algorithm is fixed first and a nature can choose a worst problem instance to each fixed algorithm. Therefore, if we have a \"good\" problem in practice or if we can choose a \"good\" learning algorithm for a given particular problem instance, then the NFL does not mention any limitation about this particular problem instance. See for example. To understand the results of the NFL along with \"seemingly\" contradicting results from other papers, it is important to actually understand the mathematical logic of the NFL instead of intuitive notation of the NFL. All results including the NFL and are indeed consistent.\n\nTo illustrate one of the counter-intuitive implications of NFL, suppose we fix two supervised learning algorithms, C and D. We then sample a target function f to produce a set of input-output pairs, \"d\". How should we choose whether to train C or D on \"d\", in order to make predictions for what output would be associated with a point lying outside of \"d?\"\n\nIt is common in almost of all science and statistics to answer this question - to choose between C and D - by running cross-validation on \"d\" with those two algorithms. In other words, to decide whether to generalize from \"d\" with either C or D\",\" we see which of them has better out-of-sample performance when tested within \"d\".\n\nNote that since C and D are fixed, this use of cross-validation to choose between them is itself an algorithm, i.e., a way of generalizing from an arbitrary dataset. Call this algorithm A. (Arguably, A is a simplified model of the scientific method itself.)\n\nNote as well though that we could also use \"anti\"-cross-validation to make our choice. In other words, we could choose between C and D based on which has \"worse\" out-of-sample performance within \"d\". Again, since C and D are fixed, this use of anti-cross-validation is itself an algorithm. Call that algorithm B.\n\nNFL tells us (loosely speaking) that B must beat A on just as many target functions (and associated datasets \"d\") as A beats B. In this very specific sense, the scientific method will lose to the \"anti\" scientific method just as readily as it wins.\n\nHowever, note that NFL only applies if the target function is chosen from a uniform distribution of all possible functions. If this is not the case, and certain target functions are more likely to be chosen than others, then A may perform better than B overall. The contribution of NFL is that it tells us choosing an appropriate algorithm requires making assumptions about the kinds of target functions the algorithm is being used for. With no assumptions, no \"meta-algorithm\", such as the scientific method, performs better than random choice.\n\nWhile some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research. If Occam's razor is correct, for example if sequences of lower Kolmogorov complexity are more probable than sequences of higher complexity, then (as is observed in real life) some algorithms, such as cross-validation, perform better on average on practical problems (when compared with random choice or with anti-cross-validation).\n\n"}
{"id": "40360489", "url": "https://en.wikipedia.org/wiki?curid=40360489", "title": "Oliver Sin", "text": "Oliver Sin\n\nOliver Sin (born 18 May 1985) is a Hungarian artist.\n\nBorn in Budapest, Hungary, Oliver Sin was raised with his sister Judit by his father, Zoltan Sin, a ropemaker. He became interested in art and science from an early age. He studied visual art from 2003 to 2009 at Dunakeszi's Miklós Radnóti Gymnasium. After graduation Sin enrolled in Szombathely's University of West Hungary where he majored in Visual Arts and Croatian.\n\nSin' work first came to international attention after his collaboration with Fibenare Guitars Co., when they made the Fibenare - Oliver Sin Collaboration Guitar in 2012. He made the cover of \"Guitar Connoisseur Magazine\" (in New York, USA) in 2013.\n\n\n\n\n\n\nSin's work is in private and public collections such as MODESSQE (Poland).\n\n\n"}
{"id": "4921531", "url": "https://en.wikipedia.org/wiki?curid=4921531", "title": "Patlak plot", "text": "Patlak plot\n\nA Patlak plot (sometimes called Gjedde–Patlak plot, Patlak–Rutland plot, or Patlak analysis) is a graphical analysis technique based on the compartment model that uses linear regression to identify and analyze pharmacokinetics of tracers involving irreversible uptake, such as in the case of deoxyglucose. It is used for the evaluation of nuclear medicine imaging data after the injection of a radioopaque or radioactive tracer.\n\nThe method is model-independent because it does not depend on any specific compartmental model configuration for the tracer, and the minimal assumption is that the behavior of the tracer can be approximated by two compartments – a \"central\" (or reversible) compartment that is in rapid equilibrium with plasma, and a \"peripheral\" (or irreversible) compartment, where tracer enters without ever leaving during the time of the measurements. The amount of tracer in the region of interest is accumulating according to the equation:\n\nwhere formula_2 represents time after tracer injection, formula_3 is the amount of tracer in region of interest, formula_4 is the concentration of tracer in plasma or blood, formula_5 is the clearance determining the rate of entry into the peripheral (irreversible) compartment, and formula_6 is the distribution volume of the tracer in the central compartment. The first term of the right-hand side represents tracer in the peripheral compartment, and the second term tracer in the central compartment.\n\nBy dividing both sides by formula_4, one obtains:\n\nThe unknown constants formula_5 and formula_6 can be obtained by linear regression from a graph of formula_11 against formula_12.\n\n\n"}
{"id": "31228704", "url": "https://en.wikipedia.org/wiki?curid=31228704", "title": "Prime factor exponent notation", "text": "Prime factor exponent notation\n\nIn his 1557 work \"The Whetstone of Witte\", British mathematician Robert Recorde proposed an exponent notation by prime factorisation, which remained in use up until the eighteenth century and acquired the name \"Arabic exponent notation\". The principle of Arabic exponents was quite similar to Egyptian fractions; large exponents were broken down into smaller prime numbers. Squares and cubes were so called; prime numbers from five onwards were called \"sursolids\".\n\nAlthough the terms used for defining exponents differed between authors and times, the general system was the primary exponent notation until René Descartes devised the Cartesian exponent notation, which is still used today.\n\nThis is a list of Recorde's terms.\n\nBy comparison, here is a table of prime factors:\n\n\n"}
{"id": "9142932", "url": "https://en.wikipedia.org/wiki?curid=9142932", "title": "Proof sketch for Gödel's first incompleteness theorem", "text": "Proof sketch for Gödel's first incompleteness theorem\n\nThis article gives a sketch of a proof of Gödel's first incompleteness theorem. This theorem applies to any formal theory that satisfies certain technical hypotheses, which are discussed as needed during the sketch. We will assume for the remainder of the article that a fixed theory satisfying these hypotheses has been selected.\n\nThroughout this article the word \"number\" refers to a natural number. The key property these numbers possess is that any natural number can be obtained by starting with the number 0 and adding 1 a finite number of times.\n\nGödel's theorem applies to any formal theory that satisfies certain properties. Each formal theory has a signature that specifies the nonlogical symbols in the language of the theory. For simplicity, we will assume that the language of the theory is composed from the following collection of 15 (and only 15) symbols:\n\n\nThis is the language of Peano arithmetic. A well-formed formula is a sequence of these symbols that is formed so as to have a well-defined reading as a mathematical formula. Thus is well formed while is not well formed. A theory is a set of well-formed formulas with no free variables.\n\nA theory is consistent if there is no formula such that both and its negation are provable. ω-consistency is a stronger property than consistency. Suppose that is a formula with one free variable . In order to be ω-consistent, the theory cannot prove both while also proving for each natural number .\n\nThe theory is assumed to be effective, which means that the set of axioms must be recursively enumerable. This means that it is theoretically possible to write a finite-length computer program that, if allowed to run forever, would output the axioms of the theory (necessarily including every well-formed instance of the axiom schema of induction) one at a time and not output anything else. This requirement is necessary; there are theories that are complete, consistent, and include elementary arithmetic, but no such theory can be effective.\n\nThe sketch here is broken into three parts. In the first part, each formula of the theory is assigned a number, known as a Gödel number, in a manner that allows the formula to be effectively recovered from the number. This numbering is extended to cover finite sequences of formulas. In the second part, a specific formula is constructed such that for any two numbers and holds if and only if represents a sequence of formulas that constitutes a proof of the formula that represents. In the third part of the proof, we construct a self-referential formula that, informally, says \"I am not provable\", and prove that this sentence is neither provable nor disprovable within the theory.\n\nImportantly, all the formulas in the proof can be defined by primitive recursive functions, which themselves can be defined in first-order Peano arithmetic.\n\nThe first step of the proof is to represent (well-formed) formulas of the theory, and finite lists of these formulas, as natural numbers. These numbers are called the Gödel numbers of the formulas.\n\nBegin by assigning a natural number to each symbol of the language of arithmetic, similar to the manner in which the ASCII code assigns a unique binary number to each letter and certain other characters. This article will employ the following assignment, very similar to the one Douglas Hofstadter used in his \"Gödel, Escher, Bach\":\n\nThe Gödel number of a formula is obtained by concatenating the Gödel numbers of each symbol making up the formula. The Gödel numbers for each symbol are separated by a zero because by design, no Gödel number of a symbol includes a . Hence any formula may be correctly recovered from its Gödel number. Let denote the Gödel number of the formula .\n\nGiven the above Gödel numbering, the sentence asserting that addition commutes, translates as the number:\n\n(Spaces have been inserted on each side of every 0 only for readability; Gödel numbers are strict concatenations of decimal digits.) Not all natural numbers represent a formula. For example, the number\n\ntranslates to \"\", which is not well-formed.\n\nBecause each natural number can be obtained by applying the successor operation to a finite number of times, every natural number has its own Gödel number. For example, the Gödel number corresponding to , is:\n\nThe assignment of Gödel numbers can be extended to finite lists of formulas. To obtain the Gödel number of a list of formulas, write the Gödel numbers of the formulas in order, separating them by two consecutive zeros. Since the Gödel number of a formula never contains two consecutive zeros, each formula in a list of formulas can be effectively recovered from the Gödel number for the list.\n\nIt is crucial that the formal arithmetic be capable of proving a minimum set of facts. In particular, it must be able to prove that every number has a Gödel number. A second fact that the theory must prove is that given any Gödel number of a formula with one free variable and any number , there is a Gödel number of the formula obtained by replacing all occurrences of in with , and that this second Gödel number can be effectively obtained from the Gödel number of as a function of . To see that this is in fact possible, note that given the Gödel number for , one can recreate the original formula, make the substitution, and then find the Gödel number of the resulting formula. This is a uniform procedure.\n\nDeduction rules can then be represented by binary relations on Gödel numbers of lists of formulas. In other words, suppose that there is a deduction rule , by which one can move from the formulas to a new formula . Then the relation corresponding to this deduction rule says that is related to (in other words, holds) if is the Gödel number of a list of formulas containing and and is the Gödel number of the list of formulas consisting of those in the list coded by together with . Because each deduction rule is concrete, it is possible to effectively determine for any natural numbers and whether they are related by the relation.\n\nThe second stage in the proof is to use the Gödel numbering, described above, to show that the notion of provability can be expressed within the formal language of the theory. Suppose the theory has deduction rules: . Let be their corresponding relations, as described above.\n\nEvery provable statement is either an axiom itself, or it can be deduced from the axioms by a finite number of applications of the deduction rules. We wish to define a set of numbers that represents all these provable statements. We define as the minimal set consisting of all numbers in (representing axioms) and closed under all the relations . This means that whenever is in the set and for some numbers and , the number is also in the set . It is not hard to see that represents the set of provable statements. That is, the members of are the Gödel numbers of the provable statements.\nA proof of a formula is itself a string of mathematical statements related by particular relations (each is either an axiom or related to former statements by deduction rules), where the last statement is . Thus one can define the Gödel number of a proof. Moreover, one may define a statement form , which for every two numbers and is provable if and only if is the Gödel number of a proof of the statement and .\n\nThe detailed construction of the formula makes essential use of the assumption that the theory is effective; it would not be possible to construct this formula without such an assumption.\n\nFor every number and every formula , where is a free variable, we define , a relation between two numbers and , such that it corresponds to the statement \" is not the Gödel number of a proof of \". Here, can be understood as with its own Gödel number as its argument.\n\nNote that takes as an argument , the Gödel number of . In order to prove either , or , it is necessary to perform number-theoretic operations on that mirror the following steps: decode the number into the formula , replace all occurrences of in with the number , and then compute the Gödel number of the resulting formula .\n\nNote that for every specific number and formula is a straightforward (though complicated) arithmetical relation between two numbers and , building on the relation defined earlier. Further, is provable if the finite list of formulas encoded by is not a proof of , and is provable if the finite list of formulas encoded by is a proof of . Given any numbers and , either or (but not both) is provable.\n\nAny proof of can be encoded by a Gödel number , such that does not hold. If holds for all natural numbers , then there is no proof of . In other words, , a formula about natural numbers, corresponds to \"there is no proof of \".\n\nWe now define the formula , where is a free variable. The formula itself has a Gödel number as does every formula.\n\nThis formula has a free variable . Suppose we replace it with ,\nthe Gödel number of a formula , where is a free variable. Then, corresponds to \"there is no proof of \", as we have seen.\n\nConsider the formula . This formula concerning the number corresponds to \"there is no proof of \". We have here the self-referential feature that is crucial to the proof: A formula of the formal theory that somehow relates to its own provability within that formal theory. Very informally, says: \"I am not provable\".\n\nWe will now show that neither the formula , nor its negation , is provable.\n\nSuppose is provable. Let be the Gödel number of a proof of . Then, as seen earlier, the formula is provable. Proving both and violates the consistency of the formal theory. We therefore conclude that is not provable.\n\nConsider any number . Suppose is provable.\nThen, must be the Gödel number of a proof of . But we have just proved that is not provable. Since either or must be provable, we conclude that, for all natural numbers is provable.\n\nSuppose the negation of , , is provable. Proving both , and , for all natural numbers , violates ω-consistency of the formal theory. Thus if the theory is ω-consistent, is not provable.\n\nWe have sketched a proof showing that:\n\nFor any formal, recursively enumerable (i.e. effectively generated) theory of Peano Arithmetic,\n\nThe proof of Gödel's incompleteness theorem just sketched is proof-theoretic (also called syntactic) in that it shows that if certain proofs exist (a proof of or its negation) then they can be manipulated to produce a proof of a contradiction. This makes no appeal to whether is \"true\", only to whether it is provable. Truth is a model-theoretic, or semantic, concept, and is not equivalent to provability except in special cases.\n\nBy analyzing the situation of the above proof in more detail, it is possible to obtain a conclusion about the truth of in the standard model ℕ of natural numbers. As just seen, is provable for each natural number , and is thus true in the model ℕ. Therefore, within this model,\n\nholds. This is what the statement \" is true\" usually refers to—the sentence is true in the intended model. It is not true in every model, however: If it were, then by Gödel's completeness theorem it would be provable, which we have just seen is not the case.\n\nGeorge Boolos (1989) vastly simplified the proof of the First Theorem, if one agrees that the theorem is equivalent to:\n\n\"There is no algorithm whose output contains all true sentences of arithmetic and no false ones.\"\n\n\"Arithmetic\" refers to Peano or Robinson arithmetic, but the proof invokes no specifics of either, tacitly assuming that these systems allow '<' and '×' to have their usual meanings. Boolos proves the theorem in about two pages. His proof employs the language of first-order logic, but invokes no facts about the connectives or quantifiers. The domain of discourse is the natural numbers. The Gödel sentence builds on Berry's paradox.\n\nLet abbreviate successive applications of the successor function, starting from . Boolos then asserts (the details are only sketched) that there exists a defined predicate that comes out true iff an arithmetic formula containing symbols names the number . This proof sketch contains the only mention of Gödel numbering; Boolos merely assumes that every formula can be so numbered. Here, a formula \"names\" the number iff the following is provable:\n\nBoolos then defines the related predicates:\n\n\nThe above predicates contain the only existential quantifiers appearing in the entire proof. The '<' and '×' appearing in these predicates are the only defined arithmetical notions the proof requires. The proof nowhere mentions recursive functions or any facts from number theory, and Boolos claims that his proof dispenses with diagonalization. For more on this proof, see Berry's paradox.\n\n\n"}
{"id": "929709", "url": "https://en.wikipedia.org/wiki?curid=929709", "title": "Pseudomathematics", "text": "Pseudomathematics\n\nPseudomathematics or mathematical crankery is a form of mathematics-like activity that does not work within the framework, definitions, rules, or rigor of formal mathematical practice. Pseudomathematics has equivalents in other scientific fields, such as pseudophysics, and overlaps with these to some extent.\n\nExcessive pursuit of pseudomathematics can result in the practitioner being labelled a crank. The topic of mathematical crankery has been extensively studied by mathematician Underwood Dudley, who has written several popular works about mathematical cranks and their ideas. Because it is based on non-mathematical principles, pseudomathematics is not related to attempts at genuine proofs that contain mistakes. Indeed, such mistakes are common in the careers of amateur mathematicians who go on to produce celebrated results.\n\nOne common type of approach is claiming to have solved classical problems in terms that have been proven mathematically impossible. Common examples include the following constructions in Euclidean geometry using only compass and straightedge:\n\nFor more than 2,000 years, many people had tried and failed to find such constructions; in the 19th century, they were all proven impossible.\n\nAnother common approach is to misapprehend standard mathematical methods, and insisting that the use or knowledge of higher mathematics is somehow cheating or misleading.\n\nThe term \"pseudomath\" was coined by the logician Augustus De Morgan, discoverer of De Morgan's laws, in his \"A Budget of Paradoxes\" (1915). De Morgan wrote,\nDe Morgan gave as example of a pseudomath a certain James Smith who claimed persistently to have proved that formula_1. Of Smith, De Morgan wrote, \"He is beyond a doubt the ablest head at unreasoning, and the greatest hand at writing it, of all who have tried in our day to attach their names to an error.\" The term \"pseudomath\" was adopted later by Tobias Dantzig. Dantzig observed,\n\nMore recently, the term \"pseudomathematics\" has been applied to creationist attempts to refute the theory of evolution by way of spurious arguments purportedly based in probability or complexity theory.\n\n\n"}
{"id": "6034957", "url": "https://en.wikipedia.org/wiki?curid=6034957", "title": "Ramanujan's lost notebook", "text": "Ramanujan's lost notebook\n\nRamanujan's lost notebook is the manuscript in which the Indian mathematician Srinivasa Ramanujan recorded the mathematical discoveries of the last year (1919–1920) of his life. Its whereabouts were unknown to all but a few mathematicians until it was rediscovered by George Andrews in 1976, in a box of effects of G. N. Watson stored at the Wren Library at Trinity College, Cambridge. The \"notebook\" is not a book, but consists of loose and unordered sheets of paper — \"more than one hundred pages written on 138 sides in Ramanujan's distinctive handwriting. The sheets contained over six hundred mathematical formulas listed consecutively without proofs.\"\nhave published several books in which they give proofs for Ramanujan's formulas included in the notebook. Berndt says of the notebooks' discovery: \"The discovery of this 'Lost Notebook' caused roughly as much stir in the mathematical world as the discovery of Beethoven’s tenth symphony would cause in the musical world.\" \n\nAfter Ramanujan died on April 26, 1920, at the age of 32, his wife gave his notebooks to the University of Madras. On August 30, 1923, the registrar Francis Drewsbury sent much of this material to G. H. Hardy, probably including the lost notebook.\n\nSome time between 1934 and 1947 Hardy probably passed the notebook on to G. N. Watson, who with B. M. Wilson started on the project of editing Ramanujan's notebooks. However, Wilson died in 1935 and Watson seems to have lost interest in the project in the late 1930s. After Watson's death in 1965, J. M. Whittaker examined Watson's papers (which were a complete mess, due to be incinerated in a few days) and found Ramanujan's notebook, which he and R. A. Rankin sent to Trinity College Wren library on December 26, 1968. , following a suggestion by Lucy Slater, found the lost notebook in the spring of 1976 while on a visit to Trinity College. It was published on December 22, 1987, by Narosa publishing house.\n\nGeorge Andrews, an American mathematician, wrote an account of the discovery for the 125th celebration of Ramanujan's birth. In his account, Andrews states that he was already an advanced researcher in fields, such as mock theta functions and hypergeometric series, related closely to works of Ramanujan. In 1970, anticipating a sabbatical, he wrote to British mathematician Lucy Slater. Slater \"intriguingly\" stated in her reply that she had inherited a \"great collection\" of papers from mathematicians such as Watson, Bailey, Jackson and Rogers, which were unsorted, including one of the last by Ramanujan. She also mentioned other papers were held by the Trinity College library. \n\nAlthough unable to travel to Europe in 1970, Andrews became able to do so in 1976, when he was due to attend a European conference in Strasbourg, near the France-Germany border. He obtained permission and support from Slater, from the Trinity College library, and from his professor, Ben Noble, to visit Cambridge after the conference, in order to investigate the \"invaluable\" unpublished writings of Watson \"et al\". Noble agreed, adding that if he could attempt to find a lost paper by James Clerk Maxwell at the same time, it would be appreciated. The library's documents included a list of matters held from Watson's estate. The list included the item: \"A 139 page manuscript by S. Ramanujan on q-series\", containing the work from Ramanujan's final year. \n\nAlthough not labelled as such, the identity of the papers was settled because Ramanujan's final letters to Hardy had referred to the discovery of what Ramanujan called mock theta functions, although without great detail, and the manuscript included what appeared to be his full notes on these.\n\n described the lost notebook in detail. The majority of the formulas are about \"q\"-series and mock theta functions, about a third are about modular equations and singular moduli, and the remaining formulas are mainly about integrals, Dirichlet series, congruences, and asymptotics. The mock theta functions in the notebook have been found to be useful for calculating the entropy of black holes.\n\n\n"}
{"id": "5308894", "url": "https://en.wikipedia.org/wiki?curid=5308894", "title": "Space (mathematics)", "text": "Space (mathematics)\n\nIn mathematics, a space is a set (sometimes called a universe) with some added structure.\n\nWhile modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, it does not define the notion of \"space\" itself.\n\nA space consists of selected mathematical objects that are treated as points, and selected relationships between these points. \nThe nature of the points can vary widely: for example, the points can be elements of a set, functions on another space, or subspaces of another space. It is the relationships that define the nature of the space. More precisely, isomorphic spaces are considered identical, where an isomorphism between two spaces is a one-to-one correspondence between their points that preserves the relationships. For example, the relationships between the points of a three-dimensional Euclidean space are uniquely determined by Euclid's axioms, and all three-dimensional Euclidean spaces are considered identical.\n\nTopological notions such as continuity have natural definitions in every Euclidean space. \nHowever, topology does not distinguish straight lines from curved lines, and the relation between Euclidean and topological spaces is thus \"forgetful\". Relations of this kind are sketched in Figure 1, and treated in more detail in the Section \"Types of spaces\". \n\nIt is not always clear whether a given mathematical object should be considered as a geometric \"space\", or an algebraic \"structure\". A general definition of \"structure\", proposed by Bourbaki, embraces all common types of spaces, provides a general definition of isomorphism, and justifies the transfer of properties between isomorphic structures.\n\nIn ancient Greek mathematics, \"space\" was a geometric abstraction of the three-dimensional reality observed in everyday life. About 300 BC, Euclid gave axioms for the properties of space. Euclid built all of mathematics on these geometric foundations, going so far as to define numbers by comparing the lengths of line segments to the length of a chosen reference segment.\n\nThe method of coordinates (analytic geometry) was adopted by René Descartes in 1637. At that time, geometric theorems were treated as absolute objective truths knowable through intuition and reason, similar to objects of natural science; and axioms were treated as obvious implications of definitions.\n\nTwo equivalence relations between geometric figures were used: congruence and similarity. Translations, rotations and reflections transform a figure into congruent figures; homotheties — into similar figures. For example, all circles are mutually similar, but ellipses are not similar to circles. A third equivalence relation, introduced by Gaspard Monge in 1795, occurs in projective geometry: not only ellipses, but also parabolas and hyperbolas, turn into circles under appropriate projective transformations; they all are projectively equivalent figures.\n\nThe relation between the two geometries, Euclidean and projective, shows that mathematical objects are not given to us \"with their structure\". Rather, each mathematical theory describes its objects by \"some\" of their properties, precisely those that are put as axioms at the foundations of the theory.\n\nDistances and angles cannot appear in theorems of projective geometry, since these notions are neither mentioned in the axioms of projective geometry nor defined from the notions mentioned there. The question \"what is the sum of the three angles of a triangle\" is meaningful in Euclidean geometry but meaningless in projective geometry.\n\nA different situation appeared in the 19th century: in some geometries the sum of the three angles of a triangle is well-defined but different from the classical value (180 degrees). Non-Euclidean hyperbolic geometry, introduced by Nikolai Lobachevsky in 1829 and János Bolyai in 1832 (and Carl Friedrich Gauss in 1816, unpublished) stated that the sum depends on the triangle and is always less than 180 degrees. Eugenio Beltrami in 1868 and Felix Klein in 1871 obtained Euclidean \"models\" of the non-Euclidean hyperbolic geometry, and thereby completely justified this theory as a logical possibility.\n\nThis discovery forced the abandonment of the pretensions to the absolute truth of Euclidean geometry. It showed that axioms are not \"obvious\", nor \"implications of definitions\". Rather, they are hypotheses. To what extent do they correspond to an experimental reality? This important physical problem no longer has anything to do with mathematics. Even if a \"geometry\" does not correspond to an experimental reality, its theorems remain no less \"mathematical truths\".\n\nA Euclidean model of a non-Euclidean geometry is a choice of some objects existing in Euclidean space and some relations between these objects that satisfy all axioms (and therefore, all theorems) of the non-Euclidean geometry. These Euclidean objects and relations \"play\" the non-Euclidean geometry like contemporary actors playing an ancient performance. Actors can imitate a situation that never occurred in reality. Relations between the actors on the stage imitate relations between the characters in the play. Likewise, the chosen relations between the chosen objects of the Euclidean model imitate the non-Euclidean relations. It shows that relations between objects are essential in mathematics, while the nature of the objects is not.\n\nThe word \"geometry\" (from Ancient Greek: geo- \"earth\", -metron \"measurement\") initially meant a practical way of processing lengths, regions and volumes in the space in which we live, but was then extended widely (as well as the notion of space in question here).\n\nAccording to Bourbaki, the period between 1795 (\"Géométrie descriptive\" of Monge) and 1872 (the \"Erlangen programme\" of Klein) can be called the golden age of geometry. The original space investigated by Euclid is now called three-dimensional Euclidean space. Its axiomatization, started by Euclid 23 centuries ago, was reformed with Hilbert's axioms, Tarski's axioms and Birkhoff's axioms. These axiom systems describe the space via primitive notions (such as \"point\", \"between\", \"congruent\") constrained by a number of axioms.\n\nAnalytic geometry made great progress and succeeded in replacing theorems of classical geometry with computations via invariants of transformation groups. Since that time, new theorems of classical geometry have been of more interest to amateurs than to professional mathematicians. However, the heritage of classical geometry was not lost. According to Bourbaki, \"passed over in its role as an autonomous and living science, classical geometry is thus transfigured into a universal language of contemporary mathematics\".\n\nSimultaneously, numbers began to displace geometry as the foundation of mathematics. For instance, in Richard Dedekind's 1872 essay \"Stetigkeit und irrationale Zahlen\" (\"Continuity and irrational numbers\"), he asserts that points on a line ought to have the properties of Dedekind cuts, and that therefore a line was the same thing as the set of real numbers. Dedekind is careful to note that this is an assumption that is incapable of being proven. In modern treatments, Dedekind's assertion is often taken to be the definition of a line, thereby reducing geometry to arithmetic. Three-dimensional Euclidean space is defined to be an affine space whose associated vector space of differences of its elements is equipped with an inner product. A definition \"from scratch\", as in Euclid, is now not often used, since it does not reveal the relation of this space to other spaces. Also, a three-dimensional projective space is now defined as the space of all one-dimensional subspaces (that is, straight lines through the origin) of a four-dimensional vector space. This shift in foundations requires a new set of axioms, and if these axioms are adopted, the classical axioms of geometry become theorems.\n\nA space now consists of selected mathematical objects (for instance, functions on another space, or subspaces of another space, or just elements of a set) treated as points, and selected relationships between these points. Therefore, spaces are just mathematical structures of convenience. One may expect that the structures called \"spaces\" are perceived more geometrically than other mathematical objects, but this is not always true.\n\nAccording to the famous inaugural lecture given by Bernhard Riemann in 1854, every mathematical object parametrized by \"n\" real numbers may be treated as a point of the \"n\"-dimensional space of all such objects. Contemporary mathematicians follow this idea routinely and find it extremely suggestive to use the terminology of classical geometry nearly everywhere.\n\nFunctions are important mathematical objects. Usually they form infinite-dimensional function spaces, as noted already by Riemann and elaborated in the 20th century by functional analysis.\n\nWhile each type of spaces has its own definition, the general idea of \"space\" evades formalization. Some structures are called spaces, other are not, without a formal criterion. Moreover, there is no consensus on the general idea of \"structure\".\nAccording to Pudlák, \"Mathematics [...] cannot be explained completely by a single concept such as the mathematical structure. Nevertheless, Bourbaki's structuralist approach is the best that we have.\"\nWe will return to Bourbaki's structuralist approach in the last section \"Spaces and structures\", while we now outline a possible classification of spaces (and structures) in the spirit of Bourbaki.\n\nWe classify spaces on three levels. Given that each mathematical theory describes its objects by some of their properties, the first question to ask is: which properties? This leads to the first (upper) classification level. On the second level, one takes into account answers to especially important questions (among the questions that make sense according to the first level). On the third level of classification, one takes into account answers to all possible questions.\n\nFor example, the \"upper-level classification\" distinguishes between Euclidean and projective spaces, since the distance between two points is defined in Euclidean spaces but undefined in projective spaces. \nAnother example. The question \"what is the sum of the three angles of a triangle\" makes sense in a Euclidean space but not in a projective space. In a non-Euclidean space the question makes sense but is answered differently, which is not an upper-level distinction.\n\nAlso, the distinction between a Euclidean plane and a Euclidean 3-dimensional space is not an upper-level distinction; the question \"what is the dimension\" makes sense in both cases.\n\nThe \"second-level classification\" distinguishes, for example, between Euclidean and non-Euclidean spaces; between finite-dimensional and infinite-dimensional spaces; between compact and non-compact spaces, etc.\nIn Bourbaki's terms, the second-level classification is the classification by \"species\". Unlike biological taxonomy, a space may belong to several species.\n\nThe \"third-level classification\" distinguishes, for example, between spaces of different dimension, but does not distinguish between a plane of a three-dimensional Euclidean space, treated as a two-dimensional Euclidean space, and the set of all pairs of real numbers, also treated as a two-dimensional Euclidean space. Likewise it does not distinguish between different Euclidean models of the same non-Euclidean space.\nMore formally, the third level classifies spaces up to isomorphism. An isomorphism between two spaces is defined as a one-to-one correspondence between the points of the first space and the points of the second space, that preserves all relations stipulated according to the first level. Mutually isomorphic spaces are thought of as copies of a single space. If one of them belongs to a given species then they all do.\n\nThe notion of isomorphism sheds light on the upper-level classification. Given a one-to-one correspondence between two spaces of the same upper-level class, one may ask whether it is an isomorphism or not. This question makes no sense for two spaces of different classes.\n\nAn isomorphism to itself is called an automorphism. Automorphisms of a Euclidean space are shifts, rotations, reflections and compositions of these. Euclidean space is homogeneous in the sense that every point can be transformed into every other point by some automorphism.\n\nEuclidean axioms leave no freedom; they determine uniquely all geometric properties of the space. More exactly: all three-dimensional Euclidean spaces are mutually isomorphic. In this sense we have \"the\" three-dimensional Euclidean space. In Bourbaki's terms, the corresponding theory is \"univalent\". In contrast, topological spaces are generally non-isomorphic; their theory is \"multivalent\". A similar idea occurs in mathematical logic: a theory is called categorical if all its models of the same cardinality are mutually isomorphic. According to Bourbaki, the study of multivalent theories is the most striking feature which distinguishes modern mathematics from classical mathematics.\n\nTopological notions (continuity, convergence, open sets, closed sets etc.) are defined naturally in every Euclidean space. In other words, every Euclidean space is also a topological space. Every isomorphism between two Euclidean spaces is also an isomorphism between the corresponding topological spaces (called \"homeomorphism\"), but the converse is wrong: a homeomorphism may distort distances. In Bourbaki's terms, \"topological space\" is an \"underlying\" structure of the \"Euclidean space\" structure. Similar ideas occur in category theory: the category of Euclidean spaces is a concrete category over the category of topological spaces; the forgetful (or \"stripping\") functor maps the former category to the latter category.\n\nA three-dimensional Euclidean space is a special case of a Euclidean space. In Bourbaki's terms, the species of three-dimensional Euclidean space is \"richer\" than the species of Euclidean space. Likewise, the species of compact topological space is richer than the species of topological space.\n\nSuch relations between species of spaces may be expressed diagrammatically as shown in Fig. 3. An arrow from A to B means that every is also a or may be treated as a or provides a etc. Treating A and B as classes of spaces one may interpret the arrow as a transition from A to B. (In Bourbaki's terms, \"procedure of deduction\" of a from a Not quite a function unless the classes A,B are sets; this nuance does not invalidate the following.) The two arrows on Fig. 3 are not invertible, but for different reasons. \n\nThe transition from \"Euclidean\" to \"topological\" is forgetful. Topology distinguishes continuous from discontinuous, but does not distinguish rectilinear from curvilinear. Intuition tells us that the Euclidean structure cannot be restored from the topology. A proof uses an automorphism of the topological space (that is, self-homeomorphism) that is not an automorphism of the Euclidean space (that is, not a composition of shifts, rotations and reflections). Such transformation turns the given Euclidean structure into a (isomorphic but) different Euclidean structure; both Euclidean structures correspond to a single topological structure.\n\nIn contrast, the transition from \"3-dim Euclidean\" to \"Euclidean\" is not forgetful; a Euclidean space need not be 3-dimensional, but if it happens to be 3-dimensional, it is full-fledged, no structure is lost. In other words, the latter transition is injective (one-to-one), while the former transition is not injective (many-to-one). We denote injective transitions by an arrow with a barbed tail, \"↣\" rather than \"→\".\n\nBoth transitions are not surjective, that is, not every B-space results from some A-space. First, a 3-dim Euclidean space is a special (not general) case of a Euclidean space. Second, a topology of a Euclidean space is a special case of topology (for instance, it must be non-compact, and connected, etc). We denote surjective transitions by a two-headed arrow, \"↠\" rather than \"→\". See for example Fig. 4; there, the arrow from \"real linear topological\" to \"real linear\" is two-headed, since every real linear space admits some (at least one) topology compatible with its linear structure.\n\nSuch topology is non-unique in general, but unique when the real linear space is finite-dimensional. For these spaces the transition is both injective and surjective, that is, bijective; see the arrow from \"finite-dim real linear topological\" to \"finite-dim real linear\" on Fig. 4. The inverse transition exists (and could be shown by a second, backward arrow). The two species of structures are thus equivalent. In practice, one makes no distinction between equivalent species of structures. Equivalent structures may be treated as a single structure, as shown by a large box on Fig. 4.\n\nThe transitions denoted by the arrows obey isomorphisms. That is, two isomorphic lead to two isomorphic .\n\nThe diagram on Fig. 4 is commutative. That is, all directed paths in the diagram with the same start and endpoints lead to the same result. Other diagrams below are also commutative, except for dashed arrows on Fig. 9. The arrow from \"topological\" to \"measurable\" is dashed for the reason explained there: \"In order to turn a topological space into a measurable space one endows it with a σ-algebra. The σ-algebra of Borel sets is the most popular, but not the only choice.\" A solid arrow denotes a prevalent, so-called \"canonical\" transition that suggests itself naturally and is widely used, often implicitly, by default. For example, speaking about a continuous function on a Euclidean space, one need not specify its topology explicitly. In fact, alternative topologies exist and are used sometimes, for example, the fine topology; but these are always specified explicitly, since they are much less notable that the prevalent topology. A dashed arrow indicates that several transitions are in use and no one is quite prevalent.\n\nTwo basic spaces are linear spaces (also called vector spaces) and topological spaces.\n\nLinear spaces are of algebraic nature; there are real linear spaces (over the field of real numbers),\ncomplex linear spaces (over the field of complex numbers), and more generally, linear spaces over any field. Every complex linear space is also a real linear space (the latter \"underlies\" the former), since each real number is also a complex number.\nMore generally, a vector space over a field also has the structure of a vector space over a subfield of that field.\nLinear operations, given in a linear space by definition, lead to such notions as straight lines (and planes, and other linear subspaces); parallel lines; ellipses (and ellipsoids). However, it is impossible to define \northogonal (perpendicular) lines, or to single out circles among ellipses, because in a linear space \nthere is no structure like a scalar product that could be used for measuring angles. The dimension of a linear space is defined as the maximal number of linearly independent vectors or, equivalently, as the minimal number of vectors that span the space; it may be finite or infinite. Two linear spaces over the same field are isomorphic if and only if they are of the same dimension. A complex linear space is also a real linear space.\n\nTopological spaces are of analytic nature. Open sets, given in a topological space by definition, lead to such notions as continuous functions, paths, maps; convergent sequences, limits; interior, boundary, exterior. However, uniform continuity, bounded sets, Cauchy sequences, differentiable functions (paths, maps) remain undefined. Isomorphisms between topological spaces are traditionally called homeomorphisms; these are one-to-one correspondences continuous in both directions. The open interval (0,1) is homeomorphic to the whole real line (-∞,∞) but not homeomorphic to the closed interval [0,1], nor to a circle. The surface of a cube is homeomorphic to a sphere (the surface of a ball) but not homeomorphic to a torus. Euclidean spaces of different dimensions are not homeomorphic, which seems evident, but is not easy to prove. The dimension of a topological space is difficult to define; inductive dimension (based on the observation that the dimension of the boundary of a geometric figure is usually one less than the dimension of the figure itself) and Lebesgue covering dimension can be used. In the case of a Euclidean space, both topological dimensions are equal to \"n\".\n\nEvery subset of a topological space is itself a topological space (in contrast, only \"linear\" subsets of a linear space are linear spaces). Arbitrary topological spaces, investigated by general topology (called also point-set topology) are too diverse for a complete classification up to homeomorphism. Compact topological spaces are an important class of topological spaces (\"species\" of this \"type\"). Every continuous function is bounded on such space. The closed interval [0,1] and the extended real line [-∞,∞] are compact; the open interval (0,1) and the line (-∞,∞) are not. Geometric topology investigates manifolds (another \"species\" of this \"type\"); these are topological spaces locally homeomorphic to Euclidean spaces (and satisfying a few extra conditions). Low-dimensional manifolds are completely classified up to homeomorphism.\n\nBoth the linear and topological structures underly the linear topological space (in other words, topological vector space) structure. A linear topological space is both a real or complex linear space and a topological space, such that the linear operations are continuous. So a linear space that is also topological is not in general a linear topological space. \n\nEvery finite-dimensional real or complex linear space is a linear topological space in the sense that it carries one and only one topology that makes it a linear topological space. The two structures, \"finite-dimensional real or complex linear space\" and \"finite-dimensional linear topological space\", are thus equivalent, that is, mutually underlying. Accordingly, every invertible linear transformation of a finite-dimensional linear topological space is a homeomorphism. The three notions of dimension (one algebraic and two topological) agree for finite-dimensional real linear spaces. In infinite-dimensional spaces, however, different topologies can conform to a given linear structure, and invertible linear transformations are generally not homeomorphisms.\n\nIt is convenient to introduce affine and projective spaces by means of linear spaces, as follows. A linear subspace of a linear space, being itself a linear space, is not homogeneous; it contains a special point, the origin. Shifting it by a vector external to it, one obtains a affine subspace. It is homogeneous. An affine space need not be included into a linear space, but is isomorphic to an affine subspace of a linear space. All affine spaces are mutually isomorphic. In the words of John Baez, \"an affine space is a vector space that's forgotten its origin\". In particular, every linear space is also an affine space.\n\nGiven an affine subspace \"A\" in a linear space \"L\", a straight line in \"A\" may be defined as the intersection of \"A\" with a linear subspace of \"L\" that intersects \"A\": in other words, with a plane through the origin that is not parallel to \"A\". More generally, a affine subspace of \"A\" is the intersection of \"A\" with a linear subspace of \"L\" that intersects \"A\".\n\nEvery point of the affine subspace \"A\" is the intersection of \"A\" with a linear subspace of \"L\". However, some subspaces of \"L\" are parallel to \"A\"; in some sense, they intersect \"A\" at infinity. The set of all linear subspaces of a linear space is, by definition, a projective space. And the affine subspace \"A\" is embedded into the projective space as a proper subset. However, the projective space itself is homogeneous. A straight line in the projective space corresponds to a linear subspace of the (n+1)-dimensional linear space. More generally, a projective subspace of the projective space corresponds to a linear subspace of the (n+1)-dimensional linear space, and is isomorphic to the projective space.\n\nDefined this way, affine and projective spaces are of algebraic nature; they can be real, complex, and more generally, over any field.\n\nEvery real or complex affine or projective space is also a topological space. An affine space is a non-compact manifold; a projective space is a compact manifold. In a real projective space a straight line is homeomorphic to a circle, therefore compact, in contrast to a straight line in a linear of affine space.\n\nDistances between points are defined in a metric space. Isomorphisms between metric spaces are called isometries. Every metric space is also a topological space. A topological space is called metrizable, if it underlies a metric space. All manifolds are metrizable.\n\nIn a metric space, we can define \nbounded sets and Cauchy sequences. A metric space is called complete if all Cauchy sequences converge. Every incomplete space is isometrically embedded, as a dense subset, into a complete space (the completion). Every compact metric space is complete; the real line is non-compact but complete; the open interval (0,1) is incomplete. \n\nEvery Euclidean space is also a complete metric space. Moreover, all geometric notions immanent to a Euclidean space can be characterized in terms of its metric. For example, the straight segment connecting two given points \"A\" and \"C\" consists of all points \"B\" such that the distance between \"A\" and \"C\" is equal to the sum of two distances, between \"A\" and \"B\" and between \"B\" and \"C\".\n\nThe Hausdorff dimension (related to the number of small balls that cover the given set) applies to metric spaces, and can be non-integer (especially for fractals). For a Euclidean space, the Hausdorff dimension is equal to \"n\".\n\nUniform spaces do not introduce distances, but still allow one to use uniform continuity, Cauchy sequences (or filters or nets), completeness and completion. Every uniform space is also a topological space. Every \"linear\" topological space (metrizable or not) is also a uniform space, and is complete in finite dimension but generally incomplete in infinite dimension. More generally, every commutative topological group is also a uniform space. A non-commutative topological group, however, carries two uniform structures, one left-invariant, the other right-invariant.\n\nVectors in a Euclidean space form a linear space, but each vector formula_1 has also a length, in other words, norm, formula_2. A real or complex linear space endowed with a norm is a normed space. Every normed space is both a linear topological space and a metric space. A Banach space is a complete normed space. Many spaces of sequences or functions are infinite-dimensional Banach spaces.\n\nThe set of all vectors of norm less than one is called the unit ball of a normed space. It is a convex, centrally symmetric set, generally not an ellipsoid; for example, it may be a polygon (in the plane) or. more generally, a polytope (in arbitrary finite dimension). The parallelogram law (called also parallelogram identity) \ngenerally fails in normed spaces, but holds for vectors in Euclidean spaces, which follows from the fact that the squared Euclidean norm of a vector is its inner product with itself, formula_4.\n\nAn inner product space is a real or complex linear space, endowed with a bilinear or respectively sesquilinear form, satisfying some conditions and called an inner product. Every inner product space is also a normed space. A normed space underlies an inner product space if and only if it satisfies the parallelogram law, or equivalently, if its unit ball is an ellipsoid. Angles between vectors are defined in inner product spaces. A Hilbert space is defined as a complete inner product space. (Some authors insist that it must be complex, others admit also real Hilbert spaces.) Many spaces of sequences or functions are infinite-dimensional Hilbert spaces. Hilbert spaces are very important for quantum theory.\n\nAll real inner product spaces are mutually isomorphic. One may say that the Euclidean space is the real inner product space that forgot its origin.\n\nSmooth manifolds are not called \"spaces\", but could be. Every smooth manifold is a topological manifold, and can be embedded into a finite-dimensional linear space. Smooth surfaces in a finite-dimensional linear space are smooth manifolds: for example, the surface of an ellipsoid is a smooth manifold, a polytope is not. Real or complex finite-dimensional linear, affine and projective spaces are also smooth manifolds.\n\nAt each one of its points, a smooth path in a smooth manifold has a tangent vector that belongs to the manifold's tangent space at this point. Tangent spaces to an smooth manifold are linear spaces. The differential of a smooth function on a smooth manifold provides a linear functional on the tangent space at each point. \n\nA Riemannian manifold, or Riemann space, is a smooth manifold whose tangent spaces are endowed with inner products satisfying some conditions. Euclidean spaces are also Riemann spaces. Smooth surfaces in Euclidean spaces are Riemann spaces. A hyperbolic space is also a Riemann space. A curve in a Riemann space has a length, and the length of the shortest curve between two points defines a distance, such that the Riemann space is a metric space. The angle between two curves intersecting at a point is the angle between their tangent lines.\n\nWaiving positivity of inner products on tangent spaces, one obtains pseudo-Riemann spaces, including the Lorentzian spaces that are very important for general relativity.\n\nWaiving distances and angles while retaining volumes (of geometric bodies) one reaches measure theory. Besides the volume, a measure generalizes the notions of area, length, mass (or charge) distribution, and also probability distribution, according to Andrey Kolmogorov's approach to probability theory.\n\nA \"geometric body\" of classical mathematics is much more regular than just a set of points. The boundary of the body is of zero volume. Thus, the volume of the body is the volume of its interior, and the interior can be exhausted by an infinite sequence of cubes. In contrast, the boundary of an arbitrary set of points can be of non-zero volume (an example: the set of all rational points inside a given cube). Measure theory succeeded in extending the notion of volume to a vast class of sets, the so-called measurable sets. Indeed, non-measurable sets almost never occur in applications.\n\nMeasurable sets, given in a measurable space by definition, lead to measurable functions and maps. In order to turn a topological space into a measurable space one endows it with a The of Borel sets is the most popular, but not the only choice. (Baire sets, universally measurable sets, etc, are also used sometimes.) \nThe topology is not uniquely determined by the Borel for example, the norm topology and the weak topology on a separable Hilbert space lead to the same Borel .\nNot every is the Borel of some topology.\nActually, a can be generated by a given collection of sets (or functions) irrespective of any topology. Every subset of a measurable space is itself a measurable space.\n\nStandard measurable spaces (also called standard Borel spaces) are especially useful due to some similarity to compact spaces (see EoM). Every bijective measurable mapping between standard measurable spaces is an isomorphism; that is, the inverse mapping is also measurable. And a mapping between such spaces is measurable if and only if its graph is measurable in the product space. Similarly, every bijective continuous mapping between compact metric spaces is a homeomorphism; that is, the inverse mapping is also continuous. And a mapping between such spaces is continuous if and only if its graph is closed in the product space.\n\nEvery Borel set in a Euclidean space (and more generally, in a complete separable metric space), endowed with the Borel is a standard measurable space. All uncountable standard measurable spaces are mutually isomorphic.\n\nA measure space is a measurable space endowed with a measure. A Euclidean space with the Lebesgue measure is a measure space. Integration theory defines integrability and integrals of measurable functions on a measure space.\n\nSets of measure 0, called null sets, are negligible. Accordingly, a \"mod 0 isomorphism\" is defined as isomorphism between subsets of full measure (that is, with negligible complement).\n\nA probability space is a measure space such that the measure of the whole space is equal to 1. The product of any family (finite or not) of probability spaces is a probability space. In contrast, for measure spaces in general, only the product of finitely many spaces is defined. Accordingly, there are many infinite-dimensional probability measures (especially, Gaussian measures), but no infinite-dimensional Lebesgue measures.\n\nStandard probability spaces are especially useful. On a standard probability space a conditional expectation may be treated as the integral over the conditional measure (regular conditional probabilities, see also disintegration of measure). Given two standard probability spaces, every homomorphism of their measure algebras is induced by some measure preserving map. Every probability measure on a standard measurable space leads to a standard probability space. The product of a sequence (finite or not) of standard probability spaces is a standard probability space. All non-atomic standard probability spaces are mutually isomorphic mod 0; one of them is the interval (0,1) with the Lebesgue measure.\n\nThese spaces are less geometric. In particular, the idea of dimension, applicable (in one form or another) to all other spaces, does not apply to measurable, measure and probability spaces.\n\nThe theoretical study of calculus, known as mathematical analysis, led in the early 20th century to the consideration of linear spaces of real-valued or complex-valued functions. The earliest examples of these were function spaces, each one adapted to its own class of problems. These examples shared many common features, and these features were soon abstracted into Hilbert spaces, Banach spaces, and more general topological vector spaces. These were a powerful toolkit for the solution of a wide range of mathematical problems.\n\nThe most detailed information was carried by a class of spaces called Banach algebras. These are Banach spaces together with a continuous multiplication operation. An important early example was the Banach algebra of essentially bounded measurable functions on a measure space \"X\". This set of functions is a Banach space under pointwise addition and scalar multiplication. With the operation of pointwise multiplication, it becomes a special type of Banach space, one now called a commutative von Neumann algebra. Pointwise multiplication determines a representation of this algebra on the Hilbert space of square integrable functions on \"X\". An early observation of John von Neumann was that this correspondence also worked in reverse: Given some mild technical hypotheses, a commutative von Neumann algebra together with a representation on a Hilbert space determines a measure space, and these two constructions (of a von Neumann algebra plus a representation and of a measure space) are mutually inverse.\n\nVon Neumann then proposed that non-commutative von Neumann algebras should have geometric meaning, just as commutative von Neumann algebras do. Together with Francis Murray, he produced a classification of von Neumann algebras. The direct integral construction shows how to break any von Neumann algebra into a collection of simpler algebras called \"factors\". Von Neumann and Murray classified factors into three types. Type I was nearly identical to the commutative case. Types II and III exhibited new phenomena. A type II von Neumann algebra determined a geometry with the peculiar feature that the dimension could be any non-negative real number, not just an integer. Type III algebras were those that were neither types I nor II, and after several decades of effort, these were proven to be closely related to type II factors.\n\nA slightly different approach to the geometry of function spaces developed at the same time as von Neumann and Murray's work on the classification of factors. This approach is the theory of Here, the motivating example is the formula_5, where \"X\" is a locally compact Hausdorff topological space. By definition, this is the algebra of continuous complex-valued functions on \"X\" that vanish at infinity (which loosely means that the farther you go from a chosen point, the closer the function gets to zero) with the operations of pointwise addition and multiplication. The Gelfand–Naimark theorem implied that there is a correspondence between commutative and geometric objects: Every commutative is of the form formula_5 for some locally compact Hausdorff space \"X\". Consequently it is possible to study locally compact Hausdorff spaces purely in terms of commutative Non-commutative geometry takes this as inspiration for the study of non-commutative If there were such a thing as a \"non-commutative space \"X\",\" then its formula_5 would be a non-commutative ; if in addition the Gelfand–Naimark theorem applied to these non-existent objects, then spaces (commutative or not) would be the same as so, for lack of a direct approach to the definition of a non-commutative space, a non-commutative space is \"defined\" to be a non-commutative Many standard geometric tools can be restated in terms of and this gives geometrically-inspired techniques for studying non-commutative .\n\nBoth of these examples are now cases of a field called non-commutative geometry. The specific examples of von Neumann algebras and are known as non-commutative measure theory and non-commutative topology, respectively. Non-commutative geometry is not merely a pursuit of generality for its own sake and is not just a curiosity. Non-commutative spaces arise naturally, even inevitably, from some constructions. For example, consider the non-periodic Penrose tilings of the plane by kites and darts. It is a theorem that, in such a tiling, every finite patch of kites and darts appears infinitely often. As a consequence, there is no way to distinguish two Penrose tilings by looking at a finite portion. This makes it impossible to assign the set of all tilings a topology in the traditional sense. Despite this, the Penrose tilings determine a non-commutative and consequently they can be studied by the techniques of non-commutative geometry. Another example, and one of great interest within differential geometry, comes from foliations of manifolds. These are ways of splitting the manifold up into smaller-dimensional submanifolds called \"leaves\", each of which is locally parallel to others nearby. The set of all leaves can be made into a topological space. However, the example of an irrational rotation shows that this topological space can be inacessible to the techniques of classical measure theory. However, there is a non-commutative von Neumann algebra associated to the leaf space of a foliation, and once again, this gives an otherwise unintelligible space a good geometric structure.\n\nAlgebraic geometry studies the geometric properties of polynomial equations. Polynomials are a type of function defined from the basic arithmetic operations of addition and multiplication. Because of this, they are closely tied to algebra. Algebraic geometry offers a way to apply geometric techniques to questions of pure algebra, and vice versa.\n\nPrior to the 1940s, algebraic geometry worked exclusively over the complex numbers, and the most fundamental variety was projective space. The geometry of projective space is closely related to the theory of perspective, and its algebra is described by homogeneous polynomials. All other varieties were defined as subsets of projective space. Projective varieties were subsets defined by a set of homogeneous polynomials. At each point of the projective variety, all the polynomials in the set were required to equal zero. The complement of the zero set of a linear polynomial is an affine space, and an affine variety was the intersection of a projective variety with an affine space.\n\nAndré Weil saw that geometric reasoning could sometimes be applied in number-theoretic situations where the spaces in question might be discrete or even finite. In pursuit of this idea, Weil rewrote the foundations of algebraic geometry, both freeing algebraic geometry from its reliance on complex numbers and introducing \"abstract algebraic varieties\" which were not embedded in projective space. These are now simply called \"varieties\".\n\nThe type of space that underlies most modern algebraic geometry is even more general than Weil's abstract algebraic varieties. It was introduced by Alexander Grothendieck and is called a scheme. One of the motivations for scheme theory is that polynomials are unusually structured among functions, and algebraic varieties are consequently rigid. This presents problems when attempting to study degenerate situations. For example, almost any pair of points on a circle determines a unique line called the secant line, and as the two points move around the circle, the secant line varies continuously. However, when the two points collide, the secant line degenerates to a tangent line. The tangent line is unique, but the geometry of this configuration—a single point on a circle—is not expressive enough to determine a unique line. Studying situations like this requires a theory capable of assigning extra data to degenerate situations.\n\nOne of the building blocks of a scheme is a topological space. Topological spaces have continuous functions, but continuous functions are too general to reflect the underlying algebraic structure of interest. The other ingredient in a scheme, therefore, is a sheaf on the topological space, called the \"structure sheaf\". On each open subset of the topological space, the sheaf specifies a collection of functions, called \"regular functions\". The topological space and the structure sheaf together are required to satisfy conditions that mean the functions come from algebraic operations.\n\nLike manifolds, schemes are defined as spaces that are locally modeled on a familiar space. In the case of manifolds, the familiar space is Euclidean space. For a scheme, the local models are called affine schemes. Affine schemes provide a direct link between algebraic geometry and commutative algebra. The fundamental objects of study in commutative algebra are commutative rings. If formula_8 is a commutative ring, then there is a corresponding affine scheme formula_9 which translates the algebraic structure of formula_8 into geometry. Conversely, every affine scheme determines a commutative ring, namely, the ring of global sections of its structure sheaf. These two operations are mutually inverse, so affine schemes provide a new language with which to study questions in commutative algebra. By definition, every point in a scheme has an open neighborhood which is an affine scheme.\n\nThere are many schemes that are not affine. In particular, projective spaces satisfy a condition called properness which is analogous to compactness. Affine schemes cannot be proper (except in trivial situations like when the scheme has only a single point), and hence no projective space is an affine scheme (except for zero-dimensional projective spaces). Projective schemes, meaning those that arise as closed subschemes of a projective space, are the single most important family of schemes.\n\nSeveral generalizations of schemes have been introduced. Michael Artin defined an algebraic space as the quotient of a scheme by the equivalence relations that define étale morphisms. Algebraic spaces retain many of the useful properties of schemes while simultaneously being more flexible. For instance, the Keel–Mori theorem can be used to show that many moduli spaces are algebraic spaces.\n\nMore general than an algebraic space is a Deligne–Mumford stack. DM stacks are similar to schemes, but they permit singularities that cannot be described solely in terms of polynomials. They play the same role for schemes that orbifolds do for manifolds. For example, the quotient of the affine plane by a finite group of rotations around the origin yields a Deligne–Mumford stack that is not a scheme or an algebraic space. Away from the origin, the quotient by the group action identifies finite sets of equally spaced points on a circle. But at the origin, the circle consists of only a single point, the origin itself, and the group action fixes this point. In the quotient DM stack, however, this point comes with the extra data of being a quotient. This kind of refined structure is useful in the theory of moduli spaces, and in fact, it was originally introduced to describe moduli of algebraic curves.\n\nA further generalization are the algebraic stacks, also called Artin stacks. DM stacks are limited to quotients by finite group actions. While this suffices for many problems in moduli theory, it is too restrictive for others, and Artin stacks permit more general quotients.\n\nIn Grothendieck's work on the Weil conjectures, he introduced a new type of topology now called a Grothendieck topology. A topological space (in the ordinary sense) axiomatizes the notion of \"nearness,\" making two points be nearby if and only if they lie in many of the same open sets. By contrast, a Grothendieck topology axiomatizes the notion of \"covering\". A covering of a space is a collection of subspaces that jointly contain all the information of the ambient space. Since sheaves are defined in terms of coverings, a Grothendieck topology can also be seen as an axiomatization of the theory of sheaves.\n\nGrothendieck's work on his topologies led him to the theory of topoi. In his memoir \"Récoltes et Semailles\", he called them his \"most vast conception\". A sheaf (either on a topological space or with respect to a Grothendieck topology) is used to express local data. The category of all sheaves carries all possible ways of expressing local data. Since topological spaces are constructed from points, which are themselves a kind of local data, the category of sheaves can therefore be used as a replacement for the original space. Grothendieck consequently defined a topos to be a category of sheaves and studied topoi as objects of interest in their own right. These are now called Grothendieck topoi.\n\nEvery topological space determines a topos, and vice versa. There are topological spaces where taking the associated topos loses information, but these are generally considered pathological. (A necessary and sufficient condition is that the topological space be a sober space.) Conversely, there are topoi whose associated topological spaces do not capture the original topos. But, far from being pathological, these topoi can be of great mathematical interest. For instance, Grothendieck's theory of étale cohomology (which eventually led to the proof of the Weil conjectures) can be phrased as cohomology in the étale topos of a scheme, and this topos does not come from a topological space.\n\nTopological spaces in fact lead to very special topoi called locales. The set of open subsets of a topological space determines a lattice. The axioms for a topological space cause these lattices to be complete Heyting algebras. The theory of locales takes this as its starting point. A locale is defined to be a complete Heyting algebra, and the elementary properties of topological spaces are re-expressed and reproved in these terms. The concept of a locale turns out to be more general than a topological space, in that every sober topological space determines a unique locale, but many interesting locales do not come from topological spaces. Because locales need not have points, the study of locales is somewhat jokingly called pointless topology.\n\nTopoi also display deep connections to mathematical logic. Every Grothendieck topos has a special sheaf called a subobject classifier. This subobject classifier functions like the set of all possible truth values. In the topos of sets, the subobject classifier is the set formula_11, corresponding to \"False\" and \"True\". But in other topoi, the subobject classifier can be much more complicated. Lawvere and Tierney recognized that axiomatizing the subobject classifier yielded a more general kind of topos, now known as an elementary topos, and that elementary topoi were models of intuitionistic logic. In addition to providing a powerful way to apply tools from logic to geometry, this made possible the use of geometric methods in logic.\n\nAccording to Kevin Carlson,\n\nNevertheless, a general definition of \"structure\" was proposed by Bourbaki; it embraces all types of spaces mentioned above, (nearly?) all types of mathematical structures used till now, and more. It provides a general definition of isomorphism, and justifies transfer of properties between isomorphic structures. However, it was never used actively in mathematical practice (not even in the mathematical treatises written by Bourbaki himself). Here are the last phrases from a review by Robert Reed of a book by Leo Corry:\n\nFor more information on mathematical structures see Wikipedia: mathematical structure, equivalent definitions of mathematical structures, and transport of structure.\n\nThe distinction between geometric \"spaces\" and algebraic \"structures\" is sometimes clear, sometimes elusive. Clearly, groups are algebraic, while Euclidean spaces are geometric. Modules over rings are as algebraic as groups. In particular, when the ring appears to be a field, the module appears to be a linear space; is it algebraic or geometric? In particular, when it is finite-dimensional, over real numbers, and endowed with inner product, it becomes Euclidean space; now geometric. The (algebraic?) field of real numbers is the same as the (geometric?) real line. Its algebraic closure, the (algebraic?) field of complex numbers, is the same as the (geometric?) complex plane. It is first of all \"a place we do analysis\" (rather than algebra or geometry).\n\nEvery space treated in Section \"Types of spaces\" above, except for \"Non-commutative geometry\", \"Schemes\" and \"Topoi\" subsections, is a set (the \"principal base set\" of the structure, according to Bourbaki) endowed with some additional structure; elements of the base set are usually called \"points\" of this space. In contrast, elements of (the base set of) an algebraic structure usually are not called \"points\".\n\nHowever, sometimes one uses more than one principal base set. For example, two-dimensional projective geometry may be formalized via two base sets, the set of points and the set of lines. Moreover, a striking feature of projective planes is the symmetry of the roles played by points and lines. A less geometric example: a graph may be formalized via two base sets, the set of vertices (called also nodes or points) and the set of edges (called also arcs or lines). Generally, finitely many principal base sets and finitely many auxiliary base sets are stipulated by Bourbaki.\n\nMany mathematical structures of geometric flavor treated in the \"Non-commutative geometry\", \"Schemes\" and \"Topoi\" subsections above do not stipulate a base set of points. For example, \"pointless topology\" (in other words, point-free topology, or locale theory) starts with a single base set whose elements imitate open sets in a topological space (but are not sets of points); see also mereotopology and point-free geometry.\n\n\n"}
{"id": "246160", "url": "https://en.wikipedia.org/wiki?curid=246160", "title": "Summation", "text": "Summation\n\nIn mathematics, summation (denoted with an enlarged capital Greek sigma symbol formula_1) is the addition of a sequence of numbers; the result is their \"sum\" or \"total\". If numbers are added sequentially from left to right, any intermediate result is a partial sum, prefix sum, or running total of the summation.\n\nThe numbers to be summed (called \"addends\", or sometimes \"summands\") may be integers, rational numbers, real numbers, or complex numbers. Besides numbers, other types of values can be added as well: vectors, matrices, polynomials and, in general, elements of any additive group (or even monoid).\n\nFor finite sequences of such elements, summation always produces a well-defined sum. The summation of an infinite sequence of values is called a series. A value of such a series may often be defined by means of a limit (although sometimes the value may be infinite, and often no value results at all). Another notion involving limits of finite sums is integration.\n\nThe summation of the sequence <nowiki>[</nowiki>1, 2, 4, 2<nowiki>]</nowiki> is an expression whose value is the sum of each of the members of the sequence. In the example, = 9. Because addition is associative, the sum does not depend on how the additions are grouped, for instance and both have the value 9; therefore, parentheses are usually omitted in repeated additions. Addition is also commutative, so permuting the terms of a finite sequence does not change its sum. For infinite summations this property may fail. See Absolute convergence for conditions under which it still holds.\n\nThere is no special notation for the summation of such explicit sequences, as the corresponding repeated addition expression will do. There is only a slight difficulty if the sequence has fewer than two elements: the summation of a sequence of one term involves no plus sign (it is indistinguishable from the term itself) and the summation of the empty sequence cannot even be written down (but one can write its value \"0\" in its place). If, however, the terms of the sequence are given by a regular pattern, possibly of variable length, then a summation operator may be useful or even essential.\n\nFor the summation of the sequence of consecutive integers from 1 to 100, one could use an addition expression involving an ellipsis to indicate the missing terms: . In this case, the reader can easily guess the pattern. However, for more complicated patterns, one needs to be precise about the rule used to find successive terms, which can be achieved by using the summation operator \"Σ\". Using this sigma notation the above summation is written as:\n\nThe value of this summation is 5050. It can be found without performing 99 additions, since it can be shown (for instance by mathematical induction) that\n\nfor all natural numbers \"n\". More generally, formulae exist for many summations of terms following a regular pattern.\n\nThe term \"indefinite summation\" refers to the search for an inverse image of a given infinite sequence \"s\" of values for the forward difference operator, in other words for a sequence, called antidifference of \"s\", whose finite differences are given by \"s\". By contrast, summation as discussed in this article is called \"definite summation\".\n\nWhen it is necessary to clarify that numbers are added with their signs, the term algebraic sum is used. For example, in electric circuit theory Kirchhoff's circuit laws consider the algebraic sum of currents in a network of conductors meeting at a point, assigning opposite signs to currents flowing in and out of the node.\n\nMathematical notation uses a symbol that compactly represents summation of many similar terms: the \"summation symbol\", formula_1, an enlarged form of the upright capital Greek letter Sigma. This is defined as:\n\nwhere \"i\" represents the index of summation; \"a\" is an indexed variable representing each successive term in the series; \"m\" is the lower bound of summation, and \"n\" is the upper bound of summation. The \"i = m\" under the summation symbol means that the index \"i\" starts out equal to \"m\". The index, \"i\", is incremented by 1 for each successive term, stopping when \"i\" = \"n\".\n\nHere is an example showing the summation of squares:\n\nInformal writing sometimes omits the definition of the index and bounds of summation when these are clear from context, as in:\n\nOne often sees generalizations of this notation in which an arbitrary logical condition is supplied, and the sum is intended to be taken over all values satisfying the condition. Here are some common examples:\nis the sum of formula_9 over all (integers) formula_10 in the specified range,\nis the sum of formula_12 over all elements formula_13 in the set formula_14, and\nis the sum of formula_16 over all positive integers formula_17 dividing formula_18.\n\nThere are also ways to generalize the use of many sigma signs. For example,\nis the same as\n\nA similar notation is applied when it comes to denoting the product of a sequence, which is similar to its summation, but which uses the multiplication operation instead of addition (and gives 1 for an empty sequence instead of 0). The same basic structure is used, with formula_21, an enlarged form of the Greek capital letter Pi, replacing the formula_1.\n\nIt is possible to sum fewer than 2 numbers:\n\nThese degenerate cases are usually only used when the summation notation gives a degenerate result in a special case.\nFor example, if formula_25 in the definition above, then there is only one term in the sum; if formula_26, then there is none.\n\nSummation may be defined recursively as follows \n\nIn the notation of measure and integration theory, a sum can be expressed as a definite integral,\n\nwhere formula_31 is the subset of the integers from formula_32 to formula_33, and where formula_34 is the counting measure.\n\nGiven a function that is defined over the integers in the interval , one has\n\nThis is the analogue in calculus of finite differences of the fundamental theorem of calculus, which states\nwhere \nis the derivative of .\n\nAn example of application of the above equation is \nUsing binomial theorem, this may be rewritten \n\nThe above formula is more commonly used for inverting of the difference operator formula_40 defined by\nwhere is a function defined on the nonnegative integers.\nThus, given such a function , the problem is to compute the antidifference of , that is, a function formula_42 such that formula_43, that is,formula_44\nThis function is defined up to the addition of a constant, and may be chosen as\n\nThere is not always a closed-form expression for such a summation, but Faulhaber's formula provides a closed form in the case of formula_46 and, by linearity for every polynomial function of .\n\nMany such approximations can be obtained by the following connection between sums and integrals, which holds for any:\n\nincreasing function \"f\":\n\ndecreasing function \"f\":\n\nFor more general approximations, see the Euler–Maclaurin formula.\n\nFor summations in which the summand is given (or can be interpolated) by an integrable function of the index, the summation can be interpreted as a Riemann sum occurring in the definition of the corresponding definite integral. One can therefore expect that for instance\n\nsince the right hand side is by definition the limit for formula_50 of the left hand side. However, for a given summation \"n\" is fixed, and little can be said about the error in the above approximation without additional assumptions about \"f\": it is clear that for wildly oscillating functions the Riemann sum can be arbitrarily far from the Riemann integral.\n\nThe formulae below involve finite sums; for infinite summations or finite summations of expressions involving trigonometric functions or other transcendental functions, see list of mathematical series.\n\nMore generally,\nwhere formula_72 denotes a Bernoulli number (that is Faulhaber's formula).\n\nIn the following summations, is supposed to be different of 1.\n\nThere exist very many summation identities involving binomial coefficients (a whole chapter of \"Concrete Mathematics\" is devoted to just the basic techniques). Some of the most basic ones are the following.\n\nIn the following summations, formula_83 is the number of -permutations of.\n\nThe following are useful approximations (using theta notation):\n\n\n"}
{"id": "12745577", "url": "https://en.wikipedia.org/wiki?curid=12745577", "title": "The Quadrature of the Parabola", "text": "The Quadrature of the Parabola\n\nThe Quadrature of the Parabola () is a treatise on geometry, written by Archimedes in the 3rd century BC. Written as a letter to his friend Dositheus, the work presents 24 propositions regarding parabolas, culminating in a proof that the area of a parabolic segment (the region enclosed by a parabola and a line) is 4/3 that of a certain inscribed triangle.\n\nThe statement of the problem used the method of exhaustion. Archimedes may have dissected the area into infinitely many triangles whose areas form a geometric progression. He computes the sum of the resulting geometric series, and proves that this is the area of the parabolic segment. This represents the most sophisticated use of the method of exhaustion in ancient mathematics, and remained unsurpassed until the development of integral calculus in the 17th century, being succeeded by Cavalieri's quadrature formula.\n\nA parabolic segment is the region bounded by a parabola and line. To find the area of a parabolic segment, Archimedes considers a certain inscribed triangle. The base of this triangle is the given chord of the parabola, and the third vertex is the point on the parabola such that the tangent to the parabola at that point is parallel to the chord. By Proposition 1 (Quadrature of the Parabola), a line from the third vertex drawn parallel to the axis divides the chord into equal segments. The main theorem claims that the area of the parabolic segment is 4/3 that of the inscribed triangle.\nArchimedes gives two proofs of the main theorem. The first uses abstract mechanics, with Archimedes arguing that the weight of the segment will balance the weight of the triangle when placed on an appropriate lever. The second, more famous proof uses pure geometry, specifically the method of exhaustion.\n\nOf the twenty-four propositions, the first three are quoted without proof from Euclid's \"Elements of Conics\" (a lost work by Euclid on conic sections). Propositions four and five establish elementary properties of the parabola; propositions six through seventeen give the mechanical proof of the main theorem; and propositions eighteen through twenty-four present the geometric proof.\n\nThe main idea of the proof is the dissection of the parabolic segment into infinitely many triangles, as shown in the figure to the right. Each of these triangles is inscribed in its own parabolic segment in the same way that the blue triangle is inscribed in the large segment.\n\nIn propositions eighteen through twenty-one, Archimedes proves that the area of each green triangle is one eighth of the area of the blue triangle. From a modern point of view, this is because the green triangle has half the width and a fourth of the height:\n\nBy extension, each of the yellow triangles has one eighth the area of a green triangle, each of the red triangles has one eighth the area of a yellow triangle, and so on. Using the method of exhaustion, it follows that the total area of the parabolic segment is given by\n\nHere \"T\" represents the area of the large blue triangle, the second term represents the total area of the two green triangles, the third term represents the total area of the four yellow triangles, and so forth. This simplifies to give\n\nTo complete the proof, Archimedes shows that\n\nThe formula above is a geometric series—each successive term is one fourth of the previous term. In modern mathematics, that formula is a special case of the sum formula for a geometric series.\n\nArchimedes evaluates the sum using an entirely geometric method, illustrated in the adjacent picture. This picture shows a unit square which has been dissected into an infinity of smaller squares. Each successive purple square has one fourth the area of the previous square, with the total purple area being the sum\n\nHowever, the purple squares are congruent to either set of yellow squares, and so cover 1/3 of the area of the unit square. It follows that the series above sums to 4/3.\n\n\n\n"}
{"id": "41391212", "url": "https://en.wikipedia.org/wiki?curid=41391212", "title": "Timeline of women in mathematics in the United States", "text": "Timeline of women in mathematics in the United States\n\nThere is a long history of women in mathematics in the United States. All women mentioned here are American unless otherwise noted. \n\n1829: The first public examination of an American girl in geometry was held.\n\n1886: Winifred Edgerton Merrill became the first American woman to earn a PhD in mathematics, which she earned from Columbia University.\n\n1913: Mildred Sanderson published her theorem about modular invariants in her thesis. It states: “To any modular invariant i of a system of forms under any group G of linear transformations with coefficients in the GF[pn], there corresponds a formal invariant I under G such that I = i for all sets of values in the field of the coefficients of the system of forms.” She was Leonard Dickson’s first female graduate student, and he later wrote of her thesis, “This paper is a highly important contribution to this field of work; its importance lies partly in the fact that it establishes a correspondence between modular and formal invariants. Her main theorem has already been frequently quoted on account of its fundamental character. Her proof is a remarkable piece of mathematics.” E.T. Bell wrote, “Miss Sanderson’s single contribution (1913) to modular invariants has been rated by competent judges as one of the classics of the subject.”\n\n1927: Anna Pell-Wheeler became the first woman to present a lecture at the American Mathematical Society Colloquium.\n\n1943: Euphemia Haynes became the first African-American woman to earn a Ph.D. in mathematics, which she earned from Catholic University.\n\n1949: Gertrude Mary Cox became the first woman elected into the International Statistical Institute.\n\n1956: Gladys West began collecting data from satellites at the Naval Surface Warfare Center Dahlgren Division. Her calculations directly impacted the development of accurate GPS systems.\n\n1962: Mina Rees became the first woman to win the Yueh-Gin Gung and Dr. Charles Y. Hu Award for Distinguished Service to Mathematics, which is the most prestigious award made by the Mathematical Association of America.\n\n1966: Mary L. Boas published \"Mathematical Methods in the Physical Sciences\", which was still widely used in college classrooms as of 1999.\n\n1970: Mina Rees became the first female president of the American Association for the Advancement of Science.\n\n1971: Mary Ellen Rudin constructed the first Dowker space.\n\n1971: The Association for Women in Mathematics (AWM) was founded. It is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. It is incorporated in the state of Massachusetts.\n\n1971: The Joint Committee on Women in the Mathematical Sciences (JCW), was founded as a committee of the American Mathematical Society (AMS). It is now a joint committee of seven mathematical and statistical societies which works to identify mechanisms for the enhancement of opportunities for women in the mathematical and statistical sciences, recommend actions to the governing bodies of the member societies in support of these opportunities, and document its recommendations by presenting data.\n\n1973: Jean Taylor published her dissertation on “Regularity of the Singular Set of Two-Dimensional Area-Minimizing Flat Chains Modulo 3 in R3” which solved a long-standing problem about length and smoothness of soap-film triple function curves.\n\n1974: Joan Birman published the book \"Braids, Links, and Mapping Class Groups\". It has become a standard introduction, with many of today’s researchers having learned the subject through it.\n\n1975–1977: Marjorie Rice, who had no formal training in mathematics beyond high school, discovered three new types of tessellating pentagons and more than sixty distinct tessellations by pentagons.\n\n1975: Julia Robinson became the first female mathematician elected to the National Academy of Sciences.\n\n1979: Dorothy Lewis Bernstein became the first female president of the Mathematical Association of America.\n\n1979: Mary Ellen Rudin became the first woman to present the Earle Raymond Hedrick Lectures; these lectures were established by the Mathematical Association of America in 1952 to present to the Association a lecturer of known skill as an expositor of mathematics \"who will present a series of at most three lectures accessible to a large fraction of those who teach college mathematics.\"\n\n1981: Doris Schattschneider became the first female editor of \"Mathematics Magazine\", a refereed bimonthly publication of the Mathematical Association of America.\n\n1983: Julia Robinson became the first female president of the American Mathematical Society.\n\n1983: Julia Robinson became the first female mathematician to be awarded a MacArthur Fellowship.\n\n1988: Doris Schattschneider became the first woman to present the J. Sutherland Frame Lectures, which are presented at the summer meeting of the Mathematical Association of America.\n\n1992: Gloria Gilmer became the first woman to deliver a major National Association of Mathematicians lecture (it was the Cox-Talbot address).\n\n1995: Margaret Wright became the first female president of the Society for Industrial and Applied Mathematics.\n\n1996: Joan Birman became the first woman to receive the Chauvenet Prize, which is awarded annually by the Mathematical Association of America to the author of an outstanding expository article on a mathematical topic by a member of the association.\n\n1998: Melanie Wood became the first female American to make the U.S. International Math Olympiad Team. She won silver medals in the 1998 and 1999 International Mathematical Olympiads.\n\n2002: Melanie Wood became the first American woman and second woman overall to be named a Putnam Fellow in 2002. Putnam Fellows are the top five (or six, in case of a tie) scorers on The William Lowell Putnam Mathematical Competition.\n\n2004: Melanie Wood became the first woman to win the Frank and Brennie Morgan Prize for Outstanding Research in Mathematics by an Undergraduate Student. It is an annual award given to an undergraduate student in the US, Canada, or Mexico who demonstrates superior mathematics research.\n\n2004: Alison Miller became the first female gold medal winner on the U.S. International Math Olympiad Team.\n\n2006: Stefanie Petermichl, a German mathematical analyst then at the University of Texas at Austin, became the first woman to win the Salem Prize, an annual award given to young mathematicians considered to have done outstanding work in Raphael Salem's field of interest, primarily Fourier series and related areas in analysis. She shared the prize with Artur Avila.\n\nTimeline of women in mathematics\n\n"}
{"id": "51331209", "url": "https://en.wikipedia.org/wiki?curid=51331209", "title": "Traffic model", "text": "Traffic model\n\nA traffic model is a mathematical model of real-world traffic, usually, but not restricted to, road traffic. Traffic modeling draws heavily on theoretical foundations like network theory and certain theories from physics like the kinematic wave model. The interesting quantity being modeled and measured is the traffic flow, i.e. the throughput of mobile units (e.g. vehicles) per time and transportation medium capacity (e.g. road or lane width). Models can teach researchers and engineers how to ensure an optimal flow with a minimum number of traffic jams.\n\nTraffic models often are the basis of a traffic simulation.\n\n\n\n\n"}
{"id": "34666424", "url": "https://en.wikipedia.org/wiki?curid=34666424", "title": "Trombi–Varadarajan theorem", "text": "Trombi–Varadarajan theorem\n\nIn mathematics, the Trombi–Varadarajan theorem, introduced by , gives an isomorphism between a certain space of spherical functions on a semisimple Lie group, and a certain space of holomorphic functions defined on a tubular neighborhood of the dual of a Cartan subalgebra.\n"}
{"id": "4910264", "url": "https://en.wikipedia.org/wiki?curid=4910264", "title": "Umbilic torus", "text": "Umbilic torus\n\nThe umbilic torus or umbilic bracelet is a single-edged 3-dimensional shape. The lone edge goes three times around the ring before returning to the starting point. The shape also has a single external face. A cross section of the surface forms a deltoid.\n\nThe umbilic torus occurs in the mathematical subject of singularity theory, in particular in the classification of umbilical points which are determined by real cubic forms formula_1. The equivalence classes of such cubics form a three-dimensional real projective space and the subset of parabolic forms define a surface – the umbilic torus. Christopher Zeeman named this set the umbilic bracelet in 1976.\n\nThe torus is defined by the following set of parametric equations.\n\nJohn Robinson created a sculpture \"Eternity\" based on the shape in 1989, this had a triangular cross-section rather than a deltoid of a true Umbilic bracelet. This appeared on the cover of Geometric Differentiation by Ian R. Porteous.\n\nHelaman Ferguson has created a 27-inch (69 centimeters) bronze sculpture, \"Umbilic Torus\", and it is his most widely known piece of art. In 2010, it was announced that Jim Simons had commissioned an Umbilic Torus sculpture to be constructed outside the Math and Physics buildings at Stony Brook University, in proximity to the Simons Center for Geometry and Physics. The torus is made out of cast bronze, and is mounted on a stainless steel column. The total weight of the sculpture is 65 tonnes, and has a height of . The torus has a diameter of , the same diameter as the granite base. Various mathematical formulas defining the torus are inscribed on the base. Installation was completed in September, 2012.\n\n\n"}
{"id": "54863674", "url": "https://en.wikipedia.org/wiki?curid=54863674", "title": "Van Genuchten–Gupta model", "text": "Van Genuchten–Gupta model\n\nThe van Genuchten–Gupta model is an inverted S-curve applicable to crop yield and soil salinity relations.\n\nThe mathematical expression is:\n\nwhere Y = yield, Ym = maximum yield of the model, C = salt concentration of the soil, C = C value at 50% yield, P = an exponent to be found by optimization and maximizing the model's goodness of fit to the data.\n\nIn the figure: Ym = 3.1, C = 12.4, P = 3.75\n\nAs an alternative, the logistic S-function can be used.\n\nThe mathematical expression is:\n\nwhere:\n\nwith Y =Yield, Yn = minimum Y, Ym = maximum Y, X = salt concentration of the soil, while A, B and C are constants to be determined by optimization and maximizing the model's goodness of fit to the data.\n\nIf the minimum Yn=0 then the expression can be simplified to:\n\nIn the figure: Ym = 3.43, Yn = 0.47, A = 0.112, B = -3.16, C = 1.42.\n\nThe third degree or cubic regression also offers a useful alternative.\n\nThe equation reads:\n\nwith Y =Yield, X = salt concentration of the soil, while A, B, C and D are constants to be determined by the regression.\n\nIn the figure: A = 0.0017, B = 0.0604, C=0.3874, D = 2.3788. These values were calculated with Microsoft Excel\n\nThe curvature is more pronounced than in the other models.\n\n"}
{"id": "3603745", "url": "https://en.wikipedia.org/wiki?curid=3603745", "title": "Vertex configuration", "text": "Vertex configuration\n\nIn geometry, a vertex configuration is a shorthand notation for representing the vertex figure of a polyhedron or tiling as the sequence of faces around a vertex. For uniform polyhedra there is only one vertex type and therefore the vertex configuration fully defines the polyhedron. (Chiral polyhedra exist in mirror-image pairs with the same vertex configuration.)\n\nA vertex configuration is given as a sequence of numbers representing the number of sides of the faces going around the vertex. The notation \"a.b.c\" describes a vertex that has 3 faces around it, faces with \"a\", \"b\", and \"c\" sides.\n\nFor example, \"3.5.3.5\" indicates a vertex belonging to 4 faces, alternating triangles and pentagons. This vertex configuration defines the vertex-transitive icosidodecahedron. The notation is cyclic and therefore is equivalent with different starting points, so 3.5.3.5 is the same as 5.3.5.3. The order is important, so 3.3.5.5 is different from 3.5.3.5. (The first has two triangles followed by two pentagons.) Repeated elements can be collected as exponents so this example is also represented as (3.5).\n\nIt has variously been called a vertex description, vertex type, vertex symbol, vertex arrangement, vertex pattern, face-vector. It is also called a Cundy and Rollett symbol for its usage for the Archimedean solids in their 1952 book \"Mathematical Models\".\n\nA \"vertex configuration\" can also be represented as a polygonal vertex figure showing the faces around the vertex. This \"vertex figure\" has a 3-dimensional structure since the faces are not in the same plane for polyhedra, but for vertex-uniform polyhedra all the neighboring vertices are in the same plane and so this plane projection can be used to visually represent the vertex configuration.\n\nDifferent notations are used, sometimes with a comma (,) and sometimes a period (.) separator. The period operator is useful because it looks like a product and an exponent notation can be used. For example, 3.5.3.5 is sometimes written as (3.5).\n\nThe notation can also be considered an expansive form of the simple Schläfli symbol for regular polyhedra. The Schläfli notation {\"p\",\"q\"} means \"q\" \"p\"-gons around each vertex. So {\"p\",\"q\"} can be written as \"p.p.p...\" (\"q\" times) or \"p\". For example, an icosahedron is {3,5} = 3.3.3.3.3 or 3.\n\nThis notation applies to polygonal tilings as well as polyhedra. A planar vertex configuration denotes a uniform tiling just like a nonplanar vertex configuration denotes a uniform polyhedron.\n\nThe notation is ambiguous for chiral forms. For example, the snub cube has clockwise and counterclockwise forms which are identical across mirror images. Both have a 3.3.3.3.4 vertex configuration.\n\nThe notation also applies for nonconvex regular faces, the star polygons. For example, a pentagram has the symbol {5/2}, meaning it has 5 sides going around the centre twice.\n\nFor example, there are 4 regular star polyhedra with regular polygon or star polygon vertex figures. The small stellated dodecahedron has the Schläfli symbol of {5/2,5} which expands to an explicit vertex configuration 5/2.5/2.5/2.5/2.5/2 or combined as (5/2). The great stellated dodecahedron, {5/2,3} has a triangular vertex figure and configuration (5/2.5/2.5/2) or (5/2). The great dodecahedron, {5,5/2} has a pentagrammic vertex figure, with \"vertex configuration\" is (5.5.5.5.5)/2 or (5)/2. A great icosahedron, {3,5/2} also has a pentagrammic vertex figure, with vertex configuration (3.3.3.3.3)/2 or (3)/2.\nFaces on a vertex figure are considered to progress in one direction. Some uniform polyhedra have vertex figures with inversions where the faces progress retrograde. A vertex figure represents this in the star polygon notation of sides \"p/q\" such that \"p\"<2\"q\", where \"p\" is the number of sides and \"q\" the number of turns around a circle. For example, \"3/2\" means a triangle that has vertices that go around twice, which is the same as backwards once. Similarly \"5/3\" is a backwards pentagram 5/2.\n\nSemiregular polyhedra have vertex configurations with positive angle defect.\n\nNOTE: The vertex figure can represent a regular or semiregular tiling on the plane if its defect is zero. It can represent a tiling of the hyperbolic plane if its defect is negative.\n\nFor uniform polyhedra, the angle defect can be used to compute the number of vertices. Descartes' theorem states that all the angle defects in a topological sphere must sum to 4\"π\" radians or\n720 degrees.\n\nSince uniform polyhedra have all identical vertices, this relation allows us to compute the number of vertices, which is 4\"π\"/\"defect\" or\n720/\"defect\".\n\nExample: A truncated cube 3.8.8 has an angle defect of 30 degrees. Therefore, it has\n\nIn particular it follows that {\"a\",\"b\"} has vertices.\n\nEvery enumerated vertex configuration potentially uniquely defines a semiregular polyhedron. However, not all configurations are possible.\n\nTopological requirements limit existence. Specifically \"p.q.r\" implies that a \"p\"-gon is surrounded by alternating \"q\"-gons and \"r\"-gons, so either \"p\" is even or \"q\" equals \"r\". Similarly \"q\" is even or \"p\" equals \"r\", and \"r\" is even or \"p\" equals \"q\". Therefore, potentially possible triples are 3.3.3, 3.4.4, 3.6.6, 3.8.8, 3.10.10, 3.12.12, 4.4.\"n\" (for any \"n\">2), 4.6.6, 4.6.8, 4.6.10, 4.6.12, 4.8.8, 5.5.5, 5.6.6, 6.6.6. In fact, all these configurations with three faces meeting at each vertex turn out to exist.\n\nThe number in parentheses is the number of vertices, determined by the angle defect.\n\n\n\n\n\nThe uniform dual or Catalan solids, including the bipyramids and trapezohedra, are \"vertically-regular\" (face-transitive) and so they can be identified by a similar notation which is sometimes called face configuration. Cundy and Rollett prefixed these dual symbols by a \"V\". In contrast, \"Tilings and Patterns\" uses square brackets around the symbol for isohedral tilings.\n\nThis notation represents a sequential count of the number of faces that exist at each vertex around a face. For example, V3.4.3.4 or V(3.4) represents the rhombic dodecahedron which is face-transitive: every face is a rhombus, and alternating vertices of the rhombus contain 3 or 4 faces each.\n\nSpherical\n\n\nRegular\n\n\nSemi-regular\n\n\nHyperbolic\n\n\n\n\n\n\n\n"}
{"id": "42596301", "url": "https://en.wikipedia.org/wiki?curid=42596301", "title": "WRF-SFIRE", "text": "WRF-SFIRE\n\nWRF-SFIRE is a coupled atmosphere-wildfire model, which combines the Weather Research and Forecasting Model (WRF) with a fire-spread model, implemented by the level-set method. A version from 2010 was released based on the WRF 3.2 as WRF-Fire.\n\n\n"}
