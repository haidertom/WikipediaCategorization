{"id": "655", "url": "https://en.wikipedia.org/wiki?curid=655", "title": "Abacus", "text": "Abacus\n\nThe abacus (\"plural\" abaci or abacuses), also called a counting frame, is a calculating tool that was in use in Europe, China and Russia, centuries before the adoption of the written Hindu–Arabic numeral system. The exact origin of the abacus is still unknown. Today, abacuses are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal.\n\nAbacuses come in different designs. Some designs, like the bead frame consisting of beads divided into tens, are used mainly to teach arithmetic, although they remain popular in the post-Soviet states as a tool. Other designs, such as the Japanese soroban, have been used for practical calculations even involving several digits. For any particular abacus design, there usually are numerous different methods to perform a certain type of calculation, which may include basic operations like addition and multiplication, or even more complex ones, such as calculating square roots. Some of these methods may work with non-natural numbers (numbers such as and ).\n\nAlthough today many use calculators and computers instead of abacuses to calculate, abacuses still remain in common use in some countries. Merchants, traders and clerks in some parts of Eastern Europe, Russia, China and Africa use abacuses, and they are still used to teach arithmetic to children. Some people who are unable to use a calculator because of visual impairment may use an abacus.\n\nThe use of the word \"abacus\" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus. The Latin word came from Greek ἄβαξ \"abax\" which means something without base, and improperly, any piece of rectangular board or plank. \nAlternatively, without reference to ancient texts on etymology, it has been suggested that it means \"a square tablet strewn with dust\", or \"drawing-board covered with dust (for the use of mathematics)\" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ἄβακoς \"abakos\"). Whereas the table strewn with dust definition is popular, there are those that do not place credence in this at all and in fact state that it is not proven. Greek ἄβαξ itself is probably a borrowing of a Northwest Semitic, perhaps Phoenician, word akin to Hebrew \"ʾābāq\" (אבק), \"dust\" (or in post-Biblical sense meaning \"sand used as a writing surface\").\n\nThe preferred plural of \"abacus\" is a subject of disagreement, with both \"abacuses\" and \"abaci\" (hard \"c\") in use. The user of an abacus is called an \"abacist\".\n\nThe period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.\n\nSome scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Old Babylonian scholars such as Carruccio that Old Babylonians \"may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations\".\n\nThe use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered.\n\nDuring the Achaemenid Empire, around 600 BC the Persians first began to use the abacus. Under the Parthian, Sassanian and Iranian empires, scholars concentrated on exchanging knowledge and inventions with the countries around them – India, China, and the Roman Empire, when it is thought to have been exported to other countries.\n\nThe earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. Also Demosthenes (384 BC–322 BC) talked of the need to use pebbles for calculations too difficult for your head. A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius mention men that sometimes stood for more and sometimes for less, like the pebbles on an abacus. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.\n\nA tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet), dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line. Also from this time frame the \"Darius Vase\" was unearthed in 1851. It was covered with pictures including a \"treasurer\" holding a wax tablet in one hand while manipulating counters on a table with the other.\n\nThe earliest known written documentation of the Chinese abacus dates to the 2nd century BC.\n\nThe Chinese abacus, known as the \"suanpan\" (算盤, lit. \"calculating tray\"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam; beads moved toward the beam are counted, while those moved away from it are not. The \"suanpan\" can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.\n\n\"Suanpan\" can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it.\n\nIn the long scroll \"Along the River During the Qingming Festival\" painted by Zhang Zeduan during the Song dynasty (960–1297), a \"suanpan\" is clearly visible beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).\n\nThe similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abacuses may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard \"suanpan\" has 5 plus 2. (Incidentally, this allows use with a hexadecimal numeral system, which was used for traditional Chinese measures of weight.) Instead of running on wires as in the Chinese, Korean, and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.\n\nAnother possible source of the \"suanpan\" is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang dynasty (618–907) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.\n\nThe normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles (\"calculi\") were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century. Due to Pope Sylvester II's reintroduction of the abacus with modifications, it became widely used in Europe once again during the 11th century This abacus used beads on wires, unlike the traditional Roman counting boards, which meant the abacus could be used much faster.\n\nWriting in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.\n\nOne example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, related to the Roman numerals. The short grooves on the right may have been used for marking Roman \"ounces\" (i.e. fractions).\n\nThe decimal number system invented in India replaced the abacus in Western Europe.\n\nThe \"Abhidharmakośabhāṣya\" of Vasubandhu (316-396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that \"placing a wick (Sanskrit \"vartikā\") on the number one (\"ekāṅka\") means it is a one, while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is a thousand\". It is unclear exactly what this arrangement may have been. Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus. Hindu texts used the term \"śūnya\" (zero) to indicate the empty column on the abacus.\n\nIn Japanese, the abacus is called \"soroban\" (, lit. \"Counting tray\"), imported from China in the 14th century. It was probably in use by the working class a century or more before the ruling class started, as the class structure did not allow for devices used by the lower class to be adopted or used by the ruling class. The 1/4 abacus, which is suited to decimal calculation, appeared circa 1930, and became widespread as the Japanese abandoned hexadecimal weight calculation which was still common in China. The abacus is still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation. Using visual imagery of a soroban, one can arrive at the answer in the same time as, or even faster than, is possible with a physical instrument.\n\nThe Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it \"jupan\" (주판), \"supan\" (수판) or \"jusan\" (주산).\n\nSome sources mention the use of an abacus called a \"nepohualtzintzin\" in ancient Aztec culture. This Mesoamerican abacus used a 5-digit base-20 system.\nThe word Nepōhualtzintzin comes from Nahuatl and it is formed by the roots; \"Ne\" – personal -; \"pōhual\" or \"pōhualli\" – the account -; and \"tzintzin\" – small similar elements. Its complete meaning was taken as: counting with small similar elements by somebody. Its use was taught in the Calmecac to the \"temalpouhqueh\" , who were students dedicated to take the accounts of skies, from childhood.\n\nThe Nepōhualtzintzin was divided in two main parts separated by a bar or intermediate cord. In the left part there were four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.\n\nAltogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nepōhualtzintzin. This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens. One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximate a year (1 days short). When translated into modern computer arithmetic, the Nepōhualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed.\n\nThe rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his wanderings throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them made in gold, jade, encrustations of shell, etc. There have also been found very old Nepōhualtzintzin attributed to the Olmec culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.\n\nGeorge I. Sanchez, \"Arithmetic in Maya\", Austin-Texas, 1961 found another base 5, base 4 abacus in the Yucatán Peninsula that also computed calendar data. This was a finger abacus, on one hand 0, 1, 2, 3, and 4 were used; and on the other hand 0, 1, 2 and 3 were used. Note the use of zero at the beginning and end of the two cycles. Sanchez worked with Sylvanus Morley, a noted Mayanist.\n\nThe quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (Quechua for \"counting tool\"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed by Italian mathematician Nicolino De Pasquale. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at a minimum.\n\nThe Russian abacus, the \"schoty\" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire, usually positioned near the user, with four beads for quarter-ruble fractions). Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916. The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different color from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.\n\nAs a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. The Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. Today it is regarded as an archaism and replaced by the handheld calculator.\n\nThe Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid. The Turks and the Armenian people also used abacuses similar to the Russian schoty. It was named a \"coulba\" by the Turks and a \"choreb\" by the Armenians.\n\nAround the world, abacuses have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.\n\nIn Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy.\n\nThe wire frame may be used either with positional notation like other abacuses (thus the 10-wire version may represent numbers up to 9,999,999,999), or each bead may represent one unit (so that e.g. 74 can be represented by shifting all beads on 7 wires and 4 beads on the 8th wire, so numbers up to 100 may be represented). In the bead frame shown, the gap between the 5th and 6th wire, corresponding to the color change between the 5th and the 6th bead on each wire, suggests the latter use.\n\nThe red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons. The twenty bead version, referred to by its Dutch name \"rekenrek\" (\"calculating frame\"), is often used, sometimes on a string of beads, sometimes on a rigid framework.\n\nBy learning how to calculate with abacus, one can improve his mental calculation which becomes faster and more accurate in doing large number calculations. Abacus‐based mental calculation (AMC) was derived from the abacus which means doing calculation, including addition, subtraction, multiplication, and division, in mind with an imaged abacus. It is a high-level cognitive skill that run through calculations with an effective algorithm. People doing long-term AMC training shows higher numerical memory capacity and has more effectively connected neural pathways. They are able to retrieve memory to deal with complex processes to calculate. The processing of AMC involves both the visuospatial and visuomotor processing which generate the visual abacus and perform the movement of the imagery bead. Since the only thing needed to be remembered is the finial position of beads, it takes less memory and less computation time.\n\nAn adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cube root.\n\nAlthough blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.\n\nThe binary abacus is used to explain how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an \"on\" or \"off\" position.\n\n\n\n\n"}
{"id": "12137424", "url": "https://en.wikipedia.org/wiki?curid=12137424", "title": "Akamai Foundation", "text": "Akamai Foundation\n\nThe Akamai Foundation is a private corporate foundation dedicated to fostering excellence in mathematics, with the aim of promoting math’s importance and encouraging America’s next generation of technology innovators.\n\nThe Akamai Foundation is a core tenet of Akamai’s overarching commitment to corporate responsibility. The company has a long history of supporting programs designed to attract more diversity to the technology industry through initiatives such as Akamai Technical Academy and Girls Who Code; providing disaster relief and humanitarian aid globally; enabling volunteerism by connecting employees to the communities in which Akamai operates; and promoting environmental sustainability through investments in alternative energy.\n"}
{"id": "1118832", "url": "https://en.wikipedia.org/wiki?curid=1118832", "title": "Arbitrarily large", "text": "Arbitrarily large\n\nIn mathematics, the phrases arbitrarily large, arbitrarily small, and arbitrarily long are used in statements such as:\n\nwhich is shorthand for:\n\n\"Arbitrarily large\" is not equivalent to \"sufficiently large\". For instance, while it is true that prime numbers can be arbitrarily large since there are an infinite number of them, it is not true that all sufficiently large numbers are prime. \"Arbitrarily large\" does not mean \"infinitely large\" because although prime numbers can be arbitrarily large, an infinitely large prime does not exist since all prime numbers (as well as all other integers) are finite.\n\nIn some cases, phrases such as \"P(\"x\") is true for arbitrarily large \"x\"\" are used primarily for emphasis, as in \"P(\"x\") is true for all \"x\", no matter how large \"x\" is.\" In these cases, the phrase \"arbitrarily large\" does not have the meaning indicated above but is in fact logically synonymous with \"all.\"\n\nTo say that there are \"arbitrarily long arithmetic progressions of prime numbers\" does not mean that there exists any infinitely long arithmetic progression of prime numbers (there is not), nor that there exists any particular arithmetic progression of prime numbers that is in some sense \"arbitrarily long\", but rather that no matter how large a number \"n\" is, there exists some arithmetic progression of prime numbers of length at least \"n\".\n\nThe statement \"ƒ(\"x\") is non-negative for arbitrarily large \"x\".\" could be rewritten as:\n\nUsing \"sufficiently large\" instead yields:\n\n"}
{"id": "20459000", "url": "https://en.wikipedia.org/wiki?curid=20459000", "title": "Ars inveniendi", "text": "Ars inveniendi\n\nArs inveniendi (Latin for \"art of invention\") is a chief notion of mathesis universalis and implies ascertaining truth through the use of mathematics.\n"}
{"id": "54391334", "url": "https://en.wikipedia.org/wiki?curid=54391334", "title": "Banach lattice", "text": "Banach lattice\n\nIn mathematics, specifically in functional analysis and order theory, a Banach lattice formula_1 is a Riesz space with a norm formula_2 such that formula_3 is a Banach space and for all formula_4 the implication formula_5 holds, where as usual formula_6.\n\n\n"}
{"id": "57184342", "url": "https://en.wikipedia.org/wiki?curid=57184342", "title": "Basin-hopping", "text": "Basin-hopping\n\nIn applied mathematics, Basin-hopping is a global optimization technique that iterates by performing random perturbation of coordinates, performing local optimization, and accepting or rejecting new coordinates based on a minimized function value. The algorithm was described in 1997 by David J. Wales and Jonathan Doye.\n"}
{"id": "35764367", "url": "https://en.wikipedia.org/wiki?curid=35764367", "title": "Bounded growth", "text": "Bounded growth\n\nBounded growth occurs when the growth rate of a mathematical function is constantly increasing at a decreasing rate. Asymptotically, bounded growth approaches a fixed value.\nThis contrasts with exponential growth, which is constantly increasing at an accelerating rate, and therefore approaches infinity in the limit.\n\nAn example of bounded growth is the logistic function.\n\n"}
{"id": "515096", "url": "https://en.wikipedia.org/wiki?curid=515096", "title": "Canonical form", "text": "Canonical form\n\nIn mathematics and computer science, a canonical, normal, or standard form of a mathematical object is a standard way of presenting that object as a mathematical expression. The distinction between \"canonical\" and \"normal\" forms varies by subfield. In most fields, a canonical form specifies a \"unique\" representation for every object, while a normal form simply specifies its form, without the requirement of uniqueness.\n\nThe canonical form of a positive integer in decimal representation is a finite sequence of digits that does not begin with zero.\n\nMore generally, for a class of objects on which an equivalence relation is defined, a canonical form consists in the choice of a specific object in each class. For example, Jordan normal form is a canonical form for matrix similarity, and the row echelon form is a canonical form, when one considers as equivalent a matrix and its left product by an invertible matrix.\n\nIn computer science, and more specifically in computer algebra, when representing mathematical objects in a computer, there are usually many different ways to represent the same object. In this context, a canonical form is a representation such that every object has a unique representation. Thus, the equality of two objects can easily be tested by testing the equality of their canonical forms. However canonical forms frequently depend on arbitrary choices (like ordering the variables), and this introduces difficulties for testing the equality of two objects resulting on independent computations. Therefore, in computer algebra, \"normal form\" is a weaker notion: A normal form is a representation such that zero is uniquely represented. This allows testing for equality by putting the difference of two objects in normal form.\n\nCanonical form can also mean a differential form that is defined in a natural (canonical) way.\n\nIn computer science, data that has more than one possible representation can often be canonicalized into a completely unique representation called its canonical form. Putting something into canonical form is canonicalization.\n\nSuppose we have some set \"S\" of objects, with an equivalence relation \"R\". A canonical form is given by designating some objects of \"S\" to be \"in canonical form\", such that every object under consideration is equivalent to exactly one object in canonical form. In other words, the canonical forms in \"S\" represent the equivalence classes, once and only once. To test whether two objects are equivalent, it then suffices to test their canonical forms for equality.\nA canonical form thus provides a classification theorem and more, in that it not just classifies every class, but gives a distinguished (canonical) representative.\n\nFormally, a canonicalization with respect to an equivalence relation \"R\" on a set \"S\" is a mapping \"c\":\"S\"→\"S\" such that for all \"s\", \"s\", \"s\" ∈ \"S\":\nProperty 3 is redundant, it follows by applying 2 to 1.\n\nIn practical terms, one wants to be able to recognize the canonical forms. There is also a practical, algorithmic question to consider: how to pass from a given object \"s\" in \"S\" to its canonical form \"s\"*? Canonical forms are generally used to make operating with equivalence classes more effective. For example, in modular arithmetic, the canonical form for a residue class is usually taken as the least non-negative integer in it. Operations on classes are carried out by combining these representatives and then reducing the result to its least non-negative residue.\nThe uniqueness requirement is sometimes relaxed, allowing the forms to be unique up to some finer equivalence relation, like allowing reordering of terms (if there is no natural ordering on terms).\n\nA canonical form may simply be a convention, or a deep theorem.\n\nFor example, polynomials are conventionally written with the terms in descending powers: it is more usual to write \"x\" + \"x\" + 30 than \"x\" + 30 + \"x\", although the two forms define the same polynomial. By contrast, the existence of Jordan canonical form for a matrix is a deep theorem.\n\nNote: in this section, \"up to\" some equivalence relation E means that the canonical form is not unique in general, but that if one object has two different canonical forms, they are E-equivalent.\n\n\n\nIn analytic geometry:\n\nBy contrast, there are alternative forms for writing equations. For example, the equation of a line may be written as a linear equation in point-slope and slope-intercept form.\n\nConvex polyhedra can be put into canonical form such that:\n\nStandard form is used by many mathematicians and scientists to write extremely large numbers in a more concise and understandable way.\n\n\n\n\n\n\n\nIn graph theory, a branch of mathematics, graph canonization is the problem finding a canonical form of a given graph \"G\". A canonical form is a labeled graph Canon(\"G\") that is isomorphic to \"G\", such that every graph that is isomorphic to \"G\" has the same canonical form as \"G\". Thus, from a solution to the graph canonization problem, one could also solve the problem of graph isomorphism: to test whether two graphs \"G\" and \"H\" are isomorphic, compute their canonical forms Canon(\"G\") and Canon(\"H\"), and test whether these two canonical forms are identical.\n\nCanonical differential forms include the canonical one-form and canonical symplectic form, important in the study of Hamiltonian mechanics and symplectic manifolds.\n\nIn computing, the reduction of data to any kind of canonical form is commonly called \"data normalization\".\n\nFor instance, database normalization is the process of organizing the fields and tables of a relational database to minimize redundancy and dependency. \n\nIn the field of software security, a common vulnerability is unchecked malicious input. The mitigation for this problem is proper input validation. Before input validation may be performed, the input must be normalized, i.e., eliminating encoding (for instance HTML encoding) and reducing the input data to a single common character set.\n\nOther forms of data, typically associated with signal processing (including audio and imaging) or machine learning, can be normalized in order to provide a limited range of values.\n\n\n"}
{"id": "1401020", "url": "https://en.wikipedia.org/wiki?curid=1401020", "title": "Christoffel symbols", "text": "Christoffel symbols\n\nIn mathematics and physics, the Christoffel symbols are an array of numbers describing a metric connection. The metric connection is a specialization of the affine connection to surfaces or other manifolds endowed with a metric, allowing distances to be measured on that surface. In differential geometry, an affine connection can be defined without any reference to a metric, and many additional concepts follow: parallel transport, covariant derivatives, geodesics, etc. also do not require the concept of a metric. However, when a metric is available, these concepts can be directly tied to the \"shape\" of the manifold itself; that shape is determined by how the tangent space is attached to the cotangent space by the metric tensor. Abstractly, one would say that the manifold has an associated (orthonormal) frame bundle, with each \"frame\" being a possible choice of a coordinate frame. An invariant metric implies that the structure group of the frame bundle is the orthogonal group . As a result, such a manifold is necessarily a (pseudo-)Riemannian manifold. The Christoffel symbols provide a concrete representation of the connection of (pseudo-)Riemannian geometry in terms of coordinates on the manifold. Additional concepts, such as parallel transport, geodesics, etc. can then be expressed in terms of Christoffel symbols.\n\nIn general, there are an infinite number of metric connections for a given metric tensor; however, there is one, unique connection, the Levi-Civita connection, that is free of any torsion. It is very common in physics and general relativity to work almost exclusively with the Levi-Civita connection, by working in coordinate frames (called holonomic coordinates) where the torsion vanishes. For example, in Euclidean spaces, the Christoffel symbols describe how the local coordinate bases change from point to point.\n\nAt each point of the underlying -dimensional manifold, for any local coordinate system around that point, the Christoffel symbols are denoted for . Each entry of this array is a real number. Under \"linear\" coordinate transformations on the manifold, the Christoffel symbols transform like the components of a tensor, but under general coordinate transformations (diffeomorphisms) they do not. Most of the algebraic properties of the Christoffel symbols follow from their relationship to the affine connection; only a few follow from the fact that the structure group is the orthogonal group (or the Lorentz group for general relativity).\n\nChristoffel symbols are used for performing practical calculations. For example, the Riemann curvature tensor can be expressed entirely in terms of the Christoffel symbols and their first partial derivatives. In general relativity, the connection plays the role of the gravitational force field with the corresponding gravitational potential being the metric tensor. When the coordinate system and the metric tensor share some symmetry, many of the are zero.\n\nThe Christoffel symbols are named for Elwin Bruno Christoffel (1829–1900).\n\nThe definitions given below are valid for both Riemannian manifolds and pseudo-Riemannian manifolds, such as those of general relativity, with careful distinction being made between upper and lower indices (contra-variant and co-variant indices). The formulas hold for either sign convention, unless otherwise noted.\n\nEinstein summation convention is used in this article, with vectors indicated by bold font. The connection coefficients of the Levi-Civita connection (or pseudo-Riemannian connection) expressed in a coordinate basis are called \"Christoffel symbols\".\n\nGiven a coordinate system for on an -manifold , the tangent vectors\nwhere is the position vector, define what is referred to as the local basis of the tangent space to at each point of its domain. These can be used to define the metric tensor:\n\nand its inverse:\n\nwhich can in turn be used to define the dual basis:\n\nIn Euclidean space, the general definition given below for the Christoffel symbols of the second kind can be proven to be equivalent to:\n\nChristoffel symbols of the first kind can then be found via index juggling:\n\nRearranging, we see that:\n\nIn words, the arrays represented by the Christoffel symbols track how the basis changes from point to point. Symbols of the second kind decompose the change with respect to the basis, while symbols of the first kind decompose it with respect to the dual basis. These expressions fail as definitions when such decompositions are not possible - in particular, when the direction of change does not lie in the tangent space, which can occur on a curved surface. In this form, it easy to see the symmetry of the lower or last two indices:\n\nformula_8 and formula_9,\n\nfrom the definition of formula_10 and the fact that partial derivatives commute (as long as the manifold and coordinate system are well behaved).\n\nThe same numerical values for Christoffel symbols of the second kind also relate to derivatives of the dual basis, as seen in the expression:\n\nwhich we can rearrange as:\n\nThe Christoffel symbols of the first kind can be derived either from the Christoffel symbols of the second kind and the metric,\nor from the metric alone,\n\nAs an alternative notation one also finds\n\nIt is worth noting that .\n\nThe Christoffel symbols of the second kind are the connection coefficients—in a coordinate basis—of the Levi-Civita connection, and since this connection has zero torsion, then in this basis the connection coefficients are symmetric, i.e., . For this reason, a torsion-free connection is often called \"symmetric\".\n\nIn other words, the Christoffel symbols of the second kind\nholds, where is the Levi-Civita connection on taken in the coordinate direction (i.e., ) and where is a local coordinate (holonomic) basis.\n\nThe Christoffel symbols can be derived from the vanishing of the covariant derivative of the metric tensor :\n\nAs a shorthand notation, the nabla symbol and the partial derivative symbols are frequently dropped, and instead a semicolon and a comma are used to set off the index that is being used for the derivative. Thus, the above is sometimes written as\n\nUsing that the symbols are symmetric in the lower two indices, one can solve explicitly for the Christoffel symbols as a function of the metric tensor by permuting the indices and resumming:\n\nwhere is the inverse of the matrix , defined as (using the Kronecker delta, and Einstein notation for summation) . Although the Christoffel symbols are written in the same notation as tensors with index notation, they are not tensors,\nsince they do not transform like tensors under a change of coordinates.\n\nThe Christoffel symbols are most typically defined in a coordinate basis, which is the convention followed here. In other words, the name Christoffel symbols is reserved only for coordinate (i.e., holonomic) frames. However, the connection coefficients can also be defined in an arbitrary (i.e., nonholonomic) basis of tangent vectors by\nExplicitly, in terms of the metric tensor, this is\n\nwhere are the commutation coefficients of the basis; that is,\nwhere are the basis vectors and is the Lie bracket. The standard unit vectors in spherical and cylindrical coordinates furnish an example of a basis with non-vanishing commutation coefficients. The difference between the connection in such a frame, and the Levi-Civita connection is known as the contorsion tensor.\n\nWhen we choose the basis orthonormal: then . This implies that\nand the connection coefficients become antisymmetric in the first two indices:\nwhere\n\nIn this case, the connection coefficients are called the Ricci rotation coefficients.\n\nEquivalently, one can define Ricci rotation coefficients as follows:\nwhere is an orthonormal nonholonomic basis and its \"co-basis\".\n\nLet and be vector fields with components and . Then the th component of the covariant derivative of with respect to is given by\n\nHere, the Einstein notation is used, so repeated indices indicate summation over indices and contraction with the metric tensor serves to raise and lower indices:\n\nKeep in mind that and that , the Kronecker delta. The convention is that the metric tensor is the one with the lower indices; the correct way to obtain from is to solve the linear equations .\n\nThe statement that the connection is torsion-free, namely that\nis equivalent to the statement that—in a coordinate basis—the Christoffel symbol is symmetric in the lower two indices:\n\nThe index-less transformation properties of a tensor are given by pullbacks for covariant indices, and pushforwards for contravariant indices. The article on covariant derivatives provides additional discussion of the correspondence between index-free notation and indexed notation.\n\nThe covariant derivative of a vector field is\n\nThe covariant derivative of a scalar field is just\n\nand the covariant derivative of a covector field is\n\nThe symmetry of the Christoffel symbol now implies\n\nfor any scalar field, but in general the covariant derivatives of higher order tensor fields do not commute (see curvature tensor).\n\nThe covariant derivative of a type (2,0) tensor field is\n\nthat is,\n\nIf the tensor field is mixed then its covariant derivative is\n\nand if the tensor field is of type then its covariant derivative is\n\nTo find the contravariant derivative of a vector field, we must first transform \nit into a covariant derivative using the metric tensor\n\nUnder a change of variable from to , vectors transform as\n\nand so\n\nwhere the overline denotes the Christoffel symbols in the coordinate system. Note that the Christoffel symbol does not transform as a tensor, but rather as an object in the jet bundle. More precisely, the Christoffel symbols can be considered as functions on the jet bundle of the frame bundle of , independent of any local coordinate system. Choosing a local coordinate system determines a local section of this bundle, which can then be used to pull back the Christoffel symbols to functions on , though of course these functions then depend on the choice of local coordinate system.\n\nAt each point, there exist coordinate systems in which the Christoffel symbols vanish at the point. These are called (geodesic) normal coordinates, and are often used in Riemannian geometry.\n\nThe Christoffel symbols find frequent use in Einstein's theory of general relativity, where spacetime is represented by a curved 4-dimensional Lorentz manifold with a Levi-Civita connection. The Einstein field equations—which determine the geometry of spacetime in the presence of matter—contain the Ricci tensor, and so calculating the Christoffel symbols is essential. Once the geometry is determined, the paths of particles and light beams are calculated by solving the geodesic equations in which the Christoffel symbols explicitly appear.\n\n\n"}
{"id": "56260115", "url": "https://en.wikipedia.org/wiki?curid=56260115", "title": "Clearing denominators", "text": "Clearing denominators\n\nIn mathematics, the method of clearing denominators, also called clearing fractions, is a technique for simplifying an equation equating two expressions that each are a sum of rational expressions – which includes simple fractions.\n\nConsider the equation\n\nThe smallest common multiple of the two denominators 6 and 15\"z\" is 30\"z\", so one multiplies both sides by 30\"z\":\n\nThe result is an equation with no fractions.\n\nThe simplified equation is not entirely equivalent to the original. For when we substitute and in the last equation, both sides simplify to 0, so we get , a mathematical truth. But the same substitution applied to the original equation results in , which is mathematically meaningless.\n\nWithout loss of generality, we may assume that the right-hand side of the equation is 0, since an equation may equivalently be rewritten in the form .\n\nSo let the equation have the form\n\nThe first step is to determine a common denominator of these fractions – preferably the least common denominator, which is the least common multiple of the .\n\nThis means that each is a factor of , so for some expression that is not a fraction. Then\n\nprovided that does not assume the value 0 – in which case also equals 0.\n\nSo we have now\n\nProvided that does not assume the value 0, the latter equation is equivalent with\n\nin which the denominators have vanished.\n\nAs shown by the provisos, care has to be taken not to introduce zeros of – viewed as a function of the unknowns of the equation – as spurious solutions.\n\nConsider the equation\n\nThe least common denominator is .\n\nFollowing the method as described above results in\n\nSimplifying this further gives us the solution .\n\nIt is easily checked that none of the zeros of – namely , , and – is a solution of the final equation, so no spurious solutions were introduced.\n"}
{"id": "13673454", "url": "https://en.wikipedia.org/wiki?curid=13673454", "title": "Conference Board of the Mathematical Sciences", "text": "Conference Board of the Mathematical Sciences\n\nThe Conference Board of the Mathematical Sciences (CBMS) is an umbrella organization of seventeen professional societies in the mathematical sciences in the United States. \nIt and its member societies are recognized by the International Mathematical Union as the national mathematical societies for their country.\n\nThe CBMS was founded in 1960 as the successor organization to the six-organization Policy Committee for Mathematics (founded by the American Mathematical Society and the Mathematical Association of America as the War Policy Committee in 1942) and the 1958 Conference Organization of the Mathematical Sciences. As well as representing US mathematics at the IMU, it acts as a communication channel between its member societies and the US Government, and coordinates joint projects of its member societies.\n\n\n"}
{"id": "57574496", "url": "https://en.wikipedia.org/wiki?curid=57574496", "title": "Contracted Bianchi identities", "text": "Contracted Bianchi identities\n\nIn general relativity and tensor calculus, the contracted Bianchi identities are:\n\nwhere formula_2 is the Ricci tensor, formula_3 the scalar curvature, and formula_4 indicates covariant differentiation.\n\nA proof can be found in the entry Proofs involving covariant derivatives.\n\nThese identities are named after Luigi Bianchi, although they had been already derived by Aurel Voss in 1880.\n\n\n"}
{"id": "20644971", "url": "https://en.wikipedia.org/wiki?curid=20644971", "title": "Count On", "text": "Count On\n\nCount On is a major mathematics education project in the United Kingdom which was announced by education secretary David Blunkett at the end of 2000. It was the follow-on to Maths Year 2000 which was the UK's contribution to UNICEF's World Mathematical Year.\n\nCount On had two main strands:\n\nThe MathFests were run largely by MatheMagic and the University of York.\n\nThe project has now been handed over to the NCETM.\n\n\"Count On\" and \"Maths Year 2000\" were some of the first big Popularisation of Mathematics projects. Others are listed below.\n\n"}
{"id": "47824717", "url": "https://en.wikipedia.org/wiki?curid=47824717", "title": "Echo removal", "text": "Echo removal\n\nEcho removal is the process of removing echo and reverberation artifacts from audio signals. The reverberation is typically modeled as the convolution of a (sometimes time-varying) impulse response with a hypothetical clean input signal, where both the clean input signal (which is to be recovered) and the impulse response are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the input signal to uniquely determine a plausible original image, making it an ill-posed problem. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions.\n\nThis problem is analogous to deblurring in the image processing domain.\n\n"}
{"id": "2625993", "url": "https://en.wikipedia.org/wiki?curid=2625993", "title": "Erdős–Bacon number", "text": "Erdős–Bacon number\n\nA person's Erdős–Bacon number is the sum of one's Erdős number—which measures the \"collaborative distance\" in authoring academic papers between that person and Hungarian mathematician Paul Erdős—and one's Bacon number—which represents the number of links, through roles in films, by which the individual is separated from American actor Kevin Bacon. The lower the number, the closer a person is to Erdős and Bacon, which reflects a small world phenomenon in academia and entertainment.\n\nThe combined Erdős/Bacon [sic] number was introduced by mathematicians Tim Hsu and David Grabiner sometime before late-January 1999, when they pointed out that Daniel Kleitman has a combined number of 3: a Bacon number of 2 and Erdős number of 1.\n\nTo have a defined Erdős–Bacon number, it is necessary to have both appeared in a film and co-authored an academic paper, although this in and of itself is not sufficient.\nMathematician Daniel Kleitman has the Erdős–Bacon number of 3; it is the lowest among scientists: he is a co-author of Erdős on multiple papers, and has a Bacon number of 2, via Minnie Driver in \"Good Will Hunting\".\n\nMathematician Ken Ono has an Erdős–Bacon number of 4; 2 for Erdős and 2 for Bacon.\n\nMathematician Doron Zeilberger has an Erdős-Bacon number of 5. Computer scientist Tom Porter also has an Erdős-Bacon number of 5; 3 for Erdős in two ways and 2 for Bacon.\n\nAstronomer Carl Sagan has an Erdős number of 4 (via Steven J. Ostro) and a Bacon number of 2 (Sagan and Bacon having appeared with Johnny Carson on episodes of \"The Tonight Show\"), for a total of 6. Physicist Richard Feynman has an Erdős number of 3, and a Bacon number of 3, having appeared in the film \"Anti-Clock\" alongside Tony Tang. Geneticist Jonathan Pritchard appeared in the 1998 movie \"Without Limits\" which gives him a Bacon number of 2. Pritchard has an Erdős number of 4 thus giving him an Erdős–Bacon number of 6. Theoretical physicist Stephen Hawking has an Erdős–Bacon number of 6: his Bacon number of 2 (via his appearance alongside John Cleese in \"Monty Python Live (Mostly)\" who acted alongside Kevin Bacon in \"The Big Picture\") is lower than his Erdős number of 4.\n\nCanadian actor Albert M. Chan has an Erdős-Bacon number of 4. He co-authored a peer-reviewed paper on Orthogonal frequency-division multiplexing, giving him an Erdős number of 3, and was cast alongside Kevin Bacon in \"Patriots Day\", giving him a Bacon number of 1.\n\nDanica McKellar, who played Winnie Cooper in \"The Wonder Years\", has an Erdős–Bacon number of 6, having coauthored a mathematics paper published while an undergraduate at the University of California, Los Angeles. Her paper gives her an Erdős number of 4, and she has a Bacon number of 2, having worked with Margaret Easley.\n\nAmerican actress Natalie Portman has an Erdős–Bacon number of 7. She collaborated (using her birth name, Natalie Hershlag) with Abigail A. Baird, who has a collaboration path leading to Joseph Gillis, who has an Erdős number of 1. Portman appeared in \"A Powerful Noise Live\" (2009) with Sarah Michelle Gellar, who appeared in \"The Air I Breathe\" (2007) with Bacon, giving Portman a Bacon number of 2 and an Erdős number of 5.\n\nBritish actor Colin Firth has an Erdős–Bacon number of 7. Firth is credited as co-author of a neuroscience paper, \"Political Orientations Are Correlated with Brain Structure in Young Adults\", after he suggested on BBC Radio 4 that such a study could be done. Another author of that paper, Geraint Rees, has an Erdős number of 5, which gives Firth an Erdős number of 6. Firth's Bacon number of 1 is due to his appearance in \"Where the Truth Lies\".\n\nKristen Stewart has an Erdős–Bacon number of 7; she is credited as a co-author on an artificial intelligence paper that was written after a technique was used for her short film \"Come Swim\", giving her an Erdős number of 5, and she co-starred with Michael Sheen in \"Twilight\", who co-starred with Bacon in \"Frost/Nixon\", giving her a Bacon number of 2.\n\nAmerican scholar and actor Michael M. Chemers has an Erdös-Bacon number of 6. He co-authored a 2018 paper about \"Game of Thrones\" with mathematician Andrew Beveridge, who has an Erdös number of 2, giving him an Erdös number of 3, and co-starred in two films (\"When Tyrants Kiss,\" 2004; \"Before the Thunder,\" 2018) both of which give him a Bacon number of 3 through a number of co-stars.\n\nNotes:\n"}
{"id": "191933", "url": "https://en.wikipedia.org/wiki?curid=191933", "title": "Exponential growth", "text": "Exponential growth\n\nExponential growth is exhibited when the rate of change—the change per instant or unit of time—of the value of a mathematical function is proportional to the function's current value, resulting in its value at any time being an exponential function of time, i.e., a function in which the time value is the exponent.\nExponential decay occurs in the same way when the growth rate is negative. In the case of a discrete domain of definition with equal intervals, it is also called geometric growth or geometric decay, the function values forming a geometric progression. In either exponential growth or exponential decay, the ratio of the rate of change of the quantity to its current size remains constant over time.\n\nThe formula for exponential growth of a variable \"x\" at the growth rate \"r\", as time \"t\" goes on in discrete intervals (that is, at integer times 0, 1, 2, 3, ...), is\n\nwhere \"x\" is the value of \"x\" at time 0. This formula is transparent when the exponents are converted to multiplication. For instance, with a starting value of 50 and a growth rate of per interval, the passage of one interval would give ; two intervals would give ; and three intervals would give . In this way, each increase in the exponent by a full interval can be seen to increase the previous total by another five percent. (The order of multiplication does not change the result based on the associative property of multiplication.)\n\nSince the time variable, which is the input to this function, occurs as the exponent, this is an exponential function. This contrasts with growth based on a power function, where the time variable is the base value raised to a fixed exponent, such as cubic growth (or in general terms denoted as polynomial growth).\n\n\nA quantity \"x\" depends exponentially on time \"t\" if\n\nwhere the constant \"a\" is the initial value of \"x\",\n\nthe constant \"b\" is a positive growth factor, and \"τ\" is the time constant—the time required for \"x\" to increase by one factor of \"b\":\n\nIf and , then \"x\" has exponential growth. If and , or and 0 < , then \"x\" has exponential decay.\n\nExample: \"If a species of bacteria doubles every ten minutes, starting out with only one bacterium, how many bacteria would be present after one hour?\" The question implies \"a\" = 1, \"b\" = 2 and \"τ\" = 10 min.\n\nAfter one hour, or six ten-minute intervals, there would be sixty-four bacteria.\n\nMany pairs (\"b\", \"τ\") of a dimensionless non-negative number \"b\" and an amount of time \"τ\" (a physical quantity which can be expressed as the product of a number of units and a unit of time) represent the same growth rate, with \"τ\" proportional to log \"b\". For any fixed \"b\" not equal to 1 (e.g. \"e\" or 2), the growth rate is given by the non-zero time \"τ\". For any non-zero time \"τ\" the growth rate is given by the dimensionless positive number \"b\".\n\nThus the law of exponential growth can be written in different but mathematically equivalent forms, by using a different base. The most common forms are the following:\n\nwhere \"x\" expresses the initial quantity \"x\"(0).\n\nParameters (negative in the case of exponential decay):\nThe quantities \"k\", \"τ\", and \"T\", and for a given \"p\" also \"r\", have a one-to-one connection given by the following equation (which can be derived by taking the natural logarithm of the above):\n\nwhere \"k\" = 0 corresponds to \"r\" = 0 and to \"τ\" and \"T\" being infinite.\n\nIf \"p\" is the unit of time the quotient \"t\"/\"p\" is simply the number of units of time. Using the notation \"t\" for the (dimensionless) number of units of time rather than the time itself, \"t\"/\"p\" can be replaced by \"t\", but for uniformity this has been avoided here. In this case the division by \"p\" in the last formula is not a numerical division either, but converts a dimensionless number to the correct quantity including unit.\n\nA popular approximated method for calculating the doubling time from the growth rate is the rule of 70,\ni.e. formula_9.\n\nIf a variable \"x\" exhibits exponential growth according to formula_10, then the log (to any base) of \"x\" grows linearly over time, as can be seen by taking logarithms of both sides of the exponential growth equation:\n\nThis allows an exponentially growing variable to be modeled with a log-linear model. For example, if one wishes to empirically estimate the growth rate from intertemporal data on \"x\", one can linearly regress log \"x\" on \"t\".\n\nThe exponential function formula_12 satisfies the linear differential equation:\n\nsaying that the change per instant of time of \"x\" at time \"t\" is proportional to the value of \"x\"(\"t\"), and \"x\"(\"t\") has the initial value\n\nThe differential equation is solved by direct integration:\n\nso that\n\nIn the above differential equation, if , then the quantity experiences exponential decay.\n\nFor a nonlinear variation of this growth model see logistic function.\n\nThe difference equation\n\nhas solution\n\nshowing that \"x\" experiences exponential growth.\n\nIn the long run, exponential growth of any kind will overtake linear growth of any kind (the basis of the Malthusian catastrophe) as well as any polynomial growth, i.e., for all α:\n\nThere is a whole hierarchy of conceivable growth rates that are slower than exponential and faster than linear (in the long run). See Degree of a polynomial#The degree computed from the function values.\n\nGrowth rates may also be faster than exponential. In the most extreme case, when growth increases without bound in finite time, it is called hyperbolic growth. In between exponential and hyperbolic growth lie more classes of growth behavior, like the hyperoperations beginning at tetration, and formula_20, the diagonal of the Ackermann function.\n\nExponential growth models of physical phenomena only apply within limited regions, as unbounded growth is not physically realistic. Although growth may initially be exponential, the modelled phenomena will eventually enter a region in which previously ignored negative feedback factors become significant (leading to a logistic growth model) or other underlying assumptions of the exponential growth model, such as continuity or instantaneous feedback, break down.\n\nAccording to an old legend, vizier Sissa Ben Dahir presented an Indian King Sharim with a beautiful, hand-made chessboard. The king asked what he would like in return for his gift and the courtier surprised the king by asking for one grain of rice on the first square, two grains on the second, four grains on the third etc. The king readily agreed and asked for the rice to be brought. All went well at first, but the requirement for 2 grains on the \"n\"th square demanded over a million grains on the 21st square, more than a million million ( trillion) on the 41st and there simply was not enough rice in the whole world for the final squares. (From Swirski, 2006)\n\nThe second half of the chessboard is the time when an exponentially growing influence is having a significant economic impact on an organization's overall business strategy.\n\nFrench children are told a story in which they imagine having a pond with water lily leaves floating on the surface. The lily population doubles in size every day and, if left unchecked, it will smother the pond in thirty days killing all the other living things in the water. Day after day, the plant's growth is small and so it is decided that it shall be cut down when the water lilies cover half of the pond. The children are then asked on what day will half of the pond be covered in water lilies. The solution is simple when one considers that the water lilies must double to completely cover the pond on the thirtieth day. Therefore, the water lilies will cover half of the pond on the twenty-ninth day. There is only one day to save the pond. (From Meadows \"et al\". 1972)\n\n\n"}
{"id": "52722478", "url": "https://en.wikipedia.org/wiki?curid=52722478", "title": "Fractal expressionism", "text": "Fractal expressionism\n\nThe term fractal expressionism was coined by physicist-artist Richard Taylor and co-authors to distinguish fractal art generated directly by artists from fractal art generated using mathematics and/or computers. Fractals are patterns that repeat at increasingly fine scales and are prevalent in natural scenery (examples include clouds, rivers, and mountains). Fractal expressionism implies a direct expression of nature's patterns in an art work.\n\nThe initial studies of fractal expressionism focused on the poured paintings by the American artist Jackson Pollock (1912-1956), whose work has traditionally been associated with the abstract expressionist movement. Pollock’s patterns had previously been referred to as “natural” and “organic”, inviting speculation by author John Briggs in 1992 that Pollock's work featured fractals. In 1997, Taylor built a pendulum device called the Pollockizer which painted fractal patterns bearing a similarity to Pollock’s work. Computer analysis of Pollock's work published by Taylor et al. in a 1999 Nature article found that Pollock's painted patterns have characteristics that match those displayed by nature's fractals (specifically, they demonstrate a statistical self-similarity quantified by a non-integer dimension over a magnification range of 1.5-2 orders of magnitude). This analysis supported clues (see below) that Pollock's patterns are fractal and reflect \"the fingerprint of nature\".\n\nTaylor noted several similarities between Pollock's painting style and the processes used by nature to construct its landscapes. For instance, he cites Pollock's propensity to revisit paintings that he had not adjusted in several weeks as being comparable to cyclic processes in nature, such as the seasons or the tides. Furthermore, Taylor observed several visual similarities between the patterns produced by nature and those produced by Pollock as he painted. He points out that Pollock abandoned the use of a traditional frame for his paintings, preferring instead to roll out his canvas on the floor; this, Taylor asserts, is more compatible with how nature works than traditional painting techniques because the patterns in nature's scenery are not artificially bounded.\n\nThe perceived similarities between the processes and patterns involved in Pollock's paintings and those of nature compelled Taylor to posit that the same \"basic trademark\" of nature's pattern construction also appears in Pollock's work. Since some natural fractals are generated by a process known as \"chaos\", including fractals in human physiology, Taylor believed that Pollock's painting process might also have been chaotic, and could therefore leave behind a fractal pattern. Taylor's hypothesis seems to be reflected in Pollock's statement \"I am nature\", which he made when asked if nature was a source of inspiration for his work. Furthermore, Pollock is also quoted as stating \"No chaos, damn it\", in response to a Time magazine article that referred to his paintings as \"chaotic\". However, chaos theory was not understood until after Pollock's death, so he could not have been referring to the chaotic systems in nature but rather its common usage to mean disorder. In the famous film footage of Hans Namuth, Pollock says his paintings are no accident, and that he was able to control the flow of paint onto the canvas.\n\nTaylor points to two aspects of Pollock's painting process that have the potential to introduce fractal patterns. The first is Pollock's motion as he moved around the canvas, which Taylor hypothesized followed a Levy flight, a type of chaotic motion that is known to leave behind a fractal pattern. More specifically, a number of studies have shown that the motions associated with human balance have fractal characteristics. The second source of chaos could be introduced through Pollock's pouring technique. Falling fluid has the capability of changing from a non-chaotic to a chaotic flow, meaning that Pollock could have introduced a chaotic flow of paint as he dripped it onto the canvas. Although the fractal characteristics of human balance and falling liquid are generated on Pollock's painting time and length scales, physicist Predrag Cvitanovic notes that it would be quite an artistic challenge to control them: such parameters \"are in no sense observable and measurable on the length-scales and time-scales dominated by chaotic dynamics\".\n\nSince Taylor's initial Pollock analysis in 1999, more than ten research groups have used various forms of fractal analysis to successfully quantify Pollock’s work. In addition to analyzing Pollock's work for fractal content, some groups such as that of computer scientist Bruce Gooch, have used computers to generate Pollock-like images by varying their fractal characteristics. Mathematician Benoit Mandelbrot (who invented the term fractal) and art theorist Francis O’Connor (the chief Pollock scholar) are well known advocates of fractal expressionism.\n\nFractal expressionism is related to fractal fluency because the latter provides an appealing motivation for why artists such as Pollock might aspire to Fractal Expressionism. Fractal fluency is a neuroscience model that proposes that, through exposure to nature’s fractal scenery, people’s visual systems have adapted to efficiently process fractals with ease. This adaptation occurs at many stages of the visual system, from the way people’s eyes move to which regions of the brain get activated. Fluency puts the viewer in a ‘comfort zone’ so inducing an aesthetic experience. Neuroscience experiments have shown that Pollock’s paintings induce the same positive physiological responses in the observer as nature’s fractals and mathematical fractals.\n\nIn light of fractal fluency and the associated aesthetics, other artists might be expected to display fractal expressionism. One year before Taylor’s publication, mathematician Richard Voss quantified Chinese art using fractal analysis. Subsequently, other groups have used computer analysis to identify fractal content in a number of Western and Eastern artists, most recently in Pollock’s colleague Willem De Kooning’s work.\n\nIn addition to the above analyzed works, symbolic representations of fractals can be found in cultures across the continents spanning several centuries, including Roman, Egyptian, Aztec, Incan and Mayan civilizations. They frequently predate patterns named after the mathematicians who subsequently developed their visual characteristics. For example, although von Koch is famous for developing The Koch Curve in 1904, a similar shape featuring repeating triangles was first used to depict waves in friezes by Hellenic artists (300 B.C.E.). In the 13th century, repetition of triangles in Cosmati Mosaics generated a shape later known in mathematics as The Sierpinski Triangle (named after Sierpinski’s 1915 pattern). Triangular repetitions are also found in the 12th century pulpit of The Ravello Cathedral in Italy. The lavish artwork within The Book of Kells (circa 800 C.E.) and the sculpted arabesques in The Jain Dilwara Temple in Mount Abu, India (1031 C.E.) also both reveal stunning examples of exact fractals. The artistic works of Leonardo da Vinci and Katsushika Hokusai serve as more recent examples from Europe and Asia, each reproducing the recurring patterns that they saw in nature. Da Vinci’s sketch of turbulence in water, The Deluge (1571–1518), was composed of small swirls within larger swirls of water. In The Great Wave off Kanagawa (1830–1833), Hokusai portrayed a wave crashing on a shore with small waves on top of a large wave. Other woodcuts from the same period also feature repeating patterns at several size scales: The Ghost of Kohada Koheiji shows fissures in a skull and The Falls At Mt. Kurokami features branching channels in a waterfall.\n\nVoss's 1998 study of Chinese art was the first demonstration of using fractal analysis to distinguish between the works of different artists. Following Taylor's 1999 Pollock publication, Art conservator Jim Coddington proposed that fractal analysis should be explored as a technique to help authenticate Pollock paintings. In 2005, Taylor and colleagues published a fractal analysis of 14 authentic and 37 imitation Pollocks suggesting that, when combined with other techniques, fractal analysis might be useful for authenticating Pollock's work. In the same year, The Pollock-Krasner Foundation requested a fractal analysis to be used for the first time in an authenticity dispute, The analysis identified “significant deviations from Pollock’s characteristics.” Taylor cautioned that the results should be “coupled with other important information such as provenance, connoisseurship and materials analysis.” Two years later, materials scientists showed that pigments on the paintings dated from after Pollock’s death.\nIn 2006, the use of fractals to authenticate Pollocks stirred controversy. This controversy was triggered by physicists Katherine Jones-Smith and Harsh Mathur who claimed that the fractal characteristics identified by Taylor et al. are also present in crude sketches made in Adobe Photoshop, and deliberately fraudulent poured paintings made by other artists Thus, according to Jones-Smith and Mathur, labeling Pollock's paintings as \"fractal\" is meaningless, because the same characteristics are found in other non-fractal images. However, Taylor's rebuttal published in Nature showed that Taylor's group's fractal analysis could distinguish between Pollock paintings and the crude sketches, and identified further limitations in Jones-Smith and Mathur's analysis.\n\nJones-Smith and Mathur raised a valid concern applicable to all forms of fractal expressionism: are art works too small for the painted patterns to repeat over sufficient magnifications to assume the visual characteristics of fractals? In the case of Pollock paintings, the largest range used by Taylor et al. to determine each fractal parameter in a Pollock painting is less than two orders of magnitude in magnification. Nature's fractals repeat over limited magnification ranges (typically just over one order of magnitude), prompting scientists to debate what range is required to reliably establish fractal behavior.<ref name=\"Ave/Man\">[Avnir, David, Ofer Biham, Daniel M. Lidar, and Ofer Malcai. \"Is the Geometry of Nature Fractal?\" Science 279.5347 (1998): 39-40. Print.]</ref> Mandelbrot refused to include a required magnification range in his definition of fractals and instead noted that it is the range necessary to generate the properties associated with fractal repetition. In the case of Pollock's work, this would be the magnification range necessary for the patterns to generate the fractal aesthetics. Neuroscience experiments have shown that this magnification range is less than two orders and that Pollock’s paintings do indeed induce the same physiological responses as nature’s fractals and mathematical fractals Mandelbrot concluded \"I do believe that Pollocks are fractal.\"\n\nAt the time of the controversy, Coddington summarized as follows: “Fractal geometry has begun to play an important role in the authentication of the work of Jackson Pollock. We believe such analyses are necessary for pushing the field forward.” The most recent results, In 2015, by computer scientist Lior Shamir showed that, when combined with other pattern parameters, fractal analysis can be used to distinguish between real and imitation Pollocks with 93% accuracy. He found that the fractal parameters were the most powerful contributors to the detection accuracy\n"}
{"id": "12751687", "url": "https://en.wikipedia.org/wiki?curid=12751687", "title": "God Created the Integers", "text": "God Created the Integers\n\nGod Created the Integers: The Mathematical Breakthroughs That Changed History is an anthology, edited by Stephen Hawking, of \"excerpts from thirty-one of the most important works in the history of mathematics.\" \n\nThe title of the book is a reference to a quotation attributed to mathematician Leopold Kronecker, who once wrote that \"God made the integers; all else is the work of man.\" \n\nThe works are grouped by author and ordered chronologically. Each section is prefaced by notes on the mathematician's life and work. The anthology includes works by the following mathematicians:\n\nSelections from the works of Euler, Bolyai, Lobachevsky and Galois, which are included in the second edition of the book (published in 2007), were not included in the first edition.\n"}
{"id": "7392980", "url": "https://en.wikipedia.org/wiki?curid=7392980", "title": "Graduate Texts in Mathematics", "text": "Graduate Texts in Mathematics\n\nGraduate Texts in Mathematics (GTM) (ISSN 0072-5285) is a series of graduate-level textbooks in mathematics published by Springer-Verlag. The books in this series, like the other Springer-Verlag mathematics series, are yellow books of a standard size (with variable numbers of pages). The GTM series is easily identified by a white band at the top of the book.\n\nThe books in this series tend to be written at a more advanced level than the similar Undergraduate Texts in Mathematics series, although there is a fair amount of overlap between the two series in terms of material covered and difficulty level.\n\n\n\n"}
{"id": "3612259", "url": "https://en.wikipedia.org/wiki?curid=3612259", "title": "Hierarchy (mathematics)", "text": "Hierarchy (mathematics)\n\nIn mathematics, a hierarchy is a set-theoretical object, consisting of a preorder defined on a set. This is often referred to as an ordered set, though that is an ambiguous term that many authors reserve for partially ordered sets or totally ordered sets. The term \"pre-ordered set\" is unambiguous, and is always synonymous with a mathematical hierarchy. The term \"hierarchy\" is used to stress a \"hierarchical\" relation among the elements.\n\nSometimes, a set comes equipped with a natural hierarchical structure. For example, the set of natural numbers \"N\" is equipped with a natural pre-order structure, where formula_1 whenever we can find some other number formula_2 so that formula_3. That is, formula_4 is bigger than formula_5 only because we can get to formula_4 from formula_5 \"using\" formula_2. This is true for any commutative monoid. On the other hand, the set of integers \"Z\" requires a more sophisticated argument for its hierarchical structure, since we can always solve the equation formula_3 by writing formula_10.\n\nA mathematical hierarchy (a pre-ordered set) should not be confused with the more general concept of a hierarchy in the social realm, particularly when one is constructing computational models which are used to describe real-world social, economic or political systems. These hierarchies, or complex networks, are much too rich to be described in the category Set of sets. This is not just a pedantic claim; there are also mathematical hierarchies which are not describable using set theory.\n\nAnother natural hierarchy arises in computer science, where the word refers to partially ordered sets whose elements are classes of objects of increasing complexity. In that case, the preorder defining the hierarchy is the class-containment relation. Containment hierarchies are thus special cases of hierarchies.\n\nIndividual elements of a hierarchy are often called levels and a hierarchy is said to be infinite if it has infinitely many distinct levels but said to collapse if it has only finitely many distinct levels.\n\nIn theoretical computer science, the time hierarchy is a classification of decision problems according to the amount of time required to solve them.\n"}
{"id": "20885039", "url": "https://en.wikipedia.org/wiki?curid=20885039", "title": "History of combinatorics", "text": "History of combinatorics\n\nThe mathematical field of combinatorics was studied to varying degrees in numerous ancient societies. Its study in Europe dates to the work of Leonardo Fibonacci in the 13th century AD, which introduced Arabian and Indian ideas to the continent. It has continued to be studied in the modern era.\n\nThe earliest recorded use of combinatorial techniques comes from problem 79 of the Rhind papyrus, which dates to the 16th century BCE. The problem concerns a certain geometric series, and has similarities to Fibonacci's problem of counting the number of compositions of 1s and 2s that sum to a given total.\n\nIn Greece, Plutarch wrote that Xenocrates of Chalcedon (396–314 BC) discovered the number of different syllables possible in the Greek language. This would have been the first attempt on record to solve a difficult problem in permutations and combinations. The claim, however, is implausible: this is one of the few mentions of combinatorics in Greece, and the number they found, 1.002 × 10, seems too round to be more than a guess.\n\nThe Bhagavati Sutra had the first mention of a combinatorics problem; the problem asked how many possible combinations of tastes were possible from selecting tastes in ones, twos, threes, etc. from a selection of six different tastes (sweet, pungent, astringent, sour, salt, and bitter). The Bhagavati is also the first text to mention the choose function. In the second century BC, Pingala included an enumeration problem in the Chanda Sutra (also Chandahsutra) which asked how many ways a six-syllable meter could be made from short and long notes. Pingala found the number of meters that had formula_1 long notes and formula_2 short notes; this is equivalent to finding the binomial coefficients.\n\nThe ideas of the Bhagavati were generalized by the Indian mathematician Mahavira in 850 AD, and Pingala's work on prosody was expanded by Bhāskara II and Hemacandra in 1100 AD. Bhaskara was the first known person to find the generalised choice function, although Brahmagupta may have known earlier. Hemacandra asked how many meters existed of a certain length if a long note was considered to be twice as long as a short note, which is equivalent to finding the Fibonacci numbers. \nThe ancient Chinese book of divination I Ching describes a hexagram as a permutation with repetitions of six lines where each line can be one of two states: solid or dashed. In describing hexagrams in this fashion they determine that there are formula_3 possible hexagrams. A Chinese monk also may have counted the number of configurations to a game similar to Go around 700 AD. Although China had relatively few advancements in enumerative combinatorics, around 100 AD they solved the Lo Shu Square which is the combinatorial design problem of the normal magic square of order three. Magic squares remained an interest of China, and they began to generalize their original formula_4 square between 900 and 1300 AD. China corresponded with the Middle East about this problem in the 13th century. The Middle East also learned about binomial coefficients from Indian work and found the connection to polynomial expansion. The work of Hindus influenced Arabs as seen in the work of al-Khalil ibn Ahmad who considered the possible arrangements of letters to form syllables. His calculations show an understanding of permutations and combinations. In a passage from the work of Arab mathematician Umar al-Khayyami that dates to around 1100, it is corroborated that the Hindus had knowledge of binomial coefficients, but also that their methods reached the middle east.\n\nIn Greece, Plutarch wrote that Xenocrates discovered the number of different syllables possible in the Greek language. While unlikely, this is one of the few mentions of Combinatorics in Greece. The number they found, 1.002 × 10, also seems too round to be more than a guess.\n\nAbū Bakr ibn Muḥammad ibn al Ḥusayn Al-Karaji (c.953-1029) wrote on the binomial theorem and Pascal's triangle. In a now lost work known only from subsequent quotation by al-Samaw'al, Al-Karaji introduced the idea of argument by mathematical induction.\n\nThe philosopher and astronomer Rabbi Abraham ibn Ezra (c. 1140) counted the permutations with repetitions in vocalization of Divine Name. He also established the symmetry of binomial coefficients, while a closed formula was obtained later by the talmudist and mathematician Levi ben Gerson (better known as Gersonides), in 1321.\nThe arithmetical triangle— a graphical diagram showing relationships among the binomial coefficients— was presented by mathematicians in treatises dating as far back as the 10th century, and would eventually become known as Pascal's triangle. Later, in Medieval England, campanology provided examples of what is now known as Hamiltonian cycles in certain Cayley graphs on permutations.\n\nCombinatorics came to Europe in the 13th century through mathematicians Leonardo Fibonacci and Jordanus de Nemore. Fibonacci's Liber Abaci introduced many of the Arabian and Indian ideas to Europe, including that of the Fibonacci numbers. Jordanus was the first person to arrange the binomial coefficients in a triangle, as he did in proposition 70 of \"De Arithmetica\". This was also done in the Middle East in 1265, and China around 1300. Today, this triangle is known as Pascal's triangle.\n\nPascal's contribution to the triangle that bears his name comes from his work on formal proofs about it, and the connections he made between Pascal's triangle and probability. From a letter Leibniz sent to Daniel Bernoulli we learn that Leibniz was formally studying the mathematical theory of partitions in the 17th century, although no formal work was published. Together with Leibniz, Pascal published De Arte Combinatoria in 1666 which was reprinted later. Pascal and Leibniz are considered the founders of modern combinatorics.\n\nBoth Pascal and Leibniz understood that the binomial expansion was equivalent to the choice function. The notion that algebra and combinatorics corresponded was expanded by De Moivre, who found the expansion of a multinomial. De Moivre also found the formula for derangements using the principle of principle of inclusion-exclusion, a method different from Nikolaus Bernoulli, who had found it previously. De Moivre also managed to approximate the binomial coefficients and factorial, and found a closed form for the Fibonacci numbers by inventing generating functions.\n\nIn the 18th century, Euler worked on problems of combinatorics, and several problems of probability which are linked to combinatorics. Problems Euler worked on include the Knights tour, Graeco-Latin square, Eulerian numbers, and others. To solve the Seven Bridges of Königsberg problem he invented graph theory, which also led to the formation of topology. Finally, he broke ground with partitions by the use of generating functions.\n\nIn the 19th century, the subject of partially ordered sets and lattice theory originated in the work of Dedekind, Peirce, and Schröder. However, it was Garrett Birkhoff's seminal work in his book \"Lattice Theory\" published in 1967, and the work of John von Neumann that truly established the subjects. In the 1930s, Hall (1936) and Weisner (1935) independently stated the general Möbius inversion formula. In 1964, Gian-Carlo Rota's \"On the Foundations of Combinatorial Theory I. Theory of Miibius Functions \" introduced poset and lattice theory as theories in Combinatorics. Richard P. Stanley has had a big impact in contemporary combinatorics for his work in matroid theory, for introducing Zeta polynomials, for explicitly defining Eulerian posets, developing the theory of binomial posets along with Rota and Peter Doubilet, and more.\n\n"}
{"id": "386519", "url": "https://en.wikipedia.org/wiki?curid=386519", "title": "History of computing", "text": "History of computing\n\nThe history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables. \n\nDigital computing is intimately tied to the representation of numbers. But long before abstractions like \"the number\" arose, there were mathematical concepts to serve the purposes of civilization. These concepts are implicit in concrete practices such as :\n\nEventually, the concept of numbers became concrete and familiar enough for counting to arise, at times with sing-song mnemonics to teach sequences to others. All known languages have words for at least \"one\" and \"two\" (although this is disputed: see Piraha language), and even some animals like the blackbird can distinguish a surprising number of items.\n\nAdvances in the numeral system and mathematical notation eventually led to the discovery of mathematical operations such as addition, subtraction, multiplication, division, squaring, square root, and so forth. Eventually the operations were formalized, and concepts about the operations became understood well enough to be stated formally, and even proven. See, for example, Euclid's algorithm for finding the greatest common divisor of two numbers.\n\nBy the High Middle Ages, the positional Hindu–Arabic numeral system had reached Europe, which allowed for systematic computation of numbers. During this period, the representation of a calculation on paper actually allowed calculation of mathematical expressions, and the tabulation of mathematical functions such as the square root and the common logarithm (for use in multiplication and division) and the trigonometric functions. By the time of Isaac Newton's research, paper or vellum was an important computing resource, and even in our present time, researchers like Enrico Fermi would cover random scraps of paper with calculation, to satisfy their curiosity about an equation. Even into the period of programmable calculators, Richard Feynman would unhesitatingly compute any steps which overflowed the memory of the calculators, by hand, just to learn the answer.\n\nThe earliest known tool for use in computation is the Sumerian abacus, and it was thought to have been invented in Babylon c. 2700–2300 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known computer and most advanced system of calculation known to date - preceding Greek methods by 2,000 years.\n\nIn c. 1050–771 BC, the south-pointing chariot was invented in ancient China. It was the first known geared mechanism to use a differential gear, which was later used in analog computers. The Chinese also invented a more sophisticated abacus from around the 2nd century BC known as the Chinese abacus.\n\nIn the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.\n\nIn the 3rd century BC, Archimedes used the mechanical principle of balance (see Archimedes Palimpsest#Mathematical content) to calculate mathematical problems, such as the number of grains of sand in the universe (\"The sand reckoner\"), which also required a recursive notation for numbers (e.g., the myriad myriad).\n\nAround 200 BC the development of gears had made it possible to create devices in which the positions of wheels would correspond to positions of astronomical objects. By about 100 AD Hero of Alexandria had described an odometer-like device that could be driven automatically and could effectively count in digital form. But it was not until the 1600s that mechanical devices for digital computation appear to have actually been built.\n\nThe Antikythera mechanism is believed to be the earliest known mechanical analog computer. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to \"circa\" 100 BC.\n\nMechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari's humanoid robots and \"castle clock\", which is considered to be the first programmable analog computer.\n\nDuring the Middle Ages, several European philosophers made attempts to produce analog computer devices. Influenced by the Arabs and Scholasticism, Majorcan philosopher Ramon Llull (1232–1315) devoted a great part of his life to defining and designing several \"logical machines\" that, by combining simple and undeniable philosophical truths, could produce all possible knowledge. These machines were never actually built, as they were more of a thought experiment to produce new knowledge in systematic ways; although they could make simple logical operations, they still needed a human being for the interpretation of results. Moreover, they lacked a versatile architecture, each machine serving only very concrete purposes. In spite of this, Llull's work had a strong influence on Gottfried Leibniz (early 18th century), who developed his ideas further, and built several calculating tools using them.\n\nIndeed, when John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. The apex of this early era of formal computing can be seen in the difference engine and its successor the analytical engine (which was never completely constructed but was designed in detail), both by Charles Babbage. The analytical engine combined concepts from his work and that of others to create a device that if constructed as designed would have possessed many properties of a modern electronic computer. These properties include such features as an internal \"scratch memory\" equivalent to RAM, multiple forms of output including a bell, a graph-plotter, and simple printer, and a programmable input-output \"hard\" memory of punch cards which it could modify as well as read. The key advancement which Babbage's devices possessed beyond those created before his was that each component of the device was independent of the rest of the machine, much like the components of a modern electronic computer. This was a fundamental shift in thought; previous computational devices served only a single purpose, but had to be at best disassembled and reconfigured to solve a new problem. Babbage's devices could be reprogramed to solve new problems by the entry of new data, and act upon previous calculations within the same series of instructions. Ada Lovelace took this concept one step further, by creating a program for the analytical engine to calculate Bernoulli numbers, a complex calculation requiring a recursive algorithm. This is considered to be the first example of a true computer program, a series of instructions that act upon data not known in full until the program is run.\n\nSeveral examples of analog computation survived into recent times. A planimeter is a device which does integrals, using distance as the analog quantity. Until the 1980s, HVAC systems used air both as the analog quantity and the controlling element. Unlike modern digital computers, analog computers are not very flexible, and need to be reconfigured (i.e., reprogrammed) manually to switch them from working on one problem to another. Analog computers had an advantage over early digital computers in that they could be used to solve complex problems using behavioral analogues while the earliest attempts at digital computers were quite limited.\nSince computers were rare in this era, the solutions were often \"hard-coded\" into paper forms such as nomograms, which could then produce analog solutions to these problems, such as the distribution of pressures and temperatures in a heating system.\n\nNone of the early computational devices were really computers in the modern sense, and it took considerable advancement in mathematics and theory before the first modern computers could be designed.\n\nThe first recorded idea of using digital electronics for computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. From 1934 to 1936, NEC engineer Akira Nakashima published a series of papers introducing switching circuit theory, using digital electronics for Boolean algebraic operations, influencing Claude Shannon's seminal 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\".\n\nThe 1937 Atanasoff–Berry computer design was the first digital electronic computer (though not programmable), and the Z3 computer from 1941, by German inventor Konrad Zuse was the first working programmable, fully automatic computing machine.\n\nAlan Turing modelled computation in terms of a one-dimensional storage tape, leading to the idea of the Turing machine and Turing-complete programming systems.\n\nDuring World War II, ballistics computing was done by women, who were hired as \"computers.\" The term computer remained one that referred to mostly women (now seen as \"operator\") until 1945, after which it took on the modern definition of machinery it presently holds.\n\nThe ENIAC (Electronic Numerical Integrator And Computer) was the first electronic general-purpose computer, announced to the public in 1946. It was Turing-complete, digital, and capable of being reprogrammed to solve a full range of computing problems. Women implemented the programming for machines like the ENIAC, and men created the hardware.\n\nThe Manchester Baby was the first electronic stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. The first stored-program transistor computer was the ETL Mark III, developed by Japan's Electrotechnical Laboratory from 1954 to 1956.\n\nThe microprocessor was introduced with the Intel 4004. It began with the \"Busicom Project\" as Masatoshi Shima's three-chip CPU design in 1968, before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968. The Intel 4004 was then developed as a single-chip microprocessor from 1969 to 1970, led by Intel's Marcian Hoff and Federico Faggin and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers, and the microcomputer revolution.\n\nThe 1980s brought about significant advances with microprocessor that greatly impacted the fields of engineering and other sciences. The Motorola 68000 microprocessor had a processing speed that was far superior to the other microprocessors being used at the time. Because of this, having a newer, faster microprocessor allowed for the newer microcomputers that came along after to be more efficient in the amount of computing they were able to do. This was evident in the 1983 release of the Apple computer Lisa. Lisa was the first personal computer with graphical user interface (GUI) that was sold commercially, she ran on the Motorola 68000, dual floppy disk drives, a 5 MB hard drive and had 1MB of RAM . After successfully launching Lisa, a year later Apple released its first Macintosh computer still running on the Motorola 68000 microprocessor. Another advancement because of microprocessors came from Texas Instruments. Texas Instruments first introduced their TMS9900 processor in June 1976. They then used their microprocessor in their TI 99/4 computer.\n\nLate 1980s and beginning in the early 1990s we see more advances with actual computers to aid with actual computing. In 1990, Apple released the Macintosh Portable, it was heavy weighing and extremely expensive. It was not met with great success and was discontinued only two years later. That same year Intel introduced the Touchstone Delta supercomputer, which had 512 microprocessors. This technological advancement was very significant as it was used as a model for some of the fastest multi-processors systems in the world. It was even used a prototype for Caltech researchers who used the model for projects like real time processing of satellite images and simulating molecular models for various fields of research.\n\nStarting with known special cases, the calculation of logarithms and trigonometric functions can be performed by looking up numbers in a mathematical table, and interpolating between known cases. For small enough differences, this linear operation was accurate enough for use in navigation and astronomy in the Age of Exploration. The uses of interpolation have thrived in the past 500 years: by the twentieth century Leslie Comrie and W.J. Eckert systematized the use of interpolation in tables of numbers for punch card calculation.\n\nThe numerical solution of differential equations, notably the Navier-Stokes equations was an important stimulus to computing,\nwith Lewis Fry Richardson's numerical approach to solving differential equations. The first computerised weather forecast was performed in 1950 by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and ENIAC programmer Klara Dan von Neumann. To this day, some of the most powerful computer systems on Earth are used for weather forecasts.\n\nBy the late 1960s, computer systems could perform symbolic algebraic manipulations well enough to pass college-level calculus courses.\n\n\n\n\n"}
{"id": "13595525", "url": "https://en.wikipedia.org/wiki?curid=13595525", "title": "ICTP Ramanujan Prize", "text": "ICTP Ramanujan Prize\n\nThe ICTP Ramanujan Prize for Young Mathematicians from Developing Countries is a mathematics prize awarded annually by the International Centre for Theoretical Physics and named after the mathematician Srinivasa Ramanujan. It was founded in 2004, and was first awarded in 2005.\n\nThe prize is awarded to a researcher from a developing country less than 45 years of age who has conducted outstanding research in a developing country. The prize is supported by the Ministry of Science and Technology (India) and Norwegian Academy of Science and Letters through the Abel Fund, with the cooperation of the International Mathematical Union.\n\nSource: International Mathematical Union\n\n\n\n"}
{"id": "4190174", "url": "https://en.wikipedia.org/wiki?curid=4190174", "title": "Limitation of size", "text": "Limitation of size\n\nIn the philosophy of mathematics, specifically the philosophical foundations of set theory, limitation of size is a concept developed by Philip Jourdain and/or Georg Cantor to avoid Cantor's paradox. It identifies certain \"inconsistent multiplicities\", in Cantor's terminology, that cannot be sets because they are \"too large\". In modern terminology these are called proper classes.\n\nThe axiom of limitation of size is an axiom in some versions of von Neumann–Bernays–Gödel set theory or Morse–Kelley set theory. This axiom says that any class which is not \"too large\" is a set, and a set cannot be \"too large\". \"Too large\" is defined as being large enough that the class of all sets can be mapped one-to-one into it.\n"}
{"id": "1749665", "url": "https://en.wikipedia.org/wiki?curid=1749665", "title": "List of finite simple groups", "text": "List of finite simple groups\n\nIn mathematics, the classification of finite simple groups states that every finite simple group is cyclic, or alternating, or in one of 16 families of groups of Lie type, or one of 26 sporadic groups.\n\nThe list below gives all finite simple groups, together with their order, the size of the Schur multiplier, the size of the outer automorphism group, usually some small representations, and lists of all duplicates.\n\nThe following table is a complete list of the 18 families of finite simple groups and the 26 sporadic simple groups, along with their orders. Any non-simple members of each family are listed, as well as any members duplicated within a family or between families. (In removing duplicates it is useful to note that no two finite simple groups have the same order, except that the group A = \"A\"(2) and \"A\"(4) both have order 20160, and that the group \"B\"(\"q\") has the same order as \"C\"(\"q\") for \"q\" odd, \"n\" > 2. The smallest of the latter pairs of groups are \"B\"(3) and \"C\"(3) which both have order 4585351680.)\n\nThere is an unfortunate conflict between the notations for the alternating groups A and the groups of Lie type \"A\"(\"q\"). Some authors use various different fonts for A to distinguish them. In particular,\nin this article we make the distinction by setting the alternating groups A in Roman font and the Lie-type groups \"A\"(\"q\") in italic.\n\nIn what follows, \"n\" is a positive integer, and \"q\" is a positive power of a prime number \"p\", with the restrictions noted. The notation (\"a\",\"b\") represents the greatest common divisor of the integers \"a\" and \"b\".\n\nSimplicity: Simple for \"p\" a prime number.\n\nOrder: \"p\"\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Cyclic of order \"p\" − 1.\n\nOther names: Z/\"p\"Z\n\nRemarks: These are the only simple groups that are not perfect.\n\nSimplicity: Solvable for \"n\" < 5, otherwise simple.\n\nOrder: \"n\"!/2 when \"n\" > 1.\n\nSchur multiplier: 2 for \"n\" = 5 or \"n\" > 7, 6 for \"n\" = 6 or 7; see \"Covering groups of the alternating and symmetric groups\"\n\nOuter automorphism group: In general 2. Exceptions: for \"n\" = 1, \"n\" = 2, it is trivial, and for \"n\" = 6, it has order 4 (elementary abelian).\n\nOther names: Alt.\n\nIsomorphisms: A and A are trivial. A is cyclic of order 3. A is isomorphic to \"A\"(3) (solvable). A is isomorphic to \"A\"(4) and to \"A\"(5). A is isomorphic to \"A\"(9) and to the derived group \"B\"(2)′. A is isomorphic to \"A\"(2).\n\nRemarks: An index 2 subgroup of the symmetric group of permutations of \"n\" points when \"n\" > 1.\n\nNotation: \"n\" is a positive integer, \"q\" > 1 is a power of a prime number \"p\", and is the order of some underlying finite field. The order of the outer automorphism group is written as \"d\"⋅\"f\"⋅\"g\", where \"d\" is the order of the group of \"diagonal automorphisms\", \"f\" is the order of the (cyclic) group of \"field automorphisms\" (generated by a Frobenius automorphism), and \"g\" is the order of the group of \"graph automorphisms\" (coming from automorphisms of the Dynkin diagram). The notation (\"a\",\"b\") represents the greatest common divisor of the integers \"a\" and \"b\".\n\nSimplicity: Simple for \"n\" ≥ 1. The group\n\"B\"(2) is solvable.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 2.\n\nSchur multiplier: Trivial for \"n\" ≠ 1, elementary abelian of order 4\nfor \"B\"(8).\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1.\n\nOther names: Suz(2), Sz(2).\n\nIsomorphisms: \"B\"(2) is the Frobenius group of order 20.\n\nRemarks: Suzuki group are Zassenhaus groups acting on sets of size (2) + 1, and have 4-dimensional representations over the field with 2 elements. They are the only non-cyclic simple groups whose order is not divisible by 3. They are not related to the sporadic Suzuki group.\n\nSimplicity: Simple for \"n\" ≥ 1. The derived group \"F\"(2)′ is simple of index 2\nin \"F\"(2), and is called the Tits group,\nnamed for the Belgian mathematician Jacques Tits.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 2.\n\nThe Tits group has order 17971200 = 2 ⋅ 3 ⋅ 5 ⋅ 13.\n\nSchur multiplier: Trivial for \"n\" ≥ 1 and for the Tits group.\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1. Order 2 for the Tits group.\n\nRemarks: Unlike the other simple groups of Lie type, the Tits group does not have a BN pair, though its automorphism group does so most authors count it as a sort of honorary group of Lie type.\n\nSimplicity: Simple for \"n\" ≥ 1. The group \"G\"(3) is not simple, but its derived group \"G\"(3)′ is a simple subgroup of index 3.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 3\n\nSchur multiplier: Trivial for \"n\" ≥ 1 and for \"G\"(3)′.\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1.\n\nOther names: Ree(3), R(3), E(3) .\n\nIsomorphisms: The derived group \"G\"(3)′ is isomorphic to \"A\"(8).\n\nRemarks: \"G\"(3) has a doubly transitive permutation representation on 3 + 1 points and acts on a 7-dimensional vector space over the field with 3 elements.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 = 44352000\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Order 2.\n\nRemarks: It acts as a rank 3 permutation group on the Higman Sims graph with 100 points, and is contained in Co and in Co.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 = 898128000\n\nSchur multiplier: Order 3.\n\nOuter automorphism group: Order 2.\n\nRemarks: Acts as a rank 3 permutation group on the McLaughlin graph with 275 points, and is contained in Co and in Co.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 17 = 4030387200\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Order 2.\n\nOther names: Held–Higman–McKay group, HHM, \"F\", HTH\n\nRemarks: Centralizes an element of order 7 in the monster group.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 13 ⋅ 29 = 145926144000\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Trivial.\n\nRemarks: The double cover acts on a 28-dimensional lattice over the Gaussian integers.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 13 = 448345497600\n\nSchur multiplier: Order 6.\n\nOuter automorphism group: Order 2.\n\nOther names: Sz\n\nRemarks: The 6 fold cover acts on a 12-dimensional lattice over the Eisenstein integers. It is not related to the Suzuki groups of Lie type.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 19 ⋅ 31 = 460815505920\n\nSchur multiplier: Order 3.\n\nOuter automorphism group: Order 2.\n\nOther names: O'Nan–Sims group, O'NS, O–S\n\nRemarks:\nThe triple cover has two 45-dimensional representations over the field with 7 elements, exchanged by an outer automorphism.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 19 = 273030912000000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Order 2.\n\nOther names: \"F\", \"D\"\n\nRemarks: Centralizes an element of order 5 in the monster group.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 31 ⋅ 37 ⋅ 67 = 51765179004000000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: Lyons–Sims group, LyS\n\nRemarks: Has a 111-dimensional representation over the field with 5 elements.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 13 ⋅ 19 ⋅ 31 = 90745943887872000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\", \"E\"\n\nRemarks: Centralizes an element of order 3 in the monster, and is contained in \"E\"(3), so has a 248-dimensional representation over the field with 3 elements.\n\nOrder:\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\"\n\nRemarks: The double cover is contained in the monster group. It has a representation of dimension 4371 over the complex numbers (with no nontrivial invariant product), and a representation of dimension 4370 over the field with 2 elements preserving a commutative but non-associative product.\n\nOrder:\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\", M, Monster group, Friendly giant, Fischer's monster.\n\nRemarks: Contains all but 6 of the other sporadic groups as subquotients. Related to monstrous moonshine. The monster is the automorphism group of the 196,883-dimensional Griess algebra and the infinite-dimensional monster vertex operator algebra, and acts naturally on the monster Lie algebra.\n\n lists the 56 non-cyclic simple groups of order less than a million.\n\n\n\n"}
{"id": "33074893", "url": "https://en.wikipedia.org/wiki?curid=33074893", "title": "List of long mathematical proofs", "text": "List of long mathematical proofs\n\nThis is a list of unusually long mathematical proofs.\n\n, the longest mathematical proof, measured by number of published journal pages, is the classification of finite simple groups with well over 10000 pages. There are several proofs that would be far longer than this if the details of the computer calculations they depend on were published in full.\n\nThe length of unusually long proofs has increased with time. As a rough rule of thumb, 100 pages in 1900, or 200 pages in 1950, or 500 pages in 2000 is unusually long for a proof.\n\n\nThere are many mathematical theorems that have been checked by long computer calculations. If these were written out as proofs many would be far longer than most of the proofs above. There is not really a clear distinction between computer calculations and proofs, as several of the proofs above, such as the 4-color theorem and the Kepler conjecture, use long computer calculations as well as many pages of mathematical argument. For the computer calculations in this section, the mathematical arguments are only a few pages long, and the length is due to long but routine calculations. Some typical examples of such theorems include:\n\n\nKurt Gödel showed how to find explicit examples of statements in formal systems that are provable in that system but whose shortest proof is absurdly long. For example, the statement:\nis provable in Peano arithmetic but the shortest proof has at least a googolplex symbols. It has a short proof in a more powerful system: in fact it is easily provable in Peano arithmetic together with the statement that Peano arithmetic is consistent (which cannot be proved in Peano arithmetic by Gödel's incompleteness theorem).\n\nIn this argument, Peano arithmetic can be replaced by any more powerful consistent system, and a googolplex can be replaced by any number that can be described concisely in the system.\n\nHarvey Friedman found some explicit natural examples of this phenomenon, giving some explicit statements in Peano arithmetic and other formal systems whose shortest proofs are ridiculously long . For example, the statement\nis provable in Peano arithmetic, but the shortest proof has length at least \"A\"(1000), where \"A\"(0)=1 and \"A\"(\"n\"+1)=2. The statement is a special case of Kruskal's theorem and has a short proof in second order arithmetic.\n\n\n"}
{"id": "1168363", "url": "https://en.wikipedia.org/wiki?curid=1168363", "title": "List of mathematical knots and links", "text": "List of mathematical knots and links\n\nThis article contains a list of mathematical knots and links. See also list of knots, list of geometric topology topics.\n\n\n\n"}
{"id": "346167", "url": "https://en.wikipedia.org/wiki?curid=346167", "title": "List of mathematical logic topics", "text": "List of mathematical logic topics\n\nThis is a list of mathematical logic topics, by Wikipedia page.\n\nFor traditional syllogistic logic, see the list of topics in logic. See also the list of computability and complexity topics for more theory of algorithms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971813", "url": "https://en.wikipedia.org/wiki?curid=5971813", "title": "List of mathematicians (I)", "text": "List of mathematicians (I)\n\n\n\n\n"}
{"id": "3437245", "url": "https://en.wikipedia.org/wiki?curid=3437245", "title": "Malthusian growth model", "text": "Malthusian growth model\n\nA Malthusian growth model, sometimes called a simple exponential growth model, is essentially exponential growth based on the idea of the function being proportional to the speed to which the function grows. The model is named after Thomas Robert Malthus, who wrote \"An Essay on the Principle of Population\" (1798), one of the earliest and most influential books on population.\n\nMalthusian models have the following form:\nwhere\n\n\nThe model can also been written in the form of a differential equation:\n\ndP/dt = rP\n\nwith initial condition:\nP(0)= P\n\nThis model is often referred to as the \"exponential law\". It is widely regarded in the field of population ecology as the first principle of population dynamics, with Malthus as the founder. The exponential law is therefore also sometimes referred to as the \"Malthusian Law\". By now, it is a widely accepted view to analogize Malthusian growth in Ecology to Newton's First Law of uniform motion in physics.\n\nMalthus wrote that all life forms, including humans, have a propensity to exponential population growth when resources are abundant but that actual growth is limited by available resources:\nA model of population growth bounded by resource limitations was developed by Pierre Francois Verhulst in 1838, after he had read Malthus' essay. Verhulst named the model a logistic function.\n\n\n"}
{"id": "279293", "url": "https://en.wikipedia.org/wiki?curid=279293", "title": "Mathematics and architecture", "text": "Mathematics and architecture\n\nMathematics and architecture are related, since, as with other arts, architects use mathematics for several reasons. Apart from the mathematics needed when engineering buildings, architects use geometry: to define the spatial form of a building; from the Pythagoreans of the sixth century BC onwards, to create forms considered harmonious, and thus to lay out buildings and their surroundings according to mathematical, aesthetic and sometimes religious principles; to decorate buildings with mathematical objects such as tessellations; and to meet environmental goals, such as to minimise wind speeds around the bases of tall buildings.\n\nIn Ancient Egypt, Ancient Greece, India, and the Islamic world, buildings including pyramids, temples, mosques, palaces and mausoleums were laid out with specific proportions for religious reasons. In Islamic architecture, geometric shapes and geometric tiling patterns are used to decorate buildings, both inside and outside. Some Hindu temples have a fractal-like structure where parts resemble the whole, conveying a message about the infinite in Hindu cosmology. In Chinese architecture, the tulou of Fujian province are circular, communal defensive structures. In the twenty-first century, mathematical ornamentation is again being used to cover public buildings.\n\nIn Renaissance architecture, symmetry and proportion were deliberately emphasized by architects such as Leon Battista Alberti, Sebastiano Serlio and Andrea Palladio, influenced by Vitruvius's \"De architectura\" from Ancient Rome and the arithmetic of the Pythagoreans from Ancient Greece.\nAt the end of the nineteenth century, Vladimir Shukhov in Russia and Antoni Gaudí in Barcelona pioneered the use of hyperboloid structures; in the Sagrada Família, Gaudí also incorporated hyperbolic paraboloids, tessellations, catenary arches, catenoids, helicoids, and ruled surfaces. In the twentieth century, styles such as modern architecture and Deconstructivism explored different geometries to achieve desired effects. Minimal surfaces have been exploited in tent-like roof coverings as at Denver International Airport, while Richard Buckminster Fuller pioneered the use of the strong thin-shell structures known as geodesic domes.\n\nThe architects Michael Ostwald and Kim Williams, considering the relationships between architecture and mathematics, note that the fields as commonly understood might seem to be only weakly connected, since architecture is a profession concerned with the practical matter of making buildings, while mathematics is the pure study of number and other abstract objects. But, they argue, the two are strongly connected, and have been since antiquity. In Ancient Rome, Vitruvius described an architect as a man who knew enough of a range of other disciplines, primarily geometry, to enable him to oversee skilled artisans in all the other necessary areas, such as masons and carpenters. The same applied in the Middle Ages, where graduates learnt arithmetic, geometry and aesthetics alongside the basic syllabus of grammar, logic, and rhetoric (the trivium) in elegant halls made by master builders who had guided many craftsmen. A master builder at the top of his profession was given the title of architect or engineer. In the Renaissance, the quadrivium of arithmetic, geometry, music and astronomy became an extra syllabus expected of the Renaissance man such as Leon Battista Alberti. Similarly in England, Sir Christopher Wren, known today as an architect, was firstly a noted astronomer.\n\nWilliams and Ostwald, further overviewing the interaction of mathematics and architecture since 1500 according to the approach of the German sociologist Theodor Adorno, identify three tendencies among architects, namely: to be revolutionary, introducing wholly new ideas; reactionary, failing to introduce change; or revivalist, actually going backwards. They argue that architects have avoided looking to mathematics for inspiration in revivalist times. This would explain why in revivalist periods, such as the Gothic Revival in 19th century England, architecture had little connection to mathematics. Equally, they note that in reactionary times such as the Italian Mannerism of about 1520 to 1580, or the 17th century Baroque and Palladian movements, mathematics was barely consulted. In contrast, the revolutionary early 20th century movements such as Futurism and Constructivism actively rejected old ideas, embracing mathematics and leading to Modernist architecture. Towards the end of the 20th century, too, fractal geometry was quickly seized upon by architects, as was aperiodic tiling, to provide interesting and attractive coverings for buildings.\n\nArchitects use mathematics for several reasons, leaving aside the necessary use of mathematics in the engineering of buildings. Firstly, they use geometry because it defines the spatial form of a building. Secondly, they use mathematics to design forms that are considered beautiful or harmonious. From the time of the Pythagoreans with their religious philosophy of number, architects in Ancient Greece, Ancient Rome, the Islamic world and the Italian Renaissance have chosen the proportions of the built environment – buildings and their designed surroundings – according to mathematical as well as aesthetic and sometimes religious principles. Thirdly, they may use mathematical objects such as tessellations to decorate buildings. Fourthly, they may use mathematics in the form of computer modelling to meet environmental goals, such as to minimise whirling air currents at the base of tall buildings.\n\nThe influential Ancient Roman architect Vitruvius argued that the design of a building such as a temple depends on two qualities, proportion and \"symmetria\". Proportion ensures that each part of a building relates harmoniously to every other part. \"Symmetria\" in Vitruvius's usage means something closer to the English term modularity than mirror symmetry, as again it relates to the assembling of (modular) parts into the whole building. In his Basilica at Fano, he uses ratios of small integers, especially the triangular numbers (1, 3, 6, 10, ...) to proportion the structure into (Vitruvian) modules. Thus the Basilica's width to length is 1:2; the aisle around it is as high as it is wide, 1:1; the columns are five feet thick and fifty feet high, 1:10.\n\nVitruvius named three qualities required of architecture in his \"De architectura\", c. 15 B.C.: firmness, usefulness (or \"Commodity\" in Henry Wotton's 16th century English), and delight. These can be used as categories for classifying the ways in which mathematics is used in architecture. Firmness encompasses the use of mathematics to ensure a building stands up, hence the mathematical tools used in design and to support construction, for instance to ensure stability and to model performance. Usefulness derives in part from the effective application of mathematics, reasoning about and analysing the spatial and other relationships in a design. Delight is an attribute of the resulting building, resulting from the embodying of mathematical relationships in the building; it includes aesthetic, sensual and intellectual qualities.\n\nThe Pantheon in Rome has survived intact, illustrating classical Roman structure, proportion, and decoration. The main structure is a dome, the apex left open as a circular oculus to let in light; it is fronted by a short colonnade with a triangular pediment. The height to the oculus and the diameter of the interior circle are the same, , so the whole interior would fit exactly within a cube, and the interior could house a sphere of the same diameter. These dimensions make more sense when expressed in ancient Roman units of measurement: The dome spans 150 Roman feet); the oculus is 30 Roman feet in diameter; the doorway is 40 Roman feet high. The Pantheon remains the world's largest unreinforced concrete dome.\n\nThe first Renaissance treatise on architecture was Leon Battista Alberti's 1450 \"De re aedificatoria\" (On the Art of Building); it became the first printed book on architecture in 1485. It was partly based on Vitruvius's \"De architectura\" and, via Nicomachus, Pythagorean arithmetic. Alberti starts with a cube, and derives ratios from it. Thus the diagonal of a face gives the ratio 1:, while the diameter of the sphere which circumscribes the cube gives 1:. Alberti also documented Filippo Brunelleschi's discovery of linear perspective, developed to enable the design of buildings which would look beautifully proportioned when viewed from a convenient distance.\n\nThe next major text was Sebastiano Serlio's \"Regole generali d'architettura\" (General Rules of Architecture); the first volume appeared in Venice in 1537; the 1545 volume (books 1 and 2) covered geometry and perspective. Two of Serlio's methods for constructing perspectives were wrong, but this did not stop his work being widely used.\nIn 1570, Andrea Palladio published the influential \"I quattro libri dell'architettura\" (The Four Books of Architecture) in Venice. This widely printed book was largely responsible for spreading the ideas of the Italian Renaissance throughout Europe, assisted by proponents like the English diplomat Henry Wotton with his 1624 \"The Elements of Architecture\". The proportions of each room within the villa were calculated on simple mathematical ratios like 3:4 and 4:5, and the different rooms within the house were interrelated by these ratios. Earlier architects had used these formulas for balancing a single symmetrical facade; however, Palladio's designs related to the whole, usually square, villa. Palladio permitted a range of ratios in the \"Quattro libri\", stating:\n\nIn 1615, Vincenzo Scamozzi published the late Renaissance treatise \"L'Idea dell'Architettura Universale\" (The Idea of a Universal Architecture). He attempted to relate the design of cities and buildings to the ideas of Vitruvius and the Pythagoreans, and to the more recent ideas of Palladio.\n\nHyperboloid structures were used starting towards the end of the nineteenth century by Vladimir Shukhov for masts, lighthouses and cooling towers. Their striking shape is both aesthetically interesting and strong, using structural materials economically. Shukhov's first hyperboloidal tower was exhibited in Nizhny Novgorod in 1896.\n\nThe early twentieth century movement Modern Architecture, pioneered by Russian Constructivism, used rectilinear Euclidean (also called Cartesian) geometry. In the De Stijl movement, the horizontal and the vertical were seen as constituting the universal. The architectural form consists of putting these two directional tendencies together, using roof planes, wall planes and balconies, which either slide past or intersect each other, as in the 1924 Rietveld Schröder House by Gerrit Rietveld.\nModernist architects were free to make use of curves as well as planes. Charles Holden's 1933 Arnos station has a circular ticket hall in brick with a flat concrete roof. In 1938, the Bauhaus painter Laszlo Moholy-Nagy adopted Raoul Heinrich Francé's seven biotechnical elements, namely the crystal, the sphere, the cone, the plane, the (cuboidal) strip, the (cylindrical) rod, and the spiral, as the supposed basic building blocks of architecture inspired by nature.\n\nLe Corbusier proposed an anthropometric scale of proportions in architecture, the Modulor, based on the supposed height of a man. Le Corbusier's 1955 Chapelle Notre Dame du Haut uses free-form curves not describable in mathematical formulae. The shapes are said to be evocative of natural forms such as the prow of a ship or praying hands. The design is only at the largest scale: there is no hierarchy of detail at smaller scales, and thus no fractal dimension; the same applies to other famous twentieth-century buildings such as the Sydney Opera House, Denver International Airport, and the Guggenheim Museum, Bilbao.\n\nContemporary architecture, in the opinion of the 90 leading architects who responded to a 2010 World Architecture Survey, is extremely diverse; the best was judged to be Frank Gehry's Guggenheim Museum, Bilbao.\n\nDenver International Airport's terminal building, completed in 1995, has a fabric roof supported as a minimal surface (i.e., its mean curvature is zero) by steel cables. It evokes Colorado's snow-capped mountains and the teepee tents of Native Americans.\n\nThe architect Richard Buckminster Fuller is famous for designing strong thin-shell structures known as geodesic domes. The Montréal Biosphère dome is high; its diameter is .\n\nSydney Opera House has a dramatic roof consisting of soaring white vaults, reminiscent of ship's sails; to make them possible to construct using standardized components, the vaults are all composed of triangular sections of spherical shells with the same radius. These have the required uniform curvature in every direction.\n\nThe late twentieth century movement \"Deconstructivism\" creates deliberate disorder with what Nikos Salingaros in \"A Theory of Architecture\" calls random forms of high complexity by using non-parallel walls, superimposed grids and complex 2-D surfaces, as in Frank Gehry's Disney Concert Hall and Guggenheim Museum, Bilbao. Until the twentieth century, architecture students were obliged to have a grounding in mathematics. Salingaros argues that first \"overly simplistic, politically-driven\" Modernism and then \"anti-scientific\" Deconstructivism have effectively separated architecture from mathematics. He believes that this \"reversal of mathematical values\" is harmful, as the \"pervasive aesthetic\" of non-mathematical architecture trains people \"to reject mathematical information in the built environment\"; he argues that this has negative effects on society.\n\nThe pyramids of Ancient Egypt are tombs constructed with deliberately chosen proportions, but which these were has been debated. The face angle is about 51°85’, and the ratio of the slant height to half the base length is 1.619, less than 1% from the golden ratio. If this was the design method, it would imply the use of Kepler's triangle (face angle 51°49’). However it is more likely that the pyramids' slope was chosen from the 3-4-5 triangle (face angle 53°8’), known from the Rhind Mathematical Papyrus (c. 1650 – 1550 BC); or from the triangle with base to hypotenuse ratio 1:4/π (face angle 51°50’).\n\nThe possible use of the 3-4-5 triangle to lay out right angles, such as for the ground plan of a pyramid, and the knowledge of Pythagoras theorem which that would imply, has been much asserted. It was first conjectured by the historian Moritz Cantor in 1882. It is known that right angles were laid out accurately in Ancient Egypt; that their surveyors did use knotted cords for measurement; that Plutarch recorded in \"Isis and Osiris\" (around 100 AD) that the Egyptians admired the 3-4-5 triangle; and that the Berlin Papyrus 6619 from the Middle Kingdom (before 1700 BC) stated that \"the area of a square of 100 is equal to that of two smaller squares. The side of one is ½ + ¼ the side of the other.\" The historian of mathematics Roger L. Cooke observes that \"It is hard to imagine anyone being interested in such conditions without knowing the Pythagorean theorem.\" Against this, Cooke notes that no Egyptian text before 300 BC actually mentions the use of the theorem to find the length of a triangle's sides, and that there are simpler ways to construct a right angle. Cooke concludes that Cantor's conjecture remains uncertain: he guesses that the Ancient Egyptians probably did know the Pythagorean theorem, but that \"there is no evidence that they used it to construct right angles\".\n\nVaastu Shastra, the ancient Indian canons of architecture and town planning, employs symmetrical drawings called mandalas. Complex calculations are used to arrive at the dimensions of a building and its components. The designs are intended to integrate architecture with nature, the relative functions of various parts of the structure, and ancient beliefs utilizing geometric patterns (yantra), symmetry and directional alignments. However, early builders may have come upon mathematical proportions by accident. The mathematician Georges Ifrah notes that simple \"tricks\" with string and stakes can be used to lay out geometric shapes, such as ellipses and right angles.\n\nThe mathematics of fractals has been used to show that the reason why existing buildings have universal appeal and are visually satisfying is because they provide the viewer with a sense of scale at different viewing distances. For example, in the tall gopuram gatehouses of Hindu temples such as the Virupaksha Temple at Hampi built in the seventh century, and others such as the Kandariya Mahadev Temple at Khajuraho, the parts and the whole have the same character, with fractal dimension in the range 1.7 to 1.8. The cluster of smaller towers (\"shikhara\", lit. 'mountain') about the tallest, central, tower which represents the holy Mount Kailash, abode of Lord Shiva, depicts the endless repetition of universes in Hindu cosmology. The religious studies scholar William J. Jackson observed of the pattern of towers grouped among smaller towers, themselves grouped among still smaller towers, that:\n\nThe Meenakshi Amman Temple is a large complex with multiple shrines, with the streets of Madurai laid out concentrically around it according to the shastras. The four gateways are tall towers (gopurams) with fractal-like repetitive structure as at Hampi. The enclosures around each shrine are rectangular and surrounded by high stone walls.\n\nPythagoras (c. 569 – c. 475 B.C.) and his followers, the Pythagoreans, held that \"all things are numbers\". They observed the harmonies produced by notes with specific small-integer ratios of frequency, and argued that buildings too should be designed with such ratios. The Greek word \"symmetria\" originally denoted the harmony of architectural shapes in precise ratios from a building's smallest details right up to its entire design.\nThe Parthenon is long, wide and high to the cornice. This gives a ratio of width to length of 4:9, and the same for height to width. Putting these together gives height:width:length of 16:36:81, or to the delight of the Pythagoreans 4:6:9. This sets the module as 0.858 m. A 4:9 rectangle can be constructed as three contiguous rectangles with sides in the ratio 3:4. Each half-rectangle is then a convenient 3:4:5 right triangle, enabling the angles and sides to be checked with a suitably knotted rope. The inner area (naos) similarly has 4:9 proportions ( wide by 48.3 m long); the ratio between the diameter of the outer columns, , and the spacing of their centres, , is also 4:9.\n\nThe Parthenon is considered by authors such as John Julius Norwich \"the most perfect Doric temple ever built\". Its elaborate architectural refinements include \"a subtle correspondence between the curvature of the stylobate, the taper of the naos walls and the \"entasis\" of the columns\". \"Entasis\" refers to the subtle diminution in diameter of the columns as they rise. The stylobate is the platform on which the columns stand. As in other classical Greek temples, the platform has a slight parabolic upward curvature to shed rainwater and reinforce the building against earthquakes. The columns might therefore be supposed to lean outwards, but they actually lean slightly inwards so that if they carried on, they would meet about a mile above the centre of the building; since they are all the same height, the curvature of the outer stylobate edge is transmitted to the architrave and roof above: \"all follow the rule of being built to delicate curves\".\n\nThe golden ratio was known in 300 B.C., when Euclid described the method of geometric construction. It has been argued that the golden ratio was used in the design of the Parthenon and other ancient Greek buildings, as well as sculptures, paintings, and vases. More recent authors such as Nikos Salingaros, however, doubt all these claims. Experiments by the computer scientist George Markowsky failed to find any preference for the golden rectangle.\n\nThe historian of Islamic art Antonio Fernandez-Puertas suggests that the Alhambra, like the Great Mosque of Cordoba, was designed using the Hispano-Muslim foot or \"codo\" of about . In the palace's Court of the Lions, the proportions follow a series of surds. A rectangle with sides 1 and has (by Pythagoras's theorem) a diagonal of , which describes the right triangle made by the sides of the court; the series continues with (giving a 1:2 ratio), and so on. The decorative patterns are similarly proportioned, generating squares inside circles and eight-pointed stars, generating six-pointed stars. There is no evidence to support earlier claims that the golden ratio was used in the Alhambra. The Court of the Lions is bracketed by the Hall of Two Sisters and the Hall of the Abencerrajes; a regular hexagon can be drawn from the centres of these two halls and the four inside corners of the Court of the Lions.\n\nThe Selimiye Mosque in Edirne, Turkey, was built by Mimar Sinan to provide a space where the mihrab could be see from anywhere inside the building. The very large central space is accordingly arranged as an octagon, formed by 8 enormous pillars, and capped by a circular dome of diameter and high. The octagon is formed into a square with four semidomes, and externally by four exceptionally tall minarets, tall. The building's plan is thus a circle inside an octagon inside a square.\n\nMughal architecture, as seen in the abandoned imperial city of Fatehpur Sikri and the Taj Mahal complex, has a distinctive mathematical order and a strong aesthetic based on symmetry and harmony.\n\nThe Taj Mahal exemplifies Mughal architecture, both representing paradise and displaying the Mughal Emperor Shah Jahan's power through its scale, symmetry and costly decoration. The white marble mausoleum, decorated with pietra dura, the great gate (\"Darwaza-i rauza\"), other buildings, the gardens and paths together form a unified hierarchical design. The buildings include a mosque in red sandstone on the west, and an almost identical building, the Jawab or 'answer' on the east to maintain the bilateral symmetry of the complex. The formal charbagh ('fourfold garden') is in four parts, symbolising the four rivers of paradise, and offering views and reflections of the mausoleum. These are divided in turn into 16 parterres.\n\nThe Taj Mahal complex was laid out on a grid, subdivided into smaller grids. The historians of architecture Koch and Barraud agree with the traditional accounts that give the width of the complex as 374 Mughal yards or gaz, the main area being three 374-gaz squares. These were divided in areas like the bazaar and caravanserai into 17-gaz modules; the garden and terraces are in modules of 23 gaz, and are 368 gaz wide (16 x 23). The mausoleum, mosque and guest house are laid out on a grid of 7 gaz. Koch and Barraud observe that if an octagon, used repeatedly in the complex, is given sides of 7 units, then it has a width of 17 units, which may help to explain the choice of ratios in the complex.\n\nThe Christian patriarchal basilica of Haghia Sophia in Byzantium (now Istanbul), first constructed in 537 (and twice rebuilt), was for a thousand years the largest cathedral ever built. It inspired many later buildings including Sultan Ahmed and other mosques in the city. The Byzantine architecture includes a nave crowned by a circular dome and two half-domes, all of the same diameter (), with a further five smaller half-domes forming an apse and four rounded corners of a vast rectangular interior. This was interpreted by mediaeval architects as representing the mundane below (the square base) and the divine heavens above (the soaring spherical dome). The emperor Justinian used two geometers, Isidore of Miletus and Anthemius of Tralles as architects; Isidore compiled the works of Archimedes on solid geometry, and was influenced by him.\n\nThe importance of water baptism in Christianity was reflected in the scale of baptistry architecture. The oldest, the Lateran Baptistry in Rome, built in 440, set a trend for octagonal baptistries; the baptismal font inside these buildings was often octagonal, though Italy's largest baptistry, at Pisa, built between 1152 and 1363, is circular, with an octagonal font. It is high, with a diameter of (a ratio of 8:5). Saint Ambrose wrote that fonts and baptistries were octagonal \"because on the eighth day, by rising, Christ loosens the bondage of death and receives the dead from their graves.\"\nSaint Augustine similarly described the eighth day as \"everlasting ... hallowed by the resurrection of Christ\". The octagonal Baptistry of Saint John, Florence, built between 1059 and 1128, is one of the oldest buildings in that city, and one of the last in the direct tradition of classical antiquity; it was extremely influential in the subsequent Florentine Renaissance, as major architects including Francesco Talenti, Alberti and Brunelleschi used it as the model of classical architecture.\n\nThe number five is used \"exuberantly\" in the 1721 Pilgrimage Church of St John of Nepomuk at Zelená hora, near Žďár nad Sázavou in the Czech republic, designed by Jan Blažej Santini Aichel. The nave is circular, surrounded by five pairs of columns and five oval domes alternating with ogival apses. The church further has five gates, five chapels, five altars and five stars; a legend claims that when Saint John of Nepomuk was martyred, five stars appeared over his head. The fivefold architecture may also symbolise the five wounds of Christ and the five letters of \"Tacui\" (Latin: \"I kept silence\" [about secrets of the confessional]).\n\nAntoni Gaudí used a wide variety of geometric structures, some being minimal surfaces, in the Sagrada Família, Barcelona, started in 1882 (and not completed as of 2015). These include hyperbolic paraboloids and hyperboloids of revolution, tessellations, catenary arches, catenoids, helicoids, and ruled surfaces. This varied mix of geometries is creatively combined in different ways around the church. For example, in the Passion Façade of Sagrada Família, Gaudí assembled stone \"branches\" in the form of hyperbolic paraboloids, which overlap at their tops (directrices) without, therefore, meeting at a point. In contrast, in the colonnade there are hyperbolic paraboloidal surfaces that smoothly join other structures to form unbounded surfaces. Further, Gaudí exploits natural patterns, themselves mathematical, with columns derived from the shapes of trees, and lintels made from unmodified basalt naturally cracked (by cooling from molten rock) into hexagonal columns.\n\nThe 1971 Cathedral of Saint Mary of the Assumption, San Francisco has a saddle roof composed of eight segments of hyperbolic paraboloids, arranged so that the bottom horizontal cross section of the roof is a square and the top cross section is a Christian cross. The building is a square on a side, and high. The 1970 Cathedral of Brasília by Oscar Niemeyer makes a different use of a hyperboloid structure; it is constructed from 16 identical concrete beams, each weighing 90 tonnes, arranged in a circle to form a hyperboloid of revolution, the white beams creating a shape like hands praying to heaven. Only the dome is visible from outside: most of the building is below ground.\n\nSeveral medieval churches in Scandinavia are circular, including four on the Danish island of Bornholm. One of the oldest of these, Østerlars Church from c. 1160, has a circular nave around a massive circular stone column, pierced with arches and decorated with a fresco. The circular structure has three storeys and was apparently fortified, the top storey having served for defence.\n\nIslamic buildings are often decorated with geometric patterns which typically make use of several mathematical tessellations, formed of ceramic tiles (girih, zellige) that may themselves be plain or decorated with stripes. Symmetries such as stars with six, eight, or multiples of eight points are used in Islamic patterns. Some of these are based on the 'Khatem Sulemani' or Solomon's seal motif, which is an eight-pointed star made of two squares, one rotated 45 degrees from the other on the same centre. Islamic patterns exploit many of the 17 possible wallpaper groups; as early as 1944, Edith Müller showed that the Alhambra made use of 11 wallpaper groups in its decorations, while in 1986 Branko Grünbaum claimed to have found 13 wallpaper groups in the Alhambra, asserting controversially that the remaining 4 groups are not found anywhere in Islamic ornament.\n\nTowards the end of the 20th century, novel mathematical constructs such as fractal geometry and aperiodic tiling were seized upon by architects to provide interesting and attractive coverings for buildings. In 1913, the Modernist architect Adolf Loos had declared that \"Ornament is a crime\", influencing architectural thinking for the rest of the 20th century. In the 21st century, architects are again starting to explore the use of ornament. 21st century ornamentation is extremely diverse. Henning Larsen's 2011 Harpa Concert and Conference Centre, Reykjavik has what looks like a crystal wall of rock made of large blocks of glass. Foreign Office Architects' 2010 Ravensbourne College, London is tessellated decoratively with 28,000 anodised aluminium tiles in red, white and brown, interlinking circular windows of differing sizes. The tessellation uses three types of tile, an equilateral triangle and two irregular pentagons. Kazumi Kudo's Kanazawa Umimirai Library creates a decorative grid made of small circular blocks of glass set into plain concrete walls.\n\nThe architecture of fortifications evolved from medieval fortresses, which had high masonry walls, to low, symmetrical star forts able to resist artillery bombardment between the mid-fifteenth and nineteenth centuries. The geometry of the star shapes was dictated by the need to avoid dead zones where attacking infantry could shelter from defensive fire; the sides of the projecting points were angled to permit such fire to sweep the ground, and to provide crossfire (from both sides) beyond each projecting point. Well-known architects who designed such defences include Michelangelo, Baldassare Peruzzi, Vincenzo Scamozzi and Sébastien Le Prestre de Vauban.\n\nThe architectural historian Siegfried Giedion argued that the star-shaped fortification had a formative influence on the patterning of the Renaissance ideal city: \"The Renaissance was hypnotized by one city type which for a century and a half—from Filarete to Scamozzi—was impressed upon all utopian schemes: this is the star-shaped city.\"\n\nIn Chinese architecture, the tulou of Fujian province are circular, communal defensive structures with mainly blank walls and a single iron-plated wooden door, some dating back to the sixteenth century. The walls are topped with roofs that slope gently both outwards and inwards, forming a ring. The centre of the circle is an open cobbled courtyard, often with a well, surrounded by timbered galleries up to five stories high.\n\nArchitects may also select the form of a building to meet environmental goals. For example, Foster and Partners' 30 St Mary Axe, London, known as \"The Gherkin\" for its cucumber-like shape, is a solid of revolution designed using parametric modelling. Its geometry was chosen not purely for aesthetic reasons, but to minimise whirling air currents at its base. Despite the building's apparently curved surface, all the panels of glass forming its skin are flat, except for the lens at the top. Most of the panels are quadrilaterals, as they can be cut from rectangular glass with less wastage than triangular panels.\n\nThe traditional yakhchal (ice pit) of Persia functioned as an evaporative cooler. Above ground, the structure had a domed shape, but had a subterranean storage space for ice and sometimes food as well. The subterranean space and the thick heat-resistant construction insulated the storage space year round. The internal space was often further cooled with windcatchers. The ice was available in the summer to make the frozen dessert faloodeh.\n\n\n"}
{"id": "326471", "url": "https://en.wikipedia.org/wiki?curid=326471", "title": "Mathematics education", "text": "Mathematics education\n\nIn contemporary education, mathematics education is the practice of teaching and learning mathematics, along with the associated scholarly research.\n\nResearchers in mathematics education are primarily concerned with the tools, methods and approaches that facilitate practice or the study of practice; however, mathematics education research, known on the continent of Europe as the didactics or pedagogy of mathematics, has developed into an extensive field of study, with its own concepts, theories, methods, national and international organisations, conferences and literature. This article describes some of the history, influences and recent controversies.\n\nElementary mathematics was part of the education system in most ancient civilisations, including Ancient Greece, the Roman Empire, Vedic society and ancient Egypt. In most cases, a formal education was only available to male children with a sufficiently high status, wealth or caste.\nIn Plato's division of the liberal arts into the trivium and the quadrivium, the quadrivium included the mathematical fields of arithmetic and geometry. This structure was continued in the structure of classical education that was developed in medieval Europe. Teaching of geometry was almost universally based on Euclid's \"Elements\". Apprentices to trades such as masons, merchants and money-lenders could expect to learn such practical mathematics as was relevant to their profession.\n\nIn the Renaissance, the academic status of mathematics declined, because it was strongly associated with trade and commerce, and considered somewhat un-Christian. Although it continued to be taught in European universities, it was seen as subservient to the study of Natural, Metaphysical and Moral Philosophy. The first modern arithmetic curriculum (starting with addition, then subtraction, multiplication, and division) arose at reckoning schools in Italy in the 1300s. Spreading along trade routes, these methods were designed to be used in commerce. They contrasted with Platonic math taught at universities, which was more philosophical and concerned numbers as concepts rather than calculating methods. They also contrasted with mathematical methods learned by artisan apprentices, which were specific to the tasks and tools at hand. For example, the division of a board into thirds can be accomplished with a piece of string, instead of measuring the length and using the arithmetic operation of division.\n\nThe first mathematics textbooks to be written in English and French were published by Robert Recorde, beginning with \"The Grounde of Artes\" in 1540. However, there are many different writings on mathematics and mathematics methodology that date back to 1800 BCE. These were mostly located in Mesopotamia where the Sumerians were practicing multiplication and division. There are also artifacts demonstrating their own methodology for solving equations like the quadratic equation. After the Sumerians some of the most famous ancient works on mathematics come from Egypt in the form of the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus. The more famous Rhind Papyrus has been dated to approximately 1650 BCE but it is thought to be a copy of an even older scroll. This papyrus was essentially an early textbook for Egyptian students.\n\nThe social status of mathematical study was improving by the seventeenth century, with the University of Aberdeen creating a Mathematics Chair in 1613, followed by the Chair in Geometry being set up in University of Oxford in 1619 and the Lucasian Chair of Mathematics being established by the University of Cambridge in 1662. However, it was uncommon for mathematics to be taught outside of the universities. Isaac Newton, for example, received no formal mathematics teaching until he joined Trinity College, Cambridge in 1661.\n\nIn the 18th and 19th centuries, the Industrial Revolution led to an enormous increase in urban populations. Basic numeracy skills, such as the ability to tell the time, count money and carry out simple arithmetic, became essential in this new urban lifestyle. Within the new public education systems, mathematics became a central part of the curriculum from an early age.\n\nBy the twentieth century, mathematics was part of the core curriculum in all developed countries.\n\nDuring the twentieth century, mathematics education was established as an independent field of research. Here are some of the main events in this development:\n\n\nIn the 20th century, the cultural impact of the \"electronic age\" (McLuhan) was also taken up by educational theory and the teaching of mathematics. While previous approach focused on \"working with specialized 'problems' in arithmetic\", the emerging structural approach to knowledge had \"small children meditating about number theory and 'sets'.\"\n\nAt different times and in different cultures and countries, mathematics education has attempted to achieve a variety of different objectives. These objectives have included:\n\n\nThe method or methods used in any particular context are largely determined by the objectives that the relevant educational system is trying to achieve. Methods of teaching mathematics include the following:\n\n\n\nDifferent levels of mathematics are taught at different ages and in somewhat different sequences in different countries. Sometimes a class may be taught at an earlier age than typical as a special or honors class.\n\nElementary mathematics in most countries is taught in a similar fashion, though there are differences. In the United States fractions are typically taught starting from 1st grade, whereas in other countries they are usually taught later, since the metric system does not require young children to be familiar with them. Most countries tend to cover fewer topics in greater depth than in the United States. K-12 topics include elementary arithmetic (addition, subtraction, multiplication, and division), and pre-algebra.\n\nIn most of the U.S., algebra, geometry and analysis (pre-calculus and calculus) are taught as separate courses in different years of high school. Mathematics in most other countries (and in a few U.S. states) is integrated, with topics from all branches of mathematics studied every year. Students in many countries choose an option or pre-defined course of study rather than choosing courses \"à la carte\" as in the United States. Students in science-oriented curricula typically study differential calculus and trigonometry at age 16–17 and integral calculus, complex numbers, analytic geometry, exponential and logarithmic functions, and infinite series in their final year of secondary school. Probability and statistics may be taught in secondary education classes.\n\nScience and engineering students in colleges and universities may be required to take multivariable calculus, differential equations, linear algebra. Applied mathematics is also used in specific majors; for example, civil engineers may be required to study fluid mechanics, while \"math for computer science\" might include graph theory, permutation, probability, and proofs. Mathematics students would continue to study potentially any area.\n\nThroughout most of history, standards for mathematics education were set locally, by individual schools or teachers, depending on the levels of achievement that were relevant to, realistic for, and considered socially appropriate for their pupils.\n\nIn modern times, there has been a move towards regional or national standards, usually under the umbrella of a wider standard school curriculum. In England, for example, standards for mathematics education are set as part of the National Curriculum for England, while Scotland maintains its own educational system. In the USA, the National Governors Association Center for Best Practices and the Council of Chief State School Officers have published the national mathematics Common Core State Standards Initiative.\n\nMa (2000) summarised the research of others who found, based on nationwide data, that students with higher scores on standardised mathematics tests had taken more mathematics courses in high school. This led some states to require three years of mathematics instead of two. But because this requirement was often met by taking another lower level mathematics course, the additional courses had a “diluted” effect in raising achievement levels.\n\nIn North America, the National Council of Teachers of Mathematics has published the Principles and Standards for School Mathematics, which boosted the trend towards reform mathematics. In 2006, they released \"Curriculum Focal Points\", which recommend the most important mathematical topics for each grade level through grade 8. However, these standards are enforced as American states and Canadian provinces choose. A US state's adoption of the Common Core State Standards in mathematics is at the discretion of the state, and is not mandated by the Federal Government. \"States routinely review their academic standards and may choose to change or add onto the standards to best meet the needs of their students.\" The National Council of Teachers of Mathematics has state affiliates that have different education standards at the state level. For example, Missouri has the Missouri Council of Teachers of Mathematics (MCTM) which has its own pillars and standards of education listed on its website. The MCTM also offers membership opportunities to teachers and future teachers so they can stay up to date on the changes in math educational standards. \n\n\"Robust, useful theories of classroom teaching do not yet exist\". However, there are useful theories on how children learn mathematics and much research has been conducted in recent decades to explore how these theories can be applied to teaching. The following results are examples of some of the current findings in the field of mathematics education:\n\n\n\n\n\n\n\nAs with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods in order to test their effects. They depend on large samples to obtain statistically significant results.\n\nQualitative research, such as case studies, action research, discourse analysis, and clinical interviews, depend on small but focused samples in an attempt to understand student learning and to look at how and why a given method gives the results it does. Such studies cannot conclusively establish that one method is better than another, as randomized trials can, but unless it is understood \"why\" treatment X is better than treatment Y, application of results of quantitative studies will often lead to \"lethal mutations\" of the finding in actual classrooms. Exploratory qualitative research is also useful for suggesting new hypotheses, which can eventually be tested by randomized experiments. Both qualitative and quantitative studies therefore are considered essential in education—just as in the other social sciences. Many studies are “mixed”, simultaneously combining aspects of both quantitative and qualitative research, as appropriate.\n\nThere has been some controversy over the relative strengths of different types of research. Because randomized trials provide clear, objective evidence on “what works”, policy makers often take only those studies into consideration. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes. In other disciplines concerned with human subjects, like biomedicine, psychology, and policy evaluation, controlled, randomized experiments remain the preferred method of evaluating treatments. Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods. On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective, or the difficulty of assuring rigid control of the independent variable in fluid, real school settings.\n\nIn the United States, the National Mathematics Advisory Panel (NMAP) published a report in 2008 based on studies, some of which used randomized assignment of treatments to experimental units, such as classrooms or students. The NMAP report's preference for randomized experiments received criticism from some scholars. In 2010, the What Works Clearinghouse (essentially the research arm for the Department of Education) responded to ongoing controversy by extending its research base to include non-experimental studies, including regression discontinuity designs and single-case studies.\n\nThe following are some of the people who have had a significant influence on the teaching of mathematics at various periods in history:\n\n\nThe following people all taught mathematics at some stage in their lives, although they are better known for other things:\n\n\n\n\n\n\n\n"}
{"id": "42908722", "url": "https://en.wikipedia.org/wiki?curid=42908722", "title": "Model order reduction", "text": "Model order reduction\n\nModel order reduction (MOR) is a technique for reducing the computational complexity of mathematical models in numerical simulations. As such it is closely related to the concept of metamodeling with applications in all areas of mathematical modelling.\n\nMany modern mathematical models of real-life processes pose challenges when used in numerical simulations, due to complexity and large size (dimension). Model order reduction aims to lower the computational complexity of such problems, for example, in simulations of large-scale dynamical systems and control systems. By a reduction of the model's associated state space dimension or degrees of freedom, an approximation to the original model is computed which is commonly referred to as a reduced order model.\n\nReduced order models are useful in settings where it is often unfeasible to perform numerical simulations using the complete full order model. This can be due to limitations in computational resources or the requirements of the simulations setting, for instance real-time simulation settings or many-query settings in which a large number of simulations needs to be performed. Examples of Real-time simulation settings include control systems in electronics and visualization of model results while examples for a many-query setting can include optimization problems and design exploration. In order to be applicable to real-world problems, often the requirements of a reduced order model are:\n\nModel order reduction techniques used most commonly nowadays can be broadly classified into 4 classes:\n\n\nThe simplified physics approach can be described to be analogous to the traditional Mathematical modelling approach, in which a less complex description of a system is constructed based on assumptions and simplifications using physical insight or otherwise derived information. However, this approach is not often the topic of discussion in the context of model order reduction as it is a general method in science, engineering and mathematics and is not the subject of the current article.\n\nThe remaining listed methods fall into the category of projection-based reduction. Projection-based reduction relies on the projection of either the model equations or the solution onto a basis of reduced dimensionality compared to the original solution space. Methods that also fall into this class but are perhaps less commonly found are:\n\n\nModel order reduction finds application within all fields involving mathematical modelling and many reviews exist for the topics of electronics, fluid- and structural mechanics.\n\nCurrent Problems in fluid mechanics involve large dynamical systems representing many effects on many different scales. Computational fluid dynamics studies often involve models solving the navier-stokes with a number of degrees of freedom in the order of magnitude upwards of formula_1. The first usage of model order reduction techniques dates back to the work of Lumley in 1967 where it was used to gain insight into the mechanisms and intensity of turbulence and large coherent structures present in fluid flow problems. Model order reduction also finds modern applications in Aeronautics to model the flow over the body of aircraft. An example can be found in Lieu et al in which the full order model of an F16 fighter-aircraft with over 2.1 million degrees of freedom, was reduced to a model of just 90 degrees of freedom. Additionally reduced order modelling has been applied to study rheology in Hemodynamics and the Fluid–structure interactionbetween the blood flowing through the vascular system and the vascular walls.\n\n\n"}
{"id": "12266333", "url": "https://en.wikipedia.org/wiki?curid=12266333", "title": "Modern Arabic mathematical notation", "text": "Modern Arabic mathematical notation\n\nModern Arabic mathematical notation is a mathematical notation based on the Arabic script, used especially at pre-university levels of education. Its form is mostly derived from Western notation, but has some notable features that set it apart from its Western counterpart. The most remarkable of those features is the fact that it is written from right to left following the normal direction of the Arabic script. Other differences include the replacement of the Latin alphabet letters for symbols with Arabic letters and the use of Arabic names for functions and relations.\n\n\nNotation differs slightly from region to another. In tertiary education, most regions use the Western notation. The notation mainly differs in numeral system used, and in mathematical symbol used.\n\nThere are three numeral systems used in right to left mathematical notation.\n\nWritten numerals are arranged with their lowest-value digit to the right, with higher value positions added to the left. That is identical to the arrangement used by Western texts using Hindu-Arabic numerals even though Arabic script is read from right to left. The symbols \"٫\" and \"٬\" may be used as the decimal mark and the thousands separator respectively when writing with Eastern Arabic numerals, e.g. \"3.14159265358\", \"1,000,000,000\". Negative signs are written to the left of magnitudes, e.g. \"−3\". In-line fractions are written with the numerator and denominator on the left and right of the fraction slash respectively, e.g. \"2/7\".\n\nSometimes, symbols used in Arabic mathematical notation differ according to the region:\n\nSometimes, mirrored Latin symbols are used in Arabic mathematical notation (especially in western Arabic regions):\n\nHowever, in Iran, usually Latin symbols are used.\n\nThe letter ( \"zayn\", from the first letter of the second word of \"hyperbolic function\") is added to the end of trigonometric functions to express hyperbolic functions. This is similar to the way formula_1 is added to the end of trigonometric functions in Latin-based notation.\nFor inverse trigonometric functions, the superscript in Arabic notation is similar in usage to the superscript formula_2 in Latin-based notation.\n\n\n"}
{"id": "3134388", "url": "https://en.wikipedia.org/wiki?curid=3134388", "title": "Nearest integer function", "text": "Nearest integer function\n\nIn computer science, the nearest integer function of real number \"x\" denoted variously by formula_1, formula_2, formula_3, nint(\"x\"), or Round(\"x\"), is a function which returns the nearest integer to \"x\". To avoid ambiguity when operating on half-integers, a rounding rule must be chosen. On most computer implementations, the selected rule is to round half-integers to the nearest even integer—for example, \nThis is in accordance with the IEEE 754 standards and helps reduce bias in the result.\n\nThere are many other possible rules for tie breaking when rounding a half integer include rounding up, rounding down, rounding to or away from zero, or random rounding up or down.\n\n"}
{"id": "355814", "url": "https://en.wikipedia.org/wiki?curid=355814", "title": "Outline of discrete mathematics", "text": "Outline of discrete mathematics\n\nDiscrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying \"smoothly\", the objects studied in discrete mathematics – such as integers, graphs, and statements in logic – do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as calculus and analysis.\n\nIncluded below are many of the standard terms used routinely in university-level courses and in research papers. This is not, however, intended as a complete list of mathematical terms; just a selection of typical \"terms of art\" that may be encountered.\n\n\nFor further reading in discrete mathematics, beyond a basic level, see these pages. Many of these disciplines are closely related to computer science.\n\n\n\n\n\nElementary algebra\n\n\n\nCombinatorics\n\nProbability\n\n\n\n"}
{"id": "15882673", "url": "https://en.wikipedia.org/wiki?curid=15882673", "title": "Plate notation", "text": "Plate notation\n\nIn Bayesian inference, plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.\n\nIn this example, we consider Latent Dirichlet allocation, a Bayesian network that models how documents in a corpus are topically related. There are two variables not in any plate; \"α\" is the parameter of the uniform Dirichlet prior on the per-document topic distributions, and \"β\" is the parameter of the uniform Dirichlet prior on the per-topic word distribution.\n\nThe outermost plate represents all the variables related to a specific document, including formula_1, the topic distribution for document \"i\". The \"M\" in the corner of the plate indicates that the variables inside are repeated \"M\" times, once for each document. The inner plate represents the variables associated with each of the formula_2 words in document \"i\": formula_3 is the topic for the \"j\"th word in document \"i\", and formula_4 is the actual word used.\n\nThe \"N\" in the corner represents the repetition of the variables in the inner plate formula_2 times, once for each word in document \"i\". The circle representing the individual words is shaded, indicating that each formula_4 is observable, and the other circles are empty, indicating that the other variables are latent variables. The directed edges between variables indicate dependencies between the variables: for example, each formula_4 depends on formula_3 and \"β\".\n\nA number of extensions have been created by various authors to express more information than simply the conditional relationships. However, few of these have become standard. Perhaps the most commonly used extension is to use rectangles in place of circles to indicate non-random variables—either parameters to be computed, hyperparameters given a fixed value (or computed through empirical Bayes), or variables whose values are computed deterministically from a random variable.\n\nThe diagram on the right shows a few more non-standard conventions used in some articles in Wikipedia (e.g. variational Bayes):\n\nPlate notation has been implemented in various TeX/LaTeX drawing packages, but also as part of graphical user interfaces to Bayesian statistics programs such as BUGS and BayesiaLab.\n"}
{"id": "1015795", "url": "https://en.wikipedia.org/wiki?curid=1015795", "title": "Proof by contrapositive", "text": "Proof by contrapositive\n\nIn logic, the contrapositive of a conditional statement is formed by negating both terms and reversing the direction of inference. Explicitly, the contrapositive of the statement \"if A, then B\" is \"if not B, then not A.\" A statement and its contrapositive are logically equivalent: if the statement is true, then its contrapositive is true, and vice versa.\n\nIn mathematics, proof by contraposition is a rule of inference used in proofs. This rule infers a conditional statement from its contrapositive. In other words, the conclusion \"if A, then B\" is drawn from the single premise \"if not B, then not A.\"\n\nLet \"x\" be an integer.\n\nAlthough a direct proof can be given, we choose to prove this statement by contraposition. The contrapositive of the above statement is:\n\nThis latter statement can be proven as follows. Suppose \"x\" is not even. Then \"x\" is odd. The product of two odd numbers is odd, hence \"x\"² = \"x\"·\"x\" is odd. Thus \"x\"² is not even.\n\nHaving proved the contrapositive, we infer the original statement.\n\n"}
{"id": "510629", "url": "https://en.wikipedia.org/wiki?curid=510629", "title": "Quantitative analyst", "text": "Quantitative analyst\n\nA quantitative analyst (or, in financial jargon, a quant) is a person who specializes in the application of mathematical and statistical methods – such as numerical or quantitative techniques – to financial and risk management problems. The occupation is similar to those in industrial mathematics in other industries.\n\nAlthough the original quantitative analysts were \"sell side quants\" from market maker firms, concerned with derivatives pricing and risk management, the meaning of the term has expanded over time to include those individuals involved in almost any application of mathematics in finance, including the buy side. Examples include statistical arbitrage, quantitative investment management, algorithmic trading, and electronic market making. \n\nQuantitative finance started in 1900 with Louis Bachelier's doctoral thesis \"Theory of Speculation\", which provided a model to price options under a Normal Distribution.\n\nHarry Markowitz's 1952 doctoral thesis \"Portfolio Selection\" and its published version was one of the first efforts in economics journals to formally adapt mathematical concepts to finance (mathematics was until then confined to mathematics, statistics or specialized economics journals). Markowitz formalized a notion of mean return and covariances for common stocks which allowed him to quantify the concept of \"diversification\" in a market. He showed how to compute the mean return and variance for a given portfolio and argued that investors should hold only those portfolios whose variance is minimal among all portfolios with a given mean return. Although the language of finance now involves Itō calculus, management of risk in a quantifiable manner underlies much of the modern theory.\n\nIn 1965 Paul Samuelson introduced stochastic calculus into the study of finance. In 1969 Robert Merton promoted continuous stochastic calculus and continuous-time processes. Merton was motivated by the desire to understand how prices are set in financial markets, which is the classical economics question of \"equilibrium,\" and in later papers he used the machinery of stochastic calculus to begin investigation of this issue.\n\nAt the same time as Merton's work and with Merton's assistance, Fischer Black and Myron Scholes developed the Black–Scholes model, which was awarded the 1997 Nobel Memorial Prize in Economic Sciences. It provided a solution for a practical problem, that of finding a fair price for a European call option, i.e., the right to buy one share of a given stock at a specified price and time. Such options are frequently purchased by investors as a risk-hedging device. In 1981, Harrison and Pliska used the general theory of continuous-time stochastic processes to put the Black–Scholes model on a solid theoretical basis, and showed how to price numerous other derivative securities.\n\nEmanuel Derman's 2004 book \"My Life as a Quant\" helped to both make the role of a quantitative analyst better known outside of finance, and to popularize the abbreviation \"quant\" for a quantitative analyst.\n\nQuantitative analysts often come from applied mathematics, physics or engineering backgrounds rather than economics-related fields, and quantitative analysis is a major source of employment for people with mathematics and physics PhD degrees, or with financial mathematics DEA degrees in the French education system. Typically, a quantitative analyst will also need extensive skills in computer programming, most commonly C, C++, Java, R, MATLAB, Mathematica, Python.\n\nThis demand for quantitative analysts has led to a resurgence in demand for actuarial qualifications as well as creation of specialized Masters and PhD courses in financial engineering, mathematical finance, computational finance, and/or financial reinsurance. In particular, Master's degrees in mathematical finance, financial engineering, operations research, computational statistics, machine learning, and financial analysis are becoming more popular with students and with employers. See Master of Quantitative Finance; Master of Financial Economics.\n\nData science and machine learning analysis and modelling methods are being increasingly employed in portfolio performance and portfolio risk modelling, and as such data science and machine learning Master's graduates are also in demand as quantitative analysts.\n\nIn sales & trading, quantitative analysts work to determine prices, manage risk, and identify profitable opportunities. Historically this was a distinct activity from trading but the boundary between a desk quantitative analyst and a quantitative trader is increasingly blurred, and it is now difficult to enter trading as a profession without at least some quantitative analysis education. In the field of algorithmic trading it has reached the point where there is little meaningful difference. Front office work favours a higher speed to quality ratio, with a greater emphasis on solutions to specific problems than detailed modeling. FOQs typically are significantly better paid than those in back office, risk, and model validation. Although highly skilled analysts, FOQs frequently lack software engineering experience or formal training, and bound by time constraints and business pressures tactical solutions are often adopted.\n\nQuantitative analysis is used extensively by asset managers. Some, such as FQ, AQR or Barclays, rely almost exclusively on quantitative strategies while others, such as Pimco, Blackrock or Citadel use a mix of quantitative and fundamental methods.\n\nMajor firms invest large sums in an attempt to produce standard methods of evaluating prices and risk. These differ from front office tools in that Excel is very rare, with most development being in C++, though Java and C# are sometimes used in non-performance critical tasks. LQs spend more time modeling ensuring the analytics are both efficient and correct, though there is tension between LQs and FOQs on the validity of their results. LQs are required to understand techniques such as Monte Carlo methods and finite difference methods, as well as the nature of the products being modeled.\n\nOften the highest paid form of Quant, ATQs make use of methods taken from signal processing, game theory, gambling Kelly criterion, market microstructure, econometrics, and time series analysis. Algorithmic trading includes statistical arbitrage, but includes techniques largely based upon speed of response, to the extent that some ATQs modify hardware and Linux kernels to achieve ultra low latency.\n\nThis has grown in importance in recent years, as the credit crisis exposed holes in the mechanisms used to ensure that positions were correctly hedged, though in no bank does the pay in risk approach that in front office. A core technique is value at risk, and this is backed up with various forms of stress test (financial), economic capital analysis and direct analysis of the positions and models used by various bank's divisions.\n\nIn the aftermath of the financial crisis, there surfaced the recognition that quantitative valuation methods were generally too narrow in their approach. An agreed upon fix adopted by numerous financial institutions has been to improve collaboration.\n\nModel validation (MV) takes the models and methods developed by front office, library, and modeling quantitative analysts and determines their validity and correctness. The MV group might well be seen as a superset of the quantitative operations in a financial institution, since it must deal with new and advanced models and trading techniques from across the firm. Before the crisis however, the pay structure in all firms was such that MV groups struggle to attract and retain adequate staff, often with talented quantitative analysts leaving at the first opportunity. This gravely impacted corporate ability to manage model risk, or to ensure that the positions being held were correctly valued. An MV quantitative analyst would typically earn a fraction of quantitative analysts in other groups with similar length of experience. In the years following the crisis, this has changed. Regulators now typically talk directly to the quants in the middle office such as the model validators, and since profits highly depend of the regulatory infrastructure, model validation has gained in weight and importance with respect to the quants in the front office.\n\nQuantitative developers are computer specialists that assist, implement and maintain the quantitative models. They tend to be highly specialised language technicians that bridge the gap between software developer and quantitative analysts.\n\nBecause of their backgrounds, quantitative analysts draw from various forms of mathematics: statistics and probability, calculus centered around partial differential equations, linear algebra, discrete mathematics, and econometrics. Some on the buy side may use machine learning. The\nmajority of quantitative analysts have received little formal education in mainstream economics, and often apply a mindset drawn from the physical sciences. Quants use mathematical skills learned from diverse fields such as computer science, physics and engineering. These skills include (but are not limited to) advanced statistics, linear algebra and partial differential equations as well as solutions to these based upon numerical analysis.\n\nCommonly used numerical methods are:\n\nA typical problem for a mathematically oriented quantitative analyst would be to develop a model for pricing, hedging, and risk-managing a complex derivative product. These quantitative analysts tend to rely more on numerical analysis than statistics and econometrics. The mindset is to prefer a deterministically \"correct\" answer, as once there is agreement on input values and market variable dynamics, there is only one correct price for any given security (which can be demonstrated, albeit often inefficiently, through a large volume of Monte Carlo simulations).\n\nA typical problem for a statistically oriented quantitative analyst would be to develop a model for deciding which stocks are relatively expensive and which stocks are relatively cheap. The model might include a company's book value to price ratio, its trailing earnings to price ratio, and other accounting factors. An investment manager might implement this analysis by buying the underpriced stocks, selling the overpriced stocks, or both. Statistically oriented quantitative analysts tend to have more of a reliance on statistics and econometrics, and less of a reliance on sophisticated numerical techniques and object-oriented programming. These quantitative analysts tend to be of the psychology that enjoys trying to find the best approach to modeling data, and can accept that there is no \"right answer\" until time has passed and we can retrospectively see how the model performed. Both types of quantitative analysts demand a strong knowledge of sophisticated mathematics and computer programming proficiency.\n\nOne of the principal mathematical tools of quantitative finance is stochastic calculus.\n\n\n\n\n\n\n"}
{"id": "35411073", "url": "https://en.wikipedia.org/wiki?curid=35411073", "title": "Raymond Clare Archibald", "text": "Raymond Clare Archibald\n\nRaymond Clare Archibald (7 October 1875 – 26 July 1955) was a prominent Canadian-American mathematician. He is known for his work as a historian of mathematics, his editorships of mathematical journals and his contributions to the teaching of mathematics.\n\nRaymond Clare Archibald was born in South Branch, Stewiacke, Nova Scotia on 7 October 1875. He was the son of Abram Newcomb Archibald (1849—1883) and Mary Mellish Archibald (1849—1901). He was the fourth cousin twice removed of the famous Canadian-American astronomer and mathematician Simon Newcomb (1835—1909).\n\nArchibald graduated in 1894 from Mount Allison College with B.A. degree in mathematics and teacher's certificate in violin. After teaching mathematics and violin for a year at the Mount Allison Ladies’ College he went to Harvard where he received a B.A. 1896 and a M.A. in 1897. He then traveled to Europe where he attended the University of Berlin during 1898 and received a Ph.D.cum laude from the University of Strassburg in 1900. His advisor was Karl Theodor Reye and title of his dissertation was The Cardioide and Some of its Related Curves.\n\nHe returned to Canada in 1900 and taught mathematics and violin at the Mount Allison Ladies’ College until 1907. After a one-year appointment at Acadia University he accepted an invitation of join the mathematics department at Brown University. He stayed at Brown for the rest of his career becoming a Professor Emeritus in 1943. While at Brown he created one of the finest mathematical libraries in the western hemisphere.\n\nArchibald returned to Mount Allison in 1954 to curate the Mary Mellish Archibald Memorial Library, the library he had founded in 1905 to honor his mother. At his death the library contained 23,000 volumes, 2,700 records, and 70,000 songs in American and English poetry and drama.\n\nRaymond Clare Archibald was a world-renowned historian of mathematics with a lifelong concern for the teaching of mathematics in secondary schools. At the presentation of his portrait to Brown University the head of the mathematics department, Professor Clarence Raymond Adams (1898–1965) said of him:\n\n\"The instincts of the bibliophile were also his from early years. Possessing a passion for accurate detail, systematic by nature and blessed with a memory that was the marvel of his friends, he gradually acquired a knowledge of mathematical books and their values which has scarcely been equalled. This knowledge and an untiring energy he dedicated to the upbuilding of the mathematical library at Brown University. From modest beginnings he has developed this essential equipment of the mathematical investigator to a point where it has no superior, in completeness and in convenience for the user.\"\n\nArchibald received honorary degrees from the University of Padua (LL.D., 1922), Mount Allison University (LL.D., 1923) and from Brown University (M.A. ad eundem, 1943).\n\n\nArchibald’s bibliography contains over 1,000 entries. He contributed to over 20 different journals, mathematical, scientific, educational and literary. The following are the books of which he is an author:\n\n\n\n"}
{"id": "37554140", "url": "https://en.wikipedia.org/wiki?curid=37554140", "title": "Ruth Lyttle Satter Prize in Mathematics", "text": "Ruth Lyttle Satter Prize in Mathematics\n\nThe Ruth Lyttle Satter Prize in Mathematics, also called the Satter Prize, is one of twenty-one prizes given out by the American Mathematical Society (AMS). It is presented biennially in recognition of an outstanding contribution to mathematics research by a woman in the previous six years. The award was established in 1990 using a donation from Joan Birman, in memory of her sister, Ruth Lyttle Satter, who worked primarily in biological sciences, and was a proponent for equal opportunities for women in science. First awarded in 1991, the award is intended to \"honor [Satter's] commitment to research and to encourage women in science\". The winner is selected by the council of the AMS, based on the recommendation of a selection committee. The prize is awarded at the Joint Mathematics Meetings during odd numbered years, and has always carried a modest cash reward. Since 2003, the prize has been $5,000, while from 1997 to 2001, the prize came with $1,200, and prior to that it was $4,000. If a joint award is made, the prize money is split between the recipients.\n\n, the award has been given 14 times, to 15 different individuals. Dusa McDuff was the first recipient of the award, for her work on symplectic geometry. A joint award was made for the only time in 2001, when Karen E. Smith and Sijue Wu shared the award. The 2013 prize winner was Maryam Mirzakhani, who, in 2014, was the first woman to be awarded the Fields Medal. This is considered to be the highest honor a mathematician can receive. She won both awards for her work on \"the geometry of Riemann surfaces and their moduli spaces\". The most recent winner is Laura DeMarco, who was awarded the prize in 2017 for her \"fundamental contributions to complex dynamics, potential theory, and the emerging field of arithmetic dynamics\".\n\nThe Association for Women in Science have a similarly titled award, the Ruth Satter Memorial Award, which is a cash prize of $1,000 for \"an outstanding graduate student who interrupted her education for at least 3 years to raise a family\".\n"}
{"id": "562695", "url": "https://en.wikipedia.org/wiki?curid=562695", "title": "Simulink", "text": "Simulink\n\nSimulink, developed by MathWorks, is a graphical programming environment for modeling, simulating and analyzing multidomain dynamical systems. Its primary interface is a graphical block diagramming tool and a customizable set of block libraries. It offers tight integration with the rest of the MATLAB environment and can either drive MATLAB or be scripted from it. Simulink is widely used in automatic control and digital signal processing for multidomain simulation and Model-Based Design.\n\nMathWorks and other third-party hardware and software products can be used with Simulink. For example, Stateflow extends Simulink with a design environment for developing state machines and flow charts.\n\nMathWorks claims that, coupled with another of their products, Simulink can automatically generate C source code for real-time implementation of systems. As the efficiency and flexibility of the code improves, this is becoming more widely adopted for production systems, in addition to being a tool for embedded system design work because of its flexibility and capacity for quick iteration. Embedded Coder creates code efficient enough for use in embedded systems.\n\nSimulink Real-Time (formerly known as xPC Target), together with x86-based real-time systems, is an environment for simulating and testing Simulink and Stateflow models in real-time on the physical system. Another MathWorks product also supports specific embedded targets. When used with other generic products, Simulink and Stateflow can automatically generate synthesizable VHDL and Verilog.\nSimulink Verification and Validation enables systematic verification and validation of models through modeling style checking, requirements traceability and model coverage analysis. Simulink Design Verifier uses formal methods to identify design errors like integer overflow, division by zero and dead logic, and generates test case scenarios for model checking within the Simulink environment.\n\nSimEvents is used to add a library of graphical building blocks for modeling queuing systems to the Simulink environment, and to add an event-based simulation engine to the time-based simulation engine in Simulink.\n\nTherefore in Simulink any type of simulation can be done and the model can be simulated at any point in this environment.\n\nDifferent type of blocks can be accessed using the Simulink library browser. And therefore the benefit could be taken out from this environment efficiently.\n\n"}
{"id": "53561047", "url": "https://en.wikipedia.org/wiki?curid=53561047", "title": "Sphuṭacandrāpti", "text": "Sphuṭacandrāpti\n\nSphuṭacandrāpti (Computation of True Moon) is a treatise in Sanskrit composed by the fourteenth-century CE Kerala astronomer-mathematician Sangamagrama Madhava. The treatise enunciates a method for the computation of the position of the moon at intervals of 40 minutes each throughout the day. This is one of only two works of Madhava that have survived to modern times, the other one being \"Veṇvāroha\". However, both \"Sphuṭacandrāpti\" and \"Veṇvāroha\" have more or less the same contents, that of the latter being apparently a more refined version of that of the former.\n\nK. V. Sarma while working in Vishveshvaranand Institute of Sanskrit and Indological Studies, Hoshiarpur, has brought out in 1973 a critical edition of the treatise with an introduction, translation and notes. The full text of this work, which has only 65 pages, can be accessed from Internet Archive at the following link:\nScanned copies of the pages of the text referred to above are available in Wikimedia Commons at the following link:\n\n"}
{"id": "222390", "url": "https://en.wikipedia.org/wiki?curid=222390", "title": "Table of prime factors", "text": "Table of prime factors\n\nThe tables contain the prime factorization of the natural numbers from 1 to 1000.\n\nWhen \"n\" is a prime number, the prime factorization is just \"n\" itself, written in bold below.\n\nThe number 1 is called a unit. It has no prime factors and is neither prime nor composite.\n\n\"See also: Table of divisors\" (prime and non-prime divisors for 1 to 1000)\nMany properties of a natural number \"n\" can be seen or directly computed from the prime factorization of \"n\".\nThe divisors of \"n\" are all products of some or all prime factors of \"n\" (including the empty product 1 of no prime factors).\nThe number of divisors can be computed by increasing all multiplicities by 1 and then multiplying them.\nDivisors and properties related to divisors are shown in table of divisors.\n\n"}
{"id": "19537780", "url": "https://en.wikipedia.org/wiki?curid=19537780", "title": "Timeline of mathematics", "text": "Timeline of mathematics\n\nThis is a timeline of pure and applied mathematics history.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8997784", "url": "https://en.wikipedia.org/wiki?curid=8997784", "title": "What Is Mathematics?", "text": "What Is Mathematics?\n\nWhat Is Mathematics? is a mathematics book written by Richard Courant and Herbert Robbins, published in England by Oxford University Press. It is an introduction to mathematics, intended both for the mathematics student and for the general public.\n\nFirst published in 1941, it discusses number theory, geometry, topology and calculus. \nA second edition was published in 1996 with an additional chapter on recent progress in mathematics, written by Ian Stewart.\n\nThe book was based on Courant's course material.\nAlthough Robbins assisted in writing a large part of the book, \nhe had to fight for authorship.\nNevertheless, Courant alone held the copyright for the book.\nThis resulted in Robbins receiving a smaller share of the royalties.\n\nMichael Katehakis remembers Robbins' interest in the literature and Tolstoy in particular and he is convinced that the title of the book is most likely \ndue to Robbins, who was inspired by the title of the essay What is Art? by Leo Tolstoy. \nRobbins did the same in the book \n\" Great Expectations : The Theory of Optimal Stopping \" he co-authored with Yuan-Shih Chow and David Siegmund, \nwhere one can not miss the connection with the title of the novel \"Great Expectations\" by Charles Dickens.\n\nAccording to Constance Reid, Courant finalized the title after a conversation with Thomas Mann.\n\n\n\n\n"}
{"id": "24320887", "url": "https://en.wikipedia.org/wiki?curid=24320887", "title": "Yupana", "text": "Yupana\n\nA \"yupana\" (from Quechua yupay: count) is an abacus used to perform arithmetic operations dating back to the time of the Incas.\n\nThe term \"yupana\" refers to two distinct classes of objects:\n\n\nAlthough very different from each other, most of the scholars who have dealt with table-yupana, have then extended its reasoning and theories to the yupana of Poma de Ayala and vice versa, perhaps in an attempt to find a unifying thread or a common method. It should also be noted that the Nueva Coronica was discovered only in 1916 in the library of Copenhagen and that part of the studies on it were based on previous studies and theories regarding table-yupanas.\n\nSeveral chroniclers of the Indies described, unfortunately approximately, the Incan abacus and its operation.\n\nThe first was Guaman Poma de Ayala that in 1615 approximately, wrote:\n\nIn addition to providing this brief description, Poma de Ayala draws a picture of the yupana: a board of five rows and four columns in which are designed a series of white and black circles.\n\nThe father Jesuit \"José de Acosta\" wrote:\n\nFather \"Juan de Velasco\" wrote:\n\nThe first table-yupana which we know was found in 1869 in Chordeleg in the department of Cuenca (Ecuador). It is a rectangular table (33x27 cm) of wood which contains 17 compartments, of which 14 square, 2 rectangular and one octagonal. On two edges of the table there are other square compartments (12x12 cm) raised and symmetrically arranged one another, to which two square platforms (7x7 cm), are overlapped. These structures are called towers. The table presents a symmetry of the compartments with respect to the diagonal of the rectangle. The four sides of the board are also engraved with figures of human heads and a crocodile. As a result of this discovery, Charles Wiener began in 1877 a systematic study of these objects. Wiener came to the conclusion that the table-yupanas served to calculate the taxes that farmers paid to the Incan empire.\n\nFound at Caraz in 1878 - 1879, this table-yupana is different from that of Chordeleg as the material of construction is the stone and the central compartment of octagonal shape is replaced with a rectangular one; towers also have three shelves instead of two.\n\nA series of table-yupanas much different from the first, was described by Erland Nordenskiöld in 1931. These yupana, made of stone, present a series of rectangular and square compartments. The tower is composed of two rectangular compartments. The compartments are arranged symmetrically with respect to the axis of the smaller side of the table.\n\nThese yupana, made of stone, have 18 compartments of triangular shape, arranged around the table. On one side there is a rectangular tower with only one floor and three triangular compartments. In the central part there are four square compartments, coupled between them.\n\nIdentical to the yupana of Chordeleg, both for the material and the arrangement of the compartments, this table-yupana was found in the archaeological complex of Chan Chan in Peru in 1967.\n\nDiscovered in the province of Pisco (Peru), these table-yupanas are two tables in clay and bone. The first is rectangular (47x32 cm), has 22 square (5x5 cm) and three rectangular (16x18 cm) compartments, and has no towers. The second is rectangular (32x23 cm) containing 22 square compartments, two L-shaped and three rectangular in the center. The compartments are arranged symmetrically with respect to the axis of the longer side.\n\nDiscovered in the upper Ecuador by Max Uhle in 1922, this yupana is made of stone and its bins are drawn. It has the shape of a scale consisting of 10 overlapping rectangles: four on the first floor, three on the second, two in the third and one in the fourth. This yupana is the one that is closest to the picture by Poma de Ayala in Nueva Coronica, while having a line less and being half drawn.\n\nC. Florio presents a study \nwhich does not identify a yupana in these archaeological findings, but an object whose name is unknown and which has been forgotten. Instead, this object is to connect to the tocapu (an ideogram already used by pre-Incas civilizations) called “llave inca” (i.e. Inca key) and to the yanantin-masintin philosophy. The scholar reaches this conclusion starting from the lack of objective evidences which recognize a yupana in this object, a belief that consolidated over years only for the repeat of this hypothesis never demonstrated, and by crossing data from the Miccinelli Documents and the tocapu(s) catalogued by Victoria de la Jara. \nSupposing to colour the different compartments of the table-yupana (fig. A), C. Florio identifies a drawing (fig. B) very similar to a really existing tocapu (fig. C) and catalogued by Victoria de la Jara. In addition, in the tocapu reported in figure D, also catalogued by V. de la Jara, Florio identifies a stylization of the tocapu C and the departure point for creating the tocapu “llave inca” (Inca key). She finds the relation between the table-yupana and the Inca key also in their connection with the concept of duality: the table-yupana structure is clearly dual and Blas Valera in “Exul Immeritus Blas Valera populo suo” (one of the two Miccinelli Documents) describes the tocapu we call Inca key as representing the concept of the “opposite forces” and the “number 2”, both strictly linked to the concept of duality.\n\nAccording to C. Florio, the real yupana used by the Incas is that of Guáman Poma, but with more columns and rows. Guáman Poma would have represented just the part of the yupana useful for carrying out a specific calculation, which Florio identifies to be a multiplication (see below).\n\nIn 1931, Henry Wassen studied the yupana of Poma de Ayala, proposing for the first time a possible representation of the numbers on the board and the operations of addition and multiplication. He interpreted the white circles as gaps, carved into yupana in which to insert the seeds described by chroniclers: so the white circles correspond to empty gaps, while the blacks circles correspond to the same gaps filled with a black seed.\n\nThe numbering system at the base of the abacus was positional notation in base 10 (in line with the writings of the chroniclers of the Indies).\n\nThe representation of the numbers, then followed a vertical progression such that the units were positioned in the first row from the bottom, in the second the tens, hundreds in the third, and so on.\n\nWassen proposed a progression of values of the seeds that depends on their position in the table: 1, 5, 15, 30, respectively, depending on who occupy a gap in the first, second, third and fourth columns (see the table below). Only a maximum of five seeds could be included in a box belonging to the first column, so that the maximum value of said box was 5, multiplied by the power of the corresponding line. These seeds could be replaced with one seed of the next column, useful during arithmetic operations. According to the theory of Wassen, therefore, the operations of sum and product were carried out horizontally.\n\nThis theory received a lot of criticism due to the high complexity of the calculations and was therefore considered inadequate and soon abandoned.\n\nBy way of example, the following table shows the number 13457.\n\nThis first interpretation of the yupana of Poma de Ayala was the starting point for the theories developed by subsequent authors, up to the present day. In particular, no one ever moved away from the positional numbering system until 2008.\n\nEmilio Mendizabal was the first to propose in 1976 that the Inca were using, as well as the decimal representation, also a representation based on the progression 1,2,3,5. Mendizabal in the same publication pointed out that the series of numbers 1,2,3 and 5, in the drawing of Poma de Ayala, are part of the Fibonacci sequence, and stressed the importance of \"magic\" that had the number 5 for civilization the north of Peru, and the number 8 for the civilizations of the south of Peru.\n\nIn 1979, Carlos Radicati di Primeglio emphasized the difference of table-yupana from that of Poma de Ayala, describing the state of the art of the research and theories advanced so far. He also proposed the algorithms for calculating the four basic arithmetic operations for yupana of Poma de Ayala, according to a new interpretation for which it was possible to have up to nine seeds in each box with vertical progression for powers of ten. The choice of Radicati was to associate to each gap a value of 1.\n\nIn the following table is represented the number 13457\n\nIn 1981, the English textile engineer William Burns Glynn proposed a positional base 10 solution for the yupana of Poma de Ayala.\n\nGlynn, as Radicati, adopted the same Wassen's idea of full and empty gaps, as well as a vertical progression of the powers of ten, but proposed an architecture that allowed to greatly simplify the arithmetic operations.\n\nThe horizontal progression of the values of the seeds in its representation is 1, 1, 1 for the first three columns, so that in each row is possible to deposit a maximum of ten seeds (5 + 3 + 2 seeds). Ten seeds of any row is correspond to a single seed of the upper line.\n\nThe last column is dedicated to the \"memory\", which is a place where you can drop momentarily ten seeds, waiting to move them to the upper line. According to the author, this is very useful during arithmetic operations in order to reduce the possibility of error.\n\nThe solution of Glynn has been adopted in various teaching projects all over the world, and even today some of its variants are used in some schools of South America.\n\nIn the following table is represented the number 13457\n\nThe Italian engineer Nicolino de Pasquale in 2001 proposed a positional solution in base 40 of the yupana of Poma de Ayala, taking the representation theory of Fibonacci already proposed by Emilio Mendizabal and developing it for the four operations.\n\nDe Pasquale also adopts a vertical progression to represent numbers by powers of 40. The representation of the numbers is based on the fact that the sum of the values of the circles in each row gives as total 39, if each circle takes the value 5 in the first column, 3 in the second column, 2 in the third and 1 in the fourth one; it is thus possible to represent 39 numbers, united to neutral element ( zero or no seeds in the table); this forms the basis of 40 symbols necessary for the numbering system.\n\nOne of the possible representations of the number 13457 in the yupana by De Pasquale is shown in the following table:\n\nThe theory of De Pasquale opened, in the years after his birth, great controversy among researchers who divided mainly into two groups: one supporting the base 10 theory and another supporting the base 40 one. It should be noted in this regard that the Spanish chronicles of the time of the conquest of the Americas indicated that the Incas used a decimal system and that since 2003 the base 10 has been proposed as the basis for calculating both with the abacus and the quipu\n\nDe Pasquale has recently proposed the use of yupana as astronomical calendar running in mixed base 36/40 and provided its own interpretation of the Quechua word \"huno\", translating it as 0.1. This interpretation diverges from all the chroniclers of the Indies, starting from Domingo de Santo Tomas which in 1560 translated \"huno\" with \"chunga guaranga\" (ten thousand).\n\nIn 2008 Cinzia Florio proposes an alternative and revolutionary approach in respect to all the theories proposed so far. For the first time we deviate from the positional numbering system and we adopt the additive, or sign-value notation.\n\nRelying exclusively on the design of Poma de Ayala, the author explains the arrangement of white and black circles and interprets the use of the abacus as a board for making multiplications, in which the multiplicand is represented in the right column, the multiplier in the two central columns and the result (product) is shown in the left column. See the following table.\n\nThe theory differs from all the previous by several aspects: first, the white and black circles would not be any gaps that may be filled with a seed, but different colors of the seeds, representatives respectively tens and units (this according to the chronicler Juan de Velasco ).\n\nSecondly, the multiplicand is entered in the first column respecting the sign-value notation: so, the seeds can be entered in any order and the number is given by the sum of the values of these seeds.\n\nThe multiplier is represented as the sum of two factors, since the procedure for obtaining the product is based on the distributive property of multiplication over addition.\n\nThe table multiplier drawn by Poma de Ayala with that provision of the seeds, represent according to the author, the calculation: 32 x 5, where the multiplier 5 is decomposed into 3 + 2. The sequence of numbers 1,2,3,5 would be casual, contingent to the calculation done and not related to the Fibonacci series.\n\nKey: o = 10; • = 1; The operation represented is: 32 x 5 = 32 x (2 + 3) = (32 x 2) + (32 x 3) = 64 + 96 = 160\n\nThe numbers represented in the columns are, from left to right: 32 (the multiplicand), 64 = 32 x 2 and 32 x 3 = 96 (which together constitute the multiplicand, multiplied by the two factors in which the multiplier has been broken down) and finally 151. In this issue (error) are based all possible criticisms of this interpretation, since 151 is obviously not the sum of 96 and 64. Florio, however, notes that a mistake of Poma de Ayala, in designing a black circle instead of a white one, would have been possible. In this case, changing just a black circle with a white one in the last column, we obtain the number 160, which is exactly the product sought as the sum of the quantities present in the central columns.\n\nWith a yupana as the one designed by Poma de Ayala can not be represented every multiplicands, but it is necessary to extend the yupana vertically (adding rows) to represent numbers whose sum of digits exceeds 5. The same thing goes for the multipliers: to represent all the numbers is necessary to extend the number of columns. It should be emphasized that this interpretation, apart the supposed error calculation (or representation by the designer), is the only one that identifies in the yupana of Poma de Ayala a mathematical and consistent message (multiplication) and not a series of random numbers as in other interpretations.\n\n\n\n\n\n\n\n"}
{"id": "25046926", "url": "https://en.wikipedia.org/wiki?curid=25046926", "title": "Zermelo's theorem (game theory)", "text": "Zermelo's theorem (game theory)\n\nIn game theory, Zermelo’s theorem, named after Ernst Zermelo, says that in any finite two-person game of perfect information in which the players move alternatingly and in which chance does not affect the decision making process, if the game cannot end in a draw, then one of the two players must have a winning strategy (i.e. force a win). It can alternately be stated as saying that in such a game, either the first-player can force a win, or the second-player can force a win, or both players can force a draw.\n\nZermelo's work shows that in two-person zero-sum games with perfect information, if a player is in a winning position, then he can always force a win no matter what strategy the other player may employ. Furthermore, and as a consequence, if a player is in a winning position, it will never require more moves than there are positions in the game (with a position defined as position of pieces as well as the player next to move).\n\nZermelo's original paper describing the theorem,\n\"Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels\", was published in German in 1913. Ulrich Schwalbe and Paul Walker translated Zermelo's paper into English in 1997 and published the translation in the appendix to \"Zermelo and the Early History of Game Theory\".\n\nZermelo considers the class of two-person games without chance, where players have strictly opposing interests and where only a finite number of positions are possible. Although in the game only finitely many positions are possible, Zermelo allows infinite sequences of moves since he does not consider stopping rules. Thus, he allows for the possibility of infinite games. Then he addresses two problems:\n\n\nTo answer the first question, Zermelo states that a necessary and sufficient condition is the nonemptyness of a certain set, containing all possible sequences of moves such that a player wins independently of how the other player plays. But should this set be empty, the best a player could achieve would be a draw. So he defines another set containing all possible sequences of moves such that a player can postpone his loss for an infinite number of moves, which implies a draw. This set may also be empty, i. e., the player can avoid his loss for only finitely many moves if his opponent plays correctly. But this is equivalent to the opponent being able to force a win. This is the basis for all modern versions of Zermelo's theorem.\n\nAbout the second question, Zermelo claimed that it will never take more moves than there are positions in the game. His proof is a proof by contradiction: Assume that a player can win in a number of moves larger than the number of positions. Of course, at least one winning position must have appeared twice. So the player could have played at the first occurrence in the same way as he does at the second and thus could have won in fewer moves than there are positions.\n\nWhen applied to chess, Zermelo's Theorem states \"either White can force a win, or Black can force a win, or both sides can force at least a draw\".\n\n"}
