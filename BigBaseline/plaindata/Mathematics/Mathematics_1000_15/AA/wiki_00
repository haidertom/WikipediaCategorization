{"id": "55464594", "url": "https://en.wikipedia.org/wiki?curid=55464594", "title": "ABACABA pattern", "text": "ABACABA pattern\n\nThe ABACABA pattern is a recursive fractal pattern that shows up in many places in the real world (such as in geometry, art, music, poetry, number systems, literature and higher dimensions). Patterns often show a DABACABA type subset.\n\nIn order to generate the next sequence, first take the previous pattern, add the next letter from the alphabet, and then repeat the previous pattern. The first few steps are listed here.\n\n\n"}
{"id": "54171755", "url": "https://en.wikipedia.org/wiki?curid=54171755", "title": "Analysis of Boolean functions", "text": "Analysis of Boolean functions\n\nIn mathematics and theoretical computer science, analysis of Boolean functions is the study of real-valued functions on formula_1 or formula_2 from a spectral perspective (such functions are sometimes known as pseudo-Boolean functions). The functions studied are often, but not always, Boolean-valued, making them Boolean functions. The area has found many applications in combinatorics, social choice theory, random graphs, and theoretical computer science, especially in hardness of approximation, property testing and PAC learning.\n\nWe will mostly consider functions defined on the domain formula_2. Sometimes it is more convenient to work with the domain formula_1 instead. If formula_5 is defined on formula_2, then the corresponding function defined on formula_1 is\n\nSimilarly, for us a Boolean function is a formula_9-valued function, though often it is more convenient to consider formula_10-valued functions instead.\n\nEvery real-valued function formula_11 has a unique expansion as a multilinear polynomial:\n\nThis is the Hadamard transform of the function formula_5, which is the Fourier transform in the group formula_14. The coefficients formula_15 are known as \"Fourier coefficients\", and the entire sum is known as the \"Fourier expansion\" of formula_5. The functions formula_17 are known as \"Fourier characters\", and they form an orthonormal basis for the space of all functions over formula_2, with respect to the inner product formula_19.\n\nThe Fourier coefficients can be calculated using an inner product:\n\nIn particular, this shows that formula_21 Parseval's identity states that\n\nIf we skip formula_23, then we get the variance of formula_5:\n\nThe \"degree\" of a function formula_11 is the maximum formula_27 such that formula_28 for some set formula_29 of size formula_27. In other words, the degree of formula_5 is its degree as a multilinear polynomial.\n\nIt is convenient to decompose the Fourier expansion into \"levels\": the Fourier coefficient formula_15 is on level formula_33.\n\nThe \"degree formula_27\" part of formula_5 is\n\nUsing this we can define the noise stability and the noise sensitivity, as before.\n\nThe Russo–Margulis formula states that for monotone Boolean functions formula_37,\n\nBoth the influence and the probabilities are taken with respect to formula_39, and on the right-hand side we have the average sensitivity of formula_5. If we think of formula_5 as a property, then the formula states that as formula_42 varies, the derivative of the probability that formula_5 occurs at formula_42 equals the average sensitivity at formula_42.\n\nThe Russo–Margulis formula is key for proving sharp threshold theorems such as Friedgut's.\n\nOne of the deepest results in the area, the invariance principle, connects the distribution of functions on the Boolean cube formula_2 to their distribution on \"Gaussian space\", which is the space formula_47 endowed with the standard formula_48-dimensional Gaussian measure.\n\nMany of the basic concepts of Fourier analysis on the Boolean cube have counterparts in Gaussian space:\n\n\nGaussian space is more symmetric than the Boolean cube (for example, it is rotation invariant), and supports continuous arguments which may be harder to get through in the discrete setting of the Boolean cube. The invariance principle links the two settings, and allows deducing results on the Boolean cube from results on Gaussian space.\n\nIf formula_54 has degree at most 1, then formula_5 is either constant, equal to a coordinate, or equal to the negation of a coordinate. In particular, formula_5 is a \"dictatorship\": a function depending on at most one coordinate.\n\nThe Friedgut–Kalai–Naor theorem, also known as the \"FKN theorem\", states that if formula_5 \"almost\" has degree 1 then it is \"close\" to a dictatorship. Quantitatively, if formula_54 and formula_59, then formula_5 is formula_61-close to a dictatorship, that is, formula_62 for some Boolean dictatorship formula_63, or equivalently, formula_64 for some Boolean dictatorship formula_63.\n\nSimilarly, a Boolean function of degree at most formula_27 depends on at most formula_67 coordinates, making it a \"junta\" (a function depending on a constant number of coordinates). The Kindler–Safra theorem generalizes the Friedgut–Kalai–Naor theorem to this setting. It states that if formula_54 satisfies formula_69 then formula_5 is formula_61-close to a Boolean function of degree at most formula_27.\n\nThe Poincaré inequality for the Boolean cube (which follows from formulas appearing above) states that for a function formula_11,\n\nThis implies that formula_75.\n\nThe Kahn–Kalai–Linial theorem, also known as the \"KKL theorem\", states that if formula_5 is Boolean then formula_77.\n\nThe bound given by the Kahn–Kalai–Linial theorem is tight, and is achieved by the \"Tribes\" function of Ben-Or and Linial:\n\nThe Kahn–Kalai–Linial theorem was one of the first results in the area, and was the one introducing hypercontractivity into the context of Boolean functions.\n\nIf formula_54 is an formula_80-junta (a function depending on at most formula_80 coordinates) then formula_82 according to the Poincaré inequality.\n\nFriedgut's theorem is a converse to this result. It states that for any formula_83, the function formula_5 is formula_85-close to a Boolean junta depending on formula_86 coordinates.\n\nCombined with the Russo–Margulis lemma, Friedgut's junta theorem implies that for every formula_42, every monotone function is close to a junta with respect to formula_88 for some formula_89.\n\nThe invariance principle generalizes the Berry–Esseen theorem to non-linear functions.\n\nThe Berry–Esseen theorem states (among else) that if formula_90 and no formula_91 is too large compared to the rest, then the distribution of formula_5 over formula_2 is close to a normal distribution with the same mean and variance.\n\nThe invariance principle (in a special case) informally states that if formula_5 is a multilinear polynomial of bounded degree over formula_95 and all influences of formula_5 are small, then the distribution of formula_5 under the uniform measure over formula_2 is close to its distribution in Gaussian space.\n\nMore formally, let formula_99 be a univariate Lipschitz function, let formula_100, let formula_101, and let\nformula_102. Suppose that formula_103. Then\n\nBy choosing appropriate formula_99, this implies that the distributions of formula_5 under both measures are close in CDF distance, which is given by formula_107.\n\nThe invariance principle was the key ingredient in the original proof of the \"Majority is Stablest\" theorem.\n\nA Boolean function formula_54 is \"linear\" if it satisfies formula_109, where formula_110. It is not hard to show that the Boolean linear functions are exactly the characters formula_17.\n\nIn property testing we want to test whether a given function is linear. It is natural to try the following test: choose formula_112 uniformly at random, and check that formula_109. If formula_5 is linear then it always passes the test. Blum, Luby and Rubinfeld showed that if the test passes with probability formula_115 then formula_5 is formula_61-close to a Fourier character. Their proof was combinatorial.\n\nBellare et al. gave an extremely simple Fourier-analytic proof, that also shows that if the test succeeds with probability formula_118, then formula_5 is correlated with a Fourier character. Their proof relies on the following formula for the success probability of the test:\n\nArrow's impossibility theorem states that for three and more candidates, the only unanimous voting rule for which there is always a Condorcet winner is a dictatorship.\n\nThe usual proof of Arrow's theorem is combinatorial. Kalai gave an alternative proof of this result in the case of three candidates using Fourier analysis. If formula_54 is the rule that assigns a winner among two candidates given their relative orders in the votes, then the probability that there is a Condorcet winner given a uniformly random vote is formula_122, from which the theorem easily follows.\n\nThe FKN theorem implies that if formula_5 is a rule for which there is almost always a Condorcet winner, then formula_5 is close to a dictatorship.\n\nA classical result in the theory of random graphs states that the probability that a formula_125 random graph is connected tends to formula_126 if formula_127. This is an example of a \"sharp threshold\": the width of the \"threshold window\", which is formula_128, is asymptotically smaller than the threshold itself, which is roughly formula_129. In contrast, the probability that a formula_125 graph contains a triangle tends to formula_131 when formula_132. Here both the threshold window and the threshold itself are formula_133, and so this is a \"coarse threshold\".\n\nFriedgut's sharp threshold theorem states, roughly speaking, that a monotone graph property (a graph property is a property which doesn't depend on the names of the vertices) has a sharp threshold unless it is correlated with the appearance of small subgraphs. This theorem has been widely applied to analyze random graphs and percolation.\n\nOn a related note, the KKL theorem implies that the width of threshold window is always at most formula_134.\n\nLet formula_135 denote the majority function on formula_48 coordinates. Sheppard's formula gives the asymptotic noise stability of majority:\n\nThis is related to the probability that if we choose formula_138 uniformly at random and form formula_139 by flipping each bit of formula_140 with probability formula_141, then the majority stays the same:\n\nThere are Boolean functions with larger noise stability. For example, a dictatorship formula_143 has noise stability formula_53.\n\nThe Majority is Stablest theorem states, informally, then the only functions having noise stability larger than majority have influential coordinates. Formally, for every formula_83 there exists formula_146 such that if formula_54 has expectation zero and formula_148, then formula_149.\n\nThe first proof of this theorem used the invariance principle in conjunction with an isoperimetric theorem of Borell in Gaussian space; since then more direct proofs were devised.\n\nMajority is Stablest implies that the Goemans–Williamson approximation algorithm for MAX-CUT is optimal, assuming the unique games conjecture. This implication, due to Khot et al., was the impetus behind proving the theorem.\n"}
{"id": "2033586", "url": "https://en.wikipedia.org/wiki?curid=2033586", "title": "Back-and-forth method", "text": "Back-and-forth method\n\nIn mathematical logic, especially set theory and model theory, the back-and-forth method is a method for showing isomorphism between countably infinite structures satisfying specified conditions. In particular:\n\n\nSuppose that\n\n\nFix enumerations (without repetition) of the underlying sets:\n\nNow we construct a one-to-one correspondence between \"A\" and \"B\" that is strictly increasing. Initially no member of \"A\" is paired with any member of \"B\".\n\nIt still has to be checked that the choice required in step (1) and (2) can actually be made in accordance to the requirements. Using step (1) as an example:\n\nIf there are already \"a\" and \"a\" in \"A\" corresponding to \"b\" and \"b\" in \"B\" respectively such that \"a\" < \"a\" < \"a\" and \"b\" < \"b\", we choose \"b\" in between \"b\" and \"b\" using density. Otherwise, we choose a suitable large or small element of \"B\" using the fact that \"B\" has neither a maximum nor a minimum. Choices made in step (2) are dually possible. Finally, the construction ends after countably many steps because \"A\" and \"B\" are countably infinite. Note that we had to use all the prerequisites.\n\nAccording to Hodges (1993):\nWhile the theorem on countable densely ordered sets is due to Cantor (1895), the back-and-forth method with which it is now proved was developed by Huntington (1904) and Hausdorff (1914). Later it was applied in other situations, most notably by Roland Fraïssé in model theory.\n\n\n"}
{"id": "4472133", "url": "https://en.wikipedia.org/wiki?curid=4472133", "title": "Chamfered dodecahedron", "text": "Chamfered dodecahedron\n\nThe chamfered dodecahedron is a convex polyhedron with 80 vertices, 120 edges, and 42 faces: 30 hexagons and 12 pentagons. It is constructed as a chamfer (geometry) (edge-truncation) of a regular dodecahedron. The pentagons are reduced in size and new hexagonal faces are added in place of all the original edges. Its dual is the pentakis icosidodecahedron.\n\nIt is also called a truncated rhombic triacontahedron, constructed as a truncation of the rhombic triacontahedron. It can more accurately be called an order-5 truncated rhombic triacontahedron because only the order-5 vertices are truncated.\n\nThese 12 order-5 vertices can be truncated such that all edges are equal length. The original 30 rhombic faces become non-regular hexagons, and the truncated vertices become regular pentagons.\n\nThe hexagon faces can be equilateral but not regular with D symmetry. The angles at the two vertices with vertex configuration \"6.6.6\" are arccos(-1/sqrt(5)) = 116.565 degrees, and at the remaining four vertices with \"5.6.6\", they are 121.717 degrees each.\n\nIt is the Goldberg polyhedron G(2,0), containing pentagonal and hexagonal faces.\n\nIt also represents the exterior envelope of a cell-centered orthogonal projection of the 120-cell, one of six (convex regular 4-polytopes).\n\nThis is the shape of the fullerene C; sometimes this shape is denoted C(I) to describe its icosahedral symmetry and distinguish it from other less-symmetric 80-vertex fullerenes. It is one of only four fullerenes found by to have a skeleton that can be isometrically embeddable into an L space.\n\nThis polyhedron looks very similar to the uniform truncated icosahedron which has 12 pentagons, but only 20 hexagons.\n\nThe chamfered dodecahedron creates more polyhedra by basic Conway polyhedron notation. The zip chamfered dodecahedron makes a chamfered truncated icosahedron, and Goldberg (2,2).\n\nIn geometry, the chamfered truncated icosahedron is a convex polyhedron with 240 vertices, 360 edges, and 122 faces, 110 hexagons and 12 pentagons.\n\nIt is constructed by a chamfer operation to the truncated icosahedron, adding new hexagons in place of original edges. It can also be constructed as a zip (= dk = dual of kis of) operation from the \"chamfered dodecahedron\". In other words, raising pentagonal and hexagonal pyramids on a chamfered dodecahedron (kis operation) will yield the (2,2) geodesic polyhedron. Taking the dual of that yields the (2,2) Goldberg polyhedron, which is the chamfered truncated icosahedron, and is also Fullerene C.\n\n\n"}
{"id": "7203729", "url": "https://en.wikipedia.org/wiki?curid=7203729", "title": "D'Alembert's equation", "text": "D'Alembert's equation\n\nIn mathematics, d'Alembert's equation is a first order nonlinear ordinary differential equation, named after the French mathematician Jean le Rond d'Alembert. The equation reads as\n\nwhere formula_2. After differentiating once, and rearranging we have\n\nThe above equation is linear.\n"}
{"id": "1300778", "url": "https://en.wikipedia.org/wiki?curid=1300778", "title": "DeWitt notation", "text": "DeWitt notation\n\nPhysics often deals with classical models where the dynamical variables are a collection of functions \n\nIn the DeWitt notation (named after theoretical physicist Bryce DeWitt), φ(\"x\") is written as φ where \"i\" is now understood as an index covering both \"α\" and \"x\".\n\nSo, given a smooth functional \"A\", \"A\" stands for the functional derivative\n\nas a functional of \"φ\". In other words, a \"1-form\" field over the infinite dimensional \"functional manifold\".\n\nIn integrals, the Einstein summation convention is used. Alternatively,\n"}
{"id": "47277561", "url": "https://en.wikipedia.org/wiki?curid=47277561", "title": "Diameter (group theory)", "text": "Diameter (group theory)\n\nIn the area of abstract algebra known as group theory, the diameter of a finite group is a measure of its complexity.\n\nConsider a finite group formula_1, and any set of generators . Define formula_2 to be the graph diameter of the Cayley graph formula_3. Then the diameter of formula_1 is the largest value of formula_2 taken over all generating sets .\n\nFor instance, every finite cyclic group of order , the Cayley graph for a generating set with one generator is an -vertex cycle graph. The diameter of this graph, and of the group, is formula_6.\n\nIt is conjectured, for all non-abelian finite simple groups , that\n\nMany partial results are known but the full conjecture remains open.\n"}
{"id": "59112216", "url": "https://en.wikipedia.org/wiki?curid=59112216", "title": "ENO methods", "text": "ENO methods\n\nIn numerical solution of differential equations, ENO (essentially non-oscillatory) methods are classes of high-resolution schemes. The first ENO scheme is developed by Harten, Engquist, Osher and Chakravarthy in 1987. In 1994, the first weighted version of ENO is developed.\n\n"}
{"id": "36727478", "url": "https://en.wikipedia.org/wiki?curid=36727478", "title": "Foundations of Differential Geometry", "text": "Foundations of Differential Geometry\n\nFoundations of Differential Geometry is an influential 2-volume mathematics book on differential geometry written by Shoshichi Kobayashi and Katsumi Nomizu. The first volume was published in 1963 and the second in 1969, by Interscience Publishers. Both were published again in 1996 as Wiley Classics Library.\n\nThe first volume considers manifolds, fiber bundles, tensor analysis, connections in bundles, and the role of Lie groups. It also covers holonomy, the de Rham decomposition theorem and the Hopf–Rinow theorem. According to the review of James Eells, it has a \"fine expositional style\" and consists of a \"special blend of algebraic, analytic, and geometric concepts\". Eells says it is \"essentially a textbook (even though there are no exercises)\". An advanced text, it has a \"pace geared to a [one] term graduate course\".\n\nThe second volume considers submanifolds of Riemannian manifolds, the Gauss map, and the second fundamental form. It continues with geodesics on Riemannian manifolds, Jacobi fields, the Morse index, the Rauch comparison theorems, and the Cartan–Hadamard theorem. Then it ascends to complex manifolds, Kähler manifolds, homogeneous spaces, and symmetric spaces. In a discussion of curvature representation of characteristic classes of principal bundles (Chern–Weil theory), it covers Euler classes, Chern classes, and Pontryagin classes. The second volume also received a favorable review by J. Eells in \"Mathematical Reviews\".\n\n"}
{"id": "169358", "url": "https://en.wikipedia.org/wiki?curid=169358", "title": "Foundations of mathematics", "text": "Foundations of mathematics\n\nFoundations of mathematics is the study of the philosophical and logical and/or algorithmic basis of mathematics, or, in a broader sense, the mathematical investigation of what underlies the philosophical theories concerning the nature of mathematics. In this latter sense, the distinction between foundations of mathematics and philosophy of mathematics turns out to be quite vague.\nFoundations of mathematics can be conceived as the study of the basic mathematical concepts (set, function, geometrical figure, number, etc.) and how they form hierarchies of more complex structures and concepts, especially the fundamentally important structures that form the language of mathematics (formulas, theories and their models giving a meaning to formulas, definitions, proofs, algorithms, etc.) also called metamathematical concepts, with an eye to the philosophical aspects and the unity of mathematics. The search for foundations of mathematics is a central question of the philosophy of mathematics; the abstract nature of mathematical objects presents special philosophical challenges.\n\nThe foundations of mathematics as a whole does not aim to contain the foundations of every mathematical topic.\nGenerally, the \"foundations\" of a field of study refers to a more-or-less systematic analysis of its most basic or fundamental concepts, its conceptual unity and its natural ordering or hierarchy of concepts, which may help to connect it with the rest of human knowledge. The development, emergence and clarification of the foundations can come late in the history of a field, and may not be viewed by everyone as its most interesting part.\n\nMathematics always played a special role in scientific thought, serving since ancient times as a model of truth and rigor for rational inquiry, and giving tools or even a foundation for other sciences (especially physics). Mathematics' many developments towards higher abstractions in the 19th century brought new challenges and paradoxes, urging for a deeper and more systematic examination of the nature and criteria of mathematical truth, as well as a unification of the diverse branches of mathematics into a coherent whole.\n\nThe systematic search for the foundations of mathematics started at the end of the 19th century and formed a new mathematical discipline called mathematical logic, with strong links to theoretical computer science.\nIt went through a series of crises with paradoxical results, until the discoveries stabilized during the 20th century as a large and coherent body of mathematical knowledge with several aspects or components (set theory, model theory, proof theory, etc.), whose detailed properties and possible variants are still an active research field.\nIts high level of technical sophistication inspired many philosophers to conjecture that it can serve as a model or pattern for the foundations of other sciences.\n\nWhile the practice of mathematics had previously developed in other civilizations, special interest in its theoretical and foundational aspects was clearly evident in the work of the Ancient Greeks.\n\nEarly Greek philosophers disputed as to which is more basic, arithmetic or geometry.\nZeno of Elea (490 c. 430 BC) produced four paradoxes that seem to show the impossibility of change. The Pythagorean school of mathematics originally insisted that only natural and rational numbers exist. The discovery of the irrationality of , the ratio of the diagonal of a square to its side (around 5th century BC), was a shock to them which they only reluctantly accepted. The discrepancy between rationals and reals was finally resolved by Eudoxus of Cnidus (408–355 BC), a student of Plato, who reduced the comparison of irrational ratios to comparisons of multiples (rational ratios), thus anticipating the definition of real numbers by Richard Dedekind (1831–1916).\n\nIn the \"Posterior Analytics\", Aristotle (384–322 BC) laid down the axiomatic method for organizing a field of knowledge logically by means of primitive concepts, axioms, postulates, definitions, and theorems. Aristotle took a majority of his examples for this from arithmetic and from geometry.\nThis method reached its high point with Euclid's \"Elements\" (300 BC), a treatise on mathematics structured with very high standards of rigor: Euclid justifies each proposition by a demonstration in the form of chains of syllogisms (though they do not always conform strictly to Aristotelian templates).\nAristotle's syllogistic logic, together with the axiomatic method exemplified by Euclid's \"Elements\", are recognized as scientific achievements of ancient Greece.\n\nStarting from the end of the 19th century, a Platonist view of mathematics became common among practicing mathematicians.\n\nThe \"concepts\" or, as Platonists would have it, the \"objects\" of mathematics are abstract and remote from everyday perceptual experience: geometrical figures are conceived as idealities to be distinguished from effective drawings and shapes of objects, and numbers are not confused with the counting of concrete objects. Their existence and nature present special philosophical challenges: How do mathematical objects differ from their concrete representation? Are they located in their representation, or in our minds, or somewhere else? How can we know them?\n\nThe ancient Greek philosophers took such questions very seriously. Indeed, many of their general philosophical discussions were carried on with extensive reference to geometry and arithmetic. Plato (424/423 BC 348/347 BC) insisted that mathematical objects, like other platonic \"Ideas\" (forms or essences), must be perfectly abstract and have a separate, non-material kind of existence, in a world of mathematical objects independent of humans. He believed that the truths about these objects also exist independently of the human mind, but is \"discovered\" by humans. In the \"Meno\" Plato's teacher Socrates asserts that it is possible to come to know this truth by a process akin to memory retrieval.\n\nAbove the gateway to Plato's academy appeared a famous inscription: \"Let no one who is ignorant of geometry enter here\". In this way Plato indicated his high opinion of geometry. He regarded geometry as \"the first essential in the training of philosophers\", because of its abstract character.\n\nThis philosophy of \"Platonist mathematical realism\" is shared by many mathematicians. It can be argued that Platonism somehow comes as a necessary assumption underlying any mathematical work.\n\nIn this view, the laws of nature and the laws of mathematics have a similar status, and the effectiveness ceases to be unreasonable. Not our axioms, but the very real world of mathematical objects forms the foundation.\n\nAristotle dissected and rejected this view in his Metaphysics. These questions provide much fuel for philosophical analysis and debate.\n\nFor over 2,000 years, Euclid's Elements stood as a perfectly solid foundation for mathematics, as its methodology of rational exploration guided mathematicians, philosophers, and scientists well into the 19th century.\n\nThe Middle Ages saw a dispute over the ontological status of the universals (platonic Ideas): Realism asserted their existence independently of perception; conceptualism asserted their existence within the mind only; nominalism denied either, only seeing universals as names of collections of individual objects (following older speculations that they are words, \"logoi\").\n\nRené Descartes published \"La Géométrie\" (1637), aimed at reducing geometry to algebra by means of coordinate systems, giving algebra a more foundational role (while the Greeks embedded arithmetic into geometry by identifying whole numbers with evenly spaced points on a line). Descartes' book became famous after 1649 and paved the way to infinitesimal calculus.\n\nIsaac Newton (1642–1727) in England and Leibniz (1646–1716) in Germany independently developed the infinitesimal calculus based on heuristic methods greatly efficient, but direly lacking rigorous justifications. Leibniz even went on to explicitly describe infinitesimals as actual infinitely small numbers (close to zero). Leibniz also worked on formal logic but most of his writings on it remained unpublished until 1903.\n\nThe Protestant philosopher George Berkeley (1685–1753), in his campaign against the religious implications of Newtonian mechanics, wrote a pamphlet on the lack of rational justifications of infinitesimal calculus: \"They are neither finite quantities, nor quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?\"\n\nThen mathematics developed very rapidly and successfully in physical applications, but with little attention to logical foundations.\n\nIn the 19th century, mathematics became increasingly abstract. Concerns about logical gaps and inconsistencies in different fields led to the development of axiomatic systems.\n\nCauchy (1789–1857) started the project of formulating and proving the theorems of infinitesimal calculus in a rigorous manner, rejecting the heuristic principle of the generality of algebra exploited by earlier authors. In his 1821 work \"Cours d'Analyse\" he defines infinitely small quantities in terms of decreasing sequences that converge to 0, which he then used to define continuity. But he did not formalize his notion of convergence.\n\nThe modern (ε, δ)-definition of limit and continuous functions was first developed by Bolzano in 1817, but remained relatively unknown. It gives a rigorous foundation of infinitesimal calculus based on the set of real numbers, arguably resolving the Zeno paradoxes and Berkeley's arguments.\n\nMathematicians such as Karl Weierstrass (1815–1897) discovered pathological functions such as continuous, nowhere-differentiable functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, to axiomatize analysis using properties of the natural numbers.\n\nIn 1858, Dedekind proposed a definition of the real numbers as cuts of rational numbers. This reduction of real numbers and continuous functions in terms of rational numbers, and thus of natural numbers, was later integrated by Cantor in his set theory, and axiomatized in terms of second order arithmetic by Hilbert and Bernays.\n\nFor the first time, the limits of mathematics were explored. Niels Henrik Abel (1802–1829), a Norwegian, and Évariste Galois, (1811–1832) a Frenchman, investigated the solutions of various polynomial equations, and proved that there is no general algebraic solution to equations of degree greater than four (Abel–Ruffini theorem). With these concepts, Pierre Wantzel (1837) proved that straightedge and compass alone cannot trisect an arbitrary angle nor double a cube. In 1882, Lindemann building on the work of Hermite showed that a straightedge and compass quadrature of the circle (construction of a square equal in area to a given circle) was also impossible by proving that is a transcendental number. Mathematicians had attempted to solve all of these problems in vain since the time of the ancient Greeks.\n\nAbel and Galois's works opened the way for the developments of group theory (which would later be used to study symmetry in physics and other fields), and abstract algebra. Concepts of vector spaces emerged from the conception of barycentric coordinates by Möbius in 1827, to the modern definition of vector spaces and linear maps by Peano in 1888. Geometry was no more limited to three dimensions.\nThese concepts did not generalize numbers but combined notions of functions and sets which were not yet formalized, breaking away from familiar mathematical objects.\n\nAfter many failed attempts to derive the parallel postulate from other axioms, the study of the still hypothetical hyperbolic geometry by Johann Heinrich Lambert (1728–1777) led him to introduce the hyperbolic functions and compute the area of a hyperbolic triangle (where the sum of angles is less than 180°). Then the Russian mathematician Nikolai Lobachevsky (1792–1856) established in 1826 (and published in 1829) the coherence of this geometry (thus the independence of the parallel postulate), in parallel with the Hungarian mathematician János Bolyai (1802–1860) in 1832, and with Gauss.\nLater in the 19th century, the German mathematician Bernhard Riemann developed Elliptic geometry, another non-Euclidean geometry where no parallel can be found and the sum of angles in a triangle is more than 180°. It was proved consistent by defining point to mean a pair of antipodal points on a fixed sphere and line to mean a great circle on the sphere. At that time, the main method for proving the consistency of a set of axioms was to provide a model for it.\n\nOne of the traps in a deductive system is circular reasoning, a problem that seemed to befall projective geometry until it was resolved by Karl von Staudt. As explained by Russian historians:\n\nThe purely geometric approach of von Staudt was based on the complete quadrilateral to express the relation of projective harmonic conjugates. Then he created a means of expressing the familiar numeric properties with his Algebra of Throws. English language versions of this process of deducing the properties of a field can be found in either the book by Oswald Veblen and John Young, \"Projective Geometry\" (1938), or more recently in John Stillwell's \"Four Pillars of Geometry\" (2005). Stillwell writes on page 120\n\nThe algebra of throws is commonly seen as a feature of cross-ratios since students ordinarily rely upon numbers without worry about their basis. However, cross-ratio calculations use metric features of geometry, features not admitted by purists. For instance, in 1961 Coxeter wrote \"Introduction to Geometry\" without mention of cross-ratio.\n\nAttempts of formal treatment of mathematics had started with Leibniz and Lambert (1728–1777), and continued with works by algebraists such as George Peacock (1791–1858).\nSystematic mathematical treatments of logic came with the British mathematician George Boole (1847) who devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1 and logical combinations (conjunction, disjunction, implication and negation) are operations similar to the addition and multiplication of integers. Additionally, De Morgan published his laws in 1847. Logic thus became a branch of mathematics. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.\n\nCharles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\n\nThe German mathematician Gottlob Frege (1848–1925) presented an independent development of logic with quantifiers in his Begriffsschrift (formula language) published in 1879, a work generally considered as marking a turning point in the history of logic. He exposed deficiencies in Aristotle's \"Logic\", and pointed out the three expected properties of a mathematical theory:\n\n\nHe then showed in \"Grundgesetze der Arithmetik (Basic Laws of Arithmetic)\" how arithmetic could be formalised in his new logic.\n\nFrege's work was popularized by Bertrand Russell near the turn of the century. But Frege's two-dimensional notation had no success. Popular notations were (x) for universal and (∃x) for existential quantifiers, coming from Giuseppe Peano and William Ernest Johnson until the ∀ symbol was introduced by Gerhard Gentzen in 1935 and became canonical in the 1960s.\n\nFrom 1890 to 1905, Ernst Schröder published \"Vorlesungen über die Algebra der Logik\" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\nThe formalization of arithmetic (the theory of natural numbers) as an axiomatic theory started with Peirce in 1881 and continued with Richard Dedekind and Giuseppe Peano in 1888. This was still a second-order axiomatization (expressing induction in terms of arbitrary subsets, thus with an implicit use of set theory) as concerns for expressing theories in first-order logic were not yet understood. In Dedekind's work, this approach appears as completely characterizing natural numbers and providing recursive definitions of addition and multiplication from the successor function and mathematical induction.\n\nThe foundational crisis of mathematics (in German \"Grundlagenkrise der Mathematik\") was the early 20th century's term for the search for proper foundations of mathematics.\n\nSeveral schools of the philosophy of mathematics ran into difficulties one after the other in the 20th century, as the assumption that mathematics had any foundation that could be consistently stated within mathematics itself was heavily challenged by the discovery of various paradoxes (such as Russell's paradox).\n\nThe name \"paradox\" should not be confused with \"contradiction\". A contradiction in a formal theory is a formal proof of an absurdity inside the theory (such as ), showing that this theory is inconsistent and must be rejected. But a paradox may be either a surprising but true result in a given formal theory, or an informal argument leading to a contradiction, so that a candidate theory, if it is to be formalized, must disallow at least one of its steps; in this case the problem is to find a satisfying theory without contradiction. Both meanings may apply if the formalized version of the argument forms the proof of a surprising truth. For instance, Russell's paradox may be expressed as \"there is no set of all sets\" (except in some marginal axiomatic set theories).\n\nVarious schools of thought opposed each other. The leading school was that of the formalist approach, of which David Hilbert was the foremost proponent, culminating in what is known as Hilbert's program, which thought to ground mathematics on a small basis of a logical system proved sound by metamathematical finitistic means. The main opponent was the intuitionist school, led by L. E. J. Brouwer, which resolutely discarded formalism as a meaningless game with symbols (van Dalen, 2008). The fight was acrimonious. In 1920 Hilbert succeeded in having Brouwer, whom he considered a threat to mathematics, removed from the editorial board of \"Mathematische Annalen\", the leading mathematical journal of the time.\n\nAt the beginning of the 20th century, three schools of philosophy of mathematics opposed each other: Formalism, Intuitionism and Logicism.\n\nIt has been claimed that formalists, such as David Hilbert (1862–1943), hold that mathematics is only a language and a series of games. Indeed, he used the words \"formula game\" in his 1927 response to L. E. J. Brouwer's criticisms:\n\nThus Hilbert is insisting that mathematics is not an \"arbitrary\" game with \"arbitrary\" rules; rather it must agree with how our thinking, and then our speaking and writing, proceeds.\n\nThe foundational philosophy of formalism, as exemplified by David Hilbert, is a response to the paradoxes of set theory, and is based on formal logic. Virtually all mathematical theorems today can be formulated as theorems of set theory. The truth of a mathematical statement, in this view, is represented by the fact that the statement can be derived from the axioms of set theory using the rules of formal logic.\n\nMerely the use of formalism alone does not explain several issues: why we should use the axioms we do and not some others, why we should employ the logical rules we do and not some others, why do \"true\" mathematical statements (e.g., the laws of arithmetic) appear to be true, and so on. Hermann Weyl would ask these very questions of Hilbert:\n\nIn some cases these questions may be sufficiently answered through the study of formal theories, in disciplines such as reverse mathematics and computational complexity theory. As noted by Weyl, formal logical systems also run the risk of inconsistency; in Peano arithmetic, this arguably has already been settled with several proofs of consistency, but there is debate over whether or not they are sufficiently finitary to be meaningful. Gödel's second incompleteness theorem establishes that logical systems of arithmetic can never contain a valid proof of their own consistency. What Hilbert wanted to do was prove a logical system \"S\" was consistent, based on principles \"P\" that only made up a small part of \"S\". But Gödel proved that the principles \"P\" could not even prove \"P\" to be consistent, let alone \"S\".\n\nIntuitionists, such as L. E. J. Brouwer (1882–1966), hold that mathematics is a creation of the human mind. Numbers, like fairy tale characters, are merely mental entities, which would not exist if there were never any human minds to think about them.\n\nThe foundational philosophy of \"intuitionism\" or \"constructivism\", as exemplified in the extreme by Brouwer and Stephen Kleene, requires proofs to be \"constructive\" in nature the existence of an object must be demonstrated rather than inferred from a demonstration of the impossibility of its non-existence. For example, as a consequence of this the form of proof known as reductio ad absurdum is suspect.\n\nSome modern theories in the philosophy of mathematics deny the existence of foundations in the original sense. Some theories tend to focus on mathematical practice, and aim to describe and analyze the actual working of mathematicians as a social group. Others try to create a cognitive science of mathematics, focusing on human cognition as the origin of the reliability of mathematics when applied to the real world. These theories would propose to find foundations only in human thought, not in any objective outside construct. The matter remains controversial.\n\nLogicism is a school of thought, and research programme, in the philosophy of mathematics, based on the thesis that mathematics is an extension of a logic or that some or all mathematics may be derived in a suitable formal system whose axioms and rules of inference are 'logical' in nature . Bertrand Russell and Alfred North Whitehead championed this theory initiated by Gottlob Frege and influenced by Richard Dedekind\n\nMany researchers in axiomatic set theory have subscribed to what is known as set-theoretic Platonism, exemplified by Kurt Gödel.\n\nSeveral set theorists followed this approach and actively searched for axioms that may be considered as true for heuristic reasons and that would decide the continuum hypothesis. Many large cardinal axioms were studied, but the hypothesis always remained independent from them and it is now considered unlikely that CH can be resolved by a new large cardinal axiom. Other types of axioms were considered, but none of them has reached consensus on the continuum hypothesis yet. Recent work by Hamkins proposes a more flexible alternative: a set-theoretic multiverse allowing free passage between set-theoretic universes that satisfy the continuum hypothesis and other universes that do not.\n\nThis argument by Willard Quine and Hilary Putnam says (in Putnam's shorter words),\n\nHowever Putnam was not a Platonist.\n\nFew mathematicians are typically concerned on a daily, working basis over logicism, formalism or any other philosophical position. Instead, their primary concern is that the mathematical enterprise as a whole always remains productive. Typically, they see this as insured by remaining open-minded, practical and busy; as potentially threatened by becoming overly-ideological, fanatically reductionistic or lazy. \n\nSuch a view was has also been expressed by some well-known physicists.\n\nFor example, the Physics Nobel Prize laureate Richard Feynman said\n\nAnd Steven Weinberg:\n\nWeinberg believed that any undecidability in mathematics, such as the continuum hypothesis, could be potentially resolved despite the incompleteness theorem, by finding suitable further axioms to add to set theory.\n\nGödel's completeness theorem establishes an equivalence in first-order logic between the formal provability of a formula and its truth in all possible models. Precisely, for any consistent first-order theory it gives an \"explicit construction\" of a model described by the theory; this model will be countable if the language of the theory is countable. However this \"explicit construction\" is not algorithmic. It is based on an iterative process of completion of the theory, where each step of the iteration consists in adding a formula to the axioms if it keeps the theory consistent; but this consistency question is only semi-decidable (an algorithm is available to find any contradiction but if there is none this consistency fact can remain unprovable).\n\nThis can be seen as a giving a sort of justification to the Platonist view that the objects of our mathematical theories are real. More precisely, it shows that the mere assumption of the existence of the set of natural numbers as a totality (an actual infinity) suffices to imply the existence of a model (a world of objects) of any consistent theory. However several difficulties remain:\n\n\nAnother consequence of the completeness theorem is that it justifies the conception of infinitesimals as actual infinitely small nonzero quantities, based on the existence of non-standard models as equally legitimate to standard ones. This idea was formalized by Abraham Robinson into the theory of nonstandard analysis.\n\n\nStarting in 1935, the Bourbaki group of French mathematicians started publishing a series of books to formalize many areas of mathematics on the new foundation of set theory.\n\nThe intuitionistic school did not attract many adherents, and it was not until Bishop's work in 1967 that constructive mathematics was placed on a sounder footing.\n\nOne may consider that Hilbert's program has been partially completed, so that the crisis is essentially resolved, satisfying ourselves with lower requirements than Hilbert's original ambitions. His ambitions were expressed in a time when nothing was clear: it was not clear whether mathematics could have a rigorous foundation at all.\n\nThere are many possible variants of set theory, which differ in consistency strength, where stronger versions (postulating higher types of infinities) contain formal proofs of the consistency of weaker versions, but none contains a formal proof of its own consistency. Thus the only thing we don't have is a formal proof of consistency of whatever version of set theory we may prefer, such as ZF.\n\nIn practice, most mathematicians either do not work from axiomatic systems, or if they do, do not doubt the consistency of ZFC, generally their preferred axiomatic system. In most of mathematics as it is practiced, the incompleteness and paradoxes of the underlying formal theories never played a role anyway, and in those branches in which they do or whose formalization attempts would run the risk of forming inconsistent theories (such as logic and category theory), they may be treated carefully.\n\nThe development of category theory in the middle of the 20th century showed the usefulness of set theories guaranteeing the existence of larger classes than does ZFC, such as Von Neumann–Bernays–Gödel set theory or Tarski–Grothendieck set theory, albeit that in very many cases the use of large cardinal axioms or Grothendieck Universes is formally eliminable.\n\nOne goal of the Reverse Mathematics program is to identify whether there are areas of 'core mathematics' in which foundational issues may again provoke a crisis.\n\n\n\n"}
{"id": "16515493", "url": "https://en.wikipedia.org/wiki?curid=16515493", "title": "History of group theory", "text": "History of group theory\n\nThe history of group theory, a mathematical domain studying groups in their various forms, has evolved in various parallel threads. There are three historical roots of group theory: the theory of algebraic equations, number theory and geometry. Joseph Louis Lagrange, Niels Henrik Abel and Évariste Galois were early researchers in the field of group theory.\nThe earliest study of groups as such probably goes back to the work of Lagrange in the late 18th century. However, this work was somewhat isolated, and 1846 publications of Augustin Louis Cauchy and Galois are more commonly referred to as the beginning of group theory. The theory did not develop in a vacuum, and so three important threads in its pre-history are developed here.\n\nOne foundational root of group theory was the quest of solutions of polynomial equations of degree higher than 4. \n\nAn early source occurs in the problem of forming an equation of degree \"m\" having as its roots \"m\" of the roots of a given equation of degree formula_1. For simple cases the problem goes back to Johann van Waveren Hudde (1659). Nicholas Saunderson (1740) noted that the determination of the quadratic factors of a biquadratic expression necessarily leads to a sextic equation, and Le Sœur (1748) and Edward Waring (1762 to 1782) still further elaborated the idea.\n\nA common foundation for the theory of equations on the basis of the group of permutations was found by Lagrange (1770, 1771), and on this was built the theory of substitutions. He discovered that the roots of all resolvents (\"résolvantes, réduites\") which he examined are rational functions of the roots of the respective equations. To study the properties of these functions he invented a \"Calcul des Combinaisons\". The contemporary work of Alexandre-Théophile Vandermonde (1770) also foreshadowed the coming theory.\n\nPaolo Ruffini (1799) attempted a proof of the impossibility of solving the quintic and higher equations. Ruffini distinguished what are now called intransitive and transitive, and imprimitive and primitive groups, and (1801) uses the group of an equation under the name \"l'assieme delle permutazioni\". He also published a letter from Pietro Abbati to himself, in which the group idea is prominent.\nGalois found that if formula_2 are the \"n\" roots of an equation, there is always a group of permutations of the \"r\"'s such that \nIn modern terms, the solvability of the Galois group attached to the equation determines the solvability of the equation with radicals.\n\nGalois is the first to use the words \"group\" (\"groupe\" in French) and \"primitive\" in their modern meanings. He did not use \"primitive group\" but called \"equation primitive\" an equation whose Galois group is primitive. He discovered the notion of normal subgroups and found that a solvable primitive group may be identified to a subgroup of the affine group of an affine space over a finite field of prime order.\n\nGalois also contributed to the theory of modular equations and to that of elliptic functions. His first publication on group theory was made at the age of eighteen (1829), but his contributions attracted little attention until the publication of his collected papers in 1846 (Liouville, Vol. XI). Galois is honored as the first mathematician linking group theory and field theory, with the theory that is now called Galois theory.\n\nGroups similar to Galois groups are (today) called permutation groups, a concept investigated in particular by Cauchy. A number of important theorems in early group theory are due to Cauchy. Arthur Cayley's \"On the theory of groups, as depending on the symbolic equation formula_3\" (1854) gives the first abstract definition of finite groups.\n\nSecondly, the systematic use of groups in geometry, mainly in the guise of symmetry groups, was initiated by Felix Klein's 1872 Erlangen program. The study of what are now called Lie groups started systematically in 1884 with Sophus Lie, followed by work of Wilhelm Killing, Eduard Study, Issai Schur, Ludwig Maurer, and Élie Cartan. The discontinuous (discrete group) theory was built up by Klein, Lie, Henri Poincaré, and Charles Émile Picard, in connection in particular with modular forms and monodromy.\n\nThe third root of group theory was number theory. Certain abelian group structures had been implicitly used in number-theoretical work by Carl Friedrich Gauss, and more explicitly by Leopold Kronecker. Early attempts to prove Fermat's last theorem were led to a climax by Ernst Kummer by introducing groups describing factorization into prime numbers.\n\nGroup theory as an increasingly independent subject was popularized by Serret, who devoted section IV of his algebra to the theory; by Camille Jordan, whose \"Traité des substitutions et des équations algébriques\" (1870) is a classic; and to Eugen Netto (1882), whose \"Theory of Substitutions and its Applications to Algebra\" was translated into English by Cole (1892). Other group theorists of the 19th century were Joseph Louis François Bertrand, Charles Hermite, Ferdinand Georg Frobenius, Kronecker, and Émile Mathieu; as well as William Burnside, Leonard Eugene Dickson, Otto Hölder, E. H. Moore, Ludwig Sylow, and Heinrich Martin Weber.\n\nThe convergence of the above three sources into a uniform theory started with Jordan's \"Traité\" and Walther von Dyck (1882) who first defined a group in the full modern sense. The textbooks of Weber and Burnside helped establish group theory as a discipline. The abstract group formulation did not apply to a large portion of 19th century group theory, and an alternative formalism was given in terms of Lie algebras.\n\nGroups in the 1870-1900 period were described as the continuous groups of Lie, the discontinuous groups, finite groups of substitutions of roots (gradually being called permutations), and finite groups of linear substitutions (usually of finite fields). During the 1880-1920 period, groups described by presentations came into a life of their own through the work of Cayley, Walther von Dyck, Max Dehn, Jakob Nielsen, Otto Schreier, and continued in the 1920-1940 period with the work of H. S. M. Coxeter, Wilhelm Magnus, and others to form the field of combinatorial group theory.\n\nFinite groups in the 1870-1900 period saw such highlights as the Sylow theorems, Hölder's classification of groups of square-free order, and the early beginnings of the character theory of Frobenius. Already by 1860, the groups of automorphisms of the finite projective planes had been studied (by Mathieu), and in the 1870s Klein's group-theoretic vision of geometry was being realized in his Erlangen program. The automorphism groups of higher dimensional projective spaces were studied by Jordan in his \"Traité\" and included composition series for most of the so-called classical groups, though he avoided non-prime fields and omitted the unitary groups. The study was continued by Moore and Burnside, and brought into comprehensive textbook form by Leonard Dickson in 1901. The role of simple groups was emphasized by Jordan, and criteria for non-simplicity were developed by Hölder until he was able to classify the simple groups of order less than 200. The study was continued by Frank Nelson Cole (up to 660) and Burnside (up to 1092), and finally in an early \"millennium project\", up to 2001 by Miller and Ling in 1900.\n\nContinuous groups in the 1870-1900 period developed rapidly. Killing and Lie's foundational papers were published, Hilbert's theorem in invariant theory 1882, etc. \n\nIn the period 1900-1940, infinite \"discontinuous\" (now called discrete groups) groups gained life of their own. Burnside's famous problem ushered in the study of arbitrary subgroups of finite-dimensional linear groups over arbitrary fields, and indeed arbitrary groups. Fundamental groups and reflection groups encouraged the developments of J. A. Todd and Coxeter, such as the Todd–Coxeter algorithm in combinatorial group theory. Algebraic groups, defined as solutions of polynomial equations (rather than acting on them, as in the earlier century), benefited heavily from the continuous theory of Lie. Bernard Neumann and Hanna Neumann produced their study of varieties of groups, groups defined by group theoretic equations rather than polynomial ones. \n\nContinuous groups also had explosive growth in the 1900-1940 period. Topological groups began to be studied as such. There were many great achievements in continuous groups: Cartan's classification of semisimple Lie algebras, Hermann Weyl's theory of representations of compact groups, Alfréd Haar's work in the locally compact case.\n\nFinite groups in the 1900-1940 grew immensely. This period witnessed the birth of character theory by Frobenius, Burnside, and Schur which helped answer many of the 19th century questions in permutation groups, and opened the way to entirely new techniques in abstract finite groups. This period saw the work of Philip Hall: on a generalization of Sylow's theorem to arbitrary sets of primes which revolutionized the study of finite soluble groups, and on the power-commutator structure of p-groups, including the ideas of regular p-groups and isoclinism of groups, which revolutionized the study of p-groups and was the first major result in this area since Sylow. This period saw Hans Zassenhaus's famous Schur-Zassenhaus theorem on the existence of complements to Hall's generalization of Sylow subgroups, as well as his progress on Frobenius groups, and a near classification of Zassenhaus groups. \n\nBoth depth, breadth and also the impact of group theory subsequently grew. The domain started branching out into areas such as algebraic groups, group extensions, and representation theory. Starting in the 1950s, in a huge collaborative effort, group theorists succeeded to classify all finite simple groups in 1982. Completing and simplifying the proof of the classification are areas of active research.\n\nAnatoly Maltsev also made important contributions to group theory during this time; his early work was in logic in the 1930s, but in the 1940s he proved important embedding properties of semigroups into groups, studied the isomorphism problem of group rings, established the Malçev correspondence for polycyclic groups, and in the 1960s return to logic proving various theories within the study of groups to be undecidable. Earlier, Alfred Tarski proved elementary group theory undecidable.\nThe period of 1960-1980 was one of excitement in many areas of group theory.\n\nIn finite groups, there were many independent milestones. One had the discovery of 22 new sporadic groups, and the completion of the first generation of the classification of finite simple groups. One had the influential idea of the Carter subgroup, and the subsequent creation of formation theory and the theory of classes of groups. One had the remarkable extensions of Clifford theory by Green to the indecomposable modules of group algebras. During this era, the field of computational group theory became a recognized field of study, due in part to its tremendous success during the first generation classification.\n\nIn discrete groups, the geometric methods of Jacques Tits and the availability the surjectivity of Serge Lang's map allowed a revolution in algebraic groups. The Burnside problem had tremendous progress, with better counterexamples constructed in the 1960s and early 1980s, but the finishing touches \"for all but finitely many\" were not completed until the 1990s. The work on the Burnside problem increased interest in Lie algebras in exponent \"p\", and the methods of Michel Lazard began to see a wider impact, especially in the study of \"p\"-groups.\n\nContinuous groups broadened considerably, with \"p\"-adic analytic questions becoming important. Many conjectures were made during this time, including the coclass conjectures.\n\nThe last twenty years of the 20th century enjoyed the successes of over one hundred years of study in group theory.\n\nIn finite groups, post classification results included the O'Nan–Scott theorem, the Aschbacher classification, the classification of multiply transitive finite groups, the determination of the maximal subgroups of the simple groups and the corresponding classifications of primitive groups. In finite geometry and combinatorics, many problems could now be settled. The modular representation theory entered a new era as the techniques of the classification were axiomatized, including fusion systems, Luis Puig's theory of pairs and nilpotent blocks. The theory of finite soluble groups was likewise transformed by the influential book of Klaus Doerk and Trevor Hawkes which brought the theory of projectors and injectors to a wider audience.\n\nIn discrete groups, several areas of geometry came together to produce exciting new fields. Work on knot theory, orbifolds, hyperbolic manifolds, and groups acting on trees (the Bass–Serre theory), much enlivened the study of hyperbolic groups, automatic groups. Questions such as William Thurston's 1982 geometrization conjecture, inspired entirely new techniques in geometric group theory and low-dimensional topology, and was involved in the solution of one of the Millennium Prize Problems, the Poincaré conjecture.\n\nContinuous groups saw the solution of the problem of hearing the shape of a drum in 1992 using symmetry groups of the laplacian operator. Continuous techniques were applied to many aspects of group theory using function spaces and quantum groups. Many 18th and 19th century problems are now revisited in this more general setting, and many questions in the theory of the representations of groups have answers.\n\nGroup theory continues to be an intensely studied matter. Its importance to contemporary mathematics as a whole may be seen from the 2008 Abel Prize, awarded to John Griggs Thompson and Jacques Tits for their contributions to group theory.\n\n"}
{"id": "4492813", "url": "https://en.wikipedia.org/wiki?curid=4492813", "title": "Indiana Pi Bill", "text": "Indiana Pi Bill\n\nThe Indiana Pi Bill is the popular name for bill #246 of the 1897 sitting of the Indiana General Assembly, one of the most notorious attempts to establish mathematical truth by legislative fiat. Despite its name, the main result claimed by the bill is a method to square the circle, rather than to establish a certain value for the mathematical constant , the ratio of the circumference of a circle to its diameter. The bill, written by amateur mathematician Edward J. Goodwin, does imply various incorrect values of , such as 3.2.\n\nThe bill never became law, due to the intervention of Professor C. A. Waldo of Purdue University, who happened to be present in the legislature on the day it went up for a vote.\n\nThe impossibility of squaring the circle using only compass and straightedge constructions, suspected since ancient times, was rigorously proven in 1882 by Ferdinand von Lindemann. Better approximations of than those implied by the bill have been known since ancient times.\n\nIn 1894, Indiana physician and amateur mathematician Edward J. Goodwin (ca. 1825–1902) believed that he had discovered a correct way of squaring the circle. He proposed a bill to state representative Taylor I. Record, which Record introduced in the House under the long title \"A Bill for an act introducing a new mathematical truth and offered as a contribution to education to be used only by the State of Indiana free of cost by paying any royalties whatever on the same, provided it is accepted and adopted by the official action of the Legislature of 1897\".\n\nThe text of the bill consists of a series of mathematical claims (detailed below), followed by a recitation of Goodwin's previous accomplishments:\n\nGoodwin's \"solutions\" were indeed published in the \"American Mathematical Monthly\", though with a disclaimer of \"published by request of the author\".\n\nUpon its introduction in the Indiana House of Representatives, the bill's language and topic occasioned confusion among the membership; a member from Bloomington proposed that it be referred to the Finance Committee, but the Speaker accepted another member's recommendation to refer the bill to the Committee on Swamplands, where the bill could \"find a deserved grave\". It was transferred to the Committee on Education, which reported favorably; following a motion to suspend the rules, the bill passed on February 6, without a dissenting vote. The news of the bill occasioned an alarmed response from \"Der Tägliche Telegraph\", a German-language newspaper in Indianapolis, which viewed the event with significantly less favor than its English-speaking competitors. As this debate concluded, Purdue University Professor C. A. Waldo arrived in Indianapolis to secure the annual appropriation for the Indiana Academy of Science. An assemblyman handed him the bill, offering to introduce him to the genius who wrote it. He declined, saying that he already met as many crazy people as he cared to.\n\nWhen it reached the Indiana Senate, the bill was not treated so kindly, for Waldo had coached the senators previously. The committee to which it had been assigned reported it unfavorably, and the Senate tabled it on February 12; it was nearly passed, but opinion changed when one senator observed that the General Assembly lacked the power to define mathematical truth. Influencing some of the senators was a report that major newspapers, such as the \"Chicago Tribune\", had begun to ridicule the situation.\n\nAccording to the \"Indianapolis News\" article of February 13, page 11, column 3:\n\n... the bill was brought up and made fun of. The Senators made bad puns about it, ridiculed it and laughed over it. The fun lasted half an hour. Senator Hubbell said that it was not meet for the Senate, which was costing the State $250 a day, to waste its time in such frivolity. He said that in reading the leading newspapers of Chicago and the East, he found that the Indiana State Legislature had laid itself open to ridicule by the action already taken on the bill. He thought consideration of such a proposition was not dignified or worthy of the Senate. He moved the indefinite postponement of the bill, and the motion carried.\n\nAlthough the bill has become known as the \"Pi Bill\", its text does not mention the name \"pi\" at all, and Goodwin appears to have thought of the ratio between the circumference and diameter of a circle as distinctly secondary to his main aim of squaring the circle. Towards the end of Section 2 the following passage appears:\n\nThis comes close to an explicit claim that formula_1, and that formula_2.\n\nThis quotation is often read as three mutually incompatible assertions, but they fit together well if the statement about is taken to be about the inscribed square (with the circle's diameter as diagonal) rather than the square on the radius (with the chord of 90° as diagonal). Together they describe the circle shown in the figure, whose diameter is 10 and circumference is 32; the chord of 90° is taken to be 7. Both of the values 7 and 32 are within a few percent of the true lengths for a diameter-10 circle (which does not justify Goodwin's presentation of them as exact). The circumference should be nearer to 31.4159 and the diagonal \"7\" should be the square root of 50 (=25+25), or nearer to 7.071.\n\nGoodwin's main goal was not to measure lengths in the circle but to \"square\" it, which he interpreted literally as finding a square with the same area as the circle. He knew that Archimedes' formula for the area of a circle, which calls for multiplying the diameter by one fourth of the circumference, is not considered a solution to the ancient problem of squaring the circle. This is because the problem is to \"construct\" the area using compass and straightedge only, and Archimedes did not give a method for constructing a straight line with the same length as the circumference. Apparently, Goodwin was unaware of this central requirement; he believed that the problem with the Archimedean formula is that it gives wrong numerical results, and that a solution of the ancient problem should consist of replacing it with a \"correct\" formula. In the bill he proposed, without argument, his own method:\n\nThis appears needlessly convoluted, as an \"equilateral rectangle\" is, by definition, a square. In simple terms, the assertion is that the area of a circle is the same as that of a square with the same perimeter. This claim results in other mathematical contradictions to which Goodwin attempts to respond. For example, right after the above quote the bill goes on to say:\n\nIn the model circle above, the Archimedean area (accepting Goodwin's values for the circumference and diameter) would be 80, whereas Goodwin's proposed rule leads to an area of 64. Now, 80 exceeds 64 by one fifth \"of 80\", and Goodwin appears to confuse 64 = 80×(1−) with 80 = 64×(1+), an approximation that works only for fractions much smaller than .\n\nThe area found by Goodwin's rule is times the true area of the circle, which in many accounts of the Pi Bill is interpreted as a claim that = 4. However, there is no internal evidence in the bill that Goodwin intended to make such a claim; on the contrary, he repeatedly denies that the area of the circle has anything to do with its diameter.\n\nThe relative \"area\" error of 1− works out to about 21 percent, which is much more grave than the approximations of the \"lengths\" in the model circle of the previous section. It is unknown what made Goodwin believe that his rule could be correct. In general, figures with identical perimeters do not have identical area (see isoperimetry); the typical demonstration of this fact is to compare a long thin shape with a small enclosed area (the area approaching zero as the width decreases) to one of the same perimeter that is approximately as tall as it is wide (the area approaching the square of the width), obviously of much greater area.\n\n\n"}
{"id": "383424", "url": "https://en.wikipedia.org/wiki?curid=383424", "title": "Italian school of algebraic geometry", "text": "Italian school of algebraic geometry\n\nIn relation with the history of mathematics, the Italian school of algebraic geometry refers to the work over half a century or more (flourishing roughly 1885–1935) done internationally in birational geometry, particularly on algebraic surfaces. There were in the region of 30 to 40 leading mathematicians who made major contributions, about half of those being in fact Italian. The leadership fell to the group in Rome of Guido Castelnuovo, Federigo Enriques and Francesco Severi, who were involved in some of the deepest discoveries, as well as setting the style.\n\nThe emphasis on algebraic surfaces—algebraic varieties of dimension two—followed on from an essentially complete geometric theory of algebraic curves (dimension 1). The position in around 1870 was that the curve theory had incorporated with Brill–Noether theory the Riemann–Roch theorem in all its refinements (via the detailed geometry of the theta-divisor).\n\nThe classification of algebraic surfaces was a bold and successful attempt to repeat the division of curves by their genus \"g\". It corresponds to the rough classification into the three types: \"g\" = 0 (projective line); \"g\" = 1 (elliptic curve); and \"g\" > 1 (Riemann surfaces with independent holomorphic differentials). In the case of surfaces, the Enriques classification was into five similar big classes, with three of those being analogues of the curve cases, and two more (elliptic fibrations, and K3 surfaces, as they would now be called) being with the case of two-dimension abelian varieties in the 'middle' territory. This was an essentially sound, breakthrough set of insights, recovered in modern complex manifold language by Kunihiko Kodaira in the 1950s, and refined to include mod p phenomena by Zariski, the Shafarevich school and others by around 1960. The form of the Riemann–Roch theorem on a surface was also worked out.\n\nSome proofs produced by the school are not considered satisfactory because of foundational difficulties. These included frequent use of birational models in dimension three of surfaces that can have non-singular models only when embedded in higher-dimensional projective space. In order to avoid these issues, a sophisticated theory of handling a linear system of divisors was developed (in effect, a line bundle theory for hyperplane sections of putative embeddings in projective space). Many modern techniques were found, in embryonic form, and in some cases the articulation of these ideas exceeded the available technical language.\n\nAccording to Guerraggio & Nastasi (page 9, 2005) Luigi Cremona is \"considered the founder of the Italian school of algebraic geometry\". Later they explain that in Turin the collaboration of Enrico D'Ovidio and Corrado Segre \"would bring, either by their own efforts or those of their students, Italian algebraic geometry to full maturity\". A one-time student of Segre, H.F. Baker wrote (1926, page 269), [Corrado Segre] \"may probably be said to be the father of that wonderful Italian school which has achieved so much in the birational theory of algebraical loci.\" On this topic, Brigaglia & Ciliberto (2004) say \"Segre had headed and maintained the school of geometry that Luigi Cremona had established in 1860.\" Reference to the Mathematics Genealogy Project shows that, in terms of \"Italian doctorates\", the real productivity of the school began with Guido Castelnuovo and Federigo Enriques. In the USA Oscar Zariski inspired many Ph.D.s.\n\nThe roll of honour of the school includes the following other Italians: Giacomo Albanese, Eugenio Bertini, Luigi Campedelli, Oscar Chisini, Michele De Franchis, Pasquale del Pezzo, Beniamino Segre, Francesco Severi, Guido Zappa (with contributions also from Gino Fano, Carlo Rosati, Giuseppe Torelli, Giuseppe Veronese).\n\nElsewhere it involved H. F. Baker and Patrick du Val (UK), Arthur Byron Coble (USA), Georges Humbert and Charles Émile Picard (France), Lucien Godeaux (Belgium), Hermann Schubert and Max Noether, and later Erich Kähler (Germany), H. G. Zeuthen (Denmark).\n\nThese figures were all involved in algebraic geometry, rather than the pursuit of projective geometry as synthetic geometry, which during the period under discussion was a huge (in volume terms) but secondary subject (when judged by its importance as research).\n\nThe new algebraic geometry that would succeed the Italian school was distinguished also by the intensive use of algebraic topology. The founder of that tendency was Henri Poincaré; during the 1930s it was developed by Lefschetz, Hodge and Todd. The modern synthesis brought together their work, that of the Cartan school, and of W.L. Chow and Kunihiko Kodaira, with the traditional material.\n\nIn the earlier years of the Italian school under Castelnuovo, the standards of rigor were as high as most areas of mathematics. Under Enriques it gradually became acceptable to use somewhat more informal arguments instead of complete rigorous proofs, such as the \"principle of continuity\" saying that what is true up to the limit is true at the limit, a claim that had neither a rigorous proof nor even a precise statement. At first this did not matter too much, as Enriques's intuition was so good that essentially all the results he claimed were in fact correct, and using this more informal style of argument allowed him to produce spectacular results about algebraic surfaces. \nUnfortunately, from about 1930 onwards under Severi's leadership the standards of accuracy declined further, to the point where some of the claimed results were not just inadequately proved, but were hopelessly wrong. \nFor example, in 1934 Severi claimed that the space of rational equivalence classes of cycles on an algebraic surface is finite-dimensional, but showed that this is false for surfaces of positive geometric genus, and in 1946 Severi published a paper claiming to prove that a degree-6 surface in 3-dimensional projective space has at most 52 nodes, but the Barth sextic has 65 nodes.\nSeveri did not accept that his arguments were inadequate, leading to some acrimonious disputes as to the status of some results.\n\nBy about 1950 it had become too difficult to tell which of the results claimed were correct, and the informal intuitive school of algebraic geometry simply collapsed due to its inadequate foundations.\nFrom about 1950 to 1980 there was considerable effort to salvage as much as possible from the wreckage, and convert it into the rigorous algebraic style of algebraic geometry set up by Weil and Zariski. In particular in the 1960s Kodaira and Shafarevich and his students rewrote the Enriques classification of algebraic surfaces in a more rigorous style, and also extended it to all compact complex surfaces, while in the 1970s Fulton and MacPherson put the classical calculations of intersection theory on rigorous foundations.\n\n\n"}
{"id": "45712630", "url": "https://en.wikipedia.org/wiki?curid=45712630", "title": "LINGO (mathematical modeling language)", "text": "LINGO (mathematical modeling language)\n\nLINGO is a mathematical modeling language designed for formulating and solving optimization problems, including linear, integer, and nonlinear programming problems.\n"}
{"id": "352181", "url": "https://en.wikipedia.org/wiki?curid=352181", "title": "List of abstract algebra topics", "text": "List of abstract algebra topics\n\nAbstract algebra is the subject area of mathematics that studies algebraic structures, such as groups, rings, fields, modules, vector spaces, and algebras. The phrase abstract algebra was coined at the turn of the 20th century to distinguish this area from what was normally referred to as algebra, the study of the rules for manipulating formulae and algebraic expressions involving unknowns and real or complex numbers, often now called \"elementary algebra\". The distinction is rarely made in more recent writings.\n\nAlgebraic structures are defined primarily as sets with \"operations\". \n\nStructure preserving maps called \"homomorphisms\" are vital in the study of algebraic objects.\n\nThere are several basic ways to combine algebraic objects of the same type to produce a third object of the same type. These constructions are used throughout algebra.\n\nAdvanced concepts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation theory\n\n\n\n\n\n"}
{"id": "734881", "url": "https://en.wikipedia.org/wiki?curid=734881", "title": "List of examples in general topology", "text": "List of examples in general topology\n\nThis is a list of useful examples in general topology, a field of mathematics.\n\n\n"}
{"id": "3232460", "url": "https://en.wikipedia.org/wiki?curid=3232460", "title": "List of exceptional set concepts", "text": "List of exceptional set concepts\n\nThis is a list of exceptional set concepts. In mathematics, and in particular in mathematical analysis, it is very useful to be able to characterise subsets of a given set \"X\" as 'small', in some definite sense, or 'large' if their complement in \"X\" is small. There are numerous concepts that have been introduced to study 'small' or 'exceptional' subsets. In the case of sets of natural numbers, it is possible to define more than one concept of 'density', for example. See also list of properties of sets of reals.\n\n"}
{"id": "341127", "url": "https://en.wikipedia.org/wiki?curid=341127", "title": "List of functional analysis topics", "text": "List of functional analysis topics\n\nThis is a list of functional analysis topics, by Wikipedia page.\n\n\n\n\n\n\n\n\n\n\"See also list of mathematical topics in quantum theory\"\n\n\n\n\n"}
{"id": "234969", "url": "https://en.wikipedia.org/wiki?curid=234969", "title": "List of integrals of inverse trigonometric functions", "text": "List of integrals of inverse trigonometric functions\n\nThe following is a list of indefinite integrals (antiderivatives) of expressions involving the inverse trigonometric functions. For a complete list of integral formulas, see lists of integrals.\n\n"}
{"id": "1631654", "url": "https://en.wikipedia.org/wiki?curid=1631654", "title": "List of mathematical identities", "text": "List of mathematical identities\n\nThis page lists mathematical identities, that is, \"identically true relations\" holding in mathematics.\n\n\n\n"}
{"id": "5971801", "url": "https://en.wikipedia.org/wiki?curid=5971801", "title": "List of mathematicians (D)", "text": "List of mathematicians (D)\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971813", "url": "https://en.wikipedia.org/wiki?curid=5971813", "title": "List of mathematicians (I)", "text": "List of mathematicians (I)\n\n\n\n\n"}
{"id": "5971814", "url": "https://en.wikipedia.org/wiki?curid=5971814", "title": "List of mathematicians (J)", "text": "List of mathematicians (J)\n\n\n\n\n\n"}
{"id": "44787301", "url": "https://en.wikipedia.org/wiki?curid=44787301", "title": "List of mathematicians born in the 19th century", "text": "List of mathematicians born in the 19th century\n\nMathematicians born in the 19th century listed by nationality.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13186787", "url": "https://en.wikipedia.org/wiki?curid=13186787", "title": "List of price index formulas", "text": "List of price index formulas\n\nA number of different formulae, more than hundred, have been proposed as means of calculating price indexes. While price index formulae all use price and possibly quantity data, they aggregate these in different ways. A price index aggregates various combinations of base period prices (formula_1), later period prices (formula_2), base period quantities (formula_3), and later period quantities (formula_4). Price index numbers are usually defined either in terms of (actual or hypothetical) expenditures (expenditure = price * quantity) or as different weighted averages of price relatives (formula_5). These tell the relative change of the price in question. Two of the most commonly used price index formulae were defined by German economists and statisticians Étienne Laspeyres and Hermann Paasche, both around 1875 when investigating price changes in Germany.\n\nDeveloped in 1871 by Laspeyres, the formula:\n\ncompares the total cost of the same basket of goods formula_3 at the old and new prices.\n\nDeveloped in 1874 by Paasche, the formula:\n\ncompares the total cost of a new basket of goods formula_4 at the old and new prices.\n\nThe geometric means index:\n\nincorporates quantity information through the share of expenditure in the base period.\n\nUnweighted, or \"elementary\", price indices only compare prices of a single type of good between two periods. They do not make any use of quantities or expenditure weights. They are called \"elementary\" because they are often used at the lower levels of aggregation for more comprehensive price indices. In such a case, they are not indices but merely an intermediate stage in the calculation of an index. At these lower levels, it is argued that weighting is not necessary since only one type of good is being aggregated. However this implicitly assumes that only one type of the good is available (e.g. only one brand and one package size of frozen peas) and that it has not changed in quality etc between time periods.\n\nDeveloped in 1764 by Carli, an Italian economist, this formula is the arithmetic mean of the price relative between a period \"t\" and a base period \"0\".\n\nOn 17 August 2012 the BBC Radio 4 program \"More or Less\" noted that the Carli index, used in part in the British Retail Price Index measure, has a built-in bias towards recording inflation even when over successive periods there is no increase in prices overall.\n\nIn 1738 French economist Dutot proposed using an index calculated by dividing the average price in period \"t\" by the average price in period \"0\".\n\nIn 1863, English economist Jevons proposed taking the geometric average of the price relative of period \"t\" and base period \"0\". When used as an elementary aggregate, the Jevons index is considered a constant elasticity of substitution index since it allows for product substitution between time periods.\n\nThis is the formula that was used for the old Financial Times stock market index (the predecessor of the FTSE 100 Index). It was inadequate for that purpose. In particular, if the price of any of the constituents were to fall to zero, the whole index would fall to zero. That is an extreme case; in general the formula will understate the total cost of a basket of goods (or of any subset of that basket) unless their prices all change at the same rate. Also, as the index is unweighted, large price changes in selected constituents can transmit to the index to an extent not representing their importance in the average portfolio.\n\nThe harmonic average counterpart to the Carli index. The index was proposed by Jevons in 1865 and by Coggeshall in 1887.\n\nIs the geometric mean of the Carli and the harmonic price indexes. In 1922 Fisher wrote that this and the Jevons were the two best unweighted indexes based on Fisher's test approach to index number theory.\n\nThe ratio of harmonic means or \"Harmonic means\" price index is the harmonic average counterpart to the Dutot index.\n\nThe Marshall-Edgeworth index, credited to Marshall (1887) and Edgeworth (1925), is a weighted relative of current period to base period sets of prices. This index uses the arithmetic average of the current and based period quantities for weighting. It is considered a pseudo-superlative formula and is symmetric. The use of the Marshall-Edgeworth index can be problematic in cases such as a comparison of the price level of a large country to a small one. In such instances, the set of quantities of the large country will overwhelm those of the small one.\n\nSuperlative indices treat prices and quantities equally across periods. They are symmetrical and provide close approximations of cost of living indices and other theoretical indices used to provide guidelines for constructing price indices. All superlative indices produce similar results and are generally the favored formulas for calculating price indices. A superlative index is defined technically as \"an index that is exact for a flexible functional form that can provide a second-order approximation to other twice-differentiable functions around the same point.\"\n\nThe change in a Fisher index from one period to the next is the geometric mean of the changes in Laspeyres's and Paasche's indexes between those periods, and these are chained together to make comparisons over many periods:\n\nThis is also called Fisher's \"ideal\" price index.\n\nThe Törnqvist or Törnqvist-Theil index is the geometric average of the n price relatives of the current to base period prices (for n goods) weighted by the arithmetic average of the value shares for the two periods.\n\nThe Walsh price index is the weighted sum of the current period prices divided by the weighted sum of the base period prices with the geometric average of both period quantities serving as the weighting mechanism:\n\n"}
{"id": "40658915", "url": "https://en.wikipedia.org/wiki?curid=40658915", "title": "List of works by Nicolas Minorsky", "text": "List of works by Nicolas Minorsky\n\nList of works by Nicolas Minorsky.\n\n\n\n\n\n\n"}
{"id": "18974136", "url": "https://en.wikipedia.org/wiki?curid=18974136", "title": "Mathematical beauty", "text": "Mathematical beauty\n\nMathematical beauty describes the notion that some mathematicians may derive aesthetic pleasure from their work, and from mathematics in general. They express this pleasure by describing mathematics (or, at least, some aspect of mathematics) as \"beautiful\". Mathematicians describe mathematics as an art form or, at a minimum, as a creative activity. Comparisons are often made with music and poetry.\n\nBertrand Russell expressed his sense of mathematical beauty in these words:\n\nMathematics, rightly viewed, possesses not only truth, but supreme beauty—a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. The true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as poetry.\nPaul Erdős expressed his views on the ineffability of mathematics when he said, \"Why are numbers beautiful? It's like asking why is Beethoven's Ninth Symphony beautiful. If you don't see why, someone can't tell you. I \"know\" numbers are beautiful. If they aren't beautiful, nothing is\".\n\nMathematicians describe an especially pleasing method of proof as \"elegant\". Depending on context, this may mean:\n\n\nIn the search for an elegant proof, mathematicians often look for different independent ways to prove a result—the first proof that is found may not be the best. The theorem for which the greatest number of different proofs have been discovered is possibly the Pythagorean theorem, with hundreds of proofs having been published. Another theorem that has been proved in many different ways is the theorem of quadratic reciprocity—Carl Friedrich Gauss alone published eight different proofs of this theorem.\n\nConversely, results that are logically correct but involve laborious calculations, over-elaborate methods, very conventional approaches, or that rely on a large number of particularly powerful axioms or previous results are not usually considered to be elegant, and may be called \"ugly\" or \"clumsy\".\n\nSome mathematicians see beauty in mathematical results that establish connections between two areas of mathematics that at first sight appear to be unrelated. These results are often described as \"deep\".\n\nWhile it is difficult to find universal agreement on whether a result is deep, some examples are often cited. One is Euler's identity:\n\nformula_1\n\nThis is a special case of Euler's formula, which the physicist Richard Feynman called \"our jewel\" and \"the most remarkable formula in mathematics\". Modern examples include the modularity theorem, which establishes an important connection between elliptic curves and modular forms (work on which led to the awarding of the Wolf Prize to Andrew Wiles and Robert Langlands), and \"monstrous moonshine\", which connects the Monster group to modular functions via string theory for which Richard Borcherds was awarded the Fields Medal.\n\nOther examples of deep results include unexpected insights into mathematical structures. For example, Gauss's Theorema Egregium is a deep theorem which relates a local phenomenon (curvature) to a global phenomenon (area) in a surprising way. In particular, the area of a triangle on a curved surface is proportional to the excess of the triangle and the proportionality is curvature. Another example is the fundamental theorem of calculus (and its vector versions including Green's theorem and Stokes' theorem).\n\nThe opposite of \"deep\" is \"trivial\". A trivial theorem may be a result that can be derived in an obvious and straightforward way from other known results, or which applies only to a specific set of particular objects such as the empty set. Sometimes, however, a statement of a theorem can be original enough to be considered deep, even though its proof is fairly obvious.\n\nIn his \"A Mathematician's Apology\", Hardy suggests that a beautiful proof or result possesses \"inevitability\", \"unexpectedness\", and \"economy\".\n\nRota, however, disagrees with unexpectedness as a sufficient condition for beauty and proposes a counterexample:\n\nPerhaps ironically, Monastyrsky writes:\n\nThis disagreement illustrates both the subjective nature of mathematical beauty and its connection with mathematical results: in this case, not only the existence of exotic spheres, but also a particular realization of them.\n\nInterest in pure mathematics separate from empirical study has been part of the experience of various civilizations, including that of the ancient Greeks, who \"did mathematics for the beauty of it\". The aesthetic pleasure that mathematical physicists tend to experience in Einstein's theory of general relativity has been attributed (by Paul Dirac, among others) to its \"great mathematical beauty\". The beauty of mathematics is experienced when the physical reality of objects are represented by mathematical models. Group theory, developed in the early 1800s for the sole purpose of solving polynomial equations, became a fruitful way of categorizing elementary particles—the building blocks of matter. Similarly, the study of knots provides important insights into string theory and loop quantum gravity.\n\nSome believe that in order to appreciate mathematics, one must engage in doing mathematics.\nThere are some teachers that encourage student engagement by teaching mathematics in a kinesthetic way (see kinesthetic learning). For example, Math Circle is an afterschool enrichment program where students do mathematics through games and activities; in a general Math Circle lesson, students use pattern finding, observation, and exploration to make their own mathematical discoveries. For example, mathematical beauty arises in a Math Circle activity on symmetry designed for 2nd and 3rd graders. In this activity, students create their own snowflakes by folding a square piece of paper and cutting out designs of their choice along the edges of the folded paper. When the paper is unfolded, a symmetrical design reveals itself. In a day to day elementary school mathematics class, symmetry can be presented as such in an artistic manner where students see aesthetically pleasing results in mathematics.\n\nSome teachers prefer to use mathematical manipulatives to present mathematics in an aesthetically pleasing way. Examples of a manipulative include algebra tiles, cuisenaire rods, and pattern blocks. For example, one can teach the method of completing the square by using algebra tiles. Cuisenaire rods can be used to teach fractions, and pattern blocks can be used to teach geometry. Using mathematical manipulatives helps students gain a conceptual understanding that might not be seen immediately in written mathematical formulas. \n\nAnother example involves origami. Origami, the art of paper folding, has aesthetic qualities and many mathematical connections. One can study the mathematics of paper folding by observing the crease pattern on unfolded origami pieces.\n\nCombinatorics (the study of counting) has artistic representations that some find mathematically beautiful. There are many visual examples that illustrate combinatorial concepts. Here are some topics and objects seen in combinatorics courses with visual representations:\n\nSome mathematicians are of the opinion that the doing of mathematics is closer to discovery than invention, for example:\n\nThese mathematicians believe that the detailed and precise results of mathematics may be reasonably taken to be true without any dependence on the universe in which we live. For example, they would argue that the theory of the natural numbers is fundamentally valid, in a way that does not require any specific context. Some mathematicians have extrapolated this viewpoint that mathematical beauty is truth further, in some cases becoming mysticism.\n\nPythagorean mathematicians believed in the literal reality of numbers. The discovery of the existence of irrational numbers was a shock to them, since they considered the existence of numbers not expressible as the ratio of two natural numbers to be a flaw in nature (the Pythagorean world view did not contemplate the limits of infinite sequences of ratios of natural numbers—the modern notion of a real number). From a modern perspective, their mystical approach to numbers may be viewed as numerology.\n\nIn Plato's philosophy there were two worlds, the physical one in which we live and another abstract world which contained unchanging truth, including mathematics. He believed that the physical world was a mere reflection of the more perfect abstract world.\n\nHungarian mathematician Paul Erdős spoke of an imaginary book, in which God has written down all the most beautiful mathematical proofs. When Erdős wanted to express particular appreciation of a proof, he would exclaim \"This one's from The Book!\"\n\nTwentieth-century French philosopher Alain Badiou claims that ontology is mathematics. Badiou also believes in deep connections between mathematics, poetry and philosophy.\n\nIn some cases, natural philosophers and other scientists who have made extensive use of mathematics have made leaps of inference between beauty and physical truth in ways that turned out to be erroneous. For example, at one stage in his life, Johannes Kepler believed that the proportions of the orbits of the then-known planets in the Solar System have been arranged by God to correspond to a concentric arrangement of the five Platonic solids, each orbit lying on the circumsphere of one polyhedron and the insphere of another. As there are exactly five Platonic solids, Kepler's hypothesis could only accommodate six planetary orbits and was disproved by the subsequent discovery of Uranus.\n\nIn the 1970s, Abraham Moles and Frieder Nake analyzed links between beauty, information processing, and information theory. In the 1990s, Jürgen Schmidhuber formulated a mathematical theory of observer-dependent subjective beauty based on algorithmic information theory: the most beautiful objects among subjectively comparable objects have short algorithmic descriptions (i.e., Kolmogorov complexity) relative to what the observer already knows. Schmidhuber explicitly distinguishes between beautiful and interesting. The latter corresponds to the first derivative of subjectively perceived beauty:\nthe observer continually tries to improve the predictability and compressibility of the observations by discovering regularities such as repetitions and symmetries and fractal self-similarity. Whenever the observer's learning process (possibly a predictive artificial neural network) leads to improved data compression such that the observation sequence can be described by fewer bits than before, the temporary interestingness of the data corresponds to the compression progress, and is proportional to the observer's internal curiosity reward.\n\nExamples of the use of mathematics in music include the stochastic music of Iannis Xenakis, Fibonacci in Tool's Lateralus, counterpoint of Johann Sebastian Bach, polyrhythmic structures (as in Igor Stravinsky's \"The Rite of Spring\"), the Metric modulation of Elliott Carter, permutation theory in serialism beginning with Arnold Schoenberg, and application of Shepard tones in Karlheinz Stockhausen's \"Hymnen\".\n\nExamples of the use of mathematics in the visual arts include applications of chaos theory and fractal geometry to computer-generated art, symmetry studies of Leonardo da Vinci, projective geometries in development of the perspective theory of Renaissance art, grids in Op art, optical geometry in the camera obscura of Giambattista della Porta, and multiple perspective in analytic cubism and futurism.\n\nThe Dutch graphic designer M. C. Escher created mathematically inspired woodcuts, lithographs, and mezzotints. These feature impossible constructions, explorations of infinity, architecture, visual paradoxes and tessellations. British constructionist artist John Ernest created reliefs and paintings inspired by group theory. A number of other British artists of the constructionist and systems schools also draw on mathematics models and structures as a source of inspiration, including Anthony Hill and Peter Lowe. Computer-generated art is based on mathematical algorithms.\n\n\n"}
{"id": "20590", "url": "https://en.wikipedia.org/wiki?curid=20590", "title": "Mathematical model", "text": "Mathematical model\n\nA mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in the social sciences (such as economics, psychology, sociology, political science). \n\nA model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.\n\nMathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\n\nIn the physical sciences, a traditional mathematical model contains most of the following elements:\n\nMathematical models are usually composed of relationships and \"variables\". Relationships can be described by \"operators\", such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:\n\nMathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.\n\nThroughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.\n\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schrödinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.\n\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.\n\nSince prehistorical times simple models such as maps and diagrams have been used.\n\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.\n\nA mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\n\nIn business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.\n\nDecision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants.\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\n\nObjectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an \"index of performance\", as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\n\nFor example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.\n\nMathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\n\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\n\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\n\nSometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.\n\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.\n\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification .\n\nFor example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.\n\nAny model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by an artificial neural network or other machine learning, the optimization of parameters is called \"training\", while the optimization of model hyperparameters is called \"tuning\" and often uses cross-validation. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by \"curve fitting\".\n\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.\n\nUsually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.\n\nDefining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.\n\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.\n\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data.\n\nThe question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.\n\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\n\nMany types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\n\nAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.\n\n\"M\" = (\"Q\", Σ, δ, \"q\", \"F\") where\n\nThe state \"S\" represents that there has been an even number of 0s in the input so far, while \"S\" signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, \"M\" will finish in state \"S\", an accepting state, so the input string will be accepted.\n\nThe language recognized by \"M\" is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where \"*\" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols \"1\".\n\n\nthat can be written also as:\n\n\n\n\n\n\n\n"}
{"id": "43794390", "url": "https://en.wikipedia.org/wiki?curid=43794390", "title": "Mathematical theory", "text": "Mathematical theory\n\nA mathematical theory is a subfield of mathematics that is an area of mathematical research. A theory can be a body of knowledge, and so in this sense a \"mathematical theory\" refers to an area of mathematical research. This is distinct from the idea of mathematical models. Branches of mathematics like group theory and number theory are examples of this.\n\n\n"}
{"id": "22468661", "url": "https://en.wikipedia.org/wiki?curid=22468661", "title": "Mathematics and art", "text": "Mathematics and art\n\nMathematics and art are related in a variety of ways. Mathematics has itself been described as an art motivated by beauty. Mathematics can be discerned in arts such as music, dance, painting, architecture, sculpture, and textiles. This article focuses, however, on mathematics in the visual arts.\n\nMathematics and art have a long historical relationship. Artists have used mathematics since the 4th century BC when the Greek sculptor Polykleitos wrote his \"Canon\", prescribing proportions based on the ratio 1: for the ideal male nude. Persistent popular claims have been made for the use of the golden ratio in ancient art and architecture, without reliable evidence. In the Italian Renaissance, Luca Pacioli wrote the influential treatise \"De Divina Proportione\" (1509), illustrated with woodcuts by Leonardo da Vinci, on the use of the golden ratio in art. Another Italian painter, Piero della Francesca, developed Euclid's ideas on perspective in treatises such as \"De Prospectiva Pingendi\", and in his paintings. The engraver Albrecht Dürer made many references to mathematics in his work \"Melencolia I\". In modern times, the graphic artist M. C. Escher made intensive use of tessellation and hyperbolic geometry, with the help of the mathematician H. S. M. Coxeter, while the De Stijl movement led by Theo van Doesburg and Piet Mondrian explicitly embraced geometrical forms. Mathematics has inspired textile arts such as quilting, knitting, cross-stitch, crochet, embroidery, weaving, Turkish and other carpet-making, as well as kilim. In Islamic art, symmetries are evident in forms as varied as Persian girih and Moroccan zellige tilework, Mughal jali pierced stone screens, and widespread muqarnas vaulting.\n\nMathematics has directly influenced art with conceptual tools such as linear perspective, the analysis of symmetry, and mathematical objects such as polyhedra and the Möbius strip. Magnus Wenninger creates colourful stellated polyhedra, originally as models for teaching. Mathematical concepts such as recursion and logical paradox can be seen in paintings by Rene Magritte and in engravings by M. C. Escher. Computer art often makes use of fractals including the Mandelbrot set, and sometimes explores other mathematical objects such as cellular automata. Controversially, the artist David Hockney has argued that artists from the Renaissance onwards made use of the camera lucida to draw precise representations of scenes; the architect Philip Steadman similarly argued that Vermeer used the camera obscura in his distinctively observed paintings.\n\nOther relationships include the algorithmic analysis of artworks by X-ray fluorescence spectroscopy, the finding that traditional batiks from different regions of Java have distinct fractal dimensions, and stimuli to mathematics research, especially Filippo Brunelleschi's theory of perspective, which eventually led to Girard Desargues's projective geometry. A persistent view, based ultimately on the Pythagorean notion of harmony in music, holds that everything was arranged by Number, that God is the geometer of the world, and that therefore the world's geometry is sacred, as seen in artworks such as William Blake's \"The Ancient of Days\".\n\nPolykleitos the elder (c.450–420 BC) was a Greek sculptor from the school of Argos, and a contemporary of Phidias. His works and statues consisted mainly of bronze and were of athletes. According to the philosopher and mathematician Xenocrates, Polykleitos is ranked as one of the most important sculptors of Classical antiquity for his work on the \"Doryphorus\" and the statue of Hera in the Heraion of Argos. While his sculptures may not be as famous as those of Phidias, they are much admired. In the \"Canon\" of Polykleitos, a treatise he wrote designed to document the \"perfect\" anatomical proportions of the male nude, Polykleitos gives us a mathematical approach towards sculpturing the human body.\n\nPolykleitos uses the distal phalanx of the little finger as the basic module for determining the proportions of the human body. Polykleitos multiplies the length of the distal phalanx by the square root of two () to get the distance of the second phalanges and multiplies the length again by to get the length of the third phalanges. Next, he takes the finger length and multiplies that by to get the length of the palm from the base of the finger to the ulna. This geometric series of measurements progresses until Polykleitos has formed the arm, chest, body, and so on.\n\nThe influence of the \"Canon\" of Polykleitos is immense in Classical Greek, Roman, and Renaissance sculpture, many sculptors following Polykleitos's prescription. While none of Polykleitos's original works survive, Roman copies demonstrate his ideal of physical perfection and mathematical precision. Some scholars argue that Pythagorean thought influenced the \"Canon\" of Polykleitos. The \"Canon\" applies the basic mathematical concepts of Greek geometry, such as the ratio, proportion, and \"symmetria\" (Greek for \"harmonious proportions\") and turns it into a system capable of describing the human form through a series of continuous geometric progressions.\n\nIn classical times, rather than making distant figures smaller with linear perspective, painters sized objects and figures according to their thematic importance. In the Middle Ages, some artists used reverse perspective for special emphasis. The Muslim mathematician Alhazen (Ibn al-Haytham) described a theory of optics in his \"Book of Optics\" in 1021, but never applied it to art. The Renaissance saw a rebirth of Classical Greek and Roman culture and ideas, among them the study of mathematics to understand nature and the arts. Two major motives drove artists in the late Middle Ages and the Renaissance towards mathematics. First, painters needed to figure out how to depict three-dimensional scenes on a two-dimensional canvas. Second, philosophers and artists alike were convinced that mathematics was the true essence of the physical world and that the entire universe, including the arts, could be explained in geometric terms.\n\nThe rudiments of perspective arrived with Giotto (1266/7 – 1337), who attempted to draw in perspective using an algebraic method to determine the placement of distant lines. In 1415, the Italian architect Filippo Brunelleschi and his friend Leon Battista Alberti demonstrated the geometrical method of applying perspective in Florence, using similar triangles as formulated by Euclid, to find the apparent height of distant objects. Brunelleschi's own perspective paintings are lost, but Masaccio's painting of the Holy Trinity shows his principles at work.\nThe Italian painter Paolo Uccello (1397–1475) was fascinated by perspective, as shown in his paintings of \"The Battle of San Romano\" (c. 1435–1460): broken lances lie conveniently along perspective lines.\n\nThe painter Piero della Francesca (c.1415–1492) exemplified this new shift in Italian Renaissance thinking. He was an expert mathematician and geometer, writing books on solid geometry and perspective, including \"De Prospectiva Pingendi (On Perspective for Painting)\", \"Trattato d'Abaco (Abacus Treatise)\", and \"De corporibus regularibus (On Regular Solids)\". The historian Vasari in his \"Lives of the Painters\" calls Piero the \"greatest geometer of his time, or perhaps of any time.\" Piero's interest in perspective can be seen in his paintings including the Polyptych of Perugia, the \"San Agostino altarpiece\" and \"The Flagellation of Christ\". His work on geometry influenced later mathematicians and artists including Luca Pacioli in his \"De Divina Proportione\" and Leonardo da Vinci. Piero studied classical mathematics and the works of Archimedes. He was taught commercial arithmetic in \"abacus schools\"; his writings are formatted like abacus school textbooks, perhaps including Leonardo Pisano (Fibonacci)'s 1202 \"Liber Abaci\". Linear perspective was just being introduced into the artistic world. Alberti explained in his 1435 \"De pictura\": \"light rays travel in straight lines from points in the observed scene to the eye, forming a kind of pyramid with the eye as vertex.\" A painting constructed with linear perspective is a cross-section of that pyramid.\n\nIn \"De Prospectiva Pingendi\", Piero transforms his empirical observations of the way aspects of a figure change with point of view into mathematical proofs. His treatise starts in the vein of Euclid: he defines the point as \"the tiniest thing that is possible for the eye to comprehend\". He uses deductive logic to lead the reader to the perspective representation of a three-dimensional body.\n\nThe artist David Hockney argued in his book \"\" that artists started using a camera lucida from the 1420s, resulting in a sudden change in precision and realism, and that this practice was continued by major artists including Ingres, Van Eyck, and Caravaggio. Critics disagree on whether Hockney was correct. Similarly, the architect Philip Steadman argued controversially that Vermeer had used a different device, the camera obscura, to help him create his distinctively observed paintings.\n\nIn 1509, Luca Pacioli (c. 1447–1517) published \"De divina proportione\" on mathematical and artistic proportion, including in the human face. Leonardo da Vinci (1452–1519) illustrated the text with woodcuts of regular solids while he studied under Pacioli in the 1490s. Leonardo's drawings are probably the first illustrations of skeletonic solids. These, such as the rhombicuboctahedron, were among the first to be drawn to demonstrate perspective by being overlaid on top of each other. The work discusses perspective in the works of Piero della Francesca, Melozzo da Forlì, and Marco Palmezzano. Da Vinci studied Pacioli's \"Summa\", from which he copied tables of proportions. In \"Mona Lisa\" and \"The Last Supper\", Da Vinci's work incorporated linear perspective with a vanishing point to provide apparent depth. \"The Last Supper\" is constructed in a tight ratio of 12:6:4:3, as is Raphael's \"The School of Athens\", which includes Pythagoras with a tablet of ideal ratios, sacred to the Pythagoreans. In \"Vitruvian Man\", Leonardo expressed the ideas of the Roman architect Vitruvius, innovatively showing the male figure twice, and centring him in both a circle and a square.\n\nAs early as the 15th century, curvilinear perspective found its way into paintings by artists interested in image distortions. Jan van Eyck's 1434 \"Arnolfini Portrait\" contains a convex mirror with reflections of the people in the scene, while Parmigianino's \"Self-portrait in a Convex Mirror\", c. 1523–1524, shows the artist's largely undistorted face at the centre, with a strongly curved background and artist's hand around the edge.\n\nThree-dimensional space can be represented convincingly in art, as in technical drawing, by means other than perspective. Oblique projections, including cavalier perspective (used by French military artists to depict fortifications in the 18th century), were used continuously and ubiquitously by Chinese artists from the first or second centuries until the 18th century. The Chinese acquired the technique from India, which acquired it from Ancient Rome. Oblique projection is seen in Japanese art, such as in the Ukiyo-e paintings of Torii Kiyonaga (1752–1815).\n\nThe golden ratio (roughly equal to 1.618) was known to Euclid. The golden ratio has persistently been claimed in modern times to have been used in art and architecture by the ancients in Egypt, Greece and elsewhere, without reliable evidence. The claim may derive from confusion with \"golden mean\", which to the Ancient Greeks meant \"avoidance of excess in either direction\", not a ratio. Pyramidologists since the nineteenth century have argued on dubious mathematical grounds for the golden ratio in pyramid design. The Parthenon, a 5th-century BC temple in Athens, has been claimed to use the golden ratio in its façade and floor plan, but these claims too are disproved by measurement. The Great Mosque of Kairouan in Tunisia has similarly been claimed to use the golden ratio in its design, but the ratio does not appear in the original parts of the mosque. The historian of architecture Frederik Macody Lund argued in 1919 that the Cathedral of Chartres (12th century), Notre-Dame of Laon (1157–1205) and Notre Dame de Paris (1160) are designed according to the golden ratio, drawing regulator lines to make his case. Other scholars argue that until Pacioli's work in 1509, the golden ratio was unknown to artists and architects. For example, the height and width of the front of Notre-Dame of Laon have the ratio 8/5 or 1.6, not 1.618. Such Fibonacci ratios quickly become hard to distinguish from the golden ratio. After Pacioli, the golden ratio is more definitely discernible in artworks including Leonardo's \"Mona Lisa\".\n\nAnother ratio, the only other morphic number, was named the plastic number in 1928 by the Dutch architect Hans van der Laan (originally named \"le nombre radiant\" in French). Its value is the solution of the cubic equation\n\nan irrational number which is approximately 1.325. According to the architect Richard Padovan, this has characteristic ratios and , which govern the limits of human perception in relating one physical size to another. Van der Laan used these ratios when designing the 1967 St. Benedictusberg Abbey church in the Netherlands.\n\nPlanar symmetries have for millennia been exploited in artworks such as carpets, lattices, textiles and tilings.\n\nMany traditional rugs, whether pile carpets or flatweave kilims, are divided into a central field and a framing border; both can have symmetries, though in handwoven carpets these are often slightly broken by small details, variations of pattern and shifts in colour introduced by the weaver. In kilims from Anatolia, the motifs used are themselves usually symmetrical. The general layout, too, is usually present, with arrangements such as stripes, stripes alternating with rows of motifs, and packed arrays of roughly hexagonal motifs. The field is commonly laid out as a wallpaper with a wallpaper group such as pmm, while the border may be laid out as a frieze of frieze group pm11, pmm2 or pma2. Turkish and Central Asian kilims often have three or more borders in different frieze groups. Weavers certainly had the intention of symmetry, without explicit knowledge of its mathematics.\nThe mathematician and architectural theorist Nikos Salingaros suggests that the \"powerful presence\" (aesthetic effect) of a \"great carpet\" such as the best Konya two-medallion carpets of the 17th century is created by mathematical techniques related to the theories of the architect Christopher Alexander. These techniques include making opposites couple; opposing colour values; differentiating areas geometrically, whether by using complementary shapes or balancing the directionality of sharp angles; providing small-scale complexity (from the knot level upwards) and both small- and large-scale symmetry; repeating elements at a hierarchy of different scales (with a ratio of about 2.7 from each level to the next). Salingaros argues that \"all successful carpets satisfy at least nine of the above ten rules\", and suggests that it might be possible to create a metric from these rules.\n\nElaborate lattices are found in Indian Jali work, carved in marble to adorn tombs and palaces. Chinese lattices, always with some symmetry, exist in 14 of the 17 wallpaper groups; they often have mirror, double mirror, or rotational symmetry. Some have a central medallion, and some have a border in a frieze group. Many Chinese lattices have been analysed mathematically by Daniel S. Dye; he identifies Sichuan as the centre of the craft.\n\nSymmetries are prominent in textile arts including quilting, knitting, cross-stitch, crochet, embroidery and weaving, where they may be purely decorative or may be marks of status. Rotational symmetry is found in circular structures such as domes; these are sometimes elaborately decorated with symmetric patterns inside and out, as at the 1619 Sheikh Lotfollah Mosque in Isfahan. Items of embroidery and lace work such as tablecloths and table mats, made using bobbins or by tatting, can have a wide variety of reflectional and rotational symmetries which are being explored mathematically.\n\nIslamic art exploits symmetries in many of its artforms, notably in girih tilings. These are formed using a set of five tile shapes, namely a regular decagon, an elongated hexagon, a bow tie, a rhombus, and a regular pentagon. All the sides of these tiles have the same length; and all their angles are multiples of 36° (π/5 radians), offering fivefold and tenfold symmetries. The tiles are decorated with strapwork lines (girih), generally more visible than the tile boundaries. In 2007, the physicists Peter Lu and Paul Steinhardt argued that girih resembled quasicrystalline Penrose tilings. Elaborate geometric zellige tilework is a distinctive element in Moroccan architecture. Muqarnas vaults are three-dimensional but were designed in two dimensions with drawings of geometrical cells.\n\nThe Platonic solids and other polyhedra are a recurring theme in Western art. They are found, for instance, in a marble mosaic featuring the small stellated dodecahedron, attributed to Paolo Uccello, in the floor of the San Marco Basilica in Venice; in Leonardo da Vinci's diagrams of regular polyhedra drawn as illustrations for Luca Pacioli's 1509 book \"The Divine Proportion\"; as a glass rhombicuboctahedron in Jacopo de Barbari's portrait of Pacioli, painted in 1495; in the truncated polyhedron (and various other mathematical objects) in Albrecht Dürer's engraving Melencolia I; and in Salvador Dalí's painting \"The Last Supper\" in which Christ and his disciples are pictured inside a giant dodecahedron.\n\nAlbrecht Dürer (1471–1528) was a German Renaissance printmaker who made important contributions to polyhedral literature in his 1525 book, \"Underweysung der Messung (Education on Measurement)\", meant to teach the subjects of linear perspective, geometry in architecture, Platonic solids, and regular polygons. Dürer was likely influenced by the works of Luca Pacioli and Piero della Francesca during his trips to Italy. While the examples of perspective in \"Underweysung der Messung\" are underdeveloped and contain inaccuracies, there is a detailed discussion of polyhedra. Dürer is also the first to introduce in text the idea of polyhedral nets, polyhedra unfolded to lie flat for printing. Dürer published another influential book on human proportions called \"Vier Bücher von Menschlicher Proportion (Four Books on Human Proportion)\" in 1528.\n\nDürer's well-known engraving \"Melencolia I\" depicts a frustrated thinker sitting by a truncated triangular trapezohedron and a magic square. These two objects, and the engraving as a whole, have been the subject of more modern interpretation than the contents of almost any other print, including a two-volume book by Peter-Klaus Schuster, and an influential discussion in Erwin Panofsky's monograph of Dürer.\nSalvador Dalí's \"Corpus Hypercubus\" depicts an unfolded three-dimensional net for a hypercube, a four-dimensional regular polyhedron.\n\nTraditional Indonesian wax-resist batik designs on cloth combine representational motifs (such as floral and vegetal elements) with abstract and somewhat chaotic elements, including imprecision in applying the wax resist, and random variation introduced by cracking of the wax. Batik designs have a fractal dimension between 1 and 2, varying in different regional styles. For example, the batik of Cirebon has a fractal dimension of 1.1; the batiks of Yogyakarta and Surakarta (Solo) in Central Java have a fractal dimension of 1.2 to 1.5; and the batiks of Lasem on the north coast of Java and of Tasikmalaya in West Java have a fractal dimension between 1.5 and 1.7.\n\nThe drip painting works of the modern artist Jackson Pollock are similarly distinctive in their fractal dimension. His 1948 \"Number 14\" has a coastline-like dimension of 1.45, while his later paintings had successively higher fractal dimensions and accordingly more elaborate patterns. One of his last works, \"Blue Poles\", took six months to create, and has the fractal dimension of 1.72.\n\nThe astronomer Galileo Galilei in his \"Il Saggiatore\" wrote that \"[The universe] is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures.\" Artists who strive and seek to study nature must first, in Galileo's view, fully understand mathematics. Mathematicians, conversely, have sought to interpret and analyse art through the lens of geometry and rationality. The mathematician Felipe Cucker suggests that mathematics, and especially geometry, is a source of rules for \"rule-driven artistic creation\", though not the only one. Some of the many strands of the resulting complex relationship are described below.\n\nThe mathematician Jerry P. King describes mathematics as an art, stating that \"the keys to mathematics are beauty and elegance and not dullness and technicality\", and that beauty is the motivating force for mathematical research. King cites the mathematician G. H. Hardy's 1940 essay \"A Mathematician's Apology\". In it, Hardy discusses why he finds two theorems of classical times as first rate, namely Euclid's proof there are infinitely many prime numbers, and the proof that the square root of 2 is irrational. King evaluates this last against Hardy's criteria for mathematical elegance: \"\"seriousness, depth, generality, unexpectedness, inevitability\", and \"economy\"\" (King's italics), and describes the proof as \"aesthetically pleasing\". The Hungarian mathematician Paul Erdős agreed that mathematics possessed beauty but considered the reasons beyond explanation: \"Why are numbers beautiful? It's like asking why is Beethoven's Ninth Symphony beautiful. If you don't see why, someone can't tell you. I \"know\" numbers are beautiful.\"\n\nMathematics can be discerned in many of the arts, such as music, dance, painting, architecture, and sculpture. Each of these is richly associated with mathematics. Among the connections to the visual arts, mathematics can provide tools for artists, such as the rules of linear perspective as described by Brook Taylor and Johann Lambert, or the methods of descriptive geometry, now applied in software modelling of solids, dating back to Albrecht Dürer and Gaspard Monge. Artists from Luca Pacioli in the Middle Ages and Leonardo da Vinci and Albrecht Dürer in the Renaissance have made use of and developed mathematical ideas in the pursuit of their artistic work. The use of perspective began, despite some embryonic usages in the architecture of Ancient Greece, with Italian painters such as Giotto in the 13th century; rules such as the vanishing point were first formulated by Brunelleschi in about 1413, his theory influencing Leonardo and Dürer. Isaac Newton's work on the optical spectrum influenced Goethe's \"Theory of Colours\" and in turn artists such as Philipp Otto Runge, J. M. W. Turner, the Pre-Raphaelites and Wassily Kandinsky. Artists may also choose to analyse the symmetry of a scene. Tools may be applied by mathematicians who are exploring art, or artists inspired by mathematics, such as M. C. Escher (inspired by H. S. M. Coxeter) and the architect Frank Gehry, who more tenuously argued that computer aided design enabled him to express himself in a wholly new way.\n\nThe artist Richard Wright argues that mathematical objects that can be constructed can be seen either \"as processes to simulate phenomena\" or as works of \"computer art\". He considers the nature of mathematical thought, observing that fractals were known to mathematicians for a century before they were recognised as such. Wright concludes by stating that it is appropriate to subject mathematical objects to any methods used to \"come to terms with cultural artifacts like art, the tension between objectivity and subjectivity, their metaphorical meanings and the character of representational systems.\" He gives as instances an image from the Mandelbrot set, an image generated by a cellular automaton algorithm, and a computer-rendered image, and discusses, with reference to the Turing test, whether algorithmic products can be art. Sasho Kalajdzievski's \"Math and Art: An Introduction to Visual Mathematics\" takes a similar approach, looking at suitably visual mathematics topics such as tilings, fractals and hyperbolic geometry.\n\nSome of the first works of computer art were created by Desmond Paul Henry's \"Drawing Machine 1\", an analogue machine based on a bombsight computer and exhibited in 1962. The machine was capable of creating complex, abstract, asymmetrical, curvilinear, but repetitive line drawings. More recently, Hamid Naderi Yeganeh has created shapes suggestive of real world objects such as fish and birds, using formulae that are successively varied to draw families of curves or angled lines. Artists such as Mikael Hvidtfeldt Christensen create works of generative or algorithmic art by writing scripts for a software system such as \"Structure Synth\": the artist effectively directs the system to apply a desired combination of mathematical operations to a chosen set of data.\n\nThe mathematician and theoretical physicist Henri Poincaré's \"Science and Hypothesis\" was widely read by the Cubists, including Pablo Picasso and Jean Metzinger. Poincaré viewed Euclidean geometry as just one of many possible geometric configurations, rather than as an absolute objective truth. The possible existence of a fourth dimension inspired artists to question classical : non-Euclidean geometry became a valid alternative. The concept that painting could be expressed mathematically, in colour and form, contributed to Cubism, the art movement that led to abstract art. Metzinger, in 1910, wrote that: \"[Picasso] lays out a free, mobile perspective, from which that ingenious mathematician Maurice Princet has deduced a whole geometry\". Later, Metzinger wrote in his memoirs:\n\nMaurice Princet joined us often ... it was as an artist that he conceptualized mathematics, as an aesthetician that he invoked \"n\"-dimensional continuums. He loved to get the artists interested in the new views on space that had been opened up by Schlegel and some others. He succeeded at that.\n\nThe impulse to make teaching or research models of mathematical forms naturally creates objects that have symmetries and surprising or pleasing shapes. Some of these have inspired artists such as the Dadaists Man Ray, Marcel Duchamp and Max Ernst, and following Man Ray, Hiroshi Sugimoto.\nMan Ray photographed some of the mathematical models in the Institut Henri Poincaré in Paris, including \"Objet mathematique\" (Mathematical object). He noted that this represented Enneper surfaces with constant negative curvature, derived from the pseudo-sphere. This mathematical foundation was important to him, as it allowed him to deny that the object was \"abstract\", instead claiming that it was as real as the urinal that Duchamp made into a work of art. Man Ray admitted that the object's [Enneper surface] formula \"meant nothing to me, but the forms themselves were as varied and authentic as any in nature.\" He used his photographs of the mathematical models as figures in his series he did on Shakespeare's plays, such as his 1934 painting \"Antony and Cleopatra\". The art reporter Jonathan Keats, writing in \"ForbesLife\", argues that Man Ray photographed \"the elliptic paraboloids and conic points in the same sensual light as his pictures of Kiki de Montparnasse\", and \"ingeniously repurposes the cool calculations of mathematics to reveal the topology of desire\". Twentieth century sculptors such as Henry Moore, Barbara Hepworth and Naum Gabo took inspiration from mathematical models. Moore wrote of his 1938 \"Stringed Mother and Child\": \"Undoubtedly the source of my stringed figures was the Science Museum ... I was fascinated by the mathematical models I saw there ... It wasn't the scientific study of these models but the ability to look through the strings as with a bird cage and to see one form within another which excited me.\"\n\nThe artists Theo van Doesburg and Piet Mondrian founded the De Stijl movement, which they wanted to \"establish a visual vocabulary elementary geometrical forms comprehensible by all and adaptable to any discipline\". Many of their artworks visibly consist of ruled squares and triangles, sometimes also with circles. De Stijl artists worked in painting, furniture, interior design and architecture. After the breakup of De Stijl, Van Doesburg founded the Avant-garde Art Concret movement, describing his 1929–1930 \"Arithmetic Composition\", a series of four black squares on the diagonal of a squared background, as \"a structure that can be controlled, a \"definite\" surface without chance elements or individual caprice\", yet \"not lacking in spirit, not lacking the universal and not ... empty as there is \"everything\" which fits the internal rhythm\". The art critic Gladys Fabre observes that two progressions are at work in the painting, namely the growing black squares and the alternating backgrounds.\n\nThe mathematics of tessellation, polyhedra, shaping of space, and self-reference provided the graphic artist M. C. Escher (1898—1972) with a lifetime's worth of materials for his woodcuts. In the \"Alhambra Sketch\", Escher showed that art can be created with polygons or regular shapes such as triangles, squares, and hexagons. Escher used irregular polygons when tiling the plane and often used reflections, glide reflections, and translations to obtain further patterns. Many of his works contain impossible constructions, made using geometrical objects which set up a contradiction between perspective projection and three dimensions, but are pleasant to the human sight. Escher's \"Ascending and Descending\" is based on the \"impossible staircase\" created by the medical scientist Lionel Penrose and his son the mathematician Roger Penrose.\n\nSome of Escher's many tessellation drawings were inspired by conversations with the mathematician H. S. M. Coxeter on hyperbolic geometry. Escher was especially interested in five specific polyhedra, which appear many times in his work. The Platonic solids—tetrahedrons, cubes, octahedrons, dodecahedrons, and icosahedrons—are especially prominent in \"Order and Chaos\" and \"Four Regular Solids\". These stellated figures often reside within another figure which further distorts the viewing angle and conformation of the polyhedrons and provides a multifaceted perspective artwork.\n\nThe visual intricacy of mathematical structures such as tessellations and polyhedra have inspired a variety of mathematical artworks. Stewart Coffin makes polyhedral puzzles in rare and beautiful woods; George W. Hart works on the theory of polyhedra and sculpts objects inspired by them; Magnus Wenninger makes \"especially beautiful\" models of complex stellated polyhedra.\n\nThe distorted perspectives of anamorphosis have been explored in art since the sixteenth century, when Hans Holbein the Younger incorporated a severely distorted skull in his 1533 painting \"The Ambassadors\". Many artists since then, including Escher, have make use of anamorphic tricks.\n\nThe mathematics of topology has inspired several artists in modern times. The sculptor John Robinson (1935–2007) created works such as \"Gordian Knot\" and \"Bands of Friendship\", displaying knot theory in polished bronze. Other works by Robinson explore the topology of toruses. \"Genesis\" is based on Borromean rings – a set of three circles, no two of which link but in which the whole structure cannot be taken apart without breaking. The sculptor Helaman Ferguson creates complex surfaces and other topological objects. His works are visual representations of mathematical objects; \"The Eightfold Way\" is based on the projective special linear group PSL(2,7), a finite group of 168 elements. The sculptor Bathsheba Grossman similarly bases her work on mathematical structures.\n\nA liberal arts inquiry project examines connections between mathematics and art through the Möbius strip, flexagons, origami and panorama photography.\n\nMathematical objects including the Lorenz manifold and the hyperbolic plane have been crafted using fiber arts including crochet. The American weaver Ada Dietz wrote a 1949 monograph \"Algebraic Expressions in Handwoven Textiles\", defining weaving patterns based on the expansion of multivariate polynomials. The mathematician J. C. P. Miller used the Rule 90 cellular automaton to design tapestries depicting both trees and abstract patterns of triangles. The \"mathekniticians\" Pat Ashforth and Steve Plummer use knitted versions of mathematical objects such as hexaflexagons in their teaching, though their Menger sponge proved too troublesome to knit and was made of plastic canvas instead. Their \"mathghans\" (Afghans for Schools) project introduced knitting into the British mathematics and technology curriculum.\n\nModelling is far from the only possible way to illustrate mathematical concepts. Giotto's \"Stefaneschi Triptych\", 1320, illustrates recursion in the form of \"mise en abyme\"; the central panel of the triptych contains, lower left, the kneeling figure of Cardinal Stefaneschi, holding up the triptych as an offering. Giorgio Chirico's metaphysical paintings such as his 1917 \"Great Metaphysical Interior\" explore the question of levels of representation in art by depicting paintings within his paintings.\n\nArt can exemplify logical paradoxes, as in some paintings by the surrealist René Magritte, which can be read as semiotic jokes about confusion between levels. In \"La condition humaine\" (1933), Magritte depicts an easel (on the real canvas), seamlessly supporting a view through a window which is framed by \"real\" curtains in the painting. Similarly, Escher's \"Print Gallery\" (1956) is a print which depicts a distorted city which contains a gallery which recursively contains the picture, and so \"ad infinitum\". Magritte made use of spheres and cuboids to distort reality in a different way, painting them alongside an assortment of houses in his 1931 \"Mental Arithmetic\" as if they were children's building blocks, but house-sized. \"The Guardian\" observed that the \"eerie toytown image\" prophesied Modernism's usurpation of \"cosy traditional forms\", but also plays with the human tendency to seek patterns in nature.\n\nSalvador Dalí's last painting, \"The Swallow's Tail\" (1983), was part of a series inspired by René Thom's catastrophe theory. The Spanish painter and sculptor Pablo Palazuelo (1916–2007) focused on the investigation of form. He developed a style that he described as the geometry of life and the geometry of all nature. Consisting of simple geometric shapes with detailed patterning and coloring, in works such as \"Angular I\" and \"Automnes\", Palazuelo expressed himself in geometric transformations.\n\nThe artist Adrian Gray practises stone balancing, exploiting friction and the centre of gravity to create striking and seemingly impossible compositions.\n\nArtists, however, do not necessarily take geometry literally. As Douglas Hofstadter writes in his 1980 reflection on human thought, \"Gödel, Escher, Bach\", by way of (among other things) the mathematics of art: \"The difference between an Escher drawing and non-Euclidean geometry is that in the latter, comprehensible interpretations can be found for the undefined terms, resulting in a comprehensible total system, whereas for the former, the end result is not reconcilable with one's conception of the world, no matter how long one stares at the pictures.\" Hofstadter discusses the seemingly paradoxical lithograph \"Print Gallery\" by M. C. Escher; it depicts a seaside town containing an art gallery which seems to contain a painting of the seaside town, there being a \"strange loop, or tangled hierarchy\" to the levels of reality in the image. The artist himself, Hofstadter observes, is not seen; his reality and his relation to the lithograph are not paradoxical. The image's central void has also attracted the interest of mathematicians Bart de Smit and Hendrik Lenstra, who propose that it could contain a Droste effect copy of itself, rotated and shrunk; this would be a further illustration of recursion beyond that noted by Hofstadter.\n\nAlgorithmic analysis of images of artworks, for example using X-ray fluorescence spectroscopy, can reveal information about art. Such techniques can uncover images in layers of paint later covered over by an artist; help art historians to visualize an artwork before it cracked or faded; help to tell a copy from an original, or distinguish the brushstroke style of a master from those of his apprentices.\nJackson Pollock's drip painting style has a definite fractal dimension; among the artists who may have influenced Pollock's controlled chaos, Max Ernst painted Lissajous figures directly by swinging a punctured bucket of paint over a canvas.\n\nThe computer scientist Neil Dodgson investigated whether Bridget Riley's stripe paintings could be characterised mathematically, concluding that while separation distance could \"provide some characterisation\" and global entropy worked on some paintings, autocorrelation failed as Riley's patterns were irregular. Local entropy worked best, and correlated well with the description given by the art critic Robert Kudielka.\n\nThe American mathematician George Birkhoff's 1933 \"Aesthetic Measure\" proposes a quantitative metric of the aesthetic quality of an artwork. It does not attempt to measure the connotations of a work, such as what a painting might mean, but is limited to the \"elements of order\" of a polygonal figure. Birkhoff first combines (as a sum) five such elements: whether there is a vertical axis of symmetry; whether there is optical equilibrium; how many rotational symmetries it has; how wallpaper-like the figure is; and whether there are unsatisfactory features such as having two vertices too close together. This metric, \"O\", takes a value between −3 and 7. The second metric, \"C\", counts elements of the figure, which for a polygon is the number of different straight lines containing at least one of its sides. Birkhoff then defines his aesthetic measure of an object's beauty as \"O/C\". This can be interpreted as a balance between the pleasure looking at the object gives, and the amount of effort needed to take it in. Birkhoff's proposal has been criticized in various ways, not least for trying to put beauty in a formula, but he never claimed to have done that.\n\nArt has sometimes stimulated the development of mathematics, as when Brunelleschi's theory of perspective in architecture and painting started a cycle of research that led to the work of Brook Taylor and Johann Heinrich Lambert on the mathematical foundations of perspective drawing, and ultimately to the mathematics of projective geometry of Girard Desargues and Jean-Victor Poncelet.\n\nThe Japanese paper-folding art of origami has been reworked mathematically by Tomoko Fusé using modules, congruent pieces of paper such as squares, and making them into polyhedra or tilings. Paper-folding was used in 1893 by T. Sundara Rao in his \"Geometric Exercises in Paper Folding\" to demonstrate geometrical proofs. The mathematics of paper folding has been explored in Maekawa's theorem, Kawasaki's theorem, and the Huzita–Hatori axioms.\n\nOptical illusions such as the Fraser spiral strikingly demonstrate limitations in human visual perception, creating what the art historian Ernst Gombrich called a \"baffling trick.\" The black and white ropes that appear to form spirals are in fact concentric circles. The mid-twentieth century Op art or optical art style of painting and graphics exploited such effects to create the impression of movement and flashing or vibrating patterns seen in the work of artists such as Bridget Riley, Spyros Horemis, and Victor Vasarely.\n\nA strand of art from Ancient Greece onwards sees God as the geometer of the world, and the world's geometry therefore as sacred. The belief that God created the universe according to a geometric plan has ancient origins. Plutarch attributed the belief to Plato, writing that \"Plato said God geometrizes continually\" (\"Convivialium disputationum\", liber 8,2). This image has influenced Western thought ever since. The Platonic concept derived in its turn from a Pythagorean notion of harmony in music, where the notes were spaced in perfect proportions, corresponding to the lengths of the lyre's strings; indeed, the Pythagoreans held that everything was arranged by Number. In the same way, in Platonic thought, the regular or Platonic solids dictate the proportions found in nature, and in art. A Mediaeval manuscript illustration may refer to a verse in the Old Testament: \"When he established the heavens I was there: when he set a compass upon the face of the deep\" (Proverbs 8:27), showing God drawing out the universe with a pair of compasses. In 1596, the mathematical astronomer Johannes Kepler modelled the universe as a set of nested Platonic solids, determining the relative sizes of the orbits of the planets. William Blake's \"Ancient of Days\" and his painting of the physicist Isaac Newton, naked and drawing with a compass, attempt to depict the contrast between the mathematically perfect spiritual world and the imperfect physical world, as in a different way does Salvador Dalí's 1954 \"Crucifixion (Corpus Hypercubus)\", which depicts the cross as a hypercube, representing the divine perspective with four dimensions rather than the usual three. In Dali's \"The Sacrament of the Last Supper\" (1955) Christ and his disciples are pictured inside a giant dodecahedron.\n\n\n"}
{"id": "13410380", "url": "https://en.wikipedia.org/wiki?curid=13410380", "title": "Mathematics and fiber arts", "text": "Mathematics and fiber arts\n\nIdeas from Mathematics have been used as inspiration for fiber arts including quilt making, knitting, cross-stitch, crochet, embroidery and weaving. A wide range of mathematical concepts have been used as inspiration including topology, graph theory, number theory and algebra. Some techniques such as counted-thread embroidery are naturally geometrical; other kinds of textile provide a ready means for the colorful physical expression of mathematical concepts.\n\nThe IEEE Spectrum has organized a number of competitions on quilt block design, and several books have been published on the subject. Notable quiltmakers include Diana Venters and Elaine Ellison, who have written a book on the subject \"Mathematical Quilts: No Sewing Required\". Examples of mathematical ideas used in the book as the basis of a quilt include the golden rectangle, conic sections, Leonardo da Vinci's Claw, the Koch curve, the Clifford torus, San Gaku, Mascheroni's cardioid, Pythagorean triples, spidrons, and the six trigonometric functions.\n\nKnitted mathematical objects include the Platonic solids, Klein bottles and Boy's surface.\nThe Lorenz manifold and the hyperbolic plane have been crafted using crochet. Knitted and crocheted tori have also been constructed depicting toroidal embeddings of the complete graph \"K\" and of the Heawood graph. The crocheting of hyperbolic planes has been popularized by the Institute For Figuring; a book by Daina Taimina on the subject, \"Crocheting Adventures with Hyperbolic Planes\", won the 2009 Bookseller/Diagram Prize for Oddest Title of the Year.\n\nEmbroidery techniques such as counted-thread embroidery including cross-stitch and some canvas work methods such as Bargello (needlework) make use of the natural pixels of the weave, lending themselves to geometric designs.\n\nAda Dietz (1882 – 1950) was an American weaver best known for her 1949 monograph \"Algebraic Expressions in Handwoven Textiles\", which defines weaving patterns based on the expansion of multivariate polynomials.\n\nMargaret Greig was a mathematician who articulated the mathematics of worsted spinning.\n\nThe silk scarves from DMCK Designs' 2013 collection are all based on Douglas McKenna's space-filling curve patterns. The designs are either generalized Peano curves, or based on a new space-filling construction technique.\n\nThe Issey Miyake Fall-Winter 2010–2011 ready-to-wear collection featured designs from a collaboration between fashion designer Dai Fujiwara and mathematician William Thurston. The designs were inspired by Thurston's geometrization conjecture, the statement that every 3-manifold can be decomposed into pieces with one of eight different uniform geometries, a proof of which had been sketched in 2003 by Grigori Perelman as part of his proof of the Poincaré conjecture.\n\n\n"}
{"id": "326471", "url": "https://en.wikipedia.org/wiki?curid=326471", "title": "Mathematics education", "text": "Mathematics education\n\nIn contemporary education, mathematics education is the practice of teaching and learning mathematics, along with the associated scholarly research.\n\nResearchers in mathematics education are primarily concerned with the tools, methods and approaches that facilitate practice or the study of practice; however, mathematics education research, known on the continent of Europe as the didactics or pedagogy of mathematics, has developed into an extensive field of study, with its own concepts, theories, methods, national and international organisations, conferences and literature. This article describes some of the history, influences and recent controversies.\n\nElementary mathematics was part of the education system in most ancient civilisations, including Ancient Greece, the Roman Empire, Vedic society and ancient Egypt. In most cases, a formal education was only available to male children with a sufficiently high status, wealth or caste.\nIn Plato's division of the liberal arts into the trivium and the quadrivium, the quadrivium included the mathematical fields of arithmetic and geometry. This structure was continued in the structure of classical education that was developed in medieval Europe. Teaching of geometry was almost universally based on Euclid's \"Elements\". Apprentices to trades such as masons, merchants and money-lenders could expect to learn such practical mathematics as was relevant to their profession.\n\nIn the Renaissance, the academic status of mathematics declined, because it was strongly associated with trade and commerce, and considered somewhat un-Christian. Although it continued to be taught in European universities, it was seen as subservient to the study of Natural, Metaphysical and Moral Philosophy. The first modern arithmetic curriculum (starting with addition, then subtraction, multiplication, and division) arose at reckoning schools in Italy in the 1300s. Spreading along trade routes, these methods were designed to be used in commerce. They contrasted with Platonic math taught at universities, which was more philosophical and concerned numbers as concepts rather than calculating methods. They also contrasted with mathematical methods learned by artisan apprentices, which were specific to the tasks and tools at hand. For example, the division of a board into thirds can be accomplished with a piece of string, instead of measuring the length and using the arithmetic operation of division.\n\nThe first mathematics textbooks to be written in English and French were published by Robert Recorde, beginning with \"The Grounde of Artes\" in 1540. However, there are many different writings on mathematics and mathematics methodology that date back to 1800 BCE. These were mostly located in Mesopotamia where the Sumerians were practicing multiplication and division. There are also artifacts demonstrating their own methodology for solving equations like the quadratic equation. After the Sumerians some of the most famous ancient works on mathematics come from Egypt in the form of the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus. The more famous Rhind Papyrus has been dated to approximately 1650 BCE but it is thought to be a copy of an even older scroll. This papyrus was essentially an early textbook for Egyptian students.\n\nThe social status of mathematical study was improving by the seventeenth century, with the University of Aberdeen creating a Mathematics Chair in 1613, followed by the Chair in Geometry being set up in University of Oxford in 1619 and the Lucasian Chair of Mathematics being established by the University of Cambridge in 1662. However, it was uncommon for mathematics to be taught outside of the universities. Isaac Newton, for example, received no formal mathematics teaching until he joined Trinity College, Cambridge in 1661.\n\nIn the 18th and 19th centuries, the Industrial Revolution led to an enormous increase in urban populations. Basic numeracy skills, such as the ability to tell the time, count money and carry out simple arithmetic, became essential in this new urban lifestyle. Within the new public education systems, mathematics became a central part of the curriculum from an early age.\n\nBy the twentieth century, mathematics was part of the core curriculum in all developed countries.\n\nDuring the twentieth century, mathematics education was established as an independent field of research. Here are some of the main events in this development:\n\n\nIn the 20th century, the cultural impact of the \"electronic age\" (McLuhan) was also taken up by educational theory and the teaching of mathematics. While previous approach focused on \"working with specialized 'problems' in arithmetic\", the emerging structural approach to knowledge had \"small children meditating about number theory and 'sets'.\"\n\nAt different times and in different cultures and countries, mathematics education has attempted to achieve a variety of different objectives. These objectives have included:\n\n\nThe method or methods used in any particular context are largely determined by the objectives that the relevant educational system is trying to achieve. Methods of teaching mathematics include the following:\n\n\n\nDifferent levels of mathematics are taught at different ages and in somewhat different sequences in different countries. Sometimes a class may be taught at an earlier age than typical as a special or honors class.\n\nElementary mathematics in most countries is taught in a similar fashion, though there are differences. In the United States fractions are typically taught starting from 1st grade, whereas in other countries they are usually taught later, since the metric system does not require young children to be familiar with them. Most countries tend to cover fewer topics in greater depth than in the United States. K-12 topics include elementary arithmetic (addition, subtraction, multiplication, and division), and pre-algebra.\n\nIn most of the U.S., algebra, geometry and analysis (pre-calculus and calculus) are taught as separate courses in different years of high school. Mathematics in most other countries (and in a few U.S. states) is integrated, with topics from all branches of mathematics studied every year. Students in many countries choose an option or pre-defined course of study rather than choosing courses \"à la carte\" as in the United States. Students in science-oriented curricula typically study differential calculus and trigonometry at age 16–17 and integral calculus, complex numbers, analytic geometry, exponential and logarithmic functions, and infinite series in their final year of secondary school. Probability and statistics may be taught in secondary education classes.\n\nScience and engineering students in colleges and universities may be required to take multivariable calculus, differential equations, linear algebra. Applied mathematics is also used in specific majors; for example, civil engineers may be required to study fluid mechanics, while \"math for computer science\" might include graph theory, permutation, probability, and proofs. Mathematics students would continue to study potentially any area.\n\nThroughout most of history, standards for mathematics education were set locally, by individual schools or teachers, depending on the levels of achievement that were relevant to, realistic for, and considered socially appropriate for their pupils.\n\nIn modern times, there has been a move towards regional or national standards, usually under the umbrella of a wider standard school curriculum. In England, for example, standards for mathematics education are set as part of the National Curriculum for England, while Scotland maintains its own educational system. In the USA, the National Governors Association Center for Best Practices and the Council of Chief State School Officers have published the national mathematics Common Core State Standards Initiative.\n\nMa (2000) summarised the research of others who found, based on nationwide data, that students with higher scores on standardised mathematics tests had taken more mathematics courses in high school. This led some states to require three years of mathematics instead of two. But because this requirement was often met by taking another lower level mathematics course, the additional courses had a “diluted” effect in raising achievement levels.\n\nIn North America, the National Council of Teachers of Mathematics has published the Principles and Standards for School Mathematics, which boosted the trend towards reform mathematics. In 2006, they released \"Curriculum Focal Points\", which recommend the most important mathematical topics for each grade level through grade 8. However, these standards are enforced as American states and Canadian provinces choose. A US state's adoption of the Common Core State Standards in mathematics is at the discretion of the state, and is not mandated by the Federal Government. \"States routinely review their academic standards and may choose to change or add onto the standards to best meet the needs of their students.\" The National Council of Teachers of Mathematics has state affiliates that have different education standards at the state level. For example, Missouri has the Missouri Council of Teachers of Mathematics (MCTM) which has its own pillars and standards of education listed on its website. The MCTM also offers membership opportunities to teachers and future teachers so they can stay up to date on the changes in math educational standards. \n\n\"Robust, useful theories of classroom teaching do not yet exist\". However, there are useful theories on how children learn mathematics and much research has been conducted in recent decades to explore how these theories can be applied to teaching. The following results are examples of some of the current findings in the field of mathematics education:\n\n\n\n\n\n\n\nAs with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods in order to test their effects. They depend on large samples to obtain statistically significant results.\n\nQualitative research, such as case studies, action research, discourse analysis, and clinical interviews, depend on small but focused samples in an attempt to understand student learning and to look at how and why a given method gives the results it does. Such studies cannot conclusively establish that one method is better than another, as randomized trials can, but unless it is understood \"why\" treatment X is better than treatment Y, application of results of quantitative studies will often lead to \"lethal mutations\" of the finding in actual classrooms. Exploratory qualitative research is also useful for suggesting new hypotheses, which can eventually be tested by randomized experiments. Both qualitative and quantitative studies therefore are considered essential in education—just as in the other social sciences. Many studies are “mixed”, simultaneously combining aspects of both quantitative and qualitative research, as appropriate.\n\nThere has been some controversy over the relative strengths of different types of research. Because randomized trials provide clear, objective evidence on “what works”, policy makers often take only those studies into consideration. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes. In other disciplines concerned with human subjects, like biomedicine, psychology, and policy evaluation, controlled, randomized experiments remain the preferred method of evaluating treatments. Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods. On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective, or the difficulty of assuring rigid control of the independent variable in fluid, real school settings.\n\nIn the United States, the National Mathematics Advisory Panel (NMAP) published a report in 2008 based on studies, some of which used randomized assignment of treatments to experimental units, such as classrooms or students. The NMAP report's preference for randomized experiments received criticism from some scholars. In 2010, the What Works Clearinghouse (essentially the research arm for the Department of Education) responded to ongoing controversy by extending its research base to include non-experimental studies, including regression discontinuity designs and single-case studies.\n\nThe following are some of the people who have had a significant influence on the teaching of mathematics at various periods in history:\n\n\nThe following people all taught mathematics at some stage in their lives, although they are better known for other things:\n\n\n\n\n\n\n\n"}
{"id": "9518854", "url": "https://en.wikipedia.org/wiki?curid=9518854", "title": "Microscopic traffic flow model", "text": "Microscopic traffic flow model\n\nMicroscopic traffic flow models are a class of scientific models of vehicular traffic dynamics.\n\nIn contrast to macroscopic models, microscopic traffic flow models simulate single vehicle-driver units, so the dynamic variables of the models represent microscopic properties like the position and velocity of single vehicles.\n\nAlso known as \"time-continuous models\", all car-following models have in common that they are defined by ordinary differential equations describing the complete dynamics of the vehicles' positions formula_1 and velocities formula_2. It is assumed that the input stimuli of the drivers are restricted to their own velocity formula_2, the net distance (bumper-to-bumper distance) formula_4 to the leading vehicle formula_5 (where formula_6 denotes the vehicle length), and the velocity formula_7 of the leading vehicle. The equation of motion of each vehicle is characterized by an acceleration function that depends on those input stimuli:\n\nIn general, the driving behavior of a single driver-vehicle unit formula_9 might not merely depend on the immediate leader formula_5 but on the formula_11 vehicles in front. The equation of motion in this more generalized form reads:\n\n\nCellular automaton (CA) models use integer variables to describe the dynamical properties of the system. The road is divided into sections of a certain length formula_13 and the time is discretized to steps of formula_14. Each road section can either be occupied by a vehicle or empty and the dynamics are given by update rules of the form:\n\n(the simulation time formula_17 is measured in units of formula_14 and the vehicle positions formula_1 in units of formula_13).\n\nThe time scale is typically given by the reaction time of a human driver, formula_21. With formula_14 fixed, the length of the road sections determines the granularity of the model. At a complete standstill, the average road length occupied by one vehicle is approximately 7.5 meters. Setting formula_13 to this value leads to a model where one vehicle always occupies exactly one section of the road and a velocity of 5 corresponds to formula_24, which is then set to be the maximum velocity a driver wants to drive at. However, in such a model, the smallest possible acceleration would be formula_25 which is unrealistic. Therefore, many modern CA models use a finer spatial discretization, for example formula_26, leading to a smallest possible acceleration of formula_27.\n\nAlthough cellular automaton models lack the accuracy of the time-continuous car-following models, they still have the ability to reproduce a wide range of traffic phenomena. Due to the simplicity of the models, they are numerically very efficient and can be used to simulate large road networks in realtime or even faster.\n\n\n"}
{"id": "54225729", "url": "https://en.wikipedia.org/wiki?curid=54225729", "title": "Moschovakis coding lemma", "text": "Moschovakis coding lemma\n\nThe Moschovakis coding lemma is a lemma from descriptive set theory involving sets of real numbers in the axiom of determinacy (the principle that every two-player integer game is determined). The lemma was developed and named after the mathematician, Yiannis N. Moschovakis.\n\nThe lemma may be expressed generally as follows:\n\nLet Γ be a non-selfdual point-class closed under ∃ ω ω and ∧ , and ≺ a Γ well-founded relation on ω ω of rank θ ∈ ON. Let R ⊆ dom( ≺ ) × ω ω be such that ∀ x ∈ dom( ≺ ) ∃ y R ( x,y ) . Then there is a Γ set A ⊆ dom( ≺ ) × ω ω which is a choice set for R , that is: 1. ∀ α < θ ∃ x ∈ dom( ≺ ) ∃ y [ | x | ≺ = α ∧ A ( x,y )] . 2. ∀ x ∀ y A ( x,y ) → [ x ∈ dom( ≺ ) ∧ R ( x,y )] . Proof. We may assume θ is minimal so that the theorem fails, and fix ≺ , R , and a good universal set U ⊆ ( ω ω ) 3 for the Γ subsets of ( ω ω ) 2 . Easily, θ is a limit ordinal. For δ < θ , say u ∈ ω ω codes a δ -choice set provided (1) holds for α ≤ δ using A = U u , and (2) holds for A = U u where we replace x ∈ dom( ≺ ) with x ∈ dom( ≺ ) ∧| x | ≺ ≤ δ . By minimality of θ , for all δ < θ there are δ -choice sets. Play the game where I, II play out u,v ∈ ω ω , and II wins provided that if u codes a δ 1 -choice set for some δ 1 < θ , then v codes a δ 2 -choice set for some δ 2 > δ 1 . If I has a winning strategy, we get a Σ 1 1 set B of reals coding δ -choice sets for arbitrarily large δ < θ . Define then A ( x,y ) ↔∃ w ∈ B U ( w,x,y ), which easily works. Suppose now τ is a winning strategy for II. From the s - m - n theorem, let s : ( ω ω ) 2 → ω ω be continuous such that for all ,x,t,w, U ( s (,x ) ,t,w ) ↔ ∃ y ∃ z [ y ≺ x ∧ U (,y,z ) ∧ U ( z,t,w )]. By the recursion theorem, let 0 be such that U (0 ,x,z ) ↔ z = τ ( s (0 ,x )). A straightforward induction on | x | ≺ for x ∈ dom( ≺ ) shows that ∀ x ∈ dom( ≺ ) ∃ ! z U ( 0 ,x,z ), and ∀ x ∈ dom( ≺ ) ∀ z [ U ( 0 ,x,z ) → z codes a ≥| x | ≺ -choice set]. Let A ( x,y ) ↔∃ z ∈ dom( ≺ ) ∃ w [ U ( 0 ,z,w ) ∧ U ( w,x,y )]., \n\n"}
{"id": "5643937", "url": "https://en.wikipedia.org/wiki?curid=5643937", "title": "Music and mathematics", "text": "Music and mathematics\n\nMusic theory has no axiomatic foundation in modern mathematics, yet the basis of musical sound can be described mathematically (in acoustics) and exhibits \"a remarkable array of number properties\". Elements of music such as its form, rhythm and metre, the pitches of its notes and the tempo of its pulse can be related to the measurement of time and frequency, offering ready analogies in geometry. \n\nThe attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory. Some composers have incorporated the golden ratio and Fibonacci numbers into their work.\n\nThough ancient Chinese, Indians, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound, the Pythagoreans (in particular Philolaus and Archytas) of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios, particularly the ratios of small integers. Their central doctrine was that \"all nature consists of harmony arising out of numbers\".\n\nFrom the time of Plato, harmony was considered a fundamental branch of physics, now known as musical acoustics. Early Indian and Chinese theorists show similar approaches: all sought to show that the mathematical laws of harmonics and rhythms were fundamental not only to our understanding of the world but to human well-being. Confucius, like Pythagoras, regarded the small numbers 1,2,3,4 as the source of all perfection.\n\nWithout the boundaries of rhythmic structure – a fundamental equal and regular arrangement of pulse repetition, accent, phrase and duration – music would not be possible. Modern musical use of terms like meter and measure also reflects the historical importance of music, along with astronomy, in the development of counting, arithmetic and the exact measurement of time and periodicity that is fundamental to physics.\n\nThe elements of musical form often build strict proportions or hypermetric structures (powers of the numbers 2 and 3).\n\nMusical form is the plan by which a short piece of music is extended. The term \"plan\" is also used in architecture, to which musical form is often compared. Like the architect, the composer must take into account the function for which the work is intended and the means available, practicing economy and making use of repetition and order. The common types of form known as binary and ternary (\"twofold\" and \"threefold\") once again demonstrate the importance of small integral values to the intelligibility and appeal of music.\n\nA musical scale is a discrete set of pitches used in making or describing music. The most important scale in the Western tradition is the diatonic scale but many others have been used and proposed in various historical eras and parts of the world. Each pitch corresponds to a particular frequency, expressed in hertz (Hz), sometimes referred to as cycles per second (c.p.s.). A scale has an interval of repetition, normally the octave. The octave of any pitch refers to a frequency exactly twice that of the given pitch.\n\nSucceeding superoctaves are pitches found at frequencies four, eight, sixteen times, and so on, of the fundamental frequency. Pitches at frequencies of half, a quarter, an eighth and so on of the fundamental are called suboctaves. There is no case in musical harmony where, if a given pitch be considered accordant, that its octaves are considered otherwise. Therefore, any note and its octaves will generally be found similarly named in musical systems (e.g. all will be called doh or A or Sa, as the case may be).\n\nWhen expressed as a frequency bandwidth an octave A–A spans from 110 Hz to 220 Hz (span=110 Hz). The next octave will span from 220 Hz to 440 Hz (span=220 Hz). The third octave spans from 440 Hz to 880 Hz (span=440 Hz) and so on. Each successive octave spans twice the frequency range of the previous octave.\n\nBecause we are often interested in the relations or ratios between the pitches (known as intervals) rather than the precise pitches themselves in describing a scale, it is usual to refer to all the scale pitches in terms of their ratio from a particular pitch, which is given the value of one (often written 1/1), generally a note which functions as the tonic of the scale. For interval size comparison, cents are often used.\n\nThere are two main families of tuning systems: equal temperament and just tuning. Equal temperament scales are built by dividing an octave into intervals which are equal on a logarithmic scale, which results in perfectly evenly divided scales, but with ratios of frequencies which are irrational numbers. Just scales are built by multiplying frequencies by rational numbers, which results in simple ratios between frequencies, but with scale divisions that are uneven.\n\nOne major difference between equal temperament tunings and just tunings is differences in acoustical beat when two notes are sounded together, which affects the subjective experience of consonance and dissonance. Both of these systems, and the vast majority of music in general, have scales that repeat on the interval of every octave, which is defined as frequency ratio of 2:1. In other words, every time the frequency is doubled, the given scale repeats.\n\nBelow are Ogg Vorbis files demonstrating the difference between just intonation and equal temperament. You may need to play the samples several times before you can pick the difference.\n\n5-limit tuning, the most common form of just intonation, is a system of tuning using tones that are regular number harmonics of a single fundamental frequency. This was one of the scales Johannes Kepler presented in his Harmonices Mundi (1619) in connection with planetary motion. The same scale was given in transposed form by Scottish mathematician and musical theorist, Alexander Malcolm, in 1721 in his 'Treatise of Musick: Speculative, Practical and Historical', and by theorist Jose Wuerschmidt in the 20th century. A form of it is used in the music of northern India.\n\nAmerican composer Terry Riley also made use of the inverted form of it in his \"Harp of New Albion\". Just intonation gives superior results when there is little or no chord progression: voices and other instruments gravitate to just intonation whenever possible. However, it gives two different whole tone intervals (9:8 and 10:9) because a fixed tuned instrument, such as a piano, cannot change key. To calculate the frequency of a note in a scale given in terms of ratios, the frequency ratio is multiplied by the tonic frequency. For instance, with a tonic of A4 (A natural above middle C), the frequency is 440 Hz, and a justly tuned fifth above it (E5) is simply 440×(3:2) = 660 Hz.\n\nPythagorean tuning is tuning based only on the perfect consonances, the (perfect) octave, perfect fifth, and perfect fourth. Thus the major third is considered not a third but a ditone, literally \"two tones\", and is (9:8) = 81:64, rather than the independent and harmonic just 5:4 = 80:64 directly below. A whole tone is a secondary interval, being derived from two perfect fifths, (3:2) = 9:8.\n\nThe just major third, 5:4 and minor third, 6:5, are a syntonic comma, 81:80, apart from their Pythagorean equivalents 81:64 and 32:27 respectively. According to Carl , \"the dependent third conforms to the Pythagorean, the independent third to the harmonic tuning of intervals.\"\n\nWestern common practice music usually cannot be played in just intonation but requires a systematically tempered scale. The tempering can involve either the irregularities of well temperament or be constructed as a regular temperament, either some form of equal temperament or some other regular meantone, but in all cases will involve the fundamental features of meantone temperament. For example, the root of chord ii, if tuned to a fifth above the dominant, would be a major whole tone (9:8) above the tonic. If tuned a just minor third (6:5) below a just subdominant degree of 4:3, however, the interval from the tonic would equal a minor whole tone (10:9). Meantone temperament reduces the difference between 9:8 and 10:9. Their ratio, (9:8)/(10:9) = 81:80, is treated as a unison. The interval 81:80, called the syntonic comma or comma of Didymus, is the key comma of meantone temperament.\n\nIn equal temperament, the octave is divided into equal parts on the logarithmic scale. While it is possible to construct equal temperament scale with any number of notes (for example, the 24-tone Arab tone system), the most common number is 12, which makes up the equal-temperament chromatic scale. In western music, a division into twelve intervals is commonly assumed unless it is specified otherwise.\n\nFor the chromatic scale, the octave is divided into twelve equal parts, each semitone (half-step) is an interval of the twelfth root of two so that twelve of these equal half steps add up to exactly an octave. With fretted instruments it is very useful to use equal temperament so that the frets align evenly across the strings. In the European music tradition, equal temperament was used for lute and guitar music far earlier than for other instruments, such as musical keyboards. Because of this historical force, twelve-tone equal temperament is now the dominant intonation system in the Western, and much of the non-Western, world.\n\nEqually tempered scales have been used and instruments built using various other numbers of equal intervals. The 19 equal temperament, first proposed and used by Guillaume Costeley in the 16th century, uses 19 equally spaced tones, offering better major thirds and far better minor thirds than normal 12-semitone equal temperament at the cost of a flatter fifth. The overall effect is one of greater consonance. Twenty-four equal temperament, with twenty-four equally spaced tones, is widespread in the pedagogy and notation of Arabic music. However, in theory and practice, the intonation of Arabic music conforms to rational ratios, as opposed to the irrational ratios of equally tempered systems.\n\nWhile any analog to the equally tempered quarter tone is entirely absent from Arabic intonation systems, analogs to a three-quarter tone, or neutral second, frequently occur. These neutral seconds, however, vary slightly in their ratios dependent on maqam, as well as geography. Indeed, Arabic music historian Habib Hassan Touma has written that \"the breadth of deviation of this musical step is a crucial ingredient in the peculiar flavor of Arabian music. To temper the scale by dividing the octave into twenty-four quarter-tones of equal size would be to surrender one of the most characteristic elements of this musical culture.\"\n\nMusical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set.\n\nExpanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.\n\nTransformational theory is a branch of music theory developed by David Lewin. The theory allows for great generality because it emphasizes transformations between musical objects, rather than the musical objects themselves.\n\nTheorists have also proposed musical applications of more sophisticated algebraic concepts. The theory of regular temperaments has been extensively developed with a wide range of sophisticated mathematics, for example by associating each regular temperament with a rational point on a Grassmannian.\n\nThe chromatic scale has a free and transitive action of the cyclic group formula_1, with the action being defined via transposition of notes. So the chromatic scale can be thought of as a torsor for the group formula_1.\n\nReal and complex analysis have also been made use of, for instance by applying the theory of the Riemann zeta function to the study of equal divisions of the octave.\n\n\n"}
{"id": "169319", "url": "https://en.wikipedia.org/wiki?curid=169319", "title": "Necessity and sufficiency", "text": "Necessity and sufficiency\n\nIn logic, necessity and sufficiency are terms used to describe a conditional or implicational relationship between statements. For example, in the conditional statement \"If P then Q\", we say that \"Q is necessary for P\" because P cannot be true unless Q is true. Similarly, we say that \"P is sufficient for Q\" because P being true always implies that Q is true, but P not being true does not always imply that Q is not true. \n\nThe assertion that a statement is a \"necessary \"and\" sufficient\" condition of another means that the former statement is true if and only if the latter is true. That is, the two statements must be either simultaneously true or simultaneously false. \n\nIn ordinary English, \"necessary\" and \"sufficient\" indicate relations between conditions or states of affairs, not statements. Being a male sibling is a necessary and sufficient condition for being a brother.\n\nIn the conditional statement, \"if \"S\", then \"N\", the expression represented by \"S\" is called the antecedent and the expression represented by \"N\" is called the consequent. This conditional statement may be written in many equivalent ways, for instance, \"N\" if \"S\", \"S\" only if \"N\", \"S\" implies \"N\", \"N\" is implied by \"S\", , , or \"N\" whenever \"S\"\".\n\nIn the above situation, we also say that \"N\" is a necessary condition for \"S\". In common language this is saying that if the conditional statement is a true statement, then the consequent \"N\" must be true if \"S\" may at all be true (see \"truth table\" immediately below). Phrased differently, the antecedent \"S\" cannot be true without \"N\" being true. For example, in order for someone to be called Socrates, it is necessary for that someone to be Named.\n\nWe also say that \"S\" is a sufficient condition for \"N\". Consider the truth table again. If the conditional statement is true, then if \"S\" is true, \"N\" must be true; whereas if the conditional statement is true and N is true, then S may be true or be false. In common terms, \"\"S\" guarantees \"N\". Continuing the example, knowing that someone is called Socrates is sufficient to know that someone has a Name.\n\nA necessary and sufficient condition requires that both of the implications formula_1 and formula_2 (which can also be written as formula_3) hold. From the first of these we see that \"S\" is a sufficient condition for \"N\", and from the second that \"S\" is a necessary condition for \"N\". This is expressed as \"S\" is necessary and sufficient for \"N\" \", \"\"S\" if and only if \"N\" \", or formula_4.\n\nThe assertion that \"Q\" is necessary for \"P\" is colloquially equivalent to \"\"P\" cannot be true unless \"Q\" is true\" or \"if Q is false, then P is false\". By contraposition, this is the same thing as \"whenever \"P\" is true, so is \"Q\"\".\n\nThe logical relation between \"P\" and \"Q\" is expressed as \"if \"P\", then \"Q\" and denoted \"P\" ⇒ \"Q\" (\"P\" implies \"Q\"). It may also be expressed as any of \"P\" only if \"Q\", \"Q\", if \"P\", \"Q\" whenever \"P\", and \"Q\" when \"P\"\". One often finds, in mathematical prose for instance, several necessary conditions that, taken together, constitute a sufficient condition, as shown in Example 5.\n\n\n\n\n\n\nIf \"P\" is sufficient for \"Q\", then knowing \"P\" to be true is adequate grounds to conclude that \"Q\" is true; however, knowing \"P\" to be false does not meet a minimal need to conclude that \"Q\" is false.\n\nThe logical relation is, as before, expressed as \"if \"P\", then \"Q\" or \"P\" ⇒ \"Q\". This can also be expressed as \"P\" only if \"Q\", \"P\" implies \"Q\"\" or several other variants. It may be the case that several sufficient conditions, when taken together, constitute a single necessary condition, as illustrated in example 5.\n\n\n\n\n\n\nA condition can be either necessary or sufficient without being the other. For instance, \"being a mammal\" (\"N\") is necessary but not sufficient to \"being human\" (\"S\"), and that a number formula_11 \"is rational\" (\"S\") is sufficient but not necessary to formula_11 \"being a real number\" (\"N\") (since there are real numbers that are not rational).\n\nA condition can be both necessary and sufficient. For example, at present, \"today is the Fourth of July\" is a necessary and sufficient condition for \"today is Independence Day in the United States\". Similarly, a necessary and sufficient condition for invertibility of a matrix \"M\" is that \"M\" has a nonzero determinant.\n\nMathematically speaking, necessity and sufficiency are dual to one another. For any statements \"S\" and \"N\", the assertion that \"\"N\" is necessary for \"S\" is equivalent to the assertion that \"S\" is sufficient for \"N\"\". Another facet of this duality is that, as illustrated above, conjunctions (using \"and\") of necessary conditions may achieve sufficiency, while disjunctions (using \"or\") of sufficient conditions may achieve necessity. For a third facet, identify every mathematical predicate \"N\" with the set \"T\"(\"N\") of objects, events, or statements for which \"N\" holds true; then asserting the necessity of \"N\" for \"S\" is equivalent to claiming that \"T\"(\"N\") is a superset of \"T\"(\"S\"), while asserting the sufficiency of \"S\" for \"N\" is equivalent to claiming that \"T\"(\"S\") is a subset of \"T\"(\"N\").\n\nTo say that \"P\" is necessary and sufficient for \"Q\" is to say two things:\n\nOne may summarize any, and thus all, of these cases by the statement \"\"P\" if and only if \"Q\"\", which is denoted by formula_16, whereas cases tell us that formula_16 is identical to formula_15.\n\nFor example, in graph theory a graph \"G\" is called bipartite if it is possible to assign to each of its vertices the color \"black\" or \"white\" in such a way that every edge of \"G\" has one endpoint of each color. And for any graph to be bipartite, it is a necessary and sufficient condition that it contain no odd-length cycles. Thus, discovering whether a graph has any odd cycles tells one whether it is bipartite and conversely. A philosopher might characterize this state of affairs thus: \"Although the concepts of bipartiteness and absence of odd cycles differ in intension, they have identical extension.\n\nIn mathematics, theorems are often stated in the form \"\"P\" is true if and only if \"Q\" is true\". Their proofs normally first prove sufficiency, e.g. formula_14. Secondly, the opposite is proven, formula_20\n\nThis proves that the circles for Q and P match on the Venn diagrams above.\n\nBecause, as explained in previous section, necessity of one for the other is equivalent to sufficiency of the other for the first one, e.g. formula_13 is equivalent to formula_20, if \"P\" is necessary and sufficient for \"Q\", then \"Q\" is necessary and sufficient for \"P\". We can write formula_23 and say that the statements \"\"P\" is true if and only if \"Q\", is true\" and \"\"Q\" is true if and only if \"P\" is true\" are equivalent.\n\n\n\n\n\n"}
{"id": "1719992", "url": "https://en.wikipedia.org/wiki?curid=1719992", "title": "Non-perturbative", "text": "Non-perturbative\n\nIn mathematics and physics, a non-perturbative function or process is one that cannot be accurately described by perturbation theory. An example is the function\n\nformula_1.\n\nThe Taylor series at x = 0 for this function is exactly zero to all orders in perturbation theory, but the function is non-zero if \"x\" ≠ 0.\n\nThe implication of this for physics is that there are some phenomena which are impossible to understand by perturbation theory, regardless of how many orders of perturbation theory we use. Instantons are an example.\n\nTherefore, in theoretical physics, a non-perturbative solution or theory is one that does not require perturbation theory to explicate, or does not simply describe the dynamics of perturbations around some fixed background. For this reason, non-perturbative solutions and theories yield insights into areas and subjects perturbative methods cannot reveal.\n\n"}
{"id": "34858344", "url": "https://en.wikipedia.org/wiki?curid=34858344", "title": "Polymath Project", "text": "Polymath Project\n\nThe Polymath Project is a collaboration among mathematicians to solve important and difficult mathematical problems by coordinating many mathematicians to communicate with each other on finding the best route to the solution. The project began in January 2009 on Timothy Gowers' blog when he posted a problem and asked his readers to post partial ideas and partial progress toward a solution. This experiment resulted in a new answer to a difficult problem, and since then the Polymath Project has grown to describe a particular process of using an online collaboration to solve any math problem.\n\nIn January 2009, Gowers chose to start a social experiment on his blog by choosing an important unsolved mathematical problem and issuing an invitation for other people to help solve it collaboratively in the comments section of his blog. Along with the math problem itself, Gowers asked a question which was included in the title of his blog post, \"is massively collaborative mathematics possible?\" This post led to his creation of the Polymath Project.\n\nSince its inception, it has now sponsored a \"Crowdmath\" project in collaboration with MIT PRIMES program and the Art of Problem Solving. This project is built upon the same idea of the Polymath project that massive collaboration in mathematics is possible and possibly quite fruitful. However, this is specifically aimed at only high school and college students with a goal of creating \"a specific opportunity for the upcoming generation of math and science researchers.\" The problems are original research and unsolved problems in mathematics. All high school and college students from around the world with advanced background of mathematics are encouraged to participate. Older participants are welcomed to participate as mentors and encouraged not to post solutions to the problems. The first Crowdmath project began on March 1, 2016.\n\nThe initial proposed problem for this project, now called Polymath1 by the Polymath community, was to find a new combinatorial proof to the density version of the Hales–Jewett theorem. As the project took form, two main threads of discourse emerged. The first thread, which was carried out in the comments of Gowers's blog, would continue with the original goal of finding a combinatorial proof. The second thread, which was carried out in the comments of Terence Tao's blog, focused on calculating bounds on density of Hales-Jewett numbers and Moser numbers for low dimensions.\n\nAfter seven weeks, Gowers announced on his blog that the problem was \"probably solved\", though work would continue on both Gowers's thread and Tao's thread well into May 2009, some three months after the initial announcement. In total over 40 people contributed to the Polymath1 project. Both threads of the Polymath1 project have been successful, producing at least two new papers to be published under the pseudonym D.H.J. Polymath, where the initials refer to the problem itself (density Hales-Jewett).\n\nThis project was set up in order to try to solve the Erdős discrepancy problem. It was active for much of 2010 and had a brief revival in 2012, but did not end up solving the problem. However, in September 2015, Terence Tao, one of the participants of Polymath5, solved the problem in a pair of papers. One paper proved an averaged form of the Chowla and Elliott conjectures, making use of recent advances in analytic number theory concerning correlations of values of multiplicative functions. The other paper showed how this new result, combined with some arguments discovered by Polymath5, were enough to give a complete solution to the problem. Thus, Polymath5 ended up making a significant contribution to the solution.\n\nThe Polymath8 project was proposed to improve the bounds for small gaps between primes. It has two components:\n\n\nBoth components of the Polymath8 project have been successful, producing two new papers, one of which was published under the pseudonym D.H.J. Polymath.\n\n\n\n\n"}
{"id": "58472252", "url": "https://en.wikipedia.org/wiki?curid=58472252", "title": "Postmodern mathematics", "text": "Postmodern mathematics\n\nPostmodern mathematics is a thought developed as a result of postmodernism. The theory asserts that there is no such thing as ‘absolutism’ or ultimate truth in mathematics. It also declares that the term ‘mathematics’ can’t be used to define a specific object. This thought emerged from the post modernistic idea of the relativity of truth. The thought also maintains that the ideas in mathematics are subjective and that there is no ‘right’ answer to a mathematical question.\n\nThe ideas affirmed in Postmodern Mathematics can be summed up as follows:\n\nThe term 'Mathematics', in its singular form, cannot be used to describe any object. However, mathematics, in its plural form, describes the multiplicity of this term based on the context it is used in. All these various definitions are not necessarily true or false and can be right or wrong based on the context they are defined in.\n\nPostmodernism rejects the idea of universality in mathematics. It claims that there is no such thing as 'absolutism' and asserts the notion of multiplicity in mathematics. This comes from the postmodernist idea that rejects notions of absolutism and the claim that the knowledge we attain as ‘true’ is only a representation constructed by society which is not truer than any other representations. Postmodernists also reject the absolutist idea of mathematics being “human free” and objective and instead assert a “humanistic” view of mathematics which is filled with subjective and humane values. This view sees mathematics as: imbued with moral and social values which play a significant role in the development and applications of mathematics.\n\nCertainty in mathematics is not attainable. The answers proposed are perceived as probabilities and not certainties. Postmodernism in mathematics oppose Aristotelian and modernist concept of an object being either true or false and instead asserts the concept of a “degree of truth”. Thus, proclaiming the notion of partial truths and uncertainty. According to the Postmodernist thought, theories in mathematics accepted as 'true' and 'certain' are products of society as the criteria for 'certainty' and 'truth' are socially constructed. Therefore, postmodernism rejects the notion of certainty and absolute truth in mathematics as certainties in mathematics are not static and can change throughout time and societies. This is significant as mathematics is usually perceived as the \"most certain part of human knowledge\", questioning the certainty of this part of knowledge thus rejects certainty from any form of human knowledge.\n\nMathematics is a corrigible discipline as it is subject to the notion of fallibility and is in a state of constant change. This theory was first proposed by Lakatos in his book \"Proofs and Refutation\". It also constitutes the hypothetical-deductive system of mathematics which declares that the discipline of mathematics is fallible and corrigible, the development of theorems require the falsification of “falsifiers” and transmission to hypothetical knowledge. In other words, in order to develop theorems, one must first falsify the premises under which the theorem would be falsified. This would further constitute that mathematics is connected as a part of the broad human knowledge, including the culture, language and perspectives.\n\nAs theories and laws proclaimed in mathematics are representations made by the society, mathematics is constructed by the needs of societies. Hence, holds no other value other than being representations of cultures and societies.\n\nPostmodernism was a movement that started against modernism. Modernism values the faith in the existence of an objective and universal truth. For modernists, knowledge is the use of empirical methodologies in order to discover the ‘ultimate truth’. Postmodernism counters that knowledge isn’t as empirical and logical as modernism persists, rather it is subjective and is open to change. For postmodernists, knowledge is a construct of the society and is subject to change over time and place.\n\nIn mathematics, modernism asserts notions of Platonism (the view that theories in mathematics are unchangeable and perceives mathematics as perfect and eternal), Logicism (the view that perceives mathematics as a part of Logic) and Formalism. These concepts were accepted universally before the introduction of postmodernism. This change in thoughts brought about by the introduction of postmodernism caused a shift of mathematics research and practice from “logical theories” and challenged the notion of objectivity, universality and certainty in mathematics.\n\nUnlike modernist ideas of singularity, postmodernism denotes the idea of plurality and polycentrism. Postmodernism also promotes the idea of collaboration between the teachers and students of mathematics as it argues for the idea of everyone being an explorer of mathematics. Modernism, on the other hand, propose no question for the nature of mathematics or education as they contend that the two entities are not related to one another. In this thought, students in search of education, mathematics in this context, are exploring an “independent world”. This independent world remains unhinged by the student’s exploration and is not changed by the students’ encounter as they are two separate entities. Postmodernism, on the other hand, encourages the transformation of knowledge by the student as they believe that the two only exist “relative to one another”.\n\nLudwig Wittgenstein (1889-1951) developed a notion against the modernist idea of rationality and claimed that it is isn’t as evident and clear as perceived by modernists. He claimed that certainty in mathematics is “a collection of language games” and truth and false are based on the their following of “the rules of linguistic games”. It is what human beings say that is true and false: and they agree in the language they use.\n\nKarl Popper (1902-1994) introduced the notion of falsification and asserted that to prove theories in sciences, researchers should strive to disprove them. He also asserted that the laws in mathematics are subject to being proven false and hence rejected the idea of ultimate truths.\n\nImre Lakatos (1922-1974) was inspired by Karl Popper's idea of falsification in mathematics and wrote his thesis on the fallibility of mathematics and mathematical theorems. In his book, titled ‘Proofs and refutations’, Lakatos claimed that all the theorems in mathematics are fallible and the theories that are accepted as ‘true’ or ‘perfect’ are only accepted because there hasn’t been any theorem that counters them. He claimed that theories in mathematics have no basis for certainty as they are products of assumptions made by humans and are hence refutable.\n\nPaul Ernest has extensive works on the philosophy of mathematics where he draws upon the works of other postmodern philosophers including Popper and Lakatos. His works mainly reflect his belief in the social constructivism of mathematics which illustrates that mathematical theories are a construct of the society and are thus changeable. He also published the \"Philosophy of Mathematics Education Journal\" which contributes to discussions of the philosophy of mathematics and mathematics education and has extensive articles on postmodern mathematics.\n\nHe introduced the idea of 'fuzzy logic'. This idea rejects the Aristotelian Law of Truth and fallibility of an object (the notion that an object is either true or false). He instead proposed the existence of 'degrees of truth', the idea that an object is characterised into various degrees of truth.\n\nThomas Kuhn (1922-1996) introduced the idea of paradigm shifts in sciences. he asserted that the paradigms and theories in sciences are not fixed but are instead on a continuum and hence are subject to transformations. He describes his theory of paradigms as: \".. each paradigm will be shown to satisfy more or less the criteria that it dictates for itself and to fall short of a few of those dictated by its opponent. .. no paradigm ever solves all the problems it defines ..\" According to Kuhn, education and knowledge strives to interpret and represent rather than provide an objective explanation. These representations are examples of social constructivism and are products of societies throughout time.\n\nIn the postmodernist view, mathematics is learnt not for knowledge but for utility. According to this thought, teachers are not only an ‘authoritative figure’ but are also co-explorers and learners of mathematics or as described by Ernest, “ringmasters of the mathematics circus”. They also assert on the role of students in the teaching mathematics and their role in the shaping of curriculum. Postmodern mathematics asserts the diminishing of the notion of objectivity and absolutism in mathematics thus asserting that a “notion of 2 + 2 = 1” can be true given certain subjective circumstances. However, it doesn’t mean that the notion can be used based on any ‘personal situation’ rather it is relative to the mathematical context and situation it is used in such that 2 +2 = 1 is true in mod (3) arithmetic but it would never be true in any other mathematical context. Hence, mathematical theories are corrigible based on the mathematical contexts they are used in.\n\nUnlike the modernist idea of teachers being the authoritative figure and “echo-phrasing” the textbooks, postmodernism encourages collaborative work between the students and the teachers which again relates to the notion of ‘truth’ and ‘knowledge’ being subjective. This would also emphasise the notion that students should be given the opportunity to exercise alternate methods and solutions to the ones determined by the teachers.\n\nIf mathematics education is reshaped according to the postmodernist thought, it opens up windows for the implementation of mathematics by the students to their everyday life, culture and language. It enables them to reshape their knowledge of mathematics and concepts of mathematics.Mathematics becomes responsible for its uses and consequences, in education and society. Those of us in education have a special reason for wanting this more human view of mathematics. Anything else alienates and dis-empowers learners.Postmodern education also encourages the use of technology and computers in the discovery of new ideas in mathematics. Although these ideas are subject to uncertainty as denoted by postmodernism, they can still have the same or higher ‘degree of truth’ as those proven and discovered by classical methods.\n\nThe rejection of uncertainty also asserts researchers of mathematics and other knowledge to strive to achieve a higher degree of truth for human knowledge and hence encourages further research and study. This in turn expands the human knowledge vastly.\n\nPostmodern education encourages teachers, students and researchers of mathematics to focus on criticism of the known mathematical facts rather than evaluation. It asserts that mathematicians should focus not on what ‘mathematics’ is but rather what it could be and what it might be. It encourages constant criticism of the mathematical theories and trying to disprove the theories in order to get a higher degree of truth in mathematics education. Jean-François Lyotard describes this as Reality (certainty in this context) is not expressed by a phrase like X is such but by one like X is such and not such.He also claims that to prove the ‘reality’ or the ‘truth’ of a description of an entity, which in this context would be mathematics, the negation of another description is needed.\n\n"}
{"id": "4965178", "url": "https://en.wikipedia.org/wiki?curid=4965178", "title": "Quasi-triangular quasi-Hopf algebra", "text": "Quasi-triangular quasi-Hopf algebra\n\nA quasi-triangular quasi-Hopf algebra is a specialized form of a quasi-Hopf algebra defined by the Ukrainian mathematician Vladimir Drinfeld in 1989. It is also a generalized form of a quasi-triangular Hopf algebra.\n\nA quasi-triangular quasi-Hopf algebra is a set formula_1 where formula_2 is a quasi-Hopf algebra and formula_3 known as the R-matrix, is an invertible element such that \n\nso that formula_7 is the switch map and\n\nwhere formula_10 and formula_11.\n\nThe quasi-Hopf algebra becomes \"triangular\" if in addition, formula_12.\n\nThe twisting of formula_13 by formula_14 is the same as for a quasi-Hopf algebra, with the additional definition of the twisted \"R\"-matrix\n\nA quasi-triangular (resp. triangular) quasi-Hopf algebra with formula_15 is a quasi-triangular (resp. triangular) Hopf algebra as the latter two conditions in the definition reduce the conditions of quasi-triangularity of a Hopf algebra.\n\nSimilarly to the twisting properties of the quasi-Hopf algebra, the property of being quasi-triangular or triangular quasi-Hopf algebra is preserved by twisting.\n\n\n"}
{"id": "25308", "url": "https://en.wikipedia.org/wiki?curid=25308", "title": "Quasispecies model", "text": "Quasispecies model\n\nThe quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or \"cloud\" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.\n\nIt is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.\n\nWhen evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive \"fidelity\".) Evolutionarily, we are interested in the behavior and fitness of that one species or genotype over time.\n\nSome organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a \"cloud\" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.\n\nQuasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.\n\nIn a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.\n\nIn a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the \"connectedness\" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.\n\nWhen the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.\n\nQuasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.\n\nThe model rests on four assumptions:\n\nIn the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.\n\nDue to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational \"clouds\" of closely related sequences, referred to as \"quasispecies\". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in \"in vitro\" RNA replication.\n\nThe mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.\n\nA simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence \"i\". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence \"i\", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a \"j\" type parent will produce an \"i\" type organism. Then the expected fraction of offspring generated by \"j\" type organisms that would be \"i\" type organisms is formula_5,\n\nwhere formula_6.\n\nThen the total number of \"i\"-type organisms after the first round of reproduction, given as formula_7, is\n\nSometimes a death rate term formula_9 is included so that:\n\nwhere formula_11 is equal to 1 when i=j and is zero otherwise. Note that the \"n-th\" generation can be found by just taking the \"n-th\" power of W substituting it in place of W in the above formula.\n\nThis is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.\n\nW being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.\n\nThe quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:\n\nThe quasispecies equations are usually expressed in terms of concentrations formula_19 where\n\nThe above equations for the quasispecies then become for the discrete version:\n\nor, for the continuum version:\n\nThe quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:\n\nThe diagonalized matrix is:\n\nAnd the eigenvectors corresponding to these eigenvalues are:\n\nOnly the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).\n\n"}
{"id": "16416512", "url": "https://en.wikipedia.org/wiki?curid=16416512", "title": "Quaternion Society", "text": "Quaternion Society\n\nA scientific society, the Quaternion Society was an \"International Association for Promoting the Study of Quaternions and Allied Systems of Mathematics\". At its peak it consisted of about 60 mathematicians spread throughout the academic world that were experimenting with quaternions and other hypercomplex number systems. The guiding light was Alexander Macfarlane who served as its Secretary initially, and became President in 1909. The Association published a \"Bibliography\" in 1904 and a \"Bulletin\" (annual report) from 1900 to 1913.\n\nThe \"Bulletin\" became a review journal for topics in vector analysis and abstract algebra such as the theory of equipollence. The mathematical work reviewed pertained largely to matrices and linear algebra as the methods were in rapid development at the time.\nIn 1895, Professor P. Molenbroek of The Hague, Holland, and Shinkichi Kimura studying at Yale put out a call for scholars to form the society in widely circulated journals: Nature, Science, and the Bulletin of the American Mathematical Society. Giuseppe Peano also announced the society formation in his \"Rivista di Matematica\". \n\nThe call to form an Association was encouraged by Macfarlane in 1896:\n\nIn 1897 the British Association met in Toronto where vector products were discussed:\n\nA system of national secretaries was announced in the AMS Bulletin in 1899: Alexander MacAulay for Australasia, Victor Schlegel for Germany, Joly for Great Britain and Ireland, Giuseppe Peano for Italy, Kimura for Japan, Aleksandr Kotelnikov for Russia, F. Kraft for Switzerland, and Arthur Stafford Hathaway for the USA. For France the national secretary was Paul Genty, an engineer with the division of Ponts et Chaussees, and a quaternion collaborator with Charles-Ange Laisant, author of \"Methode des Quaterniones\" (1881).\n\nVictor Schlegel reported on the new institution in the Monatshefte für Mathematik.\n\nWhen the Society was organized in 1899, Peter Guthrie Tait was chosen as president, but he declined for reasons of poor health.\n\nThe first President was Robert Stawell Ball and Alexander Macfarlane served as Secretary and Treasurer. In 1905 Charles Jasper Joly took over as President and L. van Elfrinkhof as Treasurer while Macfarlane continued as Secretary. In 1909 Macfarlane became President, James Byrnie Shaw became Secretary, and van Elfrinkhof continued as Treasurer. The next year Macfarlane and Shaw continued in their posts while Macfarlane also absorbed the office of Treasurer. When Macfarlane died in 1913 after nearly completing the issue of the Bulletin, Shaw completed it and wound up the Association.\n\nThe rules state that the President had the power of veto.\n\nThe \"Bulletin of the Association Promoting the Study of Quaternions and Allied Systems of Mathematics\" was issued nine times under the editorship of Alexander Macfarlane. Every issue listed the officers of the Association, governing council, rules, members, and a financial statement from the treasurer. Today HathiTrust provides access to these publications that are mainly of historical interest:\n\nPublished in 1904 at Dublin, cradle of quaternions, the 86 page \"Bibliography of Quaternions and Allied Systems of Mathematics\" cited some one thousand references. The publication set a professional standard; for instance the \"Manual of Quaternions\" (1905) of Joly has no bibliography beyond citation of Macfarlane.\nFurthermore, in 1967 when M.J. Crowe published \"A History of Vector Analysis\", he wrote in the preface (page ix) :\n\nEvery year more papers and books appeared that were of interest to Association members so it was necessary to update the \"Bibliography\" with supplements in the \"Bulletin\". The categories used to group the items in the supplements give a sense of the changing focus of the Association:\n\nIn 1913 Macfarlane died, and as related by Dirk Struik, the Society \"became a victim of the first World War\".\n\nJames Byrnie Shaw, the surviving officer, wrote 50 book notices for American mathematical publications.\nThe final article review in the \"Bulletin\" was The Wilson and Lewis Algebra of Four-Dimensional Space written by J. B. Shaw. He summarizes,\nThe article reviewed was \"The space-time manifold of relativity, the non-euclidean geometry of mechanics, and electromagnetics\".\nHowever, when the textbook \"The Theory of Relativity\" by Ludwik Silberstein in 1914 was made available as an English understanding of Minkowski space, the algebra of biquaternions was applied, but without references to the British background or Macfarlane or other quaternionists of the Society. The language of quaternions had become international, providing content to set theory and expanded mathematical notation, and expressing mathematical physics.\n\n"}
{"id": "58444022", "url": "https://en.wikipedia.org/wiki?curid=58444022", "title": "Rudy Horne", "text": "Rudy Horne\n\nRudy Lee Horne (1968 – 2017) was an African-American mathematician and professor of mathematics at Morehouse College. He worked on dynamical systems, including nonlinear waves. He was the mathematics consultant for the film \"Hidden Figures\".\n\nHorne grew up in the south side of Chicago. His father worked at Sherwin-Williams. He graduated from Crete-Monee High School. He completed a double degree in mathematics and physics at the University of Oklahoma in 1991. He joined the University of Colorado Boulder for his postgraduate studies, earning a master's in physics in 1994 and in mathematics in 1996. He completed his doctorate, \"Collision induced timing jitter and four-wave mixing in wavelength division multiplexing soliton systems\", in 2001 which was supervised by Mark J. Ablowitz. He was the first African American to graduate from the University of Colorado Boulder Department of Applied Mathematics.\n\nAfter completing his PhD, Horne had a position at the California State University, East Bay. before working as postdoctoral researcher at the University of North Carolina at Chapel Hill, with Chris Jones. Horne joined Florida State University in 2005. Horne joined Morehouse College in 2010 and was promoted to associate professor of mathematics in 2015. He continued to study four-wave mixing. His work considered nonlinear optical phenomena. He uncovered effects in parity-time symmetric systems.\n\nHorne was recommended to serve as a mathematics consultant for Hidden Figures by Morehouse College. He worked closely with Theodore Melfi ensured the actors knew how to pronounce \"Euler's\". He spent four months working with 20th Century Fox. In particular, Horne worked with Taraji P. Henson on the mathematics she required for her role as Katherine Johnson. He taught the cast how to get excited by mathematics. His handwriting is on screen during a scene at the beginning of the film where Katherine Johnson solves a quadratic equation. He appeared on the interview series \"In the Know.\" Horne completed a Mathematical Association of America \"Maths Fest\" tour where he discussed the mathematics in Hidden Figures, focussing on the calculations that concerned Glenn's orbit around in 1962. Her appeared on NPR's Closer Look.\n\nHe died on December 11, 2017. The University of Colorado Boulder established a Rudy Lee Horne Memorial Fellowship in his honour. He was described as a \"rock star\", inspiring generations of black students. He was awarded the National Association of Mathematicians (NAM) lifetime achievement award posthumously in 2018.\n"}
{"id": "2025989", "url": "https://en.wikipedia.org/wiki?curid=2025989", "title": "Shell theorem", "text": "Shell theorem\n\nIn classical mechanics, the shell theorem gives gravitational simplifications that can be applied to objects inside or outside a spherically symmetrical body. This theorem has particular application to astronomy.\n\nIsaac Newton proved the shell theorem and stated that:\n\nA corollary is that inside a solid sphere of constant density, the gravitational force within the object varies linearly with distance from the centre, becoming zero by symmetry at the centre of mass. This can be seen as follows: take a point within such a sphere, at a distance formula_1 from the centre of the sphere. Then you can ignore all the shells of greater radius, according to the shell theorem. So, the remaining mass formula_2 is proportional to formula_3 (because it is based on volume), and the gravitational force exerted on it is proportional to formula_4 (the inverse square law), so the overall gravitational effect is proportional to formula_5, so is linear in formula_1.\n\nThese results were important to Newton's analysis of planetary motion; they are not immediately obvious, but they can be proven with calculus. (Alternatively, Gauss's law for gravity offers a much simpler way to prove the same results.)\n\nIn addition to gravity, the shell theorem can also be used to describe the electric field generated by a static spherically symmetric charge density, or similarly for any other phenomenon that follows an inverse square law. The derivations below focus on gravity, but the results can easily be generalized to the electrostatic force. Moreover, the results can be generalized to the case of general ellipsoidal bodies.\n\nA solid, spherically symmetric body can be modelled as an infinite number of concentric, infinitesimally thin spherical shells. If one of these shells can be treated as a point mass, then a system of shells (i.e. the sphere) can also be treated as a point mass. Consider one such shell (the diagram shows a cross-section):\n\nApplying Newton's Universal Law of Gravitation, the sum of the forces due to mass elements in the shaded band is\n\nHowever, since there is partial cancellation due to the vector nature of the force in conjunction with the circular band's symmetry, the leftover component (in the direction pointing toward \"m\") is given by\n\nThe total force on \"m\", then, is simply the sum of the force exerted by all the bands. By shrinking the width of each band, and increasing the number of bands, the sum becomes an integral expression:\n\nSince \"G\" and \"m\" are constants, they may be taken out of the integral:\n\nTo evaluate this integral, one must first express \"dM\" as a function of \"dθ\"\n\nThe total surface of a spherical shell is\n\nwhile the surface of the thin slice between \"θ\" and \"θ\" + \"dθ\" is\nIf the mass of the shell is \"M\", one therefore has that\n\nand\n\nBy the law of cosines,\n\nThese two relations link the three parameters \"θ\", \"ϕ\" and \"s\" that appear in the integral together. When \"θ\" increases from 0 to π radians, \"ϕ\" varies from the initial value 0 to a maximal value to finally return to zero for \"θ\" = π. \"s\" on the other hand increases from the initial value \"r\" − \"R\" to the final value \"r\" + \"R\" when \"θ\" increases from 0 to π radians. \nThis is illustrated in the following animation:\n\nTo find a primitive function to the integrand, one has to make \"s\" the independent integration variable instead of \"θ\".\n\nPerforming an implicit differentiation of the second of the \"cosine law\" expressions above yields\n\nand one gets that\n\nwhere the new integration variable \"s\" increases from \"r\" − \"R\" to \"r\" + \"R\".\n\nInserting the expression for cos(\"φ\") using the first of the \"cosine law\" expressions\nabove, one finally gets that\n\nA primitive function to the integrand is\n\nand inserting the bounds \"r\" − \"R\", \"r\" + \"R\" for the integration variable \"s\" in this primitive function, one gets that\n\nsaying that the gravitational force is the same as that of a point mass in the centre of the shell with the same mass.\n\nFinally, integrate all infinitesimally thin spherical shell with mass of \"dM\", and we can obtain the total gravity contribution of a solid ball to the object outside the ball\n\nBetween the radius of \"x\" to \"x\" + \"dx\", \"dM\" can be expressed as a function of \"x\", i.e.,\n\nTherefore, the total gravity is\n\nwhich suggests that the gravity of a solid spherical ball to an exterior object can be simplified as that of a point mass in the centre of the ball with the same mass.\n\nFor a point inside the shell, the difference is that when \"θ\" is equal to zero, \"ϕ\" takes the value π radians and \"s\" the value \"R\" - \"r\". When \"θ\" increases from 0 to\nπ radians, \"ϕ\" decreases from the initial value π radians to zero and \"s\" increases from the initial value \"R\" - \"r\" to the value \"R\" + \"r\".\n\nThis can all be seen in the following figure\nInserting these bounds into the primitive function\n\none gets that, in this case\n\nsaying that the net gravitational forces acting on the point mass from the mass elements of the shell, outside the measurement point, cancel out.\n\nGeneralization: If formula_7, the resultant force inside the shell is:\n\nThe above results into formula_8 being identically zero if and only if formula_9\n\nOutside the shell (i.e. r>R or r<-R):\n\nThe shell theorem is an immediate consequence of Gauss's law for gravity saying that\n\nwhere \"M\" is the mass of the part of the spherically symmetric mass distribution that is inside the sphere with radius \"r\" and\n\nis the surface integral of the gravitational field g over any closed surface inside which the total mass is \"M\", the unit vector formula_10 being the outward normal to the surface.\n\nThe gravitational field of a spherically symmetric mass distribution like a mass point, a spherical shell or a homogenous sphere must also be spherically symmetric. If formula_11 is a unit vector in the direction from the point of symmetry to another point the gravitational field at this other point must therefore be\n\nwhere \"g\"(\"r\") only depends on the distance \"r\" to the point of symmetry\n\nSelecting the closed surface as a sphere with radius \"r\" with center at the point of symmetry the outward normal to a point on the surface, formula_12, is precisely the direction pointing away from the point of symmetry of the mass distribution.\n\nOne, therefore, has that\n\nand\n\nas the area of the sphere is 4π\"r\".\n\nFrom Gauss's law it then follows that\ni.e. that\n\nIt is natural to ask whether the converse of the shell theorem is true, namely whether the result of the theorem implies the law of universal gravitation, or if there is some more general force law for which the theorem holds. More specifically, one may ask the question:\n\nIn fact, this allows exactly one more class of force than the (Newtonian) inverse square. The most general force as derived in is:\n\nwhere formula_13 and formula_14 can be constants taking any value. The first term is the familiar law of universal gravitation; the second is an additional force, analogous to the cosmological constant term in general relativity.\n\nIf we further constrain the force by requiring that the second part of the theorem also holds, namely that there is no force inside a hollow ball, we exclude the possibility of the additional term, and the inverse square law is indeed the unique force law satisfying the theorem.\n\nOn the other hand, if we relax the conditions, and require only that the field everywhere outside a spherically symmetric body is the same as the field from some point mass at the centre (of any mass), we allow a new class of solutions given by the Yukawa potential, of which the inverse square law is a special case.\n\nAnother generalization can be made for a disc by observing that \n\nso:\n\nwhere formula_15\n\nDoing all the intermediate calculations we get:\n\nNote that formula_16 in this example is expressed in formula_17\n\nPropositions 70 and 71 consider the force acting on a particle from a hollow sphere with an infinitesimally thin surface, whose mass density is constant over the surface. The force on the particle from a small area of the surface of the sphere is proportional to the mass of the area and inversely as the square of its distance from the particle. The first proposition considers the case when the particle is inside the sphere, the second when it is outside. The use of infinitesimals and limiting processes in geometrical constructions are simple and elegant and avoid the need for any integrations. They well illustrate Newton's method of proving many of the propositions in the \"Principia\". \n\nHis proof of Propositions 70 is trivial. In the following, it is considered in slightly greater detail than Newton provides.\n\nThe proof of Proposition 71 is more historically significant. It forms the first part of his proof that the gravitational force of a solid sphere acting on a particle outside it is inversely proportional to the square of its distance from the centre of the sphere, provided the density at any point inside the sphere is a function only of its distance from the centre of the sphere.\n\nAlthough the following are completely faithful to Newton's proofs, very minor changes have been made to attempt to make them clearer.\n\nFig. 2 is a cross-section of the hollow sphere through the centre, S and an arbitrary point, P, inside the sphere. Through P draw two lines IL and HK such that the angle KPL is very small. JM is the line through P that bisects that angle. From the geometry of circles, the triangles IPH and KPL are similar. \nThe lines KH and IL are rotated about the axis JM to form 2 cones that intersect the sphere in 2 closed curves. In Fig. 1 the sphere is seen from a distance along the line PE and is assumed transparent so both curves can be seen. \n\nThe surface of the sphere that the cones intersect can be considered to be flat, and angles formula_18\n\nSince the intersection of a cone with a plane is an ellipse, in this case the intersections form two ellipses with major axes IH and KL, where formula_19\n\nBy a similar argument, the minor axes are in the same ratio. This is clear if the sphere is viewed from above. Therefore the two ellipses are similar, so their areas are as the squares of their major axes. As the mass of any section of the surface is proportional to the area of that section, for the 2 elliptical areas the ratios of their masses formula_20.\n\nSince the force of attraction on P in the direction JM from either of the elliptic areas, is direct as the mass of the area and inversely as the square of its distance from P, it is independent of the distance of P from the sphere. Hence, the forces on P from the 2 infinitesimal elliptical areas are equal and opposite and there is no net force in the direction JM. \n\nAs the position of P and the direction of JM are both arbitrary, it follows that any particle inside a hollow sphere experiences no net force from the mass of the sphere.\n\nNote: Newton simply describes the arcs IH and KL as 'minimally small' and the areas traced out by the lines IL and HK can be any shape, not necessarily elliptic, but they will always be similar.\n\nFig. 1 is a cross-section of the hollow sphere through the centre, S with an arbitrary point, P, outside the sphere. PT is the tangent to the circle at T which passes through P. HI is a small arc on the surface such that PH is less than PT. Extend PI to intersect the sphere at L and draw SF to the point F that bisects IL. Extend PH to intersect the sphere at K and draw SE to the point E that bisects HK, and extend SF to intersect HK at D. Drop a perpendicular IQ on to the line PS joining P to the centre S. Let the radius of the sphere be a and the distance PS be D.\n\nLet arc IH be extended perpendicularly out of the plane of the diagram, by a small distance ζ. The area of the figure generated is IH.ζ, and its mass is proportional to this product.\n\nThe force due to this mass on the particle at P formula_21 and is along the line PI. \n\nThe component of this force towards the centre formula_22.\n\nIf now the arc HI is rotated completely about the line PS to form a ring of width HI and radius IQ, the length of the ring is 2π.IQ and its area is 2π.IQ.IH. The component of the force due to this ring on the particle at P in the direction PS becomes formula_23.\n\nThe perpendicular components of the force directed towards PS cancel out since the mass in the ring is distributed symmetrically about PS. Therefore, the component in the direction PS is the total force on P due to the ring formed by rotating arc HI about PS.\n\nFrom similar triangles: formula_24; formula_25, and formula_26\n\nIf HI is sufficiently small that it can be taken as a straight line, SIH is a right angle, and the angles formula_27, so that formula_28.\n\nHence the force on P due to the ring formula_29.\n\nAssume now in Fig. 2 that another particle is outside the sphere at a point, p, a different distance, d, from the centre of the sphere, with corresponding points lettered in lower case. For easy comparison, the construction of P in Fig. 1 is also shown in Fig. 2. As before, ph is less than pt.\n\nGenerate a ring with width ih and radius iq by making angle formula_30 and the slightly larger Angle formula_31, so that the distance PS is subtended by the same angle at I as is pS at i. The same holds for H and h, respectively.\n\nThe total force on p due to this ring is \n\nClearly formula_32, formula_33, and formula_34.\n\nNewton claims that DF and df can be taken as equal in the limit as the angles DPF and dpf 'vanish together'. Note that angles DPF and dpf are not equal. Although DS and dS become equal in the limit, this does not imply that the ratio of DF to df becomes equal to unity, when DF and df both approach zero. In the finite case DF depends on D, and df on d, so they are not equal.\nSince the ratio of DF to df in the limit is crucial, more detailed analysis is required. From the similar right triangles, formula_35 and formula_36, giving formula_37. Solving the quadratic for DF, in the limit as ES approaches FS, the smaller root, formula_38. More simply, as DF approaches zero, in the limit the formula_39 term can be ignored: formula_40 leading to the same result. Clearly df has the same limit, justifying Newton’s claim.\n\nComparing the force from the ring HI rotated about PS to the ring hi about pS, the ratio of these 2 forces equals formula_41.\n\nBy dividing up the arcs AT and Bt into corresponding infinitesimal rings, it follows that the ratio of the force due to the arc AT rotated about PS to that of Bt rotated about pS is in the same ratio, and similarly, the ratio of the forces due to arc TB to that of tA both rotated are in the same ratio.\n\nTherefore, the force on a particle any distance D from the centre of the hollow sphere is inversely proportional to formula_42, which proves the proposition.\n\nThere is an analog of the shell theorem in general relativity (GR). The shell theorem shows that the gravitational potential outside a spherically symmetric body is given by -GM/r—whether or not the body is changing with time. In general relativity, there is a similar theorem that demonstrating that the spacetime geometry \"outside\" a spherically symmetric mass-energy distribution is the time-independent Schwarzschild geometry—even if the central mass is undergoing gravitational collapse (Misner et al. 1973). \n\nThis equivalent of the shell theorem in GR is very useful because it allows to use the machinery of the Schwarzschild metric to understand the gravitational collapse leading to a black hole, and its effect on the motion of light-rays and particles outside and inside the event horizon (Hartle 2003, chapter 12). \n\n"}
{"id": "3350566", "url": "https://en.wikipedia.org/wiki?curid=3350566", "title": "Supersingular variety", "text": "Supersingular variety\n\nIn mathematics, a supersingular variety is (usually) a smooth projective variety in nonzero characteristic such that for all \"n\" the slopes of the Newton polygon of the \"n\"th crystalline cohomology are all \"n\"/2 . For special classes of varieties such as elliptic curves it is common to use various ad hoc definitions of \"supersingular\", which are (usually) equivalent to the one given above.\n\nThe term \"singular elliptic curve\" (or \"singular \"j\"-invariant\") was at one times used to refer to complex elliptic curves whose ring of endomorphisms has rank 2, the maximum possible. Helmut Hasse discovered that, in finite characteristic, elliptic curves can have larger rings of endomorphisms of rank 4, and these were called \"supersingular elliptic curves\". Supersingular elliptic curves can also be characterized by the slopes of their crystalline cohomology, and the term \"supersingular\" was later extended to other varieties whose cohomology has similar properties. The terms \"supersingular\" or \"singular\" do not mean that the variety has singularites.\n\nExamples include:\n"}
{"id": "222947", "url": "https://en.wikipedia.org/wiki?curid=222947", "title": "Table of bases", "text": "Table of bases\n\nThis article is about \"bases\" as that term is used in discussion of certain numeral systems.\n\nThis table of bases gives the values of 0 to 256 in bases 2 to 36. (Using A−Z for 10−35)\n\n"}
{"id": "30977", "url": "https://en.wikipedia.org/wiki?curid=30977", "title": "Theorem", "text": "Theorem\n\nIn mathematics, a theorem is a statement that has been proven on the basis of previously established statements, such as other theorems, and generally accepted statements, such as axioms. A theorem is a logical consequence of the axioms. The proof of a mathematical theorem is a logical argument for the theorem statement given in accord with the rules of a deductive system. The proof of a theorem is often interpreted as justification of the truth of the theorem statement. In light of the requirement that theorems be proved, the concept of a theorem is fundamentally \"deductive\", in contrast to the notion of a scientific law, which is \"experimental\".\n\nMany mathematical theorems are conditional statements. In this case, the proof deduces the conclusion from conditions called hypotheses or premises. In light of the interpretation of proof as justification of truth, the conclusion is often viewed as a necessary consequence of the hypotheses, namely, that the conclusion is true in case the hypotheses are true, without any further assumptions. However, the conditional could be interpreted differently in certain deductive systems, depending on the meanings assigned to the derivation rules and the conditional symbol.\n\nAlthough they can be written in a completely symbolic form, for example, within the propositional calculus, theorems are often expressed in a natural language such as English. The same is true of proofs, which are often expressed as logically organized and clearly worded informal arguments, intended to convince readers of the truth of the statement of the theorem beyond any doubt, and from which a formal symbolic proof can in principle be constructed. Such arguments are typically easier to check than purely symbolic ones—indeed, many mathematicians would express a preference for a proof that not only demonstrates the validity of a theorem, but also explains in some way \"why\" it is obviously true. In some cases, a picture alone may be sufficient to prove a theorem. Because theorems lie at the core of mathematics, they are also central to its aesthetics. Theorems are often described as being \"trivial\", or \"difficult\", or \"deep\", or even \"beautiful\". These subjective judgments vary not only from person to person, but also with time: for example, as a proof is simplified or better understood, a theorem that was once difficult may become trivial. On the other hand, a deep theorem may be stated simply, but its proof may involve surprising and subtle connections between disparate areas of mathematics. Fermat's Last Theorem is a particularly well-known example of such a theorem.\n\nLogically, many theorems are of the form of an indicative conditional: \"if A, then B\". Such a theorem does not assert \"B\", only that \"B\" is a necessary consequence of \"A\". In this case \"A\" is called the hypothesis of the theorem (\"hypothesis\" here is something very different from a conjecture) and \"B\" the conclusion (formally, \"A\" and \"B\" are termed the \"antecedent\" and \"consequent\"). The theorem \"If \"n\" is an even natural number then \"n\"/2 is a natural number\" is a typical example in which the hypothesis is \"\"n\" is an even natural number\" and the conclusion is \"\"n\"/2 is also a natural number\".\n\nTo be proved, a theorem must be expressible as a precise, formal statement. Nevertheless, theorems are usually expressed in natural language rather than in a completely symbolic form, with the intention that the reader can produce a formal statement from the informal one.\n\nIt is common in mathematics to choose a number of hypotheses within a given language and declare that the theory consists of all statements provable from these hypotheses. These hypotheses form the foundational basis of the theory and are called axioms or postulates. The field of mathematics known as proof theory studies formal languages, axioms and the structure of proofs.\nSome theorems are \"trivial\", in the sense that they follow from definitions, axioms, and other theorems in obvious ways and do not contain any surprising insights. Some, on the other hand, may be called \"deep\", because their proofs may be long and difficult, involve areas of mathematics superficially distinct from the statement of the theorem itself, or show surprising connections between disparate areas of mathematics. A theorem might be simple to state and yet be deep. An excellent example is Fermat's Last Theorem, and there are many other examples of simple yet deep theorems in number theory and combinatorics, among other areas.\n\nOther theorems have a known proof that cannot easily be written down. The most prominent examples are the four color theorem and the Kepler conjecture. Both of these theorems are only known to be true by reducing them to a computational search that is then verified by a computer program. Initially, many mathematicians did not accept this form of proof, but it has become more widely accepted. The mathematician Doron Zeilberger has even gone so far as to claim that these are possibly the only nontrivial results that mathematicians have ever proved. Many mathematical theorems can be reduced to more straightforward computation, including polynomial identities, trigonometric identities and hypergeometric identities.\n\nTo establish a mathematical statement as a theorem, a proof is required, that is, a line of reasoning from axioms in the system (and other, already established theorems) to the given statement must be demonstrated. However, the proof is usually considered as separate from the theorem statement. Although more than one proof may be known for a single theorem, only one proof is required to establish the status of a statement as a theorem. The Pythagorean theorem and the law of quadratic reciprocity are contenders for the title of theorem with the greatest number of distinct proofs.\n\nTheorems in mathematics and theories in science are fundamentally different in their epistemology. A scientific theory cannot be proved; its key attribute is that it is falsifiable, that is, it makes predictions about the natural world that are testable by experiments. Any disagreement between prediction and experiment demonstrates the incorrectness of the scientific theory, or at least limits its accuracy or domain of validity. Mathematical theorems, on the other hand, are purely abstract formal statements: the proof of a theorem cannot involve experiments or other empirical evidence in the same way such evidence is used to support scientific theories.\nNonetheless, there is some degree of empiricism and data collection involved in the discovery of mathematical theorems. By establishing a pattern, sometimes with the use of a powerful computer, mathematicians may have an idea of what to prove, and in some cases even a plan for how to set about doing the proof. For example, the Collatz conjecture has been verified for start values up to about 2.88 × 10. The Riemann hypothesis has been verified for the first 10 trillion zeroes of the zeta function. Neither of these statements is considered proved.\n\nSuch evidence does not constitute proof. For example, the Mertens conjecture is a statement about natural numbers that is now known to be false, but no explicit counterexample (i.e., a natural number \"n\" for which the Mertens function \"M\"(\"n\") equals or exceeds the square root of \"n\") is known: all numbers less than 10 have the Mertens property, and the smallest number that does not have this property is only known to be less than the exponential of 1.59 × 10, which is approximately 10 to the power 4.3 × 10. Since the number of particles in the universe is generally considered less than 10 to the power 100 (a googol), there is no hope to find an explicit counterexample by exhaustive search.\n\nThe word \"theory\" also exists in mathematics, to denote a body of mathematical axioms, definitions and theorems, as in, for example, group theory. There are also \"theorems\" in science, particularly physics, and in engineering, but they often have statements and proofs in which physical assumptions and intuition play an important role; the physical axioms on which such \"theorems\" are based are themselves falsifiable.\n\nA number of different terms for mathematical statements exist; these terms indicate the role statements play in a particular subject. The distinction between different terms is sometimes rather arbitrary and the usage of some terms has evolved over time.\n\n\n\nThere are other terms, less commonly used, that are conventionally attached to proved statements, so that certain theorems are referred to by historical or customary names. For example:\n\n\nA few well-known theorems have even more idiosyncratic names. The division algorithm (see Euclidean division) is a theorem expressing the outcome of division in the natural numbers and more general rings. Bézout's identity is a theorem asserting that the greatest common divisor of two numbers may be written as a linear combination of these numbers. The Banach–Tarski paradox is a theorem in measure theory that is paradoxical in the sense that it contradicts common intuitions about volume in three-dimensional space.\n\nA theorem and its proof are typically laid out as follows:\n\nThe end of the proof may be signalled by the letters Q.E.D. (\"quod erat demonstrandum\") or by one of the tombstone marks \"□\" or \"∎\" meaning \"End of Proof\", introduced by Paul Halmos following their usage in magazine articles.\n\nThe exact style depends on the author or publication. Many publications provide instructions or macros for typesetting in the house style.\n\nIt is common for a theorem to be preceded by definitions describing the exact meaning of the terms used in the theorem. It is also common for a theorem to be preceded by a number of propositions or lemmas which are then used in the proof. However, lemmas are sometimes embedded in the proof of a theorem, either with nested proofs, or with their proofs presented after the proof of the theorem.\n\nCorollaries to a theorem are either presented between the theorem and the proof, or directly after the proof. Sometimes, corollaries have proofs of their own that explain why they follow from the theorem.\n\nIt has been estimated that over a quarter of a million theorems are proved every year.\n\nThe well-known aphorism, , is probably due to Alfréd Rényi, although it is often attributed to Rényi's colleague Paul Erdős (and Rényi may have been thinking of Erdős), who was famous for the many theorems he produced, the number of his collaborations, and his coffee drinking.\n\nThe classification of finite simple groups is regarded by some to be the longest proof of a theorem. It comprises tens of thousands of pages in 500 journal articles by some 100 authors. These papers are together believed to give a complete proof, and several ongoing projects hope to shorten and simplify this proof. Another theorem of this type is the four color theorem whose computer generated proof is too long for a human to read. It is certainly the longest known proof of a theorem whose statement can be easily understood by a layman.\n\nLogic, especially in the field of proof theory, considers theorems as statements (called formulas or well formed formulas) of a formal language. The statements of the language are strings of symbols and may be broadly divided into nonsense and well-formed formulas. A set of deduction rules, also called transformation rules or rules of inference, must be provided. These deduction rules tell exactly when a formula can be derived from a set of premises. The set of well-formed formulas may be broadly divided into theorems and non-theorems. However, according to Hofstadter, a formal system often simply defines all its well-formed formula as theorems.\n\nDifferent sets of derivation rules give rise to different interpretations of what it means for an expression to be a theorem. Some derivation rules and formal languages are intended to capture mathematical reasoning; the most common examples use first-order logic. Other deductive systems describe term rewriting, such as the reduction rules for λ calculus.\n\nThe definition of theorems as elements of a formal language allows for results in proof theory that study the structure of formal proofs and the structure of provable formulas. The most famous result is Gödel's incompleteness theorem; by representing theorems about basic number theory as expressions in a formal language, and then representing this language within number theory itself, Gödel constructed examples of statements that are neither provable nor disprovable from axiomatizations of number theory.\n\nA theorem may be expressed in a formal language (or \"formalized\"). A formal theorem is the purely formal analogue of a theorem. In general, a formal theorem is a type of well-formed formula that satisfies certain logical and syntactic conditions. The notation formula_1 is often used to indicate that formula_1 is a theorem.\n\nFormal theorems consist of formulas of a formal language and the transformation rules of a formal system. Specifically, a formal theorem is always the last formula of a derivation in some formal system each formula of which is a logical consequence of the formulas that came before it in the derivation. The initially accepted formulas in the derivation are called its axioms, and are the basis on which the theorem is derived. A set of theorems is called a theory.\n\nWhat makes formal theorems useful and of interest is that they can be interpreted as true propositions and their derivations may be interpreted as a proof of the truth of the resulting expression. A set of formal theorems may be referred to as a formal theory. A theorem whose interpretation is a true statement about a formal system is called a metatheorem.\n\nThe concept of a formal theorem is fundamentally syntactic, in contrast to the notion of a \"true proposition,\" which introduces semantics. Different deductive systems can yield other interpretations, depending on the presumptions of the derivation rules (i.e. belief, justification or other modalities). The soundness of a formal system depends on whether or not all of its theorems are also validities. A validity is a formula that is true under any possible interpretation, e.g. in classical propositional logic validities are tautologies. A formal system is considered semantically complete when all of its tautologies are also theorems.\n\nThe notion of a theorem is very closely connected to its formal proof (also called a \"derivation\"). To illustrate how derivations are done, we will work in a very simplified formal system. Let us call ours formula_3 Its alphabet consists only of two symbols { A, B } and its formation rule for formulas is:\n\nThe single axiom of formula_3 is:\n\nThe only rule of inference (transformation rule) for formula_3 is:\n\nTheorems in formula_3 are defined as those formulae that have a derivation ending with that formula. For example,\n\n\nis a derivation. Therefore, \"ABBBAB\" is a theorem of formula_8 The notion of truth (or falsity) cannot be applied to the formula \"ABBBAB\" until an interpretation is given to its symbols. Thus in this example, the formula does not yet represent a proposition, but is merely an empty abstraction.\n\nTwo metatheorems of formula_3 are:\n\n\n\n"}
