{"id": "1076270", "url": "https://en.wikipedia.org/wiki?curid=1076270", "title": "AMPL", "text": "AMPL\n\nA Mathematical Programming Language (AMPL) is an algebraic modeling language to describe and solve high-complexity problems for large-scale mathematical computing (i.e., large-scale optimization and scheduling-type problems).\nIt was developed by Robert Fourer, David Gay, and Brian Kernighan at Bell Laboratories.\nAMPL supports dozens of solvers, both open source and commercial software, including CBC, CPLEX, FortMP, Gurobi, MINOS, IPOPT, SNOPT, KNITRO, and LGO. Problems are passed to solvers as nl files.\nAMPL is used by more than 100 corporate clients, and by government agencies and academic institutions.\n\nOne advantage of AMPL is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization. Many modern solvers available on the NEOS Server (formerly hosted at the Argonne National Laboratory, currently hosted at the University of Wisconsin, Madison) accept AMPL input. According to the NEOS statistics AMPL is the most popular format for representing mathematical programming problems.\n\nAMPL features a mix of declarative and imperative programming styles. Formulating optimization models occurs via declarative language elements such as sets, scalar and multidimensional parameters, decision variables, objectives and constraints, which allow for concise description of most problems in the domain of mathematical optimization.\n\nProcedures and control flow statements are available in AMPL for\n\nTo support re-use and simplify construction of large-scale optimization problems, AMPL allows separation of model and data.\n\nAMPL supports a wide range of problem types, among them:\n\nAMPL invokes a solver in a separate process which has these advantages:\nInteraction with the solver is done through a well-defined nl interface.\n\nAMPL is available for many popular 32- and 64-bit operating systems including Linux, Mac OS X, some Unix, and Windows.\nThe translator is proprietary software maintained by AMPL Optimization LLC. However, several online services exist, providing free modeling and solving facilities using AMPL. A free student version with limited functionality and a free full-featured version for academic courses are also available.\n\nAMPL can be used from within Microsoft Excel via the SolverStudio Excel add-in.\n\nThe AMPL Solver Library (ASL), which allows reading nl files and provides the automatic differentiation, is open-source. It is used in many solvers to implement AMPL connection.\n\nThis table present significant steps in AMPL history.\nA transportation problem from George Dantzig is used to provide a sample AMPL model. This problem finds the least cost shipping schedule that meets requirements at markets and supplies at factories.\n\nHere is a partial list of solvers supported by AMPL:\n\n\n"}
{"id": "8534576", "url": "https://en.wikipedia.org/wiki?curid=8534576", "title": "Aisenstadt Prize", "text": "Aisenstadt Prize\n\nThe André Aisenstadt Prize recognizes a young Canadian mathematician's outstanding achievement in pure or applied mathematics. \n\nIt has been awarded annually since 1992 (except in 1994, when no prize was given) by the Centre de Recherches Mathématiques at the University of Montreal. The prize consists of a $3,000 award and a medal. It is named after .\n\nSource: CRM, University of Montreal\n\n"}
{"id": "44065971", "url": "https://en.wikipedia.org/wiki?curid=44065971", "title": "Blockchain", "text": "Blockchain\n\nA blockchain, originally block chain, is a growing list of records, called \"blocks\", which are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash).\n\nBy design, a blockchain is resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain.\n\nBlockchain was invented by Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin. The invention of the blockchain for bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications, and blockchains which are readable by the public are widely used by cryptocurrencies. Blockchain is considered a type of payment rail. Private blockchains have been proposed for business use. Sources such as the \"Computerworld\" called the marketing of such blockchains without a proper security model \"snake oil\".\n\nThe first work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system where document timestamps could not be tampered with. In 1992, Bayer, Haber and Stornetta incorporated Merkle trees to the design, which improved its efficiency by allowing several document certificates to be collected into one block.\n\nThe first blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to add blocks to the chain without requiring them to be signed by a trusted party. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.\n\nIn August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.\n\nThe words \"block\" and \"chain\" were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, \"blockchain,\" by 2016. The term \"blockchain 2.0\" refers to new applications of the distributed blockchain database, first emerging in 2014. \"The Economist\" described one implementation of this second-generation programmable blockchain as coming with \"a programming language that allows users to write more sophisticated smart contracts, thus creating invoices that pay themselves when a shipment arrives or share certificates which automatically send their owners dividends if profits reach a certain level.\"\n\n, blockchain 2.0 implementations continue to require an off-chain oracle to access any \"external data or events based on time or market conditions [that need] to interact with the blockchain.\"\n\nIBM opened a blockchain innovation research center in Singapore in July 2016. A working group for the World Economic Forum met in November 2016 to discuss the development of governance models related to blockchain.\n\nAccording to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.\n\nIn May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ‘planning or [looking at] active experimentation with blockchain’.\n\nIn November 2018, Conservative MEP Emma McClarkin’s plan to utilise blockchain technology to boost trade was backed by the European Parliament’s Trade Committee. \n\nA blockchain is a decentralized, distributed and public digital ledger that is used to record transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double spending. A blockchain has been described as a \"value-exchange protocol\". This blockchain-based exchange of value can be completed quicker, safer and cheaper than with traditional systems. A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance.\n\nBlocks hold batches of valid transactions that are hashed and encoded into a Merkle tree. Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block.\n\nSometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher value can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially as more blocks are built on top of it, eventually becoming very low. For example, in a blockchain using the proof-of-work system, the chain with the most cumulative proof-of-work is always considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.\n\nThe \"block time\" is the average time it takes for the network to generate one extra block in the blockchain. Some blockchains create a new block as frequently as every five seconds. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is 10 minutes.\n\nBy storing data across its peer-to-peer network, the blockchain eliminates a number of risks that come with data being held centrally. The decentralized blockchain may use ad-hoc message passing and distributed networking.\n\nPeer-to-peer blockchain networks lack centralized points of vulnerability that computer crackers can exploit; likewise, it has no central point of failure. Blockchain security methods include the use of public-key cryptography. A \"public key\" (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A \"private key\" is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.\n\nEvery node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication and computational trust. No centralized \"official\" copy exists and no user is \"trusted\" more than any other. Transactions are broadcast to the network using software. Messages are delivered on a best-effort basis. Mining nodes validate transactions, add them to the block they are building, and then broadcast the completed block to other nodes. Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes. Alternative consensus methods include proof-of-stake. Growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.\n\nOpen blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain. Proponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases. Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain. Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision. Nikolai Hampton of \"Computerworld\" said that \"many in-house blockchain solutions will be nothing more than cumbersome databases,\" and \"without a clear security model, proprietary blockchains should be eyed with suspicion.\"\n\nThe great advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed. This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.\n\nBitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include a proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\".\n\nFinancial companies have not prioritised decentralized blockchains.\nIn 2016, venture capital investment for blockchain-related projects was weakening in the USA but increasing in China. Bitcoin and many other cryptocurrencies use open (public) blockchains. , bitcoin has the highest market capitalization.\n\nPermissioned blockchains use an access control layer to govern who has access to the network. In contrast to public blockchain networks, validators on private blockchain networks are vetted by the network owner. They do not rely on anonymous nodes to validate transactions nor do they benefit from the network effect. Permissioned blockchains can also go by the name of 'consortium' or 'hybrid' blockchains.\n\nThe \"New York Times\" noted in both 2016 and 2017 that many corporations are using blockchain networks \"with private blockchains, independent of the public system.\"\n\nNikolai Hampton pointed out in \"Computerworld\" that \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.\" This has a set of particularly profound adverse implications during a financial crisis or debt crisis like the financial crisis of 2007–08, where politically powerful actors may make decisions that favor some groups at the expense of others, and \"the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power—it's time consuming and expensive.\" He also said, \"Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\"\n\nBlockchain technology can be integrated into multiple areas. The primary use of blockchains today is as a distributed ledger for cryptocurrencies, most notably bitcoin. There are a few operational products maturing from proof of concept by late 2016.\n\n, some observers remain skeptical. Steve Wilson, of Constellation Research, believes the technology has been hyped with unrealistic claims. To mitigate risk, businesses are reluctant to place blockchain at the core of the business structure.\n\nMost cryptocurrencies use blockchain technology to record transactions. For example, the bitcoin network and Ethereum network are blockchain-based. On May 8, 2018 Facebook confirmed that it is opening a new blockchain group which will be headed by David Marcus who previously was in charge of Messenger. According to The Verge Facebook is planning to launch its own cryptocurrency for facilitating payments on the platform.\n\nBlockchain-based smart contracts are proposed contracts that could be partially or fully executed or enforced without human interaction. One of the main objectives of a smart contract is automated escrow. An IMF staff discussion reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general. But \"no viable smart contract systems have yet emerged.\" Due to the lack of widespread use their legal status is unclear.\n\nMajor portions of the financial industry are implementing distributed ledgers for use in banking, and according to a September 2016 IBM study, this is occurring faster than expected.\n\nBanks are interested in this technology because it has potential to speed up back office settlement systems.\n\nBanks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.\n\nBerenberg, a German bank, believes that blockchain is an \"overhyped technology\" that has had a large number of \"proofs of concept\", but still has major challenges, and very few success stories.\n\nSome video games are based on blockchain technology. The first such game, \"Huntercoin\", was released in February, 2014. Another blockchain game is \"CryptoKitties\", launched in November 2017. The game made headlines in December 2017 when a cryptokitty character - an-in game virtual pet - was sold for US$100,000. \"CryptoKitties\" illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network with about 30% of all Ethereum transactions being for the game.\n\nCryptokitties also demonstrated how blockchains can be used to catalog game assets (digital assets).\n\nWithin the video game industry, while blockchain use is seen as part of a marketplace mechanism, such as with Robot Cache, blockchain is also postulated as a way to share video game assets between various games. The Blockchain Game Alliance was formed in September 2018 to explore alternative uses of blockchains in video gaming with support of Ubisoft and Fig, among others.\n\nBlockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users or musicians. In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution. Imogen Heap's Mycelia service has also been proposed as blockchain-based alternative \"that gives artists more control over how their songs and associated data circulate among fans and other musicians.\" Everledger is one of the inaugural clients of IBM's blockchain-based tracking service.\n\nNew distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain. The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers. Online voting is another application of the blockchain.\n\nOther designs include:\n\nIn September 2018, IBM and a start-up Hu-manity.co launched a blockchain-based app that let patients sell anonymized data to pharmaceutical companies.\n\nCurrently, there are three types of blockchain networks - public blockchains, private blockchains and consortium blockchains.\n\nA public blockchain has absolutely no access restrictions. Anyone with an internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol). Usually, such networks offer economic incentives for those who secure them and utilize some type of a Proof of Stake or Proof of Work algorithm.\n\nSome of the largest, most known public blockchains are Bitcoin and Ethereum.\n\nA private blockchain is permissioned. One cannot join it unless invited by the network administrators. Participant and validator access is restricted.\n\nThis type of blockchains can be considered a middle-ground for companies that are interested in the blockchain technology in general but are not comfortable with a level of control offered by public networks. Typically, they seek to incorporate blockchain into their accounting and record-keeping procedures without sacrificing autonomy and running the risk of exposing sensitive data to the public internet.\n\nA consortium blockchain is often said to be semi-decentralized. It, too, is permissioned but instead of a single organization controlling it, a number of companies might each operate a node on such a network. The administrators of a consortium chain restrict users' reading rights as they see fit and only allow a limited set of trusted nodes to execute a consensus protocol.\n\nIn October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.\n\nThe Bank for International Settlements has criticized the public proof-of-work blockchains for high energy consumption.\n\nNicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley examines blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases finds it grossly inadequate.\n\nIn September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, \"Ledger\", was announced. The inaugural issue was published in December 2016. The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies such as bitcoin.\n\nThe journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address in the first page of their papers.\n\n\n\n"}
{"id": "4694434", "url": "https://en.wikipedia.org/wiki?curid=4694434", "title": "Boolean model of information retrieval", "text": "Boolean model of information retrieval\n\nThe (standard) Boolean model of information retrieval (BIR) is a classical information retrieval (IR) model and, at the same time, the first and most-adopted one. It is used by many IR systems to this day. The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the user's query are conceived as sets of terms. Retrieval is based on whether or not the documents contain the query terms. \n\nAn \"index term\" is a word or expression\",\" which may be stemmed, describing or characterizing a document, such as a keyword given for a journal article. Letformula_1be the set of all such index terms. \n\nA \"document\" is any subset of formula_2. Letformula_3be the set of all documents. \n\nA \"query\" is a Boolean expression formula_4 in normal form:formula_5where formula_6 is true for formula_7 when formula_8. (Equivalently, formula_4 could be expressed in disjunctive normal form.)\n\nWe seek to find the set of documents that satisfy formula_4. This operation is called \"retrieval\" and consists of the following two steps:\n\nLet the set of original (real) documents be, for example\n\nwhere\n\nformula_18 = \"Bayes' principle: The principle that, in estimating a parameter, one should initially assume that each possible value has equal probability (a uniform prior distribution).\"\n\nformula_19 = \"Bayesian decision theory: A mathematical theory of decision-making which presumes utility and probability functions, and according to which the act to be chosen is the Bayes act, i.e. the one with highest subjective expected utility. If one had unlimited time and calculating power with which to make every decision, this procedure would be the best way to make any decision.\"\n\nformula_20 = \"Bayesian epistemology: A philosophical theory which holds that the epistemic status of a proposition (i.e. how well proven or well established it is) is best measured by a probability and that the proper way to revise this probability is given by Bayesian conditionalisation or similar procedures. A Bayesian epistemologist would use probability to define, and explore the relationship between, concepts such as epistemic status, support or explanatory power.\"\n\nLet the set formula_21 of terms be:\n\nformula_22\n\nThen, the set formula_23 of documents is as follows:\n\nwhere formula_25\n\nLet the query formula_4 be:\n\nformula_27Then to retrieve the relevant documents:\nThis means that the original document formula_19 (corresponding to formula_36) is the answer to formula_4.\n\nObviously, if there is more than one document with the same representation, every such document is retrieved. Such documents are indistinguishable in the BIR (in other words, equivalent).\n\n\n\nFrom a pure formal mathematical point of view, the BIR is straightforward. From a practical point of view, however, several further problems should be solved that relate to algorithms and data structures, such as, for example, the choice of terms (manual or automatic selection or both), stemming, hash tables, inverted file structure, and so on.\n\nAnother possibility is to use hash sets. Each document is represented by a hash table which contains every single term of that document. Since hash table size increases and decreases in real time with the addition and removal of terms, each document will occupy much less space in memory. However, it will have a slowdown in performance because the operations are more complex than with bit vectors. On the worst-case performance can degrade from O(\"n\") to O(\"n\"). On the average case, the performance slowdown will not be that much worse than bit vectors and the space usage is much more efficient.\n"}
{"id": "7593", "url": "https://en.wikipedia.org/wiki?curid=7593", "title": "Calculator", "text": "Calculator\n\nAn electronic calculator is typically a portable electronic device used to perform calculations, ranging from basic arithmetic to complex mathematics.\n\nThe first solid-state electronic calculator was created in the early 1960s. Pocket-sized devices became available in the 1970s, especially after the Intel 4004, the first microprocessor, was developed by Intel for the Japanese calculator company Busicom. They later became used commonly within the petroleum industry (oil and gas).\n\nModern electronic calculators vary from cheap, give-away, credit-card-sized models to sturdy desktop models with built-in printers. They became popular in the mid-1970s as the incorporation of integrated circuits reduced their size and cost. By the end of that decade, prices had dropped to the point where a basic calculator was affordable to most and they became common in schools.\n\nComputer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all personal digital assistant (PDA) type devices, the exceptions being a few dedicated address book and dictionary devices.\n\nIn addition to general purpose calculators, there are those designed for specific markets. For example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space. , basic calculators cost little, but scientific and graphing models tend to cost more.\n\nIn 1986, calculators still represented an estimated 41% of the world's general-purpose hardware capacity to compute information. By 2007, this diminished to less than 0.05%.\n\nElectronic calculators contain a keyboard with buttons for digits and arithmetical operations; some even contain \"00\" and \"000\" buttons to make larger or smaller numbers easier to enter. Most basic calculators assign only one digit or operation on each button; however, in more specific calculators, a button can perform multi-function working with key combinations.\n\nCalculators usually have liquid-crystal displays (LCD) as output in place of historical light-emitting diode (LED) displays and vacuum fluorescent displays (VFD); details are provided in the section \"Technical improvements\".\n\nLarge-sized figures are often used to improve readability; while using decimal separator (usually a point rather than a comma) instead of or in addition to vulgar fractions. Various symbols for function commands may also be shown on the display. Fractions such as are displayed as decimal approximations, for example rounded to . Also, some fractions (such as , which is ; to 14 significant figures) can be difficult to recognize in decimal form; as a result, many scientific calculators are able to work in vulgar fractions or mixed numbers.\n\nCalculators also have the ability to store numbers into computer memory. Basic calculators usually store only one number at a time; more specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulas. Some models have the ability to extend memory capacity to store more numbers; the extended memory address is termed an array index.\n\nPower sources of calculators are: batteries, solar cells or mains electricity (for old models), turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off (for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid). Crank-powered calculators were also common in the early computer era.\n\nThe following keys are common to most pocket calculators. While the arrangement of the digits is standard, the positions of other keys vary from model to model; the illustration is an example.\n\nIn general, a basic electronic calculator consists of the following components:\n\n\nClock rate of a processor chip refers to the frequency at which the central processing unit (CPU) is running. It is used as an indicator of the processor's speed, and is measured in \"clock cycles per second\" or the SI unit hertz (Hz). For basic calculators, the speed can vary from a few hundred hertz to the kilohertz range.\nA basic explanation as to how calculations are performed in a simple four-function calculator:\n\nTo perform the calculation , one presses keys in the following sequence on most calculators:     .\n\nOther functions are usually performed using repeated additions or subtractions.\n\nMost pocket calculators do all their calculations in BCD rather than a floating-point representation. BCD is common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to a simpler overall system than converting to and from binary.\n\nThe same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.\n\nWhere calculators have added functions (such as square root, or trigonometric functions), software algorithms are required to produce high precision results. Sometimes significant design effort is needed to fit all the desired functions in the limited memory space available in the calculator chip, with acceptable calculation time.\n\nThe fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions (such as addition, multiplication, and logarithms) built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functions, sometimes with support for programming languages (such as RPL or TI-BASIC).\n\nFor instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in read-only memory (ROM), and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require much multiplication. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. This distinction blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, more so the Z80, MC68000, and ARM architectures, and some custom designs specialized for the calculator market.\n\nThe first known tools used to aid arithmetic calculations were: bones (used to tally items), pebbles, and counting boards, and the abacus, known to have been used by Sumerians and Egyptians before 2000 BC. Except for the Antikythera mechanism (an \"out of the time\" astronomical device), development of computing tools arrived near the start of the 17th century: the geometric-military compass (by Galileo), logarithms and Napier bones (by Napier), and the slide rule (by Edmund Gunter).\nIn 1642, the Renaissance saw the invention of the mechanical calculator (by Wilhelm Schickard and several decades later Blaise Pascal), a device that was at times somewhat over-promoted as being able to perform all four arithmetic operations with minimal human intervention. Pascal's calculator could add and subtract two numbers directly and thus, if the tedium could be borne, multiply and divide by repetition. Schickard's machine, constructed several decades earlier, used a clever set of mechanised multiplication tables to ease the process of multiplication and division with the adding machine as a means of completing this operation. (Because they were different inventions with different aims a debate about whether Pascal or Schickard should be credited as the \"inventor\" of the adding machine (or calculating machine) is probably pointless.) Schickard and Pascal were followed by Gottfried Leibniz who spent forty years designing a four-operation mechanical calculator, the stepped reckoner, inventing in the process his leibniz wheel, but who couldn't design a fully operational machine. There were also five unsuccessful attempts to design a calculating clock in the 17th century.\nThe 18th century saw the arrival of some notable improvements, first by Poleni with the first fully functional calculating clock and four-operation machine, but these machines were almost always \"one of the kind\". Luigi Torchi invented the first direct multiplication machine in 1834: this was also the second key-driven machine in the world, following that of James White (1822). It was not until the 19th century and the Industrial Revolution that real developments began to occur. Although machines capable of performing all four arithmetic functions existed prior to the 19th century, the refinement of manufacturing and fabrication processes during the eve of the industrial revolution made large scale production of more compact and modern units possible. The Arithmometer, invented in 1820 as a four-operation mechanical calculator, was released to production in 1851 as an adding machine and became the first commercially successful unit; forty years later, by 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883) and Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers.\n\nIt wasn't until 1902 that the familiar push-button user interface was developed, with the introduction of the Dalton Adding Machine, developed by James L. Dalton in the United States.\n\nIn 1921, Edith Clarke invented the \"Clarke calculator\", a simple graph-based calculator for solving line equations involving hyperbolic functions. This allowed electrical engineers to simplify calculations for inductance and capacitance in power transmission lines.\n\nThe Curta calculator was developed in 1948 and, although costly, became popular for its portability. This purely mechanical hand-held device could do addition, subtraction, multiplication and division. By the early 1970s electronic pocket calculators ended manufacture of mechanical calculators, although the Curta remains a popular collectable item.\n\nThe first mainframe computers, using firstly vacuum tubes and later transistors in the logic circuits, appeared in the 1940s and 1950s. This technology was to provide a stepping stone to the development of electronic calculators.\n\nThe Casio Computer Company, in Japan, released the Model \"14-A\" calculator in 1957, which was the world's first all-electric (relatively) compact calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.\nIn October 1961, the world's first \"all-electronic desktop\" calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced. This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode \"Nixie\" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly due to its large tube system. Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names \"Plus\" and \"Sumlock\", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.\n\nThe tube technology of the ANITA was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a cathode ray tube (CRT), and introduced Reverse Polish Notation (RPN) to the calculator market for a price of $2200, which was about three times the cost of an electromechanical calculator of the time. Like Bell Punch, Friden was a manufacturer of mechanical calculators that had decided that the future lay in electronics. In 1964 more all-transistor electronic calculators were introduced: Sharp introduced the CS-10A, which weighed and cost 500,000 yen ($), and Industria Macchine Elettroniche of Italy introduced the IME 84, to which several extra keyboard and display units could be connected so that several people could make use of it (but apparently not at the same time).\n\nThere followed a series of electronic calculator models from these and other manufacturers, including Canon, Mathatronics, Olivetti, SCM (Smith-Corona-Marchant), Sony, Toshiba, and Wang. The early calculators used hundreds of germanium transistors, which were cheaper than silicon transistors, on multiple circuit boards. Display types used were CRT, cold-cathode Nixie tubes, and filament lamps. Memory technology was usually based on the delay line memory or the magnetic core memory, though the Toshiba \"Toscal\" BC-1411 appears to have used an early form of dynamic RAM built from discrete components. Already there was a desire for smaller and less power-hungry machines.\n\nThe Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.\nAnother calculator introduced in 1965 was Bulgaria's ELKA 6521, developed by the Central Institute for Calculation Technologies and built at the Elektronika factory in Sofia. The name derives from \"ELektronen KAlkulator\", and it weighed around . It is the first calculator in the world which includes the square root function. Later that same year were released the ELKA 22 (with a luminescent display) and the ELKA 25, with an in-built printer. Several other models were developed until the first pocket model, the ELKA 101, was released in 1974. The writing on it was in Roman script, and it was exported to western countries.\n\nThe \"Monroe Epic\" programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only \"branch\" instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.\n\nThe first handheld calculator was a prototype called \"Cal Tech\", whose development was led by Jack Kilby at Texas Instruments in 1967. It could add, multiply, subtract, and divide, and its output device was a paper tape.\n\nThe electronic calculators of the mid-1960s were large and heavy desktop machines due to their use of hundreds of transistors on several circuit boards with a large power consumption that required an AC power supply. There were great efforts to put the logic required for a calculator into fewer and fewer integrated circuits (chips) and calculator electronics was one of the leading edges of semiconductor development. U.S. semiconductor manufacturers led the world in large scale integration (LSI) semiconductor development, squeezing more and more functions into individual integrated circuits. This led to alliances between Japanese calculator manufacturers and U.S. semiconductor companies: Canon Inc. with Texas Instruments, Hayakawa Electric (later renamed Sharp Corporation) with North-American Rockwell Microelectronics (later renamed Rockwell International), Busicom with Mostek and Intel, and General Instrument with Sanyo.\n\nBy 1970, a calculator could be made using just a few chips of low power consumption, allowing portable models powered from rechargeable batteries. The first portable calculators appeared in Japan in 1970, and were soon marketed around the world. These included the Sanyo ICC-0081 \"Mini Calculator\", the Canon Pocketronic, and the Sharp QT-8B \"micro Compet\". The Canon Pocketronic was a development of the \"Cal-Tech\" project which had been started at Texas Instruments in 1965 as a research project to produce a portable calculator. The Pocketronic has no traditional display; numerical output is on thermal paper tape. As a result of the \"Cal-Tech\" project, Texas Instruments was granted master patents on portable calculators.\n\nSharp put in great efforts in size and power reduction and introduced in January 1971 the Sharp EL-8, also marketed as the Facit 1111, which was close to being a pocket calculator. It weighed 1.59 pounds (721 grams), had a vacuum fluorescent display, rechargeable NiCad batteries, and initially sold for US $395.\n\nHowever, the efforts in integrated circuit development culminated in the introduction in early 1971 of the first \"calculator on a chip\", the MK6010 by Mostek, followed by Texas Instruments later in the year. Although these early hand-held calculators were very costly, these advances in electronics, together with developments in display technology (such as the vacuum fluorescent display, LED, and LCD), led within a few years to the cheap pocket calculator available to all.\n\nIn 1971 Pico Electronics. and General Instrument also introduced their first collaboration in ICs, a full single chip calculator IC for the Monroe Royal Digital III calculator. Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. Pico and GI went on to have significant success in the burgeoning handheld calculator market.\n\nThe first truly pocket-sized electronic calculator was the Busicom LE-120A \"HANDY\", which was marketed early in 1971. Made in Japan, this was also the first calculator to use an LED display, the first hand-held calculator to use a single integrated circuit (then proclaimed as a \"calculator on a chip\"), the Mostek MK6010, and the first electronic calculator to run off replaceable batteries. Using four AA-size cells the LE-120A measures .\n\nThe first European-made pocket-sized calculator, DB 800 is made in May 1971 by Digitron in Buje, Croatia (former Yugoslavia) with four functions and an eight-digit display and special characters for a negative number and a warning that the calculation has too many digits to display.\n\nThe first American-made pocket-sized calculator, the Bowmar 901B (popularly termed \"The Bowmar Brain\"), measuring , came out in the Autumn of 1971, with four functions and an eight-digit red LED display, for $240, while in August 1972 the four-function Sinclair Executive became the first slimline pocket calculator measuring and weighing . It retailed for around £79 ($). By the end of the decade, similar calculators were priced less than £5 ($).\n\nThe first Soviet Union made pocket-sized calculator, the \"Elektronika B3-04\" was developed by the end of 1973 and sold at the start of 1974.\n\nOne of the first low-cost calculators was the Sinclair Cambridge, launched in August 1973. It retailed for £29.95 ($), or £5 ($) less in kit form. The Sinclair calculators were successful because they were far cheaper than the competition; however, their design led to slow and inaccurate computations of transcendental functions.\n\nMeanwhile, Hewlett-Packard (HP) had been developing a pocket calculator. Launched in early 1972, it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with \"scientific\" functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, used reverse Polish notation (RPN), also called postfix notation. A calculation like \"8 plus 5\" is, using RPN, performed by pressing , , , and ; instead of the algebraic infix notation: , , , . It had 35 buttons and was based on Mostek Mk6020 chip.\n\nThe first Soviet \"scientific\" pocket-sized calculator the \"B3-18\" was completed by the end of 1975.\n\nIn 1973, Texas Instruments (TI) introduced the SR-10, (\"SR\" signifying slide rule) an \"algebraic entry\" pocket calculator using scientific notation for $150. Shortly after the SR-11 featured an added key for entering Pi (π). It was followed the next year by the SR-50 which added log and trig functions to compete with the HP-35, and in 1977 the mass-marketed TI-30 line which is still produced.\n\nIn 1978 a new company, Calculated Industries arose which focused on specialized markets. Their first calculator, the Loan Arranger (1978) was a pocket calculator marketed to the Real Estate industry with preprogrammed functions to simplify the process of calculating payments and future values. In 1985, CI launched a calculator for the construction industry called the Construction Master which came preprogrammed with common construction calculations (such as angles, stairs, roofing math, pitch, rise, run, and feet-inch fraction conversions). This would be the first in a line of construction related calculators.\n\nThe first desktop \"programmable calculators\" were produced in the mid-1960s by Mathatronics and Casio (AL-1000). These machines were very heavy and costly. The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced \"continuous memory\", i.e., programs and data were retained in CMOS memory during power-off. In 1979, HP released the first \"alphanumeric\", programmable, \"expandable\" calculator, the HP-41C. It could be expanded with random access memory (RAM, for memory) and read-only memory (ROM, for software) modules, and peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).\n\nThe first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the start of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika \"B3-21\", was developed by the end of 1976 and released at the start of 1977. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited abilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.\n\nThis series of calculators was also noted for a large number of highly counter-intuitive mysterious undocumented features, somewhat similar to \"synthetic programming\" of the American HP-41, which were exploited by applying normal arithmetic operations to error messages, jumping to nonexistent addresses and other methods. A number of respected monthly publications, including the popular science magazine \"Nauka i Zhizn\" (\"Наука и жизнь\", \"Science and Life\"), featured special columns, dedicated to optimization methods for calculator programmers and updates on undocumented features for hackers, which grew into a whole esoteric science with many branches, named \"yeggogology\" (\"еггогология\"). The error messages on those calculators appear as a Russian word \"YEGGOG\" (\"ЕГГОГ\") which, unsurprisingly, is translated to \"Error\".\n\nA similar hacker culture in the USA revolved around the HP-41, which was also noted for a large number of undocumented features and was much more powerful than B3-34.\n\nThrough the 1970s the hand-held electronic calculator underwent rapid development. The red LED and blue/green vacuum fluorescent displays consumed a lot of power and the calculators either had a short battery life (often measured in hours, so rechargeable nickel-cadmium batteries were common) or were large so that they could take larger, higher capacity batteries. In the early 1970s liquid-crystal displays (LCDs) were in their infancy and there was a great deal of concern that they only had a short operating lifetime. Busicom introduced the Busicom \"LE-120A \"HANDY\"\" calculator, the first pocket-sized calculator and the first with an LED display, and announced the Busicom \"LC\" with LCD. However, there were problems with this display and the calculator never went on sale. The first successful calculators with LCDs were manufactured by Rockwell International and sold from 1972 by other companies under such names as: Dataking \"LC-800\", Harden \"DT/12\", Ibico \"086\", Lloyds \"40\", Lloyds \"100\", Prismatic \"500\" (a.k.a. \"P500\"), Rapid Data \"Rapidman 1208LC\". The LCDs were an early form using the \"Dynamic Scattering Mode DSM\" with the numbers appearing as bright against a dark background. To present a high-contrast display these models illuminated the LCD using a filament lamp and solid plastic light guide, which negated the low power consumption of the display. These models appear to have been sold only for a year or two.\n\nA more successful series of calculators using a reflective DSM-LCD was launched in 1972 by Sharp Inc with the Sharp \"EL-805\", which was a slim pocket calculator. This, and another few similar models, used Sharp's \"Calculator On Substrate\" (COS) technology. An extension of one glass plate needed for the liquid crystal display was used as a substrate to mount the needed chips based on a new hybrid technology. The COS technology may have been too costly since it was only used in a few models before Sharp reverted to conventional circuit boards.\nIn the mid-1970s the first calculators appeared with field-effect, \"twisted nematic\" (TN) LCDs with dark numerals against a grey background, though the early ones often had a yellow filter over them to cut out damaging ultraviolet rays. The advantage of LCDs is that they are passive light modulators reflecting light, which require much less power than light-emitting displays such as LEDs or VFDs. This led the way to the first credit-card-sized calculators, such as the Casio \"Mini Card LC-78\" of 1978, which could run for months of normal use on button cells.\n\nThere were also improvements to the electronics inside the calculators. All of the logic functions of a calculator had been squeezed into the first \"calculator on a chip\" integrated circuits (ICs) in 1971, but this was leading edge technology of the time and yields were low and costs were high. Many calculators continued to use two or more ICs, especially the scientific and the programmable ones, into the late 1970s.\n\nThe power consumption of the integrated circuits was also reduced, especially with the introduction of CMOS technology. Appearing in the Sharp \"EL-801\" in 1972, the transistors in the logic cells of CMOS ICs only used any appreciable power when they changed state. The LED and VFD displays often required added driver transistors or ICs, whereas the LCDs were more amenable to being driven directly by the calculator IC itself.\n\nWith this low power consumption came the possibility of using solar cells as the power source, realised around 1978 by calculators such as the Royal \"Solar 1\", Sharp \"EL-8026\", and Teal \"Photon\".\n\nAt the start of the 1970s, hand-held electronic calculators were very costly, at two or three weeks' wages, and so were a luxury item. The high price was due to their construction requiring many mechanical and electronic components which were costly to produce, and production runs that were too small to exploit economies of scale. Many firms saw that there were good profits to be made in the calculator business with the margin on such high prices. However, the cost of calculators fell as components and their production methods improved, and the effect of economies of scale was felt.\n\nBy 1976, the cost of the cheapest four-function pocket calculator had dropped to a few dollars, about 1/20th of the cost five years before. The results of this were that the pocket calculator was affordable, and that it was now difficult for the manufacturers to make a profit from calculators, leading to many firms dropping out of the business or closing down. The firms that survived making calculators tended to be those with high outputs of higher quality calculators, or producing high-specification scientific and programmable calculators.\n\nThe first calculator capable of symbolic computing was the HP-28C, released in 1987. It could, for example, solve quadratic equations symbolically. The first graphing calculator was the Casio fx-7000G released in 1985.\n\nThe two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.\n\nThe HP 12c financial calculator is still produced. It was introduced in 1981 and is still being made with few changes. The HP 12c featured the reverse Polish notation mode of data entry. In 2003 several new models were released, including an improved version of the HP 12c, the \"HP 12c platinum edition\" which added more memory, more built-in functions, and the addition of the algebraic mode of data entry.\n\nCalculated Industries competed with the HP 12c in the mortgage and real estate markets by differentiating the key labeling; changing the “I”, “PV”, “FV” to easier labeling terms such as \"Int\", \"Term\", \"Pmt\", and not using the reverse Polish notation. However, CI's more successful calculators involved a line of construction calculators, which evolved and expanded in the 1990s to present. According to Mark Bollman, a mathematics and calculator historian and associate professor of mathematics at Albion College, the \"Construction Master is the first in a long and profitable line of CI construction calculators\" which carried them through the 1980s, 1990s, and to the present.\n\nPersonal computers often come with a calculator utility program that emulates the appearance and functions of a calculator, using the graphical user interface to portray a calculator. One such example is Windows Calculator. Most personal data assistants (PDAs) and smartphones also have such a feature.\n\nIn most countries, students use calculators for schoolwork. There was some initial resistance to the idea out of fear that basic or elementary arithmetic skills would suffer. There remains disagreement about the importance of the ability to perform calculations \"in the head\", with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching estimation methods and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become \"too dependent\" on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum. In the United States, many math educators and boards of education enthusiastically endorsed the National Council of Teachers of Mathematics (NCTM) standards and actively promoted the use of classroom calculators from kindergarten through high school.\n\n\n\n\n"}
{"id": "30176598", "url": "https://en.wikipedia.org/wiki?curid=30176598", "title": "Charles Haros", "text": "Charles Haros\n\nCharles Haros was a geometer (mathematician) in the French Bureau du Cadastre at the end of the eighteenth century and the beginning of the nineteenth century.\n\nOne of the primary tasks of the Bureau du Cadastre was the accurate mapping of France for the purpose of taxation but from time to time the bureau also provided computational services to other parts of the government.\n\nOne of the changes instituted by the French revolution was to convert France to the metric system and this necessitated changing from a fractional to a decimal representation of rational numbers. While Haros was involved many computation projects at the Bureau du Cadastre including the computation of de Prony’s tables of logarithms and the preparation of the French ephemeris, Connaissance des Temps, he is best known for a small table he prepared to convert fractions to their decimal equivalents.\n\nHaros’ conversion table appeared in a tract, \"Instruction Abrégée sur les nouvelles Mesures qui dovient étre introduites dans toute république, au vendémiaire an 10; avec tables de rapports et reductions\", that was presented to the Mathematics Section of the Institut de France and subsequently abstracted in Journal de l'École Polytechnique under the title ‘’Tables pour évaluer une fraction ordinaire avec autant de decimals qu’on voudra; et pour trouver la fraction ordinaire la plus simple, et qui approche sensiblement d’une fraction décimale.‘’\n\nIn preparing his table Haros needed to create the list of all 3,003 irreducible (vulgar) fractions with denominators less than 100. In order to make sure he got them all he harnessed an algorithm elucidated by Nicolas Chuquet some one-hundred and fifty years earlier. Chuquet called it his ‘’règle des nombres moyens‘’. Today we call it the mediant. The mediant is the fraction between two fractions a/c and b/d whose numerator is the sum of the numerators, a+b, and whose denominator is the sum of the denominators, c+d. That is, the mediant of the fractions a/c and b/d is the fraction (a+b)/(c+d).\n\nIn his paper Haros demonstrated that the mediant is always irreducible and, more importantly for this purposes, if you start with the sequence of fractions\n\nand just keep applying the rule, only keeping the result if the denominator is less than one-hundred, then you generate all 3,003.\n\nRoughly fifteen years later in England, Henry Goodwyn set out to create a much more ambitious version of Haros’ table. In particular, Goodwyn wanted to tabulate the decimal values for all irreducible fractions with denominators less than or equal to 1,024. There are 318,963 such fractions. As a warm up and a test of the commercial market for such a table in 1816 he published for private circulation The First Centenary of a Series of Concise and Useful Tables of all the Complete Decimal Quotients, which can arise from dividing a unit, or any whole Number less than each Divisor by all Integers from 1 to 1024.\n\nJohn Farey observed the mediant property in this table and mused in a letter to The Philosophical Magazine and Journal as follows:\n\nAugustin Cauchy read Farey’s letter and published a paper “Démonstration d’un Théorème Curieux sur les Nombres” reproving Haros’ results without acknowledgement. In his paper Cauchy referred to the mediant as “a remarkable property of ordinary fractions observed by M. J. Farey.” Thus, an ordered sequence of all vulgar fractions with denominators less than a given value became known as a Farey sequence rather than perhaps more rightfully as either a Chuquet sequence or a Haros sequence.\n\n\n\n\n"}
{"id": "138484", "url": "https://en.wikipedia.org/wiki?curid=138484", "title": "Commutative diagram", "text": "Commutative diagram\n\nIn mathematics, and especially in category theory, a commutative diagram is a diagram such that all directed paths in the diagram with the same start and endpoints lead to the same result. Commutative diagrams play the role in category theory that equations play in algebra (see Barr–Wells, Section 1.7).\nParts of the diagram:\nIn algebra texts, the type of morphism can be denoted with different arrow usages: \n\nThese conventions are common enough that texts often do not explain the meanings of the different types of arrow.\n\nCommutativity makes sense for a polygon of any finite number of sides (including just 1 or 2), and a diagram is commutative if every polygonal subdiagram is commutative.\n\nNote that a diagram may be non-commutative, i.e., the composition of different paths in the diagram may not give the same result.\n\nPhrases like \"this commutative diagram\" or \"the diagram commutes\" may be used.\n\nIn the bottom-left diagram, which expresses the first isomorphism theorem, commutativity means that formula_7 while in the bottom-right diagram, commutativity of the square means formula_8:\nFor the diagram below to commute, we must have the three equalities: (1) formula_9 (2) formula_10 and (3) formula_11. \nSince the first equality follows from the last two, for the diagram to commute it suffices to show (2) and (3). However, since equality (3) does not generally follow from the other two equalities, for this diagram to commute it is generally not enough to only have equalities (1) and (2).\n\nDiagram chasing (also called diagrammatic search) is a method of mathematical proof used especially in homological algebra. Given a commutative diagram, a proof by diagram chasing involves the formal use of the properties of the diagram, such as injective or surjective maps, or exact sequences. A syllogism is constructed, for which the graphical display of the diagram is just a visual aid. It follows that one ends up \"chasing\" elements around the diagram, until the desired element or result is constructed or verified.\n\nExamples of proofs by diagram chasing include those typically given for the five lemma, the snake lemma, the zig-zag lemma, and the nine lemma.\n\nIn higher category theory, one considers not only objects and arrows, but arrows between the arrows, arrows between arrows between arrows, and so on ad infinitum. For example, the category of small categories Cat is naturally a 2-category, with functors as its arrows and natural transformations as the arrows between functors. In this setting, commutative diagrams may include these higher arrows as well, which are often depicted in the following style: formula_12. For example, the following (somewhat trivial) diagram depicts two categories ' and ', together with two functors , : ' → ' and a natural transformation : ⇒ :\n\nThere are two kinds of composition in a 2-category (called vertical composition and horizontal composition), and they may also be depicted via pasting diagrams, see 2-category#Definition for examples.\n\nA commutative diagram in a category \"C\" can be interpreted as a functor from an index category \"J\" to \"C;\" one calls the functor a diagram.\n\nMore formally, a commutative diagram is a visualization of a diagram indexed by a poset category:\n\nConversely, given a commutative diagram, it defines a poset category:\n\nHowever, not every diagram commutes (the notion of diagram strictly generalizes commutative diagram): most simply, the diagram of a single object with an endomorphism (formula_13), or with two parallel arrows (formula_14, that is, formula_15, sometimes called the free quiver), as used in the definition of equalizer need not commute. Further, diagrams may be messy or impossible to draw when the number of objects or morphisms is large (or even infinite).\n\n\n\n"}
{"id": "2815048", "url": "https://en.wikipedia.org/wiki?curid=2815048", "title": "Computational model", "text": "Computational model\n\nA computational model is a mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.\n\nThe system under study is often a complex nonlinear system for which simple, intuitive analytical solutions are not readily available. Rather than deriving a mathematical analytical solution to the problem, experimentation with the model is done by adjusting the parameters of the system in the computer, and studying the differences in the outcome of the experiments. Operation theories of the model can be derived/deduced from these computational experiments.\n\nExamples of common computational models are weather forecasting models, earth simulator models, flight simulator models, molecular protein folding models, and neural network models.\n\n"}
{"id": "2840305", "url": "https://en.wikipedia.org/wiki?curid=2840305", "title": "Computer-assisted proof", "text": "Computer-assisted proof\n\nA computer-assisted proof is a mathematical proof that has been at least partially generated by computer.\n\nMost computer-aided proofs to date have been implementations of large proofs-by-exhaustion of a mathematical theorem. The idea is to use a computer program to perform lengthy computations, and to provide a proof that the result of these computations implies the given theorem. In 1976, the four color theorem was the first major theorem to be verified using a computer program.\n\nAttempts have also been made in the area of artificial intelligence research to create smaller, explicit, new proofs of mathematical theorems from the bottom up using machine reasoning techniques such as heuristic search. Such automated theorem provers have proved a number of new results and found new proofs for known theorems. Additionally, interactive proof assistants allow mathematicians to develop human-readable proofs which are nonetheless formally verified for correctness. Since these proofs are generally human-surveyable (albeit with difficulty, as with the proof of the Robbins conjecture) they do not share the controversial implications of computer-aided proofs-by-exhaustion.\n\nOne method for using computers in mathematical proofs is by means of so-called validated numerics or rigorous numerics. This means computing numerically yet with mathematical rigour. One uses set-valued arithmetic and inclusion principle in order to ensure that the set-valued output of a numerical program encloses the solution of the original mathematical problem. This is done by controlling, enclosing and propagating round-off and truncation errors using for example interval arithmetic. More precisely, one reduces the computation to a sequence of elementary operations, say formula_1. In a computer, the result of each elementary operation is rounded off by the computer precision. However, one can construct an interval provided by upper and lower bounds on the result of an elementary operation. Then one proceeds by replacing numbers with intervals and performing elementary operations between such intervals of representable numbers.\n\nComputer-assisted proofs are the subject of some controversy in the mathematical world, with Thomas Tymoczko first to articulate objections. Those who adhere to Tymoczko's arguments believe that lengthy computer-assisted proofs are not, in some sense, 'real' mathematical proofs because they involve so many logical steps that they are not practically verifiable by human beings, and that mathematicians are effectively being asked to replace logical deduction from assumed axioms with trust in an empirical computational process, which is potentially affected by errors in the computer program, as well as defects in the runtime environment and hardware.\n\nOther mathematicians believe that lengthy computer-assisted proofs should be regarded as \"calculations\", rather than \"proofs\": the proof algorithm itself should be proved valid, so that its use can then be regarded as a mere \"verification\". Arguments that computer-assisted proofs are subject to errors in their source programs, compilers, and hardware can be resolved by providing a formal proof of correctness for the computer program (an approach which was successfully applied to the four-color theorem in 2005) as well as replicating the result using different programming languages, different compilers, and different computer hardware.\n\nAnother possible way of verifying computer-aided proofs is to generate their reasoning steps in a machine-readable form, and then use an automated theorem prover to demonstrate their correctness. This approach of using a computer program to prove another program correct does not appeal to computer proof skeptics, who see it as adding another layer of complexity without addressing the perceived need for human understanding.\n\nAnother argument against computer-aided proofs is that they lack mathematical elegance—that they provide no insights or new and useful concepts. In fact, this is an argument that could be advanced against any lengthy proof by exhaustion.\n\nAn additional philosophical issue raised by computer-aided proofs is whether they make mathematics into a quasi-empirical science, where the scientific method becomes more important than the application of pure reason in the area of abstract mathematical concepts. This directly relates to the argument within mathematics as to whether mathematics is based on ideas, or \"merely\" an exercise in formal symbol manipulation. It also raises the question whether, if according to the Platonist view, all possible mathematical objects in some sense \"already exist\", whether computer-aided mathematics is an observational science like astronomy, rather than an experimental one like physics or chemistry. This controversy within mathematics is occurring at the same time as questions are being asked in the physics community about whether twenty-first century theoretical physics is becoming too mathematical, and leaving behind its experimental roots.\n\nThe emerging field of experimental mathematics is confronting this debate head-on by focusing on numerical experiments as its main tool for mathematical exploration.\n\nIn 2010, academics at The University of Edinburgh offered people the chance to \"buy their own theorem\" created through a computer-assisted proof. This new theorem would be named after the purchaser.\n\nInclusion in this list does not imply that a formal computer-checked proof exists, but rather, that a computer program has been involved in some way. See the main articles for details.\n\n\n\n"}
{"id": "5021705", "url": "https://en.wikipedia.org/wiki?curid=5021705", "title": "Conway polyhedron notation", "text": "Conway polyhedron notation\n\nIn geometry, Conway polyhedron notation, invented by John Horton Conway and promoted by George W. Hart, is used to describe polyhedra based on a seed polyhedron modified by various prefix operations.\n\nConway and Hart extended the idea of using operators, like truncation as defined by Kepler, to build related polyhedra of the same symmetry. For example, \"tC\" represents a truncated cube, and \"taC\", parsed as , is a truncated cuboctahedron. The simplest operator dual swaps vertex and face elements; e.g., a dual cube is an octahedron: \"dC\"=\"O\". Applied in a series, these operators allow many higher order polyhedra to be generated. Conway defined the operators \"abdegjkmost\", while Hart added \"r\" and \"p\". Conway's basic operations are sufficient to generate the Archimedean and Catalan solids from the Platonic solids. Some basic operations can be made as composites of others. Later implementations named further operators, sometimes referred to as \"extended\" operators.\n\nIn general, it is difficult to predict the resulting appearance of the composite of two or more operations from a given seed polyhedron. For instance, ambo applied twice is the expand operation: \"aa\" = \"e\", while a truncation after ambo produces bevel: \"ta\" = \"b\". Many basic questions about Conway operators remain open, for instance, how many operators of a given \"size\" exist.\n\nIn Conway's notation, operations on polyhedra are applied like functions, from right to left. For example, a cuboctahedron is an \"ambo cube\", i.e. , and a truncated cuboctahedron is . Repeated application of an operator can be denoted with an exponent: \"j\" = \"o\". In general, Conway operators are not commutative. The resulting polyhedron has a fixed topology (vertices, edges, faces), while exact geometry is not specified: it can be thought of as one of many embeddings of a polyhedral graph on the sphere. Often the polyhedron is put into canonical form.\n\nIndividual operators can be visualized in terms of \"chambers\", as below. Each white chamber is a rotated version of the others. For achiral operators, the red chambers are a reflection of the white chambers. Achiral and chiral operators are also called local symmetry-preserving operations (LSP) and local operations that preserve orientation-preserving symmetries (LOPSP), respectively, although the exact definition is a little more restrictive.\n\nThe relationship between the number of vertices, edges, and faces of the seed and the polyhedron created by the operations listed in this article can be expressed as a matrix formula_1. When \"x\" is the operator, formula_2 are the vertices, edges, and faces of the seed (respectively), and formula_3 are the vertices, edges, and faces of the result, then \n\nThe matrix for the composition of two operators is just the product of the matrixes for the two operators. Distinct operators may have the same matrix, for example, \"p\" and \"l\". The edge count of the result is an integer multiple \"d\" of that of the seed: this is called the inflation rate, or the edge factor.\n\nThe simplest operators, the identity operator \"S\" and the dual operator \"d\", have simple matrix forms:\nTwo dual operators cancel out; \"dd\" = \"S\", and the square of formula_7 is the identity matrix. When applied to other operators, the dual operator corresponds to horizontal and vertical reflections of the matrix. Operators can be grouped into groups of four (or fewer if some forms are the same) by identifying the operators \"x\", \"xd\" (operator of dual), \"dx\" (dual of operator), and \"dxd\" (conjugate of operator). In this article, only the matrix for \"x\" is given, since the others are simple reflections.\n\nHart introduced the reflection operator \"r\", that gives the mirror image of the polyhedron. This is not strictly a LOPSP, since it does not preserve orientation (it reverses it). \"r\" has no effect on achiral seeds, and \"rr\" returns the original seed. An overline can be used to indicate the other chiral form of an operator, like = \"rsr\". \"r\" does not affect the matrix.\n\nAn operation is irreducible if it cannot be expressed as a composition of operators aside from \"d\" and \"r\". The majority of Conway's original operators are irreducible: the exceptions are \"e\", \"b\", \"o\", and \"m\".\n\nSome open questions about Conway operators include:\n\nStrictly, seed (\"S\"), needle (\"n\"), and zip (\"z\") were not included by Conway, but they are related to original Conway operations by duality so are included here.\n\nFrom here on, operations are visualized on cube seeds, drawn on the surface of that cube. Blue faces cross edges of the seed, and pink faces lie over vertices of the seed. There is some flexibility in the exact placement of vertices, especially with chiral operators. \n\nAny polyhedron can serve as a seed, as long as the operations can be executed on it. Common seeds have been assigned a letter.\nThe Platonic solids are represented by the first letter of their name (Tetrahedron, Octahedron, Cube, Icosahedron, Dodecahedron); the prisms (P) for \"n\"-gonal forms; antiprisms (A); cupolae (U); anticupolae (V); and pyramids (Y). Any Johnson solid can be referenced as J, for \"n\"=1..92.\n\nAll of the five regular polyhedra can be generated from prismatic generators with zero to two operators:\n\nThe regular Euclidean tilings can also be used as seeds:\n\nThese are operations created after Conway's original set. Note that many more operations exist than have been named; just because an operation is not here does not mean it does not exist (or is not an LSP or LOPSP). To simplify, only irreducible operators are \nincluded in this list: others can be created by composing operators together.\n\nA number of operators can be grouped together by some criteria, or have their behavior modified by an index. These are written as an operator with a subscript: \"x\".\n\nAugmentation operations retain original edges. They may be applied to any independent subset of faces, or may be converted into a \"join\"-form by removing the original edges. Conway notation supports an optional index to these operators: 0 for the join-form, or 3 or higher for how many sides affected faces have. For example, \"k\"\"Y\"=O: taking a square-based pyramid and gluing another pyramid to the square base gives an octahedron.\nThe truncate operator \"t\" also has an index form \"t\", indicating that only vertices of a certain degree are truncated. It is equivalent to \"dkd\".\n\nSome of the extended operators can be created in special cases with \"k\" and \"t\" operators. For example, a chamfered cube, \"cC\", can be constructed as \"t\"\"daC\", as a rhombic dodecahedron, \"daC\" or \"jC\", with its degree-4 vertices truncated. A lofted cube, \"lC\" is the same as \"t\"\"kC\". A quinto-dodecahedron, \"qD\" can be constructed as \"t\"\"daaD\" or \"t\"\"deD\" or \"t\"\"oD\", a deltoidal hexecontahedron, \"deD\" or \"oD\", with its degree-5 vertices truncated.\n\nMeta adds vertices at the center and along the edges, while bevel adds faces at the center, seed vertices, and along the edges. The index is how many vertices or faces are added along the edges. Meta (in its non-indexed form) is also called cantitruncation or omnitruncation. Note that 0 here does not mean the same as for augmentation operations: it means zero vertices (or faces) are added along the edges.\nMedial is like meta, except it does not add edges from the center to each seed vertex. The index 1 form is identical to Conway's ortho and expand operators: expand is also called cantellation and expansion. Note that \"o\" and \"e\" have their own indexed forms, described below. Also note that some implementations start indexing at 0 instead of 1.\nThe Goldberg-Coxeter (GC) Conway operators are two infinite families of operators that are an extension of the Goldberg-Coxeter construction. The GC construction can be thought of as taking a triangular section of a triangular lattice, or a square section of a square lattice, and laying that over each face of the polyhedron. This construction can be extended to any face by identifying the chambers of the triangle or square (the \"master polygon\"). Operators in the triangular family can be used to produce the Goldberg polyhedra and geodesic polyhedra: see List of geodesic polyhedra and Goldberg polyhedra for formulas.\n\nThe two families are the triangular GC family, \"c\" and \"u\", and the quadrilateral GC family, \"e\" and \"o\". Both the GC families are indexed by two integers formula_8 and formula_9. They possess many nice qualities:\n\nThe operators are divided into three classes (examples are written in terms of \"c\" but apply to all 4 operators):\n\nOf the original Conway operations, the only ones that do not fall into the GC family are \"g\" and \"s\" (gyro and snub). Meta and bevel (\"m\" and \"b\") can be expressed in terms of one operator from the triangular family and one from the quadrilateral family.\n\nBy basic number theory, for any values of \"a\" and \"b\", formula_10.\n\nSee also List of geodesic polyhedra and Goldberg polyhedra.\n\nConway's original set of operators can create all of the Archimedean solids and Catalan solids, using the Platonic solids as seeds. (Note that the \"r\" operator is not necessary to create both chiral forms.)\n\nThe truncated icosahedron, \"tI = zD\", can be used as a seed to create some more visually-pleasing polyhedra, although these are neither vertex nor face-transitive.\n\nEach of the convex uniform tilings can be created by applying Conway operators to the regular tilings Q, H, and Δ.\n\nConway operators can also be applied to toroidal polyhedra and polyhedra with multiple holes.\n\n\n"}
{"id": "9318685", "url": "https://en.wikipedia.org/wiki?curid=9318685", "title": "Elementary proof", "text": "Elementary proof\n\nIn mathematics, an elementary proof is a mathematical proof that only uses basic techniques. More specifically, the term is used in number theory to refer to proofs that make no use of complex analysis. For some time it was thought that certain theorems, like the prime number theorem, could only be proved using \"higher\" mathematics. However, over time, many of these results have been reproved using only elementary techniques.\n\nWhile the meaning has not always been defined precisely, the term is commonly used in mathematical jargon. An elementary proof is not necessarily simple, in the sense of being easy to understand: some elementary proofs can be quite complicated.\n\nThe distinction between elementary and non-elementary proofs has been considered especially important in regard to the prime number theorem. This theorem was first proved in 1896 by Jacques Hadamard and Charles Jean de la Vallée-Poussin using complex analysis. Many mathematicians then attempted to construct elementary proofs of the theorem, without success. G. H. Hardy expressed strong reservations; he considered that the essential \"depth\" of the result ruled out elementary proofs:\n\nHowever, in 1948, Atle Selberg produced new methods which led him and Paul Erdős to find elementary proofs of the prime number theorem. \n\nA possible formalization of the notion of \"elementary\" in connection to a proof of a number-theoretical result is the restriction that the proof can be carried out in Peano arithmetic. Also in that sense, these proofs are elementary.\n\nHarvey Friedman conjectured, \"Every theorem published in the \"Annals of Mathematics\" whose statement involves only finitary mathematical objects (i.e., what logicians call an arithmetical statement) can be proved in elementary arithmetic.\" The form of elementary arithmetic referred to in this conjecture can be formalized by a small set of axioms concerning integer arithmetic and mathematical induction. For instance, according to this conjecture, Fermat's Last Theorem should have an elementary proof; Wiles' proof of Fermat's Last Theorem is not elementary. However, there are other simple statements about arithmetic such as the existence of iterated exponential functions that cannot be proven in this theory.\n"}
{"id": "6694084", "url": "https://en.wikipedia.org/wiki?curid=6694084", "title": "Fielden Professor of Pure Mathematics", "text": "Fielden Professor of Pure Mathematics\n\nThe Fielden Chair of Pure Mathematics is an endowed professorial position in the School of Mathematics, University of Manchester, England. \n\nIn 1870 Samuel Fielden, a wealthy mill owner from Todmorden, donated £150 to Owens College (as the Victoria University of Manchester was then called) for the teaching of evening classes and a further £3000 for the development of natural sciences at the college. From 1877 this supported the Fielden Lecturer, subsequently to become the Fielden Reader with the appointment of L. J. Mordell in 1922 and then Fielden Professor in 1923. Alex Wilkie FRS was appointed to the post in 2007. Previous holders of the Fielden Chair (and lectureship) are:\n\n\nThe other endowed chairs in mathematics at the University of Manchester are the Beyer Chair of Applied Mathematics, the Sir Horace Lamb Chair and the Richardson Chair of Applied Mathematics.\n"}
{"id": "49336576", "url": "https://en.wikipedia.org/wiki?curid=49336576", "title": "Global Digital Mathematics Library", "text": "Global Digital Mathematics Library\n\nThe Global Digital Mathematics Library (GDML) is a project organized under the auspices of the International Mathematical Union (IMU) to establish a digital library focused on mathematics.\n\nA working group was convened in September 2014, following the 2014 International Congress of Mathematicians, by former IMU President Ingrid Daubechies and Chair Peter J. Olver of the IMU’s Committee on Electronic Information and Communication (CEIC). Currently the working group has eight members, namely:\n\n\nIn the spring of 2014, the Committee on Planning a Global Library of the Mathematical Sciences released a comprehensive study entitled “Developing a 21st Century Global Library for Mathematics Research.” This report states in its Strategic Plan section, “There is a compelling argument that through a combination of machine learning methods and editorial effort by both paid and volunteer editors, a significant portion of the information and knowledge in the global mathematical corpus could be made available to researchers as linked open data through the GDML.\"\n\nA workshop titled \"Semantic Representation of Mathematical Knowledge\" was held at the Fields Institute in Toronto during February 3–5, 2016. The goal of the workshop was to lay down the foundations of a prototype semantic representation language for the GDML. The workshop's organizers recognized that the extremely wide scope of mathematics as a whole made it unrealistic to map out the detailed concepts, structures, and operations needed and used in individual mathematical subjects. The workshop therefore limited itself to surveys of the status quo in mathematical representation languages including representation of prominent and fundamental theorems in certain areas that could serve as building blocks for additional mathematical results, and to discussing ways to best identify and design semantic components for individual disciplines of mathematics.\n\nThe workshop organizers are presently preparing a report summarizing the workshop's conclusions and making recommendations for further progress towards a GDML.\n"}
{"id": "1714764", "url": "https://en.wikipedia.org/wiki?curid=1714764", "title": "Harmonic (mathematics)", "text": "Harmonic (mathematics)\n\nIn mathematics, a number of concepts employ the word harmonic. The similarity of this terminology to that of music is not accidental: the equations of motion of vibrating strings, drums and columns of air are given by formulas involving Laplacians; the solutions to which are given by eigenvalues corresponding to their modes of vibration. Thus, the term \"harmonic\" is applied when one is considering functions with sinusoidal variations, or solutions of Laplace's equation and related concepts.\n\n"}
{"id": "945503", "url": "https://en.wikipedia.org/wiki?curid=945503", "title": "Hellenic Mathematical Society", "text": "Hellenic Mathematical Society\n\nThe Hellenic Mathematical Society (HMS) (Greek: Ελληνική Μαθηματική Εταιρεία) is a learned society which promotes the study of mathematics in Greece. It was founded in 1918, and published the \"Bulletin of the Greek Mathematical Society\".\n\nIt is a member of the European Mathematical Society. \n\n\n\n"}
{"id": "9479849", "url": "https://en.wikipedia.org/wiki?curid=9479849", "title": "History of manifolds and varieties", "text": "History of manifolds and varieties\n\nThe study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology. Certain special classes of manifolds also have additional algebraic structure; they may behave like groups, for instance. In that case, they are called Lie Groups. Alternatively, they may be described by polynomial equations, in which case they are called algebraic varieties, and if they additionally carry a group structure, they are called algebraic groups.\nThe term \"manifold\" comes from German \"Mannigfaltigkeit,\" by Riemann.\n\nIn English, \"manifold\" refers to spaces with a differentiable or topological structure, \nwhile \"variety\" refers to spaces with an algebraic structure, as in algebraic varieties.\n\nIn Romance languages, manifold is translated as \"variety\" – such spaces with a differentiable structure are literally translated as \"analytic varieties\", while spaces with an algebraic structure are called \"algebraic varieties\". Thus for example, the french word \"\" means topological manifold. In the same vein, the Japanese word \" (tayōtai) also encompasses both manifold and variety. (\" (tayō) means various.)\nAncestral to the modern concept of a manifold were several important results of 18th and 19th century mathematics. The oldest of these was Non-Euclidean geometry, which considers spaces where Euclid's parallel postulate fails. Saccheri first studied this geometry in 1733. Lobachevsky, Bolyai, and Riemann developed the subject further 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these are called hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to manifolds with constant, negative and positive curvature, respectively.\n\nCarl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right. His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies. In modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.\n\nAnother, more topological example of an intrinsic property of a manifold is the Euler characteristic. For a non-intersecting graph in the Euclidean plane, with \"V\" vertices (or corners), \"E\" edges and \"F\" faces (counting the exterior) Euler showed that \"V\"-\"E\"+\"F\"= 2. Thus 2 is called the Euler characteristic of the plane. By contrast, in 1813 Antoine-Jean Lhuilier showed that the Euler characteristic of the torus is 0, since the complete graph on seven points can be embedded into the torus. The Euler characteristic of other surfaces is a useful topological invariant, which has been extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gauss–Bonnet theorem linked the Euler characteristic to the Gaussian curvature.\n\nLagrangian mechanics and Hamiltonian mechanics, when considered geometrically, are naturally manifold theories. All these use the notion of several characteristic axes or dimensions (known as generalized coordinates in the latter two cases), but these dimensions do not lie along the physical dimensions of width, height, and breadth.\n\nIn the early 19th century the theory of elliptic functions succeeded in giving a basis for the theory of elliptic integrals, and this left open an obvious avenue of research. The standard forms for elliptic integrals involved the square roots of cubic and quartic polynomials. When those were replaced by polynomials of higher degree, say quintics, what would happen? \n\nIn the work of Niels Abel and Carl Jacobi, the answer was formulated: the resulting integral would involve functions of two complex variables, having four independent \"periods\" (i.e. period vectors). This gave the first glimpse of an abelian variety of dimension 2 (an abelian surface): what would now be called the \"Jacobian of a hyperelliptic curve of genus 2\".\n\nBernhard Riemann was the first to do extensive work generalizing the idea of a surface to higher dimensions. The name \"manifold\" comes from Riemann's original German term, \"Mannigfaltigkeit\", which William Kingdon Clifford translated as \"manifoldness\". In his Göttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a \"Mannigfaltigkeit\", because the variable can have \"many\" values. He distinguishes between \"stetige Mannigfaltigkeit\" and \"diskrete\" \"Mannigfaltigkeit\" (\"continuous manifoldness\" and \"discontinuous manifoldness\"), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an \"n-fach ausgedehnte Mannigfaltigkeit\" (\"n times extended manifoldness\" or \"n-dimensional manifoldness\") as a continuous stack of (n−1) dimensional manifoldnesses. Riemann's intuitive notion of a \"Mannigfaltigkeit\" evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Bernhard Riemann.\n\nIn 1857, Riemann introduced the concept of Riemann surfaces as part of a study of the process of analytic continuation; Riemann surfaces are now recognized as one-dimensional complex manifolds. He also furthered the study of abelian and other multi-variable complex functions.\n\nJohann Benedict Listing, inventor of the word \"topology\", wrote an 1847 paper \"Vorstudien zur Topologie\" in which he defined a \"complex\". He first defined the Möbius strip in 1861 (rediscovered four years later by Möbius), as an example of a non-orientable surface.\n\nAfter Abel, Jacobi, and Riemann, some of the most important contributors to the theory of abelian functions were Weierstrass, Frobenius, Poincaré and Picard. The subject was very popular at the time, already having a large literature. By the end of the 19th century, mathematicians had begun to use geometric methods in the study of abelian functions.\n\nHenri Poincaré's 1895 paper Analysis Situs studied three-and-higher-dimensional manifolds(which he called \"varieties\"), giving rigorous definitions of homology, homotopy, and Betti numbers and raised a question, today known as the Poincaré conjecture, based his new concept of the fundamental group. In 2003, Grigori Perelman proved the conjecture using Richard S. Hamilton's Ricci flow, this is after nearly a century of effort by many mathematicians.\n\nHermann Weyl gave an intrinsic definition for differentiable manifolds in 1912. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory.\n\nThe Whitney embedding theorem showed that manifolds intrinsically defined by charts could always be embedded in Euclidean space, as in the extrinsic definition, showing that the two concepts of manifold were equivalent. Due to this unification, it is said to be the first complete exposition of the modern concept of manifold.\n\nEventually, in the 1920s, Lefschetz laid the basis for the study of abelian functions in terms of complex tori. He also appears to have been the first to use the name \"abelian variety\"; in Romance languages, \"variety\" was used to translate Riemann's term \"Mannigfaltigkeit\". It was Weil in the 1940s who gave this subject its modern foundations in the language of algebraic geometry.\n\n"}
{"id": "1047173", "url": "https://en.wikipedia.org/wiki?curid=1047173", "title": "Integrator", "text": "Integrator\n\nAn integrator in measurement and control applications is an element whose output signal is the time integral of its input signal. It accumulates the input quantity over a defined time to produce a representative output.\n\nIntegration is an important part of many engineering and scientific applications. Mechanical integrators are the oldest application, and are still used in such as metering of water flow or electric power. Electronic analogue integrators are the basis of analog computers and charge amplifiers. Integration is also performed by digital computing algorithms.\n\nAn electronic integrator is a form of first-order low-pass filter, which can be performed in the continuous-time (analog) domain or approximated (simulated) in the discrete-time (digital) domain. An integrator will have a low pass filtering effect but when given an offset it will accumulate a value building it until it reaches a limit of the system or overflows.\n\nA \"voltage integrator\" is an electronic device performing a time integration of an electric voltage, thus measuring the total volt-second product.\n\nA \"current integrator\" is an electronic device performing a time integration of an electric current, thus measuring a total electric charge. A charge amplifier is an example of current integrator. A current integrator is also used to measure the electric charge on a Faraday cup in a residual gas analyzer to measure partial pressures of gasses in a vacuum. Another application of current integration is in ion beam deposition, where the measured charge directly corresponds to the number of ions deposited on a substrate, assuming the charge state of the ions is known. The two current-carrying electrical leads must to be connected to the ion source and the substrate, closing the electric circuit which in part is given by the ion beam.\n\n\nMechanical integrators were key elements in the mechanical differential analyser, used to solve practical physical problems. Mechanical integration mechanisms were also used in control systems such as regulating flows or temperature in industrial processes. Mechanisms such as the ball-and-disk integrator were used both for computation in differential analysers and as components of instruments such as naval gun directors, flow totalizers and others. A planimeter is a mechanical device used for calculating the definite integral of a curve given in graphical form, or more generally finding the area of a closed curve. An integraph is used to plot the indefinite integral of a function given in graphical form.\n\n\nThe gain of an integrator at low frequency can be limited to avoid the saturation problem if the feedback capacitor is shunted by a resistor Rf. The parallel combination of Rf and C behaves like a practical capacitor which dissipates power, unlike an ideal capacitor. For this reason this circuit is also called a lossy integrator. The resistor Rf limits the low frequency gain to (-Rf/R), generally [Rf=10*R1] and thus provides DC stabilisation.\n\n\n\n"}
{"id": "157055", "url": "https://en.wikipedia.org/wiki?curid=157055", "title": "Law of large numbers", "text": "Law of large numbers\n\nIn probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.\n\nThe LLN is important because it guarantees stable long-term results for the averages of some random events. For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the parameters of the game. It is important to remember that the law only applies (as the name indicates) when a \"large number\" of observations is considered. There is no principle that a small number of observations will coincide with the expected value or that a streak of one value will immediately be \"balanced\" by the others (see the gambler's fallacy).\n\nFor example, a single roll of a fair, six-sided die produces one of the numbers 1, 2, 3, 4, 5, or 6, each with equal probability. Therefore, the expected value of a single die roll is\nAccording to the law of large numbers, if a large number of six-sided dice are rolled, the average of their values (sometimes called the sample mean) is likely to be close to 3.5, with the precision increasing as more dice are rolled.\n\nIt follows from the law of large numbers that the empirical probability of success in a series of Bernoulli trials will converge to the theoretical probability. For a Bernoulli random variable, the expected value is the theoretical probability of success, and the average of \"n\" such variables (assuming they are independent and identically distributed (i.i.d.)) is precisely the relative frequency.\n\nFor example, a fair coin toss is a Bernoulli trial. When a fair coin is flipped once, the theoretical probability that the outcome will be heads is equal to 1/2. Therefore, according to the law of large numbers, the proportion of heads in a \"large\" number of coin flips \"should be\" roughly 1/2. In particular, the proportion of heads after \"n\" flips will almost surely converge to 1/2 as \"n\" approaches infinity.\n\nAlthough the proportion of heads (and tails) approaches 1/2, almost surely the absolute difference in the number of heads and tails will become large as the number of flips becomes large. That is, the probability that the absolute difference is a small number, approaches zero as the number of flips becomes large. Also, almost surely the ratio of the absolute difference to the number of flips will approach zero. Intuitively, expected absolute difference grows, but at a slower rate than the number of flips, as the number of flips grows.\n\nThe Italian mathematician Gerolamo Cardano (1501–1576) stated without proof that the accuracies of empirical statistics tend to improve with the number of trials. This was then formalized as a law of large numbers. A special form of the LLN (for a binary random variable) was first proved by Jacob Bernoulli. It took him over 20 years to develop a sufficiently rigorous mathematical proof which was published in his \"Ars Conjectandi\" (The Art of Conjecturing) in 1713. He named this his \"Golden Theorem\" but it became generally known as \"Bernoulli's Theorem\". This should not be confused with Bernoulli's principle, named after Jacob Bernoulli's nephew Daniel Bernoulli. In 1837, S.D. Poisson further described it under the name \"la loi des grands nombres\" (\"The law of large numbers\"). Thereafter, it was known under both names, but the \"Law of large numbers\" is most frequently used.\n\nAfter Bernoulli and Poisson published their efforts, other mathematicians also contributed to refinement of the law, including Chebyshev, Markov, Borel, Cantelli and Kolmogorov and Khinchin. Markov showed that the law can apply to a random variable that does not have a finite variance under some other weaker assumption, and Khinchin showed in 1929 that if the series consists of independent identically distributed random variables, it suffices that the expected value exists for the weak law of large numbers to be true. These further studies have given rise to two prominent forms of the LLN. One is called the \"weak\" law and the other the \"strong\" law, in reference to two different modes of convergence of the cumulative sample means to the expected value; in particular, as explained below, the strong form implies the weak.\n\nTwo different versions of the law of large numbers are described below; they are called the \" strong law of large numbers\", and the \" weak law of large numbers\".\nStated for the case where \"X\", \"X\", ... is an infinite sequence of i.i.d. Lebesgue integrable random variables with expected value E(\"X\") = E(\"X\") = ...= \"µ\", both versions of the law state that – with virtual certainty – the sample average\n\nconverges to the expected value\n\nAn assumption of finite variance Var(\"X\") = Var(\"X\") = ... = \"σ\" < ∞ is not necessary. Large or infinite variance will make the convergence slower, but the LLN holds anyway. This assumption is often used because it makes the proofs easier and shorter.\n\nMutual independence of the random variables can be replaced by pairwise independence in both versions of the law.\n\nThe difference between the strong and the weak version is concerned with the mode of convergence being asserted. For interpretation of these modes, see Convergence of random variables.\n\nThe weak law of large numbers (also called Khinchin's law) states that the sample average converges in probability towards the expected value\nThat is, for any positive number \"ε\",\n\nInterpreting this result, the weak law states that for any nonzero margin specified, no matter how small, with a sufficiently large sample there will be a very high probability that the average of the observations will be close to the expected value; that is, within the margin.\n\nAs mentioned earlier, the weak law applies in the case of i.i.d. random variables, but it also applies in some other cases. For example, the variance may be different for each random variable in the series, keeping the expected value constant. If the variances are bounded, then the law applies, as shown by Chebyshev as early as 1867. (If the expected values change during the series, then we can simply apply the law to the average deviation from the respective expected values. The law then states that this converges in probability to zero.) In fact, Chebyshev's proof works so long as the variance of the average of the first \"n\" values goes to zero as \"n\" goes to infinity. As an example, assume that each random variable in the series follows a Gaussian distribution with mean zero, but with variance equal to formula_4 At each stage, the average will be normally distributed (as the average of a set of normally distributed variables). The variance of the sum is equal to the sum of the variances, which is asymptotic to formula_5. The variance of the average is therefore asymptotic to formula_6 and goes to zero.\n\nAn example where the law of large numbers does \"not\" apply is the Cauchy distribution. Let the random numbers equal the tangent of an angle uniformly distributed between −90° and +90°. The median is zero, but the expected value does not exist, and indeed the average of \"n\" such variables has the same distribution as one such variable. It does not tend toward zero as \"n\" goes to infinity.\n\nThere are also examples of the weak law applying even though the expected value does not exist. See #Differences between the weak law and the strong law.\n\nThe strong law of large numbers states that the sample average converges almost surely to the expected value\n\\ \\mu \\qquad\\textrm{when}\\ n \\to \\infty.\n\nThat is,\n\nWhat this means is that as the number of trials \"n\" goes to infinity, the probability that the average of the observations is equal to the expected value will be equal to one.\n\nThe proof is more complex than that of the weak law. This law justifies the intuitive interpretation of the expected value (for Lebesgue integration only) of a random variable when sampled repeatedly as the \"long-term average\".\n\nAlmost sure convergence is also called strong convergence of random variables. This version is called the strong law because random variables which converge strongly (almost surely) are guaranteed to converge weakly (in probability). The strong law implies the weak law but not vice versa, when the strong law conditions hold the variable converges both strongly (almost surely) and weakly (in probability). \nHowever the weak law may hold in conditions where the strong law does not hold and then the convergence is only weak (in probability).\n\nThe strong law of large numbers can itself be seen as a special case of the pointwise ergodic theorem.\n\nThe strong law applies to independent identically distributed random variables having an expected value (like the weak law). This was proved by Kolmogorov in 1930. It can also apply in other cases. Kolmogorov also showed, in 1933, that if the variables are independent and identically distributed, then for the average to converge almost surely on \"something\" (this can be considered another statement of the strong law), it is necessary that they have an expected value (and then of course the average will converge almost surely on that).\n\nIf the summands are independent but not identically distributed, then\n\nprovided that each \"X\" has a finite second moment and\n\nThis statement is known as \"Kolmogorov's strong law\", see e.g. .\n\nAn example of a series where the weak law applies but not the strong law is when \"X\" is plus or minus formula_10 (starting at sufficiently large \"k\" so that the denominator is positive) with probability 1/2 for each. The variance of \"X\" is then formula_11 Kolmogorov's strong law does not apply because the partial sum in his criterion up to \"k=n\" is asymptotic to formula_12 and this is unbounded.\n\nIf we replace the random variables with Gaussian variables having the same variances, namely formula_13 then the average at any point will also be normally distributed. The width of the distribution of the average will tend toward zero (standard deviation asymptotic to formula_14), but for a given ε, there is probability which does not go to zero with \"n\" that the average sometime after the \"n\"th trial will come back up to ε. Since this probability does not go to zero , it must have a positive lower bound \"p\"(ε), which means there is a probability of at least \"p\"(ε) that the average will attain ε after \"n\" trials. It will happen with probability \"p\"(ε)/2 before some \"m\" which depends on \"n\". But even after \"m\", there is still a probability of at least \"p\"(ε) that it will happen. (This seems to indicate that \"p\"(ε)=1 and the average will attain ε an infinite number of times.)\n\nThe \"weak law\" states that for a specified large \"n\", the average formula_15 is likely to be near \"μ\". Thus, it leaves open the possibility that formula_16 happens an infinite number of times, although at infrequent intervals. (Not necessarily formula_17 for all n).\n\nThe \"strong law\" shows that this almost surely will not occur. In particular, it implies that with probability 1, we have that for any the inequality formula_18 holds for all large enough \"n\".\n\nThe strong law does not hold in the following cases, but the weak law does.\n\n1. Let X be an exponentially distributed random variable with parameter 1. The random variable formula_19 has no expected value according to Lebesgue integration, but using conditional convergence and interpreting the integral as a Dirichlet integral, which is an improper Riemann integral, we can say:\n\n2. Let x be geometric distribution with probability 0.5. The random variable formula_21 does not have an expected value in the conventional sense because the infinite series is not absolutely convergent, but using conditional convergence, we can say:\n\n3. If the cumulative distribution function of a random variable is\n\nSuppose \"f\"(\"x\",\"θ\") is some function defined for \"θ\" ∈ Θ, and continuous in \"θ\". Then for any fixed \"θ\", the sequence {\"f\"(\"X\",\"θ\"), \"f\"(\"X\",\"θ\"), …} will be a sequence of independent and identically distributed random variables, such that the sample mean of this sequence converges in probability to E[\"f\"(\"X\",\"θ\")]. This is the \"pointwise\" (in \"θ\") convergence.\n\nThe uniform law of large numbers states the conditions under which the convergence happens \"uniformly\" in \"θ\". If\n\n\nThen E[\"f\"(\"X\",\"θ\")] is continuous in \"θ\", and\n\nThis result is useful to derive consistency of a large class of estimators (see Extremum estimator).\n\nBorel's law of large numbers, named after Émile Borel, states that if an experiment is repeated a large number of times, independently under identical conditions, then the proportion of times that any specified event occurs approximately equals the probability of the event's occurrence on any particular trial; the larger the number of repetitions, the better the approximation tends to be. More precisely, if \"E\" denotes the event in question, \"p\" its probability of occurrence, and \"N\"(\"E\") the number of times \"E\" occurs in the first \"n\" trials, then with probability one,\n\nThis theorem makes rigorous the intuitive notion of probability as the long-run relative frequency of an event's occurrence. It is a special case of any of several more general laws of large numbers in probability theory.\n\nChebyshev's inequality. Let \"X\" be a random variable with finite expected value \"μ\" and finite non-zero variance \"σ\". Then for any real number ,\n\nGiven \"X\", \"X\", ... an infinite sequence of i.i.d. random variables with finite expected value \"E\"(\"X\") = \"E\"(\"X\") = ... = µ < ∞, we are interested in the convergence of the sample average\n\nThe weak law of large numbers states:\nThis proof uses the assumption of finite variance formula_30 (for all formula_31). The independence of the random variables implies no correlation between them, and we have that\n\nThe common mean μ of the sequence is the mean of the sample average:\n\nUsing Chebyshev's inequality on formula_34 results in\n\nThis may be used to obtain the following:\n\nAs \"n\" approaches infinity, the expression approaches 1. And by definition of convergence in probability, we have obtained\n\nBy Taylor's theorem for complex functions, the characteristic function of any random variable, \"X\", with finite mean μ, can be written as\n\nAll \"X\", \"X\", ... have the same characteristic function, so we will simply denote this \"φ\".\n\nAmong the basic properties of characteristic functions there are\n\nThese rules can be used to calculate the characteristic function of formula_39 in terms of \"φ\":\n\nThe limit  \"e\"  is the characteristic function of the constant random variable μ, and hence by the Lévy continuity theorem, formula_41 converges in distribution to μ:\n\nμ is a constant, which implies that convergence in distribution to μ and convergence in probability to μ are equivalent (see Convergence of random variables.) Therefore,\n\nThis shows that the sample mean converges in probability to the derivative of the characteristic function at the origin, as long as the latter exists.\n\n\n\n"}
{"id": "23992011", "url": "https://en.wikipedia.org/wiki?curid=23992011", "title": "List of derivatives and integrals in alternative calculi", "text": "List of derivatives and integrals in alternative calculi\n\nThere are many alternatives to the classical calculus of Newton and Leibniz; for example, each of the infinitely many non-Newtonian calculi. Occasionally an alternative calculus is more suited than the classical calculus for expressing a given scientific or mathematical idea.\n\nThe table below is intended to assist people working with the alternative calculus called the \"geometric calculus\" (or its discrete analog). Interested readers are encouraged to improve the table by inserting citations for verification, and by inserting more functions and more calculi.\n\nIn the following table formula_1 is the digamma function, formula_2 is the K-function, formula_3 is subfactorial, formula_4 are the generalized to real numbers Bernoulli polynomials.\n\n\n"}
{"id": "32869192", "url": "https://en.wikipedia.org/wiki?curid=32869192", "title": "List of dualities", "text": "List of dualities\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\".\n\n\n\n\n"}
{"id": "601070", "url": "https://en.wikipedia.org/wiki?curid=601070", "title": "List of inequalities", "text": "List of inequalities\n\nThis page lists Wikipedia articles about named mathematical inequalities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "444250", "url": "https://en.wikipedia.org/wiki?curid=444250", "title": "List of numerical analysis topics", "text": "List of numerical analysis topics\n\nThis is a list of numerical analysis topics.\n\n\nError analysis (mathematics)\n\n\nNumerical linear algebra — study of numerical algorithms for linear algebra problems\n\n\n\nEigenvalue algorithm — a numerical algorithm for locating the eigenvalues of a matrix\n\n\nInterpolation — construct a function going through some given data points\n\nPolynomial interpolation — interpolation by polynomials\n\nSpline interpolation — interpolation by piecewise polynomials\n\nTrigonometric interpolation — interpolation by trigonometric polynomials\n\n\nApproximation theory\n\n\nRoot-finding algorithm — algorithms for solving the equation \"f\"(\"x\") = 0\n\nMathematical optimization — algorithm for finding maxima or minima of a given function\n\n\nLinear programming (also treats \"integer programming\") — objective function and constraints are linear\n\nConvex optimization\n\nNonlinear programming — the most general optimization problem in the usual framework\n\nOptimal control\n\nInfinite-dimensional optimization\n\n\n\n\n\nNumerical integration — the numerical evaluation of an integral\n\nNumerical methods for ordinary differential equations — the numerical solution of ordinary differential equations (ODEs)\n\nNumerical partial differential equations — the numerical solution of partial differential equations (PDEs)\n\nFinite difference method — based on approximating differential operators with difference operators\n\nFinite element method — based on a discretization of the space of solutions\nGradient discretisation method — based on both the discretization of the solution and of its gradient\n\n\n\n\n\n\n\nFor a large list of software, see the list of numerical analysis software.\n"}
{"id": "57942226", "url": "https://en.wikipedia.org/wiki?curid=57942226", "title": "Loeschian number", "text": "Loeschian number\n\nLoeschian numbers take the quadratic form \"x\" + \"xy\" + \"y\" for integer \"x\", \"y\". They correspond to the norms of vectors in an A lattice.\n\n"}
{"id": "32744249", "url": "https://en.wikipedia.org/wiki?curid=32744249", "title": "Matheass", "text": "Matheass\n\nMatheAss (former Math-Assist) is a computer program for numerical solutions in school mathematics and functions in some points similar to Microsoft Mathematics. \"MatheAss is widely spread in math classes\" in Germany. For schools in the federal state of Hessen (Germany) exists a state license, which allows all secondary schools to use MatheAss\nIts functionality is limited compared to other numerical programs, for example, MatheAss has no script language and does no symbolic computation. On the other side it is easy to use and offers the user fully worked out solutions, in which only the necessary quantities need to be entered. MatheAss covers the topics algebra, geometry, analysis, stochastics, and linear algebra.\nAfter a precursor for the home computers, usual around 1980, MatheAss appeared in 1983 as a shareware version for the PC, so it was one of the first shareware programs on the German market. MatheAss is available on the manufacturer's website for download for various versions of the Windows operating system.\nSince version 8.2 (released in February 2011) MatheAss again offers a context-sensitive help, which was supplemented in many places by showing mathematical examples and background information. The MatheAss help file can also be viewed online.\n"}
{"id": "19266946", "url": "https://en.wikipedia.org/wiki?curid=19266946", "title": "Mathematical diagram", "text": "Mathematical diagram\n\nMathematical diagrams are diagrams in the field of mathematics, and diagrams using mathematics such as charts and graphs, that are mainly designed to convey mathematical relationships, for example, comparisons over time.\n\nA complex number can be visually represented as a pair of numbers forming a vector on a diagram called an Argand diagram\nThe complex plane is sometimes called the \"Argand plane\" because it is used in \"Argand diagrams\". These are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian-Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the poles and zeroes of a function in the complex plane.\n\nThe concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates — the magnitude or \"modulus\" of the product is the product of the two absolute values, or moduli, and the angle or \"argument\" of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.\n\nIn the context of fast Fourier transform algorithms, a butterfly is a portion of the computation that combines the results of smaller discrete Fourier transforms (DFTs) into a larger DFT, or vice versa (breaking a larger DFT up into subtransforms). The name \"butterfly\" comes from the shape of the data-flow diagram in the radix-2 case, as described below. The same structure can also be found in the Viterbi algorithm, used for finding the most likely sequence of hidden states.\n\nThe butterfly diagram show a data-flow diagram connecting the inputs \"x\" (left) to the outputs \"y\" that depend on them (right) for a \"butterfly\" step of a radix-2 Cooley–Tukey FFT algorithm. This diagram resembles a butterfly as in the morpho butterfly shown for comparison), hence the name.\n\nIn mathematics, and especially in category theory, a commutative diagram is a diagram of objects, also known as vertices, and morphisms, also known as arrows or edges, such that when selecting two objects any directed path through the diagram leads to the same result by composition.\n\nCommutative diagrams play the role in category theory that equations play in algebra.\n\nA Hasse diagram is a simple picture of a finite partially ordered set, forming a drawing of the partial order's transitive reduction. Concretely, one represents each element of the set as a vertex on the page and draws a line segment or curve that goes upward from \"x\" to \"y\" precisely when \"x\" < \"y\" and there is no \"z\" such that \"x\" < \"z\" < \"y\". In this case, we say y covers x, or y is an immediate successor of x. In a Hasse diagram, it is required that the curves be drawn so that each meets exactly two vertices: its two endpoints. Any such diagram (given that the vertices are labeled) uniquely determines a partial order, and any partial order has a unique transitive reduction, but there are many possible placements of elements in the plane, resulting in different Hasse diagrams for a given order that may have widely varying appearances.\n\nIn Knot theory a useful way to visualise and manipulate knots is to project the knot onto a plane—;think of the knot casting a shadow on the wall. A small perturbation in the choice of projection will ensure that it is one-to-one except at the double points, called \"crossings\", where the \"shadow\" of the knot crosses itself once transversely\n\nAt each crossing we must indicate which section is \"over\" and which is \"under\", so as to be able to recreate the original knot. This is often done by creating a break in the strand going underneath. If by following the diagram the knot alternately crosses itself \"over\" and \"under\", then the diagram represents a particularly well-studied class of knot, alternating knots.\n\nA Venn diagram is a representation of mathematical sets: a mathematical diagram representing sets as circles, with their relationships to each other expressed through their overlapping positions, so that all possible relationships between the sets are shown.\n\nThe Venn diagram is constructed with a collection of simple closed curves drawn in the plane. The principle of these diagrams is that classes be represented by regions in such relation to one another that all the possible logical relations of these classes can be indicated in the same diagram. That is, the diagram initially leaves room for any possible relation of the classes, and the actual or given relation, can then be specified by indicating that some particular region is null or is notnull.\n\nA Voronoi diagram is a special kind of decomposition of a metric space determined by distances to a specified discrete set of objects in the space, e.g., by a discrete set of points. This diagram is named after Georgy Voronoi, also called a Voronoi tessellation, a Voronoi decomposition, or a Dirichlet tessellation after Peter Gustav Lejeune Dirichlet.\n\nIn the simplest case, we are given a set of points S in the plane, which are the Voronoi sites. Each site s has a Voronoi cell V(s) consisting of all points closer to s than to any other site. The segments of the Voronoi diagram are all the points in the plane that are equidistant to two sites. The Voronoi nodes are the points equidistant to three (or more) sites\n\nA wallpaper group or \"plane symmetry group\" or \"plane crystallographic group\" is a mathematical classification of a two-dimensional repetitive pattern, based on the symmetries in the pattern. Such patterns occur frequently in architecture and decorative art. There are 17 possible distinct groups.\n\nWallpaper groups are two-dimensional symmetry groups, intermediate in complexity between the simpler frieze groups and the three-dimensional crystallographic groups, also called space groups. Wallpaper groups categorize patterns by their symmetries. Subtle differences may place similar patterns in different groups, while patterns which are very different in style, color, scale or orientation may belong to the same group.\n\nA \"Young diagram\" or Young tableau, also called Ferrers diagram, is a finite collection of boxes, or cells, arranged in left-justified rows, with the row sizes weakly decreasing (each row has the same or shorter length than its predecessor). \n\nListing the number of boxes in each row gives a partition formula_1 of a positive integer \"n\", the total number of boxes of the diagram. The Young diagram is said to be of shape formula_1, and it carries the same information as that partition. Listing the number of boxes in each column gives another partition, the conjugate or \"transpose\" partition of formula_1; one obtains a Young diagram of that shape by reflecting the original diagram along its main diagonal.\n\nYoung tableaux were introduced by Alfred Young, a mathematician at Cambridge University, in 1900. They were then applied to the study of symmetric group by Georg Frobenius in 1903. Their theory was further developed by many mathematicians.\n\n\n\n"}
{"id": "18902", "url": "https://en.wikipedia.org/wiki?curid=18902", "title": "Mathematician", "text": "Mathematician\n\nA mathematician is someone who uses an extensive knowledge of mathematics in his or her work, typically to solve mathematical problems.\n\nMathematics is concerned with numbers, data, quantity, structure, space, models, and change.\n\nOne of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.\n\nThe number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was \"All is number\". It was the Pythagoreans who coined the term \"mathematics\", and with whom the study of mathematics for its own sake begins.\n\nThe first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).\n\nScience and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on optics, maths and astronomy of Ibn al-Haytham.\n\nThe Renaissance brought an increased emphasis on mathematics and science to Europe. During this period of transition from a mainly feudal and ecclesiastical culture to a predominantly secular one, many notable mathematicians had other occupations: Luca Pacioli (founder of accounting); Niccolò Fontana Tartaglia (notable engineer and bookkeeper); Gerolamo Cardano (earliest founder of probability and binomial expansion); Robert Recorde (physician) and François Viète (lawyer).\n\nAs time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the scientists Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag[ing] productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve.\n\nBritish universities of this period adopted some approaches familiar to the Italian and German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt’s idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”\n\nMathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.\n\nMathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers.\n\nThe discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term \"applied mathematics\" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, \"applied mathematicians\" look into the \"formulation, study, and use of mathematical models\" in science, engineering, business, and other areas of mathematical practice.\n\nPure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as \"speculative mathematics\", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and other applications.\n\nAnother insightful view put forth is that \"pure mathematics is not necessarily applied mathematics\": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.\n\nTo develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be \"pure\" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.\n\nMany professional mathematicians also engage in the teaching of mathematics. Duties may include:\n\nMany careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.\n\nAs another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock (\"see: Valuation of options; Financial modeling\").\n\nAccording to the Dictionary of Occupational Titles occupations in mathematics include the following.\n\n\nThe following are quotations about mathematicians, or by mathematicians.\n\nThere is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.\n\nThe American Mathematical Society, Association for Women in Mathematics, and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.\n\nSeveral well known mathematicians have written autobiographies in part to explain to a general audience what it is about mathematics that has made them want to devote their lives to its study. These provide some of the best glimpses into what it means to be a mathematician. The following list contains some works that are not autobiographies, but rather essays on mathematics and mathematicians with strong autobiographical elements.\n\n\n"}
{"id": "51056252", "url": "https://en.wikipedia.org/wiki?curid=51056252", "title": "Mathematics mastery", "text": "Mathematics mastery\n\nMathematics mastery is an approach to mathematics education which is based on mastery learning in which most students are expected to achieve a high level of competence before progressing. This technique is used in countries such as China and Singapore where good results have been achieved and so the approach is now being promoted in the UK by people such as schools minister Nick Gibb. Chinese teachers were brought to the UK to demonstrate the Shanghai mastery approach in 2015. A trial was made in the UK with about 10,000 students of ages 5–6 and 11–12. In one year, test scores indicated that the students were about a month ahead of students in schools using other approaches. This result was considered small but significant.\n\nThe National Association of Mathematics Advisers has highlighted five issues in understanding this approach.\n\n"}
{"id": "645602", "url": "https://en.wikipedia.org/wiki?curid=645602", "title": "Minimax theorem", "text": "Minimax theorem\n\nA minimax theorem is a theorem providing conditions that guarantee that the max–min inequality is also an equality. \nThe first theorem in this sense is von Neumann's minimax theorem from 1928, which was considered the starting point of game theory. \nSince then, several generalizations and alternative versions of von Neumann's original theorem have appeared in the literature.\n\nThe minimax theorem was first proven and published in 1928 by John von Neumann, who is quoted as saying \"As far as I can see, there could be no theory of games … without that theorem … I thought there was nothing worth publishing until the Minimax Theorem was proved\".\n\nFormally, von Neumann's minimax theorem states:\n\nLet formula_1 and formula_2 be compact convex sets. If formula_3 is a continuous function that is convex-concave, i.e.\n\nThen we have that\n\n"}
{"id": "1719992", "url": "https://en.wikipedia.org/wiki?curid=1719992", "title": "Non-perturbative", "text": "Non-perturbative\n\nIn mathematics and physics, a non-perturbative function or process is one that cannot be accurately described by perturbation theory. An example is the function\n\nformula_1.\n\nThe Taylor series at x = 0 for this function is exactly zero to all orders in perturbation theory, but the function is non-zero if \"x\" ≠ 0.\n\nThe implication of this for physics is that there are some phenomena which are impossible to understand by perturbation theory, regardless of how many orders of perturbation theory we use. Instantons are an example.\n\nTherefore, in theoretical physics, a non-perturbative solution or theory is one that does not require perturbation theory to explicate, or does not simply describe the dynamics of perturbations around some fixed background. For this reason, non-perturbative solutions and theories yield insights into areas and subjects perturbative methods cannot reveal.\n\n"}
{"id": "38803848", "url": "https://en.wikipedia.org/wiki?curid=38803848", "title": "Open energy system models", "text": "Open energy system models\n\nOpen energy system models are energy system models that are open source. Similarly open energy system data employs open data methods to produce and distribute datasets primarily for use by open energy system models.\n\nEnergy system models are used to explore future energy systems and are often applied to questions involving energy and climate policy. The models themselves vary widely in terms of their type, design, programming, application, scope, level of detail, sophistication, and shortcomings. The open energy modeling projects listed here fall exclusively within the bottom-up paradigm, in which a model is a relatively literal representation of the underlying system. For many models, some form of mathematical optimization is used to inform the solution process.\n\nSeveral drivers favor the development of open models and open data. There is an increasing interest in making public policy energy models more transparent to improve their acceptance by policymakers and the public. There is also a desire to leverage the benefits that open data and open software development can bring, including reduced duplication of effort, better sharing of ideas and information, improved quality, and wider engagement and adoption. Model development is therefore usually a team effort and constituted as either an academic project, a commercial venture, or a genuinely inclusive community initiative.\n\nThis article does not cover projects which simply make their source code or spreadsheets available for public download, but which omit a recognized free and open-source software license. The absence of a license agreement creates a state of legal uncertainty whereby potential users cannot know which limitations the owner may want to enforce in the future. The projects listed here are deemed suitable for inclusion through having pending or published academic literature or by being reported in secondary sources.\n\nAn open energy system modeling project typically comprises a codebase, datasets, and software documentation and perhaps scientific publications. The project repository may be hosted on an institutional server or on a public code-hosting site, such as GitHub. Some projects release only their codebase, while others ship some or all of their datasets as well. Projects may also offer email lists, chat rooms, and web forums to aid collaboration.\n\nThe majority of projects are based within university research groups, either singingly or as academic collaborations.\n\nA 2017 paper lists the benefits of open data and models and discusses the reasons that many projects nonetheless remain closed. The paper makes a number of recommendations for projects wishing to transition to a more open approach. The authors also conclude that, in terms of openness, energy research has lagged behind other fields, most notably physics, biotechnology, and medicine.\n\nOpen energy system modeling came of age in the 2010s. Just two projects were cited in a 2011 paper on the topic: OSeMOSYS and TEMOA. Balmorel was also active at that time, having been made public in 2001. , this article lists 25 such undertakings (with a further six waiting to be ).\n\nThe use of open energy system models and open energy data represents one attempt to improve the transparency, comprehensibility, and reproducibility of energy system models, particularly those used to aid public policy development.\n\nA 2010 paper concerning energy efficiency modeling argues that \"an open peer review process can greatly support model verification and validation, which are essential for model development\". To further honor the process of peer review, researchers argue, in a 2012 paper, that it is essential to place both the source code and datasets under publicly accessible version control so that third-parties can run, verify, and scrutinize specific models. A 2016 paper contends that model-based energy scenario studies, seeking to influence decision-makers in government and industry, must become more comprehensible and more transparent. To these ends, the paper provides a checklist of transparency criteria that should be completed by modelers. The authors however state that they \"consider open source approaches to be an extreme case of transparency that does not automatically facilitate the comprehensibility of studies for policy advice.\"\n\nA one-page opinion piece from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for peer review.\n\nState-sponsored open source projects in any domain are a relatively new phenomena.\n\n, the European Commission now supports several open source energy system modeling projects to aid the transition to a low-carbon energy system for Europe. The Dispa-SET project (below) is modeling the European electricity system and hosts its codebase on GitHub. The MEDEAS project, which will design and implement a new open source energy-economy model for Europe, held its kick-off meeting in February 2016. , the project had yet to publish any source code. The established OSeMOSYS project (below) is developing a multi-sector energy model for Europe with Commission funding to support stakeholder outreach. The flagship model however remains closed source.\n\nThe United States NEMS national model is available but nonetheless difficult to use. NEMS does not classify as an open source project in the accepted sense.\n\nOpen electricity sector models are confined to just the electricity sector. These models invariably have a temporal resolution of one hour or less. Some models concentrate on the engineering characteristics of the system, including a good representation of high-voltage transmission networks and AC power flow. Others models depict electricity spot markets and are known as dispatch models. While other models embed autonomous agents to capture, for instance, bidding decisions using techniques from bounded rationality. The ability to handle variable renewable energy, transmission systems, and grid storage are becoming important considerations.\n\nDIETER stands for Dispatch and Investment Evaluation Tool with Endogenous Renewables. DIETER is a dispatch and investment model. It was first used to study the role of power storage and other flexibility options in a future greenfield setting with high shares of renewable generation. DIETER is being developed at the German Institute for Economic Research (DIW), Berlin, Germany. The codebase and datasets for Germany can be downloaded from the project website. The basic model is fully described in a DIW working paper and a journal article. DIETER is written in GAMS and was developed using the CPLEX commercial solver.\n\nDIETER is framed as a pure linear (no integer variables) cost minimization problem. In the initial formulation, the decision variables include the investment in and dispatch of generation, storage, and DSM capacities in the German wholesale and balancing electricity markets. Later model extensions include vehicle-to-grid interactions and prosumage of solar electricity.\n\nThe first study using DIETER examines the power storage requirements for renewables uptake ranging from 60% to 100%. Under the baseline scenario of 80% (the lower bound German government target for 2050), grid storage requirements remain moderate and other options on both the supply side and demand side offer flexibility at low cost. Nonetheless storage plays an important role in the provision of reserves. Storage becomes more pronounced under higher shares of renewables, but strongly depends on the costs and availability of other flexibility options, particularly biomass availability.\n\nUnder development at the European Commission's Joint Research Centre (JRC), Petten, the Netherlands, is a unit commitment and dispatch model intended primarily for Europe. It is written in Python (with Pyomo) and GAMS and uses Python for data processing. A valid GAMS license is required. The model is formulated as a mixed integer problem and JRC uses the proprietary CPLEX sover although open source libraries may also be deployed. Technical descriptions are available for versions2.0 and2.1. is hosted on GitHub, together with a trial dataset, and third-party contributions are encouraged. The codebase has been tested on Windows, macOS, and Linux. Online documentation is available.\n\nThe SET in the project name refers to the European Strategic Energy Technology Plan (SET-Plan), which seeks to make Europe a leader in energy technologies that can fulfill future (2020 and 2050) energy and climate targets. Energy system modeling, in various forms, is central to this European Commission initiative.\n\nThe model power system is managed by a single operator with full knowledge of the economic and technical characteristics of the generation units, the loads at each node, and the heavily simplified transmission network. Demand is deemed fully inelastic. The system is subject to intra-period and inter-period unit commitment constraints (the latter covering nuclear and thermal generation for the most part) and operated under economic dispatch. Hourly data is used and the simulation horizon is normally one year. But to ensure the model remains tractable, two day rolling horizon optimization is employed. The model advances in steps of one day, optimizing the next 48hours ahead but retaining results for just the first 24hours.\n\nTwo related publications describe the role and representation of flexibility measures within power systems facing ever greater shares of variable renewable energy (VRE). These flexibility measures comprise: dispatchable generation (with constraints on efficiency, ramp rate, part load, and up and down times), conventional storage (predominantly pumped-storage hydro), cross-border interconnectors, demand side management, renewables curtailment, last resort load shedding, and nascent power-to-X solutions (with X being gas, heat, or mobility). The modeler can set a target for renewables and place caps on and other pollutants. Planned extensions to the software include support for simplified AC power flow (transmission is currently treated as a transportation problem), new constraints (like cooling water supply), stochastic scenarios, and the inclusion of markets for ancillaryservices.\n\nEMLab-Generation is an agent-based model covering two interconnected electricity markets – be they two adjoining countries or two groups of countries. The software is being developed at the Energy Modelling Lab, Delft University of Technology, Delft, the Netherlands. A factsheet is available. And software documentation is available. EMLab-Generation is written in Java.\n\nEMLab-Generation simulates the actions of power companies investing in generation capacity and uses this to explore the long-term effects of various energy and climate protection policies. These policies may target renewable generation, emissions, security of supply, and/or energy affordability. The power companies are the main agents: they bid into power markets and they invest based on the net present value (NPV) of prospective power plant projects. They can adopt a variety of technologies, using scenarios from the 2011 IEA World Energy Outlook. The agent-based methodology enables different sets of assumptions to be tested, such as the heterogeneity of actors, the consequences of imperfect expectations, and the behavior of investors outside of ideal conditions.\n\nEMLab-Generation offers a new way of modeling the effects of public policy on electricity markets. It can provide insights into actor and system behaviors over time – including such things as investment cycles, abatement cycles, delayed responses, and the effects of uncertainty and risk on investment decisions.\n\nA 2014 study using EMLab-Generation investigates the effects of introducing floor and ceiling prices for under the EU ETS. And in particular, their influence on the dynamic investment pathway of two interlinked electricity markets (loosely Great Britain and Central Western Europe). The study finds a common, moderate auction reserve price results in a more continuous decarbonisation pathway and reduces price volatility. Adding a ceiling price can shield consumers from extreme price shocks. Such price restrictions should not lead to an overshoot of emissions targets in the long-run.\n\nEMMA is the European Electricity Market Model. It is a techno-economic model covering the integrated Northwestern European power system. EMMA is being developed by the energy economics consultancy Neon Neue Energieökonomik, Berlin, Germany. The source code and datasets can be downloaded from the project website. A manual is available. EMMA is written in GAMS and uses the CPLEX commercial solver.\n\nEMMA models electricity dispatch and investment, minimizing the total cost with respect to investment, generation, and trades between market areas. In economic terms, EMMA classifies as a partial equilibrium model of the wholesale electricity market with a focus on the supply-side. EMMA identifies short-term or long-term optima (or equilibria) and estimates the corresponding capacity mix, hourly prices, dispatch, and cross-border trading. Technically, EMMA is a pure linear program (no integer variables) with about two million variables. , the model covers Belgium, France, Germany, the Netherlands, and Poland and supports conventional generation, renewable generation, and cogeneration.\n\nEMMA has been used to study the economic effects of the increasing penetration of variable renewable energy (VRE), specifically solar power and wind power, in the Northwestern European power system. A 2013 study finds that increasing VRE shares will depress prices and, as a consequence, the competitive large-scale deployment of renewable generation will be more difficult to accomplish than many anticipate. A 2015 study estimates the welfare-optimal market share for wind and solar power. For wind, this is 20%, three-fold more than at present.\n\nAn independent 2015 study reviews the EMMA model and comments on the high assumed specific costs for renewable investments.\n\nGENESYS stands for Genetic Optimisation of a European Energy Supply System. The software is being developed jointly by the Institute of Power Systems and Power Economics (IAEW) and the Institute for Power Electronics and Electrical Drives (ISEA), both of RWTH Aachen University, Aachen, Germany. The project maintains a website where potential users can request access to the codebase and the dataset for the 2050 base scenario only. Detailed descriptions of the software are available. GENESYS is written in C++ and uses Boost libraries, the MySQL relational database, the Qt4 application framework, and optionally the CPLEX solver.\n\nThe GENESYS simulation tool is designed to optimize a future EUMENA (Europe, Middle East, and North Africa) power system and assumes a high share of renewable generation. It is able to find an economically optimal distribution of generator, storage, and transmission capacities within a 21region EUMENA. It allows for the optimization of this energy system in combination with an evolutionary method. The optimization is based on a covariance matrix adaptation evolution strategy (CMA-ES), while the operation is simulated as a hierarchical set-up of system elements which balance the load between the various regions at minimum cost using the network simplex algorithm. GENESYS ships with a set of input time series and a set of parameters for the year 2050, which the user can modify.\n\nA future EUMENA energy supply system with a high share of renewable energy sources (RES) will need a strongly interconnected energy transport grid and significant energy storage capacities. GENESYS was used to dimension the storage and transmission between the 21different regions. Under the assumption of 100% self-supply, about of RES in total and a storage capacity of about are needed, corresponding to 6% of the annual energy demand, and a HVDC transmission grid of . The combined cost estimate for generation, storage, and transmission, excluding distribution, is 6.87¢/kWh.\n\nA 2016 study looked at the relationship between storage and transmission capacity under high shares of renewable energy sources (RES) in an EUMENA power system. It found that, up to a certain extent, transmission capacity and storage capacity can substitute for each other. For a transition to a fully renewable energy system by 2050, major structural changes are required. The results indicate the optimal allocation of photovoltaics and wind power, the resulting demand for storage capacities of different technologies (battery, pumped hydro, and hydrogen storage) and the capacity of the transmission grid.\n\nNEMO, the National Electricity Market Optimiser, is a chronological dispatch model for testing and optimizing different portfolios of conventional and renewable electricity generation technologies. It applies solely to the Australian National Electricity Market (NEM), which, despite its name, is limited to east and south Australia. NEMO has been in development at the Centre for Energy and Environmental Markets (CEEM), University of New South Wales (UNSW), Sydney, Australia since 2011. The project maintains a small website and runs an email list. NEMO is written in Python. NEMO itself is described in two publications. The data sources are also noted. Optimizations are carried out using a single-objective evaluation function, with penalties. The solution space of generator capacities is searched using the CMA-ES (covariance matrix adaptation evolution strategy) algorithm. The timestep is arbitrary but one hour is normally employed.\n\nNEMO has been used to explore generation options for the year 2030 under a variety of renewable energy (RE) and abated fossil fuel technology scenarios. A 2012 study investigates the feasibility of a fully renewable system using concentrated solar power (CSP) with thermal storage, windfarms, photovoltaics, existing hydroelectricity, and biofuelled gas turbines. A number of potential systems, which also meet NEM reliability criteria, are identified. The principal challenge is servicing peak demand on winter evenings following overcast days and periods of low wind. A 2014 study investigates three scenarios using coal-fired thermal generation with carbon capture and storage (CCS) and gas-fired gas turbines with and without capture. These scenarios are compared to the 2012 analysis using fully renewable generation. The study finds that \"only under a few, and seemingly unlikely, combinations of costs can any of the fossil fuel scenarios compete economically with 100% renewable electricity in a carbon constrained world\". A 2016 study evaluates the incremental costs of increasing renewable energy shares under a range of greenhouse gas caps and carbon prices. The study finds that incremental costs increase linearly from zero to 80% RE and then escalate moderately. The study concludes that this cost escalation is not a sufficient reason to avoid renewables targets of 100%.\n\nOnSSET is the OpeN Source Spatial Electrification Toolkit. OnSSET is being developed by the Energy Systems Analysis Group (dESA), KTH Royal Institute of Technology, Stockholm, Sweden. The software is used to examine areas not served by grid-based electricity and identify the technology options and investment requirements that will provide least-cost access to electricity services. OnSSET is designed to support the United Nations' SDG7: the provision of affordable, reliable, sustainable, and modern energy for all. The Python implementation of the toolkit is known as PyOnSSET and was released on 26November 2016. PyOnSSET does not ship with data, but suitable datasets are available from energydata.info. The project maintains a website and hosts a forum on Reddit.\n\nOnSSET can estimate, analyze, and visualize the most cost-effective electrification access options, be they conventional grid, mini-grid, or stand-alone. The toolkit supports a range of conventional and renewable energy technologies, including photovoltaics, wind turbines, and small hydro generation. , bioenergy and hybrid technologies, such as wind-diesel, are being added.\n\nOnSSET utilizes energy and geographic information, the latter may include settlement size and location, existing and planned transmission and generation infrastructure, economic activity, renewable energy resources, roading networks, and nighttime lighting needs. The GIS information can be supported using the proprietary ArcGIS package or an open source equivalent such as GRASS or QGIS.\n\nOnSSET has been used for case studies in Afghanistan, Bolivia, Ethiopia, Nigeria, and Tanzania. OnSSET has also been applied in India, Kenya, and Zimbabwe. In addition, continental studies have been carried out for Sub-Saharan Africa and Latin America. , there are plans to apply OnSSET in developing Asia, to increase the resolution of the analysis, and to extend support for various productive uses of electricity.\n\nOnSSET results have contributed to the IEA \"World Energy Outlook\" reports for 2014 and 2015 and the World Bank Global Tracking Framework report in 2015.\n\npandapower is a power system analysis and optimization program being jointly developed by the Energy Management and Power System Operation research group, University of Kassel and the Department for Distribution System Operation, Fraunhofer Institute for Energy Economics and Energy System Technology (IEE), both of Kassel, Germany. The codebase is hosted on GitHub and is also available as a package. The project maintains a website, an emailing list, and online documentation. pandapower is written in Python. It uses the pandas library for data manipulation and analysis and the PYPOWER library to solve for power flow. Unlike some open source power system tools, pandapower does not depend on proprietary platforms like MATLAB.\n\npandapower supports the automated analysis and optimization of distribution and transmission networks. This allows a large of number of scenarios to be explored, based on different future grid configurations and technologies. pandapower offers a collection of power system elements, including: lines, 2-winding transformers, 3-winding transformers, and ward-equivalents. It also contains a switch model that allows the modeling of ideal bus-bus switches as well as bus-line/bus-trafo switches. The software supports topological searching. The network itself can be plotted, with or without geographical information, using the matplotlib and plotly libraries.\n\nA 2016 publication evaluates the usefulness of the software by undertaking several case studies with major distribution system operators (DSO). These studies examine the integration of increasing levels of photovoltaics into existing distribution grids. The study concludes that being able to test a large number of detailed scenarios is essential for robust grid planning. Notwithstanding, issues of data availability and problem dimensionality will continue to present challenges.\n\nA 2018 paper describes the package and its design and provides an example case study. The article explains how users work with an element-based model (EBM) which is converted internally to a bus-branch model (BBM) for computation. The package supports power system simulation, optimal power flow calculations (cost information is required), state estimation (should the system characterization lacks fidelity), and graph-based network analysis. The case study shows how a few tens of lines of scripting can interface with pandapower to advance the design of a system subject to diverse operating requirements. The associated code is hosted on GitHub as jupyter notebooks.\n\n, BNetzA, the German network regulator, is using pandapower for automated grid analysis. Energy research institutes in Germany are also following the development of pandapower.\n\nThe PowerMatcher software implements a smart grid coordination mechanism which balances distributed energy resources (DER) and flexible loads through autonomous bidding. The project is managed by the Flexiblepower Alliance Network (FAN) in Amsterdam, the Netherlands. The project maintains a website and the source code is hosted on GitHub. , existing datasets are not available. PowerMatcher is written in Java.\n\nEach device in the smart grid system – whether a washing machine, a wind generator, or an industrial turbine – expresses its willingness to consume or produce electricity in the form of a bid. These bids are then collected and used to determine an equilibrium price. The PowerMatcher software thereby allows high shares of renewable energy to be integrated into existing electricity systems and should also avoid any local overloading in possibly aging distribution networks.\n\nrenpass is an acronym for Renewable Energy Pathways Simulation System. renpass is a simulation electricity model with high regional and temporal resolution, designed to capture existing systems and future systems with up to 100% renewable generation. The software is being developed by the Centre for Sustainable Energy Systems (CSES or ZNES), University of Flensburg, Germany. The project runs a website, from where the codebase can be download. renpass is written in R and links to a MySQL database. A PDF manual is available. renpass is also described in a PhD thesis. , renpass is being extended as renpassG!S, based on oemof.\n\nrenpass is an electricity dispatch model which minimizes system costs for each time step (optimization) within the limits of a given infrastructure (simulation). Time steps are optionally 15 minutes or one hour. The method assumes perfect foresight. renpass supports the electricity systems found in Austria, Belgium, the Czech Republic, Denmark, Estonia, France, Finland, Germany, Latvia, Lithuania, Luxembourg, the Netherlands, Norway, Poland, Sweden, and Switzerland.\n\nThe optimization problem for each time step is to minimize the electricity supply cost using the existing power plant fleet for all regions. After this regional dispatch, the exchange between the regions is carried out and is restricted by the grid capacity. This latter problem is solved with a heuristic procedure rather than calculated deterministically. The input is the merit order, the marginal power plant, the excess energy (renewable energy that could be curtailed), and the excess demand (the demand that cannot be supplied) for each region. The exchange algorithm seeks the least cost for all regions, thus the target function is to minimize the total costs of all regions, given the existing grid infrastructure, storage, and generating capacities. The total cost is defined as the residual load multiplied by the price in each region, summed over all regions.\n\nA 2012 study uses renpass to examine the feasibility of a 100% renewable electricity system for the Baltic Sea region (Denmark, Estonia, Finland, Germany, Latvia, Lithuania, Poland, and Sweden) in the year 2050. The base scenario presumes conservative renewable potentials and grid enhancements, a 20% drop in demand, a moderate uptake of storage options, and the deployment of biomass for flexible generation. The study finds that a 100% renewable electricity system is possible, albeit with occasional imports from abutting countries, and that biomass plays a key role in system stability. The costs for this transition are estimated at 50€/MWh. A 2014 study uses renpass to model Germany and its neighbors. A 2014 thesis uses renpass to examine the benefits of both a new cable between Germany and Norway and new pumped storage capacity in Norway, given 100% renewable electricity systems in both countries. Another 2014 study uses renpass to examine the German \"Energiewende\", the transition to a sustainable energy system for Germany. The study also argues that the public trust needed to underpin such a transition can only be built through the use of transparent open source energy models.\n\nSciGRID, short for Scientific Grid, is an open source model of the German and European electricity transmission networks. The research project is managed by Next Energy (officially the EWE Research Centre for Energy Technology) located at the University of Oldenburg, Oldenburg, Germany. The project maintains a website and an email newsletter. SciGRID is written in Python and uses a PostgreSQL database. The first release (v0.1) was made on 15June 2015.\n\nSciGRID aims to rectify the lack of open research data on the structure of electricity transmission networks within Europe. This lack of data frustrates attempts to build, characterise, and compare high resolution energy system models. SciGRID utilizes transmission network data available from the OpenStreetMap project, available under the Open Database License (ODbL), to automatically author transmission connections. SciGRID will not use data from closed sources. SciGRID can also mathematically decompose a given network into a simpler representation for use in energy models.\n\nA related project is GridKit, released under an MIT license. GridKit is being developed to investigate the possibility of a 'heuristic' analysis to augment the route-based analysis used in SciGRID. Data is available for network models of the European and North-American high-voltage electricity grids.\n\nSIREN stands for SEN Integrated Renewable Energy Network Toolkit. The project is run by Sustainable Energy Now, an NGO based in Perth, Australia. The project maintains a website. SIREN runs on Windows and the source code is hosted on SourceForge. The software is written in Python and uses the SAM model (System Advisor Model) from the US National Renewable Energy Laboratory to perform energy calculations. SIREN uses hourly datasets to model a given geographic region. Users can use the software to explore the location and scale of renewable energy sources to meet a specified electricity demand. SIREN utilizes a number of open or publicly available data sources: maps can be created from OpenStreetMap tiles and weather datasets can be created using NASA MERRA-2 satellite data.\n\nA 2016 study using SIREN to analyze Western Australia's South-West Interconnected System (SWIS) finds that it can transition to 85% renewable energy (RE) for the same cost as new coal and gas. In addition, 11.1million tonnes of eq emissions would be avoided. The modeling assumes a carbon price of AUD$30/t. Further scenarios examine the goal of 100% renewable generation.\n\nSWITCH is a loose acronym for solar, wind, conventional and hydroelectric generation, and transmission. SWITCH is an optimal planning model for power systems with large shares of renewable energy. SWITCH is being developed by the Department of Electrical Engineering, University of Hawai'i, Mānoa, Hawaii, USA. The project runs a small website and hosts its codebase and datasets on GitHub. SWITCH is written in Pyomo, an optimization components library programmed in Python. It can use either the open source GLPK solver or the commercial CPLEX and Gurobi solvers.\n\nSWITCH is a power system model, focused on renewables integration. It can identify which generator and transmission projects to build in order to satisfy electricity demand at the lowest cost over a several year period while also reducing emissions. SWITCH utilizes multi-stage stochastic linear optimization with the objective of minimizing the present value of the cost of power plants, transmission capacity, fuel usage, and an arbitrary per-tonne charge (to represent either a carbon tax or a certificate price), over the course of a multi-year investment period. It has two major sets of decision variables. First, at the start of each investment period, SWITCH selects how much generation capacity to build in each of several geographic load zones, how much power transfer capability to add between these zones, and whether to operate existing generation capacity during the investment period or to temporarily mothball it to avoid fixed operation and maintenance costs. Second, for a set of sample days within each investment period, SWITCH makes hourly decisions about how much power to generate from each dispatchable power plant, store at each pumped hydro facility, or transfer along each transmission interconnector. The system must also ensure enough generation and transmission capacity to provide a planning reserve margin of 15% above the load forecasts. For each sampled hour, SWITCH uses electricity demand and renewable power production based on actual measurements, so that the weather-driven correlations between these elements remain intact.\n\nFollowing the optimization phase, SWITCH is used in a second phase to test the proposed investment plan against a more complete set of weather conditions and to add backstop generation capacity so that the planning reserve margin is always met. Finally, in a third phase, the costs are calculated by freezing the investment plan and operating the proposed power system over a full set of weather conditions.\n\nA 2012 paper uses California from 2012 to 2027 as a case study for SWITCH. The study finds that there is no ceiling on the amount of wind and solar power that could be used and that these resources could potentially reduce emissions by 90% or more (relative to 1990 levels) without reducing reliability or severely raising costs. Furthermore, policies that encourage electricity customers to shift demand to times when renewable power is most abundant (for example, though the well-timed charging of electric vehicles) could achieve radical emission reductions at moderate cost.\n\nURBS, Latin for city, is a linear programming model for exploring capacity expansion and unit commitment problems and is particularly suited to distributed energy systems (DES). It is being developed by the Institute for Renewable and Sustainable Energy Systems, Technical University of Munich, Germany. The codebase is hosted on GitHub. URBS is written in Python and uses the Pyomo optimization packages.\n\nURBS classes as an energy modeling framework and attempts to minimize the total discounted cost of the system. A particular model selects from a set of technologies to meet a predetermined electricity demand. It uses a time resolution of one hour and the spatial resolution is model-defined. The decision variables are the capacities for the production, storage, and transport of electricity and the time scheduling for their operation.\n\nThe software has been used to explore cost-optimal extensions to the European transmission grid using projected wind and solar capacities for 2020. A 2012 study, using high spatial and technological resolutions, found variable renewable energy (VRE) additions cause lower revenues for conventional power plants and that grid extensions redistribute and alleviate this effect. The software has also been used to explore energy systems spanning Europe, the Middle East, and North Africa (EUMENA) and Indonesia, Malaysia, and Singapore.\n\nOpen energy system models capture some or all of the energy commodities found in an energy system. All models include the electricity sector. Some models add the heat sector, which can be important for countries with significant district heating. Other models add gas networks. With the advent of emobility, other models still include aspects of the transport sector. Indeed, coupling these various sectors using power-to-X technologies is an emerging area of research.\n\nBalmorel is a market-based energy system model from Denmark. Development was originally financed by the Danish Energy Research Program in 2001. The codebase was made public in March 2001. The Balmorel project maintains an extensive website, from where the codebase and datasets can be download as a zip file. Users are encouraged to register. Documentation is available from the same site. Balmorel is written in GAMS.\n\nThe original aim of the Balmorel project was to construct a partial equilibrium model of the electricity and CHP sectors in the Baltic Sea region, for the purposes of policy analysis. These ambitions and limitations have long since been superseded and Balmorel is no longer tied to its original geography and policy questions. Balmorel classes as a dispatch and investment model and uses a time resolution of one hour. It models electricity and heat supply and demand, and supports the intertemporal storage of both. Balmorel is structured as a pure linear program (no integer variables).\n\n, Balmorel has been the subject of some 22publications. A 2008 study uses Balmorel to explore the Nordic energy system in 2050. The focus is on renewable energy supply and the deployment of hydrogen as the main transport fuel. Given certain assumptions about the future price of oil and carbon and the uptake of hydrogen, the model shows that it is economically optimal to cover, using renewable energy, more than 95% of the primary energy consumption for electricity and district heat and 65% of the transport. A 2010 study uses Balmorel to examine the integration of plug-in hybrid vehicles (PHEV) into a system comprising one quarter wind power and three quarters thermal generation. The study shows that PHEVs can reduce the emissions from the power system if actively integrated, whereas a hands-off approach – letting people charge their cars at will – is likely to result in an increase in emissions. A 2013 study uses Balmorel to examine cost-optimized wind power investments in the Nordic-Germany region. The study investigates the best placement of wind farms, taking into account wind conditions, distance to load, and the generation and transmission infrastructure already in place.\n\nCalliope is an energy system modeling framework, with a focus on flexibility, high spatial and temporal resolution, and the ability to execute different runs using the same base-case dataset. The project is being developed at the Department of Environmental Systems Science, ETH Zurich, Zürich, Switzerland. The project maintains a website, hosts the codebase at GitHub, operates an issues tracker, and runs two email lists. Calliope is written in Python and uses the Pyomo library. It can link to the open source GLPK solver and the commercial CPLEX and Gurobi solvers. PDF documentation is available.\n\nA Calliope model consists of a collection of structured text files, in YAML and CSV formats, that define the technologies, locations, and resource potentials. Calliope takes these files, constructs a pure linear optimization (no integer variables) problem, solves it, and reports the results in the form of pandas data structures for analysis. The framework contains five abstract base technologies – supply, demand, conversion, storage, transmission – from which new concrete technologies can be derived. The design of Calliope enforces the clear separation of framework (code) and model (data).\n\nA 2015 study uses Calliope to compare the future roles of nuclear power and CSP in South Africa. It finds CSP could be competitive with nuclear by 2030 for baseload and more competitive when producing above baseload. CSP also offers less investment risk, less environmental risk, and other co-benefits. A second 2015 study compares a large number of cost-optimal future power systems for Great Britain. Three generation technologies are tested: renewables, nuclear power, and fossil fuels with and without carbon capture and storage (CCS). The scenarios are assessed on financial cost, emissions reductions, and energy security. Up to 60% of variable renewable capacity is possible with little increase in cost, while higher shares require large-scale storage, imports, and/or dispatchable renewables such as tidal range.\n\nDESSTinEE stands for Demand for Energy Services, Supply and Transmission in EuropE. DESSTinEE is a model of the European energy system in 2050 with a focus on the electricity system. DESSTinEE is being developed primarily at the Imperial College Business School, Imperial College London (ICL), London, United Kingdom. The software can be downloaded from the project website. DESSTinEE is written in Excel/VBA and comprises a set of standalone spreadsheets. A flier is available.\n\nDESSTinEE is designed to investigate assumptions about the technical requirements for energy transport – particularly electricity – and the scale of the economic challenge to develop the necessary infrastructure. Forty countries are considered in and around Europe and ten forms of primary and secondary energy are supported. The model uses a predictive simulation technique, rather than solving for either partial or general equilibrium. The model projects annual energy demands for each country to 2050, synthesizes hourly profiles for electricity demand in 2010 and 2050, and simulates the least-cost generation and transmission of electricity around the region.\n\nA 2016 study using DESSTinEE (and a second model eLOAD) examines the evolution of electricity load curves in Germany and Britain from the present until 2050. In 2050, peak loads and ramp rates rise 20–60% and system utilization falls 15–20%, in part due to the substantial uptake of heat pumps and electric vehicles. These are significant changes.\n\nThe Energy Transition Model (ETM) is an interactive web-based model using a holistic description of a country's energy system. It is being developed by Quintel Intelligence, Amsterdam, the Netherlands. The project maintains a project website, an interactive website, and a GitHub repository. ETM is written in Ruby (on Rails) and displays in a web browser. ETM consists of several software components as described in the documentation.\n\nETM is fully interactive. After selecting a region (France, Germany, the Netherlands, Poland, Spain, United Kingdom, EU-27, or Brazil) and a year (2020, 2030, 2040, or 2050), the user can set 300 sliders (or enter numerical values) to explore the following:\n\n\nETM is based on an energy graph (digraph) where nodes (vertices) can convert from one type of energy to another, possibly with losses. The connections (directed edges) are the energy flows and are characterized by volume (in megajoules) and carrier type (such as coal, electricity, usable-heat, and so forth). Given a demand and other choices, ETM calculates the primary energy use, the total cost, and the resulting emissions. The model is demand driven, meaning that the digraph is traversed from \"useful demand\" (such as space heating, hot water usage, and car-kilometers) to \"primary demand\" (the extraction of gas, the import of coal, and so forth).\n\nEnergyPATHWAYS is a bottom-up energy sector model used to explore the near-term implications of long-term deep decarbonization. The lead developer is energy and climate protection consultancy, Evolved Energy Research, San Francisco, USA. The code is hosted on GitHub. EnergyPATHWAYS is written in Python and links to the open source Cbc solver. Alternatively, the GLPK, CPLEX, or Gurobi solvers can be employed. EnergyPATHWAYS utilizes the PostgreSQL object-relational database management system (ORDBMS) to manage its data.\n\nEnergyPATHWAYS is a comprehensive accounting framework used to construct economy-wide energy infrastructure scenarios. While portions of the model do use linear programming techniques, for instance, for electricity dispatch, the EnergyPATHWAYS model is not fundamentally an optimization model and embeds few decision dynamics. EnergyPATHWAYS offers detailed energy, cost, and emissions accounting for the energy flows from primary supply to final demand. The energy system representation is flexible, allowing for differing levels of detail and the nesting of cities, states, and countries. The model uses hourly least-cost electricity dispatch and supports power-to-gas, short-duration energy storage, long-duration energy storage, and demand response. Scenarios typically run to 2050.\n\nA predecessor of the EnergyPATHWAYS software, named simply PATHWAYS, has been used to construct policy models. The California PATHWAYS model was used to inform Californian state climate targets for 2030. And the US PATHWAYS model contributed to the UN Deep Decarbonization Pathways Project (DDPP) assessments for the United States. , the DDPP plans to employ EnergyPATHWAYS for future analysis.\n\nETEM stands for Energy Technology Environment Model. The ETEM model offers a similar structure to OSeMOSYS but is aimed at urban planning. The software is being developed by the ORDECSYS company, Chêne-Bougeries, Switzerland, supported with European Union and national research grants. The project has two websites. The software can be downloaded from first of these websites (but , this looks out of date). A manual is available with the software. ETEM is written in MathProg. Presentations describing ETEM are available.\n\nETEM is a bottom-up model that identifies the optimal energy and technology options for a regional or city. The model finds an energy policy with minimal cost, while investing in new equipment (new technologies), developing production capacity (installed technologies), and/or proposing the feasible import/export of primary energy. ETEM typically casts forward 50years, in two or five year steps, with time slices of four seasons using typically individual days or finer. The spatial resolution can be highly detailed. Electricity and heat are both supported, as are district heating networks, household energy systems, and grid storage, including the use of plug-in hybrid electric vehicles (PHEV). ETEM-SG, a development, supports demand response, an option which would be enabled by the development of smart grids.\n\nThe ETEM model has been applied to Luxembourg, the Geneva and Basel-Bern-Zurich cantons in Switzerland, and the Grenoble metropolitan and Midi-Pyrénées region in France. A 2005 study uses ETEM to study climate protection in the Swiss housing sector. The ETEM model was coupled with the GEMINI-E3 world computable general equilibrium model (CGEM) to complete the analysis. A 2012 study examines the design of smart grids. As distribution systems become more intelligent, so must the models needed to analysis them. ETEM is used to assess the potential of smart grid technologies using a case study, roughly calibrated on the Geneva canton, under three scenarios. These scenarios apply different constraints on emissions and electricity imports. A stochastic approach is used to deal with the uncertainty in future electricity prices and the uptake of electric vehicles.\n\nficus is a mixed integer optimization model for local energy systems. It is being developed at the Institute for Energy Economy and Application Technology, Technical University of Munich, Munich, Germany. The project maintains a website. The project is hosted on GitHub. ficus is written in Python and uses the Pyomo library. The user can choose between the open source GLPK solver or the commercial CPLEX and Gurobi solvers.\n\nBased on URBS, ficus was originally developed for optimizing the energy systems of factories and has now been extended to include local energy systems. ficus supports multiple energy commodities – goods that can be imported or exported, generated, stored, or consumed – including electricity and heat. It supports multiple-input and multiple-output energy conversion technologies with load-dependent efficiencies. The objective of the model is to supply the given demand at minimal cost. ficus uses exogenous cost time series for imported commodities as well as peak demand charges with a configurable timebase for each commodity in use.\n\noemof stands for Open Energy Modelling Framework. The project is managed by the Reiner Lemoine Institute, Berlin, Germany and the Center for Sustainable Energy Systems (CSES or ZNES) at the University of Flensburg and the Flensburg University of Applied Sciences, both Flensburg, Germany. The project runs two websites and a GitHub repository. oemof is written in Python and uses Pyomo and COIN-OR components for optimization. Energy systems can be represented using spreadsheets (CSV) which should simplify data preparation. was released on 1December 2016.\n\noemof classes as an energy modeling framework. It consists of a linear or mixed integer optimization problem formulation library (solph), an input data generation library (feedin-data), and other auxiliary libraries. The solph library is used to represent multi-regional and multi-sectoral (electricity, heat, gas, mobility) systems and can optimize for different targets, such as financial cost or emissions. Furthermore, it is possible to switch between dispatch and investment modes. In terms of scope, oemof can capture the European power system or alternatively it can describe a complex local power and heat sector scheme.\n\nOSeMOSYS stands for Open Source Energy Modelling System. OSeMOSYS is intended for national and regional policy development and uses an intertemperal optimization framework. The model posits a single socially motivated operator/investor with perfect foresight. The OSeMOSYS project is a community endeavor, supported by the Energy Systems Analysis Group (dESA), KTH Royal Institute of Technology, Stockholm, Sweden. The project maintains a website providing background. The project also offers several active internet forums on Reddit. OSeMOSYS was originally written in MathProg, a high-level mathematical programming language. It was subsequently reimplemented in GAMS and Python and all three codebases are now maintained. The project also provides a test model called UTOPIA. A manual is available.\n\nOSeMOSYS provides a framework for the analysis of energy systems over the medium (10–15 years) and long term (50–100 years). OSeMOSYS uses pure linear optimization, with the option of mixed integer programming for the treatment of, for instance, discrete power plant capacity expansions. It covers most energy sectors, including heat, electricity, and transport. OSeMOSYS is driven by exogenously defined energy services demands. These are then met through a set of technologies which draw on a set of resources, both characterized by their potentials and costs. These resources are not limited to energy commodities and may include, for example, water and land-use. This enables OSeMOSYS to be applied in domains other than energy, such as water systems. Technical constraints, economic restrictions, and/or environmental targets may also be imposed to reflect policy considerations. OSeMOSYS is available in extended and compact MathProg formulations, either of which should give identical results. In its extended version, OSeMOSYS comprises a little more than 400 lines of code.\n\nA key paper describing OSeMOSYS is available. A 2011 study uses OSeMOSYS to investigate the role of household investment decisions. A 2012 study extends OSeMOSYS to capture the salient features of a smart grid. The paper explains how to model variability in generation, flexible demand, and grid storage and how these impact on the stability of the grid. OSeMOSYS has been applied to village systems. A 2015 paper compares the merits of stand-alone, mini-grid, and grid electrification for rural areas in Timor-Leste under differing levels of access. In a 2016 study, OSeMOSYS is modified to take into account realistic consumer behavior. Another 2016 study uses OSeMOSYS to build a local multi-regional energy system model of the Lombardy region in Italy. One of the aims of the exercise was to encourage citizens to participate in the energy planning process. Preliminary results indicate that this was successful and that open modeling is needed to properly include both the technological dynamics and the non-technological issues. A 2017 paper covering Alberta, Canada factors in the risk of overrunning specified emissions targets because of technological uncertainty. Among other results, the paper finds that solar and wind technologies are built out seven and five years earlier respectively when emissions risks are included. Another 2017 paper analyses the electricity system in Cyprus and finds that, after European Union environmental regulations are applied post-2020, a switch from oil-fired to natural gas generation is indicated.\n\nOSeMOSYS has been used to construct wide-area electricity models for Africa, comprising 45countries and South America, comprising 13countries. It has also been used to support United Nations' regional climate, land, energy, and water strategies (CLEWS) for the Sava river basin, central Europe, the Syr Darya river basin, eastern Europe, and Mauritius. Models have previously been built for the Baltic States, Bolivia, Nicaragua, and Sweden.\n\nIn 2016, work started on a browser-based interface to OSeMOSYS, known as the Model Management Infrastructure (MoManI). Lead by the UN Department of Economic and Social Affairs (DESA), MoManI is being trialled in selected countries. The interface can be used to construct models, visualize results, and develop better scenarios. Atlantis is the name of a fictional country case-study for training purposes.\n\nThe OSeMBE reference model covering western and central Europe was announced on 27 April 2018. The model uses the MathProg implemention of OSeMOSYS but requires a small patch first. The model, funded as part of Horizon 2020 and falling under work package WP7 of the REEEM project, will be used to help stakeholders engage with a range of sustainable energy futures for Europe. The REEEM project runs from early-2016 till mid-2020.\n\nOSeMOSYS is used for university teaching. To that end, a 2017 paper describes the basic UTOPIA model, with an explanation on how to generate Pareto frontiers for a given system.\n\nPyPSA stands for Python for Power System Analysis. PyPSA is a free software toolbox for simulating and optimizing electric power systems and allied sectors. It supports conventional generation, variable wind and solar generation, electricity storage, coupling to the natural gas, hydrogen, heat, and transport sectors, and hybrid alternating and direct current networks. Moreover, PyPSA is designed to scale well. The project is managed by the Institute for Automation and Applied Informatics (IAI), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany, although the project itself exists independently under its own name and accounts. The project maintains a website and runs an email list. PyPSA itself is written in Python and uses the Pyomo library. The source code is hosted on GitHub and is also released periodically as a PyPI package.\n\nThe basic functionality of PyPSA is described in a 2018 paper. PyPSA sits between traditional steady-state power flow analysis software and full multi-period energy system models. It can be invoked using either non-linear power flow equations for system simulation or linearized approximations to enable the joint optimization of operations and investment across multiple periods. Generator ramping and multi-period up and down-times can be specified, DSM is supported, but demand remains price inelastic.\n\nA 2018 study examines potential synergies between sector coupling and transmission reinforcement in a future European energy system constrained to reduce carbon emissions by 95%. The PyPSA-Eur-Sec-30 model captures the demand-side management potential of battery electric vehicles (BEV) as well as the role that power-to-gas, long-term thermal energy storage, and related technologies can play. Results indicate that BEVs can smooth the daily variations in solar power while the remaining technologies smooth the synoptic and seasonal variations in both demand and renewable supply. Substantial buildout of the electricity grid is required for a least-cost configuration. More generally, such a system is both feasible and affordable. The underlying datasets are available from Zenodo.\n\n, PyPSA is used by more than a dozen research institutes and companies worldwide. Some research groups have independently extended the software, for instance to model integer transmission expansion.\n\nTEMOA stands for Tools for Energy Model Optimization and Analysis. The software is being developed by the Department of Civil, Construction, and Environmental Engineering, North Carolina State University, Raleigh, North Carolina, USA. The project runs a website and a forum. The source code is hosted on GitHub. The model is programmed in Pyomo, an optimization components library written in Python. TEMOA can be used with any solver that Pyomo supports, including the open source GLPK solver. TEMOA uses version control to publicly archive source code and datasets and thereby enable third-parties to verify all published modeling work.\n\nTEMOA classes as a modeling framework and is used to conduct analysis using a bottom-up, technology rich energy system model. The model objective is to minimize the system-wide cost of energy supply by deploying and utilizing energy technologies and commodities over time to meet a set of exogenously specified end-use demands. TEMOA is \"strongly influenced by the well-documented MARKAL/TIMES model generators\".\n\nStatistics for the 25 open energy modeling projects listed are as follows:\n\nThe GAMS language requires a proprietary environment and its significant cost effectively limits participation to those who can access an institutional copy.\n\nA number of technical component models are now also open source. While these component models do not constitute systems models aimed at public policy development (the focus of this page), they nonetheless warrant a mention. Component models can be linked or otherwise adapted into these broader initiatives.\n\nA number of electricity auction models have been written in GAMS, AMPL, MathProg, and other languages. These include:\n\n\n\n\nMany projects rely on a pure linear or mixed integer solver to perform classical optimization, constraint satisfaction, or some mix of the two. While there are several open source solver projects, the most commonly deployed solver is GLPK. GLPK has been adopted by Calliope, ETEM, ficus, OSeMOSYS, SWITCH, and TEMOA. Another alternative is the Clp solver. Proprietary solvers outperform open source solvers by a considerable margin (perhaps ten-fold), so choosing an open solver will limit performance in terms of both speed and memory consumption.\n\nGeneral\n\n\nSoftware\n\n\n\n"}
{"id": "1969007", "url": "https://en.wikipedia.org/wiki?curid=1969007", "title": "Percentage point", "text": "Percentage point\n\nA percentage point or percent point is the unit for the arithmetic difference of two percentages. For example, moving up from 40% to 44% is a 4 \"percentage point\" increase, but is an actual 10 percent increase in what is being measured. In the literature, the percentage point unit is usually either written out, or abbreviated as \"pp\" or \"p.p.\" to avoid ambiguity. After the first occurrence, some writers abbreviate by using just \"point\" or \"points\".\n\nConsider the following hypothetical example: In 1980, 50 percent of the population smoked, and in 1990 only 40 percent smoked. One can thus say that from 1980 to 1990, the prevalence of smoking decreased by 10 \"percentage points\" although smoking did not decrease by 10 percent (it decreased by \"20 percent\") – percentages indicate ratios, not differences.\n\nPercentage-point differences are one way to express a risk or probability. Consider a drug that cures a given disease in 70 percent of all cases, while without the drug, the disease heals spontaneously in only 50 percent of cases. The drug reduces absolute risk by 20 percentage points. Alternatives may be more meaningful to consumers of statistics, such as the reciprocal, also known as the number needed to treat (NNT). In this case, the reciprocal transform of the percentage-point difference would be 1/(20pp) = 1/0.20 = 5. Thus if 5 patients are treated with the drug, one could expect to heal one more case of the disease than would have occurred in the absence of the drug.\n\nFor measurements involving percentages as a unit, such as, growth, yield, or ejection fraction, statistical deviations and related descriptive statistics, including the standard deviation and root-mean-square error, the result should be expressed in units of percentage points instead of percentage. Mistakenly using percentage as the unit for the standard deviation is confusing, since percentage is also used as a unit for the relative standard deviation, i.e. standard deviation divided by average value (coefficient of variation).\n\n"}
{"id": "15882673", "url": "https://en.wikipedia.org/wiki?curid=15882673", "title": "Plate notation", "text": "Plate notation\n\nIn Bayesian inference, plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.\n\nIn this example, we consider Latent Dirichlet allocation, a Bayesian network that models how documents in a corpus are topically related. There are two variables not in any plate; \"α\" is the parameter of the uniform Dirichlet prior on the per-document topic distributions, and \"β\" is the parameter of the uniform Dirichlet prior on the per-topic word distribution.\n\nThe outermost plate represents all the variables related to a specific document, including formula_1, the topic distribution for document \"i\". The \"M\" in the corner of the plate indicates that the variables inside are repeated \"M\" times, once for each document. The inner plate represents the variables associated with each of the formula_2 words in document \"i\": formula_3 is the topic for the \"j\"th word in document \"i\", and formula_4 is the actual word used.\n\nThe \"N\" in the corner represents the repetition of the variables in the inner plate formula_2 times, once for each word in document \"i\". The circle representing the individual words is shaded, indicating that each formula_4 is observable, and the other circles are empty, indicating that the other variables are latent variables. The directed edges between variables indicate dependencies between the variables: for example, each formula_4 depends on formula_3 and \"β\".\n\nA number of extensions have been created by various authors to express more information than simply the conditional relationships. However, few of these have become standard. Perhaps the most commonly used extension is to use rectangles in place of circles to indicate non-random variables—either parameters to be computed, hyperparameters given a fixed value (or computed through empirical Bayes), or variables whose values are computed deterministically from a random variable.\n\nThe diagram on the right shows a few more non-standard conventions used in some articles in Wikipedia (e.g. variational Bayes):\n\nPlate notation has been implemented in various TeX/LaTeX drawing packages, but also as part of graphical user interfaces to Bayesian statistics programs such as BUGS and BayesiaLab.\n"}
{"id": "2142840", "url": "https://en.wikipedia.org/wiki?curid=2142840", "title": "Procept", "text": "Procept\n\nA procept is an amalgam of three components: a process which produces a mathematical object and a symbol which is used to represent either process or object. It derives from the work of Eddie Gray and David O. Tall, and is a much used construct in mathematics education research.\n\nThe notion was first published in a paper in the Journal for Research in Mathematics Education in 1994, and is part of the process-object literature. This body of literature suggests that mathematical objects are formed by encapsulating processes, that is to say that the mathematical object 3 is formed by an encapsulation of the process of counting: 1,2,3...\n\nGray & Tall's notion of procept improved upon the existing literature by noting that mathematical notation is often ambiguous as to whether it refers to process or object. Examples of such notations are:\n\n\n"}
{"id": "510629", "url": "https://en.wikipedia.org/wiki?curid=510629", "title": "Quantitative analyst", "text": "Quantitative analyst\n\nA quantitative analyst (or, in financial jargon, a quant) is a person who specializes in the application of mathematical and statistical methods – such as numerical or quantitative techniques – to financial and risk management problems. The occupation is similar to those in industrial mathematics in other industries.\n\nAlthough the original quantitative analysts were \"sell side quants\" from market maker firms, concerned with derivatives pricing and risk management, the meaning of the term has expanded over time to include those individuals involved in almost any application of mathematics in finance, including the buy side. Examples include statistical arbitrage, quantitative investment management, algorithmic trading, and electronic market making. \n\nQuantitative finance started in 1900 with Louis Bachelier's doctoral thesis \"Theory of Speculation\", which provided a model to price options under a Normal Distribution.\n\nHarry Markowitz's 1952 doctoral thesis \"Portfolio Selection\" and its published version was one of the first efforts in economics journals to formally adapt mathematical concepts to finance (mathematics was until then confined to mathematics, statistics or specialized economics journals). Markowitz formalized a notion of mean return and covariances for common stocks which allowed him to quantify the concept of \"diversification\" in a market. He showed how to compute the mean return and variance for a given portfolio and argued that investors should hold only those portfolios whose variance is minimal among all portfolios with a given mean return. Although the language of finance now involves Itō calculus, management of risk in a quantifiable manner underlies much of the modern theory.\n\nIn 1965 Paul Samuelson introduced stochastic calculus into the study of finance. In 1969 Robert Merton promoted continuous stochastic calculus and continuous-time processes. Merton was motivated by the desire to understand how prices are set in financial markets, which is the classical economics question of \"equilibrium,\" and in later papers he used the machinery of stochastic calculus to begin investigation of this issue.\n\nAt the same time as Merton's work and with Merton's assistance, Fischer Black and Myron Scholes developed the Black–Scholes model, which was awarded the 1997 Nobel Memorial Prize in Economic Sciences. It provided a solution for a practical problem, that of finding a fair price for a European call option, i.e., the right to buy one share of a given stock at a specified price and time. Such options are frequently purchased by investors as a risk-hedging device. In 1981, Harrison and Pliska used the general theory of continuous-time stochastic processes to put the Black–Scholes model on a solid theoretical basis, and showed how to price numerous other derivative securities.\n\nEmanuel Derman's 2004 book \"My Life as a Quant\" helped to both make the role of a quantitative analyst better known outside of finance, and to popularize the abbreviation \"quant\" for a quantitative analyst.\n\nQuantitative analysts often come from applied mathematics, physics or engineering backgrounds rather than economics-related fields, and quantitative analysis is a major source of employment for people with mathematics and physics PhD degrees, or with financial mathematics DEA degrees in the French education system. Typically, a quantitative analyst will also need extensive skills in computer programming, most commonly C, C++, Java, R, MATLAB, Mathematica, Python.\n\nThis demand for quantitative analysts has led to a resurgence in demand for actuarial qualifications as well as creation of specialized Masters and PhD courses in financial engineering, mathematical finance, computational finance, and/or financial reinsurance. In particular, Master's degrees in mathematical finance, financial engineering, operations research, computational statistics, machine learning, and financial analysis are becoming more popular with students and with employers. See Master of Quantitative Finance; Master of Financial Economics.\n\nData science and machine learning analysis and modelling methods are being increasingly employed in portfolio performance and portfolio risk modelling, and as such data science and machine learning Master's graduates are also in demand as quantitative analysts.\n\nIn sales & trading, quantitative analysts work to determine prices, manage risk, and identify profitable opportunities. Historically this was a distinct activity from trading but the boundary between a desk quantitative analyst and a quantitative trader is increasingly blurred, and it is now difficult to enter trading as a profession without at least some quantitative analysis education. In the field of algorithmic trading it has reached the point where there is little meaningful difference. Front office work favours a higher speed to quality ratio, with a greater emphasis on solutions to specific problems than detailed modeling. FOQs typically are significantly better paid than those in back office, risk, and model validation. Although highly skilled analysts, FOQs frequently lack software engineering experience or formal training, and bound by time constraints and business pressures tactical solutions are often adopted.\n\nQuantitative analysis is used extensively by asset managers. Some, such as FQ, AQR or Barclays, rely almost exclusively on quantitative strategies while others, such as Pimco, Blackrock or Citadel use a mix of quantitative and fundamental methods.\n\nMajor firms invest large sums in an attempt to produce standard methods of evaluating prices and risk. These differ from front office tools in that Excel is very rare, with most development being in C++, though Java and C# are sometimes used in non-performance critical tasks. LQs spend more time modeling ensuring the analytics are both efficient and correct, though there is tension between LQs and FOQs on the validity of their results. LQs are required to understand techniques such as Monte Carlo methods and finite difference methods, as well as the nature of the products being modeled.\n\nOften the highest paid form of Quant, ATQs make use of methods taken from signal processing, game theory, gambling Kelly criterion, market microstructure, econometrics, and time series analysis. Algorithmic trading includes statistical arbitrage, but includes techniques largely based upon speed of response, to the extent that some ATQs modify hardware and Linux kernels to achieve ultra low latency.\n\nThis has grown in importance in recent years, as the credit crisis exposed holes in the mechanisms used to ensure that positions were correctly hedged, though in no bank does the pay in risk approach that in front office. A core technique is value at risk, and this is backed up with various forms of stress test (financial), economic capital analysis and direct analysis of the positions and models used by various bank's divisions.\n\nIn the aftermath of the financial crisis, there surfaced the recognition that quantitative valuation methods were generally too narrow in their approach. An agreed upon fix adopted by numerous financial institutions has been to improve collaboration.\n\nModel validation (MV) takes the models and methods developed by front office, library, and modeling quantitative analysts and determines their validity and correctness. The MV group might well be seen as a superset of the quantitative operations in a financial institution, since it must deal with new and advanced models and trading techniques from across the firm. Before the crisis however, the pay structure in all firms was such that MV groups struggle to attract and retain adequate staff, often with talented quantitative analysts leaving at the first opportunity. This gravely impacted corporate ability to manage model risk, or to ensure that the positions being held were correctly valued. An MV quantitative analyst would typically earn a fraction of quantitative analysts in other groups with similar length of experience. In the years following the crisis, this has changed. Regulators now typically talk directly to the quants in the middle office such as the model validators, and since profits highly depend of the regulatory infrastructure, model validation has gained in weight and importance with respect to the quants in the front office.\n\nQuantitative developers are computer specialists that assist, implement and maintain the quantitative models. They tend to be highly specialised language technicians that bridge the gap between software developer and quantitative analysts.\n\nBecause of their backgrounds, quantitative analysts draw from various forms of mathematics: statistics and probability, calculus centered around partial differential equations, linear algebra, discrete mathematics, and econometrics. Some on the buy side may use machine learning. The\nmajority of quantitative analysts have received little formal education in mainstream economics, and often apply a mindset drawn from the physical sciences. Quants use mathematical skills learned from diverse fields such as computer science, physics and engineering. These skills include (but are not limited to) advanced statistics, linear algebra and partial differential equations as well as solutions to these based upon numerical analysis.\n\nCommonly used numerical methods are:\n\nA typical problem for a mathematically oriented quantitative analyst would be to develop a model for pricing, hedging, and risk-managing a complex derivative product. These quantitative analysts tend to rely more on numerical analysis than statistics and econometrics. The mindset is to prefer a deterministically \"correct\" answer, as once there is agreement on input values and market variable dynamics, there is only one correct price for any given security (which can be demonstrated, albeit often inefficiently, through a large volume of Monte Carlo simulations).\n\nA typical problem for a statistically oriented quantitative analyst would be to develop a model for deciding which stocks are relatively expensive and which stocks are relatively cheap. The model might include a company's book value to price ratio, its trailing earnings to price ratio, and other accounting factors. An investment manager might implement this analysis by buying the underpriced stocks, selling the overpriced stocks, or both. Statistically oriented quantitative analysts tend to have more of a reliance on statistics and econometrics, and less of a reliance on sophisticated numerical techniques and object-oriented programming. These quantitative analysts tend to be of the psychology that enjoys trying to find the best approach to modeling data, and can accept that there is no \"right answer\" until time has passed and we can retrospectively see how the model performed. Both types of quantitative analysts demand a strong knowledge of sophisticated mathematics and computer programming proficiency.\n\nOne of the principal mathematical tools of quantitative finance is stochastic calculus.\n\n\n\n\n\n\n"}
{"id": "159735", "url": "https://en.wikipedia.org/wiki?curid=159735", "title": "Quasi-empirical method", "text": "Quasi-empirical method\n\nQuasi-empirical methods are methods applied in science and mathematics to achieve epistemology similar to that of empiricism (thus \"quasi- + empirical\") when experience cannot falsify the ideas involved. Empirical research relies on empirical evidence, and its empirical methods involve experimentation and disclosure of apparatus for reproducibility, by which scientific findings are validated by other scientists. Empirical methods are studied extensively in the philosophy of science, but they cannot be used directly in fields whose hypotheses cannot be falsified by real experiment (for example, mathematics, philosophy, theology, and ideology). Because of such empirical limits in science, the scientific method must rely not only on empirical methods but sometimes also on quasi-empirical ones. The prefix \"quasi-\" came to denote methods that are \"almost\" or \"socially approximate\" an ideal of truly empirical methods.\n\nIt is unnecessary to find all counterexamples to a theory; all that is required to disprove a theory logically is one counterexample. The converse does not prove a theory; Bayesian inference simply makes a theory more likely, by weight of evidence.\n\nOne can argue that no science is capable of finding all counter-examples to a theory, therefore, no science is strictly empirical, it's all quasi-empirical. But usually, the term \"quasi-empirical\" refers to the means of choosing problems to focus on (or ignore), selecting prior work on which to build an argument or proof, notations for informal claims, peer review and acceptance, and incentives to discover, ignore, or correct errors. These are common to both science and mathematics, and do not include experimental method.\n\nAlbert Einstein's discovery of the general relativity theory relied upon thought experiments and mathematics. Empirical methods only became relevant when confirmation was sought. Furthermore, some empirical confirmation was found only some time after the general acceptance of the theory.\n\nThought experiments are almost standard procedure in philosophy, where a conjecture is tested out in the imagination for possible effects on experience; when these are thought to be implausible, unlikely to occur, or not actually occurring, then the conjecture may be either rejected or amended. Logical positivism was a perhaps extreme version of this practice, though this claim is open to debate.\n\nPost-20th-century philosophy of mathematics is mostly concerned with quasi-empirical mathematical methods, especially as reflected in the actual mathematical practice of working mathematicians. \n\n"}
{"id": "58444022", "url": "https://en.wikipedia.org/wiki?curid=58444022", "title": "Rudy Horne", "text": "Rudy Horne\n\nRudy Lee Horne (1968 – 2017) was an African-American mathematician and professor of mathematics at Morehouse College. He worked on dynamical systems, including nonlinear waves. He was the mathematics consultant for the film \"Hidden Figures\".\n\nHorne grew up in the south side of Chicago. His father worked at Sherwin-Williams. He graduated from Crete-Monee High School. He completed a double degree in mathematics and physics at the University of Oklahoma in 1991. He joined the University of Colorado Boulder for his postgraduate studies, earning a master's in physics in 1994 and in mathematics in 1996. He completed his doctorate, \"Collision induced timing jitter and four-wave mixing in wavelength division multiplexing soliton systems\", in 2001 which was supervised by Mark J. Ablowitz. He was the first African American to graduate from the University of Colorado Boulder Department of Applied Mathematics.\n\nAfter completing his PhD, Horne had a position at the California State University, East Bay. before working as postdoctoral researcher at the University of North Carolina at Chapel Hill, with Chris Jones. Horne joined Florida State University in 2005. Horne joined Morehouse College in 2010 and was promoted to associate professor of mathematics in 2015. He continued to study four-wave mixing. His work considered nonlinear optical phenomena. He uncovered effects in parity-time symmetric systems.\n\nHorne was recommended to serve as a mathematics consultant for Hidden Figures by Morehouse College. He worked closely with Theodore Melfi ensured the actors knew how to pronounce \"Euler's\". He spent four months working with 20th Century Fox. In particular, Horne worked with Taraji P. Henson on the mathematics she required for her role as Katherine Johnson. He taught the cast how to get excited by mathematics. His handwriting is on screen during a scene at the beginning of the film where Katherine Johnson solves a quadratic equation. He appeared on the interview series \"In the Know.\" Horne completed a Mathematical Association of America \"Maths Fest\" tour where he discussed the mathematics in Hidden Figures, focussing on the calculations that concerned Glenn's orbit around in 1962. Her appeared on NPR's Closer Look.\n\nHe died on December 11, 2017. The University of Colorado Boulder established a Rudy Lee Horne Memorial Fellowship in his honour. He was described as a \"rock star\", inspiring generations of black students. He was awarded the National Association of Mathematicians (NAM) lifetime achievement award posthumously in 2018.\n"}
{"id": "39402121", "url": "https://en.wikipedia.org/wiki?curid=39402121", "title": "SAMPL", "text": "SAMPL\n\nSAMPL, which stands for \"Stochastic AMPL\", is an algebraic modeling language resulting by expanding the well-known language AMPL with extended syntax and keywords. It is designed specifically for representing stochastic programming problems and, through recent extensions, problems with chance constraints, integrated chance constraints and robust optimization problems. \nIt can generate the deterministic equivalent version of the instances, using all the solvers AMPL connects to, or generate an SMPS representation and use specialized decomposition based solvers, like FortSP.\n\nSAMPL shares all language features with AMPL, and adds some constructs specifically designed for expressing scenario based stochastic programming and robust optimization.\n\nTo express scenario-based SP problems, additional constructs describe the tree structure and group the decision variable into stages. Moreover, it is possible to specify which parameter stores the probabilities for each branch of the tree and which set represents the scenario set. Other constructs to easily define chance constraints and integrated chance constraint in an SP problem are available as well.\nUsing these language constructs allows to retain the structure of the problem, hence making it available to the solvers, which might exploit it using specialized decomposition methods like Benders' decomposition to speed-up the solution.\n\nSAMPL supports constructs to describe three types of robust optimization formulations:\n\nSAMPL is currently available as a part of the software AMPLDev (distributed by www.optirisk-systems.com). It supports many popular 32- and 64-bit platforms including Windows, Linux and Mac OS X. A free evaluation version with limited functionality is available.\n\nThe following is the SAMPL version of a simple problem (Dakota), to show the SP related constructs. It does not include the data file, which follows the normal AMPL syntax (see the example provided in the AMPL Wikipedia page for further reference).\n\nSAMPL instance level format for SP problems is SMPS, and therefore the problem can be solved by any solver which supports that standard. One of such solvers (FortSP) is included in the standard SAMPL distribution. Regarding robust optimization problems, the needed solver depend on the specific formulation used, as Ben-Tal and Nemirovski formulation need a second-order cone capable solver.\n\n\n"}
{"id": "8965926", "url": "https://en.wikipedia.org/wiki?curid=8965926", "title": "Sand table", "text": "Sand table\n\nA sand table uses constrained sand for modelling or educational purposes. The original version of a sand table may be the abax used by early Greek students. In the modern era, one common use for a sand table is to make terrain models for military planning and wargaming.\n\nAn abax was a table covered with sand commonly used by students, particularly in Greece, to perform studies such as writing, geometry, and calculations.\n\nAn abax was the predecessor to the abacus. Objects, such as stones, were added for counting and then columns for place-valued arithmetic. The demarcation between an abax and an abacus seems to be poorly defined in history; moreover, modern definitions of the word \"abacus\" universally describe it as a frame with rods and beads and, in general, do not include the definition of \"sand table\".\n\nThe sand table may well have been the predecessor to some board games. (\"The word abax, or abacus, is used both for the reckoning-board with its counters and the play-board with its pieces, ...\"). \"Abax\" is from the old Greek for \"sand table\".\n\nAn Arabic word for sand (or dust) is \"ghubar\" (or \"gubar\"), and Western numerals (the decimal digits 0–9) are derived from the style of digits written on \"ghubar\" tables in North-West Africa and Iberia, also described as the 'West Arabic' or 'gubar' style.\n\nSand tables have been used for military planning and wargaming for many years as a field expedient, small-scale map, and in training for military actions. In 1890 a Sand table room was built at the Royal Military College of Canada for use in teaching cadets military tactics; this replaced the old sand table room in a pre-college building, in which the weight of the sand had damaged the floor. The use of sand tables increasingly fell out of favour with improved maps, aerial and satellite photography, and later, with digital terrain simulations. More modern sand tables have incorporated Augmented Reality, such as the Augmented Reality Sandtable (ARES) developed by the Army Research Laboratory. Today, virtual and conventional sand tables are used in operations training.\n\nIn 1991, \"Special Forces teams discovered an elaborate sand-table model of the Iraqi military plan for the defense of Kuwait City. Four huge red arrows from the sea pointed at the coastline of Kuwait City and the huge defensive effort positioned there. Small fences of concertina wire marked the shoreline and models of artillery pieces lined the shore area. Throughout the city were plastic models of other artillery and air defense positions, while thin, red-painted strips of board designated supply routes and main highways.\"\n\nIn 2006, Google Earth users looking at satellite photography of China found a several \"kilometre\" large \"sand table\" scale model, strikingly reminiscent of a mountainous region (Aksai Chin) which China occupies militarily in a disputed zone with India, 2400 km from the model's location. Speculation has been rife that the terrain is used for military exercises of familiarisation.\nA sand table is a device useful for teaching in the early grades and for special needs children.\n\n\n\n\n"}
{"id": "1700591", "url": "https://en.wikipedia.org/wiki?curid=1700591", "title": "School Mathematics Study Group", "text": "School Mathematics Study Group\n\nThe School Mathematics Study Group (SMSG) was an American academic think tank focused on the subject of reform in mathematics education. Directed by Edward G. Begle and financed by the National Science Foundation, the group was created in the wake of the Sputnik crisis in 1958 and tasked with creating and implementing mathematics curricula for primary and secondary education, which it did until its termination in 1977. The efforts of the SMSG yielded a reform in mathematics education known as New Math which was promulgated in a series of reports, culminating in a series published by Random House called the \"New Mathematical Library\". In the early years, SMSG also produced a set of draft textbooks in typewritten paperback format for elementary, middle and high school students. \n\nPerhaps the most authoritative collection of materials from the School Mathematics Study Group is now housed in the Archives of American Mathematics in the University of Texas at Austin's Center for American History.\n\n\n\n"}
{"id": "48347082", "url": "https://en.wikipedia.org/wiki?curid=48347082", "title": "Scope (logic)", "text": "Scope (logic)\n\nIn logic, the scope of a quantifier or a quantification is the range in the formula where the quantifier \"engages in\". It is put right after the quantifier, often in parentheses. Some authors describe this as including the variable put right after the forall or exists symbol. In the formula , for example, (or ) is the scope of the quantifier (or ).\n\nA variable in the formula is free, if and only if it does not occur in the scope of any quantifier for that variable. A term is free for a variable in the formula (i.e. free to substitute that variable that occurs free), if and only if that variable does not occur free in the scope of any quantifier for any variable in the term.\n\n"}
{"id": "24824751", "url": "https://en.wikipedia.org/wiki?curid=24824751", "title": "Self-dissimilarity", "text": "Self-dissimilarity\n\nSelf-dissimilarity is a measure of complexity defined in a series of papers by David Wolpert and William G. Macready.\nThe degrees of self-dissimilarity between the patterns of a system observed at various scales (e.g. the average matter density of a physical body for volumes at different orders of magnitude) constitute a complexity \"signature\" of that system.\n\n"}
{"id": "45696716", "url": "https://en.wikipedia.org/wiki?curid=45696716", "title": "Sheaf of planes", "text": "Sheaf of planes\n\nIn mathematics, a sheaf of planes is the set of all planes that have the same common line. It may also be known as a pencil or fan of planes.\n\nWhen extending the concept of line to the line at infinity, a set of parallel planes can be seen as a \"sheaf of planes\" intersecting in a \"line at infinity\". To distinguish it from the more common definition the adjective \"parallel\" can be added to it, resulting in the expression: \"parallel sheaf of planes\".\n\n"}
{"id": "6129269", "url": "https://en.wikipedia.org/wiki?curid=6129269", "title": "Transport of structure", "text": "Transport of structure\n\nIn mathematics, transport of structure is the definition of a new structure on an object by reference to another object on which a similar structure already exists. Definitions by transport of structure are regarded as canonical.\n\nSince mathematical structures are often defined in reference to an underlying space, many examples of transport of structure involve spaces and mappings between them. For example, if \"V\" and \"W\" are vector spaces, and if formula_1 is an isomorphism, and if formula_2 is an inner product on formula_3, then we can define an inner product formula_4 on \"V\" by\nAlthough the equation makes sense even when formula_6 is not an isomorphism, it only defines an inner product on \"V\" when formula_6 is, since otherwise it will cause formula_8 to be degenerate. The idea is that formula_6 allows us to consider \"V\" and \"W\" as \"the same\" vector space, and if we follow this analogy, we can transport an inner product from one to the other.\n\nA more involved example comes from differential topology, in which we have the notion of a smooth manifold. If \"M\" is such a manifold, and if \"X\" is any topological space which is homeomorphic to \"M\", we can consider \"X\" as a smooth manifold as well. That is, let formula_10 be a homeomorphism; we must define coordinate charts on \"X\", which we will do by \"pulling back\" coordinate charts on \"M\" through formula_6. Recall that a coordinate chart on formula_12 is an open set \"U\" together with an injective map\nfor some \"n\"; to get such a chart on \"X\", we let\nFurthermore, it is required that the charts cover \"M\", we must check that the transported charts cover \"X\", which follows immediately from the fact that formula_6 is a bijection. Finally, since \"M\" is a \"smooth\" manifold, we have that if \"U\" and \"V\", with their maps\nare two charts on \"M\", then the composition, the \"transition map\"\nis smooth. We must check this for our transported charts on \"X\". We have\nand therefore\nTherefore the transition map for formula_24 and formula_25 is the same as that for \"U\" and \"V\", hence smooth. Therefore \"X\" is a smooth manifold via transport of structure.\n\nAlthough the second example involved considerably more checking, the principle was the same, and any experienced mathematician would have no difficulty performing the necessary verifications. Therefore when such an operation is indicated, it is invoked merely as \"transport of structure\" and the details left to the reader, if desired.\n\nThe second example also illustrates why \"transport of structure\" is not always desirable. Namely, we can take \"M\" to be the plane, and we can take \"X\" to be an infinite one-sided cone. By \"flattening\" the cone we achieve a homeomorphism of \"X\" and \"M\", and therefore the structure of a smooth manifold on \"X\", but the cone is not \"naturally\" a smooth manifold. That is, we can consider \"X\" as a subspace of 3-space, in which context it is not smooth at the cone point. A more surprising example is that of exotic spheres, discovered by Milnor, which states that there are exactly 28 smooth manifolds which are homeomorphic (but by definition \"not\" diffeomorphic) to formula_26, the 7-dimensional sphere in 8-space. Thus, transport of structure is most productive when there exists a canonical isomorphism between the two objects.\n"}
{"id": "186023", "url": "https://en.wikipedia.org/wiki?curid=186023", "title": "Unifying theories in mathematics", "text": "Unifying theories in mathematics\n\nThere have been several attempts in history to reach a unified theory of mathematics. Some of the greatest mathematicians have expressed views that the whole subject should be fitted into one theory.\n\nThe process of unification might be seen as helping to define what constitutes mathematics as a discipline.\n\nFor example, mechanics and mathematical analysis were commonly combined into one subject during the 18th century, united by the differential equation concept; while algebra and geometry were considered largely distinct. Now we consider analysis, algebra, and geometry, but not mechanics, as parts of mathematics because they are primarily deductive formal sciences, while mechanics like physics must proceed from observation. There is no major loss of content, with analytical mechanics in the old sense now expressed in terms of symplectic topology, based on the newer theory of manifolds.\n\nThe term \"theory\" is used informally within mathematics to mean a self-consistent body of definitions, axioms, theorems, examples, and so on. (Examples include group theory, Galois theory, control theory, and K-theory.) In particular there is no connotation of \"hypothetical\". Thus the term \"unifying theory\" is more like a sociological term used to study the actions of mathematicians. It may assume nothing conjectural that would be analogous to an undiscovered scientific link. There is really no cognate within mathematics to such concepts as \"Proto-World\" in linguistics or the Gaia hypothesis.\n\nNonetheless there have been several episodes within the history of mathematics in which sets of individual theorems were found to be special cases of a single unifying result, or in which a single perspective about how to proceed when developing an area of mathematics could be applied fruitfully to multiple branches of the subject.\n\nA well-known example was the development of analytic geometry, which in the hands of mathematicians such as Descartes and Fermat showed that many theorems about curves and surfaces of special types could be stated in algebraic language (then new), each of which could then be proved using the same techniques. That is, the theorems were very similar algebraically, even if the geometrical interpretations were distinct.\n\nIn 1859 Arthur Cayley initiated a unification of metric geometries through use of the Cayley-Klein metrics. Later Felix Klein used such metrics to provide a foundation for non-Euclidean geometry.\n\nIn 1872, Felix Klein noted that the many branches of geometry which had been developed during the 19th century (affine geometry, projective geometry, hyperbolic geometry, etc.) could all be treated in a uniform way. He did this by considering the groups under which the geometric objects were invariant. This unification of geometry goes by the name of the Erlangen programme.\n\nEarly in the 20th century, many parts of mathematics began to be treated by delineating useful sets of axioms and then studying their consequences. Thus, for example, the studies of \"hypercomplex numbers\", such as considered by the Quaternion Society, were put onto an axiomatic footing as branches of ring theory (in this case, with the specific meaning of associative algebras over the field of complex numbers.) In this context, the quotient ring concept is one of the most powerful unifiers.\n\nThis was a general change of methodology, since the needs of applications had up until then meant that much of mathematics was taught by means of algorithms (or processes close to being algorithmic). Arithmetic is still taught that way. It was a parallel to the development of mathematical logic as a stand-alone branch of mathematics. By the 1930s symbolic logic itself was adequately included within mathematics.\n\nIn most cases, mathematical objects under study can be defined (albeit non-canonically) as sets or, more informally, as sets with additional structure such as an addition operation. Set theory now serves as a \"lingua franca\" for the development of mathematical themes.\n\nThe cause of axiomatic development was taken up in earnest by the Bourbaki group of mathematicians. Taken to its extreme, this attitude was thought to demand mathematics developed in its greatest generality. One started from the most general axioms, and then specialized, for example, by introducing modules over commutative rings, and limiting to vector spaces over the real numbers only when absolutely necessary. The story proceeded in this fashion, even when the specializations were the theorems of primary interest.\n\nIn particular, this perspective placed little value on fields of mathematics (such as combinatorics) whose objects of study are very often special, or found in situations which can only superficially be related to more axiomatic branches of the subject.\n\nCategory theory is a unifying theory of mathematics that was initially developed in the second half of the 20th century. In this respect it is an alternative and complement to set theory. A key theme from the \"categorical\" point of view is that mathematics requires not only certain kinds of objects (Lie groups, Banach spaces, etc.) but also mappings between them that preserve their structure.\n\nIn particular, this clarifies exactly what it means for mathematical objects to be considered to be \"the same\". (For example, are all equilateral triangles \"the same\", or does size matter?) Saunders Mac Lane proposed that any concept with enough 'ubiquity' (occurring in various branches of mathematics) deserved isolating and studying in its own right. Category theory is arguably better adapted to that end than any other current approach. The disadvantages of relying on so-called \"abstract nonsense\" are a certain blandness and abstraction in the sense of breaking away from the roots in concrete problems. Nevertheless, the methods of category theory have steadily advanced in acceptance, in numerous areas (from D-modules to categorical logic).\n\nOn a less grandiose scale, there are frequent instances in which it appears that sets of results in two different branches of mathematics are similar, and one might ask whether there is a unifying framework which clarifies the connections. We have already noted the example of analytic geometry, and more generally the field of algebraic geometry thoroughly develops the connections between geometric objects (algebraic varieties, or more generally schemes) and algebraic ones (ideals); the touchstone result here is Hilbert's Nullstellensatz which roughly speaking shows that there is a natural one-to-one correspondence between the two types of objects.\n\nOne may view other theorems in the same light. For example, the fundamental theorem of Galois theory asserts that there is a one-to-one correspondence between extensions of a field and subgroups of the field's Galois group. The Taniyama–Shimura conjecture for elliptic curves (now proven) establishes a one-to-one correspondence between curves defined as modular forms and elliptic curves defined over the rational numbers. A research area sometimes nicknamed Monstrous Moonshine developed connections between modular forms and the finite simple group known as the Monster, starting solely with the surprise observation that in each of them the rather unusual number 196884 would arise very naturally. Another field, known as the Langlands program, likewise starts with apparently haphazard similarities (in this case, between number-theoretical results and representations of certain groups) and looks for constructions from which both sets of results would be corollaries.\n\nA short list of these theories might include:\n\nA well-known example is the Taniyama–Shimura conjecture, now the modularity theorem, which proposed that each elliptic curve over the rational numbers can be translated into a modular form (in such a way as to preserve the associated L-function). There are difficulties in identifying this with an isomorphism, in any strict sense of the word. Certain curves had been known to be both elliptic curves (of genus 1) and modular curves, before the conjecture was formulated (about 1955). The surprising part of the conjecture was the extension to factors of Jacobians of modular curves of genus > 1. It had probably not seemed plausible that there would be 'enough' such rational factors, before the conjecture was enunciated; and in fact the numerical evidence was slight until around 1970, when tables began to confirm it. The case of elliptic curves with complex multiplication was proved by Shimura in 1964. This conjecture stood for decades before being proved in generality.\n\nIn fact the Langlands program (or philosophy) is much more like a web of unifying conjectures; it really does postulate that the general theory of automorphic forms is regulated by the L-groups introduced by Robert Langlands. His \"principle of functoriality\" with respect to the L-group has a very large explanatory value with respect to known types of \"lifting\" of automorphic forms (now more broadly studied as automorphic representations). While this theory is in one sense closely linked with the Taniyama–Shimura conjecture, it should be understood that the conjecture actually operates in the opposite direction. It requires the existence of an automorphic form, starting with an object that (very abstractly) lies in a category of motives.\n\nAnother significant related point is that the Langlands approach stands apart from the whole development triggered by monstrous moonshine (connections between elliptic modular functions as Fourier series, and the group representations of the Monster group and other sporadic groups). The Langlands philosophy neither foreshadowed nor was able to include this line of research.\n\nAnother case, which so far is less well-developed but covers a wide range of mathematics, is the conjectural basis of some parts of K-theory. The Baum–Connes conjecture, now a long-standing problem, has been joined by others in a group known as the isomorphism conjectures in K-theory. These include the Farrell–Jones conjecture and Bost conjecture.\n\n"}
{"id": "26639238", "url": "https://en.wikipedia.org/wiki?curid=26639238", "title": "Venvaroha", "text": "Venvaroha\n\nVeṇvāroha is a work in Sanskrit composed by Mādhava (c.1350 – c.1425) of Sangamagrāma the founder of the Kerala school of astronomy and mathematics. It is a work in 74 verses describing methods for the computation of the true positions of the Moon at intervals of about half an hour for various days in an anomalistic cycle. This work is an elaboration of an earlier and shorter work of Mādhava himself titled \"Sphutacandrāpti\". \"Veṇvāroha\" is the most popular astronomical work of Mādhava. It is dated 1403 CE. Acyuta Piṣārati (1550–1621), another prominent mathematician/astronomer of the Kerala school, has composed a Malayalam commentary on \"Veṇvāroha\". This astronomical treatise is of a type generally described as Karaṇa texts in India. Such works are characterized by the fact that they are compilations of computational methods of practical astronomy. The title \"Veṇvāroha\" literally means Bamboo Climbing and it is indicative of the computational procedure expounded in the text. The computational scheme is like climbing a bamboo tree, going up and up step by step at measured equal heights.\n\nThe novelty and ingenuity of the method attracted the attention of several of the followers of Mādhava and they composed similar texts thereby creating a genre of works in Indian mathematical tradition collectively referred to as ‘veṇvāroha texts’. These include \"Drik-veṇvārohakriya\" of unknown authorship of epoch 1695 and \"Veṇvārohastaka\" of Putuman Somāyaji.\n\nIn the technical terminology of astronomy, the ingenuity introduced by Mādhava in \"Veṇvāroha\" can be explained thus: Mādhava has endeavored to compute the true longitude of the Moon by making use of the true motions rather than the epicyclic astronomy of the Aryabhata tradition. He made use of the anomalistic revolutions for computing the true positions of the Moon using the successive true daily velocity specified in \"Candravākyas\" (Table of Moon-mnemonics) for easy memorization and use.\n\nVeṇvāroha has been studied from a modern perspective and the process is explained using the properties of periodic functions.\n\n\n"}
{"id": "6290771", "url": "https://en.wikipedia.org/wiki?curid=6290771", "title": "Whitehead's point-free geometry", "text": "Whitehead's point-free geometry\n\nIn mathematics, point-free geometry is a geometry whose primitive ontological notion is \"region\" rather than point. Two axiomatic systems are set out below, one grounded in mereology, the other in mereotopology and known as \"connection theory\". A point can mark a space or objects.\n\nPoint-free geometry was first formulated in Whitehead (1919, 1920), not as a theory of geometry or of spacetime, but of \"events\" and of an \"extension relation\" between events. Whitehead's purposes were as much philosophical as scientific and mathematical.\n\nWhitehead did not set out his theories in a manner that would satisfy present-day canons of formality. The two formal first order theories described in this entry were devised by others in order to clarify and refine Whitehead's theories. The domain for both theories consists of \"regions.\" All unquantified variables in this entry should be taken as tacitly universally quantified; hence all axioms should be taken as universal closures. No axiom requires more than three quantified variables; hence a translation of first order theories into relation algebra is possible. Each set of axioms has but four existential quantifiers.\n\nThe axioms G1-G7 are, but for numbering, those of Def. 2.1 in Gerla and Miranda (2008) (see also Gerla (1995)). The identifiers of the form WPn, included in the verbal description of each axiom, refer to the corresponding axiom in Simons (1987: 83).\n\nThe fundamental primitive binary relation is \"Inclusion\", denoted by infix \"≤\". (\"Inclusion\" corresponds to the binary \"Parthood\" relation that is a standard feature of all mereological theories.) The intuitive meaning of \"x\"≤\"y\" is \"\"x\" is part of \"y\".\" Assuming that identity, denoted by infix \"=\", is part of the background logic, the binary relation \"Proper Part\", denoted by infix \"<\", is defined as:\n\nformula_1\n\nThe axioms are:\n\n\n\n\n\nA model of G1–G7 is an \"inclusion space\".\n\nDefinition (Gerla and Miranda 2008: Def. 4.1). Given some inclusion space, an abstractive class is a class \"G\" of regions such that \"G\" is totally ordered by Inclusion. Moreover, there does not exist a region included in all of the regions included in \"G\".\n\nIntuitively, an abstractive class defines a geometrical entity whose dimensionality is less than that of the inclusion space. For example, if the inclusion space is the Euclidean plane, then the corresponding abstractive classes are points and lines.\n\nInclusion-based point-free geometry (henceforth \"point-free geometry\") is essentially an axiomatization of Simons's (1987: 83) system W. In turn, W formalizes a theory in Whitehead (1919) whose axioms are not made explicit. Point-free geometry is W with this defect repaired. Simons (1987) did not repair this defect, instead proposing in a footnote that the reader do so as an exercise. The primitive relation of W is Proper Part, a strict partial order. The theory of Whitehead (1919) has a single primitive binary relation \"K\" defined as \"xKy\" ↔ \"y\"<\"x\". Hence \"K\" is the converse of Proper Part. Simons's WP1 asserts that Proper Part is irreflexive and so corresponds to G1. G3 establishes that inclusion, unlike Proper Part, is anti-symmetric.\n\nPoint-free geometry is closely related to a dense linear order D, whose axioms are G1-3, G5, and the totality axiom formula_9 Hence inclusion-based point-free geometry would be a proper extension of D (namely D∪{G4, G6, G7}), were it not that the D relation \"≤\" is a total order.\n\nIn his 1929 \"Process and Reality\", A. N. Whitehead proposed a different approach, one inspired by De Laguna (1922). Whitehead took as primitive the topological notion of \"contact\" between two regions, resulting in a primitive \"connection relation\" between events. Connection theory C is a first order theory that distills the first 12 of the 31 assumptions in chpt. 2 of \"Process and Reality\" into 6 axioms, C1-C6. C is a proper fragment of the theories proposed in Clarke (1981), who noted their mereological character. Theories that, like C, feature both inclusion and topological primitives, are called mereotopologies.\n\nC has one primitive relation, binary \"connection,\" denoted by the prefixed predicate letter \"C\". That \"x\" is included in \"y\" can now be defined as \"x\"≤\"y\" ↔ ∀z[\"Czx\"→\"Czy\"]. Unlike the case with inclusion spaces, connection theory enables defining \"non-tangential\" inclusion, a total order that enables the construction of abstractive classes. Gerla and Miranda (2008) argue that only thus can mereotopology unambiguously define a point.\n\nThe axioms C1-C6 below are, but for numbering, those of Def. 3.1 in Gerla and Miranda (2008).\n\n\n\n\n\n\n\nA model of C is a \"connection space\".\n\nFollowing the verbal description of each axiom is the identifier of the corresponding axiom in Casati and Varzi (1999). Their system SMT (\"strong mereotopology\") consists of C1-C3, and is essentially due to Clarke (1981). Any mereotopology can be made atomless by invoking C4, without risking paradox or triviality. Hence C extends the atomless variant of SMT by means of the axioms C5 and C6, suggested by chpt. 2 of \"Process and Reality\". For an advanced and detailed discussion of systems related to C, see Roeper (1997).\n\nBiacino and Gerla (1991) showed that every model of Clarke's theory is a Boolean algebra, and models of such algebras cannot distinguish connection from overlap. It is doubtful whether either fact is faithful to Whitehead's intent.\n\n\n\n"}
{"id": "20115268", "url": "https://en.wikipedia.org/wiki?curid=20115268", "title": "Wildfire modeling", "text": "Wildfire modeling\n\nIn computational science, wildfire modeling is concerned with numerical simulation of wildland fires in order to understand and predict fire behavior. Wildfire modeling can ultimately aid wildland fire suppression, namely increase safety of firefighters and the public, reduce risk, and minimize damage. Wildfire modeling can also aid in protecting ecosystems, watersheds, and air quality.\n\nWildfire modeling attempts to reproduce fire behavior, such as how quickly the fire spreads, in which direction, how much heat it generates. A key input to behavior modeling is the Fuel Model, or type of fuel, through which the fire is burning. Behavior modeling can also include whether the fire transitions from the surface (a \"surface fire\") to the tree crowns (a \"crown fire\"), as well as extreme fire behavior including rapid rates of spread, fire whirls, and tall well-developed convection columns. Fire modeling also attempts to estimate fire effects, such as the ecological and hydrological effects of the fire, fuel consumption, tree mortality, and amount and rate of smoke produced.\n\nWildland fire behavior is affected by weather, fuel characteristics, and topography.\n\nWeather influences fire through wind and moisture. Wind increases the fire spread in the wind direction, higher temperature makes the fire burn faster, while higher relative humidity, and precipitation (rain or snow) may slow it down or extinguish it altogether. Weather involving fast wind changes can be particularly dangerous, since they can suddenly change the fire direction and behavior. Such weather includes cold fronts, foehn winds, thunderstorm downdrafts, sea and land breeze, and diurnal slope winds.\n\nWildfire fuel includes grass, wood, and anything else that can burn. Small dry twigs burn faster while large logs burn slower; dry fuel ignites more easily and burns faster than wet fuel.\n\nTopography factors that influence wildfires include the orientation toward the sun, which influences the amount of energy received from the sun, and the slope (fire spreads faster uphill). Fire can accelerate in narrow canyons and it can be slowed down or stopped by barriers such as creeks and roads.\n\nThese factors act in combination. Rain or snow increases the fuel moisture, high relative humidity slows the drying of the fuel, while winds can make fuel dry faster. Wind can change the fire-accelerating effect of slopes to effects such as downslope windstorms (called Santa Anas, foehn winds, East winds, depending on the geographic location). Fuel properties may vary with topography as plant density varies with elevation or aspect with respect to the sun.\n\nIt has long been recognized that \"fires create their own weather.\" That is, the heat and moisture created by the fire feed back into the atmosphere, creating intense winds that drive the fire behavior. The heat produced by the wildfire changes the temperature of the atmosphere and creates strong updrafts, which can change the direction of surface winds. The water vapor released by the fire changes the moisture balance of the atmosphere. The water vapor can be carried away, where the latent heat stored in the vapor is released through condensation.\n\nLike all models in computational science, fire models need to strike a balance between fidelity, availability of data, and fast execution. Wildland fire models span a vast range of complexity, from simple cause and effect principles to the most physically complex presenting a difficult supercomputing challenge that cannot hope to be solved faster than real time.\n\nForest-fire models have been developed since 1940 to the present, but a lot of chemical and thermodynamic questions related to fire behaviour are still to be resolved. Scientists and their forest fire models from 1940 till 2003 are listed in article. Models can be divided into three groups: Empirical, Semi-empirical, and Physically based.\n\nConceptual models from experience and intuition from past fires can be used to anticipate the future. Many semi-empirical fire spread equations, as in those published by the USDA Forest Service, Forestry Canada, Nobel, Bary, and Gill, and Cheney, Gould, and Catchpole for Australasian fuel complexes have been developed for quick estimation of fundamental parameters of interest such as fire spread rate, flame length, and fireline intensity of surface fires at a point for specific fuel complexes, assuming a representative point-location wind and terrain slope. Based on the work by Fons's in 1946, and Emmons in 1963, the quasi-steady equilibrium spread rate calculated for a surface fire on flat ground in no-wind conditions was calibrated using data of piles of sticks burned in a flame chamber/wind tunnel to represent other wind and slope conditions for the fuel complexes tested.\n\nTwo-dimensional fire growth models such as FARSITE and Prometheus, the Canadian wildland fire growth model designed to work in Canadian fuel complexes, have been developed that apply such semi-empirical relationships and others regarding ground-to-crown transitions to calculate fire spread and other parameters along the surface. Certain assumptions must be made in models such as FARSITE and Prometheus to shape the fire growth. For example, Prometheus and FARSITE use the Huygens principle of wave propagation. A set of equations that can be used to propagate (shape and direction) a fire front using an elliptical shape was developed by Richards in 1990. Although more sophisticated applications use a three-dimensional numerical weather prediction system to provide inputs such as wind velocity to one of the fire growth models listed above, the input was passive and the feedback of the fire upon the atmospheric wind and humidity are not accounted for.\n\nA simplified physically based two-dimensional fire spread models based upon conservation laws that use radiation as the dominant heat transfer mechanism and convection, which represents the effect of wind and slope, lead to reaction–diffusion systems of partial differential equations.\n\nMore complex physical models join computational fluid dynamics models with a wildland fire component and allow the fire to feed back upon the atmosphere. These models include NCAR's Coupled Atmosphere-Wildland Fire-Environment (CAWFE) model developed in 2005, WRF-Fire at NCAR and University of Colorado Denver which combines the Weather Research and Forecasting Model with a spread model by the level-set method, University of Utah's Coupled Atmosphere-Wildland Fire Large Eddy Simulation developed in 2009, Los Alamos National Laboratory's FIRETEC developed in, the WUI (Wildland Urban Interface) Fire Dynamics Simulator (WFDS) developed in 2007, and, to some degree, the two-dimensional model FIRESTAR. These tools have different emphases and have been applied to better understand the fundamental aspects of fire behavior, such as fuel inhomogeneities on fire behavior, feedbacks between the fire and the atmospheric environment as the basis for the universal fire shape, and are beginning to be applied to wildland urban interface house-to-house fire spread at the community-scale.\n\nThe cost of added physical complexity is a corresponding increase in computational cost, so much so that a full three-dimensional explicit treatment of combustion in wildland fuels by direct numerical simulation (DNS) at scales relevant for atmospheric modeling does not exist, is beyond current supercomputers, and does not currently make sense to do because of the limited skill of weather models at spatial resolution under 1 km. Consequently, even these more complex models parameterize the fire in some way, for example, papers by Clark use equations developed by Rothermel for the USDA forest service to calculate local fire spread rates using fire-modified local winds. And, although FIRETEC and WFDS carry prognostic conservation equations for the reacting fuel and oxygen concentrations, the computational grid cannot be fine enough to resolve the reaction rate-limiting mixing of fuel and oxygen, so approximations must be made concerning the subgrid-scale temperature distribution or the combustion reaction rates themselves. These models also are too small-scale to interact with a weather model, so the fluid motions use a computational fluid dynamics model confined in a box much smaller than the typical wildfire.\n\nAttempts to create the most complete theoretical model were made by Albini F.A. in USA and Grishin A.M. in Russia. Grishin's work is based on the fundamental laws of physics, conservation and theoretical justifications are provided. The simplified two-dimensional model of running crown forest fire was developed in Belarusian State University by Barovik D.V. and Taranchuk V.B..\n\nData assimilation periodically adjusts the model state to incorporate new data using statistical methods. Because fire is highly nonlinear and irreversible, data assimilation for fire models poses special challenges, and standard methods, such as the ensemble Kalman filter (EnKF) do not work well. Statistical variability of corrections and especially large corrections may result in nonphysical states, which tend to be preceded or accompanied by large spatial gradients. In order to ease this problem, the regularized EnKF penalizes large changes of spatial gradients in the Bayesian update in EnKF. The regularization technique has a stabilizing effect on the simulations in the ensemble but it does not improve much the ability of the EnKF to track the data: The posterior ensemble is made out of linear combinations of the prior ensemble, and if a reasonably close location and shape of the fire cannot be found between the linear combinations, the data assimilation is simply out of luck, and the ensemble cannot approach the data. From that point on, the ensemble evolves essentially without regard to the data. This is called filter divergence. So, there is clearly a need to adjust the simulation state by a position change rather than an additive correction only. The \"morphing EnKF\" combines the ideas of data assimilation with image registration and morphing to provide both additive and position correction in a natural manner, and can be used to change a model state reliably in response to data.\n\nThe limitations on fire modeling are not entirely computational. At this level, the models encounter limits in knowledge about the composition of pyrolysis products and reaction pathways, in addition to gaps in basic understanding about some aspects of fire behavior such as fire spread in live fuels and surface-to-crown fire transition.\n\nThus, while more complex models have value in studying fire behavior and testing fire spread in a range of scenarios, from the application point of view, FARSITE and Palm-based applications of BEHAVE have shown great utility as practical in-the-field tools because of their ability to provide estimates of fire behavior in real time. While the coupled fire-atmosphere models have the ability to incorporate the ability of the fire to affect its own local weather, and model many aspects of the explosive, unsteady nature of fires that cannot be incorporated in current tools, it remains a challenge to apply these more complex models in a faster-than-real-time operational environment. Also, although they have reached a certain degree of realism when simulating specific natural fires, they must yet address issues such as identifying what specific, relevant operational information they could provide beyond current tools, how the simulation time could fit the operational time frame for decisions (therefore, the simulation must run substantially faster than real time), what temporal and spatial resolution must be used by the model, and how they estimate the inherent uncertainty in numerical weather prediction in their forecast. These operational constraints must be used to steer model development.\n\n\n"}
