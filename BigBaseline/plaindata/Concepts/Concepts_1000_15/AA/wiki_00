{"id": "6395956", "url": "https://en.wikipedia.org/wiki?curid=6395956", "title": "Analytic–synthetic distinction", "text": "Analytic–synthetic distinction\n\nThe analytic–synthetic distinction (also called the analytic–synthetic dichotomy) is a semantic distinction, used primarily in philosophy to distinguish propositions (in particular, statements that are affirmative subject–predicate judgments) into two types: analytic propositions and synthetic propositions. Analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. However, philosophers have used the terms in very different ways. Furthermore, philosophers have debated whether there is a legitimate distinction.\n\nThe philosopher Immanuel Kant uses the terms \"analytic\" and \"synthetic\" to divide propositions into two types. Kant introduces the analytic–synthetic distinction in the Introduction to his \"Critique of Pure Reason\" (1781/1998, A6–7/B10–11). There, he restricts his attention to statements that are affirmative subject-predicate judgments and defines \"analytic proposition\" and \"synthetic proposition\" as follows:\n\nExamples of analytic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nEach of these statements is an affirmative subject-predicate judgment, and, in each, the predicate concept is \"contained\" within the subject concept. The concept \"bachelor\" contains the concept \"unmarried\"; the concept \"unmarried\" is part of the definition of the concept \"bachelor\". Likewise, for \"triangle\" and \"has three sides\", and so on.\n\nExamples of synthetic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nAs with the previous examples classified as analytic propositions, each of these new statements is an affirmative subject–predicate judgment. However, in none of these cases does the subject concept contain the predicate concept. The concept \"bachelor\" does not contain the concept \"alone\"; \"alone\" is not a part of the \"definition\" of \"bachelor\". The same is true for \"creatures with hearts\" and \"have kidneys\"; even if every creature with a heart also has kidneys, the concept \"creature with a heart\" does not contain the concept \"has kidneys\".\n\nIn the Introduction to the \"Critique of Pure Reason\", Kant contrasts his distinction between analytic and synthetic propositions with another distinction, the distinction between \"a priori\" and \"a posteriori\" propositions. He defines these terms as follows:\n\nExamples of \"a priori\" propositions include:\n\nThe justification of these propositions does not depend upon experience: one need not consult experience to determine whether all bachelors are unmarried, nor whether . (Of course, as Kant would grant, experience is required to understand the concepts \"bachelor\", \"unmarried\", \"7\", \"+\" and so forth. However, the \"a priori\" / \"a posteriori\" distinction as employed here by Kant refers not to the \"origins\" of the concepts but to the \"justification\" of the propositions. Once we have the concepts, experience is no longer necessary.)\n\nExamples of \"a posteriori\" propositions include:\n\nBoth of these propositions are \"a posteriori\": any justification of them would require one's experience.\n\nThe analytic/synthetic distinction and the \"a priori\" / \"a posteriori\" distinction together yield four types of propositions:\n\nKant posits the third type as obviously self-contradictory. Ruling it out, he discusses only the remaining three types as components of his epistemological frameworkeach, for brevity's sake, becoming, respectively, \"analytic\", \"synthetic a priori\", and \"empirical\" or \"a posteriori\" propositions. This triad will account for all propositions possible.\n\nPart of Kant's argument in the Introduction to the \"Critique of Pure Reason\" involves arguing that there is no problem figuring out how knowledge of analytic propositions is possible. To know an analytic proposition, Kant argued, one need not consult experience. Instead, one needs merely to take the subject and \"extract from it, in accordance with the principle of contradiction, the required predicate\" (A7/B12). In analytic propositions, the predicate concept is contained in the subject concept. Thus, to know an analytic proposition is true, one need merely examine the concept of the subject. If one finds the predicate contained in the subject, the judgment is true.\n\nThus, for example, one need not consult experience to determine whether \"All bachelors are unmarried\" is true. One need merely examine the subject concept (\"bachelors\") and see if the predicate concept \"unmarried\" is contained in it. And in fact, it is: \"unmarried\" is part of the definition of \"bachelor\" and so is contained within it. Thus the proposition \"All bachelors are unmarried\" can be known to be true without consulting experience.\n\nIt follows from this, Kant argued, first: All analytic propositions are \"a priori\"; there are no \"a posteriori\" analytic propositions. It follows, second: There is no problem understanding how we can know analytic propositions; we can know them because we only need to consult our concepts in order to determine that they are true.\n\nAfter ruling out the possibility of analytic \"a posteriori\" propositions, and explaining how we can obtain knowledge of analytic \"a priori\" propositions, Kant also explains how we can obtain knowledge of synthetic \"a posteriori\" propositions. That leaves only the question of how knowledge of synthetic \"a priori\" propositions is possible. This question is exceedingly important, Kant maintains, because all important metaphysical knowledge is of synthetic \"a priori\" propositions. If it is impossible to determine which synthetic \"a priori\" propositions are true, he argues, then metaphysics as a discipline is impossible. The remainder of the \"Critique of Pure Reason\" is devoted to examining whether and how knowledge of synthetic \"a priori\" propositions is possible.\n\nOver a hundred years later, a group of philosophers took interest in Kant and his distinction between analytic and synthetic propositions: the logical positivists.\n\nPart of Kant's examination of the possibility of synthetic \"a priori\" knowledge involved the examination of mathematical propositions, such as\n\nKant maintained that mathematical propositions such as these are synthetic \"a priori\" propositions, and that we know them. That they are synthetic, he thought, is obvious: the concept \"equal to 12\" is not contained within the concept \"7 + 5\"; and the concept \"straight line\" is not contained within the concept \"the shortest distance between two points\". From this, Kant concluded that we have knowledge of synthetic \"a priori\" propositions.\n\nGottlob Frege's notion of analyticity included a number of logical properties and relations beyond containment: symmetry, transitivity, antonymy, or negation and so on. He had a strong emphasis on formality, in particular formal definition, and also emphasized the idea of substitution of synonymous terms. \"All bachelors are unmarried\" can be expanded out with the formal definition of bachelor as \"unmarried man\" to form \"All unmarried men are unmarried\", which is recognizable as tautologous and therefore analytic from its logical form: any statement of the form \"All \"X\" that are (\"F\" and \"G\") are \"F\"\". Using this particular expanded idea of analyticity, Frege concluded that Kant's examples of arithmetical truths are analytical \"a priori\" truths and \"not\" synthetic \"a priori\" truths.\n\nThe logical positivists agreed with Kant that we have knowledge of mathematical truths, and further that mathematical propositions are \"a priori\". However, they did not believe that any complex metaphysics, such as the type Kant supplied, are necessary to explain our knowledge of mathematical truths. Instead, the logical positivists maintained that our knowledge of judgments like \"all bachelors are unmarried\" and our knowledge of mathematics (and logic) are in the basic sense the same: all proceeded from our knowledge of the meanings of terms or the conventions of language.\n\nThus the logical positivists drew a new distinction, and, inheriting the terms from Kant, named it the \"analytic/synthetic distinction\". They provided many different definitions, such as the following:\n\nSynthetic propositions were then defined as:\n\nThese definitions applied to all propositions, regardless of whether they were of subject–predicate form. Thus, under these definitions, the proposition \"It is raining or it is not raining\" was classified as analytic, while for Kant it was analytic by virtue of its logical form. And the proposition \"\" was classified as analytic, while under Kant's definitions it was synthetic.\n\nTwo-dimensionalism is an approach to semantics in analytic philosophy. It is a theory of how to determine the sense and reference of a word and the truth-value of a sentence. It is intended to resolve a puzzle that has plagued philosophy for some time, namely: How is it possible to discover empirically that a necessary truth is true? Two-dimensionalism provides an analysis of the semantics of words and sentences that makes sense of this possibility. The theory was first developed by Robert Stalnaker, but it has been advocated by numerous philosophers since, including David Chalmers and Berit Brogaard.\n\nAny given sentence, for example, the words,\n\nis taken to express two distinct propositions, often referred to as a \"primary intension\" and a \"secondary intension\", which together compose its meaning.\n\nThe primary intension of a word or sentence is its sense, i.e., is the idea or method by which we find its referent. The primary intension of \"water\" might be a description, such as \"watery stuff\". The thing picked out by the primary intension of \"water\" could have been otherwise. For example, on some other world where the inhabitants take \"water\" to mean \"watery stuff\", but, where the chemical make-up of watery stuff is not HO, it is not the case that water is HO for that world.\n\nThe \"secondary intension\" of \"water\" is whatever thing \"water\" happens to pick out in \"this\" world, whatever that world happens to be. So if we assign \"water\" the primary intension \"watery stuff\" then the secondary intension of \"water\" is HO, since HO is \"watery stuff\" in this world. The secondary intension of \"water\" in our world is HO, which is HO in every world because unlike \"watery stuff\" it is impossible for HO to be other than HO. When considered according to its secondary intension, \"Water is HO\" is true in every world.\n\nIf two-dimensionalism is workable it solves some very important problems in the philosophy of language. Saul Kripke has argued that \"Water is HO\" is an example of the \"necessary a posteriori\", since we had to discover that water was HO, but given that it is true, it cannot be false. It would be absurd to claim that something that is water is not HO, for these are known to be \"identical\".\n\nRudolf Carnap was a strong proponent of the distinction between what he called \"internal questions\", questions entertained within a \"framework\" (like a mathematical theory), and \"external questions\", questions posed outside any framework – posed before the adoption of any framework. The \"internal\" questions could be of two types: \"logical\" (or analytic, or logically true) and \"factual\" (empirical, that is, matters of observation interpreted using terms from a framework). The \"external\" questions were also of two types: those that were confused pseudo-questions (\"one disguised in the form of a theoretical question\") and those that could be re-interpreted as practical, pragmatic questions about whether a framework under consideration was \"more or less expedient, fruitful, conducive to the aim for which the language is intended\". The adjective \"synthetic\" was not used by Carnap in his 1950 work \"Empiricism, Semantics, and Ontology\". Carnap did define a \"synthetic truth\" in his work \"Meaning and Necessity\": a sentence that is true, but not simply because \"the semantical rules of the system suffice for establishing its truth\".\n\nThe notion of a synthetic truth is of something that is true both because of what it means and because of the way the world is, whereas analytic truths are true in virtue of meaning alone. Thus, what Carnap calls internal \"factual\" statements (as opposed to internal \"logical\" statements) could be taken as being also synthetic truths because they require \"observations\", but some external statements also could be \"synthetic\" statements and Carnap would be doubtful about their status. The analytic–synthetic argument therefore is not identical with the internal–external distinction.\n\nIn 1951, Willard Van Orman Quine published the essay \"Two Dogmas of Empiricism\" in which he argued that the analytic–synthetic distinction is untenable. The argument at bottom is that there are no \"analytic\" truths, but all truths involve an empirical aspect. In the first paragraph, Quine takes the distinction to be the following:\n\nQuine's position denying the analytic-synthetic distinction is summarized as follows:\nTo summarize Quine's argument, the notion of an analytic proposition requires a notion of synonymy, but establishing synonymy inevitably leads to matters of fact – synthetic propositions. Thus, there is no non-circular (and so no tenable) way to ground the notion of analytic propositions.\n\nWhile Quine's rejection of the analytic–synthetic distinction is widely known, the precise argument for the rejection and its status is highly debated in contemporary philosophy. However, some (for example, Boghossian) argue that Quine's rejection of the distinction is still widely accepted among philosophers, even if for poor reasons.\n\nPaul Grice and P. F. Strawson criticized \"Two Dogmas\" in their 1956 article \"In Defense of a Dogma\". Among other things, they argue that Quine's skepticism about synonyms leads to a skepticism about meaning. If statements can have meanings, then it would make sense to ask \"What does it mean?\". If it makes sense to ask \"What does it mean?\", then synonymy can be defined as follows: Two sentences are synonymous if and only if the true answer of the question \"What does it mean?\" asked of one of them is the true answer to the same question asked of the other. They also draw the conclusion that discussion about correct or incorrect translations would be impossible given Quine's argument. Four years after Grice and Strawson published their paper, Quine's book \"Word and Object\" was released. In the book Quine presented his theory of indeterminacy of translation.\n\nIn \"Speech Acts\", John Searle argues that from the difficulties encountered in trying to explicate analyticity by appeal to specific criteria, it does not follow that the notion itself is void. Considering the way which we would test any proposed list of criteria, which is by comparing their extension to the set of analytic statements, it would follow that any explication of what analyticity means presupposes that we already have at our disposal a working notion of analyticity.\n\nIn \"'Two Dogmas' Revisited\", Hilary Putnam argues that Quine is attacking two different notions:\nAnalytic truth defined as a true statement derivable from a tautology by putting synonyms for synonyms is near Kant's account of analytic truth as a truth whose negation is a contradiction. Analytic truth defined as a truth confirmed no matter what, however, is closer to one of the traditional accounts of \"a priori\". While the first four sections of Quine's paper concern analyticity, the last two concern a priority. Putnam considers the argument in the two last sections as independent of the first four, and at the same time as Putnam criticizes Quine, he also emphasizes his historical importance as the first top rank philosopher to both reject the notion of a priority and sketch a methodology without it.\n\nJerrold Katz, a one-time associate of Noam Chomsky, countered the arguments of \"Two Dogmas\" directly by trying to define analyticity non-circularly on the syntactical features of sentences.\n\nIn \"Philosophical Analysis in the Twentieth Century, Volume 1 : The Dawn of Analysis\", Scott Soames has pointed out that Quine's circularity argument needs two of the logical positivists' central theses to be effective:\n\nIt is only when these two theses are accepted that Quine's argument holds. It is not a problem that the notion of necessity is presupposed by the notion of analyticity if necessity can be explained without analyticity. According to Soames, both theses were accepted by most philosophers when Quine published \"Two Dogmas\". Today, however, Soames holds both statements to be antiquated. He says: \"Very few philosophers today would accept either [of these assertions], both of which now seem decidedly antique.\"\n\nPhilosopher Leonard Peikoff, in his essay \"The Analytic-Synthetic Dichotomy\", expands upon Ayn Rand's analysis. He posits that:\n\nThe theory of the analytic-synthetic dichotomy presents men with the following choice: If your statement is proved, it says nothing about that which exists; if it is about existents, it cannot be proved. If it is demonstrated by logical argument, it represents a subjective convention; if it asserts a fact, logic cannot establish it. If you validate it by an appeal to the meanings of your \"concepts\", then it is cut off from reality; if you validate it by an appeal to your \"percepts\", then you cannot be certain of it.\n\nTo Peikoff, the critical question is: What is included in the meaning of a concept? He rejects the idea that some of the characteristics of a concept's referents are excluded from the concept. Applying Rand's theory that a concept is a \"mental integration\" of similar existents, treated as \"units\", he argues that concepts stand for and mean the actual existents, including all their characteristics, not just those used to pick out the referents or define the concept. He states,\n\nSince a concept is an integration of units, it has no content or meaning apart from its units. The meaning of a concept consists of the units — the existents — which it integrates, including all the characteristics of these units... The fact that certain characteristics are, at a given time, unknown to man, does not indicate that these characteristics are excluded from the entity — or from the concept.\n\nFurthermore, he argues that there is no valid distinction between \"necessary\" and \"contingent\" facts, and that all truths are learned and validated by the same process: the application of logic to perceptual data. Associated with the analytic-synthetic dichotomy are a cluster of other divisions that Objectivism also regards as false and artificial, such as logical truth vs. factual truth, logically possible vs. empirically possible, and a priori vs. the a posteriori.\n\n\n\n"}
{"id": "66975", "url": "https://en.wikipedia.org/wiki?curid=66975", "title": "Apophatic theology", "text": "Apophatic theology\n\nApophatic theology, also known as negative theology, is a form of theological thinking and religious practice which attempts to approach God, the Divine, by negation, to speak only in terms of what may not be said about the perfect goodness that is God. It forms a pair together with cataphatic theology, which approaches God or the Divine by affirmations or positive statements about what God \"is\".\n\nThe apophatic tradition is often, though not always, allied with the approach of mysticism, which aims at the vision of God, the perception of the divine reality beyond the realm of ordinary perception.\n\n\"Apophatic\", (adjective); from ἀπόφημι \"apophēmi\", meaning \"to deny\". From \"Online Etymology Dictionary\": \n\"Via negativa\" or \"via negationis\" (Latin), \"negative way\" or \"by way of denial\". The negative way forms a pair together with the \"kataphatic\" or positive way. According to Deirdre Carabine,\nAccording to Fagenblat, \"negative theology is as old as philosophy itself;\" elements of it can be found in Plato's \"unwritten doctrines,\" while it is also present in Neo-Platonic, Gnostic and early Christian writers. A tendency to apophatic thought can also be found in Philo of Alexandria.\n\nAccording to Carabine, \"apophasis proper\" in Greek thought starts with Neo-Platonism, with its speculations about the nature of the One, culminating in the works of Proclus. According to Carabine, there are two major points in the development of apophatic theology, namely the fusion of the Jewish tradition with Platonic philosophy in the writings of Philo, and the works of Dionysius the Pseudo-Areopagite, who infused Christian thought with Neo-Platonic ideas.\n\nThe Early Church Fathers were influenced by Philo, and Meredith even states that Philo \"is the real founder of the apophatic tradition.\" Yet, it was with Pseudo-Dionysius the Areopagite and Maximus the Confessor, whose writings shaped both Hesychasm, the contemplative tradition of the Eastern Orthodox Churches, and the mystical traditions of western Europe, that apophatic theology became a central element of Christian theology and contemplative practice.\n\nFor the ancient Greeks, knowledge of the gods was essential for proper worship. Poets had an important responsibility in this regard, and a central question was how knowledge of the Divine forms can be attained. Epiphany played an essential role in attaining this knowledge. Xenophanes (c. 570 – c. 475 BC) noted that the knowledge of the Divine forms is restrained by the human imagination, and Greek philosophers realized that this knowledge can only be mediated through myth and visual representations, which are culture-dependent.\n\nAccording to Herodotus (484–425 BCE), Homer and Hesiod (between 750 and 650 BC) taught the Greek the knowledge of the Divine bodies of the Gods. The ancient Greek poet Hesiod (between 750 and 650 BC) describes in his \"Theogony\" the birth of the gods and creation of the world, which became an \"ur-text for programmatic, first-person epiphanic narratives in Greek literature,\" but also \"explores the necessary limitations placed on human access to the divine.\" According to Platt, the statement of the Muses who grant Hesiod knowledge of the Gods \"actually accords better with the logic of apophatic religious thought.\"\n\nParmenides (fl. late sixth or early fifth century BC), in his poem \"On Nature\", gives an account of a revelation on two ways of inquiry. \"The way of conviction\" explores Being, true reality (\"what-is\"), which is \"What is ungenerated and deathless,/whole and uniform, and still and perfect.\" \"The way of opinion\" is the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. His distinction between unchanging Truth and shifting opinion is reflected in Plato's allegory of the Cave. Together with the Biblical story of Moses's ascent of Mount Sinai, it is used by Gregory of Nyssa and Pseudo-Dionysius the Areopagite to give a Christian account of the ascent of the soul toward God. Cook notes that Parmenides poem is a religious account of a mystical journey, akin to the mystery cults, giving a philosophical form to a religious outlook. Cook further notes that the philosopher's task is to \"attempt through 'negative' thinking to tear themselves loose from all that frustrates their pursuit of wisdom.\"\n\nPlato (428/427 or 424/423 – 348/347 BCE), \"deciding for Parmenides against Heraclitus\" and his theory of eternal change, had a strong influence on the development of apophatic thought.\n\nPlato further explored Parmenides's idea of timeless truth in his dialogue \"Parmenides\", which is a treatment of the eternal forms, \"Truth, Beauty and Goodness\", which are the real aims for knowledge. The Theory of Forms is Plato's answer to the problem \"how one unchanging reality or essential being can admit of many changing phenomena (and not just by dismissing them as being mere illusion).\"\n\nIn \"The Republic\", Plato argues that the \"real objects of knowledge are not the changing objects of the senses, but the immutable Forms,\" stating that the \"Form of the Good\" is the highest object of knowledge. His argument culminates in the Allegory of the Cave, in which he argues that humans are like prisoners in a cave, who can only see shadows of the Real, the \"Form of the Good\". Humans are to be educated to search for knowledge, by turning away from their bodily desires toward higher contemplation, culminating in an intellectual understanding or apprehension of the Forms, c.q. the \"first principles of all knowledge.\"\n\nAccording to Cook, the \"Theory of Forms\" has a theological flavour, and had a strong influence on the ideas of his Neo-Platonist interpreters Proclus and Plotinus. The pursuit of \"Truth, Beauty and Goodness\" became a central element in the apophatic tradition, but nevertheless, according to Carabine \"Plato himself cannot be regarded as the founder of the negative way.\" Carabine warns not to read later Neo-Platonic and Christian understandings into Plato, and notes that Plato did not identify his Forms with \"one transcendent source,\" an identification which his later interpreters made.\n\nMiddle Platonism (1st century BCE - 3rd century CE) further investigated Plato's \"Unwritten Doctrines,\" which drew on Pythagoras' first principles of the Monad and the Dyad (matter). Middle Platonism proposed a hierarchy of being, with God as its first principle at its top, identifying it with Plato's \"Form of the Good\". An influential proponent of Middle Platonism was Philo (c.25 BCE–c. 50 CE), who employed Middle Platonic philosophy in his interpretation of the Hebrew scriptures, and asserted a strong influence on early Christianity. According to Craig D. Allert, \"Philo made a monumental contribution to the creation of a vocabulary for use in negative statements about God.\" For Philo, God is undescribable, and he uses terms which emphasize God's transcendence.\n\nNeo-Platonism was a mystical or contemplative form of Platonism, which \"developed outside the mainstream of Academic Platonism.\" It started with the writings of Plotinus (204/5–270), and ended with the closing of the Platonic Academy by Emperor Justinian in 529 CE, when the pagan traditions were ousted. It is a product of Hellenistic syncretism, which developed due to the crossover between Greek thought and the Jewish scriptures, and also gave birth to Gnosticism. Proclus was the last head of the Platonic Academy; his student Pseudo-Dinosysius had a far-stretching Neo-Platonic influence on Christianity and Christian mysticism.\n\nPlotinus (204/5–270) was the founder of Neo-Platonism. In the Neo-Platonic philosophy of Plotinus and Proclus, the first principle became even more elevated as a radical unity, which was presented as an unknowable Absolute. For Plotinus, \"the One\" is the first principle, from which everything else emanates. He took it from Plato's writings, identifying the Good of the \"Republic\", as the cause of the other Forms, with \"the One\" of the first hypothesis of the second part of the \"Parmenides\". For Plotinus, \"the One\" precedes the Forms, and \"is beyond Mind and indeed beyond Being.\" From \"the One\" comes the Intellect, which contains all the Forms. \"The One\" is the principle of Being, while the Forms are the principle of the essence of beings, and the intelligibility which can recognize them as such. Plotinus's third principle is Soul, the desire for objects external to the person. The highest satisfaction of desire is the contemplation of \"the One\", which unites all existents \"as a single, all-pervasive reality.\"\n\n\"The One\" is radically simple, and does not even have self-knowledge, since self-knowledge would imply multiplicity. Nevertheless, Plotinus does urge for a search for the Absolute, turning inward and becoming aware of the \"presence of the intellect in the human soul,\" initiating an ascent of the soul by abstraction or \"taking away,\" culminating in a sudden appearance of \"the One\". In the \"Enneads\" Plotinus writes: \nCarabine notes that Plotinus' apophasis is not just a mental exercise, an acknowledgement of the unknowability of \"the One\", but a means to \"extasis\" and an ascent to \"the unapproachable light that is God.\" Pao-Shen Ho, investigating what are Plotinus' methods for reaching \"henosis\", concludes that \"Plotinus' mystical teaching is made up of two practices only, namely philosophy and negative theology.\" According to Moore, Plotinus appeals to the \"non-discursive, intuitive faculty of the soul,\" by \"calling for a sort of prayer, an invocation of the deity, that will permit the soul to lift itself up to the unmediated, direct, and intimate contemplation of that which exceeds it (V.1.6).\" Pao-Shen Ho further notes that \"for Plotinus, mystical experience is irreducible to philosophical arguments.\" The argumentation about \"henosis\" is preceded by the actual experience of it, and can only be understood when \"henosis\" has been attained. Ho further notes that Plotinus's writings have a didactic flavour, aiming to \"bring his own soul and \"the souls of others\" by way of Intellect to union with the One.\" As such, the \"Enneads\" as a spiritual or ascetic teaching device, akin to \"The Cloud of Unknowing\", demonstrating the methods of philosophical and apophatic inquiry. Ultimately, this leads to silence and the abandonment of all intellectual inquiry, leaving contemplation and unity.\n\nProclus (412-485) introduced the terminology which is being used in apophatic and cataphatic theology. He did this in the second book of his \"Platonic Theology\", arguing that Plato states that \"the One\" can be revealed \"through analogy,\" and that \"through negations [\"dia ton apophaseon\"] its transcendence over everything can be shown.\" For Proclus, apophatic and cataphonic theology form a contemplatory pair, with the apophatic approach corresponding to the manifestation of the world from \"the One\", and cataphonic theology corresponding to the return to \"the One\". The analogies are affirmations which direct us toward \"the One\", while the negations underlie the confirmations, being closer to \"the One\". According to Luz, Proclus also attracted students from other faiths, including the Samaritan Marinus. Luz notes that \"Marinus' Samaritan origins with its Abrahamic notion of a single ineffable Name of God () should also have been in many ways compatible with the school's ineffable and apophatic divine principle.\"\nThe Book of Revelation 8:1 mentions \"the silence of the perpetual choir in heaven.\" According to Dan Merkur,\nThe Early Church Fathers were influenced by Philo (c. 25 BCE – c. 50 CE), who saw Moses as \"the model of human virtue and Sinai as the archetype of man's ascent into the \"luminous darkness\" of God.\" His interpretation of Moses was followed by Clement of Alexandria, Origen, the Cappadocian Fathers, Pseudo-Dionysius, and Maximus the Confessor.\n\nGod's appearance to Moses in the burning bush was often elaborated on by the Early Church Fathers, especially Gregory of Nyssa (c. 335 – c. 395), realizing the fundamental unknowability of God; an exegesis which continued in the medieval mystical tradition. Their response is that, although God is unknowable, Jesus as person can be followed, since \"following Christ is the human way of seeing God.\"\n\nClement of Alexandria (c. 150 – c. 215) was an early proponent of apophatic theology. According to R.A. Baker, in Clement's writings the term \"theoria\" develops further from a mere intellectual \"seeing\" toward a spirutal form of contemplation. Clement's apophatic theology or philosophy is closely related to this kind of \"theoria\" and the \"mystic vision of the soul.\" For Clement, God is transcendent and immanent. According to Baker, Clement's apophaticism is mainly driven by Biblical texts, but by the Platonic tradition. His conception of an ineffable God is a synthesis of Plato and Philo, as seen from a Biblical perspective. According to Osborne, it is a synthesis in a Biblical framework; according to Baker, while the Platonic tradition accounts for the negative approach, the Biblical tradition accounts for the positive approach. \"Theoria\" and abstraction is the means to conceive of this ineffable God; it is preceded by dispassion.\n\nAccording to Tertullian (c. 155 – c. 240),\nSaint Cyril of Jerusalem (313-386), in his \"Catechetical Homilies\", states: \nAugustine of Hippo (354-430) defined God \"aliud, aliud valde\", meaning \"other, completely other\", in \"Confessions\" 7.10.16.\n\nApophatic theology found its most influential expression in the works of Pseudo-Dionysius the Areopagite (late 5th to early 6th century), a student of Proclus (412-485), combining a Christian worldview with Neo-Platonic ideas. He is a constant factor in the contemplative tradition of the eastern Orthodox Churches, and from the 9th century onwards his writings also had a strong impact on western mysticism.\n\nDionysius the Areopagite was a pseudonym, taken from Acts of the Apostles chapter 17, in which Paul gives a missionary speech to the court of the Areopagus in Athens. In Paul makes a reference to an altar-inscription, dedicated to the Unknown God, \"a safety measure honoring foreign gods still unknown to the Hellenistic world.\" For Paul, Jesus Christ is this unknown God, and as a result of Paul's speech Dionysius the Areopagite converts to Christianity. Yet, according to Stang, for Pseudo-Dionysius the Areopagite Athens is also the place of Neo-Platonic wisdom, and the term \"unknown God\" is a reversal of Paul's preaching toward an integration of Christianity with Neo-Platonism, and the union with the \"unknown God.\"\n\nAccording to Corrigan and Harrington, \"Dionysius' central concern is how a triune God, ... who is utterly unknowable, unrestricted being, beyond individual substances, beyond even goodness, can become manifest to, in, and through the whole of creation in order to bring back all things to the hidden darkness of their source.\" Drawing on Neo-Platonism, Pseudo-Dionysius described humans ascend to divinity as a process of purgation, illumination and union. Another Neo-Platonic influence was his description of the cosmos as a series of hierarchies, which overcome the distance between God and humans.\n\nIn Orthodox Christianity apophatic theology is taught as superior to cataphatic theology. The fourth-century Cappadocian Fathers stated a belief in the existence of God, but an existence unlike that of everything else: everything else that exists was created, but the Creator transcends this existence, is uncreated. The essence of God is completely unknowable; mankind can know God only through His energies. Gregory of Nyssa (c.335-c.395), John Chrysostom (c. 349 – 407), and Basil the Great (329-379) emphasized the importance of negative theology to an orthodox understanding of God. John of Damascus (c.675/676–749) employed negative theology when he wrote that positive statements about God reveal \"not the nature, but the things around the nature.\"\n\nMaximus the Confessor (580-622) took over Pseudo-Dionysius' ideas, and had a strong influence on the theology and contemplative practices of the Eastern Orthodox Churches. Gregory Palamas (1296–1359) formulated the definite theology of Hesychasm, the Orthodox practices of contemplative prayer and theosis, \"deification.\"\n\nInfluential modern Eastern Orthodox theologians are Vladimir Lossky, John Meyendorff, John S. Romanides and Georges Florovsky. Lossky argues, based on his reading of Dionysius and Maximus Confessor, that positive theology is always inferior to negative theology which is a step along the way to the superior knowledge attained by negation. This is expressed in the idea that mysticism is the expression of dogmatic theology \"par excellence\".\n\nAccording to Lossky, outside of directly revealed knowledge through Scripture and Sacred Tradition, such as the Trinitarian nature of God, God in His essence is beyond the limits of what human beings (or even angels) can understand. He is transcendent in essence (\"ousia\"). Further knowledge must be sought in a direct experience of God or His indestructible energies through \"theoria\" (vision of God). According to Aristotle Papanikolaou, in Eastern Christianity, God is immanent in his hypostasis or existences.\n\nNegative theology has a place in the Western Christian tradition as well. The 9th-century theologian John Scotus Erigena wrote: \n\nWhen he says \"\"He is not anything\" and \"God is not\"\", Scotus does not mean that there is no God, but that God cannot be said to exist in the way that creation exists, i.e. that God is uncreated. He is using apophatic language to emphasise that God is \"other\".\n\nTheologians like Meister Eckhart and Saint John of the Cross (San Juan de la Cruz) exemplify some aspects of or tendencies towards the apophatic tradition in the West. The medieval work, \"The Cloud of Unknowing\" and Saint John's \"Dark Night of the Soul\" are particularly well known. In 1215 apophatism became the official position of the Catholic Church, which, on the basis of Scripture and church tradition, during the Fourth Lateran Council formulated the following dogma:\nThomas Aquinas was born ten years later (1225-1274) and, although in his \"Summa Theologica\" he quotes Pseudo-Dionysius 1,760 times, his reading in a neo-Aristotelian key of the conciliar declaration overthrew its meaning inaugurating the \"analogical way\" as \"tertium\" between \"via negativa\" and \"via positiva\": the \"via eminentiae\" (see also \"analogia entis\"). According to Adrian Langdon,\nAccording to \"Catholic Encyclopedia\", the \"Doctor Angelicus\" and the scholastici declare [that] \nSince then Thomism has played a decisive role in resizing the negative or apophatic tradition of the magisterium.\n\nApophatic statements are still crucial to many modern theologians, restarting in 1800s by Søren Kierkegaard (see his concept of the infinite qualitative distinction) up to Rudolf Otto and Karl Barth (see their idea of \"Wholly Other\", i.e. \"ganz Andere\" or \"totaliter aliter\").\n\nC. S. Lewis, in his book \"Miracles\" (1947), advocates the use of negative theology when first thinking about God, in order to cleanse our minds of misconceptions. He goes on to say we must then refill our minds with the truth about God, untainted by mythology, bad analogies or false mind-pictures.\n\nThe mid-20th century Dutch philosopher Herman Dooyeweerd, who is often associated with a neo-Calvinistic tradition, provides a philosophical foundation for understanding why we can never absolutely know God, and yet, paradoxically, truly know something of God. Dooyeweerd made a sharp distinction between theoretical and pre-theoretical attitudes of thought. Most of the discussion of knowledge of God presupposes theoretical knowledge, in which we reflect and try to define and discuss. Theoretical knowing, by its very nature, is never absolute, always depends on religious presuppositions, and cannot grasp either God or the law side. Pre-theoretical knowing, on the other hand, is intimate engagement, and exhibits a diverse range of aspects. Pre-theoretical intuition, on the other hand, can grasp at least the law side. Knowledge of God, as God wishes to reveal it, is pre-theoretical, immediate and intuitive, never theoretical in nature. The philosopher Leo Strauss considered that the Bible, for example, should be treated as pre-theoretical (everyday) rather than theoretical in what it contains.\n\nIvan Illich (1926-2002), the historian and social critic, can be read as an apophatic theologian, according to a longtime collaborator, Lee Hoinacki, in a paper presented in memory of Illich, called \"Why Philia?\"\n\nAccording to Deirdre Carabine, negative theology has become a hot topic since the 1990s, resulting from a broad effort in the 19 and 20th century to portray Plato as a mysticist, which revived the interest in Neoplatonism and negative theology.\n\nKaren Armstrong, in her book \"The Case for God\" (2009), notices a recovery of apophatic theology in postmodern theology.\n\nThe Arabic term for \"negative theology\" is \"lahoot salbi\", which is a \"system of theology\" or \"nizaam al lahoot\" in Arabic. Different traditions/doctrine schools in Islam called Kalam schools (see Islamic schools and branches) use different theological approaches or \"nizaam al lahoot\" in approaching God in Islam (\"Allah\", Arabic الله) or the ultimate reality. The \"lahoot salbi\" or \"negative theology\" involves the use of \"ta'til\", which means \"negation,\" and the followers of the Mu'tazili school of Kalam, founded by Imam Wasil ibn Ata, are often called the \"Mu'attili\", because they are frequent users of the \"ta'tili\" methodology.\n\nRajab ʿAlī Tabrīzī, an Iranian and Shiat philosopher and mystic of the 17th century. instilled a radical apophatic theology in a generation of philosophers and theologians whose influence extended into the Qajar period. Mulla Rajab affirmed the completely unknowable,\nunqualifiable, and attributeless nature of God and upheld a general view concerning God’s attributes which can only be negatively ‘affirmed’, by means of the via\nnegativa.\n\nShia Islam adopted \"negative theology\". In the words of the Persian Ismaili missionary, Abu Yaqub al-Sijistani: \"There does not exist a tanzíh [\"transcendence\"] more brilliant and more splendid than that by which we establish the absolute transcendence of our Originator through the use of these phrases in which a negative and a negative of a negative apply to the thing denied.\" Early Sunni scholars who held to a literal reading of the Quran and hadith rejected this view, adhering to its opposite, believing that the Attributes of God such as \"Hand\", \"Foot\" etc... should be taken literally and that, therefore, God is like a human being. Today, most Sunnis, like the Ash'ari and Maturidi, adhere to a middle path between negation and anthropomorphism.\n\nMaimonides (1135/1138-1204) was \"the most influential medieval Jewish exponent of the \"via negativa\".\" Maimonides, but also Samuel ibn Tibbon, draw on Bahya ibn Paquda, who shows that our inability to describe God is related to the fact of His absolute unity. God, as the entity which is \"truly One\" (האחד האמת), must be free of properties and is thus unlike anything else and indescribable. According to Rabbi Yosef Wineberg, Maimonides stated that \"[God] is knowledge,\" and saw His Essence, Being and knowledge as completely one, \"a perfect unity and not a composite at all.\" Wineberg quotes Maimonides as stating\nIn \"The Guide for the Perplexed\" Maimonides stated:\nAccording to Fagenblat, it is only in the modern period that negative theology really gains importance in Jewish thought. Yeshayahu Leibowitz (1903-1994) was a prominent modern exponent of Jewish negative theology. According to Leibowitz, a person's faith is his commitment to obey God, meaning God's commandments, and this has nothing to do with a person’s image of God. This must be so because Leibowitz thought that God cannot be described, that God's understanding is not man's understanding, and thus all the questions asked of God are out of place.\n\nThere are interesting parallels in Indian thought, which developed largely separate from Western thought. Early Indian philosophical works which have apophatic themes include the Principal Upanishads (800 BCE to the start of common era) and the Brahma Sutras (from 450 BCE and 200 CE). An expression of negative theology is found in the Brihadaranyaka Upanishad, where Brahman is described as \"neti neti\" or \"neither this, nor that\". Further use of apophatic theology is found in the Brahma Sutras, which state:\n\nBuddhist philosophy has also strongly advocated the way of negation, beginning with the Buddha's own theory of anatta (not-atman, not-self) which denies any truly existent and unchanging essence of a person. Madhyamaka is a Buddhist philosophical school founded by Nagarjuna (2nd-3rd century CE), which is based on a fourfold negation of all assertions and concepts and promotes the theory of emptiness (shunyata). Apophatic assertions are also an important feature of Mahayana sutras, especially the prajñaparamita genre. These currents of negative theology are visible in all forms of Buddhism.\n\nApophatic movements in medieval Hindu philosophy are visible in the works of Shankara (8th century), a philosopher of Advaita Vedanta (non-dualism), and Bhartṛhari (5th century), a grammarian. While Shankara holds that the transcendent noumenon, Brahman, is realized by the means of negation of every phenomenon including language, Bhartṛhari theorizes that language has both phenomenal and noumenal dimensions, the latter of which manifests Brahman.\n\nIn Advaita, Brahman is defined as being Nirguna or without qualities. Anything imaginable or conceivable is not deemed to be the ultimate reality. The Taittiriya hymn speaks of Brahman as \"one where the mind does not reach\". Yet the Hindu scriptures often speak of Brahman's positive aspect. For instance, Brahman is often equated with bliss. These contradictory descriptions of Brahman are used to show that the attributes of Brahman are similar to ones experienced by mortals, but not the same. \n\nNegative theology also figures in the Buddhist and Hindu polemics. The arguments go something like this – Is Brahman an object of experience? If so, how do you convey this experience to others who have not had a similar experience? The only way possible is to relate this unique experience to common experiences while explicitly negating their sameness.\n\nEven though the \"via negativa\" essentially rejects theological understanding in and of itself as a path to God, some have sought to make it into an intellectual exercise, by describing God only in terms of what God is not. One problem noted with this approach is that there seems to be no fixed basis on deciding what God is not, unless the Divine is understood as an abstract experience of full aliveness unique to each individual consciousness, and universally, the perfect goodness applicable to the whole field of reality. Apophatic theology is often accused of being a version of atheism or agnosticism, since it cannot say truly that God exists. \"The comparison is crude, however, for conventional atheism treats the existence of God as a predicate that can be denied (“God is nonexistent”), whereas negative theology denies that God has predicates\". \"God or the Divine is\" without being able to attribute qualities about \"what He is\" would be the prerequisite of positive theology in negative theology that distinguishes theism from atheism. \"Negative theology is a complement to, not the enemy of, positive theology\". Since religious experience—or consciousness of the holy or sacred, is not reducible to other kinds of human experience, an abstract understanding of religious experience cannot be used as evidence or proof that religious discourse or praxis can have no meaning or value. In apophatic theology, the negation of theisms in the \"via negativa\" also requires the negation of their correlative atheisms if the dialectical method it employs is to maintain integrity.\n\n\n\n\n\n\n\n\n\n \n\n\n"}
{"id": "8934226", "url": "https://en.wikipedia.org/wiki?curid=8934226", "title": "Basic limiting principle", "text": "Basic limiting principle\n\nA Basic Limiting Principle (B.L.P.) is a general principle that limits our explanations metaphysically or epistemologically, and which normally goes unquestioned or even unnoticed in our everyday or scientific thinking. The term was introduced by the philosopher C. D. Broad in his 1949 paper \"The Relevance of Psychical research to Philosophy\":\n\n\"There are certain limiting principles which we unhesitatingly take for granted as the framework within which all our practical activities and our scientific theories are confined. Some of these seem to be self-evident. Others are so overwhelmingly supported by all the empirical facts which fall within the range of ordinary experience and the scientific elaborations of it (including under this heading orthodox psychology) that it hardly enters our heads to question them. Let us call these Basic Limiting Principles.\"\n\nBroad offers nine examples of B.L.P.s, including the principle that there can be no backward causation, that there can be no action at a distance, and that one cannot perceive physical events or material things directly, unmediated by sensations.\n\n"}
{"id": "33944016", "url": "https://en.wikipedia.org/wiki?curid=33944016", "title": "Canon (basic principle)", "text": "Canon (basic principle)\n\nThe concept of canon is very broad; in a general sense it refers to being a rule or a body of rules.\n\nThere are definitions that state it as: “the body of rules, principles, or standards accepted as axiomatic and universally binding in a field of study or art”. This can be related to such topics as literary canons or the canons of rhetoric, which is a topic within itself that describes the rules of giving a speech. There are five key principles, and when grouped together, are the principles set for giving speeches as seen with regard to Rhetoric. This is one such example of how the term canon is used in regard to rhetoric.\n\n"}
{"id": "26167139", "url": "https://en.wikipedia.org/wiki?curid=26167139", "title": "Definitionism", "text": "Definitionism\n\nDefinitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.\n"}
{"id": "152902", "url": "https://en.wikipedia.org/wiki?curid=152902", "title": "Dormant Commerce Clause", "text": "Dormant Commerce Clause\n\nThe Dormant Commerce Clause, or Negative Commerce Clause, in American constitutional law, is a legal doctrine that courts in the United States have inferred from the Commerce Clause in Article I of the US Constitution. The Dormant Commerce Clause is used to prohibit state legislation that discriminates against interstate or international commerce.\n\nFor example, it is lawful for Michigan to require food labels that specifically identify certain animal parts, if they are present in the product, because the state law applies to food produced in Michigan as well as food imported from other states and foreign countries; the state law would violate the Commerce Clause if it applied only to imported food or if it was otherwise found to favor domestic over imported products. Likewise, California law requires milk sold to contain a certain percentage of milk solids that federal law does not require, which is allowed under the Dormant Commerce Clause doctrine because California's stricter requirements apply equally to California-produced milk and imported milk and so does not discriminate against or inappropriately burden interstate commerce.\n\nThe idea that regulation of interstate commerce may to some extent be an exclusive Federal power was discussed even before adoption of the Constitution, but the framers did not use the word \"dormant\". On September 15, 1787, the Framers of the Constitution debated in Philadelphia whether to guarantee states the ability to lay duties of tonnage without Congressional interference so that the states could finance the clearing of harbors and the building of lighthouses. James Madison believed that the mere existence of the Commerce Clause would bar states from imposing any duty of tonnage: \"He was more and more convinced that the regulation of Commerce was in its nature indivisible and ought to be wholly under one authority.\"\n\nRoger Sherman disagreed: \"The power of the United States to regulate trade being supreme can control interferences of the State regulations when such interferences happen; so that there is no danger to be apprehended from a concurrent jurisdiction.\" Sherman saw the commerce power as similar to the tax power, the latter being one of the concurrent powers shared by the federal and state governments. Ultimately, the Philadelphia Convention decided upon the present language about duties of tonnage in , which says: \"No state shall, without the consent of Congress, lay any duty of tonnage ...\"\n\nThe word \"dormant,\" in connection with the Commerce Clause, originated in dicta of Chief Justice John Marshall. For example, in the case of \"Gibbons v. Ogden\", , he wrote that the power to regulate interstate commerce \"can never be exercised by the people themselves, but must be placed in the hands of agents, or lie dormant.\" Concurring Justice William Johnson was even more emphatic that the Constitution is \"altogether in favor of the exclusive grants to Congress of power over commerce.\"\n\nLater, in the case of \"Willson v. Black-Bird Creek Marsh Co.\", , Marshall wrote: \"We do not think that the [state] act empowering the Black Bird Creek Marsh Company to place a dam across the creek, can, under all the circumstances of the case, be considered as repugnant to the power to regulate commerce in its dormant state, or as being in conflict with any law passed on the subject.\"\n\nIf Marshall was suggesting that the power over interstate commerce is an exclusive federal power, the Dormant Commerce Clause doctrine eventually developed very differently: it treats regulation that does not discriminate against or unduly burden interstate commerce as a concurrent power, rather than an exclusive federal power, and it treats regulation that does so as an exclusive federal power. Thus, the modern doctrine says that congressional power over interstate commerce is somewhat exclusive but \"not absolutely exclusive\". The approach began in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court: \"Either absolutely to affirm, or deny that the nature of this [commerce] power requires exclusive legislation by Congress, is to lose sight of the nature of the subjects of this power, and to assert concerning all of them, what is really applicable but to a part.\" The first clear holding of the Supreme Court striking down a state law under the Dormant Commerce Clause came in 1873.\n\nJustice Anthony Kennedy has written that: \"The central rationale for the rule against discrimination is to prohibit state or municipal laws whose object is local economic protectionism, laws that would excite those jealousies and retaliatory measures the Constitution was designed to prevent.\" In order to determine whether a law violates a so-called \"dormant\" aspect of the Commerce Clause, the court first asks whether it discriminates on its face against interstate commerce. In this context, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter.\n\nThus, in a dormant Commerce Clause case, a court is initially concerned with whether the law facially discriminates against out-of-state actors or has the effect of favoring in-state economic interests over out-of-state interests. Discriminatory laws motivated by \"simple economic protectionism\" are subject to a \"virtually per se rule of invalidity\", \"City of Philadelphia v. New Jersey\" 437 U.S. 617 (1978), \"Dean Milk Co. v. City of Madison, Wisconsin\", 340 U.S. 349 (1951), \"Hunt v. Washington State Apple Advertising Comm.\", 432 U.S. 333 (1977) which can only be overcome by a showing that the State has no other means to advance a legitimate local purpose, \"Maine v. Taylor\", 477 U.S. 131(1986). See also \"Brown-Forman Distillers v. New York State Liquor Authority\", .\n\nOn the other hand, when a law is \"directed to legitimate local concerns, with effects upon interstate commerce that are only incidental\" (United Haulers Association, Inc.), that is, where other legislative objectives are credibly advanced and there is no patent discrimination against interstate trade, the Court has adopted a much more flexible approach, the general contours of which were outlined in \"Pike v. Bruce Church, Inc.\", 397 U.S. 137, 142 (1970) and \"City of Philadelphia v. New Jersey\", 437 U.S. at 624. If the law is not outright or intentionally discriminatory or protectionist, but still has some impact on interstate commerce, the court will evaluate the law using a balancing test. The Court determines whether the interstate burden imposed by a law outweighs the local benefits. If such is the case, the law is usually deemed unconstitutional. See \"Pike v. Bruce Church, Inc.\", . In the Pike case, the Court explained that a state regulation having only \"incidental\" effects on interstate commerce \"will be upheld unless the burden imposed on such commerce is clearly excessive in relation to the putative local benefits\". 397 U.S. at 142, 90 S.Ct. at 847. When weighing burdens against benefits, a court should consider both \"the nature of the local interest involved, and ... whether it could be promoted as well with a lesser impact on interstate activities\". Id. Thus regulation designed to implement public health and safety, or serve other legitimate state interests, but impact interstate commerce as an incident to that purpose, are subject to a test akin to the rational basis test, a minimum level of scrutiny. See \"Bibb v. Navajo Freight Lines, Inc.\" In USA Recycling, Inc. v. Town of Babylon, 66 F.3d 1272, 1281 (C.A.2 (N.Y.), 1995), the court explained:\n\nIf the state activity constitutes \"regulation\" of interstate commerce, then the court must proceed to a second inquiry: whether the activity regulates evenhandedly with only \"incidental\" effects on interstate commerce, or discriminates against interstate commerce. As we use the term here, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter. The party challenging the validity of a state statute or municipal ordinance bears the burden of showing that it discriminates against, or places some burden on, interstate commerce. \"Hughes v. Oklahoma\", 441 U.S. 322, 336, 99 S.Ct. 1727, 1736, 60 L.Ed.2d 250 (1979). If discrimination is established, the burden shifts to the state or local government to show that the local benefits of the statute outweigh its discriminatory effects, and that the state or municipality lacked a nondiscriminatory alternative that could have adequately protected the relevant local interests. If the challenging party cannot show that the statute is discriminatory, then it must demonstrate that the statute places a burden on interstate commerce that \"is clearly excessive in relation to the putative local benefits.\" \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456, 471(1981) (quoting Pike, 397 U.S. at 142, 90 S.Ct. at 847).\n\nOver the years, the Supreme Court has consistently held that the language of the Commerce Clause contains a further, negative command prohibiting certain state taxation even when Congress has failed to legislate on the subject. Examples of such cases are \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992); \"Northwestern States Portland Cement Co. v. Minnesota\", 358 U.S. 450, 458 (1959) and \"H.P. Hood & Sons, Inc. v. Du Mond\", 336 U.S. 525 (1949).\n\nMore recently, in the 2015 case of \"Comptroller of Treasury of MD. v. Wynne\", the Court addressed Maryland's unusual practice of taxing personal income earned in Maryland, and taxing personal income of its citizens earned outside Maryland, \"without\" any tax credit for income tax paid to other states. The Court held this sort of double-taxation to be a violation of the dormant Commerce Clause. The Court faulted Justice Antonin Scalia's criticism of the dormant Commerce Clause doctrine by saying that he failed to \"explain why, under his interpretation of the Constitution, the Import-Export Clause \nwould not lead to the same result that we reach under the dormant Commerce Clause\".\n\nApplication of the dormant commerce clause to state taxation is another manifestation of the Court's holdings that the Commerce Clause prevents a State from retreating into economic isolation or jeopardizing the welfare of the Nation as a whole, as it would do if it were free to place burdens on the flow of commerce across its borders that commerce wholly within those borders would not bear. The Court's taxation decisions thus \"reflected a central concern of the Framers that was an immediate reason for calling the Constitutional Convention: the conviction that in order to succeed, the new Union would have to avoid the tendencies toward economic Balkanization that had plagued relations among the Colonies and later among the States under the Articles of Confederation.\" \"Wardair Canada, Inc. v. Florida Dept. of Revenue\", 477 U.S. 1 (1986); \"Hughes v. Oklahoma\", 441 U.S. 322 (1979); \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995).\n\nAs with the Court's application of the dormant commerce clause to discriminatory regulation, the pre-New Deal Court attempted to apply a formalistic approach to state taxation alleged to interfere with interstate commerce. The history is described in \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995):\n\nThe command has been stated more easily than its object has been attained, however, and the Court's understanding of the dormant Commerce Clause has taken some turns. In its early stages, the Court held the view that interstate commerce was wholly immune from state taxation \"in any form\", \"even though the same amount of tax should be laid on (intrastate) commerce\". This position gave way in time to a less uncompromising but formal approach, according to which, for example, the Court would invalidate a state tax levied on gross receipts from interstate commerce, or upon the \"freight carried\" in interstate commerce, but would allow a tax merely measured by gross receipts from interstate commerce as long as the tax was formally imposed upon franchises, or \"'in lieu of all taxes upon (the taxpayer's) property,'\" Dissenting from this formal approach in 1927, Justice Stone remarked that it was \"too mechanical, too uncertain in its application, and too remote from actualities, to be of value.\" \n\nAccompanying the revolution in approach in the Court's Congressional powers jurisprudence, the New Deal Court began to change its approach to state taxation as well. The Jefferson Lines decision continues:\n\nIn 1938, the old formalism began to give way with Justice Stone's opinion in \"Western Live Stock v. Bureau of Revenue\", 303 U.S. 250, which examined New Mexico's franchise tax, measured by gross receipts, as applied to receipts from out-of-state advertisers in a journal produced by taxpayers in New Mexico but circulated both inside and outside the State. Although the assessment could have been sustained solely on prior precedent, Justice Stone added a dash of the pragmatism that, with a brief interlude, has since become our aspiration in this quarter of the law. ... The Court explained that \"[i]t was not the purpose of the commerce clause to relieve those engaged in interstate commerce from their just share of state tax burden even though it increases the cost of doing the business.\" \n\nDuring the transition period, some taxes were upheld based on a careful review of the actual economic impact of the tax, and other taxes were reviewed based on the kind of tax involved, whether the tax had a nefarious impact on commerce or not. Under this formalistic approach, a tax might be struck down, and then re-passed with exactly the same economic incidence, but under another name, and then withstand review.\n\nThe absurdity of this approach was made manifest in the two Railway Express cases. In the first, a tax imposed by the state of Virginia on American business concerns operating within the state was struck down because it was a business privilege tax imposed on the privilege of doing business in interstate commerce. But then, in the second, Virginia revised the wording of its statute to impose a \"franchise tax\" on \"intangible property\" in the form of \"going concern\" value as measured by gross receipts.\n\nThe Court upheld the reworded statute as not violative of the prohibition on privilege taxes, even though the impact of the old tax and new were essentially identical. There was no real economic difference between the statutes in Railway Express I and Railway Express II. The Court long since had recognized that interstate commerce may be made to pay its way. Yet under the Spector rule, the economic realities in Railway Express I became irrelevant. The Spector rule (against privilege taxes) had come to operate only as a rule of draftsmanship, and served only to distract the courts and parties from their inquiry into whether the challenged tax produced results forbidden by the Commerce Clause.\n\nThe death knell of formalism occurred in \"Complete Auto Transit, Inc v. Brady\", 430 U.S. 274 (1977), which approved a Mississippi privilege tax upon a Michigan company engaged in the business of shipping automobiles to Mississippi dealers. The Court there explained:\n\nAppellant's attack is based solely on decisions of this Court holding that a tax on the \"privilege\" of engaging in an activity in the State may not be applied to an activity that is part of interstate commerce. See, e. g., \"Spector Motor Service v. O'Connor\", 340 U.S. 602 (1951); \"Freeman v. Hewit\", 329 U.S. 249 (1946). This rule looks only to the fact that the incidence of the tax is the \"privilege of doing business\"; it deems irrelevant any consideration of the practical effect of the tax. The rule reflects an underlying philosophy that interstate commerce should enjoy a sort of \"free trade\" immunity from state taxation. \n\nComplete Auto Transit is the last in a line of cases that gradually rejected a per se approach to state taxation challenges under the commerce clause. In overruling prior decisions which struck down privilege taxes per se, the Court noted the following, in what has become a central component of commerce clause state taxation jurisprudence:\n\nWe note again that no claim is made that the activity is not sufficiently connected to the State to justify a tax, or that the tax is not fairly related to benefits provided the taxpayer, or that the tax discriminates against interstate commerce, or that the tax is not fairly apportioned.\n\nThese four factors, nexus, relationship to benefits, discrimination, and apportionment, have come to be regarded as the four Complete Auto Transit factors applied repeatedly in subsequent cases. Complete Auto Transit must be recognized as the culmination of the Court's emerging commerce clause approach, not just in taxation, but in all of its aspects. Application of Complete Auto Transit to State taxation remains a highly technical and specialized venture, requiring the application of commerce clause principles to an understanding of specialized tax law.\n\nIn addition to satisfying the four-prong test in \"Complete Auto Transit\", the Supreme Court has held state taxes which burden international commerce cannot create a substantial risk of multiple taxations and must not prevent the federal government from \"speaking with one voice when regulating commercial relations with foreign governments\". \"Japan Lines, Ltd. v. County of Los Angeles\", 441 U.S. 434 (1979).\n\nIn \"Kraft Gen. Foods, Inc. v. Iowa Dept. of Revenue and Finance\", 505 U.S. 71 (1992), the Supreme Court considered a case in which Iowa taxed dividends from foreign subsidiaries, without allowing a credit for taxes paid to foreign governments, but not dividends from domestic subsidiaries operating outside Iowa. This differential treatment arose from Iowa's adoption of the definition of \"net income\" used by the Internal Revenue Service. For federal income tax purposes, dividends from domestic subsidiaries are allowed to be exempted from the parent corporations income to avoid double taxation. The Iowa Supreme Court rejected a Commerce Clause claim because Kraft failed to show \"that Iowa businesses receive a commercial advantage over foreign commerce due to Iowa's taxing scheme.\" Considering an Equal Protection Clause challenge, the Iowa Supreme Court held that the use of the federal government's definitions of income were convenient for the state and was \"rationally related to the goal of administrative efficiency\". The Supreme Court rejected the notion that administrative convenience was a sufficient defense for subjecting foreign commerce to a higher tax burden than interstate commerce. The Supreme Court held that \"a State's preference for domestic commerce over foreign commerce is inconsistent with the Commerce Clause even if the State's own economy is not a direct beneficiary of the discrimination.\"\n\nDiscrimination in the flow of interstate commerce has arisen in a variety of contexts. A line of important cases has dealt with local processing requirements. Under the local processing requirement, a municipality seeks to force the local processing of raw materials before they are shipped in interstate commerce.\n\nThe basic idea of the local processing ordinance was to provide favored access to local processors of locally produced raw materials. Examples of Supreme Court decisions in this vein are set out in its Carbone decision. They include \"Minnesota v. Barber\", 136 U.S. 313, (1890) (striking down a Minnesota statute that required any meat sold within the State, whether originating within or without the State, to be examined by an inspector within the State); \"Foster-Fountain Packing Co. v. Haydel\", 278 U.S. 1 (1928) (striking down a Louisiana statute that forbade shrimp to be exported unless the heads and hulls had first been removed within the State); \"Johnson v. Haydel\", 278 U.S. 16 (1928) (striking down analogous Louisiana statute for oysters); \"Toomer v. Witsell\", 334 U.S. 385 (1948) (striking down South Carolina statute that required shrimp fishermen to unload, pack, and stamp their catch before shipping it to another State); \"Pike v. Bruce Church, Inc.\", supra (striking down Arizona statute that required all Arizona-grown cantaloupes to be packaged within the State prior to export); \"South-Central Timber Development, Inc. v. Wunnicke\", 467 U.S. 82 (1984) (striking down an Alaska regulation that required all Alaska timber to be processed within the State prior to export). The Court has defined \"protectionist\" state legislation as \"regulatory measures designed to benefit in-state economic interests by burdening out-of-state competitors\". \"New Energy Co. of Indiana v. Limbach\", 486 U.S. 269, 273–74 (1988).\n\nIn the 1980s, spurred by RCRA's emphasis on comprehensive local planning, many states and municipalities sought to promote investment in more costly disposal technologies, such as waste-to-energy incinerators, state-of-the-art landfills, composting and recycling. Some states and localities sought to promote private investment in these costly technologies by guaranteeing a longterm supply of customers. See Phillip Weinberg, Congress, the Courts, and Solid Waste Transport: Good Fences Don't Always Make Good Neighbors, 25 Envtl. L. 57 (1995); Atlantic Coast Demolition & Recycling, Inc., 112 F.3d 652, 657 (3d Cir. 1997). For about a decade, the use of regulation to channel private commerce to designated private disposal sites was greatly restricted as the result of the Carbone decision discussed below.\n\nFlow control laws typically came in various designs. One common theme was the decision to fund local infrastructure by guaranteeing a minimum volume of business for privately constructed landfills, incinerators, composters or other costly disposal sites. In some locales, choice of the flow control device was driven by state bonding laws, or municipal finance concerns. If a county or other municipality issued general obligation bonds for construction of a costly incinerator, for example, state laws might require a special approval process. If approval could be obtained, the bonds themselves would be counted against governmental credit limitations, or might impact the governmental body's credit rating: in either instance the ability to bond for other purposes might be impaired. But by guaranteeing customers for a privately constructed and financed facility, a private entity could issue its own bonds, privately, on the strength of the public's waste assurance.\n\nThe private character of flow control regimens can thus be explained in part by the desire to utilize particular kinds of public financing devices. It can also be explained by significant encouragement at the national level, in national legislation as well as in federal executive policy to achieve environmental objectives utilizing private resources. Ironically, these public-private efforts often took the form of local processing requirements which ultimately ran afoul of the commerce clause.\n\nThe Town of Clarkstown had decided that it wanted to promote waste assurance through a local private transfer station. The transfer station would process waste and then forward the waste to the disposal site designated by the Town. The ordinance had the following features:\n\nWaste hauling in the Town of Clarkstown was accomplished by private haulers, subject to local regulation. The scheme had the following aspects: (A) The Town promoted the financing of a privately owned transfer station through a waste assurance agreement with the private company. Thus the designated facility was a private company. (B) The Town of Clarkstown forced private haulers to bring their solid waste for local processing at the designated transfer station, even if the ultimate destination of solid waste was an out-of-state disposal site. (C) The primary rationale for forcing in-state waste into the designated private transfer station was financial; it was seen as a device to raise revenue to finance the transfer station.\n\nThe Town of Clarkstown's ordinance was designed and written right in the teeth of the long line of Supreme Court cases which had historically struck down local processing requirements. In short, it was as if the authors of the ordinance had gone to a treatise on the commerce clause and intentionally chosen a device which had been traditionally prohibited. A long line of Supreme Court case law had struck down local processing requirements when applied to goods or services in interstate commerce. As the Court in Carbone wrote:\n\nWe consider a so-called flow control ordinance, which requires all solid waste to be processed at a designated transfer station before leaving the municipality. The avowed purpose of the ordinance is to retain the processing fees charged at the transfer station to amortize the cost of the facility. Because it attains this goal by depriving competitors, including out-of-state firms, of access to a local market, we hold that the flow control ordinance violates the Commerce Clause.\n\nThe Court plainly regarded the decision as a relatively unremarkable decision, not a bold stroke. As the Court wrote: \"The case decided today, while perhaps a small new chapter in that course of decisions, rests nevertheless upon well-settled principles of our Commerce Clause jurisprudence.\" And, the Court made it plain, that the problem with Clarkstown's ordinance was that it created a local processing requirement protective of a local private processing company:\n\nIn this light, the flow control ordinance is just one more instance of local processing requirements that we long have held invalid ... The essential vice in laws of this sort is that they bar the import of the processing service. Out-of-state meat inspectors, or shrimp hullers, or milk pasteurizers, are deprived of access to local demand for their services. Put another way, the offending local laws hoard a local resource—be it meat, shrimp, or milk—for the benefit of local businesses that treat it. 511 U.S. at 392–393.\n\nThe Court's 2007 decision in \"United Haulers Association v. Oneida-Herkimer Solid Waste Management Authority\" starkly illustrates the difference in result when the Court finds that local regulation is not discriminatory. The Court dealt with a flow control regimen quite similar to that considered in Carbone. The \"only salient difference is that the laws at issue here require haulers to bring waste to facilities owned and operated by a state-created public benefit corporation.\" The Court decided that the balancing test should apply, because the regulatory scheme favored the government owned facility, but treated all private facilities equally.\n\nCompelling reasons justify treating these laws differently from laws favoring particular private businesses over their competitors. \"Conceptually, of course, any notion of discrimination assumes a comparison of substantially similar entities.\" \"General Motors Corp. v. Tracy\", 519 U.S. 278 (1997). But States and municipalities are not private businesses—far from it. Unlike private enterprise, government is vested with the responsibility of protecting the health, safety, and welfare of its citizens. See \"Metropolitan Life Ins. Co. v. Massachusetts\", 471 U.S. 724 (1985) ... These important responsibilities set state and local government apart from a typical private business.\n\nThe Court's United Haulers decision demonstrates an understanding of the regulatory justifications for flow control starkly missing in the Carbone decision:\n\nBy the 1980s, the Counties confronted what they could credibly call a solid waste \" 'crisis.' \"... Many local landfills were operating without permits and in violation of state regulations. Sixteen were ordered to close and remediate the surrounding environment, costing the public tens of millions of dollars. These environmental problems culminated in a federal clean-up action against a landfill in Oneida County; the defendants in that case named over local businesses and several municipalities and school districts as third-party defendants The \"crisis\" extended beyond health and safety concerns. The Counties had an uneasy relationship with local waste management companies, enduring price fixing, pervasive overcharging, and the influence of organized crime. Dramatic price hikes were not uncommon: In 1986, for example, a county contractor doubled its waste disposal rate on six weeks' notice\n\nThe Court would not interfere with local government's efforts to solve an important public and safety problem.\n\nThe contrary approach of treating public and private entities the same under the dormant Commerce Clause would lead to unprecedented and unbounded interference by the courts with state and local government. The dormant Commerce Clause is not a roving license for federal courts to decide what activities are appropriate for state and local government to undertake, and what activities must be the province of private market competition. In this case, the citizens of Oneida and Herkimer Counties have chosen the government to provide waste management services, with a limited role for the private sector in arranging for transport of waste from the curb to the public facilities. The citizens could have left the entire matter for the private sector, in which case any regulation they undertook could not discriminate against interstate commerce. But it was also open to them to vest responsibility for the matter with their government, and to adopt flow control ordinances to support the government effort. It is not the office of the Commerce Clause to control the decision of the voters on whether government or the private sector should provide waste management services. \"The Commerce Clause significantly limits the ability of States and localities to regulate or otherwise burden the flow of interstate commerce, but it does not elevate free trade above all other values.\"\n\nThe history of commerce clause jurisprudence evidences a distinct difference in approach where the state is seeking to exercise its public health and safety powers, on the one hand, as opposed to attempting to regulate the flow of commerce. The exact dividing line between the two interests, the right of states to exercise regulatory control over their public health and safety, and the interest of the national government in unfettered interstate commerce is not always easy to discern. One Court has written as follows:\n\nNot surprisingly, the Court's effort to preserve a national market has, on numerous occasions, come into conflict with the states' traditional power to \"legislat[e] on all subjects relating to the health, life, and safety of their citizens.\" \"Huron Portland Cement Co. v. City of Detroit\", 362 U.S. 440, 443 (1960). On these occasions, the Supreme Court has \"struggled (to put it nicely) to develop a set of rules by which we may preserve a national market without needlessly intruding upon the States' police powers, each exercise of which no doubt has some effect on the commerce of the Nation.\" \"Camps Newfound/Owatonna v. Town of Harrison\", 520 U.S. 564, 596 (1997) (Scalia, J., dissenting) (citing \"Okla. Tax Comm'n v. Jefferson Lines\", 514 U.S. 175, 180–83 (1995)); see generally Boris I. Bittker, Regulation of Interstate and Foreign Commerce § 6.01[A], at 6–5 (\"[T]he boundaries of the [State's] off-limits area are, and always have been, enveloped in a haze.\"). Those rules are \"simply stated, if not simply applied.\" Camps Newfound/Owatonna, 520 U.S. at 596 (Scalia, J., dissenting).\n\nA frequently cited example of the deference afforded to the powers of state and local government may be found in \"Exxon Corp. v. Maryland\", 437 U.S. 117 (1978), where the State of Maryland barred producers of petroleum products from operating retail service stations in the state. It is difficult to imagine a regimen which might have greater impact on the way in which markets are organized. Yet, the Court found the legislation constitutionally permitted: \"The fact that the burden of a state regulation falls on some interstate companies does not, by itself establish a claim of discrimination against interstate commerce,\" the Court wrote. The \"Clause protects interstate market, not particular interstate firms, from prohibitive or burdensome regulations.\"\n\nSimilarly, in \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456 (1981) the Court upheld a state law that banned nonreturnable milk containers made of plastic but permitted other nonreturnable milk containers. The Court found that the existence of a burden on out-of-state plastic industry was not 'clearly excessive' in comparison to the state's interest in promoting conservation. And the court continued:\n\nIn Exxon, the Court stressed that the Commerce Clause protects the interstate market, not particular interstate firms, from prohibitive or burdensome regulations. A nondiscriminatory regulation serving substantial state purpose is not invalid simply because it causes some business to shift from a predominantly out-of-state industry to a predominantly in-state industry. Only if the burden on interstate commerce clearly outweighs the State's legitimate purpose does such a regulation violate the commerce clause. When a state statute regarding safety matters applies equally to interstate and intrastate commerce, the courts are generally reluctant to invalidate it even if it may have some impact on interstate commerce. In \"Bibb v. Navajo Freight Lines\" 359 U.S. 520, 524 (1959), the United States Supreme Court stated: 'These safety measures carry a strong presumption of validity when challenged in court. If there are alternative ways of solving a problem, we do not sit to determine which of them is best suited to achieve a valid state objective. Policy decisions are for the state legislature, absent federal entry into the field. Unless we can conclude on the whole record that \"the total effect of the law as a safety measure in reducing accidents and casualties is so slight or problematical as not to outweigh the national interest in keeping interstate commerce free from interferences which seriously impede it\" we must uphold the statute.\n\nThere are two notable exceptions to the dormant Commerce Clause doctrine that can permit state laws or actions that otherwise violate the Dormant Commerce Clause to survive court challenges.\n\nThe first exception occurs when Congress has legislated on the matter. See \"Western & Southern Life Ins. v. State Board of California\", . In this case the Dormant Commerce Clause is no longer \"dormant\" and the issue is a Commerce Clause issue, requiring a determination of whether Congress has approved, preempted, or left untouched the state law at issue.\n\nThe second exception is \"market participation exception\". This occurs when the state is acting \"in the market\", like a business or customer, rather than as a \"market regulator\". For example, when a state is contracting for the construction of a building or selling maps to state parks, rather than passing laws governing construction or dictating the price of state park maps, it is acting \"in the market\". Like any other business in such cases, a state may favor or shun certain customers or suppliers.\n\nThe Supreme Court introduced the market participant doctrine in \"Hughes v. Alexandria Scrap Corp.\", 426 U.S. 794 (1976), which upheld a Maryland program that offered bounties to scrap processors to destroy abandoned automobile hulks. See also \"Wisconsin Dep't of Indus., Labor & Human Relations v. Gould Inc.\", 475 U.S. 282, 289 (1986); \"Reeves, Inc. v. Stake\", 447 U.S. 429, 437 (1980). Because Maryland required out-of-state processors, but not in-state processors, to submit burdensome documentation to claim their bounties, the state effectively favored in-state processors over out-of-state processors. The Court held that because the state was merely attaching conditions to its expenditure of state funds, the Maryland program affected the market no differently than if Maryland were a private company bidding up the price of auto hulks. Because the state was not \"regulating\" the market, its economic activity was not subject to the anti-discrimination principles underlying the dormant Commerce Clause—and the state could impose different paperwork burdens on out-of-state processors. \"Nothing in the purposes animating the Commerce Clause prohibits a State, in the absence of congressional action, from participating in the market and exercising the right to favor its own citizens over others.\"\n\nAnother important case is \"White v. Massachusetts Council of Constr. Employers, Inc.\", in which the Supreme Court held that the City of Boston could require its building contractors to hire at least fifty percent of their workforce from among Boston residents. 460 U.S. at 214–15. Because all of the employees covered by that mandate were \"in a substantial if informal sense, 'working for the city,' \" Boston was considered to be simply favoring its own residents through the expenditures of municipal funds. The Supreme Court stated, \"when a state or local government enters the market as a participant it is not subject to the restraints of the Commerce Clause.\" Id. at 208. Nothing in the Constitution precludes a local government from hiring a local company precisely because it is local.\n\nOther important cases enunciating the market participation exception principle are \"Reeves, Inc. v. Stake\", and \"South-Central Timber Development, Inc. v. Wunnicke\", . The \"Reeves\" case outlines the market participation exception test. In this case state-run cement co-ops were allowed to make restrictive rules (e.g. rules not to sell out-of-state). Here, this government-sponsored business was acting restrictively like an individually owned business and this action was held to be constitutional. \"South-Central Timber\" is important because it limits the market exception. \"South-Central Timber\" holds that the market-participant doctrine is limited in allowing a State to impose burdens on commerce within the market in which it is a participant, but allows it to go no further. The State may not impose conditions that have a substantial regulatory effect outside of that particular market.\n\nThe \"market participation exception\" to the dormant Commerce Clause does not give states unlimited authority to favor local interests, because limits from other laws and Constitutional limits still apply. In \"United Building & Construction Trades Council v. Camden\", , the city of Camden, New Jersey had passed an ordinance requiring that at least forty percent of the employees of contractors and subcontractors on city projects be Camden residents. The Supreme Court found that while the law was not infirm because of the Dormant Commerce Clause, it violated the Privileges and Immunities Clause of Article IV of the Constitution. Justice Rehnquist's opinion distinguishes the market-participant doctrine from the privileges and immunities doctrine. Similarly, Congress has the power itself under the Commerce Clause to regulate and sanction states acting as \"market participants\", but it lacks power to legislate in ways that violate Article IV.\n\nIn the 21st century, the dormant Commerce Clause has been a frequent legal issue in cases arising under state laws regulating some aspects of Internet activity. Because of the interstate, and often international, nature of Internet communications, state laws addressing internet-related subjects such as spam, online sales or online pornography can often trigger Dormant Commerce Clause issues.\n\nA \"negative\" or \"dormant\" component to the Commerce Clause has been the subject of scholarly discussion for many decades. Both Supreme Court Justices Antonin Scalia and Clarence Thomas have rejected the notion of a Dormant Commerce Clause. They believe that such a doctrine is inconsistent with an originalist interpretation of the Constitution—so much so that they believe the doctrine is a \"judicial fraud\".\n\nA number of earlier Supreme Court justices also expressed dissatisfaction with the dormant Commerce Clause doctrine. For example, Chief Justice Taney said this in 1847:\n\nIf it was intended to forbid the States from making any regulations of commerce, it is difficult to account for the omission to prohibit it, when that prohibition has been so carefully and distinctly inserted in relation to other powers ... [T]he legislation of Congress and the States has conformed to this construction from the foundation of the government ... The decisions of this court will also, in my opinion, when carefully examined, be found to sanction the construction I am maintaining.\n\nHowever, that statement by Taney in 1847 was before the doctrine morphed in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court that the Commerce Clause does not always require \"exclusive legislation by Congress\".\n\nIn \"Trailer Marine Transport Corp. v. Rivera Vázquez\", 977 F.2d 1, 7-8 (1st Cir. 1992), the First Circuit held that the dormant Commerce Clause applies to Puerto Rico.\n\n\n"}
{"id": "194143", "url": "https://en.wikipedia.org/wiki?curid=194143", "title": "Double negative", "text": "Double negative\n\nA double negative is a grammatical construction occurring when two forms of negation are used in the same sentence. Multiple negation is the more general term referring to the occurrence of more than one negative in a clause. In some languages, double negatives cancel one another and produce an affirmative; in other languages, doubled negatives intensify the negation. Languages where multiple negatives affirm each other are said to have negative concord or emphatic negation. Portuguese, Persian, Russian, Spanish, Neapolitan, Italian, Japanese, Bulgarian, Czech, Polish, Afrikaans, Hebrew, and some dialects of English, such as African-American Vernacular English, are examples of negative-concord languages, while Latin and German do not have negative concord. It is cross-linguistically observed that negative-concord languages are more common than those without.\n\nLanguages without negative concord typically have negative polarity items that are used in place of additional negatives when another negating word already occurs. Examples are \"ever\", \"anything\" and \"anyone\" in the sentence \"I haven't ever owed anything to anyone\" (cf. \"I have\"n't\" \"never\" owed \"nothing\" to \"no one\"\" in negative-concord dialects of English, and \"\"Nunca\" devi \"nada\" a \"ninguém\"\" in Portuguese, lit. \"Never have I owed nothing to no one\", or \"\"Non\" ho \"mai\" dovuto \"nulla\" a \"nessuno\"\" in Italian). Note that negative polarity can be triggered not only by direct negatives such as \"not\" or \"never\", but also by words such as \"doubt\" or \"hardly\" (\"I doubt he has ever owed anything to anyone\" or \"He has hardly ever owed anything to anyone\").\n\nStylistically, in English, double negatives can sometimes be used for affirmation (e.g. \"I'm not feeling not good\"), an understatement of the positive (\"I'm feeling good\"). The rhetorical term for this is litotes.\n\nWhen two negatives are used in one independent clause, in standard English the negatives are understood to cancel one another and produce a weakened affirmative: this is known as litotes. However, depending on how such a sentence is constructed, in some dialects if a verb or adverb is in between two negatives then the latter negative is assumed to be intensifying the former thus adding weight or feeling to the negative clause of the sentence. For this reason, it is difficult to portray double negatives in writing as the level of intonation to add weight in one's speech is lost. A double negative intensifier does not necessarily require the prescribed steps, and can easily be ascertained by the mood or intonation of the speaker.\n\nvs.\n\nThese two sentences would be different in how they are communicated by speech. Any assumption would be correct, and the first sentence can be just as right or wrong in intensifying a negative as it is in cancelling it out; thereby rendering the sentence's meaning ambiguous. Since there is no adverb or verb to support the latter negative, the usage here is ambiguous and lies totally on the context behind the sentence. In light of punctuation, the second sentence can be viewed as the intensifier; and the former being a statement thus an admonishment.\n\nIn Standard English, two negatives are understood to resolve to a positive. This rule was observed as early as 1762, when Bishop Robert Lowth wrote \"A Short Introduction to English Grammar with Critical Notes\". For instance, \"I do not disagree\" could mean \"I certainly agree\", \"I agree\", \"I sort of agree\", \"I don't understand your point of view\", \"I have no opinion\", and so on; it is a form of \"weasel words\". Further statements are necessary to resolve which particular meaning was intended.\n\nThis is opposed to the single negative \"I do not agree\", which typically means \"I disagree\". However, the statement \"I do not completely disagree\" is a similar double negative to \"I do not disagree\" but needs little or no clarification.\n\nWith the meaning \"I completely agree\", Lowth would have been referring to litotes wherein two negatives simply cancel each other out. However, the usage of intensifying negatives and examples are presented in his work, which could also imply he wanted either usage of double negatives abolished. Because of this ambiguity, double negatives are frequently employed when making back-handed compliments. The phrase \"Mr. Jones was not incompetent.\" will seldom mean \"Mr. Jones was very competent\" since the speaker would have found a more flattering way to say so. Instead, some kind of problem is implied, though Mr. Jones possesses basic competence at his tasks.\n\nDiscussing English grammar, the term \"double negative\" is often though not universally applied to the non-standard use of a second negative as an intensifier to a negation.\n\nDouble negatives are usually associated with regional and ethnical dialects such as Southern American English, African American Vernacular English, and various British regional dialects. Indeed, they were used in Middle English. Historically, Chaucer made extensive use of double, triple, and even quadruple negatives in his \"Canterbury Tales\". About the Friar, he writes \"Ther nas no man no wher so vertuous\" (\"There never was no man nowhere so virtuous\"). About the Knight, \"He nevere yet no vileynye ne sayde / In all his lyf unto no maner wight\" (\"He never yet no vileness didn't say / In all his life to no manner of man\").\n\nFollowing the battle of Marston Moor, Oliver Cromwell quoted his nephew's dying words in a letter to the boy's father Valentine Walton: \"A little after, he said one thing lay upon his spirit. I asked him what it was. He told me it was that God had not suffered him to be no more the executioner of His enemies.\" Although this particular letter has often been reprinted, it is frequently changed to read \"not ... to be any more\" instead.\n\nWhereas some double negatives may resolve to a positive, in some dialects others resolve to intensify the negative clause within a sentence. For example:\n\nIn contrast, some double negatives become positives:\n\nThe key to understanding the former examples and knowing whether a double negative is intensive or negative is finding a verb between the two negatives. If a verb is present between the two, the latter negative becomes an intensifier which does not negate the former. In the first example, the verb \"to go\" separates the two negatives; therefore the latter negative does not negate the already negated verb. Indeed, the word 'nowhere' is thus being used as an adverb and does not negate the argument of the sentence. One interesting thing to note is that double negatives such as \"I don't want to know no more\" contrasts with Romance languages such as French in \"Je ne veux pas savoir.\" \n\nAn exception is when the second negative is stressed, as in \"I'm not doing ; I'm thinking.\" A sentence can otherwise usually only become positive through consecutive uses of negatives, such as those prescribed in the later examples, where a clause is void of a verb and lacks an adverb to intensify it. Two of them also use emphasis to make the meaning clearer. The last example is a popular example of a double negative that resolves to a positive. This is because the verb 'to doubt' has no intensifier which effectively resolves a sentence to a positive. Had we added an adverb thus:\n\nThen what happens is that the verb \"to doubt\" becomes intensified, which indeed deduces that the sentence is indeed false since nothing was resolved to a positive. The same applies to the third example, where the adverb 'more' merges with the prefix \"no-\" to become a negative word, which when combined with the sentence's former negative only acts as an intensifier to the verb \"hungry\". Where people think that the sentence \"I'm not hungry no more\" resolves to a positive is where the latter negative \"no\" becomes an adjective which only describes its suffix counterpart \"more\" which effectively becomes a noun, instead of an adverb. This is a valid argument since adjectives do indeed describe the nature of a noun; yet some fail to take into account that the phrase \"no more\" is only an adverb and simply serves as an intensifier. Another argument used to support the position double negatives aren't acceptable is a mathematical analogy: negating a negative number results in a positive one; e.g., ; therefore, it is argued, \"I did not go nowhere\" resolves to \"I went somewhere\".\n\nOther forms of double negatives, which are popular to this day and do strictly enhance the negative rather than destroying it, are described thus:\n\nPhilosophies aside, this form of double negative is still in use whereby the use of 'nor' enhances the negative clause by emphasizing what isn't to be. Opponents of double negatives would have preferred \"I'm not entirely familiar with Nihilism or Existentialism\"; however this renders the sentence somewhat empty of the negative clause being advanced in the sentence. This form of double negative along with others described are standard ways of intensifying as well as enhancing a negative. The use of 'nor' to emphasise the negative clause is still popular today, and has been popular in the past through works of Shakespeare and Milton:\n\nTo the common reader the negatives herein do not cancel each other out but simply emphasizes the negative clause.\nUp to the 18th century, double negatives were used to emphasize negation. \"Prescriptive grammarians\" recorded and codified a shift away from the double negative in the 1700s. Double negatives continue to be spoken by those of Vernacular English, such as those of Appalachian English and African American Vernacular English. To such speakers, they view double negatives as emphasizing the negative rather than cancelling out the negatives. Researchers have studied African American Vernacular English (AAVE) and trace its origins back to colonial English. This shows that double negatives were present in colonial English, and thus presumably English as a whole, and were acceptable at that time. English after the 18th century was changed to become more logical and double negatives became seen as canceling each other as in mathematics. The use of double negatives became associated with being uneducated and illogical.\n\nIn his \"Essay towards a practical English Grammar\" of 1711, James Greenwood first recorded the rule: \"Two Negatives, or two Adverbs of Denying do in English affirm\". Robert Lowth stated in his grammar textbook \"A Short Introduction to English Grammar\" (1762) that \"two negatives in English destroy one another, or are equivalent to an affirmative\". Grammarians have assumed that Latin was the model for Lowth and other early grammarians in prescribing against negative concord, as Latin does not feature it. Data indicates, however, that negative concord had already fallen into disuse in Standard English by the time of Lowth's grammar, and no evidence exists that the loss was driven by prescriptivism, which was well established by the time it appeared.\n\nDouble negatives have been employed in various films and television shows. In the film \"Mary Poppins\", the chimney sweep Bert employs a double negative when he says, \"If you don't want to go nowhere...\" Another is used by the bandits in the \"Stinking Badges\" scene of John Huston's \"The Treasure of the Sierra Madre\": \"Badges? We ain't got no badges. We don't need no badges!\".\n\nMore recently, the British television show \"EastEnders\" has received some publicity over the Estuary accent of character Dot Branning, who speaks with double and triple negatives (\"I ain't never heard of no licence.\").. In the Harry Enfield sketch \"Mr Cholmondley-Warner's Guide to the Working-Class\", a stereotypical Cockney employs a septuple-negative: \"Inside toilet? I ain't never not heard of one of them nor I ain't nor nothing.\"\n\nIn music, double negatives can be employed to similar effect (as in Pink Floyd's \"Another Brick in the Wall\", in which schoolchildren chant \"We don't need no education / We don't need no thought control\") or used to establish a frank and informal tone (as in The Rolling Stones' \"(I Can't Get No) Satisfaction\").\n\nDouble negation is uncommon in other West Germanic languages. A notable exception is Afrikaans, where it is mandatory (for example, \"He cannot speak Afrikaans\" becomes \"Hy kan nie Afrikaans praat nie\", \"He cannot Afrikaans speak not\"). Dialectal Dutch, French and San have been suggested as possible origins for this trait. Its proper use follows a set of fairly complex rules as in these examples provided by Bruce Donaldson:\n\nAnother point of view is that this construction is not really an example of a \"double negative\" but simply a grammatical template for negation. The second \"nie\" cannot be understood as a noun or adverb (as can, e.g., \"pas\" in French), and cannot be substituted by any part of speech other than itself with the sentence remaining grammatical. It is a grammatical particle with no independent meaning that happens to be spelled and pronounced the same as the embedded \"nie\", meaning \"not\", through historical accident.\n\nThe second \"nie\" is used if and only if the sentence or phrase doesn't already end with \"nie\" or another negating adverb.\n\nAfrikaans shares with English the property that two negatives make a positive. For example,\n\nWhile double negation is still found in the Low Franconian dialects of west Flanders (e.g., \"Ik ne willen da nie doen\", \"I do not want to do that\") and in some villages in the central Netherlands such as Garderen, it takes a different form than that found in Afrikaans. In Belgian Dutch dialects, however, there are still some widely used expressions like \"nooit niet\" (\"never not\") for \"never\".\n\nSimilar to some dialectal English, Bavarian employs both single and double negation, with the latter denoting special emphasis. For example, compare the Bavarian \"Des hob i no nia ned g'hört\" (\"This have I yet never not heard\") with the standard German \"Das habe ich noch nie gehört\". The German emphatic \"niemals!\" (roughly \"never ever\") corresponds to Bavarian \"(går) nia ned\" or even \"nie nicht\" in Standard German pronunciation.\n\nAnother exception is Yiddish. Due to Slavic influence, the double (and sometimes even triple) negative is quite common.\n\nA few examples would be:\n\nWhile in Latin a second negative word appearing along with \"non\" turns the meaning into a positive one: \"ullus\" means \"any\", \"nullus\" means \"no\", \"non...nullus\" (\"nonnullus\") means \"some\". In the same way, \"umquam\" means \"ever\", \"numquam\" means \"never\", \"non...numquam\" (\"nonnumquam\") means \"sometimes\", in many Romance languages a second term indicated a negative is required.\n\nIn French, the usual way to express negation is to employ two negatives, e.g. \"ne [verb] pas\", \"ne [verb] plus\", or \"ne [verb] jamais\", as in the sentences \"Je ne sais pas\" (\"I do not know\"), \"Il n'y a plus de baguettes\" (\"There aren't any more baguettes\"), and \"On ne sait jamais\" (\"one never knows\"). The second term was originally an emphatic; \"pas\", for example, derives from the Latin \"passus\", meaning \"step\", so that French \"Je ne marche pas\" and Catalan \"No camino pas\" originally meant \"I will not walk a single step.\" This initial usage spread so thoroughly that it became a necessary element of any negation in the modern French language and that, in fact, in contemporary French, the original actual negative \"ne\" is mostly left away in favour of \"pas\", as in \"Je sais pas\" \"I don't know\". In Northern Catalan, \"no\" may be omitted in colloquial language, and Occitan, which uses \"non\" only as a short answer to questions. In Venetian, the double negation \"no ... mìa\" can likewise lose the first particle and rely only on the second: \"magno mìa\" (\"I eat not\") and \"vegno mìa\" (\"I come not\"). These exemplify Jespersen's cycle.\n\nItalian, Portuguese and Romanian languages usually employ doubled negative correlatives. Portuguese \"Não vejo nada\", Romanian \"Nu văd nimic\" and Italian \"Non vedo niente\" (\"I do not see nothing\") are used to express \"No, I do not see anything\". In Italian, a second following negative particle \"non\" turns the phrase into a positive one, but with a slightly different meaning. For instance, while both \"Voglio mangiare\" (\"I want to eat\") and \"Non voglio non mangiare\" (\"I don't want not to eat\") mean \"I want to eat\", the latter phrase more precisely means \"I'd prefer to eat\".\n\nOther Romance languages employ double negatives less regularly. In Asturian, an extra negative particle is used with negative adverbs: \"Yo nunca nun lu viera\" (\"I had not never seen him\") means \"I have never seen him\" and \"A mi tampoco nun me presta\" (\"I neither do not like it\") means \"I do not like it either\". Standard Catalan and Galician also used to possess a tendency to double \"no\" with other negatives, so \"Jo tampoc no l'he vista\" or \"Eu tampouco non a vira\", respectively (\"I neither have not seen her\") meant \"I have not seen her either\". That practice is dying out.\n\nIn 1974, Italy held a referendum on whether to repeal a recent law that allowed divorce. Voters were said to have been confused in that in order to support divorce, they needed to vote 'no' on the referendum which was worded so that 'yes' would support repeal. And to reframe the fundamental underlying issue as being support/non-support of the continuation of marriage, then the vote was structured as a triple negative (with divorce as the negation of the continuation of marriage being the first negative). This referendum was defeated, and without this confusion, it was said that it would have been defeated more decisively.\n\nIn spoken Welsh, the word ddim (not) often occurs with a prefixed or mutated verb form that is negative in meaning: \"Dydy hi ddim yma\" (word-for-word, \"Not-is she not here\") expresses \"She is not here\" and \"Chaiff Aled ddim mynd\" (word-for-word, \"Not-will-get Aled not go\") expresses \"Aled is not allowed to go\".\n\nNegative correlatives can also occur with already negative verb forms. In literary Welsh, the mutated verb form is caused by an initial negative particle, ni or nid. The particle is usually omitted in speech but the mutation remains: \"[Ni] wyddai neb\" (word-for-word, \"[Not] not-knew nobody\") means \"Nobody knew\" and \"[Ni] chaiff Aled fawr o bres\" (word-for-word, \"[Not] not-will-get Aled lots of money\") means \"Aled will not get much money\". This is not usually regarded as three negative markers, however, because the negative mutation is really just an effect of the initial particle on the following word.\n\nDoubled negatives are perfectly correct in Ancient Greek. With few exceptions, a simple negative (οὐ or μή) following another negative (for example, οὐδείς, \"no one\") results in an affirmation: οὐδείς οὐκ ἔπασχε τι (\"No one was not suffering\") means more simply \"Everyone was suffering\". Meanwhile, a compound negative following a negative strengthens the negation: μὴ θορυβήσῃ μηδείς (\"Do not permit no one to raise an uproar\") means \"Let not a single one among them raise an uproar\".\n\nThose constructions apply only when the negatives all refer to the same word or expression. Otherwise, the negatives simply work independently of one another: οὐ διὰ τὸ μὴ ἀκοντίζειν οὐκ ἔβαλον αὐτόν means \"It was not on account of their not throwing that they did not hit him\", and one should not blame them for not trying.\n\nIn Modern Greek, negative concord is standard and more commonly used. For example, the sentence 'You (pl.) will not find anything' can be said in two ways: 'Δε θα βρείτε τίποτα' ('Not will find nothing') is more common than 'Δε θα βρείτε κάτι' ('Not will find something'). It depends simply on the mood of the speaker, and the latter being is considered slightly more polite. An exception to that rule is the (archaic) pronoun ουδείς, also meaning \"no one\", which does not allow negation of the verb that it governs.\n\nIn Slavic languages other than Slavonic, multiple negatives are grammatically correct ways to express negation, and a single negative is often incorrect. In complex sentences, every part that could be grammatically negated should be negative. For example, in the Serbo-Croatian, \"Ni(t)ko nikad(a) nigd(j)e ništa nije uradio\" (\"Nobody never did not do nothing nowhere\") means \"Nobody has ever done anything, anywhere\", and \"Nikad nisam tamo išao/išla\" (\"Never I did not go there\") means \"I have never been there\". In Czech it is also common to use three or more negations. For example, \"Nikdy jsem nikde nikoho neviděl\" (\"I have not never seen no one nowhere\"). In Russian, \"I know nothing\" is я ничего не знаю (\"ya nichevo nye znayu\"), lit. \"I nothing don't know.\"\n\nA single negation, while syntactically correct, may result in a very unusual meaning or make no sense at all. Saying \"I saw nobody\" in Polish (\"Widziałem nikogo\") instead of the more usual \"I did not see nobody\" (\"Nikogo nie widziałem\") might mean \"I saw an instance of nobody\" or \"I saw Mr. Nobody\" but it would not have its plain English meaning. Likewise, in Slovenian, saying \"I do not know anyone\" (') in place of \"I do not know no one\" (') has the connotation \"I do not know just \"anyone\"\": I know someone important or special.\n\nAs with most synthetic \"satem\" languages double negative is mandatory in Latvian and Lithuanian. Furthermore, all verbs and indefinite pronouns in a given statement must be negated, so it could be said that multiple negative is mandatory in Latvian.\n\nFor instance, a statement \"I have not ever owed anything to anyone\" would be rendered as \"es nekad nevienam neko neesmu bijis parādā\". The only alternative would be using a negating subordinate clause and subjunctive in the main clause, which could be approximated in English as \"there has not ever been an instance that I would have owed anything to anyone\" (\"nav bijis tā, ka es kādreiz būtu kādam bijis kaut ko parādā\"), where negative pronouns (\"nekad, neviens, nekas\") are replaced by indefinite pronouns (\"kādreiz, kāds, kaut kas\") more in line with the English \"ever, any\" indefinite pronoun structures.\n\nDouble or multiple negatives are grammatically required in Hungarian with negative pronouns: \"Nincs semmim\" (word for word: \"[doesn't-exists] [nothing-of-mine]\", and translates literally as \"I do not have nothing\") means \"I do not have anything\". Negative pronouns are constructed by means of adding the prefixes \"se-,\" \"sem-,\" and \"sen-\" to interrogative pronouns.\n\nSomething superficially resembling double negation is required also in Finnish, which uses the auxiliary verb \"ei\" to express negation. Negative pronouns are constructed by adding one of the suffixes \"-an,\" \"-än,\" \"-kaan,\" or \"-kään\" to interrogative pronouns: \"Kukaan ei soittanut minulle\" means \"No one called me\". These suffices are, however, never used alone, but always in connection with \"ei\". This phenomenon is commonplace in Finnish, where many words have alternatives that are required in negative expressions, for example \"edes\" for \"jopa\" (\"even\"), as in \"jopa niin paljon\" meaning \"even so much\", and \"ei edes niin paljoa\" meaning \"not even so much\".\n\nNegative verb forms are grammatically required in Turkish phrases with negative pronouns or adverbs that impart a negative meaning on the whole phrase. For example, \"Hiçbir şeyim yok\" (literally, word for word, \"Not-one thing-of-mine exists-not\") means \"I don't have anything\". Likewise, \"Asla memnun değilim\" (literally, \"Never satisfied not-I-am\") means \"I'm never satisfied\".\n\nJapanese employs litotes to phrase ideas in a more indirect and polite manner. Thus, one can indicate necessity by emphasizing that not doing something would not be proper. For instance, しなければならない (\"shinakereba naranai\", \"must\") literally means \"not doing [it] would not be proper\". しなければいけません (\"shinakereba ikemasen\", also \"must\") similarly means \"not doing [it] cannot go forward\".\n\nOf course, indirectness can also be employed to put an edge on one's rudeness as well. \"He has studied Japanese, so he should be able to write kanji\" can be phrased 彼は日本語を勉強したから漢字で書けないわけがありません (\"kare wa nihongo o benkyō shita kara kanji de kakenai wake ga arimasen\"), there is a rather harsher idea: \"As he has studied Japanese, the reasoning that he cannot write Kanji does not exist\".\n\nMandarin Chinese also employs litotes in a like manner. One common construction is 不得不 (Pinyin: \"bùdébù\", \"cannot not\"), which is used to express (or feign) a necessity more regretful and polite than that expressed by 必须 (\"bìxū\", \"must\"). Compared with \"我必须走\" (\"Wǒ bìxū zǒu\", \"I must go\"), \"我不得不走\" (\"Wǒ bùdébù zǒu\", \"I cannot not go\") tries to emphasize that the situation is out of the speaker's hands and that the speaker has no choice in the matter: \"Unfortunately, I have got to go\". Similarly, \"没有人不知道\" (\"Méiyǒu rén bù zhīdào\", \"There is not a person who does not know\") is a more emphatic way to express \"Everyone knows\".\n\nDouble negatives nearly always resolve to a positive meaning even in colloquial speech, while triple negatives resolve to a negative meaning. For example, \"我不相信没人不来\" (\"Wǒ bù xiāngxìn méi rén bù lái\", \"I do not believe no one will not come\") means \"I do not think everyone will come\". However, triple or multiple negatives are considered obscure and are typically avoided.\n\nMany languages, including all living Germanic languages, French, Welsh and some Berber and Arabic dialects, have gone through a process known as Jespersen's cycle, where an original negative particle is replaced by another, passing through an intermediate stage employing two particles (e.g. Old French \"jeo ne dis\" → Modern Standard French \"je ne dis pas\" → Modern Colloquial French \"je dis pas\" \"I don't say\").\n\nIn many cases, the original sense of the new negative particle is not negative \"per se\" (thus in French \"pas\" \"step\", originally \"not a step\" = \"not a bit\"), but in Germanic languages, such as English and German the intermediate stage was a case of double negation, as the current negatives \"not\" and \"nicht\" in these languages originally meant \"nothing\": e.g. Old English \"ic ne seah\" \"I didn't see\" » Middle English \"I ne saugh nawiht\", lit. \"I didn't see nothing\" » Early Modern English \"I saw not\".\n\nA similar development to a circumfix from double negation can be seen in non-Indo-European languages, too: for example, in Maltese, \"kiel\" \"he ate\" is negated as \"ma kielx\" \"he did not eat\", where the verb is preceded by a negative particle \"ma\"- \"not\" and followed by the particle -\"x\", which was originally a shortened form of \"xejn\" \"nothing\" - thus, \"he didn't eat nothing\".\n\n"}
{"id": "37232", "url": "https://en.wikipedia.org/wiki?curid=37232", "title": "Fermat's principle", "text": "Fermat's principle\n\nIn optics, Fermat's principle or the principle of least time, named after French mathematician Pierre de Fermat, is the principle that the path taken between two points by a ray of light is the path that can be traversed in the least time. This principle is sometimes taken as the definition of a ray of light. However, this version of the principle is not general; a more modern statement of the principle is that rays of light traverse the path of stationary optical length with respect to variations of the path. In other words, a ray of light prefers the path such that there are other paths, arbitrarily nearby on either side, along which the ray would take almost exactly the same time to traverse.\n\nFermat's principle can be used to describe the properties of light rays reflected off mirrors, refracted through different media, or undergoing total internal reflection. It follows mathematically from Huygens' principle (at the limit of small wavelength). Fermat's text \"Analyse des réfractions\" exploits the technique of adequality to derive Snell's law of refraction and the law of reflection.\n\nFermat's principle has the same form as Hamilton's principle and it is the basis of Hamiltonian optics.\n\nThe time T a point of the electromagnetic wave needs to cover a path between the points A and B is given by:\n\n\"c\" is the speed of light in vacuum, \"ds\" an infinitesimal displacement along the ray, \"v\" = \"ds\"/\"dt\" the speed of light in a medium and \"n\" = \"c\"/\"v\" the refractive index of that medium, formula_2 is the starting time (the wave front is in A), formula_3 is the arrival time at B. The optical path length of a ray from a point A to a point B is defined by:\n\nand it is related to the travel time by \"S\" = \"cT\". The optical path length is a purely geometrical quantity since time is not considered in its calculation. An extremum in the light travel time between two points A and B is equivalent to an extremum of the optical path length between those two points. The historical form proposed by Fermat is incomplete. A complete modern statement of the variational Fermat principle is that In the context of calculus of variations this can be written as\n\nIn general, the refractive index is a scalar field of position in space, that is, formula_6 in 3D euclidean space. Assuming now that light has a component that travels along the \"x\" axis, the path of a light ray may be parametrized as formula_7 and\n\nwhere formula_9. The principle of Fermat can now be written as\n\nwhich has the same form as Hamilton's principle but in which \"x\" takes the role of time in classical mechanics. Function formula_12 is the optical Lagrangian from which the Lagrangian and Hamiltonian (as in Hamiltonian mechanics) formulations of geometrical optics may be derived.\n\nClassically, Fermat's principle can be considered as a mathematical consequence of Huygens' principle. Indeed, of all secondary waves (along all possible paths) the waves with the extremal (stationary) paths contribute most due to constructive interference. Suppose that light waves propagate from A to B by all possible routes AB, unrestricted initially by rules of geometrical or physical optics. The various optical paths AB will vary by amounts greatly in excess of one wavelength, and so the waves arriving at B will have a large range of phases and will tend to interfere destructively. But if there is a shortest route AB, and the optical path varies smoothly through it, then a considerable number of neighboring routes close to AB will have optical paths differing from AB by second-order amounts only and will therefore interfere constructively. Waves along and close to this shortest route will thus dominate and AB will be the route along which the light is seen to travel.\n\nFermat's principle is the main principle of quantum electrodynamics which states that any particle (e.g. a photon or an electron) propagates over all available, unobstructed paths and that the interference, or superposition, of its wavefunction over all those paths at the point of observation gives the probability of detecting the particle at this point. Thus, because the extremal paths (shortest, longest, or stationary) cannot be completely canceled out, they contribute most to this interference. In humans, for example, Fermat's principle can be demonstrated in a situation when a lifeguard has to find the fastest way to traverse both beach and water in order to reach a drowning swimmer. The principle has been tested in studies with ants, in which the ants' nest is on one end of a container and food is on the opposite end, but the ants choose to follow the path of least time, rather than the most direct path.\n\nIn the classic mechanics of waves, Fermat's principle follows from the extremum principle of mechanics (see variational principle).\n\nEuclid, c. 320 BCE in his Catoptrics (on mirrors, including spherical mirrors) and Optics, laid the foundations for reflection, which was repeated by Ptolemy, and then in his more detailed books that have surfaced, Hero of Alexandria (Heron) (c. 60) described the principle of reflection, which stated that a ray of light that goes from point A to point B, suffering any number of reflections on flat mirrors in the same medium, has a smaller path length than any nearby path.\n\nIbn al-Haytham (Alhacen), in his \"Book of Optics\" (1021), expanded the principle to both reflection and refraction, and expressed an early version of the principle of least time. His experiments were based on earlier works on refraction carried out by the Greek scientist Ptolemy.\n\nThe generalized principle of least time in its modern form was stated by Fermat in a letter dated January 1, 1662, to Cureau de la Chambre. It was met with objections by Claude Clerselier in May 1662, an expert in optics and leading spokesman for the Cartesians at the time. Amongst his objections, Clerselier states:\n\"... The principle which you take as the basis for your proof, namely that Nature always acts by using the simplest and shortest paths, is merely a moral, and not a physical one. It is not, and cannot be, the cause of any effect in Nature.\n\nThe original French, from Mahoney, is as follows:\n\"Le principe que vous prenez pour fondement de votre démonstration, à savoir que la nature agit toujours par les voies les plus courtes et les plus simples, n’est qu’un principe moral et non point physique, qui n’est point et qui ne peut être la cause d’aucun effet de la nature.\"\nAlthough Fermat's principle does not hold standing alone, we now know it can be derived from earlier principles such as Huygens' principle.\n\nHistorically, Fermat's principle has served as a guiding principle in the formulation of physical laws with the use of variational calculus (see Principle of least action).\n\n"}
{"id": "928779", "url": "https://en.wikipedia.org/wiki?curid=928779", "title": "First principle", "text": "First principle\n\nA first principle is a basic, foundational, self-evident proposition or assumption that cannot be deduced from any other proposition or assumption. In philosophy, first principles are taught by Aristotelians, and nuanced versions of first principles are referred to as postulates by Kantians. In mathematics, first principles are referred to as axioms or postulates. In physics and other sciences, theoretical work is said to be from first principles, or \"ab initio\", if it starts directly at the level of established science and does not make assumptions such as empirical model and parameter fitting.\n\nIn a formal logical system, that is, a set of propositions that are consistent with one another, it is possible that some of the statements can be deduced from other statements. For example, in the syllogism, \"All men are mortal; Socrates is a man; Socrates is mortal\" the last claim can be deduced from the first two.\n\nA first principle is an axiom that cannot be deduced from any other within that system. The classic example is that of Euclid's Elements; its hundreds of geometric propositions can be deduced from a set of definitions, postulates, and common notions: all three types constitute first principles.\n\nIn philosophy \"first principles\" are also commonly referred to as \"a priori\" terms and arguments, which are contrasted to \"a posteriori\" terms, reasoning or arguments, in that the former are simply assumed and exist prior to the reasoning process and the latter are deduced or inferred after the initial reasoning process. First principles are generally treated in the realm of philosophy known as epistemology, but are an important factor in any metaphysical speculation. \n\nIn philosophy \"first principles\" are often somewhat synonymous with \"a priori\", datum and axiomatic reasoning.\n\nTerence Irwin writes:\nProfoundly influenced by Euclid, Descartes was a rationalist who invented the foundationalist system of philosophy. He used the \"method of doubt\", now called Cartesian doubt, to systematically doubt everything he could possibly doubt, until he was left with what he saw as purely indubitable truths. Using these self-evident propositions as his axioms, or foundations, he went on to deduce his entire body of knowledge from them. The foundations are also called \"a priori\" truths. His most famous proposition is \"Je pense, donc je suis.\" (\"I think, therefore I am\", or \"Cogito ergo sum\")\n\nDescartes describes the concept of a first principle in the following excerpt from the preface to the \"Principles of Philosophy\" (1644):\nIn physics, a calculation is said to be \"from first principles\", or \"ab initio\", if it starts directly at the level of established laws of physics and does not make assumptions such as empirical model and fitting parameters.\n\nFor example, calculation of electronic structure using Schrödinger's equation within a set of approximations that do not include fitting the model to experimental data is an \"ab initio\" approach.\n\n\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "11273068", "url": "https://en.wikipedia.org/wiki?curid=11273068", "title": "Growing block universe", "text": "Growing block universe\n\nAccording to the growing block universe theory of time (or the growing block view), the past and present exist and the future does not exist. The present is an objective property, to be compared with a moving spotlight. By the passage of time more of the world comes into being; therefore, the block universe is said to be growing. The growth of the block is supposed to happen in the present, a very thin slice of spacetime, where more of spacetime is continually coming into being.\n\nThe growing block view is an alternative to both eternalism (according to which past, present, and future all exist) and presentism (according to which only the present exists). It is held to be closer to common-sense intuitions than the alternatives. C. D. Broad was a proponent of the theory (1923). Some modern defenders are Michael Tooley (in 1997) and Peter Forrest (in 2004).\n\nRecently several philosophers, David Braddon-Mitchell (2004), Craig Bourne and Trenton Merricks have noted that if the growing block view is correct then we have to conclude that we do not know whether now is now. (The first occurrence of \"now\" is an indexical and the second occurrence of \"now\" is the objective tensed property. Their observation implies the following sentence: \"This part of spacetime has the property of being present\".)\n\nTake Socrates discussing, in the past, with Gorgias, and at the same time thinking that the discussion is occurring now. According to the growing block view, tense is a real property of the world so his thought is about now, the objective present. He thinks, tenselessly, that his thought is occurring on the edge of being. But we know he is wrong because he is in the past; he does not know that now is now. But how can we be sure we are not in the same position? There is nothing special with Socrates. Therefore, we do not know whether now is now.\n\nHowever, some have argued that there is an ontological distinction between the past and the present. For instance, Forrest (2004) argues that although there exists a past, it is lifeless and inactive. Consciousness, as well as the flow of time, is not active within the past and can only occur at the boundary of the block universe in which the present exists.\n\n\n"}
{"id": "386398", "url": "https://en.wikipedia.org/wiki?curid=386398", "title": "Hypothetico-deductive model", "text": "Hypothetico-deductive model\n\nThe hypothetico-deductive model or method is a proposed description of scientific method. According to it, scientific inquiry proceeds by formulating a hypothesis in a form that can be falsifiable, using a test on observable data where the outcome is not yet known. A test outcome that could have and does run contrary to predictions of the hypothesis is taken as a falsification of the hypothesis. A test outcome that could have, but does not run contrary to the hypothesis corroborates the theory. It is then proposed to compare the explanatory value of competing hypotheses by testing how stringently they are corroborated by their predictions.\n\nOne example of an algorithmic statement of the hypothetico-deductive method is as follows:\n\nOne possible sequence in this model would be 1, 2, 3, 4. If the outcome of 4 holds, and 3 is not yet disproven, you may continue with 3, 4, 1, and so forth; but if the outcome of 4 shows 3 to be false, you will have to go back to 2 and try to invent a new 2, deduce a new 3, look for 4, and so forth.\n\nNote that this method can never absolutely verify (prove the truth of) 2. It can only falsify 2. (This is what Einstein meant when he said, \"No amount of experimentation can ever prove me right; a single experiment can prove me wrong.\")\n\nAdditionally, as pointed out by Carl Hempel (1905–1997), this simple view of the scientific method is incomplete; a conjecture can also incorporate probabilities, e.g., the drug is effective about 70% of the time. Tests, in this case, must be repeated to substantiate the conjecture (in particular, the probabilities). In this and other cases, we can quantify a probability for our confidence in the conjecture itself and then apply a Bayesian analysis, with each experimental result shifting the probability either up or down. Bayes' theorem shows that the probability will never reach exactly 0 or 100% (no absolute certainty in either direction), but it can still get very close to either extreme. See also confirmation holism.\n\nQualification of corroborating evidence is sometimes raised as philosophically problematic. The raven paradox is a famous example. The hypothesis that 'all ravens are black' would appear to be corroborated by observations of only black ravens. However, 'all ravens are black' is logically equivalent to 'all non-black things are non-ravens' (this is the contraposition form of the original implication). 'This is a green tree' is an observation of a non-black thing that is a non-raven and therefore corroborates 'all non-black things are non-ravens'. It appears to follow that the observation 'this is a green tree' is corroborating evidence for the hypothesis 'all ravens are black'. Attempted resolutions may distinguish:\n\nEvidence contrary to a hypothesis is itself philosophically problematic. Such evidence is called a falsification of the hypothesis. However, under the theory of confirmation holism it is always possible to save a given hypothesis from falsification. This is so because any falsifying observation is embedded in a theoretical background, which can be modified in order to save the hypothesis. Popper acknowledged this but maintained that a critical approach respecting methodological rules that avoided such \"immunizing stratagems\" is conducive to the progress of science.\n\nPhysicist Sean Carroll claims the model ignores underdetermination.\nThe hypothetico-deductive model (or approach) versus other research models\n\nThe hypothetico-deductive approach contrasts with other research models such as the inductive approach or grounded theory. In the data percolation methodology, \nthe hypothetico-deductive approach is included in a paradigm of pragmatism by which four types of relations between the variables can exist: descriptive, of influence, longitudinal or causal. The variables are classified in two groups, structural and functional, a classification that drives the formulation of hypotheses and the statistical tests to be performed on the data so as to increase the efficiency of the research. \n\n"}
{"id": "161999", "url": "https://en.wikipedia.org/wiki?curid=161999", "title": "Idea", "text": "Idea\n\nIn philosophy, ideas are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the \"idea\" of a person or a place. A new or original idea can often lead to innovation.\n\nThe word \"idea\" comes from Greek ἰδέα \"idea\" \"form, pattern,\" from the root of ἰδεῖν \"idein\", \"to see.\" \n\nOne view on the nature of ideas is that there exist some ideas (called \"innate ideas\") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from \"adventitious ideas\" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.\n\nAnother view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as \"tabula rasa\" (\"blank slate\"). Most of the confusions in the way ideas arise is at least in part due to the use of the term \"idea\" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, \"concrete ideas versus abstract ideas\", as well as \"simple ideas versus complex ideas\".\n\nPlato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (it must be noted that in Plato's Greek the word \"idea\" carries a rather different sense from our modern English term). Plato argued in dialogues such as the \"Phaedo\", \"Symposium\", \"Republic\", and \"Timaeus\" that there is a realm of ideas or forms (\"eidei\"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the \"Republic\":\nDescartes often wrote of the meaning of \"idea\" as an image or representation, often but not necessarily \"in the mind\", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his \"Meditations on First Philosophy\" he says, \"Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs.\" He sometimes maintained that ideas were innate and uses of the term \"idea\" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides \"ideas\" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.\n\nIn striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines \"idea\" as \"that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it.\" He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in \"good sense\" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas \"good-tempered, moderate, and down-to-earth.\"\n\nAs John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.\n\nIn a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.\n\nHume differs from Locke by limiting \"idea\" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an \"impression.\" Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that \"reason alone is merely the 'slave of the passions'.\" \n\nImmanuel Kant defines an \"idea\" as opposed to a \"concept\". \"Regulative ideas\" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgements of good common sense.\n\nWhereas Kant declares limits to knowledge (\"we can never know the thing in itself\"), in his epistemological work, Rudolf Steiner sees \"ideas\" as \"objects of experience\" which the mind apprehends, much as the eye apprehends light. In \"Goethean Science\" (1883), he declares, \"Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas.\" He holds this to be the premise upon which Goethe made his natural-scientific observations.\n\nWundt widens the term from Kant's usage to include \"conscious representation of some object or process of the external world\". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as \"exact methods\", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other \"objectively valuable aids\", specifically to \"those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom.\" Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of \"objective\" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.\n\nC. S. Peirce published the first full statement of pragmatism in his important works \"\" (1878) and \"\" (1877). In \"How to Make Our Ideas Clear\" he proposed that a \"clear idea\" (in his study he uses concept and \"idea\" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as \"participants\", not as \"spectators\". He felt \"the real\", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to \"ideas\".\n\nG. F. Stout and J. M. Baldwin, in the \"Dictionary of Philosophy and Psychology\", define \"idea\" as \"the reproduction with a more or less adequate image, of an object not actually present to the senses.\" They point out that an idea and a perception are by various authorities contrasted in various ways. \"Difference in degree of intensity\", \"comparative absence of bodily movement on the part of the subject\", \"comparative dependence on mental activity\", are suggested by psychologists as characteristic of an idea as compared with a perception.\n\nIt should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say \"This is a chair, that is a stool\", he has what is known as an \"abstract idea\" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.\n\nDiffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.\n\nIn the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book \"The Selfish Gene\", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term \"meme\" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.\n\nJames Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.\n\nTo protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.\n\nIn some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of \"copyright\". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).\n\nA copyright is meant to regulate some aspects of the usage of expressions of a work, \"not\" an idea. Thus, copyrights have a negative relationship to ideas.\n\nWork means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. \nConfidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.\n\n\n"}
{"id": "23276654", "url": "https://en.wikipedia.org/wiki?curid=23276654", "title": "Infinite qualitative distinction", "text": "Infinite qualitative distinction\n\nThe infinite qualitative distinction (; ), sometimes translated as infinite qualitative difference, is a concept coined by the Danish philosopher Søren Kierkegaard. The distinction emphasizes the very different attributes of finite and temporal men and the infinite and eternal qualities of a supreme being. This concept fits into the apophatic theology tradition and therefore is fundamentally at odds with theological theories which posit a supreme being able to be fully understood by man. The theologian Karl Barth made the concept of infinite qualitative distinction a cornerstone of his theology.\n\nFor Kierkegaard, direct communication with God is impossible, as the idea of God and man are infinitely different. He argues that indirect communication with God is the only way of communication. For example, in Christian belief, the Incarnation posits that Jesus Christ is God incarnate. The infinite qualitative distinction is opposed to rational theology in the sense that, whereas the latter argues one can prove empirically Jesus is God incarnate, the former argues that empirical evidence is ultimately insufficient in making that conclusion. The paradoxical nature of the Incarnation, that God is embodied in a man, is offensive to reason, and can only be comprehended indirectly, through faith.\n\nBarth's book \"The Epistle to the Romans\" also emphasizes such a gulf. In the preface to the Second Edition of his commentary, Barth writes, \"if I have a system, it is limited to a recognition of what Kierkegaard called the 'infinite qualitative distinction' between time and eternity, and to my regarding this as possessing negative as well as positive significance: 'God is in heaven, and thou art on earth'. The relation between such a God and such a man, and the relation between such a man and such a God, is for me the theme of the Bible and the essence of philosophy.\"\n\nKierkegaard doesn't believe God is so objective toward human beings but rather that he is the absolute subjective being. He put it this way in 1846: \n\n\n\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "8599305", "url": "https://en.wikipedia.org/wiki?curid=8599305", "title": "Logic model", "text": "Logic model\n\nLogic models are hypothesized descriptions of the chain of causes and effects (see Causality) leading to an outcome of interest (e.g. prevalence of cardiovascular diseases, annual traffic collision, etc). While they can be in a narrative form, logic model usually take form in a graphical depiction of the \"if-then\" (causal) relationships between the various elements leading to the outcome. However, the logic model is more than the graphical depiction: it is also the theories, scientific evidences, assumptions and beliefs that support it and the various processes behind it.\n\nLogic models are used by planners, funders, managers and evaluators of programs and interventions to plan, communicate, implement and evaluate them. They are being employed as well by health scientific community to organize and conduct literature reviews such as systematic reviews. Domains of application are various, e.g. waste management, poultry inspection, business education, heart disease and stroke prevention. Since they are used in various contexts and for different purposes, their typical components and levels of complexity varies in literature (compare for example the W.K. Kellogg Foundation presentation of logic model, mainly aimed for evaluation, and the numerous types of logic models in the Intervention Mapping framework)). In addition, depending of the purpose of the logic model, elements depicted and the relationships between them is more or less detailed.\n\nCiting Funnell and Rogers account, Joy A. Frechtling (2015) encyclopedic article traces logic model underpinnings in the 1950s. Patricia J. Rogers (2005) encyclopedic article rather trace it back to 1967 Edward A. Suchman book about evaluative research. Both encyclopedic article and LeCroy one (2018) mention an increasing interest, usage and publications about the subject.\n\nOne of the most important uses of the logic model is for program planning. It is suggested to use the logic model to focus on the intended outcomes of a particular program. The guiding questions change from \"what is being done?\" to \"what needs to be done\"? McCawley suggests that by using this new reasoning, a logic model for a program can be built by asking the following questions in sequence:\n\n\nBy placing the focus on ultimate outcomes or results, planners can \"think backward\" through the logic model to identify how best to achieve the desired results. Here it helps managers to 'plan with the end in mind', rather than just consider inputs (e.g. budgets, employees) or the tasks that must be done.\n\nThe logic model is often used in government or not-for-profit organizations, where the mission and vision are not aimed at achieving a financial benefit. Traditionally, government programs were described only in terms of their budgets. It is easy to measure the amount of money spent on a program, but this is a poor indicator of outcomes. Likewise it is relatively easy to measure the amount of work done (e.g. number of workers or number of years spent), but the workers may have just been 'spinning their wheels' without getting very far in terms of ultimate results or outcomes.\n\nHowever, nature of outcomes varies. To measure the progress toward outcomes, some initiatives may require an ad hoc measurement instrument. In addition, in programs such as in education or social programs, outcomes are usually in the long-term and may requires numerous intermediate changes (attitudes, social norm, industry practices, etc.) to advance progressively toward the outcomes.\n\nBy making clear the intended outcomes and the causal pathways leading to them, a program logic model provides the basis upon which planners and evaluators can develop a measurement plan and adequate instruments. Instead of only looking at the outcome progress, planners can open the \"black box\" and examine if the intermediate outcomes progress as planned. In addition, the pathways of numerous outcomes are still largely misunderstood due their complexity, their unpredictability and lack of scientific / practical evidences. Therefore, with proper research design, one may not only assess the progress of intermediate outcomes, but evaluate as well if the program theory of change is accurate, i.e. is successful change of an intermediate outcomes provokes the hypothesized subsequent effects in the causal pathway. Finally, outcomes may easily be achieved through processes independent of the program and an evaluation of those outcomes would suggest program success when in fact external outputs were responsible for the outcomes.\n\nMany authors and guides use the following template when speaking about logic model:\n\nMany refinements and variations have been added to the basic template. For example, many versions of logic models set out a series of outcomes/impacts, explaining in more detail the logic of how an intervention contributes to intended or observed results. Others often distinguish short-term, medium-term and long-term results, and between direct and indirect results.\n\nBartholomew et al. Intervention Mapping approach makes an extensive use of logic model through the whole life-cycle of a health promotion program. Since this method can start from as far as a vague desired outcomes (authors example is a city whose actors decide to address \"health issues\" of the city), planners goes through various steps in order to develop effective interventions and properly evaluate them (see Intervention Mapping entry for a more detailed account). Distinguishable but closely interweave logic models with different purposes are being developed through the process:\n\n\nEvaluators thereafter use the logic model of the intervention to design a proper evaluation plan to assess implementation, impact and efficiency.\n\nBy describing work in this way, managers have an easier way to define the work and measure it. Performance measures can be drawn from any of the steps. One of the key insights of the logic model is the importance of measuring final outcomes or results, because it is quite possible to waste time and money (inputs), \"spin the wheels\" on work activities, or produce outputs without achieving desired outcomes. It is these outcomes (impacts, long-term results) that are the only justification for doing the work in the first place. For commercial organizations, outcomes relate to profit. For not-for-profit or governmental organizations, outcomes relate to successful achievement of mission or program goals.\n\nThere are some potential disadvantages of logic models due to tendencies toward oversimplification. These include:\n\n"}
{"id": "1047584", "url": "https://en.wikipedia.org/wiki?curid=1047584", "title": "Man bites dog (journalism)", "text": "Man bites dog (journalism)\n\nThe phrase man bites dog is a shortened version of an aphorism in journalism which describes how an unusual, infrequent event (such as a man biting a dog) is more likely to be reported as news than an ordinary, everyday occurrence with similar consequences, such as a dog biting a man. An event is usually considered more newsworthy if there is something unusual about it; a commonplace event is less likely to be seen as newsworthy, even if the consequences of both events have objectively similar outcomes. The result is that rarer events more often appear as news stories, while more common events appear less often, thus distorting the perceptions of news consumers of what constitutes normal rates of occurrence.\n\nThe phenomenon is also described in the journalistic saying, \"You never read about a plane that did not crash\".\n\nThe phrase was coined by Alfred Harmsworth, 1st Viscount Northcliffe (1865–1922), a British newspaper magnate, but is also attributed to \"New York Sun\" editor John B. Bogart (1848–1921): \"When a dog bites a man, that is not news, because it happens so often. But if a man bites a dog, that is news.\" The quote is also attributed to Charles Anderson Dana (1819–1897).\n\nSome consider it a principle of yellow journalism.\n\nIn 2000, the \"Santa Cruz Sentinel\" ran a story titled \"Man bites dog\" about a San Francisco man who bit his own dog.\n\nReuters ran a story, \"It's News! Man Bites Dog\", about a man biting a dog in December 2007.\n\nA 2008 story of a boy biting a dog in Brazil had news outlets quoting the phrase.\n\nIn 2010, NBC Connecticut ran a story about a man who bit a police dog, prefacing it with, \"It's often said, if a dog bites a man it's not news, but if a man bites a dog, you've got a story. Well, here is that story.\"\n\nOn May 14, 2012, the \"Medway Messenger\", a British local newspaper, ran a front page story headlined \"MAN BITES DOG\" about a man who survived a vicious attack from a Staffordshire bull terrier by biting the dog back.\n\nOn September 27, 2012, the \"Toronto Star\", a Canadian newspaper, ran the story headlined \"Nearly Naked Man Bites Dog\", about a man that is alleged to have bitten a dog in Pembroke, Ontario.\n\nOn December 2, 2012, \"Sydney Morning Herald\" reported about a man that bit the dog and its unfortunate consequence; 'Man bites Dog, goes to hospital' \n\nOn May 5, 2013, \"Nine News\", an Australian news outlet, ran a story headlined \"Man bites dog to save wife\" about a man who bit a Labrador on the nose, after it attacked his wife and bit off her nose.\n\nOn March 12, 2014, Rosbalt, a Russian news agency, reported that a man in Lipetsk had burnt a bed in his apartment, run around the city in his underwear, and, finally, \"bit a fighting breed dog\" following an hours-long online debate about the situation in Ukraine.\n\nIn April 2014, CNN reported a mom bit a pit bull attacking her daughter.\n\nOn June 14, 2014, the \"South Wales Argus\" ran a front page teaser headlined \"Man Bites Dog\" about a man who has been accused of assaulting his partner and her pet dog. The Online version of this story was later amended to \"Man bites dog and escapes jail\".\n\nOn September 1, 2014 the \"Coventry Telegraph\" and the \"Daily Mirror\" ran an article about a man who had bitten a dog after it attacked his pet.\n\nOn December 17, 2014 the \"Cambridge News\" ran an article with a headline starting: \"Man bites dog then dies\".\n\nOn November 4, 2015 the \"Washington Post\" ran an article with the title \"Man bites dog. No, really.\"\n\nOn April 10, 2018 the \"Daily Telegraph\" ran such an article about a man biting a dog to defend his own dog.\n\nOn May 4, 2018, the \"Salt Lake Tribune\" ran an article about a man biting a police dog while being taken into custody.\n\nIn Terry Pratchett's novel \"The Truth\", protagonist and newspaper editor William DeWorde uncovers a plot against the ruler of the city by interviewing the sole witness, a dog, via an interpreter. DeWorde's resulting story is headlined \"Dog Bites Man\", and he notes with some amusement that he was able to make the phrase news-worthy.\n\nThere have also been a number of \"dog shoots man\" news stories.\n\nAs an example of a related phrase, a story titled \"Deer Shoots Hunter\" appeared in a 1947 issue of the Pittsburgh Press, mentioning a hunter that was shot by his own gun due to a reflex kick by the deer he had killed. And in 2005, in Michigan, there was a case of \"cat shoots man\".\n"}
{"id": "570963", "url": "https://en.wikipedia.org/wiki?curid=570963", "title": "Marginal propensity to save", "text": "Marginal propensity to save\n\nThe marginal propensity to save (MPS) is the fraction of an increase in income that is not spent on an increase in consumption. That is, the marginal propensity to save is the proportion of each additional dollar of household income that is used for saving. It is the slope of the line plotting saving against income. For example, if a household earns one extra dollar, and the marginal propensity to save is 0.35, then of that dollar, the household will spend 65 cents and save 35 cents. Likewise, it is the fractional decrease in saving that results from a decrease in income.\n\nThe MPS plays a central role in Keynesian economics as it quantifies the saving-income relation, which is the flip side of the consumption-income relation, and according to Keynes it reflects the fundamental psychological law. The marginal propensity to save is also a key variable in determining the value of the multiplier.\n\nMPS can be calculated as the change in savings divided by the change in income.\n\nOr mathematically, the marginal propensity to save (MPS) function is expressed as the derivative of the savings (S) function with respect to disposable income (Y).\n\nNow, MPS can be calculated as follows:\n\nMPS = (Change in savings) / (Change in income)\n\nThis implies that for each additional one unit of income, the savings increase by 0.4.\n\nThere are different implications of this above-mentioned formula.\n\n\nSince MPS is measured as ratio of change in savings to change in income, its value lies between 0 and 1.\nAlso, marginal propensity to save is opposite of marginal propensity to consume.\n\nMathematically, in a closed economy, MPS + MPC = 1, since an increase in one unit of income will be either consumed or saved.\n\nIn the above example, If MPS = 0.4, then MPC = 1 - 0.4 = 0.6.\n\nGenerally, it is assumed that value of marginal propensity to save for the richer is more than the marginal propensity to save for the poorer. If income increases for both parties by $1, then the propensity to save for a richer person would be more than that for the poorer person.\n\nMarginal propensity to save is also used as an alternative term for slope of saving line.\nThe slope of a saving line is given by the equation S = -a + (1-b)Y, where -a refers to autonomous savings and (1-b) refers to marginal propensity to save (here b refers to marginal propensity to consume but as MPC + MPS = 1, so (1-b) refers to MPS).\n\nIn this diagram, the savings function is an increasing function of disposable income i.e. savings increase as income increases.\n\nAn important implication of marginal propensity to save is measurement of the multiplier. A multiplier measures the magnified change in aggregate product i.e. the gross domestic product, resulting from a change in an autonomous variable (for example, government expenditure, investment expenditures, etc.).\n\nThe effect of a change in production creates a multiplied impact because it creates income which further creates consumption. However, the resulting consumption is also an expenditure which thus, generates more income, which creates more consumption. This next round of consumption leads to a further change in production, which generates even more income, and which induces even more consumption.\n\nAnd thus, as it goes on and on, it results in a magnified, multiplied change in aggregate production initially triggered by a change in autonomous variable, but amplified by the creation of more income and increase in consumption.\n\nMathematically, the above effect can be stated as:\n\nAnd it goes on and on.\nWe can express this as:\n\nThe end result is a magnified, multiplied change in aggregate production initially triggered by the change in investment, but amplified by the change in consumption i.e. the initial investment multiplied by the consumption coefficient (Marginal Propensity to consume).\n\nThe MPS enters into the process because it indicates the division of extra income between consumption and saving. It determines how much saving is induced with each change in production and income, and thus how much consumption is induced. If the MPS is smaller, then the multiplier process is also greater as less saving is induced, but more consumption is induced, with each round of activity.\n\nThus, in this highly simplified model, total magnified change in production due to change in an autonomous variable by $1\n\nThe effect of a multiplier effect can be measured as:\n\nIf the MPS is smaller, then the multiplier process is also greater as less saving is induced, and more consumption is induced with each round of activity.\n\nFor example, if MPS = 0.2, then multiplier effect is 5, and if MPS = 0.4, then the multiplier effect is 2.5. Thus, we can see that a lower propensity to save implies a higher multiplier effect.\n\n\n"}
{"id": "3785733", "url": "https://en.wikipedia.org/wiki?curid=3785733", "title": "Marginal rate of technical substitution", "text": "Marginal rate of technical substitution\n\nIn microeconomic theory, the Marginal Rate of Technical Substitution (MRTS)—or Technical Rate of Substitution (TRS)—is the amount by which the quantity of one input has to be reduced (formula_1) when one extra unit of another input is used (formula_2), so that output remains constant (formula_3).\n\nformula_4\n\nwhere formula_5 and formula_6 are the marginal products of input 1 and input 2, respectively.\n\nAlong an isoquant, the MRTS shows the rate at which one input (e.g. capital or labor) may be substituted for another, while maintaining the same level of output. Thus the MRTS is the absolute value of the slope of an isoquant at the point in question.\n\nWhen relative input usages are optimal, the marginal rate of technical substitution is equal to the relative unit costs of the inputs, and the slope of the isoquant at the chosen point equals the slope of the isocost curve (see Conditional factor demands). It is the rate at which one input is substituted for another to maintain the same level of output.\n\n"}
{"id": "1280458", "url": "https://en.wikipedia.org/wiki?curid=1280458", "title": "Marginal revenue", "text": "Marginal revenue\n\nIn microeconomics, marginal revenue (R') is the additional revenue that will be generated by increasing product sales by one unit. It can also be described as the unit revenue the last item sold has generated for the firm. In a perfectly competitive market, the additional revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good. This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price. However, a monopoly determines the entire industry's sales. As a result, it will have to lower the price of all units sold to increase sales by 1 unit. Therefore, the marginal revenue generated is always lower than the price the firm is able to charge for the unit sold, since each reduction in price causes unit revenue to decline on every good the firm sells. The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. \n\nA firms profits will be maximized when marginal revenue (MR) equals marginal cost (MC). If formula_1 then a firm should increase output for more profits, if formula_2 then a firm should decrease output for additional profits. A firm should choose the output level which is profit maximizing under perfect competition theory formula_3.\n\nMarginal revenue is equal to the ratio of the change in revenue for some change in quantity sold to that change in quantity sold. This can also be represented as a derivative when the change in quantity sold becomes arbitrarily small. More formally, define the revenue function to be the following\n\nBy the product rule, marginal revenue is then given by\n\nFor a firm facing perfect competition, price does not change with quantity sold (formula_6), so marginal revenue is equal to price. For a monopoly, the price decreases with quantity sold (formula_7), so marginal revenue is less than price (for positive formula_8).\n\nThe marginal revenue curve is affected by the same factors as the demand curve - changes in income, change in the prices of complements and substitutes, change in populations. These factors can cause the R curve to shift and rotate.\n\nThe relationship between marginal revenue and the elasticity of demand by the firm's customers can be derived as follows:\n\nwhere e is the price elasticity of demand. If demand is inelastic (e < 1) then R' will be negative, because to sell a marginal (infinitesimal) unit the firm would have to lower the selling price so much that it would lose more revenue on the pre-existing units than it would gain on the incremental unit. If demand is elastic (e > 1) R' will be positive, because the additional unit would not drive down the price by so much. If the firm is a perfect competitor, so that it is so small in the market that its quantity produced and sold has no effect on the price, then the price elasticity of demand is negative infinity, and marginal revenue simply equals the (market-determined) price.\n\nProfit maximization requires that a firm produces where marginal revenue equals marginal costs. Firm managers are unlikely to have complete information concerning their marginal revenue function or their marginal costs. Fortunately, the profit maximization conditions can be expressed in a “more easily applicable form” or rule of thumb.\n\nMarkup is the difference between price and marginal cost. The formula states that markup as a percentage of price equals the negative of the inverse of elasticity of demand. Alternatively, the relationship can be expressed as:\n\nThus if e is - 2 and mc is $5.00 then price is $10.00.\n\n(<R> - C')/ <R> = - 1/e is called the Lerner index after economist Abba Lerner. The Lerner index is a measure of market power - the ability of a firm to charge a price that exceeds marginal cost. The index varies from zero to 1. The greater the difference between price and marginal cost the closer the index value is to 1. The Lerner index increases as demand becomes less elastic.\n\nExample\nIf a company can sell 10 units at $20 each or 11 units at $19 each, then the marginal revenue from the eleventh unit is (11 × 19) - (10 × 20) = $9.\n\n\n"}
{"id": "3108261", "url": "https://en.wikipedia.org/wiki?curid=3108261", "title": "Marginal value", "text": "Marginal value\n\nA marginal value is\n(This third case is actually a special case of the second).\n\nIn the case of differentiability, at the limit, a marginal change is a mathematical differential, or the corresponding mathematical derivative.\n\nThese uses of the term “marginal” are especially common in economics, and result from conceptualizing constraints as \"borders\" or as \"margins\". The sorts of marginal values most common to economic analysis are those associated with \"unit\" changes of resources and, in mainstream economics, those associated with \"infinitesimal\" changes. Marginal values associated with units are considered because many decisions are made by unit, and marginalism explains \"unit price\" in terms of such marginal values. Mainstream economics uses infinitesimal values in much of its analysis for reasons of mathematical tractability.\n\nAssume a functional relationship\n\nIf the value of formula_2 is \"discretely\" changed from formula_3 to formula_4 while other independent variables remain unchanged, then the marginal value of the change in formula_2 is\nand the “marginal value” of formula_7 may refer to\nor to\n\nIf an individual saw her income increase from $50000 to $55000 per annum, and part of her response was to increase yearly purchases of amontillado from 2 casks to three casks, then\n\nIf \"infinitesimal\" values are considered, then a marginal value of formula_2 would be formula_11, and the “marginal value” of formula_7 would typically refer to\n\nAssume that, in some economy, aggregate consumption is well-approximated by\nwhere\nThen the \"marginal propensity to consume\" is\n\n"}
{"id": "221419", "url": "https://en.wikipedia.org/wiki?curid=221419", "title": "Marginalism", "text": "Marginalism\n\nMarginalism is a theory of economics that attempts to explain the discrepancy in the value of goods and services by reference to their secondary, or marginal, utility. The reason why the price of diamonds is higher than that of water, for example, owes to the greater additional satisfaction of the diamonds over the water. Thus, while the water has greater total utility, the diamond has greater marginal utility.\n\nAlthough the central concept of marginalism is that of marginal utility, marginalists, following the lead of Alfred Marshall, drew upon the idea of marginal physical productivity in explanation of cost. The neoclassical tradition that emerged from British marginalism abandoned the concept of utility and gave marginal rates of substitution a more fundamental role in analysis. Marginalism is an integral part of mainstream economic theory.\n\nFor issues of marginality, constraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change.\n\nNeoclassical economics usually assumes that marginal changes are infinitesimals or limits. (Though this assumption makes the analysis less robust, it increases tractability.) One is therefore often told that \"marginal\" is synonymous with \"very small\", though in more general analysis this may not be operationally true (and would not in any case be literally true). Frequently, economic analysis concerns the marginal values associated with a change of one unit of a resource, because decisions are often made in terms of units; marginalism seeks to explain unit prices in terms of such marginal values.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nMarginalism assumes, for any given agent, economic rationality and an ordering of possible states-of-the-world, such that, for any given set of constraints, there is an attainable state which is best in the eyes of that agent. Descriptive marginalism asserts that choice amongst the specific means by which various anticipated specific states-of-the-world (outcomes) might be affected is governed only by the distinctions amongst those specific outcomes; prescriptive marginalism asserts that such choice \"ought\" to be so governed.\n\nOn such assumptions, each increase would be put to the specific, feasible, previously unrealized use of greatest priority, and each decrease would result in abandonment of the use of lowest priority amongst the uses to which the good or service had been put.\n\nThe marginal utility of a good or service is the utility of its marginal use. Under the assumption of economic rationality, it is the utility of its least urgent possible use \"from\" the best feasible combination of actions in which its use is included.\n\nIn 20th century mainstream economics, the term \"utility\" has come to be formally defined as a \"quantification\" capturing preferences by assigning greater quantities to states, goods, services, or applications that are of higher priority. But marginalism and the concept of marginal utility predate the establishment of this convention within economics. The more general conception of utility is that of \"use\" or \"usefulness\", and this conception is at the heart of marginalism; the term \"marginal utility\" arose from translation of the German \"Grenznutzen\", which literally means \"border use\", referring directly to the marginal use, and the more general formulations of marginal utility do not treat quantification as an \"essential\" feature. On the other hand, none of the early marginalists insisted that utility were \"not\" quantified, some indeed treated quantification as an essential feature, and those who did not still used an assumption of quantification for expository purposes. In this context, it is not surprising to find many presentations that fail to recognize a more general approach.\n\nUnder the special case in which usefulness can be quantified, the change in utility of moving from state formula_1 to state formula_2 is\nMoreover, if formula_1 and formula_2 are distinguishable by values of just one variable formula_6 which is itself quantified, then it becomes possible to speak of the ratio of the marginal utility of the change in formula_6 to the size of that change:\n(where “c.p.” indicates that the \"only\" independent variable to change is formula_6).\n\nMainstream neoclassical economics will typically assume that\nis well defined, and use “marginal utility” to refer to a partial derivative\n\nThe \"law\" of diminishing marginal utility (also known as a \"Gossen's First Law\") is that, \"ceteris paribus\", as additional amounts of a good or service are added to available resources, their marginal utilities are decreasing. This \"law\" is sometimes treated as a tautology, sometimes as something proven by introspection, or sometimes as a mere instrumental assumption, adopted only for its perceived predictive efficacy. Actually, it is not quite any of these things, though it may have aspects of each. The \"law\" does not hold under all circumstances, so it is neither a tautology nor otherwise proveable; but it has a basis in prior observation.\n\nAn individual will typically be able to partially order the potential uses of a good or service. If there is scarcity, then a rational agent will satisfy wants of highest possible priority, so that no want is avoidably sacrificed to satisfy a want of \"lower\" priority. In the absence of complementarity across the uses, this will imply that the priority of use of any additional amount will be lower than the priority of the established uses, as in this famous example:\n\nHowever, if there \"is\" a complementarity across uses, then an amount added can bring things past a desired tipping point, or an amount subtracted cause them to fall short. In such cases, the marginal utility of a good or service might actually be \"increasing\".\n\nWithout the presumption that utility is quantified, the \"diminishing\" of utility should not be taken to be itself an arithmetic subtraction. It is the movement from use of higher to lower priority, and may be no more than a purely ordinal change.\n\nWhen quantification of utility is assumed, diminishing marginal utility corresponds to a utility function whose \"slope\" is continually or continuously decreasing. In the latter case, if the function is also smooth, then the “law” may be expressed\nNeoclassical economics usually supplements or supplants discussion of marginal utility with indifference curves, which were originally derived as the level curves of utility functions, or can be produced without presumption of quantification, but are often simply treated as axiomatic. In the absence of complementarity of goods or services, diminishing marginal utility implies convexity of indifference curves (though such convexity would also follow from quasiconcavity of the utility function).\n\nThe \"rate of substitution\" is the \"least favorable\" rate at which an agent is willing to exchange units of one good or service for units of another. The marginal rate of substitution (\"MRS\") is the rate of substitution at the margin – in other words, given some constraint(s).\n\nWhen goods and services are discrete, the least favorable rate at which an agent would trade A for B will usually be different from that at which she would trade B for A:\nBut, when the goods and services are continuously divisible, in the limiting case\nand the marginal rate of substitution is the slope of the indifference curve (multiplied by formula_15).\n\nIf, for example, Lisa will not trade a goat for anything less than two sheep, then her\nAnd if she will not trade a sheep for anything less than two goats, then her\nBut if she would trade one gram of banana for one ounce of ice cream \"and vice versa\", then\n\nWhen indifference curves (which are essentially graphs of instantaneous rates of substitution) and the convexity of those curves are not taken as given, the \"law\" of diminishing marginal utility is invoked to explain diminishing marginal rates of substitution – a willingness to accept fewer units of good or service formula_19 in substitution for formula_20 as one's holdings of formula_19 grow relative to those of formula_20. If an individual has a stock or flow of a good or service whose marginal utility is less than would be that of some other good or service for which he or she could trade, then it is in his or her interest to effect that trade. Of course, as one thing is traded-away and another is acquired, the respective marginal gains or losses from further trades are now changed. On the assumption that the marginal utility of one is diminishing, and the other is not increasing, all else being equal, an individual will demand an increasing ratio of that which is acquired to that which is sacrificed. (One important way in which all else might not be equal is when the use of the one good or service complements that of the other. In such cases, exchange ratios might be constant.) If any trader can better his or her own marginal position by offering an exchange more favorable to other traders with desired goods or services, then he or she will do so.\n\nAt the highest level of generality, a marginal cost is a marginal opportunity cost. In most contexts, however, \"marginal cost\" will refer to marginal \"pecuniary\" cost – that is to say marginal cost measured by forgone money.\n\nA thorough-going marginalism sees marginal cost as increasing under the \"law\" of diminishing marginal utility, because applying resources to one application reduces their availability to other applications. Neoclassical economics tends to disregard this argument, but to see marginal costs as increasing in consequence of diminishing returns.\n\nMarginalism and neoclassical economics typically explain price formation broadly through the interaction of curves or schedules of supply and demand. In any case buyers are modelled as pursuing typically lower quantities, and sellers offering typically higher quantities, as price is increased, with each being willing to trade until the marginal value of what they would trade-away exceeds that of the thing for which they would trade.\n\nDemand curves are explained by marginalism in terms of marginal rates of substitution.\n\nAt any given price, a prospective buyer has some marginal rate of substitution of money for the good or service in question. Given the \"law\" of diminishing marginal utility, or otherwise given convex indifference curves, the rates are such that the willingness to forgo money for the good or service decreases as the buyer would have ever more of the good or service and ever less money. Hence, any given buyer has a demand schedule that generally decreases in response to price (at least until quantity demanded reaches zero). The aggregate quantity demanded by all buyers is, at any given price, just the sum of the quantities demanded by individual buyers, so it too decreases as price increases.\n\nBoth neoclassical economics and thorough-going marginalism could be said to explain supply curves in terms of marginal cost; however, there are marked differences in conceptions of that cost.\n\nMarginalists in the tradition of Marshall and neoclassical economists tend to represent the supply curve for any producer as a curve of marginal pecuniary costs objectively determined by physical processes, with an upward slope determined by diminishing returns.\n\nA more thorough-going marginalism represents the supply curve as a \"complementary demand curve\" – where the demand is \"for\" money and the purchase is made \"with\" a good or service. The shape of that curve is then determined by marginal rates of substitution of money for that good or service.\n\nBy confining themselves to limiting cases in which sellers or buyers are both \"price takers\" – so that demand functions ignore supply functions or \"vice versa\" – Marshallian marginalists and neoclassical economists produced tractable models of \"pure\" or \"perfect\" competition and of various forms of \"imperfect\" competition, which models are usually captured by relatively simple graphs. Other marginalists have sought to present what they thought of as more realistic explanations, but this work has been relatively uninfluential on the mainstream of economic thought.\n\nThe \"law\" of diminishing marginal utility is said to explain the \"paradox of water and diamonds\", most commonly associated with Adam Smith (though recognized by earlier thinkers). Human beings cannot even survive without water, whereas diamonds, in Smith's day, were ornamentation or engraving bits. Yet water had a very small price, and diamonds a very large price. Marginalists explained that it is the \"marginal\" usefulness of any given quantity that matters, rather than the usefulness of a \"class\" or of a \"totality\". For most people, water was sufficiently abundant that the loss or gain of a gallon would withdraw or add only some very minor use if any, whereas diamonds were in much more restricted supply, so that the loss or gain was much greater.\n\nThat is not to say that the price of any good or service is simply a function of the marginal utility that it has for any one individual nor for some ostensibly typical individual. Rather, individuals are willing to trade based upon the respective marginal utilities of the goods that they have or desire (with these marginal utilities being distinct for each potential trader), and prices thus develop constrained by these marginal utilities.\n\nPerhaps the essence of a notion of diminishing marginal utility can be found in Aristotle's \"Politics\", wherein he writes \n\nA great variety of economists concluded that there was \"some\" sort of inter-relationship between utility and rarity that effected economic decisions, and in turn informed the determination of prices.\n\nEighteenth-century Italian mercantilists, such as Antonio Genovesi, Giammaria Ortes, Pietro Verri, Cesare Beccaria, and Giovanni Rinaldo, held that value was explained in terms of the general utility and of scarcity, though they did not typically work-out a theory of how these interacted. In \"Della Moneta\" (1751), Abbé Ferdinando Galiani, a pupil of Genovesi, attempted to explain value as a ratio of two ratios, \"utility\" and \"scarcity\", with the latter component ratio being the ratio of quantity to use.\n\nAnne Robert Jacques Turgot, in \"Réflexions sur la formation et la distribution de richesse\" (1769), held that value derived from the general utility of the class to which a good belonged, from comparison of present and future wants, and from anticipated difficulties in procurement.\n\nLike the Italian mercantilists, Étienne Bonnot de Condillac saw value as determined by utility associated with the class to which the good belongs, and by estimated scarcity. In \"De commerce et le gouvernement\" (1776), Condillac emphasized that value is not based upon cost but that costs were paid because of value.\n\nThis last point was famously restated by the Nineteenth Century proto-marginalist, Richard Whately, who in \"Introductory Lectures on Political Economy\" (1832) wrote (Whately's student Nassau William Senior is noted below as an early marginalist.)\n\nFrédéric Bastiat in chapters V and XI of his \"Economic Harmonies\" (1850) also develops a theory of value as ratio between services that increment utility, rather than between total utility.\n\nThe first unambiguous published statement of any sort of theory of marginal utility was by Daniel Bernoulli, in \"Specimen theoriae novae de mensura sortis\". This paper appeared in 1738, but a draft had been written in 1731 or in 1732. In 1728, Gabriel Cramer produced fundamentally the same theory in a private letter. Each had sought to resolve the St. Petersburg paradox, and had concluded that the marginal desirability of money decreased as it was accumulated, more specifically such that the desirability of a sum were the natural logarithm (Bernoulli) or square root (Cramer) thereof. However, the more general implications of this hypothesis were not explicated, and the work fell into obscurity.\n\nIn \"A Lecture on the Notion of Value as Distinguished Not Only from Utility, but also from Value in Exchange\", delivered in 1833 and included in \"Lectures on Population, Value, Poor Laws and Rent\" (1837), William Forster Lloyd explicitly offered a general marginal utility theory, but did not offer its derivation nor elaborate its implications. The importance of his statement seems to have been lost on everyone (including Lloyd) until the early 20th century, by which time others had independently developed and popularized the same insight.\n\nIn \"An Outline of the Science of Political Economy\" (1836), Nassau William Senior asserted that marginal utilities were the ultimate determinant of demand, yet apparently did not pursue implications, though some interpret his work as indeed doing just that.\n\nIn \"De la mesure de l'utilité des travaux publics\" (1844), Jules Dupuit applied a conception of marginal utility to the problem of determining bridge tolls.\n\nIn 1854, Hermann Heinrich Gossen published \"Die Entwicklung der Gesetze des menschlichen Verkehrs und der daraus fließenden Regeln für menschliches Handeln\", which presented a marginal utility theory and to a very large extent worked-out its implications for the behavior of a market economy. However, Gossen's work was not well received in the Germany of his time, most copies were destroyed unsold, and he was virtually forgotten until rediscovered after the so-called Marginal Revolution.\n\nMarginalism as a formal theory can be attributed to the work of three economists, Jevons in England, Menger in Austria, and Walras in Switzerland. William Stanley Jevons first proposed the theory in articles in 1863 and 1871. Similarly, Carl Menger presented the theory in 1871. Menger explained why individuals use marginal utility to decide amongst trade-offs, but while his illustrative examples present utility as quantified, his essential assumptions do not.\nLéon Walras introduced the theory in \"Éléments d'économie politique pure\", the first part of which was published in 1874. (American John Bates Clark is also associated with the origins of Marginalism, but did little to advance the theory.\n\nAlthough the Marginal Revolution flowed from the work of Jevons, Menger, and Walras, their work might have failed to enter the mainstream were it not for a second generation of economists. In England, the second generation were exemplified by Philip Wicksteed, by William Smart, and by Alfred Marshall; in Austria by Eugen Böhm von Bawerk and by Friedrich von Wieser; in Switzerland by Vilfredo Pareto; and in America by Herbert Joseph Davenport and by Frank A. Fetter.\n\nThere were significant, distinguishing features amongst the approaches of Jevons, Menger, and Walras, but the second generation did not maintain distinctions along national or linguistic lines. The work of von Wieser was heavily influenced by that of Walras. Wicksteed was heavily influenced by Menger. Fetter referred to himself and Davenport as part of \"the American Psychological School\", named in imitation of the Austrian \"Psychological School\". (And Clark's work from this period onward similarly shows heavy influence by Menger.) William Smart began as a conveyor of Austrian School theory to English-language readers, though he fell increasingly under the influence of Marshall.\n\nBöhm-Bawerk was perhaps the most able expositor of Menger's conception. He was further noted for producing a theory of interest and of profit in equilibrium based upon the interaction of diminishing marginal utility with diminishing marginal productivity of time and with time preference. (This theory was adopted in full and then further developed by Knut Wicksell and, with modifications including formal disregard for time-preference, by Wicksell's American rival Irving Fisher.)\n\nMarshall was the second-generation marginalist whose work on marginal utility came most to inform the mainstream of neoclassical economics, especially by way of his \"Principles of Economics\", the first volume of which was published in 1890. Marshall constructed the demand curve with the aid of assumptions that utility was quantified, and that the marginal utility of money was constant (or nearly so). Like Jevons, Marshall did not see an explanation for supply in the theory of marginal utility, so he paired a marginal explanation of demand with a more classical explanation of supply, wherein costs were taken to be objectively determined. (Marshall later actively mischaracterized the criticism that these costs were themselves ultimately determined by marginal utilities.)\n\nThe doctrines of marginalism and the Marginal Revolution are often interpreted as a response to the rise of the worker's movement, Marxian economics and the earlier (Ricardian) socialist theories of the exploitation of labour. The first volume of \"Das Kapital\" was not published until July 1867, when marginalism was already developing, but before the advent of Marxian economics, proto-marginalist ideas such as those of Gossen had largely fallen on deaf ears. It was only in the 1880s, when Marxism had come to the fore as the main economic theory of the workers' movement, that Gossen found (posthumous) recognition.\n\nAside from the rise of Marxism, E. Screpanti and S. Zamagni point to a different 'external' reason for marginalism's success, which is its successful response to the Long Depression and the resurgence of class conflict in all developed capitalist economies after the 1848-1870 period of social peace. Marginalism, Screpanti and Zamagni argue, offered a theory of the free market as perfect, as performing optimal allocation of resources, while it allowed economists to blame any adverse effects of laissez-faire economics on the interference of workers' coalitions in the proper functioning of the market.\n\nScholars have suggested that the success of the generation who followed the preceptors of the Revolution was their ability to formulate straightforward responses to Marxist economic theory. The most famous of these was that of Böhm-Bawerk, “Zum Abschluss des Marxschen Systems” (1896), but the first was Wicksteed's “The Marxian Theory of Value. \"Das Kapital\": a criticism” (1884, followed by “The Jevonian criticism of Marx: a rejoinder” in 1885). The most famous early Marxist responses were Rudolf Hilferding's \"Böhm-Bawerks Marx-Kritik\" (1904) and \"The Economic Theory of the Leisure Class\" (1914) by Nikolai Bukharin.\n\nIn his 1881 work \"Mathematical Psychics\", Francis Ysidro Edgeworth presented the indifference curve, deriving its properties from marginalist theory which assumed utility to be a differentiable function of quantified goods and services. But it came to be seen that indifference curves could be considered as somehow \"given\", without bothering with notions of utility.\n\nIn 1915, Eugen Slutsky derived a theory of consumer choice solely from properties of indifference curves. Because of the World War, the Bolshevik Revolution, and his own subsequent loss of interest, Slutsky's work drew almost no notice, but similar work in 1934 by John Hicks and R. G. D. Allen derived much the same results and found a significant audience. (Allen subsequently drew attention to Slutsky's earlier accomplishment.)\n\nAlthough some of the third generation of Austrian School economists had by 1911 rejected the quantification of utility while continuing to think in terms of marginal utility, most economists presumed that utility must be a sort of quantity. Indifference curve analysis seemed to represent a way of dispensing with presumptions of quantification, albeït that a seemingly arbitrary assumption (admitted by Hicks to be a \"rabbit out of a hat\") about decreasing marginal rates of substitution would then have to be introduced to have convexity of indifference curves.\n\nFor those who accepted that superseded marginal utility analysis had been superseded by indifference curve analysis, the former became at best somewhat analogous to the Bohr model of the atom—perhaps pedagogically useful, but “old fashioned” and ultimately incorrect.\n\nWhen Cramer and Bernoulli introduced the notion of diminishing marginal utility, it had been to address a paradox of gambling, rather than the paradox of value. The marginalists of the revolution, however, had been formally concerned with problems in which there was neither risk nor uncertainty. So too with the indifference curve analysis of Slutsky, Hicks, and Allen.\n\nThe expected utility hypothesis of Bernoulli \"et alii\" was revived by various 20th century thinkers, including Frank Ramsey (1926), John von Neumann and Oskar Morgenstern (1944), and Leonard Savage (1954). Although this hypothesis remains controversial, it brings not merely utility but a quantified conception thereof back into the mainstream of economic thought, and would dispatch the Ockhamistic argument. (It should perhaps be noted that, in expected utility analysis, the “law” of diminishing marginal utility corresponds to what is called “risk aversion”.)\n\nKarl Marx died before marginalism became the interpretation of economic value accepted by mainstream economics. His theory was based on the labor theory of value, which distinguishes between exchange value and use value. In his \"Capital\" he rejected the explanation of long-term market values by supply and demand:\n\nIn his early response to marginalism, Nikolai Bukharin argued that \"the subjective evaluation from which price is to be derived really starts from this price\", concluding:\n\nSimilarly a later Marxist critic, Ernest Mandel, argued that marginalism was \"divorced from reality\", ignored the role of production, and that:\n\nMaurice Dobb argued that prices derived through marginalism depend on the distribution of income. The ability of consumers to express their preferences is dependent on their spending power. As the theory asserts that prices arise in the act of exchange, Dobb argues that it cannot explain how the distribution of income affects prices and consequently cannot explain prices.\n\nDobb also criticized the \"motives\" behind marginal utility theory. Jevons wrote, for example, \"so far as is consistent with the inequality of wealth in every community, all commodities are distributed by exchange so as to produce the maximum social benefit.\" (See Fundamental theorems of welfare economics.) Dobb contended that this statement indicated that marginalism is intended to insulate market economics from criticism by making prices the natural result of the given income distribution.\n\nSome economists strongly influenced by the Marxian tradition such as Oskar Lange, Włodzimierz Brus, and Michał Kalecki have attempted to integrate the insights of classical political economy, marginalism, and neoclassical economics. They believed that Marx lacked a sophisticated theory of prices, and neoclassical economics lacked a theory of the social frameworks of economic activity. Some other Marxists have also argued that on one level there is no conflict between marginalism and Marxism: one could employ a marginalist theory of supply and demand within the context of a “big picture” understanding of the Marxist notion that capitalists exploit surplus labor.\n\n\n"}
{"id": "20590", "url": "https://en.wikipedia.org/wiki?curid=20590", "title": "Mathematical model", "text": "Mathematical model\n\nA mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in the social sciences (such as economics, psychology, sociology, political science). \n\nA model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.\n\nMathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\n\nIn the physical sciences, a traditional mathematical model contains most of the following elements:\n\nMathematical models are usually composed of relationships and \"variables\". Relationships can be described by \"operators\", such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:\n\nMathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.\n\nThroughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.\n\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schrödinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.\n\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.\n\nSince prehistorical times simple models such as maps and diagrams have been used.\n\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.\n\nA mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\n\nIn business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.\n\nDecision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants.\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\n\nObjectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an \"index of performance\", as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\n\nFor example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.\n\nMathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\n\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\n\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\n\nSometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.\n\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.\n\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification .\n\nFor example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.\n\nAny model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by an artificial neural network or other machine learning, the optimization of parameters is called \"training\", while the optimization of model hyperparameters is called \"tuning\" and often uses cross-validation. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by \"curve fitting\".\n\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.\n\nUsually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.\n\nDefining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.\n\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.\n\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data.\n\nThe question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.\n\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\n\nMany types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\n\nAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.\n\n\"M\" = (\"Q\", Σ, δ, \"q\", \"F\") where\n\nThe state \"S\" represents that there has been an even number of 0s in the input so far, while \"S\" signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, \"M\" will finish in state \"S\", an accepting state, so the input string will be accepted.\n\nThe language recognized by \"M\" is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where \"*\" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols \"1\".\n\n\nthat can be written also as:\n\n\n\n\n\n\n\n"}
{"id": "599917", "url": "https://en.wikipedia.org/wiki?curid=599917", "title": "Mental image", "text": "Mental image\n\nA mental image or mental picture is the representation in a person's mind of the physical world outside that person. It is an experience that, on most occasions, significantly resembles the experience of perceiving some object, event, or scene, but occurs when the relevant object, event, or scene is not actually present to the senses. There are sometimes episodes, particularly on falling asleep (hypnagogic imagery) and waking up (hypnopompic), when the mental imagery, being of a rapid, phantasmagoric and involuntary character, defies perception, presenting a kaleidoscopic field, in which no distinct object can be discerned. Mental imagery can sometimes produce the same effects as would be produced by the behavior or experience imagined.\n\nThe nature of these experiences, what makes them possible, and their function (if any) have long been subjects of research and controversy in philosophy, psychology, cognitive science, and, more recently, neuroscience. As contemporary researchers use the expression, mental images or imagery can comprise information from any source of sensory input; one may experience auditory images, olfactory images, and so forth. However, the majority of philosophical and scientific investigations of the topic focus upon \"visual\" mental imagery. It has sometimes been assumed that, like humans, some types of animals are capable of experiencing mental images. Due to the fundamentally introspective nature of the phenomenon, there is little to no evidence either for or against this view.\n\nPhilosophers such as George Berkeley and David Hume, and early experimental psychologists such as Wilhelm Wundt and William James, understood ideas in general to be mental images. Today it is very widely believed that much imagery functions as mental representations (or mental models), playing an important role in memory and thinking. William Brant (2013, p. 12) traces the scientific use of the phrase \"mental images\" back to John Tyndall's 1870 speech called the \"Scientific Use of the Imagination\". Some have gone so far as to suggest that images are best understood to be, by definition, a form of inner, mental or neural representation; in the case of hypnagogic and hypnapompic imagery, it is not representational at all. Others reject the view that the image experience may be identical with (or directly caused by) any such representation in the mind or the brain, but do not take account of the non-representational forms of imagery.\n\nIn 2010, IBM applied for a patent on a method to extract mental images of human faces from the human brain. It uses a feedback loop based on brain measurements of the fusiform face area in the brain that activates proportionate with degree of facial recognition. It was issued in 2015.\n\nThe notion of a \"mind's eye\" goes back at least to Cicero's reference to mentis oculi during his discussion of the orator's appropriate use of simile.\n\nIn this discussion, Cicero observed that allusions to \"the Syrtis of his patrimony\" and \"the Charybdis of his possessions\" involved similes that were \"too far-fetched\"; and he advised the orator to, instead, just speak of \"the rock\" and \"the gulf\" (respectively)—on the grounds that \"the eyes of the mind are more easily directed to those objects which we have seen, than to those which we have only heard\".\n\nThe concept of \"the mind's eye\" first appeared in English in Chaucer's (c. 1387) Man of Law's Tale in his \"Canterbury Tales\", where he tells us that one of the three men dwelling in a castle was blind, and could only see with \"the eyes of his mind\"; namely, those eyes \"with which all men see after they have become blind\".\n\nThe biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes:\nThe visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye – to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex - all higher cognitive processing centers of the brain.\n\nThe rudiments of a biological basis for the mind's eye is found in the deeper portions of the brain below the neocortex, or where the center of perception exists. The thalamus has been found to be discrete to other components in that it processes all forms of perceptional data relayed from both lower and higher components of the brain. Damage to this component can produce permanent perceptual damage, however when damage is inflicted upon the cerebral cortex, the brain adapts to neuroplasticity to amend any occlusions for perception. It can be thought that the neocortex is a sophisticated memory storage warehouse in which data received as an input from sensory systems are compartmentalized via the cerebral cortex. This would essentially allow for shapes to be identified, although given the lack of filtering input produced internally, one may as a consequence, hallucinate - essentially seeing something that isn't received as an input externally but rather internal (i.e. an error in the filtering of segmented sensory data from the cerebral cortex may result in one seeing, feeling, hearing or experiencing something that is inconsistent with reality).\n\nNot all people have the same internal perceptual ability. For many, when the eyes are closed, the perception of darkness prevails. However, some people are able to perceive colorful, dynamic imagery. The use of hallucinogenic drugs increases the subject's ability to consciously access visual (and auditory, and other sense) percepts.\n\nFurthermore, the pineal gland is a hypothetical candidate for producing a mind's eye; Rick Strassman and others have postulated that during near-death experiences (NDEs) and dreaming, the gland might secrete a hallucinogenic chemical \"N\",\"N\"-Dimethyltryptamine (DMT) to produce internal visuals when external sensory data is occluded. However, this hypothesis has yet to be fully supported with neurochemical evidence and plausible mechanism for DMT production.\n\nThe hypothesized condition where a person lacks a mind's eye is called aphantasia. The term was first suggested in a 2015 study.\n\nCommon examples of mental images include daydreaming and the mental visualization that occurs while reading a book. Another is of the pictures summoned by athletes during training or before a competition, outlining each step they will take to accomplish their goal. When a musician hears a song, he or she can sometimes \"see\" the song notes in their head, as well as hear them with all their tonal qualities. This is considered different from an after-effect, such as an after-image. Calling up an image in our minds can be a voluntary act, so it can be characterized as being under various degrees of conscious control.\n\nAccording to psychologist and cognitive scientist Steven Pinker, our experiences of the world are represented in our minds as mental images. These mental images can then be associated and compared with others, and can be used to synthesize completely new images. In this view, mental images allow us to form useful theories of how the world works by formulating likely sequences of mental images in our heads without having to directly experience that outcome. Whether other creatures have this capability is debatable.\n\nThere are several theories as to how mental images are formed in the mind. These include the dual-code theory, the propositional theory, and the functional-equivalency hypothesis. The dual-code theory, created by Allan Paivio in 1971, is the theory that we use two separate codes to represent information in our brains: image codes and verbal codes. Image codes are things like thinking of a picture of a dog when you are thinking of a dog, whereas a verbal code would be to think of the word \"dog\". Another example is the difference between thinking of abstract words such as \"justice\" or \"love\" and thinking of concrete words like \"elephant\" or \"chair.\" When abstract words are thought of, it is easier to think of them in terms of verbal codes—finding words that define them or describe them. With concrete words, it is often easier to use image codes and bring up a picture of a \"human\" or \"chair\" in your mind rather than words associated or descriptive of them.\n\nThe propositional theory involves storing images in the form of a generic propositional code that stores the meaning of the concept not the image itself. The propositional codes can either be descriptive of the image or symbolic. They are then transferred back into verbal and visual code to form the mental image.\n\nThe functional-equivalency hypothesis is that mental images are \"internal representations\" that work in the same way as the actual perception of physical objects. In other words, the picture of a dog brought to mind when the word \"dog\" is read is interpreted in the same way as if the person looking at an actual dog before them.\n\nResearch has occurred to designate a specific neural correlate of imagery; however, studies show a multitude of results. Most studies published before 2001 suggest neural correlates of visual imagery occur in brodmann area 17. Auditory performance imagery have been observed in the premotor areas, precunes, and medial brodmann area 40. Auditory imagery in general occurs across participants in the temporal voice area (TVA), which allows top-down imaging manipulations, processing, and storage of audition functions. Olfactory imagery research shows activation in the anterior piriform cortex and the posterior piriform cortex; experts in olfactory imagery have larger gray matter associated to olfactory areas. Tactile imagery is found to occur in the dorsolateral prefrontal area, inferior frontal gyrus, frontal gyrus, insula, precentral gyrus, and the medial frontal gyrus with basil ganglia activation in the ventral posteriomedial nucleus and putamen (hemisphere activation corresponds to the location of the imagined tactile stimulus). Research in gustatory imagery reveals activation in the anterior insular cortex, frontal operculum, and prefrontal cortex. Novices of a specific form of mental imagery show less gray matter than experts of mental imagery congruent to that form. A meta-analysis of neuroimagery studies revealed significant activation of the bilateral dorsal parietal, interior insula, and left inferior frontal regions of the brain.\n\nImagery has been thought to cooccur with perception; however, participants with damaged sense-modality receptors can sometimes perform imagery of said modality receptors. Neuroscience with imagery has been used to communicate with seemingly unconscious individuals through fMRI activation of different neural correlates of imagery, demanding further study into low quality consciousness. A study on one patient with one occipital lobe removed found the horizontal area of their visual mental image was reduced.\n\nVisual imagery is the ability to create mental representations of things, people, and places that are absent from an individual’s visual field. This ability is crucial to problem-solving tasks, memory, and spatial reasoning. Neuroscientists have found that imagery and perception share many of the same neural substrates, or areas of the brain that function similarly during both imagery and perception, such as the visual cortex and higher visual areas. Kosslyn and colleagues (1999) showed that the early visual cortex, Area 17 and Area 18/19, is activated during visual imagery. They found that inhibition of these areas through repetitive transcranial magnetic stimulation (rTMS) resulted in impaired visual perception and imagery. Furthermore, research conducted with lesioned patients has revealed that visual imagery and visual perception have the same representational organization. This has been concluded from patients in which impaired perception also experience visual imagery deficits at the same level of the mental representation.\n\nBehrmann and colleagues (1992) describe a patient C.K., who provided evidence challenging the view that visual imagery and visual perception rely on the same representational system. C.K. was a 33-year old man with visual object agnosia acquired after a vehicular accident. This deficit prevented him from being able to recognize objects and copy objects fluidly. Surprisingly, his ability to draw accurate objects from memory indicated his visual imagery was intact and normal. Furthermore, C.K. successfully performed other tasks requiring visual imagery for judgment of size, shape, color, and composition. These findings conflict with previous research as they suggest there is a partial dissociation between visual imagery and visual perception. C.K. exhibited a perceptual deficit that was not associated with a corresponding deficit in visual imagery, indicating that these two processes have systems for mental representations that may not be mediated entirely by the same neural substrates. \n\nSchlegel and colleagues (2013) conducted a functional MRI analysis of regions activated during manipulation of visual imagery. They identified 11 bilateral cortical and subcortical regions that exhibited increased activation when manipulating a visual image compared to when the visual image was just maintained. These regions included the occipital lobe and ventral stream areas, two parietal lobe regions, the posterior parietal cortex and the precuneus lobule, and three frontal lobe regions, the frontal eye fields, dorsolateral prefrontal cortex, and the prefrontal cortex. Due to their suspected involvement in working memory and attention, the authors propose that these parietal and prefrontal regions, and occipital regions, are part of a network involved in mediating the manipulation of visual imagery. These results suggest a top-down activation of visual areas in visual imagery.\n\nUsing Dynamic Causal Modeling (DCM) to determine the connectivity of cortical networks, Ishai et al. (2010) demonstrated that activation of the network mediating visual imagery is initiated by prefrontal cortex and posterior parietal cortex activity. Generation of objects from memory resulted in initial activation of the prefrontal and the posterior parietal areas, which then activate earlier visual areas through backward connectivity. Activation of the prefrontal cortex and posterior parietal cortex has also been found to be involved in retrieval of object representations from long-term memory, their maintenance in working memory, and attention during visual imagery. Thus, Ishai et al. suggest that the network mediating visual imagery is composed of attentional mechanisms arising from the posterior parietal cortex and the prefrontal cortex.\n\nVividness of visual imagery is a crucial component of an individual’s ability to perform cognitive tasks requiring imagery. Vividness of visual imagery varies not only between individuals but also within individuals. Dijkstra and colleagues (2017) found that the variation in vividness of visual imagery is dependent on the degree to which the neural substrates of visual imagery overlap with those of visual perception. They found that overlap between imagery and perception in the entire visual cortex, the parietal precuneus lobule, the right parietal cortex, and the medial frontal cortex predicted the vividness of a mental representation. The activated regions beyond the visual areas are believed to drive the imagery-specific processes rather than the visual processes shared with perception. It has been suggested that the precuneus contributes to vividness by selecting important details for imagery. The medial frontal cortex is suspected to be involved in the retrieval and integration of information from the parietal and visual areas during working memory and visual imagery. The right parietal cortex appears to be important in attention, visual inspection, and stabilization of mental representations. Thus, the neural substrates of visual imagery and perception overlap in areas beyond the visual cortex and the degree of this overlap in these areas correlates with the vividness of mental representations during imagery.\n\nMental images are an important topic in classical and modern philosophy, as they are central to the study of knowledge. In the \"Republic\", Book VII, Plato has Socrates present the Allegory of the Cave: a prisoner, bound and unable to move, sits with his back to a fire watching the shadows cast on the cave wall in front of him by people carrying objects behind his back. These people and the objects they carry are representations of real things in the world. Unenlightened man is like the prisoner, explains Socrates, a human being making mental images from the sense data that he experiences.\n\nThe eighteenth-century philosopher Bishop George Berkeley proposed similar ideas in his theory of idealism. Berkeley stated that reality is equivalent to mental images—our mental images are not a copy of another material reality but that reality itself. Berkeley, however, sharply distinguished between the images that he considered to constitute the external world, and the images of individual imagination. According to Berkeley, only the latter are considered \"mental imagery\" in the contemporary sense of the term.\n\nThe eighteenth century British writer Dr. Samuel Johnson criticized idealism. When asked what he thought about idealism, he is alleged to have replied \"I refute it thus!\" as he kicked a large rock and his leg rebounded. His point was that the idea that the rock is just another mental image and has no material existence of its own is a poor explanation of the painful sense data he had just experienced.\n\nDavid Deutsch addresses Johnson's objection to idealism in \"The Fabric of Reality\" when he states that, if we judge the value of our mental images of the world by the quality and quantity of the sense data that they can explain, then the most valuable mental image—or theory—that we currently have is that the world has a real independent existence and that humans have successfully evolved by building up and adapting patterns of mental images to explain it. This is an important idea in scientific thought.\n\nCritics of scientific realism ask how the inner perception of mental images actually occurs. This is sometimes called the \"homunculus problem\" (see also the mind's eye). The problem is similar to asking how the images you see on a computer screen exist in the memory of the computer. To scientific materialism, mental images and the perception of them must be brain-states. According to critics, scientific realists cannot explain where the images and their perceiver exist in the brain. To use the analogy of the computer screen, these critics argue that cognitive science and psychology have been unsuccessful in identifying either the component in the brain (i.e., \"hardware\") or the mental processes that store these images (i.e. \"software\").\n\nCognitive psychologists and (later) cognitive neuroscientists have empirically tested some of the philosophical questions related to whether and how the human brain uses mental imagery in cognition.\n\nOne theory of the mind that was examined in these experiments was the \"brain as serial computer\" philosophical metaphor of the 1970s. Psychologist Zenon Pylyshyn theorized that the human mind processes mental images by decomposing them into an underlying mathematical proposition. Roger Shepard and Jacqueline Metzler challenged that view by presenting subjects with 2D line drawings of groups of 3D block \"objects\" and asking them to determine whether that \"object\" is the same as a second figure, some of which rotations of the first \"object\". Shepard and Metzler proposed that if we decomposed and then mentally re-imaged the objects into basic mathematical propositions, as the then-dominant view of cognition \"as a serial digital computer\" assumed, then it would be expected that the time it took to determine whether the object is the same or not would be independent of how much the object had been rotated. Shepard and Metzler found the opposite: a linear relationship between the degree of rotation in the mental imagery task and the time it took participants to reach their answer.\n\nThis mental rotation finding implied that the human mind—and the human brain—maintains and manipulates mental images as topographic and topological wholes, an implication that was quickly put to test by psychologists. Stephen Kosslyn and colleagues showed in a series of neuroimaging experiments that the mental image of objects like the letter \"F\" are mapped, maintained and rotated as an image-like whole in areas of the human visual cortex. Moreover, Kosslyn's work showed that there are considerable similarities between the neural mappings for imagined stimuli and perceived stimuli. The authors of these studies concluded that, while the neural processes they studied rely on mathematical and computational underpinnings, the brain also seems optimized to handle the sort of mathematics that constantly computes a series of topologically-based images rather than calculating a mathematical model of an object.\n\nRecent studies in neurology and neuropsychology on mental imagery have further questioned the \"mind as serial computer\" theory, arguing instead that human mental imagery manifests both visually and kinesthetically. For example, several studies have provided evidence that people are slower at rotating line drawings of objects such as hands in directions incompatible with the joints of the human body, and that patients with painful, injured arms are slower at mentally rotating line drawings of the hand from the side of the injured arm.\n\nSome psychologists, including Kosslyn, have argued that such results occur because of interference in the brain between distinct systems in the brain that process the visual and motor mental imagery. Subsequent neuroimaging studies showed that the interference between the motor and visual imagery system could be induced by having participants physically handle actual 3D blocks glued together to form objects similar to those depicted in the line-drawings. Amorim et al. have shown that, when a cylindrical \"head\" was added to Shepard and Metzler's line drawings of 3D block figures, participants were quicker and more accurate at solving mental rotation problems. They argue that motoric embodiment is not just \"interference\" that inhibits visual mental imagery but is capable of facilitating mental imagery.\n\nAs cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain’s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain’s visual areas while subjects imagined visual objects and scenes.\n\nThe previously mentioned and numerous related studies have led to a relative consensus within cognitive science, psychology, neuroscience, and philosophy on the neural status of mental images. In general, researchers agree that, while there is no homunculus inside the head viewing these mental images, our brains do form and maintain mental images as image-like wholes. The problem of exactly how these images are stored and manipulated within the human brain, in particular within language and communication, remains a fertile area of study.\n\nOne of the longest-running research topics on the mental image has basis on the fact that people report large individual differences in the vividness of their images. Special questionnaires have been developed to assess such differences, including the Vividness of Visual Imagery Questionnaire (VVIQ) developed by David Marks. Laboratory studies have suggested that the subjectively reported variations in imagery vividness are associated with different neural states within the brain and also different cognitive competences such as the ability to accurately recall information presented in pictures Rodway, Gillies and Schepman used a novel long-term change detection task to determine whether participants with low and high vividness scores on the VVIQ2 showed any performance differences. Rodway et al. found that high vividness participants were significantly more accurate at detecting salient changes to pictures compared to low-vividness participants. This replicated an earlier study.\n\nRecent studies have found that individual differences in VVIQ scores can be used to predict changes in a person's brain while visualizing different activities. Functional magnetic resonance imaging (fMRI) was used to study the association between early visual cortex activity relative to the whole brain while participants visualized themselves or another person bench pressing or stair climbing. Reported image vividness correlates significantly with the relative fMRI signal in the visual cortex. Thus, individual differences in the vividness of visual imagery can be measured objectively.\n\nLogie, Pernet, Buonocore and Della Sala (2011) used behavioural and fMRI data for mental rotation from individuals reporting vivid and poor imagery on the VVIQ. Groups differed in brain activation patterns suggesting that the groups performed the same tasks in different ways. These findings help to explain the lack of association previously reported between VVIQ scores and mental rotation performance.\n\nSome educational theorists have drawn from the idea of mental imagery in their studies of learning styles. Proponents of these theories state that people often have learning processes that emphasize visual, auditory, and kinesthetic systems of experience. According to these theorists, teaching in multiple overlapping sensory systems benefits learning, and they encourage teachers to use content and media that integrates well with the visual, auditory, and kinesthetic systems whenever possible.\n\nEducational researchers have examined whether the experience of mental imagery affects the degree of learning. For example, imagining playing a 5-finger piano exercise (mental practice) resulted in a significant improvement in performance over no mental practice—though not as significant as that produced by physical practice. The authors of the study stated that \"mental practice alone seems to be sufficient to promote the modulation of neural circuits involved in the early stages of motor skill learning\".\n\nIn general, Vajrayana Buddhism, Bön, and Tantra utilize sophisticated visualization or \"imaginal\" (in the language of Jean Houston of Transpersonal Psychology) processes in the thoughtform construction of the yidam sadhana, kye-rim, and dzog-rim modes of meditation and in the yantra, thangka, and mandala traditions, where holding the fully realized form in the mind is a prerequisite prior to creating an 'authentic' new art work that will provide a sacred support or foundation for deity.\n\nMental imagery can act as a substitute for the imagined experience: Imagining an experience can evoke similar cognitive, physiological, and/or behavioral consequences as having the corresponding experience in reality. At least four classes of such effects have been documented.\n\n"}
{"id": "27079770", "url": "https://en.wikipedia.org/wiki?curid=27079770", "title": "Mental model theory of reasoning", "text": "Mental model theory of reasoning\n\nThe mental model theory of reasoning was developed by Philip Johnson-Laird and Ruth M.J. Byrne (Johnson-Laird and Byrne, 1991). It has been applied to the main domains of deductive inference including relational inferences such as spatial and temporal deductions; propositional inferences, such as conditional, disjunctive and negation deductions; quantified inferences such as syllogisms; and meta-deductive inferences.\n\nOngoing research on mental models and reasoning has led the theory to be extended to account for probabilistic inference (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).\n\n"}
{"id": "46476968", "url": "https://en.wikipedia.org/wiki?curid=46476968", "title": "Model worker", "text": "Model worker\n\nModel worker (, abbreviated as 劳模 or láomó) is a Communist Chinese political term referring to an exemplary worker who exhibits some or all of the traits appropriate to the ideal of the socialist worker. The idea is similar to the Soviet Stakhanovite icon. Model workers are selected in China by central and provincial-level departments. Some cities and large companies also have processes for selecting and praising model workers.\n\nThe basic criteria for model workers are patriotism, \"worship of science,\" activities in environmental protection, and the pursuit of excellence.\n\nModel workers are often afforded privileges not available to other citizens or Communist Party members. \"The possibility to become a model worker offered peasants and workers one of the few opportunities for upward mobility other than joining the army,\" writes scholar Yu Miin-lin. Model workers have an easier time joining the Communist Party, and also to become a higher-level cadre, manager, or other leader.\n\nOne of the earliest model workers was the teenage textile worker Hao Jianxiu (awarded 1951), who invented the \"Hao Jianxiu Work Method\". She was sent to study at East China Textile Engineering Institute and was elevated to the upper echelon of Chinese politics, serving as Minister of Textile Industry, secretary of the CPC Central Secretariat, and vice chair of the State Planning Commission.\nAnother prominent model worker was Ni Zhifu (awarded 1959), a fitter who invented the \"Ni Zhifu drill\". He was elevated to leadership positions in the municipal governments of Beijing, Shanghai, and Tianjin, and became a member of the Politburo of the Communist Party of China. He also served as Chairman of the All-China Federation of Trade Unions.\n"}
{"id": "48243754", "url": "https://en.wikipedia.org/wiki?curid=48243754", "title": "Negative consequentialism", "text": "Negative consequentialism\n\nNegative consequentialism is a version of the ethical theory consequentialism, which is \"one of the major theories of normative ethics.\" Like other versions of consequentialism, negative consequentialism holds that moral right and wrong depend only on the value of outcomes. That is, for negative and other versions of consequentialism, questions such as \"what should I do?\" and \"what kind of person should I be?\" are answered only based on consequences. Negative consequentialism differs from other versions of consequentialism by giving greater weight in moral deliberations to what is bad (e.g. suffering or injustice) than what is good (e.g. happiness or justice).\n\nA specific type of consequentialism is utilitarianism, which says that the consequences that matter are those that affect well-being. Consequentialism is broader than utilitarianism in that consequentialism can say that the value of outcomes depend on other things than well-being; for example, justice, fairness, and equality. Negative utilitarianism is thus a form of negative consequentialism. Much more has been written explicitly about negative utilitarianism than directly about negative consequentialism, although since negative utilitarianism is a form of negative consequentialism, everything that has been written about negative utilitarianism is by definition about a specific (utilitarian) version of negative consequentialism. Similarly to how there are many variations of consequentialism and negative utilitarianism, there are many versions of negative consequentialism, for example negative prioritarianism and negative consequentialist egalitarianism.\n\nG. E. Moore's ethics can be said to be a negative consequentialism (more precisely, a consequentialism with a negative utilitarian component), because he has been labeled a consequentialist, and he said that \"consciousness of intense pain is, by itself, a great evil\" whereas \"the mere consciousness of pleasure, however intense, does not, \"by itself\", appear to be a \"great\" good, even if it has some slight intrinsic value. In short, pain (if we understand by this expression, the consciousness of pain) appears to be a far worse evil than pleasure is a good.\" Moore wrote in the first half of the 20th century before any of the terms 'consequentialism,' 'negative utilitarianism' or 'negative consequentialism' were coined, and he did not use the term 'negative consequentialism' himself. Similarly to Moore, Ingemar Hedenius defended a consequentialism that could be called negative (or could be said to have a negative utilitarian component) because he assigned more importance to suffering than to happiness. Hedenius saw the worst in life, such as infernalistic suffering, as so evil that calculations of happiness versus suffering becomes unnecessary; he did not see that such evil could be counterbalanced by any good, such as happiness.\n\nPhilosophy professor Clark Wolf defends \"negative consequentialism as a component of a larger theory of justice.\" Walter Sinnott-Armstrong interprets Bernard Gert's moral system as a \"sophisticated form of negative objective universal public rule consequentialism.\" Jamie Mayerfeld argues for a strong duty to relieve suffering, which is consequentialist in form. He says that \"suffering is more bad than happiness is good,\" and that \"the lifelong bliss of many people, no matter how many, cannot justify our allowing the lifelong torture of one.\"\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "32108559", "url": "https://en.wikipedia.org/wiki?curid=32108559", "title": "Negative hyperconjugation", "text": "Negative hyperconjugation\n\nIn organic chemistry, negative hyperconjugation is the donation of electron density from a filled π- or p-orbital to a neighboring σ-orbital. This phenomenon, a type of resonance, can stabilize the molecule or transition state. It also causes an elongation of the σ-bond by adding electron density to its antibonding orbital.\n\nNegative hyperconjugation is most commonly observed when the σ-orbital is located on certain C–F or C–O bonds, and does not occur to an appreciable extent with normal C–H bonds.\n\nIn negative hyperconjugation, the electron density flows in the \"opposite\" direction (from π- or p-orbital to empty σ-orbital) than it does in the more common hyperconjugation (from σ-orbital to empty p-orbital).\n\n"}
{"id": "3778432", "url": "https://en.wikipedia.org/wiki?curid=3778432", "title": "Negative repetition", "text": "Negative repetition\n\nA negative repetition (negative rep) is the repetition of a technique in weight lifting in which the lifter performs the eccentric phase of a lift. Instead of pressing the weight up slowly, in proper form, a spotter generally aids in the concentric, or lifting, portion of the repetition while the lifter slowly performs the eccentric phase for 3–6 seconds. Negative reps are used to improve both muscular strength and power in subjects, this is commonly known as hypertrophy training.\n\nDue to its mechanical properties, this form of training can be used for both healthy individuals and individuals who are in rehabilitation. Studies have shown that negative repetitions or \"eccentric phase training\" combines a high amount of force on the muscle with a lower energy cost than normal concentric training, which requires 4–5 times the amount of energy. This justifies why this type of training is more beneficial and less of a risk to subjects rehabilitating or with a limited exercise capacity.\n\nEccentric training is often associated with the terms \"muscles soreness\" and \"muscle damage\". In 1902, Theodore Hough discovered and developed the term DOMS (delayed onset muscle soreness), after he found that exercises containing negative repetitions caused athletes to have sore muscles. Hough believed this was causing a rupture within the muscle; when he looked further into the subject, he found that when performing eccentric exercise that exhibited soreness, the muscle \"quickly adapts and becomes accustomed to the increase in applied stress\". The result of this was that the muscles' soreness not only decreased, but the muscular damage did too.\n\nIt has been proven that eccentric resistance training improves the functional mobility of older adults. Studies have shown that eccentric training of the lower body, in particular the , are essential in preventing falls in older adults and helping them maintain their independence. A study conducted focusing on eccentric training for the age group 65–87 years of age showed that, over 12 weeks, they had strengthened their knee extensors by up to 26%. With this evidence, it is reasonable to suggest that negative repetitions can help improve the health of older adults.\n\nStudies have shown that eccentric training may be successful in the treatment of certain tendonitis. Studies have shown that the use of eccentric training for twelve weeks may be an alternative to therapy for people suffering from Patellar Tendinopathy (Jumper's Knee). Eccentric training has also been proven successful in the treatment of chronic Achilles tendonitis, using a twelve-week eccentric calf muscle program various studies have shown the ability for people to return to normal pre-tendonitis levels. The reasoning behind the benefits of eccentric training for tendinopathy is still unclear.\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "7108905", "url": "https://en.wikipedia.org/wiki?curid=7108905", "title": "Negative return (finance)", "text": "Negative return (finance)\n\nThe term negative return is used in business or finance to describe a loss, i.e., a negative return on investment. By extension the term is also used for a project that is not worthwhile, even in a non-economic sense.\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "1593030", "url": "https://en.wikipedia.org/wiki?curid=1593030", "title": "Product/process distinction", "text": "Product/process distinction\n\nThe product/process distinction is the distinction between the product information and the process information of a consumer good. Product information is information that pertains to a consumer good, namely to its price, quality, and safety (its proximate attributes). Process information is information that pertains to the means by which the consumer good is made i.e. the working conditions under which it comes into being, as well as the treatment of animals involved in its production chain (its peripheral attributes).\n\nThe product/process distinction is used by the World Trade Organization (WTO) as a way to determine whether or not a complaint filed by an importing nation is valid and warrants trade barriers against the exporting nation. Under WTO rules, an importing nation can lodge a complaint with the WTO that the exporting nation uses methods for obtaining or producing the good in question that the importing nation finds to be immoral or unethical. If the independent World Trade Organization Advisory Board, made up of a panel of international law and trade experts, finds that the importing nation has a legitimate complaint, enforces said ethical standards for domestic production, and isn't trying to merely skirt its free trade obligations, then the Board will rule that trade barriers are justified. Despite what World Trade Organization officials have said, in practice the World Trade Organization finds these complaints illegitimate the vast majority of the time.\n\nFor example, if the European Union (EU) wants to ban imports of cosmetics that were tested on laboratory animals on grounds that such testing is unethical, it can file a complaint with the World Trade Organization and, in theory, the WTO would allow the EU to enact trade barriers provided that the EU bans its own domestic cosmetic producers from testing on laboratory animals. \nIn these cases, however, the World Trade Organization has consistently ruled that such barriers are illegal because only the process is different, while the final product itself is not. Therefore, the WTO has made the product/process distinction an important factor in determining whether trade barriers are justified.\n\nThe World Trade Organization has stated that if nations were able to enact barriers merely because the importing nation's standards differ from their own, control could be lost and barriers could be enacted around the world for frivolous reasons. However, many complain that these rulings go against the stated intentions of the World Trade Organization, and prove that the organization often puts commercial interests above environmental, ethical, and human rights issues.\n"}
{"id": "6394087", "url": "https://en.wikipedia.org/wiki?curid=6394087", "title": "Revelation principle", "text": "Revelation principle\n\nThe revelation principle is a fundamental principle in mechanism design. It states that if a social choice function can be implemented by an arbitrary mechanism (i.e. if that mechanism has an equilibrium outcome that corresponds to the outcome of the social choice function), then the same function can be implemented by an incentive-compatible-direct-mechanism (i.e. in which players truthfully report type) with the same equilibrium outcome (payoffs).\n\nIn mechanism design, the revelation principle is of utmost importance in finding solutions. The researcher need only look at the set of equilibrium characterized by incentive compatibility. That is, if the mechanism designer wants to implement some outcome or property, he can restrict his search to mechanisms in which agents are willing to reveal their private information to the mechanism designer that has that outcome or property. If no such direct and truthful mechanism exists, no mechanism can implement this outcome/property. By narrowing the area needed to be searched, the problem of finding a mechanism becomes much easier.\n\nThe principle comes in two variants corresponding to the two flavors of incentive-compatibility:\n\nConsider the following example. There is a certain item that Alice values as formula_1 and Bob values as formula_2. The government needs to decide who will receive that item and in what terms. \n\nSuppose we have an arbitrary mechanism Mech that implements Soc.\n\nWe construct a direct mechanism Mech' that is truthful and implements Soc.\n\nMech' simply simulates the equilibrium strategies of the players in Game(Mech). I.e:\n\nReporting the true valuations in Mech' is like playing the equilibrium strategies in Mech. Hence, reporting the true valuations is a Nash equilibrium in Mech', as desired. Moreover, the equilibrium payoffs are the same, as desired.\n\nThe revelation principle says that for every arbitrary \"coordinating device\" a.k.a. correlating there exists another direct device for which the state space equals the action space of each player. Then the coordination is done by directly informing each player of his action.\n\n"}
{"id": "11153041", "url": "https://en.wikipedia.org/wiki?curid=11153041", "title": "Saint-Venant's principle", "text": "Saint-Venant's principle\n\nSaint-Venant's principle, named after Adhémar Jean Claude Barré de Saint-Venant, a French elasticity theorist, may be expressed as follows:\n\nThe original statement was published in French by Saint-Venant in 1855. Although this informal statement of the principle is well known among structural and mechanical engineers, more recent mathematical literature gives a rigorous interpretation in the context of partial differential equations. An early such interpretation was made by von Mises in 1945.\n\nThe Saint-Venant's principle allows elasticians to replace complicated stress distributions or weak boundary conditions with ones that are easier to solve, as long as that boundary is geometrically short. Quite analogous to the electrostatics, where the electric field due to the \"i\"-th moment of the load (with 0th being the net charge, 1st the dipole, 2nd the quadrupole) decays as formula_1 over space, Saint-Venant's principle states that high order momentum of mechanical load (moment with order higher than torque) decays so fast that they never need to be considered for regions far from the short boundary. Therefore, the Saint-Venant's principle can be regarded as a statement on the asymptotic behavior of the Green's function by a point-load.\n\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "25606638", "url": "https://en.wikipedia.org/wiki?curid=25606638", "title": "Search neutrality", "text": "Search neutrality\n\nSearch neutrality is a principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance. This means that when a user queries a search engine, the engine should return the most relevant results found in the provider's domain (those sites which the engine has knowledge of), without manipulating the order of the results (except to rank them by relevance), excluding results, or in any other way manipulating the results to a certain bias. \nSearch neutrality is related to network neutrality in that they both aim to keep any one organization from limiting or altering a user's access to services on the Internet. Search neutrality aims to keep the organic search results (results returned because of their relevance to the search terms, as opposed to results sponsored by advertising) of a search engine free from any manipulation, while network neutrality aims to keep those who provide and govern access to the Internet from limiting the availability of resources to access any given content.\n\nThe term \"search neutrality\" in context of the internet appears as early as March 2009 in an academic paper by Andrew Odlyzko titled, \"Network Neutrality, Search Neutrality, and the Never-ending Conflict between Efficiency and Fairness in Markets\". In this paper, Odlykzo predicts that if net neutrality were to be accepted as a legal or regulatory principle, then the questions surrounding search neutrality would be the next controversies. Indeed, in December 2009 the New York Times published an opinion letter by Foundem co-founder and lead complainant in an anti-trust complaint against Google, Adam Raff, which likely brought the term to the broader public. According to Raff in his opinion letter, search neutrality ought to be \"the principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance\". On October 11, 2009, Adam and his wife Shivaun launched SearchNeutrality.org, an initiative dedicated to promoting investigations against Google's search engine practices . There, the Raffs note that they chose to frame their issue with Google as \"search neutrality\" in order to benefit from the focus and interest on net neutrality.\n\nIn contrast to net neutrality, answers to such questions, as \"what is search neutrality?\" or \"what are appropriate legislative or regulatory principles to protect search neutrality?\", appear to have less consensus. The idea that neutrality means equal treatment, regardless of the content, comes from debates on net neutrality . Neutrality in search is complicated by the fact that search engines, by design and in implementation, are not intended to be neutral or impartial. Rather, search engines and other information retrieval applications are designed to collect and store information (indexing), receive a query from a user, search for and filter relevant information based on that query (searching/filtering), and then present the user with only a subset of those results, which are ranked from most relevant to least relevant (ranking). \"Relevance\" is a form of bias used to favor some results and rank those favored results. Relevance is defined in the search engine so that a user is satisfied with the results and is therefore subject to the user's preferences. And because relevance is so subjective, putting search neutrality into practice has been so contentious.\n\nSearch neutrality became a concern after search engines, most notably Google, were accused of search bias by other companies. Competitors and companies claim search engines systematically favor some sites (and some kind of sites) over others in their lists of results, disrupting the objective results users believe they are getting. \n\nThe call for search neutrality goes beyond traditional search engines. Sites like Amazon.com and Facebook are also accused of skewing results. Amazon’s search results are influenced by companies that pay to rank higher in their search results while Facebook filters their newsfeed lists to conduct social experiments.\n\nIn order to find information on the Web, most users make use of search engines, which crawl the web, index it and show a list of results ordered by relevance. The use of search engines to access information through the web has become a key factor for online businesses companies, which depend on the flow of users visiting their pages. One of these companies is Foundem. Foundem provides a \"vertical search\" service to compare products available on online markets for the U.K. Many people see these \"vertical search\" sites as spam. Beginning in 2006 and for three and a half years following, Foundem’s traffic and business dropped significantly due to what they assert to be a penalty deliberately applied by Google. It is unclear, however, whether their claim of a penalty was self-imposed via their use of iframe HTML tags to embed the content from other websites. At the time at which Foundem claims the penalties were imposed, it was unclear whether web crawlers crawled beyond the main page of a website using iframe tags without some extra modifications. The former SEO director OMD UK, Jaamit Durrani, among others, offered this alternative explanation, stating that “Two of the major issues that Foundem had in summer was content in iFrames and content requiring javascript to load – both of which I looked at in August, and they were definitely in place. Both are huge barriers to search visibility in my book. They have been fixed somewhere between then and the lifting of the supposed ‘penalty’. I don’t think that’s a coincidence.” \n\nMost of Foundem’s accusations claim that Google deliberately applies penalties to other vertical search engines because they represent competition. Foundem is backed by a Microsoft proxy group, the 'Initiative for Competitive Online Marketplace'.\n\nThe following table details Foundem's chronology of events as found on their website:\n\nGoogle's large market share (85%) has made them a target for search neutrality litigation via antitrust laws. In February 2010, Google released an article on the Google Public Policy blog expressing their concern for fair competition, when other companies at the UK joined Foundem's cause (eJustice.fr, and Microsoft's Ciao! from Bing) also claiming being unfairly penalized by Google.\n\nAfter two years of looking into claims that Google “manipulated its search algorithms to harm vertical websites and unfairly promote its own competing vertical properties,” the Federal Trade Commission (FTC) voted unanimously to end the antitrust portion of its investigation without filing a formal complaint against Google. The FTC concluded that Google’s “practice of favoring its own content in the presentation of search results” did not violate U.S. antitrust laws. The FTC further determined that even though competitors might be negatively impacted by Google's changing algorithms, Google did not change its algorithms to hurt competitors, but as a product improvement to benefit consumers.\n\nThere are a number of arguments for and against search neutrality.\n\n\n\nGoogle’s \"Universal Search\" system has been identified as one of the least neutral search engine practices, and following the implementation of Universal Search websites, such as MapQuest, the company experienced a massive decline in web traffic. This decline has been attributed to Google linking to its own services rather than the services offered at external websites. Despite these claims, Google actually displays Google content on the first page, while rival search engines do not considerably less often than Microsoft's Bing which displays Microsoft content when rivals do not. Bing displays Microsoft content in first place more than twice as often as Google shows Google content in first place. This indicates that as far as there is any 'bias', Google is less biased than its principal competitor.\n"}
{"id": "2907387", "url": "https://en.wikipedia.org/wiki?curid=2907387", "title": "Signed zero", "text": "Signed zero\n\nSigned zero is zero with an associated sign. In ordinary arithmetic, the number 0 does not have a sign, so that −0, +0 and 0 are identical. However, in computing, some number representations allow for the existence of two zeros, often denoted by −0 (negative zero) and +0 (positive zero), regarded as equal by the numerical comparison operations but with possible different behaviors in particular operations. This occurs in the \"sign and magnitude\" and \"ones' complement\" signed number representations for integers, and in most floating-point number representations. The number 0 is usually encoded as +0, but can be represented by either +0 or −0.\n\nThe IEEE 754 standard for floating-point arithmetic (presently used by most computers and programming languages that support floating point numbers) requires both +0 and −0. Real arithmetic with signed zeros can be considered a variant of the extended real number line such that 1/−0 = −∞ and 1/+0 = +∞; division is only undefined for ±0/±0 and ±∞/±∞.\n\nNegatively signed zero echoes the mathematical analysis concept of approaching 0 from below as a one-sided limit, which may be denoted by \"x\" → 0, \"x\" → 0−, or \"x\" → ↑0. The notation \"−0\" may be used informally to denote a small negative number that has been rounded to zero. The concept of negative zero also has some theoretical applications in statistical mechanics and other disciplines.\n\nIt is claimed that the inclusion of signed zero in IEEE 754 makes it much easier to achieve numerical accuracy in some critical problems, in particular when computing with complex elementary functions. On the other hand, the concept of signed zero runs contrary to the general assumption made in most mathematical fields that negative zero is the same thing as zero. Representations that allow negative zero can be a source of errors in programs, if software developers do not take into account that while the two zero representations behave as equal under numeric comparisons, they yield different results in some operations.\n\nThe widely used two's complement encoding does not allow a negative zero. In a 1+7-bit sign-and-magnitude representation for integers, negative zero is represented by the bit string . In an 8-bit one's complement representation, negative zero is represented by the bit string . In all three encodings, positive zero is represented by .\nIn IEEE 754 binary floating point numbers, zero values are represented by the biased exponent and significand both being zero. Negative zero has the sign bit set to one. One may obtain negative zero as the result of certain computations, for instance as the result of arithmetic underflow on a negative number, or codice_1, or simply as codice_2.\n\nIn IEEE 754 decimal floating point encoding, a negative zero is represented by an exponent being any valid exponent in the range for the encoding, the true significand being zero, and the sign bit being one.\n\nThe IEEE 754 floating point standard specifies the behavior of positive zero and negative zero under various operations. The outcome may depend on the current IEEE rounding mode settings.\n\nIn systems that include both signed and unsigned zeros, the notation formula_1 and formula_2 is sometimes used for signed zeros.\n\nAddition and multiplication are commutative, but there are some special rules that have to be followed, which mean the usual mathematical rules for algebraic simplification may not apply. The formula_3 sign below shows the signed result of the operations.\n\nThe usual rule for signs is always followed when multiplying or dividing:\n\n\nThere are special rules for adding or subtracting signed zero:\n\n\nBecause of negative zero (and also when the rounding mode is upward or downward), the expressions and , for floating-point variables \"x\" and \"y\", cannot be replaced by . However can be replaced by \"x\" with rounding to nearest (except when \"x\" can be a signaling NaN).\n\nSome other special rules:\n\n\nDivision of a non-zero number by zero sets the divide by zero flag, and an operation producing a NaN sets the invalid operation flag. An exception handler is called if enabled for the corresponding flag.\n\nAccording to the IEEE 754 standard, negative zero and positive zero should compare as equal with the usual (numerical) comparison operators, like the codice_3 operators of C and Java. In those languages, special programming tricks may be needed to distinguish the two values:\n\n\nNote: Casting to integral type will not always work, especially on two's complement systems.\n\nHowever, some programming languages may provide alternative comparison operators that do distinguish the two zeros. This is the case, for example, of the equals method in Java's codice_6 wrapper class.\n\nInformally, one may use the notation \"−0\" for a negative value that was rounded to zero. This notation may be useful when a negative sign is significant; for example, when tabulating Celsius temperatures, where a negative sign means \"below freezing\".\n\nIn statistical mechanics, one sometimes uses negative temperatures to describe systems with population inversion, which can be considered to have a temperature greater than positive infinity, because the coefficient of energy in the population distribution function is −1/Temperature. In this context, a temperature of −0 is a (theoretical) temperature larger than any other negative temperature, corresponding to the (theoretical) maximum conceivable extent of population inversion, the opposite extreme to +0.\n\n\n"}
{"id": "1335297", "url": "https://en.wikipedia.org/wiki?curid=1335297", "title": "Spaceship Earth", "text": "Spaceship Earth\n\nSpaceship Earth or Spacecraft Earth is a world view encouraging everyone on Earth to act as a harmonious crew working toward the greater good.\n\nThe earliest known use is a passage in Henry George's best known work, \"Progress and Poverty\" (1879).\nFrom book IV, chapter 2: \nIt is a well-provisioned ship, this on which we sail through space. If the bread and beef above decks seem to grow scarce, we but open a hatch and there is a new supply, of which before we never dreamed. And very great command over the services of others comes to those who as the hatches are opened are permitted to say, \"This is mine!\"\n\nGeorge Orwell later paraphrases Henry George in \"The Road to Wigan Pier\":\n\nThe world is a raft sailing through space with, potentially, plenty of provisions for everybody; the idea that we must all cooperate and see to it that everyone does his fair share of the work and gets his fair share of the provisions seems so blatantly obvious that one would say that no one could possibly fail to accept it unless he had some corrupt motive for clinging to the present system.\n\nIn 1965 Adlai Stevenson made a famous speech to the UN in which he said:\n\nWe travel together, passengers on a little space ship, dependent on its vulnerable reserves of air and soil; all committed for our safety to its security and peace; preserved from annihilation only by the care, the work, and, I will say, the love we give our fragile craft. We cannot maintain it half fortunate, half miserable, half confident, half despairing, half slave—to the ancient enemies of man—half free in a liberation of resources undreamed of until this day. No craft, no crew can travel safely with such vast contradictions. On their resolution depends the survival of us all.\n\nThe following year, \"Spaceship Earth\" became the title of a book by a friend of Stevenson's, the internationally influential economist Barbara Ward.\n\nAlso in 1966, Kenneth E. Boulding, who was influenced by reading Henry George, used the phrase in the title of an essay, \"The Economics of the Coming Spaceship Earth\". Boulding described the past open economy of apparently illimitable resources, which he said he was tempted to call the \"cowboy economy\", and continued: \"The closed economy of the future might similarly be called the 'spaceman' economy, in which the earth has become a single spaceship, without unlimited reservoirs of anything, either for extraction or for pollution, and in which, therefore, man must find his place in a cyclical ecological system\".\n\nThe phrase was also popularized by Buckminster Fuller, who published a book in 1968 under the title of \"Operating Manual for Spaceship Earth\". This quotation, referring to fossil fuels, reflects his approach: \n…we can make all of humanity successful through science's world-engulfing industrial evolution provided that we are not so foolish as to continue to exhaust in a split second of astronomical history the orderly energy savings of billions of years' energy conservation aboard our Spaceship Earth. These energy savings have been put into our Spaceship's life-regeneration-guaranteeing bank account for use only in self-starter functions.\n\nUnited Nations Secretary-General U Thant spoke of Spaceship Earth on Earth Day March 21, 1971 at the ceremony of the ringing of the Japanese Peace Bell: \"May there only be peaceful and cheerful Earth Days to come for our beautiful Spaceship Earth as it continues to spin and circle in frigid space with its warm and fragile cargo of animate life.\"\n\nSpaceship Earth is the name given to the 50 m diameter geodesic sphere that greets visitors at the entrance of Walt Disney World's Epcot theme park. Housed within the sphere is a dark ride that serves to explore the history of communications and promote Epcot's founding principles, \"[a] belief and pride in man's ability to shape a world that offers hope to people everywhere.\" A previous incarnation of the ride, narrated by actor Jeremy Irons and revised in 2008, was explicit in its message:\n\nLike a grand and miraculous spaceship, our planet has sailed through the universe of time, and for a brief moment, we have been among its many passengers….We now have the ability and the responsibility to build new bridges of acceptance and co-operation between us, to create a better world for ourselves and our children as we continue our amazing journey aboard Spaceship Earth.\n\nDavid Deutsch has pointed out that the picture of Earth as a friendly \"spaceship\" habitat is difficult to defend even in metaphorical sense. The Earth environment is harsh and survival is constant struggle for life, including whole species extinction. Humans wouldn't be able to live in most of the areas where they are living now without knowledge necessary to build life-support systems such as houses, heating, water supply, etc.\n\nThe term \"Spaceship Earth\" is frequently used on the labels of Emanuel Bronner's products to refer to the Earth.\n\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
{"id": "14934822", "url": "https://en.wikipedia.org/wiki?curid=14934822", "title": "Type–token distinction", "text": "Type–token distinction\n\nThe type–token distinction is used in disciplines such as logic, linguistics, metalogic, typography, and computer programming to clarify what words mean.\n\nThe sentence \"they drive the same car\" is ambiguous. Do they drive the same \"type\" of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type–token distinction separates types (abstract descriptive concepts) from tokens (objects that instantiate concepts).\n\nFor example: \"bicycle\" represents a type: the concept of a bicycle; whereas \"my bicycle\" represents a token of that type: an object that instantiates that type. In the sentence \"the bicycle is becoming more popular\" the word \"bicycle\" represents a type that is a concept; whereas in the sentence \"the bicycle is in the garage\" the word \"bicycle\" represents a token: a particular object.\n\nThe words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is \"thorny\", \"flowering\" and \"bushy\". You might say a rose bush \"instantiates\" these three types, or \"embodies\" these three concepts, or \"exhibits\" these three properties, or \"possesses\" these three qualities, features or attributes.\n\nProperty types (e.g \"height in metres\" or \"thorny\") are often understood ontologically as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.\n\nSome say types exist in descriptions of objects, but not as tangible physical objects. They say one can show someone a particular bicycle, but cannot show someone the type \"bicycle\", as in \"\"the bicycle\" is popular.\". However types do exist in the sense that they appear in mental and documented models.\n\nSome say tokens are objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can be intangible objects of types such as \"thought\", \"tennis match\", \"government\" and \"act of kindness\".\n\nThere is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an occurrence of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: \"A rose is a rose is a rose\". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: \"rose\", \"is\" and \"a\". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, 'a', 'is' and 'rose', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type 'a', two of 'is' and three of 'rose'.\n\nThe need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them. Reflection on the simple case of occurrences of numerals is often helpful.\n\nIn typography, the type–token distinction is used to determine the presence of a text printed by movable type:\n\nThe word 'letters' was used three times in the above paragraph, each time in a different meaning. The word 'letters' is one of many words having \"type–token ambiguity\". This section disambiguates 'letters' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher Charles Sanders Peirce in 1906 using terminology that he established.\n\nThe letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.\n\nPeirce's type–token distinction, also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or concatenation theory. There is only one word type spelled el-ee-tee-tee-ee-ar, namely, 'letter'; but every time that word type is written, a new word token has been created.\n\nSome logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.\n\nThe word type 'letter' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type 'letter' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type 'letter'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.\n\nPeirce's original words are the following.\n\"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty 'thes' on a page, and, of course, they count as twenty words. In another sense of the word 'word,' however, there is but one word 'the' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice ... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. ... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies.\" – Peirce 1906, Ogden-Richards, 1923, 280-1.\n\nThese distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.\n\n\n"}
{"id": "31883", "url": "https://en.wikipedia.org/wiki?curid=31883", "title": "Uncertainty principle", "text": "Uncertainty principle\n\nIn quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position \"x\" and momentum \"p\", can be known.\n\nIntroduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. The formal inequality relating the standard deviation of position \"σ\" and the standard deviation of momentum \"σ\" was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928:\n\nwhere is the reduced Planck constant, ).\n\nHistorically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical \"explanation\" of quantum uncertainty. It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, \"the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology\". It must be emphasized that \"measurement\" does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.\n\nSince the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.\n\nThe uncertainty principle is not readily apparent on the macroscopic scales of everyday experience. So it is helpful to demonstrate how it applies to more easily understood physical situations. Two alternative frameworks for quantum physics offer different explanations for the uncertainty principle. The wave mechanics picture of the uncertainty principle is more visually intuitive, but the more abstract matrix mechanics picture formulates it in a way that generalizes more easily.\n\nMathematically, in wave mechanics, the uncertainty relation between position and momentum arises because the expressions of the wavefunction in the two corresponding orthonormal bases in Hilbert space are Fourier transforms of one another (i.e., position and momentum are conjugate variables). A nonzero function and its Fourier transform cannot both be sharply localized. A similar tradeoff between the variances of Fourier conjugates arises in all systems underlain by Fourier analysis, for example in sound waves: A pure tone is a sharp spike at a single frequency, while its Fourier transform gives the shape of the sound wave in the time domain, which is a completely delocalized sine wave. In quantum mechanics, the two key points are that the position of the particle takes the form of a matter wave, and momentum is its Fourier conjugate, assured by the de Broglie relation , where is the wavenumber.\n\nIn matrix mechanics, the mathematical formulation of quantum mechanics, any pair of non-commuting self-adjoint operators representing observables are subject to similar uncertainty limits. An eigenstate of an observable represents the state of the wavefunction for a certain measurement value (the eigenvalue). For example, if a measurement of an observable is performed, then the system is in a particular eigenstate of that observable. However, the particular eigenstate of the observable need not be an eigenstate of another observable : If so, then it does not have a unique associated measurement for it, as the system is not in an eigenstate of that observable.\n\nAccording to the de Broglie hypothesis, every object in the universe is a wave, i.e., a situation which gives rise to this phenomenon. The position of the particle is described by a wave function formula_1. The time-independent wave function of a single-moded plane wave of wavenumber \"k\" or momentum \"p\" is\n\nThe Born rule states that this should be interpreted as a probability density amplitude function in the sense that the probability of finding the particle between \"a\" and \"b\" is\n\nIn the case of the single-moded plane wave, formula_4 is a uniform distribution. In other words, the particle position is extremely uncertain in the sense that it could be essentially anywhere along the wave packet. \n\nOn the other hand, consider a wave function that is a sum of many waves, which we may write this as\n\nwhere \"A\" represents the relative contribution of the mode \"p\" to the overall total. The figures to the right show how with the addition of many plane waves, the wave packet can become more localized. We may take this a step further to the continuum limit, where the wave function is an integral over all possible modes\n\nwith formula_7 representing the amplitude of these modes and is called the wave function in momentum space. In mathematical terms, we say that formula_7 is the \"Fourier transform\" of formula_9 and that \"x\" and \"p\" are conjugate variables. Adding together all of these plane waves comes at a cost, namely the momentum has become less precise, having become a mixture of waves of many different momenta.\n\nOne way to quantify the precision of the position and momentum is the standard deviation \"σ\". Since formula_4 is a probability density function for position, we calculate its standard deviation.\n\nThe precision of the position is improved, i.e. reduced σ, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σ. Another way of stating this is that σ and σ have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the \"show\" button below to see a semi-formal derivation of the Kennard inequality using wave mechanics.\n(Ref ) \n\nIn matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the \"commutator\". For a pair of operators and , one defines their commutator as\nIn the case of position and momentum, the commutator is the canonical commutation relation\n\nThe physical meaning of the non-commutativity can be understood by considering the effect of the commutator on position and momentum eigenstates. Let formula_13 be a right eigenstate of position with a constant eigenvalue . By definition, this means that formula_14 Applying the commutator to formula_13 yields\nwhere is the identity operator.\n\nSuppose, for the sake of proof by contradiction, that formula_13 is also a right eigenstate of momentum, with constant eigenvalue . If this were true, then one could write\nOn the other hand, the above canonical commutation relation requires that\nThis implies that no quantum state can simultaneously be both a position and a momentum eigenstate.\n\nWhen a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is \"not\" a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, \n\nAs in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.\n\nThe most common general form of the uncertainty principle is the \"Robertson uncertainty relation\".\n\nFor an arbitrary Hermitian operator formula_22 we can associate a standard deviation\n\nwhere the brackets formula_24 indicate an expectation value. For a pair of operators formula_25 and formula_26, we may define their \"commutator\" as\n\nIn this notation, the Robertson uncertainty relation is given by\n\nThe Robertson uncertainty relation immediately follows from a slightly stronger inequality, the \"Schrödinger uncertainty relation\",\n\nwhere we have introduced the \"anticommutator\",\n\nSince the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below.\nSuppose we consider a quantum particle on a ring, where the wave function depends on an angular variable formula_41, which we may take to lie in the interval formula_42. Define \"position\" and \"momentum\" operators formula_25 and formula_26 by\n\nand\n\nwhere we impose periodic boundary conditions on formula_26. Note that the definition of formula_25 depends on our choice to have formula_41 range from 0 to formula_50. These operators satisfy the usual commutation relations for position and momentum operators, formula_51.\n\nNow let formula_52 be any of the eigenstates of formula_26, which are given by formula_54. Note that these states are normalizable, unlike the eigenstates of the momentum operator on the line. Note also that the operator formula_25 is bounded, since formula_41 ranges over a bounded interval. Thus, in the state formula_52, the uncertainty of formula_58 is zero and the uncertainty of formula_59 is finite, so that \nAlthough this result appears to violate the Robertson uncertainty principle, the paradox is resolved when we note that formula_52 is not in the domain of the operator formula_62, since multiplication by formula_41 disrupts the periodic boundary conditions imposed on formula_26. Thus, the derivation of the Robertson relation, which requires formula_65 and formula_66 to be defined, does not apply. (These also furnish an example of operators satisfying the canonical commutation relations but not the Weyl relations.)\n\nFor the usual position and momentum operators formula_67 and formula_68 on the real line, no such counterexamples can occur. As long as formula_69 and formula_70 are defined in the state formula_52, the Heisenberg uncertainty principle holds, even if formula_52 fails to be in the domain of formula_73 or of formula_74.\n\nConsider a one-dimensional quantum harmonic oscillator (QHO). It is possible to express the position and momentum operators in terms of the creation and annihilation operators:\n\nUsing the standard rules for creation and annihilation operators on the eigenstates of the QHO,\nthe variances may be computed directly,\nThe product of these standard deviations is then\n\nIn particular, the above Kennard bound is saturated for the ground state , for which the probability density is just the normal distribution.\n\nIn a quantum harmonic oscillator of characteristic angular frequency ω, place a state that is offset from the bottom of the potential by some displacement \"x\" as\nwhere Ω describes the width of the initial state but need not be the same as ω. Through integration over the , we can solve for the -dependent solution. After many cancelations, the probability densities reduce to\nwhere we have used the notation formula_85 to denote a normal distribution of mean μ and variance σ. Copying the variances above and applying trigonometric identities, we can write the product of the standard deviations as\n\nFrom the relations\n\nwe can conclude the following: (the right most equality holds only when Ω = \"ω\") .\n\nA coherent state is a right eigenstate of the annihilation operator,\nwhich may be represented in terms of Fock states as\n\nOne expects that the factor may be replaced by , \nwhich is only known if either or is convex.\n\nThe mathematician G. H. Hardy formulated the following uncertainty principle: it is not possible for and to both be \"very rapidly decreasing\". Specifically, if in formula_91 is such that\nand\n\nthen, if , while if , then there is a polynomial of degree such that\n\nThis was later improved as follows: if formula_96 is such that\n\nthen\nwhere is a polynomial of degree and is a real positive definite matrix.\n\nThis result was stated in Beurling's complete works without proof and proved in Hörmander (the case formula_99) and Bonami, Demange, and Jaming for the general case. Note that Hörmander–Beurling's version implies the case in Hardy's Theorem while the version by Bonami–Demange–Jaming covers the full strength of Hardy's Theorem. A different proof of Beurling's theorem based on Liouville's theorem appeared in\nref.\n\nA full description of the case as well as the following extension to Schwartz class distributions appears in ref.\n\nTheorem. If a tempered distribution formula_100 is such that\n\nand\nthen\nfor some convenient polynomial and real positive definite matrix of type .\n\nWerner Heisenberg formulated the uncertainty principle at Niels Bohr's institute in Copenhagen, while working on the mathematical foundations of quantum mechanics.\n\nIn 1925, following pioneering work with Hendrik Kramers, Heisenberg developed matrix mechanics, which replaced the ad hoc old quantum theory with modern quantum mechanics. The central premise was that the classical concept of motion does not fit at the quantum level, as electrons in an atom do not travel on sharply defined orbits. Rather, their motion is smeared out in a strange way: the Fourier transform of its time dependence only involves those frequencies that could be observed in the quantum jumps of their radiation.\n\nHeisenberg's paper did not admit any unobservable quantities like the exact position of the electron in an orbit at any time; he only allowed the theorist to talk about the Fourier components of the motion. Since the Fourier components were not defined at the classical frequencies, they could not be used to construct an exact trajectory, so that the formalism could not answer certain overly precise questions about where the electron was or how fast it was going.\n\nIn March 1926, working in Bohr's institute, Heisenberg realized that the non-commutativity implies the uncertainty principle. This implication provided a clear physical interpretation for the non-commutativity, and it laid the foundation for what became known as the Copenhagen interpretation of quantum mechanics. Heisenberg showed that the commutation relation implies an uncertainty, or in Bohr's language a complementarity. Any two variables that do not commute cannot be measured simultaneously—the more precisely one is known, the less precisely the other can be known. Heisenberg wrote:It can be expressed in its simplest form as follows: One can never know with perfect accuracy both of those two important factors which determine the movement of one of the smallest particles—its position and its velocity. It is impossible to determine accurately \"both\" the position and the direction and speed of a particle \"at the same instant\".\n\nIn his celebrated 1927 paper, \"Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik\" (\"On the Perceptual Content of Quantum Theoretical Kinematics and Mechanics\"), Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties Δx and Δp. Instead, he gave some plausible estimates in each case separately. In his Chicago lecture he refined his principle:\n\nKennard in 1927 first proved the modern inequality:\n\nwhere , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states.\n\nThroughout the main body of his original 1927 paper, written in German, Heisenberg used the word, \"Ungenauigkeit\" (\"indeterminacy\"),\nto describe the basic theoretical principle. Only in the endnote did he switch to the word, \"Unsicherheit\" (\"uncertainty\"). When the English-language version of Heisenberg's textbook, \"The Physical Principles of the Quantum Theory\", was published in 1930, however, the translation \"uncertainty\" was used, and it became the more commonly used term in the English language thereafter.\n\nThe principle is quite counter-intuitive, so the early students of quantum theory had to be reassured that naive measurements to violate it were bound always to be unworkable. One way in which Heisenberg originally illustrated the intrinsic impossibility of violating the uncertainty principle is by utilizing the observer effect of an imaginary microscope as a measuring device.\n\nHe imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it.\n\nThe combination of these trade-offs implies that no matter what photon wavelength and aperture size are used, the product of the uncertainty in measured position and measured momentum is greater than or equal to a lower limit, which is (up to a small numerical factor) equal to Planck's constant. Heisenberg did not care to formulate the uncertainty principle as an exact limit (which is elaborated below), and preferred to use it instead, as a heuristic quantitative statement, correct up to small numerical factors, which makes the radically new noncommutativity of quantum mechanics inevitable.\n\nThe Copenhagen interpretation of quantum mechanics and Heisenberg's Uncertainty Principle were, in fact, seen as twin targets by detractors who believed in an underlying determinism and realism. According to the Copenhagen interpretation of quantum mechanics, there is no fundamental reality that the quantum state describes, just a prescription for calculating experimental results. There is no way to say what the state of a system fundamentally is, only what the result of observations might be.\n\nAlbert Einstein believed that randomness is a reflection of our ignorance of some fundamental property of reality, while Niels Bohr believed that the probability distributions are fundamental and irreducible, and depend on which measurements we choose to perform. Einstein and Bohr debated the uncertainty principle for many years.\n\nWolfgang Pauli called Einstein's fundamental objection to the uncertainty principle \"the ideal of the detached observer\" (phrase translated from the German):\n\nThe first of Einstein's thought experiments challenging the uncertainty principle went as follows:\n\nBohr's response was that the wall is quantum mechanical as well, and that to measure the recoil to accuracy , the momentum of the wall must be known to this accuracy before the particle passes through. This introduces an uncertainty in the position of the wall and therefore the position of the slit equal to , and if the wall's momentum is known precisely enough to measure the recoil, the slit's position is uncertain enough to disallow a position measurement.\n\nA similar analysis with particles diffracting through multiple slits is given by Richard Feynman.\n\nBohr was present when Einstein proposed the thought experiment which has become known as Einstein's box. Einstein argued that \"Heisenberg's uncertainty equation implied that the uncertainty in time was related to the uncertainty in energy, the product of the two being related to Planck's constant.\" Consider, he said, an ideal box, lined with mirrors so that it can contain light indefinitely. The box could be weighed before a clockwork mechanism opened an ideal shutter at a chosen instant to allow one single photon to escape. \"We now know, explained Einstein, precisely the time at which the photon left the box.\" \"Now, weigh the box again. The change of mass tells the energy of the emitted light. In this manner, said Einstein, one could measure the energy emitted and the time it was released with any desired precision, in contradiction to the uncertainty principle.\"\n\nBohr spent a sleepless night considering this argument, and eventually realized that it was flawed. He pointed out that if the box were to be weighed, say by a spring and a pointer on a scale, \"since the box must move vertically with a change in its weight, there will be uncertainty in its vertical velocity and therefore an uncertainty in its height above the table. ... Furthermore, the uncertainty about the elevation above the earth's surface will result in an uncertainty in the rate of the clock,\" because of Einstein's own theory of gravity's effect on time.\n\"Through this chain of uncertainties, Bohr showed that Einstein's light box experiment could not simultaneously measure exactly both the energy of the photon and the time of its escape.\"\n\nBohr was compelled to modify his understanding of the uncertainty principle after another thought experiment by Einstein. In 1935, Einstein, Podolsky and Rosen (see EPR paradox) published an analysis of widely separated entangled particles. Measuring one particle, Einstein realized, would alter the probability distribution of the other, yet here the other particle could not possibly be disturbed. This example led Bohr to revise his understanding of the principle, concluding that the uncertainty was not caused by a direct interaction.\n\nBut Einstein came to much more far-reaching conclusions from the same thought experiment. He believed the \"natural basic assumption\" that a complete description of reality would have to predict the results of experiments from \"locally changing deterministic quantities\" and therefore would have to include more information than the maximum possible allowed by the uncertainty principle.\n\nIn 1964, John Bell showed that this assumption can be falsified, since it would imply a certain inequality between the probabilities of different experiments. Experimental results confirm the predictions of quantum mechanics, ruling out Einstein's basic assumption that led him to the suggestion of his \"hidden variables\". These hidden variables may be \"hidden\" because of an illusion that occurs during observations of objects that are too large or too small. This illusion can be likened to rotating fan blades that seem to pop in and out of existence at different locations and sometimes seem to be in the same place at the same time when observed. This same illusion manifests itself in the observation of subatomic particles. Both the fan blades and the subatomic particles are moving so fast that the illusion is seen by the observer. Therefore, it is possible that there would be predictability of the subatomic particles behavior and characteristics to a recording device capable of very high speed tracking...Ironically this fact is one of the best pieces of evidence supporting Karl Popper's philosophy of invalidation of a theory by falsification-experiments. That is to say, here Einstein's \"basic assumption\" became falsified by experiments based on Bell's inequalities. For the objections of Karl Popper to the Heisenberg inequality itself, see below.\n\nWhile it is possible to assume that quantum mechanical predictions are due to nonlocal, hidden variables, and in fact David Bohm invented such a formulation, this resolution is not satisfactory to the vast majority of physicists. The question of whether a random outcome is predetermined by a nonlocal theory can be philosophical, and it can be potentially intractable. If the hidden variables are not constrained, they could just be a list of random digits that are used to produce the measurement outcomes. To make it sensible, the assumption of nonlocal hidden variables is sometimes augmented by a second assumption—that the size of the observable universe puts a limit on the computations that these variables can do. A nonlocal theory of this sort predicts that a quantum computer would encounter fundamental obstacles when attempting to factor numbers of approximately 10,000 digits or more; a potentially achievable task in quantum mechanics.\n\nKarl Popper approached the problem of indeterminacy as a logician and metaphysical realist. He disagreed with the application of the uncertainty relations to individual particles rather than to ensembles of identically prepared particles, referring to them as \"statistical scatter relations\". In this statistical interpretation, a \"particular\" measurement may be made to arbitrary precision without invalidating the quantum theory. This directly contrasts with the Copenhagen interpretation of quantum mechanics, which is non-deterministic but lacks local hidden variables.\n\nIn 1934, Popper published \"Zur Kritik der Ungenauigkeitsrelationen\" (\"Critique of the Uncertainty Relations\") in \"Naturwissenschaften\", and in the same year \"Logik der Forschung\" (translated and updated by the author as \"The Logic of Scientific Discovery\" in 1959), outlining his arguments for the statistical interpretation. In 1982, he further developed his theory in \"Quantum theory and the schism in Physics\", writing:\n[Heisenberg's] formulae are, beyond all doubt, derivable \"statistical formulae\" of the quantum theory. But they have been \"habitually misinterpreted\" by those quantum theorists who said that these formulae can be interpreted as determining some upper limit to the \"precision of our measurements\". [original emphasis]\n\nPopper proposed an experiment to falsify the uncertainty relations, although he later withdrew his initial version after discussions with Weizsäcker, Heisenberg, and Einstein; this experiment may have influenced the formulation of the EPR experiment.\n\nThe many-worlds interpretation originally outlined by Hugh Everett III in 1957 is partly meant to reconcile the differences between Einstein's and Bohr's views by replacing Bohr's wave function collapse with an ensemble of deterministic and independent universes whose \"distribution\" is governed by wave functions and the Schrödinger equation. Thus, uncertainty in the many-worlds interpretation follows from each observer within any universe having no knowledge of what goes on in the other universes.\n\nSome scientists including Arthur Compton and Martin Heisenberg have suggested that the uncertainty principle, or at least the general probabilistic nature of quantum mechanics, could be evidence for the two-stage model of free will. One critique, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature. The standard view, however, is that this decoherence is overcome by both screening and decoherence-free subspaces found in biological cells.\n\nThere is reason to believe that violating the uncertainty principle also strongly implies the violation of the second law of thermodynamics.\n\n"}
