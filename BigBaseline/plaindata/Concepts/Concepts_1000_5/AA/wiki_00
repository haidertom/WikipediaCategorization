{"id": "34460605", "url": "https://en.wikipedia.org/wiki?curid=34460605", "title": "Andersen healthcare utilization model", "text": "Andersen healthcare utilization model\n\nThe Andersen Healthcare Utilization Model - is a conceptual model aimed at demonstrating the factors that lead to the use of health services. According to the model, usage of health services (including inpatient care, physician visits, dental care etc.) is determined by three dynamics: predisposing factors, enabling factors, and need. Predisposing factors can be characteristics such as race, age, and health beliefs. For instance, an individual who believes health services are an effective treatment for an ailment is more likely to seek care. Examples of enabling factors could be family support, access to health insurance, one's community etc. Need represents both perceived and actual need for health care services. The original model was developed by Ronald M. Andersen, a health services professor at UCLA, in 1968. The original model was expanded through numerous iterations and its most recent form models past the use of services to end at health outcomes and includes feedback loops.\n\nA major motivation for the development of the model was to offer measures of access. Andersen discusses four concepts within access that can be viewed through the conceptual framework. Potential access is the presence of enabling resources, allowing the individual to seek care if needed. Realized access is the actual use of care, shown as the outcome of interest in the earlier models. The Andersen framework also makes a distinction between equitable and inequitable access. Equitable access is driven by demographic characteristics and need whereas inequitable access is a result of social structure, health beliefs, and enabling resources.\n\nAndersen also introduces the concept of mutability of his factors. The idea here being that if a concept has a high degree of mutability (can be easily changed) perhaps policy would be justified in using its resources to do rather than a factor with low mutability. Characteristics that fall under demographics are quite difficult to change, however, enabling resources is assigned a high degree of mutability as the individual, community, or national policy can take steps to alter the level of enabling resources for an individual. For example, if the government decides to expand the Medicaid program an individual may experience an increase in enabling resources, which in turn may beget an increase in health services usage. The RAND Health Insurance Experiment (HIE) changed a highly mutable factor, out-of-pocket costs, which greatly changed individual rates of health services usage.\n\nThe initial behavior model was an attempt to study of why a family uses health services. However, due to the heterogeneity of family members the model focused on the individual rather than the family as the unit of analysis. Andersen also states that the model functions both to predict and explain use of health services.\n\nA second model was developed in the 1970s in conjunction with Aday and colleagues at the University of Chicago. This iteration includes systematic concepts of health care such as current policy, resources, and organization. The second generation model also extends the outcome of interest beyond utilization to consumer satisfaction.\n\nThe next generation of the model builds upon this idea by including health status (both perceived and evaluated) as outcomes alongside consumer satisfaction. Furthermore, this model include personal health practices as an antecedent to outcomes, acknowledging that it not solely use of health services that drives health and satisfaction. This model emphasizes a more public health approach of prevention, as advocated by Evans and Stoddart wherein personal health practices (i.e. smoking, diet, exercise) are included as a driving force towards health outcomes.\n\nThe 6th iteration of Andersen’s conceptual framework focuses on the individual as the unit of analysis and goes beyond health care utilization, adopting health outcomes as the endpoint of interest. This model is further differentiated from its predecessors by using a feedback loop to illustrate that health outcomes may affect aspects such as health beliefs, and need. It added genetic susceptibility as a predisposing determinant and quality of life as an outcome By using the framework’s relationships we can determine the directionality of the effect following a change in an individual’s characteristics or environment. For example, if one experiences an increase in need as a result of an infection, the Andersen model predicts this will lead to an increased use of services (all else equal). One potential change for a future iteration of this model is to add genetic information under predisposing characteristics. As genetic information becomes more readily available it seems likely this could impact health services usage, as well as health outcomes, beyond what is already accounted for in the current model.\n\nThe model has been criticized for not paying enough attention to culture and social interaction but Andersen argues this social structure is included in the \"predisposing characteristics\" component. Another criticism was the overemphasis of need and at the expense of health beliefs and social structure. However, Andersen argues need itself is a social construct. This is why need is split into perceived and evaluated. Where evaluated need represents a more measurable/objective need, perceived need is partly determined by health beliefs, such as whether or not people think their condition is serious enough to seek health services. Another limitation of the model is its emphasis on health care utilization or adopting health outcomes as a dichotomous factor, present or not present. Other help-seeking models also consider the type of help source, including informal sources. More recent work has taken help-seeking behaviors further, and more real-world, by including online and other non-face-to-face sources.\n"}
{"id": "2235265", "url": "https://en.wikipedia.org/wiki?curid=2235265", "title": "Anna Karenina principle", "text": "Anna Karenina principle\n\nThe Anna Karenina principle states that a deficiency in any one of a number of factors dooms an endeavor to failure. Consequently, a successful endeavor (subject to this principle) is one where every possible deficiency has been avoided.\n\nThe name of the principle derives from Leo Tolstoy's book \"Anna Karenina\", which begins:\n\nAll happy families are alike; each unhappy family is unhappy in its own way.In other words: in order to be happy, a family must be successful on \"each and every one\" of \"a\" \"range\" of criteria e.g.: sexual attraction, money issues, parenting, religion, in-laws. Failure on only \"one\" of these counts leads to \"un\"happiness. Thus there are more ways for a family to be unhappy than happy.\n\nIn statistics, the term \"Anna Karenina principle\" is used to describe significance tests: there are any number of ways in which a dataset may violate the null hypothesis and only one in which all the assumptions are satisfied.\n\nThe Anna Karenina principle was popularized by Jared Diamond in his book \"Guns, Germs and Steel\". Diamond uses this principle to illustrate why so few wild animals have been successfully domesticated throughout history, as a deficiency in any one of a great number of factors can render a species undomesticable. Therefore, all successfully domesticated species are not so because of a particular positive trait, but because of a lack of any number of possible negative traits. In chapter 9, six groups of reasons for failed domestication of animals are defined:\n\n\nMoore describes applications of the \"Anna Karenina principle\" in ecology:\n\nSuccessful ecological risk assessments are all alike; every unsuccessful ecological risk assessment fails in its own way. Tolstoy posited a similar analogy in his novel Anna Karenina : \"Happy families are all alike; every unhappy family is unhappy in its own way.\" By that, Tolstoy meant that for a marriage to be happy, it had to succeed in several key aspects. Failure on even one of these aspects, and the marriage is doomed . . . the Anna Karenina principle also applies to ecological risk assessments involving multiple stressors.\n\nMuch earlier, \"Aristotle\" states the same principle in the \"Nicomachean Ethics\" (Book 2):\n\nAgain, it is possible to fail in many ways (for evil belongs to the class of the unlimited, as the Pythagoreans conjectured, and good to that of the limited), while to succeed is possible only in one way (for which reason also one is easy and the other difficult – to miss the mark easy, to hit it difficult); for these reasons also, then, excess and defect are characteristic of vice, and the mean of virtue; For men are good in but one way, but bad in many.\n\nMany experiments and observations of groups of humans, animals, trees, grassy plants, stockmarket prices, and changes in the banking sector proved the modified Anna Karenina principle.\n\nBy studying the dynamics of correlation and variance in many systems facing external, or environmental, factors, we can typically, even before obvious symptoms of crisis appear, predict when one might occur, as correlation between individuals increases, and, at the same time, variance (and volatility) goes up... All well-adapted systems are alike, all non-adapted systems experience maladaptation in their own way... But in the chaos of maladaptation, there is an order. It seems, paradoxically, that as systems become more different they actually become more correlated within limits.\n\nThis effect is proved for many systems: from the adaptation of healthy people to a change in climate conditions to the analysis of fatal outcomes in oncological and cardiological clinics. The same effect is found in the stock market. The applicability of these two statistical indicators of stress, simultaneous increase of variance and correlations, for diagnosis of social stress in large groups was examined in the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years.\n\nVladimir Arnold in his book \"Catastrophe Theory\" describes \"The Principle of Fragility of Good Things\" which in a sense supplements the Principle of Anna Karenina: good systems must meet simultaneously a number of requirements; therefore, they are more fragile:\n\n... for systems belonging to the singular part of the stability boundary a small change of the parameters is more likely to send the system into the unstable region than into the stable region. This is a manifestation of a general principle stating that all good things (e.g. stability) are more fragile than bad things. It seems that in good situations a number of requirements must hold simultaneously, while to call a situation bad even one failure suffices. \n"}
{"id": "66975", "url": "https://en.wikipedia.org/wiki?curid=66975", "title": "Apophatic theology", "text": "Apophatic theology\n\nApophatic theology, also known as negative theology, is a form of theological thinking and religious practice which attempts to approach God, the Divine, by negation, to speak only in terms of what may not be said about the perfect goodness that is God. It forms a pair together with cataphatic theology, which approaches God or the Divine by affirmations or positive statements about what God \"is\".\n\nThe apophatic tradition is often, though not always, allied with the approach of mysticism, which aims at the vision of God, the perception of the divine reality beyond the realm of ordinary perception.\n\n\"Apophatic\", (adjective); from ἀπόφημι \"apophēmi\", meaning \"to deny\". From \"Online Etymology Dictionary\": \n\"Via negativa\" or \"via negationis\" (Latin), \"negative way\" or \"by way of denial\". The negative way forms a pair together with the \"kataphatic\" or positive way. According to Deirdre Carabine,\nAccording to Fagenblat, \"negative theology is as old as philosophy itself;\" elements of it can be found in Plato's \"unwritten doctrines,\" while it is also present in Neo-Platonic, Gnostic and early Christian writers. A tendency to apophatic thought can also be found in Philo of Alexandria.\n\nAccording to Carabine, \"apophasis proper\" in Greek thought starts with Neo-Platonism, with its speculations about the nature of the One, culminating in the works of Proclus. According to Carabine, there are two major points in the development of apophatic theology, namely the fusion of the Jewish tradition with Platonic philosophy in the writings of Philo, and the works of Dionysius the Pseudo-Areopagite, who infused Christian thought with Neo-Platonic ideas.\n\nThe Early Church Fathers were influenced by Philo, and Meredith even states that Philo \"is the real founder of the apophatic tradition.\" Yet, it was with Pseudo-Dionysius the Areopagite and Maximus the Confessor, whose writings shaped both Hesychasm, the contemplative tradition of the Eastern Orthodox Churches, and the mystical traditions of western Europe, that apophatic theology became a central element of Christian theology and contemplative practice.\n\nFor the ancient Greeks, knowledge of the gods was essential for proper worship. Poets had an important responsibility in this regard, and a central question was how knowledge of the Divine forms can be attained. Epiphany played an essential role in attaining this knowledge. Xenophanes (c. 570 – c. 475 BC) noted that the knowledge of the Divine forms is restrained by the human imagination, and Greek philosophers realized that this knowledge can only be mediated through myth and visual representations, which are culture-dependent.\n\nAccording to Herodotus (484–425 BCE), Homer and Hesiod (between 750 and 650 BC) taught the Greek the knowledge of the Divine bodies of the Gods. The ancient Greek poet Hesiod (between 750 and 650 BC) describes in his \"Theogony\" the birth of the gods and creation of the world, which became an \"ur-text for programmatic, first-person epiphanic narratives in Greek literature,\" but also \"explores the necessary limitations placed on human access to the divine.\" According to Platt, the statement of the Muses who grant Hesiod knowledge of the Gods \"actually accords better with the logic of apophatic religious thought.\"\n\nParmenides (fl. late sixth or early fifth century BC), in his poem \"On Nature\", gives an account of a revelation on two ways of inquiry. \"The way of conviction\" explores Being, true reality (\"what-is\"), which is \"What is ungenerated and deathless,/whole and uniform, and still and perfect.\" \"The way of opinion\" is the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. His distinction between unchanging Truth and shifting opinion is reflected in Plato's allegory of the Cave. Together with the Biblical story of Moses's ascent of Mount Sinai, it is used by Gregory of Nyssa and Pseudo-Dionysius the Areopagite to give a Christian account of the ascent of the soul toward God. Cook notes that Parmenides poem is a religious account of a mystical journey, akin to the mystery cults, giving a philosophical form to a religious outlook. Cook further notes that the philosopher's task is to \"attempt through 'negative' thinking to tear themselves loose from all that frustrates their pursuit of wisdom.\"\n\nPlato (428/427 or 424/423 – 348/347 BCE), \"deciding for Parmenides against Heraclitus\" and his theory of eternal change, had a strong influence on the development of apophatic thought.\n\nPlato further explored Parmenides's idea of timeless truth in his dialogue \"Parmenides\", which is a treatment of the eternal forms, \"Truth, Beauty and Goodness\", which are the real aims for knowledge. The Theory of Forms is Plato's answer to the problem \"how one unchanging reality or essential being can admit of many changing phenomena (and not just by dismissing them as being mere illusion).\"\n\nIn \"The Republic\", Plato argues that the \"real objects of knowledge are not the changing objects of the senses, but the immutable Forms,\" stating that the \"Form of the Good\" is the highest object of knowledge. His argument culminates in the Allegory of the Cave, in which he argues that humans are like prisoners in a cave, who can only see shadows of the Real, the \"Form of the Good\". Humans are to be educated to search for knowledge, by turning away from their bodily desires toward higher contemplation, culminating in an intellectual understanding or apprehension of the Forms, c.q. the \"first principles of all knowledge.\"\n\nAccording to Cook, the \"Theory of Forms\" has a theological flavour, and had a strong influence on the ideas of his Neo-Platonist interpreters Proclus and Plotinus. The pursuit of \"Truth, Beauty and Goodness\" became a central element in the apophatic tradition, but nevertheless, according to Carabine \"Plato himself cannot be regarded as the founder of the negative way.\" Carabine warns not to read later Neo-Platonic and Christian understandings into Plato, and notes that Plato did not identify his Forms with \"one transcendent source,\" an identification which his later interpreters made.\n\nMiddle Platonism (1st century BCE - 3rd century CE) further investigated Plato's \"Unwritten Doctrines,\" which drew on Pythagoras' first principles of the Monad and the Dyad (matter). Middle Platonism proposed a hierarchy of being, with God as its first principle at its top, identifying it with Plato's \"Form of the Good\". An influential proponent of Middle Platonism was Philo (c.25 BCE–c. 50 CE), who employed Middle Platonic philosophy in his interpretation of the Hebrew scriptures, and asserted a strong influence on early Christianity. According to Craig D. Allert, \"Philo made a monumental contribution to the creation of a vocabulary for use in negative statements about God.\" For Philo, God is undescribable, and he uses terms which emphasize God's transcendence.\n\nNeo-Platonism was a mystical or contemplative form of Platonism, which \"developed outside the mainstream of Academic Platonism.\" It started with the writings of Plotinus (204/5–270), and ended with the closing of the Platonic Academy by Emperor Justinian in 529 CE, when the pagan traditions were ousted. It is a product of Hellenistic syncretism, which developed due to the crossover between Greek thought and the Jewish scriptures, and also gave birth to Gnosticism. Proclus was the last head of the Platonic Academy; his student Pseudo-Dinosysius had a far-stretching Neo-Platonic influence on Christianity and Christian mysticism.\n\nPlotinus (204/5–270) was the founder of Neo-Platonism. In the Neo-Platonic philosophy of Plotinus and Proclus, the first principle became even more elevated as a radical unity, which was presented as an unknowable Absolute. For Plotinus, \"the One\" is the first principle, from which everything else emanates. He took it from Plato's writings, identifying the Good of the \"Republic\", as the cause of the other Forms, with \"the One\" of the first hypothesis of the second part of the \"Parmenides\". For Plotinus, \"the One\" precedes the Forms, and \"is beyond Mind and indeed beyond Being.\" From \"the One\" comes the Intellect, which contains all the Forms. \"The One\" is the principle of Being, while the Forms are the principle of the essence of beings, and the intelligibility which can recognize them as such. Plotinus's third principle is Soul, the desire for objects external to the person. The highest satisfaction of desire is the contemplation of \"the One\", which unites all existents \"as a single, all-pervasive reality.\"\n\n\"The One\" is radically simple, and does not even have self-knowledge, since self-knowledge would imply multiplicity. Nevertheless, Plotinus does urge for a search for the Absolute, turning inward and becoming aware of the \"presence of the intellect in the human soul,\" initiating an ascent of the soul by abstraction or \"taking away,\" culminating in a sudden appearance of \"the One\". In the \"Enneads\" Plotinus writes: \nCarabine notes that Plotinus' apophasis is not just a mental exercise, an acknowledgement of the unknowability of \"the One\", but a means to \"extasis\" and an ascent to \"the unapproachable light that is God.\" Pao-Shen Ho, investigating what are Plotinus' methods for reaching \"henosis\", concludes that \"Plotinus' mystical teaching is made up of two practices only, namely philosophy and negative theology.\" According to Moore, Plotinus appeals to the \"non-discursive, intuitive faculty of the soul,\" by \"calling for a sort of prayer, an invocation of the deity, that will permit the soul to lift itself up to the unmediated, direct, and intimate contemplation of that which exceeds it (V.1.6).\" Pao-Shen Ho further notes that \"for Plotinus, mystical experience is irreducible to philosophical arguments.\" The argumentation about \"henosis\" is preceded by the actual experience of it, and can only be understood when \"henosis\" has been attained. Ho further notes that Plotinus's writings have a didactic flavour, aiming to \"bring his own soul and \"the souls of others\" by way of Intellect to union with the One.\" As such, the \"Enneads\" as a spiritual or ascetic teaching device, akin to \"The Cloud of Unknowing\", demonstrating the methods of philosophical and apophatic inquiry. Ultimately, this leads to silence and the abandonment of all intellectual inquiry, leaving contemplation and unity.\n\nProclus (412-485) introduced the terminology which is being used in apophatic and cataphatic theology. He did this in the second book of his \"Platonic Theology\", arguing that Plato states that \"the One\" can be revealed \"through analogy,\" and that \"through negations [\"dia ton apophaseon\"] its transcendence over everything can be shown.\" For Proclus, apophatic and cataphonic theology form a contemplatory pair, with the apophatic approach corresponding to the manifestation of the world from \"the One\", and cataphonic theology corresponding to the return to \"the One\". The analogies are affirmations which direct us toward \"the One\", while the negations underlie the confirmations, being closer to \"the One\". According to Luz, Proclus also attracted students from other faiths, including the Samaritan Marinus. Luz notes that \"Marinus' Samaritan origins with its Abrahamic notion of a single ineffable Name of God () should also have been in many ways compatible with the school's ineffable and apophatic divine principle.\"\nThe Book of Revelation 8:1 mentions \"the silence of the perpetual choir in heaven.\" According to Dan Merkur,\nThe Early Church Fathers were influenced by Philo (c. 25 BCE – c. 50 CE), who saw Moses as \"the model of human virtue and Sinai as the archetype of man's ascent into the \"luminous darkness\" of God.\" His interpretation of Moses was followed by Clement of Alexandria, Origen, the Cappadocian Fathers, Pseudo-Dionysius, and Maximus the Confessor.\n\nGod's appearance to Moses in the burning bush was often elaborated on by the Early Church Fathers, especially Gregory of Nyssa (c. 335 – c. 395), realizing the fundamental unknowability of God; an exegesis which continued in the medieval mystical tradition. Their response is that, although God is unknowable, Jesus as person can be followed, since \"following Christ is the human way of seeing God.\"\n\nClement of Alexandria (c. 150 – c. 215) was an early proponent of apophatic theology. According to R.A. Baker, in Clement's writings the term \"theoria\" develops further from a mere intellectual \"seeing\" toward a spirutal form of contemplation. Clement's apophatic theology or philosophy is closely related to this kind of \"theoria\" and the \"mystic vision of the soul.\" For Clement, God is transcendent and immanent. According to Baker, Clement's apophaticism is mainly driven by Biblical texts, but by the Platonic tradition. His conception of an ineffable God is a synthesis of Plato and Philo, as seen from a Biblical perspective. According to Osborne, it is a synthesis in a Biblical framework; according to Baker, while the Platonic tradition accounts for the negative approach, the Biblical tradition accounts for the positive approach. \"Theoria\" and abstraction is the means to conceive of this ineffable God; it is preceded by dispassion.\n\nAccording to Tertullian (c. 155 – c. 240),\nSaint Cyril of Jerusalem (313-386), in his \"Catechetical Homilies\", states: \nAugustine of Hippo (354-430) defined God \"aliud, aliud valde\", meaning \"other, completely other\", in \"Confessions\" 7.10.16.\n\nApophatic theology found its most influential expression in the works of Pseudo-Dionysius the Areopagite (late 5th to early 6th century), a student of Proclus (412-485), combining a Christian worldview with Neo-Platonic ideas. He is a constant factor in the contemplative tradition of the eastern Orthodox Churches, and from the 9th century onwards his writings also had a strong impact on western mysticism.\n\nDionysius the Areopagite was a pseudonym, taken from Acts of the Apostles chapter 17, in which Paul gives a missionary speech to the court of the Areopagus in Athens. In Paul makes a reference to an altar-inscription, dedicated to the Unknown God, \"a safety measure honoring foreign gods still unknown to the Hellenistic world.\" For Paul, Jesus Christ is this unknown God, and as a result of Paul's speech Dionysius the Areopagite converts to Christianity. Yet, according to Stang, for Pseudo-Dionysius the Areopagite Athens is also the place of Neo-Platonic wisdom, and the term \"unknown God\" is a reversal of Paul's preaching toward an integration of Christianity with Neo-Platonism, and the union with the \"unknown God.\"\n\nAccording to Corrigan and Harrington, \"Dionysius' central concern is how a triune God, ... who is utterly unknowable, unrestricted being, beyond individual substances, beyond even goodness, can become manifest to, in, and through the whole of creation in order to bring back all things to the hidden darkness of their source.\" Drawing on Neo-Platonism, Pseudo-Dionysius described humans ascend to divinity as a process of purgation, illumination and union. Another Neo-Platonic influence was his description of the cosmos as a series of hierarchies, which overcome the distance between God and humans.\n\nIn Orthodox Christianity apophatic theology is taught as superior to cataphatic theology. The fourth-century Cappadocian Fathers stated a belief in the existence of God, but an existence unlike that of everything else: everything else that exists was created, but the Creator transcends this existence, is uncreated. The essence of God is completely unknowable; mankind can know God only through His energies. Gregory of Nyssa (c.335-c.395), John Chrysostom (c. 349 – 407), and Basil the Great (329-379) emphasized the importance of negative theology to an orthodox understanding of God. John of Damascus (c.675/676–749) employed negative theology when he wrote that positive statements about God reveal \"not the nature, but the things around the nature.\"\n\nMaximus the Confessor (580-622) took over Pseudo-Dionysius' ideas, and had a strong influence on the theology and contemplative practices of the Eastern Orthodox Churches. Gregory Palamas (1296–1359) formulated the definite theology of Hesychasm, the Orthodox practices of contemplative prayer and theosis, \"deification.\"\n\nInfluential modern Eastern Orthodox theologians are Vladimir Lossky, John Meyendorff, John S. Romanides and Georges Florovsky. Lossky argues, based on his reading of Dionysius and Maximus Confessor, that positive theology is always inferior to negative theology which is a step along the way to the superior knowledge attained by negation. This is expressed in the idea that mysticism is the expression of dogmatic theology \"par excellence\".\n\nAccording to Lossky, outside of directly revealed knowledge through Scripture and Sacred Tradition, such as the Trinitarian nature of God, God in His essence is beyond the limits of what human beings (or even angels) can understand. He is transcendent in essence (\"ousia\"). Further knowledge must be sought in a direct experience of God or His indestructible energies through \"theoria\" (vision of God). According to Aristotle Papanikolaou, in Eastern Christianity, God is immanent in his hypostasis or existences.\n\nNegative theology has a place in the Western Christian tradition as well. The 9th-century theologian John Scotus Erigena wrote: \n\nWhen he says \"\"He is not anything\" and \"God is not\"\", Scotus does not mean that there is no God, but that God cannot be said to exist in the way that creation exists, i.e. that God is uncreated. He is using apophatic language to emphasise that God is \"other\".\n\nTheologians like Meister Eckhart and Saint John of the Cross (San Juan de la Cruz) exemplify some aspects of or tendencies towards the apophatic tradition in the West. The medieval work, \"The Cloud of Unknowing\" and Saint John's \"Dark Night of the Soul\" are particularly well known. In 1215 apophatism became the official position of the Catholic Church, which, on the basis of Scripture and church tradition, during the Fourth Lateran Council formulated the following dogma:\nThomas Aquinas was born ten years later (1225-1274) and, although in his \"Summa Theologica\" he quotes Pseudo-Dionysius 1,760 times, his reading in a neo-Aristotelian key of the conciliar declaration overthrew its meaning inaugurating the \"analogical way\" as \"tertium\" between \"via negativa\" and \"via positiva\": the \"via eminentiae\" (see also \"analogia entis\"). According to Adrian Langdon,\nAccording to \"Catholic Encyclopedia\", the \"Doctor Angelicus\" and the scholastici declare [that] \nSince then Thomism has played a decisive role in resizing the negative or apophatic tradition of the magisterium.\n\nApophatic statements are still crucial to many modern theologians, restarting in 1800s by Søren Kierkegaard (see his concept of the infinite qualitative distinction) up to Rudolf Otto and Karl Barth (see their idea of \"Wholly Other\", i.e. \"ganz Andere\" or \"totaliter aliter\").\n\nC. S. Lewis, in his book \"Miracles\" (1947), advocates the use of negative theology when first thinking about God, in order to cleanse our minds of misconceptions. He goes on to say we must then refill our minds with the truth about God, untainted by mythology, bad analogies or false mind-pictures.\n\nThe mid-20th century Dutch philosopher Herman Dooyeweerd, who is often associated with a neo-Calvinistic tradition, provides a philosophical foundation for understanding why we can never absolutely know God, and yet, paradoxically, truly know something of God. Dooyeweerd made a sharp distinction between theoretical and pre-theoretical attitudes of thought. Most of the discussion of knowledge of God presupposes theoretical knowledge, in which we reflect and try to define and discuss. Theoretical knowing, by its very nature, is never absolute, always depends on religious presuppositions, and cannot grasp either God or the law side. Pre-theoretical knowing, on the other hand, is intimate engagement, and exhibits a diverse range of aspects. Pre-theoretical intuition, on the other hand, can grasp at least the law side. Knowledge of God, as God wishes to reveal it, is pre-theoretical, immediate and intuitive, never theoretical in nature. The philosopher Leo Strauss considered that the Bible, for example, should be treated as pre-theoretical (everyday) rather than theoretical in what it contains.\n\nIvan Illich (1926-2002), the historian and social critic, can be read as an apophatic theologian, according to a longtime collaborator, Lee Hoinacki, in a paper presented in memory of Illich, called \"Why Philia?\"\n\nAccording to Deirdre Carabine, negative theology has become a hot topic since the 1990s, resulting from a broad effort in the 19 and 20th century to portray Plato as a mysticist, which revived the interest in Neoplatonism and negative theology.\n\nKaren Armstrong, in her book \"The Case for God\" (2009), notices a recovery of apophatic theology in postmodern theology.\n\nThe Arabic term for \"negative theology\" is \"lahoot salbi\", which is a \"system of theology\" or \"nizaam al lahoot\" in Arabic. Different traditions/doctrine schools in Islam called Kalam schools (see Islamic schools and branches) use different theological approaches or \"nizaam al lahoot\" in approaching God in Islam (\"Allah\", Arabic الله) or the ultimate reality. The \"lahoot salbi\" or \"negative theology\" involves the use of \"ta'til\", which means \"negation,\" and the followers of the Mu'tazili school of Kalam, founded by Imam Wasil ibn Ata, are often called the \"Mu'attili\", because they are frequent users of the \"ta'tili\" methodology.\n\nRajab ʿAlī Tabrīzī, an Iranian and Shiat philosopher and mystic of the 17th century. instilled a radical apophatic theology in a generation of philosophers and theologians whose influence extended into the Qajar period. Mulla Rajab affirmed the completely unknowable,\nunqualifiable, and attributeless nature of God and upheld a general view concerning God’s attributes which can only be negatively ‘affirmed’, by means of the via\nnegativa.\n\nShia Islam adopted \"negative theology\". In the words of the Persian Ismaili missionary, Abu Yaqub al-Sijistani: \"There does not exist a tanzíh [\"transcendence\"] more brilliant and more splendid than that by which we establish the absolute transcendence of our Originator through the use of these phrases in which a negative and a negative of a negative apply to the thing denied.\" Early Sunni scholars who held to a literal reading of the Quran and hadith rejected this view, adhering to its opposite, believing that the Attributes of God such as \"Hand\", \"Foot\" etc... should be taken literally and that, therefore, God is like a human being. Today, most Sunnis, like the Ash'ari and Maturidi, adhere to a middle path between negation and anthropomorphism.\n\nMaimonides (1135/1138-1204) was \"the most influential medieval Jewish exponent of the \"via negativa\".\" Maimonides, but also Samuel ibn Tibbon, draw on Bahya ibn Paquda, who shows that our inability to describe God is related to the fact of His absolute unity. God, as the entity which is \"truly One\" (האחד האמת), must be free of properties and is thus unlike anything else and indescribable. According to Rabbi Yosef Wineberg, Maimonides stated that \"[God] is knowledge,\" and saw His Essence, Being and knowledge as completely one, \"a perfect unity and not a composite at all.\" Wineberg quotes Maimonides as stating\nIn \"The Guide for the Perplexed\" Maimonides stated:\nAccording to Fagenblat, it is only in the modern period that negative theology really gains importance in Jewish thought. Yeshayahu Leibowitz (1903-1994) was a prominent modern exponent of Jewish negative theology. According to Leibowitz, a person's faith is his commitment to obey God, meaning God's commandments, and this has nothing to do with a person’s image of God. This must be so because Leibowitz thought that God cannot be described, that God's understanding is not man's understanding, and thus all the questions asked of God are out of place.\n\nThere are interesting parallels in Indian thought, which developed largely separate from Western thought. Early Indian philosophical works which have apophatic themes include the Principal Upanishads (800 BCE to the start of common era) and the Brahma Sutras (from 450 BCE and 200 CE). An expression of negative theology is found in the Brihadaranyaka Upanishad, where Brahman is described as \"neti neti\" or \"neither this, nor that\". Further use of apophatic theology is found in the Brahma Sutras, which state:\n\nBuddhist philosophy has also strongly advocated the way of negation, beginning with the Buddha's own theory of anatta (not-atman, not-self) which denies any truly existent and unchanging essence of a person. Madhyamaka is a Buddhist philosophical school founded by Nagarjuna (2nd-3rd century CE), which is based on a fourfold negation of all assertions and concepts and promotes the theory of emptiness (shunyata). Apophatic assertions are also an important feature of Mahayana sutras, especially the prajñaparamita genre. These currents of negative theology are visible in all forms of Buddhism.\n\nApophatic movements in medieval Hindu philosophy are visible in the works of Shankara (8th century), a philosopher of Advaita Vedanta (non-dualism), and Bhartṛhari (5th century), a grammarian. While Shankara holds that the transcendent noumenon, Brahman, is realized by the means of negation of every phenomenon including language, Bhartṛhari theorizes that language has both phenomenal and noumenal dimensions, the latter of which manifests Brahman.\n\nIn Advaita, Brahman is defined as being Nirguna or without qualities. Anything imaginable or conceivable is not deemed to be the ultimate reality. The Taittiriya hymn speaks of Brahman as \"one where the mind does not reach\". Yet the Hindu scriptures often speak of Brahman's positive aspect. For instance, Brahman is often equated with bliss. These contradictory descriptions of Brahman are used to show that the attributes of Brahman are similar to ones experienced by mortals, but not the same. \n\nNegative theology also figures in the Buddhist and Hindu polemics. The arguments go something like this – Is Brahman an object of experience? If so, how do you convey this experience to others who have not had a similar experience? The only way possible is to relate this unique experience to common experiences while explicitly negating their sameness.\n\nEven though the \"via negativa\" essentially rejects theological understanding in and of itself as a path to God, some have sought to make it into an intellectual exercise, by describing God only in terms of what God is not. One problem noted with this approach is that there seems to be no fixed basis on deciding what God is not, unless the Divine is understood as an abstract experience of full aliveness unique to each individual consciousness, and universally, the perfect goodness applicable to the whole field of reality. Apophatic theology is often accused of being a version of atheism or agnosticism, since it cannot say truly that God exists. \"The comparison is crude, however, for conventional atheism treats the existence of God as a predicate that can be denied (“God is nonexistent”), whereas negative theology denies that God has predicates\". \"God or the Divine is\" without being able to attribute qualities about \"what He is\" would be the prerequisite of positive theology in negative theology that distinguishes theism from atheism. \"Negative theology is a complement to, not the enemy of, positive theology\". Since religious experience—or consciousness of the holy or sacred, is not reducible to other kinds of human experience, an abstract understanding of religious experience cannot be used as evidence or proof that religious discourse or praxis can have no meaning or value. In apophatic theology, the negation of theisms in the \"via negativa\" also requires the negation of their correlative atheisms if the dialectical method it employs is to maintain integrity.\n\n\n\n\n\n\n\n\n\n \n\n\n"}
{"id": "29358535", "url": "https://en.wikipedia.org/wiki?curid=29358535", "title": "Comply or explain", "text": "Comply or explain\n\nComply or explain is a regulatory approach used in the United Kingdom, Germany, the Netherlands and other countries in the field of corporate governance and financial supervision. Rather than setting out binding laws, government regulators (in the UK, the Financial Reporting Council, in Germany, under the Aktiengesetz) set out a code, which listed companies may either comply with, or if they do not comply, explain publicly why they do not. The UK Corporate Governance Code, the German Corporate Governance Code (or Deutscher Corporate Governance Kodex) and the Dutch Corporate Governance Code 'Code Tabaksblat' () use this approach in setting minimum standards for companies in their audit committees, remuneration committees and recommendations for how good companies should divide authority on their boards.\n\nThe purpose of \"comply or explain\" is to \"let the market decide\" whether a set of standards is appropriate for individual companies. Since a company may deviate from the standard, this approach rejects the view that \"one size fits all\", but because of the requirement of disclosure of explanations to market investors, anticipates that if investors do not accept a company's explanations, then investors will sell their shares, hence creating a \"market sanction\", rather than a legal one. The concept was first introduced after the recommendations of the Cadbury Report of 1992.\n\n"}
{"id": "6978", "url": "https://en.wikipedia.org/wiki?curid=6978", "title": "Concept", "text": "Concept\n\nConcepts are mental representations, abstract objects or abilities that make up the fundamental building blocks of thoughts and beliefs. They play an important role in all aspects of cognition.\n\nIn contemporary philosophy, there are at least three prevailing ways to understand what a concept is:\n\n\nConcepts can be organized into a hierarchy, higher levels of which are termed \"superordinate\" and lower levels termed \"subordinate\". Additionally, there is the \"basic\" or \"middle\" level at which people will most readily categorize a concept. For example, a basic-level concept would be \"chair\", with its superordinate, \"furniture\", and its subordinate, \"easy chair\".\n\nA concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.\n\nConcepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word \"concept\" often just means any idea.\n\nWithin the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called \"mental representations\" (colloquially understood as \"ideas in the mind\"). Mental representations, in turn, are the building blocks of what are called \"propositional attitudes\" (colloquially understood as the stances or perspectives we take towards ideas, be it \"believing\", \"doubting\", \"wondering\", \"accepting\", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.\n\nA central question in the study of concepts is the question of what concepts \"are\". Philosophers construe this question as one about the ontology of concepts – what they are really like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.\n\nPlatonist views of the mind construe concepts as abstract objects,\n\nThere is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept \"dog\" is philosophically distinct from the things in the world grouped by this concept – or the reference class or extension. Concepts that can be equated to a single word are called \"lexical concepts\".\n\nStudy of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.\n\nIn the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word \"moon\" (a concept) is not the large, bright, shape-changing object up in the sky, but only \"represents\" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.\n\nKant declared that human minds possess pure or \"a priori\" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things \"in general\", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an \"a priori\" concept can relate to individual phenomena, in a manner analogous to an \"a posteriori\" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction \"a posteriori concepts\" (meaning concepts that arise out of experience). An empirical or an \"a posteriori\" concept is a general representation (\"Vorstellung\") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)\n\nA concept is a common feature or characteristic. Kant investigated the way that empirical \"a posteriori\" concepts are created.\nIn cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or \"recollections\", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.\n\nPlato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.\n\nGottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7).\n\nAccording to Carl Benjamin Boyer, in the introduction to his \"The History of the Calculus and its Conceptual Development\", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.\n\nIn a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.\n\nConcepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. (\"Sort\" is itself another word for concept, and \"sorting\" thus means to organise into concepts.)\n\nThe classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both \"necessary\" and \"sufficient\" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example \"bachelor\" is said to be defined by \"unmarried\" and \"man\". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the \"law of the excluded middle\", which means that there are no partial members of a class, you are either in or out.\n\nThe classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy – concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic \"Time Without Change\" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.\n\nGiven that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:\n\nPrototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as \"family resemblances\". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member – the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. According to Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain, some of these are; visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.\n\nTheory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory–Theory of concepts is responding to some of the issues of prototype theory and classic theory.\n\nAccording to the theory of ideasthesia (or \"sensing concepts\"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.\n\nThere is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.\n\nThe term \"concept\" is traced back to 1554–60 (Latin \"\" – \"something conceived\").\n\n"}
{"id": "178942", "url": "https://en.wikipedia.org/wiki?curid=178942", "title": "Conceptual art", "text": "Conceptual art\n\nConceptual art, sometimes simply called conceptualism, is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic, technical, and material concerns. Some works of conceptual art, sometimes called installations, may be constructed by anyone simply by following a set of written instructions. This method was fundamental to American artist Sol LeWitt's definition of Conceptual art, one of the first to appear in print:\n\nTony Godfrey, author of \"Conceptual Art (Art & Ideas)\" (1998), asserts that conceptual art questions the nature of art, a notion that Joseph Kosuth elevated to a definition of art itself in his seminal, early manifesto of conceptual art, \"Art after Philosophy\" (1969). The notion that art should examine its own nature was already a potent aspect of the influential art critic Clement Greenberg's vision of Modern art during the 1950s. With the emergence of an exclusively language-based art in the 1960s, however, conceptual artists such as Art & Language, Joseph Kosuth (who became the american editor of Art-Language), and Lawrence Weiner began a far more radical interrogation of art than was previously possible (see below). One of the first and most important things they questioned was the common assumption that the role of the artist was to create special kinds of material objects.\nThrough its association with the Young British Artists and the Turner Prize during the 1990s, in popular usage, particularly in the UK, \"conceptual art\" came to denote all contemporary art that does not practice the traditional skills of painting and sculpture. It could be said that one of the reasons why the term \"conceptual art\" has come to be associated with various contemporary practices far removed from its original aims and forms lies in the problem of defining the term itself. As the artist Mel Bochner suggested as early as 1970, in explaining why he does not like the epithet \"conceptual\", it is not always entirely clear what \"concept\" refers to, and it runs the risk of being confused with \"intention\". Thus, in describing or defining a work of art as conceptual it is important not to confuse what is referred to as \"conceptual\" with an artist's \"intention\".\n\nThe French artist Marcel Duchamp paved the way for the conceptualists, providing them with examples of prototypically conceptual works — the readymades, for instance. The most famous of Duchamp's readymades was \"Fountain\" (1917), a standard urinal-basin signed by the artist with the pseudonym \"R.Mutt\", and submitted for inclusion in the annual, un-juried exhibition of the Society of Independent Artists in New York (which rejected it). The artistic tradition does not see a commonplace object (such as a urinal) as art because it is not made by an artist or with any intention of being art, nor is it unique or hand-crafted. Duchamp's relevance and theoretical importance for future \"conceptualists\" was later acknowledged by US artist Joseph Kosuth in his 1969 essay, \"Art after Philosophy\", when he wrote: \"All art (after Duchamp) is conceptual (in nature) because art only exists conceptually\".\n\nIn 1956 the founder of Lettrism, Isidore Isou, developed the notion of a work of art which, by its very nature, could never be created in reality, but which could nevertheless provide aesthetic rewards by being contemplated intellectually. This concept, also called \"Art esthapériste\" (or \"infinite-aesthetics\"), derived from the infinitesimals of Gottfried Wilhelm Leibniz – quantities which could not actually exist except conceptually. The current incarnation () of the Isouian movement, Excoördism, self-defines as the art of the infinitely large and the infinitely small.\n\nIn 1961 the term \"concept art\", coined by the artist Henry Flynt in his article bearing the term as its title, appeared in a proto-Fluxus publication \"An Anthology of Chance Operations\". \nHowever, it assumed a different meaning when employed by Joseph Kosuth and by the English Art and Language group, who discarded the conventional art object in favour of a documented critical inquiry, that began in Art-Language The Journal of conceptual art in 1969, into the artist's social , philosophical and psychological status. By the mid-1970s they had produced publications, indices, performances, texts and paintings to this end. In 1970 \"Conceptual Art and Conceptual Aspects\", the first dedicated conceptual-art exhibition, took place at the New York Cultural Center.\n\nConceptual art emerged as a movement during the 1960s – in part as a reaction against formalism as then articulated by the influential New York art critic Clement Greenberg. According to Greenberg Modern art followed a process of progressive reduction and refinement toward the goal of defining the essential, formal nature of each medium. Those elements that ran counter to this nature were to be reduced. The task of painting, for example, was to define precisely what kind of object a painting truly is: what makes it a painting and nothing else. As it is of the nature of paintings to be flat objects with canvas surfaces onto which colored pigment is applied, such things as figuration, 3-D perspective illusion and references to external subject matter were all found to be extraneous to the essence of painting, and ought to be removed.\n\nSome have argued that conceptual art continued this \"dematerialization\" of art by removing the need for objects altogether,\nwhile others, including many of the artists themselves, saw conceptual art as a radical break with Greenberg's kind of formalist Modernism. Later artists continued to share a preference for art to be self-critical, as well as a distaste for illusion. However, by the end of the 1960s it was certainly clear that Greenberg's stipulations for art to continue within the confines of each medium and to exclude external subject matter no longer held traction.\nConceptual art also reacted against the commodification of art; it attempted a subversion of the gallery or museum as the location and determiner of art, and the art market as the owner and distributor of art. Lawrence Weiner said: \"Once you know about a work of mine you own it. There's no way I can climb inside somebody's head and remove it.\" Many conceptual artists' work can therefore only be known about through documentation which is manifested by it, e.g. photographs, written texts or displayed objects, which some might argue are not in themselves the art. It is sometimes (as in the work of Robert Barry, Yoko Ono, and Weiner himself) reduced to a set of written instructions describing a work, but stopping short of actually making it—emphasising the idea as more important than the artifact. This reveals an explicit preference for the \"art\" side of the ostensible dichotomy between art and craft, where art, unlike craft, takes place within and engages historical discourse: for example, Ono's \"written instructions\" make more sense alongside other conceptual art of the time.\n\nLanguage was a central concern for the first wave of conceptual artists of the 1960s and early 1970s. Although the utilisation of text in art was in no way novel, only in the 1960s did the artists Lawrence Weiner, Edward Ruscha, Joseph Kosuth, Robert Barry, and Art & Language begin to produce art by exclusively linguistic means. Where previously language was presented as one kind of visual element alongside others, and subordinate to an overarching composition (e.g. Synthetic Cubism), the conceptual artists used language in place of brush and canvas, and allowed it to signify in its own right. Of Lawrence Weiner's works Anne Rorimer writes, \"The thematic content of individual works derives solely from the import of the language employed, while presentational means and contextual placement play crucial, yet separate, roles.\"\n\nThe British philosopher and theorist of conceptual art Peter Osborne suggests that among the many factors that influenced the gravitation toward language-based art, a central role for conceptualism came from the turn to linguistic theories of meaning in both Anglo-American analytic philosophy, and structuralist and post structuralist Continental philosophy during the middle of the twentieth century. This linguistic turn \"reinforced and legitimized\" the direction the conceptual artists took. Osborne also notes that the early conceptualists were the first generation of artists to complete degree-based university training in art. Osborne later made the observation that contemporary art is \"post-conceptual\" in a public lecture delivered at the Fondazione Antonio Ratti, Villa Sucota in Como on July 9, 2010. It is a claim made at the level of the ontology of the work of art (rather than say at the descriptive level of style or movement).\n\nThe American art historian Edward A. Shanken points to the example of Roy Ascott who \"powerfully demonstrates the significant intersections between conceptual art and art-and-technology, exploding the conventional autonomy of these art-historical categories.\" Ascott, the British artist most closely associated with cybernetic art in England, was not included in Cybernetic Serendipity because his use of cybernetics was primarily conceptual and did not explicitly utilize technology. Conversely, although his essay on the application of cybernetics to art and art pedagogy, \"The Construction of Change\" (1964), was quoted on the dedication page (to Sol Lewitt) of Lucy R. Lippard's seminal \"Six Years: The Dematerialization of the Art Object from 1966 to 1972\", Ascott's anticipation of and contribution to the formation of conceptual art in Britain has received scant recognition, perhaps (and ironically) because his work was too closely allied with art-and-technology. Another vital intersection was explored in Ascott's use of the thesaurus in 1963 , which drew an explicit parallel between the taxonomic qualities of verbal and visual languages – a concept would be taken up in Joseph Kosuth's \"Second Investigation, Proposition 1\" (1968) and Mel Ramsden's \"Elements of an Incomplete Map\" (1968).\n\n\"By adopting language as their exclusive medium, Weiner, Barry, Wilson, Kosuth and Art & Language were able to sweep aside the vestiges of authorial presence manifested by formal invention and the handling of materials.\"\nAn important difference between conceptual art and more \"traditional\" forms of art-making goes to the question of artistic skill. Although skill in the handling of traditional media often plays little role in conceptual art, it is difficult to argue that no skill is required to make conceptual works, or that skill is always absent from them. John Baldessari, for instance, has presented realist pictures that he commissioned professional sign-writers to paint; and many conceptual performance artists (e.g. Stelarc, Marina Abramović) are technically accomplished performers and skilled manipulators of their own bodies. It is thus not so much an absence of skill or hostility toward tradition that defines conceptual art as an evident disregard for conventional, modern notions of authorial presence and of individual artistic expression.\n\nThe first wave of the \"conceptual art\" movement extended from approximately 1967 to 1978. Early \"concept\" artists like Henry Flynt, Robert Morris, and Ray Johnson influenced the later, widely accepted movement of conceptual art. Conceptual artists like Dan Graham, Hans Haacke, and Lawrence Weiner have proven very influential on subsequent artists, and well known contemporary artists such as Mike Kelley or Tracey Emin are sometimes labeled \"second- or third-generation\" conceptualists, or \"post-conceptual\" artists.\n\nMany of the concerns of the conceptual art movement have been taken up by contemporary artists. While they may or may not term themselves \"conceptual artists\", ideas such as anti-commodification, social and/or political critique, and ideas/information as medium continue to be aspects of contemporary art, especially among artists working with installation art, performance art, net.art and electronic/digital art.\n\n\n\nBooks\n\n\nEssays\n\n\nExhibition catalogues\n\n\n"}
{"id": "633037", "url": "https://en.wikipedia.org/wiki?curid=633037", "title": "Conceptualism", "text": "Conceptualism\n\nIn metaphysics, conceptualism is a theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. Intermediate between nominalism and realism, the conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside the mind's perception of them. Conceptualism is anti-realist about abstract objects, just like immanent realism is (their difference being that immanent realism does not deny the mind-independence of universals, like conceptualism does).\n\nThe evolution of late scholastic terminology has led to the emergence of conceptualism, which stemmed from doctrines that were previously considered to be nominalistic. The terminological distinction was made in order to stress the difference between the claim that universal mental acts correspond with universal intentional objects and the perspective that dismissed the existence of universals outside the mind. The former perspective of rejection of objective universality was distinctly defined as conceptualism.\n\nPeter Abélard was a medieval thinker whose work is currently classified as having the most potential in representing the roots of conceptualism. Abélard’s view denied the existence of determinate universals within things. William of Ockham was another famous late medieval thinker who had a strictly conceptualist solution to the metaphysical problem of universals. He argued that abstract concepts have no \"fundamentum\" outside the mind.\n\nIn the 17th century conceptualism gained favour for some decades especially among the Jesuits: Hurtado de Mendoza, Rodrigo de Arriaga and Francisco Oviedo are the main figures. Although the order soon returned to the more realist philosophy of Francisco Suárez, the ideas of these Jesuits had a great impact on the early modern philosophy.\n\nConceptualism was either explicitly or implicitly embraced by most of the early modern thinkers, including René Descartes, John Locke, Baruch Spinoza, Gottfried Wilhelm Leibniz, George Berkeley, and David Hume – often in a quite simplified form if compared with the elaborate scholastic theories.\n\nSometimes the term is applied even to the radically different philosophy of Immanuel Kant, who holds that universals have no connection with external things because they are exclusively produced by our \"a priori\" mental structures and functions.\n\nIn late modern philosophy, conceptualist views were held by G. W. F. Hegel.\n\nEdmund Husserl's philosophy of mathematics has been construed as a form of conceptualism.\n\nConceptualist realism (a view put forward by David Wiggins in 1980) states that our conceptual framework maps reality.\n\nThough separate from the historical debate regarding the status of universals, there has been significant debate regarding the conceptual character of experience since the release of \"Mind and World\" by John McDowell in 1994. McDowell's touchstone is the famous refutation that Wilfrid Sellars provided for what he called the \"Myth of the Given\"—the notion that all empirical knowledge is based on certain assumed or 'given' items, such as sense data. Thus, in rejecting the Myth of the Given, McDowell argues for perceptual conceptualism, according to which perceptual content is conceptual \"from the ground up\", that is, all perceptual experience is a form of conceptual experience. McDowell's philosophy of justification is considered a form of foundationalism: it is a form of foundationalism because it allows that certain judgements are warranted by experience and it is a coherent form of this view because it maintains that experience can warrant certain judgements because experience is irreducibly conceptual.\n\nA clear motivation of contemporary conceptualism is that the kind of perception that rational creatures like humans enjoy is unique in the fact that it has conceptual character. McDowell explains his position:\n\nI have urged that our perceptual relation to the world is conceptual all the way out to the world’s impacts on our receptive capacities. The idea of the conceptual that I mean to be invoking is to be understood in close connection with the idea of rationality, in the sense that is in play in the traditional separation of mature human beings, as rational animals, from the rest of the animal kingdom. Conceptual capacities are capacities that belong to their subject’s rationality. So another way of putting my claim is to say that our perceptual experience is permeated with rationality. I have also suggested, in passing, that something parallel should be said about our agency.\n\nMcDowell's conceptualism, though rather distinct (philosophically and historically) from conceptualism's genesis, shares the view that universals are not \"given\" in perception from outside the sphere of reason. Particular objects are perceived, as it were, already infused with conceptuality stemming from the spontaneity of the rational subject herself.\n\nThe application of the term \"perceptual conceptualism\" to Kant's philosophy of perception is debatable. Other scholars have argued for a rival interpretation of Kant's work termed perceptual non-conceptualism.\n\n"}
{"id": "26167139", "url": "https://en.wikipedia.org/wiki?curid=26167139", "title": "Definitionism", "text": "Definitionism\n\nDefinitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.\n"}
{"id": "36188092", "url": "https://en.wikipedia.org/wiki?curid=36188092", "title": "Fa (concept)", "text": "Fa (concept)\n\nFa (;) is a concept in Chinese philosophy that covers ethics, logic, and law. It can be translated as \"law\" in some contexts, but more often as \"model\" or \"standard.\" First gaining importance in the Mohist school of thought, the concept was principally elaborated in Legalism. In Han Fei's philosophy, the king is the sole source of \"fa\" (law), taught to the common people so that there would be a harmonious society free of chance occurrences, disorder, and \"appeal to privilege\". High officials were not to be held above \"fa\" (law or protocol), nor were they to be allowed to independently create their own \"fa\", uniting both executive fiat and rule of law.\n\nXunzi, a philosopher that would end up being foundational in Han dynasty Confucianism, also took up \"fa\", suggesting that it could only be properly assessed by the Confucian sage (ruler), and that the most important \"fa\" were the very rituals that Mozi had ridiculed for their ostentatious waste and lack of benefit for the people at large.\n\nThe concept of \"fa\" first gained importance in the Mohist school of thought. To Mozi, a standard must stand \"three tests\" in order to determine its efficacy and morality. The first of these tests was its origin; if the standard had precedence in the actions or thought of the semi-mythological sage kings of the Xia dynasty whose examples are frequently cited in classical Chinese philosophy. The second test was one of validity; does the model stand up to evidence in the estimation of the people? The third and final test was one of applicability; this final one is a utilitarian estimation of the net good that, if implemented, the standard would have on both the people and the state.\n\nThe third test speaks to the fact that to the Mohists, a \"fa\" was not simply an abstract model, but an active tool. The real-world use and practical application of \"fa\" were vital. Yet \"fa\" as models were also used in later Mohist logic as principles used in deductive reasoning. As classical Chinese philosophical logic was based on analogy rather than syllogism, \"fa\" were used as benchmarks to determine the validity of logical claims through comparison. There were three \"fa\" in particular that were used by these later Mohists (or \"Logicians\") to assess such claims, which were mentioned earlier. The first was considered a \"root\" standard, a concern for precedence and origin. The second, a \"source\", a concern for empiricism. The third, a \"use\", a concern for the consequence and pragmatic utility of a standard. These three \"fa\" were used by the Mohists to both promote social welfare and denounce ostentation or wasteful spending.\n\n"}
{"id": "11273068", "url": "https://en.wikipedia.org/wiki?curid=11273068", "title": "Growing block universe", "text": "Growing block universe\n\nAccording to the growing block universe theory of time (or the growing block view), the past and present exist and the future does not exist. The present is an objective property, to be compared with a moving spotlight. By the passage of time more of the world comes into being; therefore, the block universe is said to be growing. The growth of the block is supposed to happen in the present, a very thin slice of spacetime, where more of spacetime is continually coming into being.\n\nThe growing block view is an alternative to both eternalism (according to which past, present, and future all exist) and presentism (according to which only the present exists). It is held to be closer to common-sense intuitions than the alternatives. C. D. Broad was a proponent of the theory (1923). Some modern defenders are Michael Tooley (in 1997) and Peter Forrest (in 2004).\n\nRecently several philosophers, David Braddon-Mitchell (2004), Craig Bourne and Trenton Merricks have noted that if the growing block view is correct then we have to conclude that we do not know whether now is now. (The first occurrence of \"now\" is an indexical and the second occurrence of \"now\" is the objective tensed property. Their observation implies the following sentence: \"This part of spacetime has the property of being present\".)\n\nTake Socrates discussing, in the past, with Gorgias, and at the same time thinking that the discussion is occurring now. According to the growing block view, tense is a real property of the world so his thought is about now, the objective present. He thinks, tenselessly, that his thought is occurring on the edge of being. But we know he is wrong because he is in the past; he does not know that now is now. But how can we be sure we are not in the same position? There is nothing special with Socrates. Therefore, we do not know whether now is now.\n\nHowever, some have argued that there is an ontological distinction between the past and the present. For instance, Forrest (2004) argues that although there exists a past, it is lifeless and inactive. Consciousness, as well as the flow of time, is not active within the past and can only occur at the boundary of the block universe in which the present exists.\n\n\n"}
{"id": "369116", "url": "https://en.wikipedia.org/wiki?curid=369116", "title": "Hume's fork", "text": "Hume's fork\n\nHume's fork is an explanation, developed by later philosophers, of David Hume's 1730s division of \"relations of ideas\" from \"matters of fact and real existence\". A distinction is made between necessary versus contingent (concerning reality), versus (concerning knowledge), and analytic versus synthetic (concerning language). Relations of abstract ideas align on one side (necessary, \"a priori\", analytic), whereas concrete truths align on the other (contingent, \"a posteriori\", synthetic).\n\nThe \"necessary\" is generally true in all possible worlds—usually by mere logical validity—whereas the \"contingent\" hinges on the way the real world is. The \"a priori\" is knowable before or without, whereas the \"a posteriori\" is knowable only after or through, experience in an area of interest. The \"analytic\" is a statement true by virtue of its terms' meanings, and therefore a tautology—necessarily true but uninformative—whereas the \"synthetic\" is true by its terms' meanings in relation to a state of facts. In other words, analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. Philosophers have used the terms differently, and there is debate over whether there is a legitimate distinction. \n\nHume's strong empiricism, as in Hume's fork as well as Hume's problem of induction, was taken as a threat to Newton's theory of motion. Immanuel Kant responded with rationalism in his 1781 \"Critique of Pure Reason\", where Kant attributed to the mind a causal role in sensory experience by the mind's aligning the environmental input by arranging those sense data into the experience of space and time. Kant thus reasoned existence of the synthetic \"a priori\"—combining meanings of terms with states of facts, yet known true without experience of the particular instance—replacing the two prongs of Hume's fork with a three-pronged-fork thesis (Kant's pitchfork) and thus saving Newton's law of universal gravitation.\n\nIn 1919, Newton's theory fell to Einstein's general theory of relativity. In the late 1920s, the logical positivists rejected Kant's synthetic \"a priori\" and asserted Hume's fork, so called, while hinging it at language—the analytic/synthetic division—while presuming that by holding to analyticity, they could develop a logical syntax entailing, as a consequence of Hume's fork, both necessity and aprioricity, thus restricting science to claims verifiable as either false or true. In the early 1950s, Willard Van Orman Quine undermined the analytic/synthetic division by explicating ontological relativity, as every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. By the early 1970s, Saul Kripke established the necessary \"a posteriori\", since if the Morning Star and the Evening Star are the same star, they are the same star by necessity, but this is known true by a human only through relevant experience.\n\nHume's fork remains basic in Anglo-American philosophy. Many deceptions and confusions are foisted by surreptitious or unwitting conversion of a synthetic claim to an analytic claim, rendered true by necessity but merely a tautology, for instance the \"No true Scotsman\" move. Simply put, Hume's fork has limitations. Related concerns are Hume's distinction of demonstrative versus probable reasoning and Hume's law. Hume makes other, important two-category distinctions, such as beliefs versus desires and as impressions versus ideas.\n\nThe first distinction is between two different areas of human study:\n\nHume's fork is often stated in such a way that statements are divided up into two types:\n\n\nIn modern terminology, members of the first group are known as analytic propositions and members of the latter as synthetic propositions. This terminology comes from Kant (Introduction to \"Critique of Pure Reason\", Section IV).\n\nInto the first class fall statements such as \"all bodies are extended\", \"all bachelors are unmarried\", and truths of mathematics and logic. Into the second class fall statements like \"the sun rises in the morning\", and \"all bodies have mass\".\n\nHume wants to prove that certainty does not exist in science. First, Hume notes that statements of the second type can never be entirely certain, due to the fallibility of our senses, the possibility of deception (see e.g. the modern brain in a vat theory) and other arguments made by philosophical skeptics. It is always logically possible that any given statement about the world is false.\n\nSecond, Hume claims that our belief in cause-and-effect relationships between events is not grounded on reason, but rather arises merely by habit or custom. Suppose one states: \"Whenever someone on earth lets go of a stone it falls.\" While we can grant that in every instance thus far when a rock was dropped on Earth it went down, this does not make it logically necessary that in the future rocks will fall when in the same circumstances. Things of this nature rely upon the future conforming to the same principles which governed the past. But that isn't something that we can know based on past experience—all past experience could tell us is that in the past, the future has resembled the past.\n\nThird, Hume notes that relations of ideas can be used only to prove other relations of ideas, and mean nothing outside of the context of how they relate to each other, and therefore tell us nothing about the world. Take the statement \"An equilateral triangle has three sides of equal length.\" While some earlier philosophers (most notably Plato and Descartes) held that logical statements such as these contained the most formal reality, since they are always true and unchanging, Hume held that, while true, they contain no formal reality, because the truth of the statements rests on the definitions of the words involved, and not on actual things in the world, since there is no such thing as a true triangle or exact equality of length in the world. So for this reason, relations of ideas cannot be used to prove matters of fact.\n\nThe results claimed by Hume as consequences of his fork are drastic. According to him, relations of ideas can be proved with certainty (by using other relations of ideas), however, they don't really mean anything about the world. Since they don't mean anything about the world, relations of ideas cannot be used to prove matters of fact. Because of this, matters of fact have no certainty and therefore cannot be used to prove anything. Only certain things can be used to prove other things for certain, but only things about the world can be used to prove other things about the world. But since we can't cross the fork, nothing is both certain and about the world, only one or the other, and so it is impossible to prove something about the world with certainty.\n\nIf accepted, Hume's fork makes it pointless to try to prove the existence of God (for example) as a matter of fact. If God is not literally made up of physical matter, and does not have an observable effect on the world, making a statement about God is not a matter of fact. Therefore, a statement about God must be a relation of ideas. In this case if we prove the statement \"God exists,\" it doesn't really tell us anything about the world; it is just playing with words. It is easy to see how Hume's fork voids the causal argument and the ontological argument for the existence of a non-observable God. However, this does not mean that the validity of Hume's fork would imply that God definitely does not exist, only that it would imply that the existence of God cannot be proven as a matter of fact without worldly evidence. \n\nHume rejected the idea of any meaningful statement that did not fall into this schema, saying:\nIf we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, Does it contain any abstract reasoning concerning quantity or number? No. Does it contain any experimental reasoning concerning matter of fact and existence? No. Commit it then to the flames: for it can contain nothing but sophistry and illusion. — \"An Enquiry Concerning Human Understanding\"\n"}
{"id": "13793087", "url": "https://en.wikipedia.org/wiki?curid=13793087", "title": "KK thesis", "text": "KK thesis\n\nThe KK thesis or KK principle is a principle of epistemic logic which states that \"If you know that p is the case then you know that you know that p is the case.\" In formal notation the principle can be stated as: \"Kp→KKp\" (literally: \"Knowing p implies the knowing of knowing p\").\n\n\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "8599305", "url": "https://en.wikipedia.org/wiki?curid=8599305", "title": "Logic model", "text": "Logic model\n\nLogic models are hypothesized descriptions of the chain of causes and effects (see Causality) leading to an outcome of interest (e.g. prevalence of cardiovascular diseases, annual traffic collision, etc). While they can be in a narrative form, logic model usually take form in a graphical depiction of the \"if-then\" (causal) relationships between the various elements leading to the outcome. However, the logic model is more than the graphical depiction: it is also the theories, scientific evidences, assumptions and beliefs that support it and the various processes behind it.\n\nLogic models are used by planners, funders, managers and evaluators of programs and interventions to plan, communicate, implement and evaluate them. They are being employed as well by health scientific community to organize and conduct literature reviews such as systematic reviews. Domains of application are various, e.g. waste management, poultry inspection, business education, heart disease and stroke prevention. Since they are used in various contexts and for different purposes, their typical components and levels of complexity varies in literature (compare for example the W.K. Kellogg Foundation presentation of logic model, mainly aimed for evaluation, and the numerous types of logic models in the Intervention Mapping framework)). In addition, depending of the purpose of the logic model, elements depicted and the relationships between them is more or less detailed.\n\nCiting Funnell and Rogers account, Joy A. Frechtling (2015) encyclopedic article traces logic model underpinnings in the 1950s. Patricia J. Rogers (2005) encyclopedic article rather trace it back to 1967 Edward A. Suchman book about evaluative research. Both encyclopedic article and LeCroy one (2018) mention an increasing interest, usage and publications about the subject.\n\nOne of the most important uses of the logic model is for program planning. It is suggested to use the logic model to focus on the intended outcomes of a particular program. The guiding questions change from \"what is being done?\" to \"what needs to be done\"? McCawley suggests that by using this new reasoning, a logic model for a program can be built by asking the following questions in sequence:\n\n\nBy placing the focus on ultimate outcomes or results, planners can \"think backward\" through the logic model to identify how best to achieve the desired results. Here it helps managers to 'plan with the end in mind', rather than just consider inputs (e.g. budgets, employees) or the tasks that must be done.\n\nThe logic model is often used in government or not-for-profit organizations, where the mission and vision are not aimed at achieving a financial benefit. Traditionally, government programs were described only in terms of their budgets. It is easy to measure the amount of money spent on a program, but this is a poor indicator of outcomes. Likewise it is relatively easy to measure the amount of work done (e.g. number of workers or number of years spent), but the workers may have just been 'spinning their wheels' without getting very far in terms of ultimate results or outcomes.\n\nHowever, nature of outcomes varies. To measure the progress toward outcomes, some initiatives may require an ad hoc measurement instrument. In addition, in programs such as in education or social programs, outcomes are usually in the long-term and may requires numerous intermediate changes (attitudes, social norm, industry practices, etc.) to advance progressively toward the outcomes.\n\nBy making clear the intended outcomes and the causal pathways leading to them, a program logic model provides the basis upon which planners and evaluators can develop a measurement plan and adequate instruments. Instead of only looking at the outcome progress, planners can open the \"black box\" and examine if the intermediate outcomes progress as planned. In addition, the pathways of numerous outcomes are still largely misunderstood due their complexity, their unpredictability and lack of scientific / practical evidences. Therefore, with proper research design, one may not only assess the progress of intermediate outcomes, but evaluate as well if the program theory of change is accurate, i.e. is successful change of an intermediate outcomes provokes the hypothesized subsequent effects in the causal pathway. Finally, outcomes may easily be achieved through processes independent of the program and an evaluation of those outcomes would suggest program success when in fact external outputs were responsible for the outcomes.\n\nMany authors and guides use the following template when speaking about logic model:\n\nMany refinements and variations have been added to the basic template. For example, many versions of logic models set out a series of outcomes/impacts, explaining in more detail the logic of how an intervention contributes to intended or observed results. Others often distinguish short-term, medium-term and long-term results, and between direct and indirect results.\n\nBartholomew et al. Intervention Mapping approach makes an extensive use of logic model through the whole life-cycle of a health promotion program. Since this method can start from as far as a vague desired outcomes (authors example is a city whose actors decide to address \"health issues\" of the city), planners goes through various steps in order to develop effective interventions and properly evaluate them (see Intervention Mapping entry for a more detailed account). Distinguishable but closely interweave logic models with different purposes are being developed through the process:\n\n\nEvaluators thereafter use the logic model of the intervention to design a proper evaluation plan to assess implementation, impact and efficiency.\n\nBy describing work in this way, managers have an easier way to define the work and measure it. Performance measures can be drawn from any of the steps. One of the key insights of the logic model is the importance of measuring final outcomes or results, because it is quite possible to waste time and money (inputs), \"spin the wheels\" on work activities, or produce outputs without achieving desired outcomes. It is these outcomes (impacts, long-term results) that are the only justification for doing the work in the first place. For commercial organizations, outcomes relate to profit. For not-for-profit or governmental organizations, outcomes relate to successful achievement of mission or program goals.\n\nThere are some potential disadvantages of logic models due to tendencies toward oversimplification. These include:\n\n"}
{"id": "3785733", "url": "https://en.wikipedia.org/wiki?curid=3785733", "title": "Marginal rate of technical substitution", "text": "Marginal rate of technical substitution\n\nIn microeconomic theory, the Marginal Rate of Technical Substitution (MRTS)—or Technical Rate of Substitution (TRS)—is the amount by which the quantity of one input has to be reduced (formula_1) when one extra unit of another input is used (formula_2), so that output remains constant (formula_3).\n\nformula_4\n\nwhere formula_5 and formula_6 are the marginal products of input 1 and input 2, respectively.\n\nAlong an isoquant, the MRTS shows the rate at which one input (e.g. capital or labor) may be substituted for another, while maintaining the same level of output. Thus the MRTS is the absolute value of the slope of an isoquant at the point in question.\n\nWhen relative input usages are optimal, the marginal rate of technical substitution is equal to the relative unit costs of the inputs, and the slope of the isoquant at the chosen point equals the slope of the isocost curve (see Conditional factor demands). It is the rate at which one input is substituted for another to maintain the same level of output.\n\n"}
{"id": "1280458", "url": "https://en.wikipedia.org/wiki?curid=1280458", "title": "Marginal revenue", "text": "Marginal revenue\n\nIn microeconomics, marginal revenue (R') is the additional revenue that will be generated by increasing product sales by one unit. It can also be described as the unit revenue the last item sold has generated for the firm. In a perfectly competitive market, the additional revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good. This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price. However, a monopoly determines the entire industry's sales. As a result, it will have to lower the price of all units sold to increase sales by 1 unit. Therefore, the marginal revenue generated is always lower than the price the firm is able to charge for the unit sold, since each reduction in price causes unit revenue to decline on every good the firm sells. The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. \n\nA firms profits will be maximized when marginal revenue (MR) equals marginal cost (MC). If formula_1 then a firm should increase output for more profits, if formula_2 then a firm should decrease output for additional profits. A firm should choose the output level which is profit maximizing under perfect competition theory formula_3.\n\nMarginal revenue is equal to the ratio of the change in revenue for some change in quantity sold to that change in quantity sold. This can also be represented as a derivative when the change in quantity sold becomes arbitrarily small. More formally, define the revenue function to be the following\n\nBy the product rule, marginal revenue is then given by\n\nFor a firm facing perfect competition, price does not change with quantity sold (formula_6), so marginal revenue is equal to price. For a monopoly, the price decreases with quantity sold (formula_7), so marginal revenue is less than price (for positive formula_8).\n\nThe marginal revenue curve is affected by the same factors as the demand curve - changes in income, change in the prices of complements and substitutes, change in populations. These factors can cause the R curve to shift and rotate.\n\nThe relationship between marginal revenue and the elasticity of demand by the firm's customers can be derived as follows:\n\nwhere e is the price elasticity of demand. If demand is inelastic (e < 1) then R' will be negative, because to sell a marginal (infinitesimal) unit the firm would have to lower the selling price so much that it would lose more revenue on the pre-existing units than it would gain on the incremental unit. If demand is elastic (e > 1) R' will be positive, because the additional unit would not drive down the price by so much. If the firm is a perfect competitor, so that it is so small in the market that its quantity produced and sold has no effect on the price, then the price elasticity of demand is negative infinity, and marginal revenue simply equals the (market-determined) price.\n\nProfit maximization requires that a firm produces where marginal revenue equals marginal costs. Firm managers are unlikely to have complete information concerning their marginal revenue function or their marginal costs. Fortunately, the profit maximization conditions can be expressed in a “more easily applicable form” or rule of thumb.\n\nMarkup is the difference between price and marginal cost. The formula states that markup as a percentage of price equals the negative of the inverse of elasticity of demand. Alternatively, the relationship can be expressed as:\n\nThus if e is - 2 and mc is $5.00 then price is $10.00.\n\n(<R> - C')/ <R> = - 1/e is called the Lerner index after economist Abba Lerner. The Lerner index is a measure of market power - the ability of a firm to charge a price that exceeds marginal cost. The index varies from zero to 1. The greater the difference between price and marginal cost the closer the index value is to 1. The Lerner index increases as demand becomes less elastic.\n\nExample\nIf a company can sell 10 units at $20 each or 11 units at $19 each, then the marginal revenue from the eleventh unit is (11 × 19) - (10 × 20) = $9.\n\n\n"}
{"id": "1402030", "url": "https://en.wikipedia.org/wiki?curid=1402030", "title": "Marginal revenue productivity theory of wages", "text": "Marginal revenue productivity theory of wages\n\nThe marginal revenue productivity theory of wages is a theory in neoclassical economics stating that wages are paid at a level equal to the marginal revenue product of labor, MRP (the value of the marginal product of labor), which is the increment to revenues caused by the increment to output produced by the last laborer employed. In a model, this is justified by an assumption that the firm is profit-maximizing and thus would employ labor only up to the point that marginal labor costs equal the marginal revenue generated for the firm. \n\nThe marginal revenue product (MRP) of a worker is equal to the product of the marginal product of labour (MP) (the increment to output from an increment to labor used) and the marginal revenue (MR) (the increment to sales revenue from an increment to output): MRP = MP × MR. The theory states that workers will be hired up to the point when the marginal revenue product is equal to the wage rate. If the marginal revenue brought by the worker is less than the wage rate, then employing that laborer would cause a decrease in profit.\n\nThe idea that payments to factors of production equal their marginal productivity had been laid out by John Bates Clark and Knut Wicksell, in simpler models. Much of the MRP theory stems from Wicksell's model.\n\nThe marginal revenue product of labour MRP is the increase in revenue per unit increase in the variable input = ∆TR/∆L \n\nThe change in output is not limited to that directly attributable to the additional worker. Assuming that the firm is operating with diminishing marginal returns then the addition of an extra worker reduces the average productivity of every other worker (and every other worker affects the marginal productivity of the additional worker).\n\nAs above noted the firm will continue to add units of labor until the MRP equals the wage rate \"w\"—mathematically until\n\nUnder perfect competition, marginal revenue product is equal to marginal physical product (extra unit produced as a result of a new employment) multiplied by price.\n\nThis is because the firm in perfect competition is a price taker. It does not have to lower the price in order to sell additional units of the good.\n\nFirms operating as monopolies or in imperfect competition face downward-sloping demand curves. To sell extra units of output, they would have to lower their output's price. Under such market conditions, marginal revenue product will not equal MPP×Price. This is because the firm is not able to sell output at a fixed price per unit. Thus the MRP curve of a firm in monopoly or in imperfect competition will slope downwards, when plotted against labor usage, at a faster rate than in perfect specific competition.\n"}
{"id": "15628625", "url": "https://en.wikipedia.org/wiki?curid=15628625", "title": "Marginal use", "text": "Marginal use\n\nAs defined by the Austrian School of economics the marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease. The useful\"ness\" of the marginal use thus corresponds to the marginal utility of the good or service.\n\nOn the assumption that an agent is economically rational, each increase would be put to the specific, feasible, previously unrealized use of greatest priority, and each decrease would result in abandonment of the use of lowest priority amongst the uses to which the good or service had been put. And, in the absence of a complementarity across uses, the “law” of diminishing marginal utility will obtain.\n\nThe Austrian School of economics explicitly arrives at its conception of marginal utility as the utility of the marginal use, and “Grenznutzen” (the Austrian School term from which “marginal utility” was originally derived in translation) literally means \"border-use\"; other schools usually do not make an explicit connection.\n\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "27079770", "url": "https://en.wikipedia.org/wiki?curid=27079770", "title": "Mental model theory of reasoning", "text": "Mental model theory of reasoning\n\nThe mental model theory of reasoning was developed by Philip Johnson-Laird and Ruth M.J. Byrne (Johnson-Laird and Byrne, 1991). It has been applied to the main domains of deductive inference including relational inferences such as spatial and temporal deductions; propositional inferences, such as conditional, disjunctive and negation deductions; quantified inferences such as syllogisms; and meta-deductive inferences.\n\nOngoing research on mental models and reasoning has led the theory to be extended to account for probabilistic inference (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).\n\n"}
{"id": "1684561", "url": "https://en.wikipedia.org/wiki?curid=1684561", "title": "Method of loci", "text": "Method of loci\n\nThe method of loci (\"loci\" being Latin for \"places\") is a method of memory enhancement which uses visualizations with the use of spatial memory, familiar information about one's environment, to quickly and efficiently recall information. The method of loci is also known as the memory journey, memory palace, or mind palace technique. This method is a mnemonic device adopted in ancient Roman and Greek rhetorical treatises (in the anonymous \"Rhetorica ad Herennium\", Cicero's \"De Oratore\", and Quintilian's \"Institutio Oratoria\"). Many memory contest champions claim to use this technique to recall faces, digits, and lists of words.\n\nThe term is most often found in specialised works on psychology, neurobiology, and memory, though it was used in the same general way at least as early as the first half of the nineteenth century in works on rhetoric, logic, and philosophy. John O'Keefe and Lynn Nadel refer to:'the method of loci', an imaginal technique known to the ancient Greeks and Romans and described by Yates (1966) in her book \"The Art of Memory\" as well as by Luria (1969). In this technique the subject memorizes the layout of some building, or the arrangement of shops on a street, or any geographical entity which is composed of a number of discrete loci. When desiring to remember a set of items the subject 'walks' through these loci in their imagination and commits an item to each one by forming an image between the item and any feature of that locus. Retrieval of items is achieved by 'walking' through the loci, allowing the latter to activate the desired items. The efficacy of this technique has been well established (Ross and Lawrence 1968, Crovitz 1969, 1971, Briggs, Hawkins and Crovitz 1970, Lea 1975), as is the minimal interference seen with its use.\n\nThe items to be remembered in this mnemonic system are mentally associated with specific physical locations. The method relies on memorized spatial relationships to establish order and recollect memorial content. It is also known as the \"Journey Method\", used for storing lists of related items, or the \"Roman Room\" technique, which is most effective for storing unrelated information.\n\nMany effective memorisers today use the \"method of loci\" to some degree. Contemporary memory competition, in particular the World Memory Championship, was initiated in 1991 and the first United States championship was held in 1997. Part of the competition requires committing to memory and recalling a sequence of digits, two-digit numbers, alphabetic letters, or playing cards. In a simple method of doing this, contestants, using various strategies well before competing, commit to long-term memory a unique vivid image associated with each item. They have also committed to long-term memory a familiar route with firmly established stop-points or loci. Then in the competition they need only deposit the image that they have associated with each item at the loci. To recall, they retrace the route, \"stop\" at each locus, and \"observe\" the image. They then translate this back to the associated item. For example, Ed Cooke, a World Memory Champion Competitor, describes to Josh Foer in his book \"Moonwalking with Einstein\" how he uses the method of loci. First, he describes a very familiar location where he can clearly remember many different smaller locations like his sink in his childhood home or his dog's bed. Cooke also advises that the more outlandish and vulgar the symbol used to memorize the material, the more likely it will stick.\n\nMemory champions elaborate on this by combining images. Eight-time World Memory Champion Dominic O'Brien uses this technique. The 2006 World Memory Champion, Clemens Mayer, used a 300-point-long journey through his house for his world record in \"number half marathon\", memorising 1040 random digits in a half-hour. Gary Shang has used the method of loci to memorise pi to over 65,536 (2) digits.\n\nUsing this technique a person with ordinary memorisation capabilities, after establishing the route stop-points and committing the associated images to long-term memory, with less than an hour of practice, can remember the sequence of a shuffled deck of cards. The world record for this is held by Simon Reinhard at 21.19 seconds.\n\nThe technique is taught as a metacognitive technique in learning-to-learn courses. It is generally applied to encoding the key ideas of a subject. Two approaches are:\n\nThe method of loci has also been shown to help sufferers of depression remember positive, self-affirming memories.\n\nA study at the University of Maryland evaluated participants ability to accurately recall two sets of familiar faces, using a traditional desktop, and with a head-mounted display. The study was designed to leverage the method of loci technique, with virtual environments resembling memory palaces. The study found an 8.8% recall improvement in favor of the head-mounted display, in part due to participants being able to leverage their vestibular and proprioceptive sensations.\n\nThe \"Rhetorica ad Herennium\" and most other sources recommend that the method of loci should be integrated with elaborative encoding (i.e., adding visual, auditory, or other details) to strengthen memory. However, due to the strength of spatial memory, simply mentally placing objects in real or imagined locations without further elaboration can be effective for simple associations.\n\nA variation of the \"method of loci\" involves creating imaginary locations (houses, palaces, roads, and cities) to which the same procedure is applied. It is accepted that there is a greater cost involved in the initial setup, but thereafter the performance is in line with the standard loci method. The purported advantage is to create towns and cities that each represent a topic or an area of study, thus offering an efficient filing of the information and an easy path for the regular review necessary for long term memory storage.\n\nSomething that is likely a reference to the \"method of loci\" techniques survives to this day in the common English phrases \"in the first place\", \"in the second place\", and so forth.\n\nThe technique is also used for second language vocabulary learning, as polyglot Timothy Doner described in his 2014 TED talk. The method is further described in Anthony Metiver's book \"How to learn and memorise German vocabulary\". What the author suggests is creating a memory palace for each letter of the German alphabet. Each memory palace then shall include a number of loci where an entry (a word or a phrase) can be stored and recalled whenever you need it.\n\nThe designation is not used with strict consistency. In some cases it refers broadly to what is otherwise known as the art of memory, the origins of which are related, according to tradition, in the story of Simonides of Ceos and the collapsing banquet hall. For example, after relating the story of how Simonides relied on remembered seating arrangements to call to mind the faces of recently deceased guests, Stephen M. Kosslyn remarks \"[t]his insight led to the development of a technique the Greeks called the method of loci, which is a systematic way of improving one's memory by using imagery.\" Skoyles and Sagan indicate that \"an ancient technique of memorization called Method of Loci, by which memories are referenced directly onto spatial maps\" originated with the story of Simonides. Referring to mnemonic methods, Verlee Williams mentions, \"One such strategy is the 'loci' method, which was developed by Simonides, a Greek poet of the fifth and sixth centuries BC.\" Loftus cites the foundation story of Simonides (more or less taken from Frances Yates) and describes some of the most basic aspects of the use of space in the art of memory. She states, \"This particular mnemonic technique has come to be called the \"method of loci\". While place or position certainly figured prominently in ancient mnemonic techniques, no designation equivalent to \"method of loci\" was used exclusively to refer to mnemonic schemes relying upon space for organization.\n\nIn other cases the designation is generally consistent, but more specific: \"The Method of Loci is a Mnemonic Device involving the creation of a Visual Map of one's house.\"\n\nThis term can be misleading: the ancient principles and techniques of the art of memory, hastily glossed in some of the works, cited above, depended equally upon images \"and\" places. The designator \"method of loci\" does not convey the equal weight placed on both elements. Training in the art or arts of memory as a whole, as attested in classical antiquity, was far more inclusive and comprehensive in the treatment of this subject.\n\nBrain scans of \"superior memorizers\", 90% of whom use the method of loci technique, have shown that it involves activation of regions of the brain involved in spatial awareness, such as the medial parietal cortex, retrosplenial cortex, and the right posterior hippocampus. The medial parietal cortex is most associated with encoding and retrieving of information. Patients who have medial parietal cortex damage have trouble linking landmarks with certain locations; many of these patients are unable to give or follow directions and often get lost. The retrosplenial cortex is also linked to memory and navigation. In one study on the effects of selective granular retrosplenial cortex lesions in rats, the researcher found that damage to the retrosplenial cortex led to impaired spatial learning abilities. Rats with damage to this area failed to recall which areas of the maze they had already visited, rarely explored different arms of the maze, almost never recalled the maze in future trials, and took longer to reach the end of the maze, as compared to rats with a fully working retrosplenial cortex.\n\nIn a classic study in cognitive neuroscience, O'Keefe and Nadel proposed \"that the hippocampus is the core of a neural memory system providing an objective spatial framework within which the items and events of an organism's experience are located and interrelated.\"\n\nIn a more recent study, memory champions during resting periods did not exhibit specific regional brain differences, but distributed functional brain network connectivity changes compared to control subjects. When volunteers trained use of the method of loci for six weeks, the training-induced changes in brain connectivity were similar to the brain network organization that distinguished memory champions from controls.\n\nFictional portrayals of the method of loci extend as far back as ancient Greek myths. The method of loci also features prominently in the BBC series \"Sherlock\", in which the titular main character uses a \"mind palace\" to store information. In the original Arthur Conan Doyle stories, Sherlock Holmes referred to his brain as an attic. In \"Hannibal Rising\" by Thomas Harris, a detailed description of Hannibal Lecter's memory palace is provided.\n"}
{"id": "46476968", "url": "https://en.wikipedia.org/wiki?curid=46476968", "title": "Model worker", "text": "Model worker\n\nModel worker (, abbreviated as 劳模 or láomó) is a Communist Chinese political term referring to an exemplary worker who exhibits some or all of the traits appropriate to the ideal of the socialist worker. The idea is similar to the Soviet Stakhanovite icon. Model workers are selected in China by central and provincial-level departments. Some cities and large companies also have processes for selecting and praising model workers.\n\nThe basic criteria for model workers are patriotism, \"worship of science,\" activities in environmental protection, and the pursuit of excellence.\n\nModel workers are often afforded privileges not available to other citizens or Communist Party members. \"The possibility to become a model worker offered peasants and workers one of the few opportunities for upward mobility other than joining the army,\" writes scholar Yu Miin-lin. Model workers have an easier time joining the Communist Party, and also to become a higher-level cadre, manager, or other leader.\n\nOne of the earliest model workers was the teenage textile worker Hao Jianxiu (awarded 1951), who invented the \"Hao Jianxiu Work Method\". She was sent to study at East China Textile Engineering Institute and was elevated to the upper echelon of Chinese politics, serving as Minister of Textile Industry, secretary of the CPC Central Secretariat, and vice chair of the State Planning Commission.\nAnother prominent model worker was Ni Zhifu (awarded 1959), a fitter who invented the \"Ni Zhifu drill\". He was elevated to leadership positions in the municipal governments of Beijing, Shanghai, and Tianjin, and became a member of the Politburo of the Communist Party of China. He also served as Chairman of the All-China Federation of Trade Unions.\n"}
{"id": "677516", "url": "https://en.wikipedia.org/wiki?curid=677516", "title": "Negative campaigning", "text": "Negative campaigning\n\nNegative campaigning or mudslinging is the process of deliberate spreading negative information about someone or something to worsen the public image of the described.\n\nDeliberate spreading of such information can be motivated either by honest desire of the campaigner to warn others against real dangers or deficiencies of the described, or by the campaigner's dishonest ideas on methods of winning in political, business or other spheres of competition against an honest rival.\n\nThe public image of an entity can be defined as reputation, esteem, respect, acceptance of the entity's appearance, values and behaviour by the general public of a given territory and/or a social group, possibly within time limits. As target groups of public and their values differ, so negativity or positivity of a public image is relative: e.g. while in most societies having an honest source of income is a positive value and stealing is discouraged, in the world of professional thieves honest work is frowned upon and stealing is encouraged. In polygamous societies monogamy is not viewed in the way it is valued in monogamous societies. Values of a society also change with time: e.g. homosexuality in Western culture was considered immoral and was criminally prosecuted until the sexual revolution of the second half of the 20 century.\nThus negative campaigning to be successful has to take into account current values of the group it addresses. The degree of strictness in practicing the group's values as opposed to its tolerance for violating the norms has also to be taken into consideration: e.g. while in the Old Testament and other traditional religious societies adultery and prostitution were outlawed and supposed to be punished by death, modern Western societies show much greater tolerance to these.\n\nIn United States politics, negative campaigning has been called \"as American as Mississippi mud\" and \"as American as apple pie\". Some research suggests negative campaigning is the norm in all political venues, mitigated only by the dynamics of a particular contest.\n\nThere are a number of techniques used in negative campaigning. Among the most effective is running advertisements attacking an opponent's personality, record, or opinion. There are two main types of ads used in negative campaigning: attack and contrast.\n\nAttack ads focus exclusively on the negative aspects of the opponent. There is no positive content in an attack ad, whether it is about the candidate or the opponent. Attack ads usually identify the risks associated with the opponent, often exploiting people’s fears to manipulate and lower the impression voters have of the opponent. Because attack ads have no positive content, they have the potential to be more influential than contrast ads in shaping voters’ views of the sponsoring candidate’s opponent.\n\nUnlike attack ads, contrast ads contain information about both the candidate and the opponent. The information about the candidate is positive, while the information about the opponent is negative. Contrast ads compare and contrast the candidate with the opponent, juxtaposing the positive information about the candidate with the negative information of the opponent. Because contrast ads must contain positive information, contrast ads are seen as less damaging to the political process than attack ads.\n\nOne of the most famous such ads was \"Daisy Girl\" by the campaign of Lyndon B. Johnson that successfully portrayed Republican Barry Goldwater as threatening nuclear war. Common negative campaign techniques include painting an opponent as soft on criminals, dishonest, corrupt, or a danger to the nation. One common negative campaigning tactic is attacking the other side for running a negative campaign.\n\nDirty tricks are also common in negative political campaigns. These generally involve secretly leaking damaging information to the media. This isolates a candidate from backlash and also does not cost any money. The material must be substantive enough to attract media interest, however, and if the truth is discovered it could severely damage a campaign. Other dirty tricks include trying to feed an opponent's team false information hoping they will use it and embarrass themselves.\n\nOften a campaign will use outside organizations, such as lobby groups, to launch attacks. These can be claimed to be coming from a neutral source and if the allegations turn out not to be true the attacking candidate will not be damaged if the links cannot be proven. Negative campaigning can be conducted by proxy. For instance, highly partisan ads were placed in the 2004 U.S. presidential election by allegedly independent bodies like MoveOn.org and Swift Boat Veterans for Truth.\n\nPush polls are attacks disguised as telephone polls. They might ask a question like \"How would you react if Candidate A was revealed to beat his wife?\", giving the impression that Candidate A might beat his wife. Members of the media and of the opposing party are deliberately not called making these tactics all but invisible and unprovable.\n\nG. Gordon Liddy played a major role in developing these tactics during the Nixon campaign playing an important advisory of rules that led to the campaign of 1972. James Carville, campaign manager of Bill Clinton's 1992 election, is also a major proponent of negative tactics. Lee Atwater, best known for being an advisor to presidents Ronald Reagan and George H.W. Bush, also pioneered many negative campaign techniques seen in political campaigns today.\n\nSponsors of overt negative campaigns often cite reasons to support mass communication of negative ideas. The Office of National Drug Control Policy uses negative campaigns to steer the public away from health risks. Similar negative campaigns have been used to rebut mass marketing by tobacco companies, or to discourage drunk driving. Those who conduct negative political campaigns sometimes say the public needs to know about the person he or she is voting for, even if it is bad. In other words, if a candidate’s opponent is a crook or a bad person, then he or she should be able to tell the public about it.\n\nMartin Wattenberg and Craig Brians, of the University of California, Irvine, considered in their study whether negative campaigning mobilizes or alienates voters. They concluded that data used by Stephen Ansolabehere in a 1994 American Political Science Review article to advance the hypothesis that negative campaigning demobilizes voters was flawed.\n\nA subsequent study done by Ansolabehere and Shanto Iyengar in 1995 corrected some of the previous study's flaws. This study concluded that negative advertising suppressed voter turnout, particularly for Independent voters. They speculated that campaigns tend to go negative only if the Independent vote is leaning toward the opponent. In doing so, they insure that the swing voters stay home, leaving the election up to base voters. They also found that negative ads have a greater impact on Democrats than on Republicans. According to them, base Republicans will vote no matter what (and will vote only for a Republican), but Democrats can be influenced to either stay home and not vote at all or to switch sides and vote for a Republican. This, combined with the effect negativity has on Independents, led them to conclude that Republicans benefit more from going negative than Democrats.\n\nOther researchers have found different, more positive outcomes from negative campaigns. Rick Farmer, PhD, an assistant professor of political science at the University of Akron found that negative ads are more memorable than positive ads when they reinforce a preexisting belief and are relevant to the central issues of a marketing campaign. Researchers at the University of Georgia found the impact of negative ads increases over time, while positive ads used to counteract negative ads lack the power of negative ads . Research also suggests negative campaigning introduces controversy and raises public awareness through additional news coverage .\n\nMost recently, Kyle Mattes and David P. Redlawsk in \"The Positive Case for Negative Campaigning\" show through surveys and experiments that negative campaigning provides informational benefits for voters. Without negativity, voters would not have full information about all of their choices, since no candidate will say anything bad about herself. They argue that candidates have to point out the flaws in their opponents for voters to be fully informed.\n\nSome strategists say that an effect of negative campaigning is that while it motivates the base of support it can alienate centrist and undecided voters from the political process, reducing voter turnout and radicalizing politics. \nIn a study done by Gina Garramone about how negative advertising affects the political process, it was found that a consequence of negative campaigning is greater image discrimination of the candidates and greater attitude polarization. While positive ads also contributed to the image discrimination and attitude polarization, Garramone found that negative campaigning played a more influential role in the discrimination and polarization than positive campaigning.\n\nNegative ads can produce a backlash. A disastrous ad was run by the Progressive Conservative Party of Canada in the 1993 Canadian federal election, apparently emphasizing Liberal Party of Canada leader Jean Chrétien's Bell's Palsy partial facial paralysis in a number of unflattering photos, with the subtext of criticizing his platforms. Chrétien took maximum advantage of the opportunity to gain the public's sympathy as a man who struggled with a physical disability and his party's subsequent overwhelming victory in the election helped reduce the governing Conservatives to two seats.\n\nA similar backlash happened to the Liberal Party in the 2006 federal election for running an attack ad that suggested that Conservative leader Stephen Harper would use Canadian soldiers to patrol Canadian cities, and impose some kind of martial law. The ad was only available from the Liberal Party's web site for a few hours prior to the release of the attack ads on television; nevertheless, it was picked up by the media and widely criticized for its absurdity, in particular the sentence \"we're not making this up; we're not allowed to make this stuff up\". Liberal MP Keith Martin expressed his disapproval of \"whoever the idiot who approved that ad was,\" shortly before Liberal leader Paul Martin (no relation) stated that he had personally approved them. The effect of the ads was to diminish the credibility of the party's other attack ads. It offended many Canadians, particularly those in the military, some of whom were fighting in Afghanistan at the time. (See Canadian federal election, 2006)\n\nMore recently, in the 2008 US Senate race in North Carolina, Republican incumbent Elizabeth Dole attempted an attack ad on Democratic challenger Kay Hagan, who had taken a small lead in polls, by tying her to atheists. Dole's campaign released an ad questioning Hagan's religion and it included a voice saying \"There is no God!\" over a picture of Kay Hagan's face. The voice was not Hagan's but it is believed the ad implied that it was. Initially, it was thought the ad would work as religion has historically been a very important issue to voters in the American south, but the ad produced a backlash across the state and Hagan responded forcefully with an ad saying that she was a Sunday school teacher and was a religious person. Hagan also claimed Dole was trying to change the subject from the economy (the ad appeared around the same time as the 2008 financial crisis). Hagan's lead in polls doubled and she won the race by a nine-point margin.\n\nBecause of the possible harm that can come from being seen as a negative campaigner, candidates often pledge to refrain from negative attacks. This pledge is usually abandoned when an opponent is perceived to be \"going negative,\" with the first retaliatory attack being, ironically, an accusation that the opponent is a negative campaigner.\n\nWhile some research has found advantages and other has found disadvantages, some studies find no difference between negative and positive approaches .\n\nResearch published in the Journal of Advertising found that negative political advertising makes the body want to turn away physically, but the mind remembers negative messages. The findings are based on research conducted by James Angelini, professor of communication at the University of Delaware, in collaboration with Samuel Bradley, assistant professor of advertising at Texas Tech University, and Sungkyoung Lee of Indiana University, which used ads that aired during the 2000 presidential election. During the study, the researchers placed electrodes under the eyes of willing participants and showed them a series of 30-second ads from both the George W. Bush and Al Gore campaigns. The electrodes picked up on the \"startle response,\" the automatic eye movement typically seen in response to snakes, spiders and other threats. Compared to positive or neutral messages, negative advertising prompted greater reflex reactions and a desire to move away.\n\n\n\n\n"}
{"id": "48243754", "url": "https://en.wikipedia.org/wiki?curid=48243754", "title": "Negative consequentialism", "text": "Negative consequentialism\n\nNegative consequentialism is a version of the ethical theory consequentialism, which is \"one of the major theories of normative ethics.\" Like other versions of consequentialism, negative consequentialism holds that moral right and wrong depend only on the value of outcomes. That is, for negative and other versions of consequentialism, questions such as \"what should I do?\" and \"what kind of person should I be?\" are answered only based on consequences. Negative consequentialism differs from other versions of consequentialism by giving greater weight in moral deliberations to what is bad (e.g. suffering or injustice) than what is good (e.g. happiness or justice).\n\nA specific type of consequentialism is utilitarianism, which says that the consequences that matter are those that affect well-being. Consequentialism is broader than utilitarianism in that consequentialism can say that the value of outcomes depend on other things than well-being; for example, justice, fairness, and equality. Negative utilitarianism is thus a form of negative consequentialism. Much more has been written explicitly about negative utilitarianism than directly about negative consequentialism, although since negative utilitarianism is a form of negative consequentialism, everything that has been written about negative utilitarianism is by definition about a specific (utilitarian) version of negative consequentialism. Similarly to how there are many variations of consequentialism and negative utilitarianism, there are many versions of negative consequentialism, for example negative prioritarianism and negative consequentialist egalitarianism.\n\nG. E. Moore's ethics can be said to be a negative consequentialism (more precisely, a consequentialism with a negative utilitarian component), because he has been labeled a consequentialist, and he said that \"consciousness of intense pain is, by itself, a great evil\" whereas \"the mere consciousness of pleasure, however intense, does not, \"by itself\", appear to be a \"great\" good, even if it has some slight intrinsic value. In short, pain (if we understand by this expression, the consciousness of pain) appears to be a far worse evil than pleasure is a good.\" Moore wrote in the first half of the 20th century before any of the terms 'consequentialism,' 'negative utilitarianism' or 'negative consequentialism' were coined, and he did not use the term 'negative consequentialism' himself. Similarly to Moore, Ingemar Hedenius defended a consequentialism that could be called negative (or could be said to have a negative utilitarian component) because he assigned more importance to suffering than to happiness. Hedenius saw the worst in life, such as infernalistic suffering, as so evil that calculations of happiness versus suffering becomes unnecessary; he did not see that such evil could be counterbalanced by any good, such as happiness.\n\nPhilosophy professor Clark Wolf defends \"negative consequentialism as a component of a larger theory of justice.\" Walter Sinnott-Armstrong interprets Bernard Gert's moral system as a \"sophisticated form of negative objective universal public rule consequentialism.\" Jamie Mayerfeld argues for a strong duty to relieve suffering, which is consequentialist in form. He says that \"suffering is more bad than happiness is good,\" and that \"the lifelong bliss of many people, no matter how many, cannot justify our allowing the lifelong torture of one.\"\n\n"}
{"id": "17235432", "url": "https://en.wikipedia.org/wiki?curid=17235432", "title": "Negative elongation factor", "text": "Negative elongation factor\n\nIn molecular biology, NELF (negative elongation factor) is a four-subunit protein (NELF-A, NELF-B, NELF-C/NELF-D, and NELF-E) that negatively impacts transcription by RNA polymerase II (Pol II) by pausing about 20-60 nucleotides downstream from the transcription start site (TSS).\n\nThe NELF-A subunit is encoded by the gene WHSC2 (Wolf-Hirschhorn syndrome candidate 2). Microsequencing analysis demonstrated that NELF-B was the protein previously identified as the protein encoded by the gene COBRA1, and was shown to interact with BRCA1. It is unknown whether or not NELF-C and NELF-D are peptides resulting from the same mRNA with different translation initiation sites, possibly differing only in an extra 9 amino acids for NELF-C at the N-terminus, or peptides from different mRNAs entirely. A single NELF complex consists of either NELF-C or NELF-D but not both. NELF-E is also known as RDBP.\n\nNELF binds in a stable complex with DSIF and RNA polymerase II together, but not with either alone. P-TEFb (positive transcription elongation factor b) inhibits the effect of NELF and DSIF on Pol II elongation, via its phosphorylation of serine-2 of the C-terminal domain of Pol II, and the SPT5 subunit of DSIF, causing dissociation of NELF. NELF homologues exist in some metazoans (e.g. insects and vertebrates) but have not been found in plants, yeast, or nematode (worms).\n"}
{"id": "1762868", "url": "https://en.wikipedia.org/wiki?curid=1762868", "title": "Negative luminescence", "text": "Negative luminescence\n\nNegative luminescence is a physical phenomenon by which an electronic device emits less thermal radiation when an electric current is passed through it than it does in thermal equilibrium (current off). When viewed by a thermal camera, an operating negative luminescent device looks colder than its environment.\n\nNegative luminescence is most readily observed in semiconductors. Incoming infrared radiation is absorbed in the material by the creation of an electron–hole pair. An electric field is used to remove the electrons and holes from the region before they have a chance to recombine and re-emit thermal radiation. This effect occurs most efficiently in regions of low charge carrier density.\n\nNegative luminescence has also been observed in semiconductors in orthogonal electric and magnetic fields. In this case, the junction of a diode is not necessary and the effect can be observed in bulk material. A term that has been applied to this type of negative luminescence is \"galvanomagnetic luminescence\".\n\nNegative luminescence might appear to be a violation of Kirchhoff's law of thermal radiation. This is not true, as the law only applies in thermal equilibrium.\n\nAnother term that has been used to describe negative luminescent devices is \"Emissivity switch\", as an electric current changes the effective emissivity.\n\nThis effect was first seen by Russian physicists in the 1960s in A.F.Ioffe Physicotechnical Institute, Leningrad, Russia. Subsequently, it was studied in semiconductors such as indium antimonide (InSb), germanium (Ge) and indium arsenide (InAs) by workers in West Germany, Ukraine (Institute of Semiconductor Physics, Kiev), Japan (Chiba University) and the United States. It was first observed in the mid-infrared (3-5 µm wavelength) in the more convenient diode structures in InSb heterostructure diodes by workers at the Defence Research Agency, Great Malvern, UK (now QinetiQ). These British workers later demonstrated LWIR band (8-12 µm) negative luminescence using mercury cadmium telluride diodes.\n\nLater the Naval Research Laboratory, Washington DC, started work on negative luminescence in mercury cadmium telluride (HgCdTe). The phenomenon has since been observed by several university groups around the world.\n\n\n\n"}
{"id": "577248", "url": "https://en.wikipedia.org/wiki?curid=577248", "title": "New riddle of induction", "text": "New riddle of induction\n\nGrue and bleen are examples of logical predicates coined by Nelson Goodman in \"Fact, Fiction, and Forecast\" to illustrate the \"new riddle of induction\". These predicates are unusual because their application is time-dependent; many have tried to solve the new riddle on those terms, but Hilary Putnam and others have argued such time-dependency depends on the language adopted, and in some languages it is equally true for natural-sounding predicates such as \"green.\" For Goodman they illustrate the problem of projectible predicates and ultimately, which empirical generalizations are law-like and which are not.\nGoodman's construction and use of \"grue\" and \"bleen\" illustrates how philosophers use simple examples in conceptual analysis.\n\nGoodman defined grue relative to an arbitrary but fixed time \"t\" as follows: An object is grue if and only if it is observed before \"t\" and is green, or else is not so observed and is blue. An object is bleen if and only if it is observed before \"t\" and is blue, or else is not so observed and is green.\n\nTo understand the problem Goodman posed, it is helpful to imagine some arbitrary future time \"t\", say January 1, 10. For all green things we observe up to time \"t\", such as emeralds and well-watered grass, both the predicates \"green\" and \"grue\" apply. Likewise for all blue things we observe up to time \"t\", such as bluebirds or blue flowers, both the predicates \"blue\" and \"bleen\" apply. On January 2, 10, however, emeralds and well-watered grass are \"bleen\" and bluebirds or blue flowers are \"grue\". Clearly, the predicates \"grue\" and \"bleen\" are not the kinds of predicates we use in everyday life or in science, but the problem is that they apply in just the same way as the predicates \"green\" and \"blue\" up until some future time \"t\". From our current perspective (i.e., before time \"t\"), how can we say which predicates are more projectible into the future: \"green\" and \"blue\" or \"grue\" and \"bleen\"?\n\nIn this section, Goodman's new riddle of induction is outlined in order to set the context for his introduction of the predicates \"grue\" and \"bleen\" and thereby illustrate their philosophical importance.\n\nGoodman poses Hume's problem of induction as a problem of the validity of the predictions we make. Since predictions are about what has yet to be observed and because there is no necessary connection between what has been observed and what will be observed, what is the justification for the predictions we make? We cannot use deductive logic to infer predictions about future observations based on past observations because there are no valid rules of deductive logic for such inferences. Hume's answer was that our observations of one kind of event following another kind of event result in our minds forming habits of regularity (i.e., associating one kind of event with another kind). The predictions we make are then based on these regularities or habits of mind we have formed.\n\nGoodman takes Hume's answer to be a serious one. He rejects other philosophers' objection that Hume is merely explaining the origin of our predictions and not their justification. His view is that Hume has identified something deeper. To illustrate this, Goodman turns to the problem of justifying a system of rules of deduction. For Goodman, the validity of a deductive system is justified by its conformity to good deductive practice. The justification of rules of a deductive system depends on our judgements about whether to reject or accept specific deductive inferences. Thus, for Goodman, the problem of induction dissolves into the same problem as justifying a deductive system and while, according to Goodman, Hume was on the right track with habits of mind, the problem is more complex than Hume realized.\n\nIn the context of justifying rules of induction, this becomes the problem of confirmation of generalizations for Goodman. However, the confirmation is not a problem of justification but instead it is a problem of precisely defining how evidence confirms generalizations. It is with this turn that \"grue\" and \"bleen\" have their philosophical role in Goodman's view of induction.\n\nThe new riddle of induction, for Goodman, rests on our ability to distinguish \"lawlike\" from \"non-lawlike\" generalizations. \"Lawlike\" generalizations are capable of confirmation while \"non-lawlike\" generalizations are not. \"Lawlike\" generalizations are required for making predictions. Using examples from Goodman, the generalization that all copper conducts electricity is capable of confirmation by a particular piece of copper whereas the generalization that all men in a given room are third sons is not \"lawlike\" but accidental. The generalization that all copper conducts electricity is a basis for predicting that this piece of copper will conduct electricity. The generalization that all men in a given room are third sons, however, is not a basis for predicting that a given man in that room is a third son.\n\nWhat then makes some generalizations \"lawlike\" and others accidental? This, for Goodman, becomes a problem of determining which predicates are projectible (i.e., can be used in \"lawlike\" generalizations that serve as predictions) and which are not. Goodman argues that this is where the fundamental problem lies. This problem, known as \"Goodman's paradox\", is as follows. Consider the evidence that all emeralds examined thus far have been green. This leads us to conclude (by induction) that all future emeralds will be green. However, whether this prediction is \"lawlike\" or not depends on the predicates used in this prediction. Goodman observed that (assuming \"t\" has yet to pass) it is equally true that every emerald that has been observed is \"grue\". Thus, by the same evidence we can conclude that all future emeralds will be \"grue\". The new problem of induction becomes one of distinguishing projectible predicates such as \"green\" and \"blue\" from non-projectible predicates such as \"grue\" and \"bleen\".\n\nHume, Goodman argues, missed this problem. We do not, by habit, form generalizations from all associations of events we have observed but only some of them. All past observed emeralds were green, and we formed a habit of thinking the next emerald will be green, but they were equally grue, and we do not form habits concerning grueness. \"Lawlike\" predictions (or projections) ultimately are distinguishable by the predicates we use. Goodman's solution is to argue that \"lawlike\" predictions are based on projectible predicates such as \"green\" and \"blue\" and not on non-projectible predicates such as \"grue\" and \"bleen\" and what makes predicates projectible is their \"entrenchment\", which depends on their successful past projections. Thus, \"grue\" and \"bleen\" function in Goodman's arguments to both illustrate the new riddle of induction and to illustrate the distinction between projectible and non-projectible predicates via their relative entrenchment.\n\nThe most obvious response is to point to the artificially disjunctive definition of grue. The notion of predicate \"entrenchment\" is not required. Goodman, however, noted that this move will not work. If we take \"grue\" and \"bleen\" as primitive predicates, we can define green as \"\"grue\" if first observed before \"t\" and \"bleen\" otherwise\", and likewise for blue. To deny the acceptability of this disjunctive definition of green would be to beg the question.\n\nAnother proposed resolution of the paradox (which Goodman addresses and rejects) that does not require predicate \"entrenchment\" is that \"\"x\" is grue\" is not solely a predicate of \"x\", but of \"x\" and a time \"t\"—we can know that an object is green without knowing the time \"t\", but we cannot know that it is grue. If this is the case, we should not expect \"\"x\" is grue\" to remain true when the time changes. However, one might ask why \"\"x\" is green\" is \"not\" considered a predicate of a particular time \"t\"—the more common definition of \"green\" does not require any mention of a time \"t\", but the definition \"grue\" does. As we have just seen, this response also begs the question because \"blue\" can be defined in terms of \"grue\" and \"bleen\", which explicitly refer to time.\n\nRichard Swinburne gets past the objection that green may be redefined in terms of \"grue\" and \"bleen\" by making a distinction based on how we test for the applicability of a predicate in a particular case. He distinguishes between qualitative and locational predicates. Qualitative predicates, like green, \"can\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event. Locational predicates, like \"grue\", \"cannot\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event, in this case whether \"x\" is being observed before or after time \"t\". Although green can be given a definition in terms of the locational predicates \"grue\" and \"bleen\", this is irrelevant to the fact that green meets the criterion for being a qualitative predicate whereas \"grue\" is merely locational. He concludes that if some \"x\"'s under examination—like emeralds—satisfy both a qualitative and a locational predicate, but projecting these two predicates yields conflicting predictions, namely, whether emeralds examined after time \"t\" shall appear blue or green, we should project the qualitative predicate, in this case green.\n\nRudolf Carnap responded to Goodman's 1946 article. Carnap's approach to inductive logic is based on the notion of \"degree of confirmation\" \"c\"(\"h\",\"e\") of a given hypothesis \"h\" by a given evidence \"e\". Both \"h\" and \"e\" are logical formulas expressed in a simple language \"L\" which allows for\nThe universe of discourse consists of denumerably many individuals, each of which is designated by its own constant symbol; such individuals are meant to be regarded as positions (\"like space-time points in our actual world\") rather than extended physical bodies. A state description is a (usually infinite) conjunction containing every possible ground atomic sentence, either negated or unnegated; such a conjunction describes a possible state of the whole universe. Carnap requires the following semantic properties:\nCarnap distinguishes three kinds of properties:\nTo illuminate this taxonomy, let \"x\" be a variable and \"a\" a constant symbol; then an example of 1. could be \"\"x\" is blue or \"x\" is non-warm\", an example of 2. \"\"x\" = \"a\", and an example of 3. \"x\" is red and not \"x\" = \"a\"\".\n\nBased on his theory of inductive logic sketched above, Carnap formalizes Goodman's notion of projectibility of a property \"W\" as follows: the higher the relative frequency of \"W\" in an observed sample, the higher is the probability that a non-observed individual has the property \"W\". Carnap suggests \"as a tentative answer\" to Goodman, that all purely qualitative properties are projectible, all purely positional properties are non-projectible, and mixed properties require further investigation.\n\nWillard Van Orman Quine discusses an approach to consider only \"natural kinds\" as projectible predicates.\nHe first relates Goodman's grue paradox to Hempel's raven paradox by defining two predicates \"F\" and \"G\" to be (simultaneously) projectible if all their shared instances count toward confirmation of the claim \"each \"F\" is a \"G\"\". Then Hempel's paradox just shows that the complements of projectible predicates (such as \"is a raven\", and \"is black\") need not be projectible, while Goodman's paradox shows that \"is green\" is projectible, but \"is grue\" is not.\n\nNext, Quine reduces projectibility to the subjective notion of \"similarity\". Two green emeralds are usually considered more similar than two grue ones if only one of them is green. Observing a green emerald makes us expect a similar observation (i.e., a green emerald) next time. Green emeralds are a \"natural kind\", but grue emeralds are not. Quine investigates \"the dubious scientific standing of a general notion of similarity, or of kind\". Both are basic to thought and language, like the logical notions of e.g. identity, negation, disjunction. However, it remains unclear how to relate the logical notions to \"similarity\" or \"kind\"; Quine therefore tries to relate at least the latter two notions to each other.\n\nRelation between similarity and kind\n\nAssuming finitely many \"kinds\" only, the notion of \"similarity\" can be defined by that of \"kind\": an object \"A\" is more similar to \"B\" than to \"C\" if \"A\" and \"B\" belong jointly to more kinds than \"A\" and \"C\" do.\n\nVice versa, it remains again unclear how to define \"kind\" by \"similarity\". Defining e.g. the kind of red things as the set of all things that are more similar to a fixed \"paradigmatical\" red object than this is to another fixed \"foil\" non-red object (cf. left picture) isn't satisfactory, since the degree of overall similarity, including e.g. shape, weight, will afford little evidence of degree of redness. (In the picture, the yellow paprika might be considered more similar to the red one than the orange.)\n\nAn alternative approach inspired by Carnap defines a natural kind to be a set whose members are more similar to each other than each non-member is to at least one member. \nHowever, Goodman argued, that this definition would make the set of all red round things, red wooden things, and round wooden things (cf. right picture) meet the proposed definition of a natural kind, while \"surely it is not what anyone means by a kind\".\n\nWhile neither of the notions of similarity and kind can be defined by the other, they at least vary together: if \"A\" is reassessed to be more similar to \"C\" than to \"B\" rather than the other way around, the assignment of \"A\", \"B\", \"C\" to kinds will be permuted correspondingly; and conversely.\n\nBasic importance of similarity and kind\n\nIn language, every general term owes its generality to some resemblance of the things referred to. Learning to use a word depends on a double resemblance, viz. between the present and past circumstances in which the word was used, and between the present and past phonetic utterances of the word.\n\nEvery reasonable expectation depends on resemblance of circumstances, together with our tendency to expect similar causes to have similar effects. This includes any scientific experiment, since it can be reproduced only under similar, but not under completely identical, circumstances. Already Heraclitus' famous saying \"No man ever steps in the same river twice\" highlighted the distinction between similar and identical circumstances.\n\nGenesis of similarity and kind\n\nIn a behavioral sense, humans and other animals have an innate standard of similarity. It is part of our animal birthright, and characteristically animal in its lack of intellectual status, e.g. its alieness to mathematics and logic, cf. bird example.\n\nInduction itself is essentially animal expectation or habit formation.\nOstensive learning\nis a case of induction, and a curiously comfortable one, since each man's spacing of qualities and kind is enough like his neighbor's.\nIn contrast, the \"brute irrationality of our sense of similarity\" offers little reason to expect it being somehow in tune with the unanimated nature, which we never made.\nWhy inductively obtained theories about it should be trusted is the perennial philosophical problem of induction. Quine, following Watanabe,\nsuggests Darwin's theory as an explanation: if people's innate spacing of qualities is a gene-linked trait, then the spacing that has made for the most successful inductions will have tended to predominate through natural selection.\nHowever, this cannot account for the human ability to dynamically refine one's spacing of qualities in the course of getting acquainted with a new area.\n\nIn his book \"Wittgenstein on Rules and Private Language\", Saul Kripke proposed a related argument that leads to skepticism about meaning rather than skepticism about induction, as part of his personal interpretation (nicknamed \"Kripkenstein\" by some) of the private language argument. He proposed a new form of addition, which he called \"quus\", which is identical with \"+\" in all cases except those in which either of the numbers added are equal to or greater than 57; in which case the answer would be 5, i.e.:\n\nHe then asks how, given certain obvious circumstances, anyone could know that previously when I thought I had meant \"+\", I had not actually meant \"quus\". Kripke then argues for an interpretation of Wittgenstein as holding that the meanings of words are not individually contained mental entities.\n\n\n"}
{"id": "387403", "url": "https://en.wikipedia.org/wiki?curid=387403", "title": "Nonsense", "text": "Nonsense\n\nNonsense is a communication, via speech, writing, or any other symbolic system, that lacks any coherent meaning. Sometimes in ordinary usage, nonsense is synonymous with absurdity or the ridiculous. Many poets, novelists and songwriters have used nonsense in their works, often creating entire works using it for reasons ranging from pure comic amusement or satire, to illustrating a point about language or reasoning. In the philosophy of language and philosophy of science, nonsense is distinguished from sense or meaningfulness, and attempts have been made to come up with a coherent and consistent method of distinguishing sense from nonsense. It is also an important field of study in cryptography regarding separating a signal from noise.\n\nThe phrase \"Colorless green ideas sleep furiously\" was coined by Noam Chomsky as an example of nonsense. However, this can easily be confused with poetic symbolism. The individual \"words\" make sense and are arranged according to proper grammatical rules, yet the result is nonsense. The inspiration for this attempt at creating verbal nonsense came from the idea of contradiction and seemingly irrelevant and/or incompatible characteristics, which conspire to make the phrase meaningless, but are open to interpretation. The phrase \"the square root of Tuesday\" (not a similar example; the lemondrop sunshine is more comparable) operates on the latter principle. This principle is behind the inscrutability of the \"kōan\" \"What is the sound of one hand clapping?\", where one hand would presumably be insufficient for clapping without the intervention of another.\n\nJames Joyce’s final novel \"Finnegans Wake\" also uses nonsense: full of portmanteau and strong words, it \"appears\" to be pregnant with multiple layers of meaning, but in many passages it is difficult to say whether any one human’s interpretation of a text could be the intended or unintended one.\n\n\"Jabberwocky\", a poem (of nonsense verse) found in \"Through the Looking-Glass, and What Alice Found There\" by Lewis Carroll (1871), is a nonsense poem written in the English language. The word \"jabberwocky\" is also occasionally used as a synonym of nonsense.\n\nNonsense verse is the verse form of literary nonsense, a genre that can manifest in many other ways. Its best-known exponent is Edward Lear, author of \"The Owl and the Pussycat\" and hundreds of limericks.\n\nNonsense verse is part of a long line of tradition predating Lear: the nursery rhyme \"Hey Diddle Diddle\" could also be termed a nonsense verse. There are also some works which \"appear\" to be nonsense verse, but actually are not, such as the popular 1940s song Mairzy Doats.\n\nLewis Carroll, seeking a nonsense riddle, once posed the question \"How is a raven like a writing desk?\". Someone answered him, \"Because Poe wrote on both\". However, there are other possible answers (e.g. \"both have inky quills\").\n\nLines of nonsense frequently figure in the refrains of folksongs, where nonsense riddles and knock-knock jokes are often encountered.\n\nThe first verse of \"Jabberwocky\" by Lewis Carroll;\nThe first four lines of \"On the Ning Nang Nong\" by Spike Milligan;\nThe first verse of \"Spirk Troll-Derisive\" by James Whitcomb Riley;\nThe first four lines of \"The Mayor of Scuttleton\" by Mary Mapes Dodge;\n\"Oh Freddled Gruntbuggly\" by Prostetnic Vogon Jeltz; a creation of Douglas Adams\n\nIn the philosophy of language and the philosophy of science, nonsense refers to a lack of sense or meaning. Different technical definitions of meaning delineate sense from nonsense.\n\nIn Ludwig Wittgenstein's writings, the word \"nonsense\" carries a special technical meaning which differs significantly from the normal use of the word. In this sense, \"nonsense\" does not refer to meaningless gibberish, but rather to the lack of sense in the context of sense and reference. In this context, logical tautologies, and purely mathematical propositions may be regarded as \"nonsense\". For example, \"1+1=2\" is a nonsensical proposition. Wittgenstein wrote in Tractatus Logico Philosophicus that some of the propositions contained in his own book should be regarded as nonsense. Used in this way, \"nonsense\" does not necessarily carry negative connotations.\n\nStarting from Wittgenstein, but through an original perspective, the Italian philosopher Leonardo Vittorio Arena, in his book \"Nonsense as the meaning\", highlights this positive meaning of nonsense to undermine every philosophical conception which does not take note of the absolute lack of meaning of the world and life. Nonsense implies the destruction of all views or opinions, on the wake of the Indian Buddhist philosopher Nagarjuna. In the name of nonsense, it is finally refused the conception of duality and the Aristotelian formal logic.\n\nThe problem of distinguishing sense from nonsense is important in cryptography and other intelligence fields. For example, they need to distinguish signal from noise. Cryptanalysts have devised algorithms to determine whether a given text is in fact nonsense or not. These algorithms typically analyze the presence of repetitions and redundancy in a text; in meaningful texts, certain frequently used words recur, for example, \"the\", \"is\" and \"and\" in a text in the English language. A random scattering of letters, punctuation marks and spaces do not exhibit these regularities. Zipf's law attempts to state this analysis mathematically. By contrast, cryptographers typically seek to make their cipher texts resemble random distributions, to avoid telltale repetitions and patterns which may give an opening for cryptanalysis.\n\nIt is harder for cryptographers to deal with the presence or absence of meaning in a text in which the level of redundancy and repetition is \"higher\" than found in natural languages (for example, in the mysterious text of the Voynich manuscript).\n\nScientists have attempted to teach machines to produce nonsense. The Markov chain technique is one method which has been used to generate texts by algorithm and randomizing techniques that seem meaningful. Another method is sometimes called the \"Mad Libs\" method: it involves creating templates for various sentence structures and filling in the blanks with noun phrases or verb phrases; these phrase-generation procedures can be looped to add recursion, giving the output the appearance of greater complexity and sophistication. Racter was a computer program which generated nonsense texts by this method; however, Racter’s book, \"The Policeman’s Beard is Half Constructed\", proved to have been the product of heavy human editing of the program's output.\n\n\n\n"}
{"id": "24097", "url": "https://en.wikipedia.org/wiki?curid=24097", "title": "Principle of bivalence", "text": "Principle of bivalence\n\nIn logic, the semantic principle (or law) of bivalence states that every declarative sentence expressing a proposition (of a theory under inspection) has exactly one truth value, either true or false. A logic satisfying this principle is called a two-valued logic or bivalent logic.\n\nIn formal logic, the principle of bivalence becomes a property that a semantics may or may not possess. It is not the same as the law of excluded middle, however, and a semantics may satisfy that law without being bivalent.\n\nThe principle of bivalence is studied in philosophical logic to address the question of which natural-language statements have a well-defined truth value. Sentences which predict events in the future, and sentences which seem open to interpretation, are particularly difficult for philosophers who hold that the principle of bivalence applies to all declarative natural-language statements. Many-valued logics formalize ideas that a realistic characterization of the notion of consequence requires the admissibility of premises which, owing to vagueness, temporal or quantum indeterminacy, or reference-failure, cannot be considered classically bivalent. Reference failures can also be addressed by free logics.\n\nThe principle of bivalence is related to the law of excluded middle though the latter is a syntactic expression of the language of a logic of the form \"P ∨ ¬P\". The difference between the principle and the law is important because there are logics which validate the law but which do not validate the principle. For example, the three-valued Logic of Paradox (LP) validates the law of excluded middle, but not the law of non-contradiction, ¬(P ∧ ¬P), and its intended semantics is not bivalent. In classical two-valued logic both the law of excluded middle and the law of non-contradiction hold.\n\nMany modern logic programming systems replace the law of the excluded middle with the concept of negation as failure. The programmer may wish to add the law of the excluded middle by explicitly asserting it as true; however, it is not assumed \"a priori\".\n\nThe intended semantics of classical logic is bivalent, but this is not true of every semantics for classical logic. In Boolean-valued semantics (for classical propositional logic), the truth values are the elements of an arbitrary Boolean algebra, \"true\" corresponds to the maximal element of the algebra, and \"false\" corresponds to the minimal element. Intermediate elements of the algebra correspond to truth values other than \"true\" and \"false\". The principle of bivalence holds only when the Boolean algebra is taken to be the two-element algebra, which has no intermediate elements.\n\nAssigning Boolean semantics to classical predicate calculus requires that the model be a complete Boolean algebra because the universal quantifier maps to the infimum operation, and the existential quantifier maps to the supremum; this is called a Boolean-valued model. All finite Boolean algebras are complete.\n\nIn order to justify his claim that true and false are the only logical values, Suszko (1977) observes that every structural Tarskian many-valued propositional logic can be provided with a bivalent semantics.\n\nA famous example is the \"contingent sea battle\" case found in Aristotle's work, \"De Interpretatione\", chapter 9:\n\nThe principle of bivalence here asserts:\n\nAristotle to embrace bivalence for such future contingents; Chrysippus, the Stoic logician, did embrace bivalence for this and all other propositions. The controversy continues to be of central importance in both the philosophy of time and the philosophy of logic.\n\nOne of the early motivations for the study of many-valued logics has been precisely this issue. In the early 20th century, the Polish formal logician Jan Łukasiewicz proposed three truth-values: the true, the false and the \"as-yet-undetermined\". This approach was later developed by Arend Heyting and L. E. J. Brouwer; see Łukasiewicz logic.\n\nIssues such as this have also been addressed in various temporal logics, where one can assert that \"\"Eventually\", either there will be a sea battle tomorrow, or there won't be.\" (Which is true if \"tomorrow\" eventually occurs.)\n\nSuch puzzles as the Sorites paradox and the related continuum fallacy have raised doubt as to the applicability of classical logic and the principle of bivalence to concepts that may be vague in their application. Fuzzy logic and some other multi-valued logics have been proposed as alternatives that handle vague concepts better. Truth (and falsity) in fuzzy logic, for example, comes in varying degrees. Consider the following statement in the circumstance of sorting apples on a moving belt:\n\nUpon observation, the apple is an undetermined color between yellow and red, or it is motled both colors. Thus the color falls into neither category \" red \" nor \" yellow \", but these are the only categories available to us as we sort the apples. We might say it is \"50% red\". This could be rephrased: it is 50% true that the apple is red. Therefore, P is 50% true, and 50% false. Now consider:\n\nIn other words, P and not-P. This violates the law of noncontradiction and, by extension, bivalence. However, this is only a partial rejection of these laws because P is only partially true. If P were 100% true, not-P would be 100% false, and there is no contradiction because P and not-P no longer holds.\n\nHowever, the law of the excluded middle is retained, because P and not-P implies P or not-P, since \"or\" is inclusive. The only two cases where P and not-P is false (when P is 100% true or false) are the same cases considered by two-valued logic, and the same rules apply.\n\nExample of a 3-valued logic applied to vague (undetermined) cases: Kleene 1952 (§64, pp. 332–340) offers a 3-valued logic for the cases when algorithms involving partial recursive functions may not return values, but rather end up with circumstances \"u\" = undecided. He lets \"t\" = \"true\", \"f\" = \"false\", \"u\" = \"undecided\" and redesigns all the propositional connectives. He observes that:\n\nThe following are his \"strong tables\":\nFor example, if a determination cannot be made as to whether an apple is red or not-red, then the truth value of the assertion Q: \" This apple is red \" is \" u \". Likewise, the truth value of the assertion R \" This apple is not-red \" is \" u \". Thus the AND of these into the assertion Q AND R, i.e. \" This apple is red AND this apple is not-red \" will, per the tables, yield \" u \". And, the assertion Q OR R, i.e. \" This apple is red OR this apple is not-red \" will likewise yield \" u \".\n\n"}
{"id": "1931801", "url": "https://en.wikipedia.org/wiki?curid=1931801", "title": "Principle of charity", "text": "Principle of charity\n\nIn philosophy and rhetoric, the principle of charity or charitable interpretation requires interpreting a speaker's statements in the most rational way possible and, in the case of any argument, considering its best, strongest possible interpretation. In its narrowest sense, the goal of this methodological principle is to avoid attributing irrationality, logical fallacies, or falsehoods to the others' statements, when a coherent, rational interpretation of the statements is available. According to Simon Blackburn \"it constrains the interpreter to maximize the truth or rationality in the subject's sayings.\"\n\nNeil L. Wilson gave the principle its name in 1958–59. Its main area of application, by his lights, is determining the referent of a proper name:\nHow should we set about discovering the significance which a person attaches to a given name? […] Let us suppose that somebody (whom I am calling \"Charles\") makes just the following five assertions containing the name \"Caesar.\" […]\n(1) Caesar conquered Gaul. (Gc) \n(2) Caesar crossed the Rubicon. (Rc) \n(3) Caesar was murdered on the Ides of March. (Mc) \n[…] And so we act on what might be called the Principle of Charity. We select as designatum that individual which will make the largest possible number of Charles' statements true. […] We might say the designatum is that individual which satisfies more of the asserted matrices containing the word \"Caesar\" than does any other individual. \nWillard Van Orman Quine and Donald Davidson provide other formulations of the principle of charity. Davidson sometimes referred to it as \"the principle of rational accommodation\". He summarized it: \"We make maximum sense of the words and thoughts of others when we interpret in a way that optimises agreement\". The principle may be invoked to make sense of a speaker's utterances when one is unsure of their meaning. In particular, Quine's use of the principle gives it this latter, wide domain.\n\nSince the time of Quine \"et al.\", other philosophers have formulated at least four versions of the principle of charity. These alternatives may conflict with one another, so which principle to use may depend on the goal of the conversation. The four principles are:\n\n\nA related principle is the principle of humanity, which states that we must assume that another speaker's beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\" (Daniel Dennett, \"Mid-Term Examination,\" in \"The Intentional Stance\", p. 343).\n\n\n"}
{"id": "249438", "url": "https://en.wikipedia.org/wiki?curid=249438", "title": "Principle of least action", "text": "Principle of least action\n\nThe principle of least action – or, more accurately, the principle of stationary action – is a variational principle that, when applied to the action of a mechanical system, can be used to obtain the equations of motion for that system. In relativity, a different action must be minimized or maximized. The principle can be used to derive Newtonian, Lagrangian and Hamiltonian equations of motion, and even general relativity (see Einstein–Hilbert action). The physicist Paul Dirac, and after him Julian Schwinger and Richard Feynman, demonstrated how this principle can also be used in quantum calculations.\nIt was historically called \"least\" because its solution requires finding the path that has the least value. Its classical mechanics and electromagnetic expressions are a consequence of quantum mechanics, but the stationary action method helped in the development of quantum mechanics.\n\nThe principle remains central in modern physics and mathematics, being applied in thermodynamics, fluid mechanics, the theory of relativity, quantum mechanics, particle physics, and string theory and is a focus of modern mathematical investigation in Morse theory. Maupertuis' principle and Hamilton's principle exemplify the principle of stationary action.\n\nThe action principle is preceded by earlier ideas in optics. In ancient Greece, Euclid wrote in his \"Catoptrica\" that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path was the shortest length and least time.\n\nScholars often credit Pierre Louis Maupertuis for formulating the principle of least action because he wrote about it in 1744 and 1746. However, Leonhard Euler discussed the principle in 1744, and evidence shows that Gottfried Leibniz preceded both by 39 years.\n\nIn 1933, Paul Dirac discerned the quantum mechanical underpinning of the principle in the quantum interference of amplitudes.\n\nThe starting point is the \"action\", denoted formula_1 (calligraphic S), of a physical system. It is defined as the integral of the Lagrangian \"L\" between two instants of time \"t\" and \"t\" - technically a functional of the \"N\" generalized coordinates q = (\"q\", \"q\", ... , \"q\") which define the configuration of the system:\n\nwhere the dot denotes the time derivative, and \"t\" is time.\n\nMathematically the principle is\n\nwhere \"δ\" (lowercase Greek delta) means a \"small\" change. In words this reads:\n\nIn applications the statement and definition of action are taken together:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nIn the 1600s, Pierre de Fermat postulated that \"\"light travels between two given points along the path of shortest time\",\" which is known as the principle of least time or Fermat's principle.\n\nCredit for the formulation of the principle of least action is commonly given to Pierre Louis Maupertuis, who felt that \"Nature is thrifty in all its actions\", and applied the principle broadly:\n\nThis notion of Maupertuis, although somewhat deterministic today, does capture much of the essence of mechanics.\n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"vis viva\",\n\nwhich is the integral of twice what we now call the kinetic energy \"T\" of the system.\n\nLeonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the \"Additamentum 2\" to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\". Beginning with the second paragraph:\n\nAs Euler states, ∫\"Mv\"d\"s\" is the integral of the momentum over distance travelled, which, in modern notation, equals the abbreviated or reduced action\n\nThus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows.\n\nMaupertuis' priority was disputed in 1751 by the mathematician Samuel König, who claimed that it had been invented by Gottfried Leibniz in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a \"copy\" of a 1707 letter from Leibniz to Jacob Hermann with the principle, but the \"original\" letter has been lost. In contentious proceedings, König was accused of forgery, and even the King of Prussia entered the debate, defending Maupertuis (the head of his Academy), while Voltaire defended König.\n\nEuler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752. The claims of forgery were re-examined 150 years later, and archival work by C.I. Gerhardt in 1898 and W. Kabitz in 1913 uncovered other copies of the letter, and three others cited by König, in the Bernoulli archives.\n\nEuler continued to write on the topic; in his \"Reflexions sur quelques loix generales de la nature\" (1748), he called the quantity \"effort\". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\nMuch of the calculus of variations was stated by Joseph-Louis Lagrange in 1760 and he proceeded to apply this to problems in dynamics. In \"Méchanique Analytique\" (1788) Lagrange derived the general equations of motion of a mechanical body. William Rowan Hamilton in 1834 and 1835 applied the variational principle to the classical Lagrangian function\n\nto obtain the Euler–Lagrange equations in their present form.\n\nIn 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian.\n\nOther extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.\n\nThe mathematical equivalence of the differential equations of motion and their integral\ncounterpart has important philosophical implications. The differential equations are statements about quantities localized to a single point in space or single moment of time. For example, Newton's second law\n\nstates that the \"instantaneous\" force F applied to a mass \"m\" produces an acceleration a at the same \"instant\". By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) an extended region of space. Moreover, in the usual formulation of classical action principles, the initial and final states of the system are fixed, e.g.,\n\nIn particular, the fixing of the \"final\" state has been interpreted as giving the action principle a teleological character which has been controversial historically. However, according to W. Yourgrau and S. Mandelstam, \"the teleological approach... presupposes that the variational principles themselves have mathematical characteristics which they \"de facto\" do not possess\" In addition, some critics maintain this apparent teleology occurs because of the way in which the question was asked. By specifying some but not all aspects of both the initial and final conditions (the positions but not the velocities) we are making some inferences about the initial conditions from the final conditions, and it is this \"backward\" inference that can be seen as a teleological explanation. Teleology can also be overcome if we consider the classical description as a limiting case of the quantum formalism of path integration, in which stationary paths are obtained as a result of interference of amplitudes along all possible paths.\n\nThe short story \"Story of Your Life\" by the speculative fiction writer Ted Chiang contains visual depictions of Fermat's Principle along with a discussion of its teleological dimension. Keith Devlin's \"The Math Instinct\" contains a chapter, \"Elvis the Welsh Corgi Who Can Do Calculus\" that discusses the calculus \"embedded\" in some animals as they solve the \"least time\" problem in actual situations.\n\n"}
{"id": "1593030", "url": "https://en.wikipedia.org/wiki?curid=1593030", "title": "Product/process distinction", "text": "Product/process distinction\n\nThe product/process distinction is the distinction between the product information and the process information of a consumer good. Product information is information that pertains to a consumer good, namely to its price, quality, and safety (its proximate attributes). Process information is information that pertains to the means by which the consumer good is made i.e. the working conditions under which it comes into being, as well as the treatment of animals involved in its production chain (its peripheral attributes).\n\nThe product/process distinction is used by the World Trade Organization (WTO) as a way to determine whether or not a complaint filed by an importing nation is valid and warrants trade barriers against the exporting nation. Under WTO rules, an importing nation can lodge a complaint with the WTO that the exporting nation uses methods for obtaining or producing the good in question that the importing nation finds to be immoral or unethical. If the independent World Trade Organization Advisory Board, made up of a panel of international law and trade experts, finds that the importing nation has a legitimate complaint, enforces said ethical standards for domestic production, and isn't trying to merely skirt its free trade obligations, then the Board will rule that trade barriers are justified. Despite what World Trade Organization officials have said, in practice the World Trade Organization finds these complaints illegitimate the vast majority of the time.\n\nFor example, if the European Union (EU) wants to ban imports of cosmetics that were tested on laboratory animals on grounds that such testing is unethical, it can file a complaint with the World Trade Organization and, in theory, the WTO would allow the EU to enact trade barriers provided that the EU bans its own domestic cosmetic producers from testing on laboratory animals. \nIn these cases, however, the World Trade Organization has consistently ruled that such barriers are illegal because only the process is different, while the final product itself is not. Therefore, the WTO has made the product/process distinction an important factor in determining whether trade barriers are justified.\n\nThe World Trade Organization has stated that if nations were able to enact barriers merely because the importing nation's standards differ from their own, control could be lost and barriers could be enacted around the world for frivolous reasons. However, many complain that these rulings go against the stated intentions of the World Trade Organization, and prove that the organization often puts commercial interests above environmental, ethical, and human rights issues.\n"}
{"id": "23556881", "url": "https://en.wikipedia.org/wiki?curid=23556881", "title": "Psychological continuum model", "text": "Psychological continuum model\n\nThe psychological continuum model (PCM) is a framework to organise prior literature from various academic disciplines to explain sport and event consumer behaviour. \nThe framework suggests four stages – awareness, attraction, attachment and allegiance to describe how sport and event involvement progressively develops with corresponding behaviours (e.g., playing, watching, buying). The PCM uses a vertical framework to characterise various psychological connections that individuals form with objects to explain the role of attitude formation and change that directs behaviours across a variety of consumption activities. Explaining the \"how\" and \"why\" of sport and event consumer behaviour, it discusses how personal, psychological and environmental factors influence a wide range of sport consumption activities.\n\nThe figure shows the four stages of the PCM - awareness, attraction, attachment and allegiance. On each stage, there is a horizontal decision making process. Inputs (green arrows) influence the internal processing (blue boxes) that creates outputs (yellow arrows). The outcomes are shown in the four different stages of the PCM (grey boxes). The unique decision making process is based upon the level of involvement of the consumer towards a sport/team/event. The following sequence is shown in each stage:\n\nInputs --> Internal Processing <--> Output\n\nThe PCM framework states that, through the processing of internal and external inputs, individuals progress upward along the four psychological connection stages. The overall evaluation of an object at a specific stage is the product of the processing of personal, psychological and environmental factors.\n\nAwareness stands for the notion when an individual first learns that a certain sport, event or team exists. In this stage the individual has not formed a preference or favourite. The PCM suggests that awareness of sport, teams and events stems from formal and informal channels, for examples parents, friends, school and media. In most cases awareness begins during childhood, but can also derive from other socializing agents. The value placed on the specific sport and event from a societal perspective is important in the awareness stage. The examples of \"I know about football\" and \"I know about Liverpool FC\" illustrate the awareness stage box.\n\nIn the attraction stage, the individual has a favourite sport, event, team or leisure hobby. Attraction is based upon a number of extrinsic and intrinsic motives. In other words, the sport, event, or leisure hobby provides the opportunity to satisfy needs and receive benefits. The motives stem from a combination of personal, psychological and environmental factors. The Attraction processing creates outcomes of positive affect and intentions, as well as engaging in consumption behaviour related to the sport and event. The examples of \"I like football\" and \"I like Liverpool FC\" illustrate the attraction stage box.\n\nIn the attachment stage the benefits and the sport object are internalised taking on a collective emotional, functional, and symbolic meaning. The psychological connection towards a sport, event, team or leisure hobby strengthens. Internal processes become more important and the influence of socializing agents decreases. Examples for the attachment stage are \"I am a football player\" or \"I am a Liverpool Fan\".\n\nAs the attachment processing continues, the internal collective meaning becomes more durable in terms of persistence and resistance and has greater impact on activities and behaviour. This is noted by the examples of \"I live for football\" and \"I live for Liverpool FC\" within the allegiance stage.\n"}
{"id": "244755", "url": "https://en.wikipedia.org/wiki?curid=244755", "title": "Sense and reference", "text": "Sense and reference\n\nIn the philosophy of language, the distinction between sense and reference was an innovation of the German philosopher and mathematician Gottlob Frege in 1892 (in his paper \"On Sense and Reference\"; German: \"Über Sinn und Bedeutung\"), reflecting the two ways he believed a singular term may have meaning.\n\nThe reference (or \"referent\"; \"Bedeutung\") of a proper name is the object it means or indicates (\"bedeuten\"), its sense (\"Sinn\") is what the name expresses. The reference of a sentence is its truth value, its sense is the thought that it expresses. Frege justified the distinction in a number of ways.\n\nMuch of analytic philosophy is traceable to Frege's philosophy of language. Frege's views on logic (i.e., his idea that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function) led to his views on a theory of reference.\n\nFrege developed his original theory of meaning in early works like \"Begriffsschrift\" ('concept script') of 1879 and \"Grundlagen\" ('foundations of arithmetic') of 1884. On this theory, the meaning of a complete sentence consists in its being true or false, and the meaning of each significant expression in the sentence is an extralinguistic entity which Frege called its \"Bedeutung\", literally 'meaning' or 'significance', but rendered by Frege's translators as 'reference', 'referent', \"'M\"eaning', 'nominatum', etc. Frege supposed that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function, but that other parts are incomplete, and contain an empty place, by analogy with the function itself. Thus 'Caesar conquered Gaul' divides into the complete term 'Caesar', whose reference is Caesar himself, and the incomplete term '—conquered Gaul', whose reference is a Concept. Only when the empty place is filled by a proper name does the reference of the completed sentence – its truth value – appear. This early theory of meaning explains how the significance or reference of a sentence (its truth value) depends on the significance or reference of its parts.\n\nFrege introduced the notion of \"sense\" (German: \"Sinn\") to accommodate difficulties in his early theory of meaning.\n\nFirst, if the entire significance of a sentence consists of its truth value, it follows that the sentence will have the same significance if we replace a word of the sentence with one having an identical reference, as this will not change its truth value. The reference of the whole is determined by the reference of the parts. If \"the evening star\" has the same reference as \"the morning star\", it follows that \"the evening star is a body illuminated by the Sun\" has the same truth value as \"the morning star is a body illuminated by the Sun\". But it is possible for someone to think that the first sentence is true while also thinking that the second is false. Therefore, the thought corresponding to each sentence cannot be its reference, but something else, which Frege called its \"sense\".\n\nSecond, sentences that contain proper names with no reference cannot have a truth value at all. Yet the sentence 'Odysseus was set ashore at Ithaca while sound asleep' obviously has a sense, even though 'Odysseus' has no reference. The thought remains the same whether or not 'Odysseus' has a reference. Furthermore, a thought cannot contain the objects that it is about. For example, Mont Blanc, 'with its snowfields', cannot be a component of the thought that Mont Blanc is more than 4,000 metres high. Nor can a thought about Etna contain lumps of solidified lava.\n\nFrege's notion of sense is somewhat obscure, and neo-Fregeans have come up with different candidates for its role. Accounts based on the work of Carnap and Church treat sense as an intension, or a function from possible worlds to extensions. For example, the intension of ‘number of planets’ is a function that maps any possible world to the number of planets in that world. John McDowell supplies cognitive and reference-determining roles. Devitt treats senses as causal-historical chains connecting names to referents.\n\nIn his theory of descriptions, Bertrand Russell held the view that most proper names in ordinary language are in fact disguised definite descriptions. For example, 'Aristotle' can be understood as \"The pupil of Plato and teacher of Alexander,\" or by some other uniquely applying description. This is known as the descriptivist theory of names. Because Frege used definite descriptions in many of his examples, he is often taken to have endorsed the descriptivist theory. Thus Russell's theory of descriptions was conflated with Frege's theory of sense, and for most of the twentieth century this 'Frege-Russell' view was the orthodox view of proper name semantics. However, Saul Kripke argued compellingly against the descriptivist theory. According to Kripke, proper names are rigid designators which designate the same object in every possible world. Descriptions such as 'the President of the U.S. in 1970' do not designate the same in every possible world. For example, someone other than Richard Nixon, e.g. Hubert Humphrey, might have been the President in 1970. Hence a description (or cluster of descriptions) cannot be a rigid designator, and thus a proper name cannot \"mean\" the same as a description.\n\nHowever, the Russellian descriptivist reading of Frege has been rejected by many scholars, in particular by Gareth Evans in \"The Varieties of Reference\" and by John McDowell in \"The Sense and Reference of a Proper Name,\" following Michael Dummett, who argued that Frege's notion of sense should not be equated with a description. Evans further developed this line, arguing that a sense without a referent was not possible. He and McDowell both take the line that Frege's discussion of empty names, and of the idea of sense without reference, are inconsistent, and that his apparent endorsement of descriptivism rests only on a small number of imprecise and perhaps offhand remarks. And both point to the power that the sense-reference distinction \"does\" have (i.e., to solve at least the first two problems), even if it is not given a descriptivist reading.\n\nAs noted above, translators of Frege have rendered the German \"Bedeutung\" in various ways. The term 'reference' has been the most widely adopted, but this fails to capture the meaning of the original German ('meaning' or 'significance'), and does not reflect the decision to standardise key terms across different editions of Frege's works published by Blackwell. The decision was based on the principle of exegetical neutrality, namely that 'if at any point in a text there is a passage that raises for the native speaker legitimate questions of exegesis, then, if at all possible, a translator should strive to confront the reader of his version with the same questions of exegesis and not produce a version which in his mind resolves those questions'. The term 'meaning' best captures the standard German meaning of \"Bedeutung\", and Frege's own use of the term sounds as odd when translated into English as it does in German. Moreover, 'meaning' captures Frege's early use of \"Bedeutung\" well, and it would be problematic to translate Frege's early use as 'meaning' and his later use as 'reference', suggesting a change in terminology not evident in the original German.\n\nThe Greek philosopher Antisthenes, a pupil of Socrates, apparently distinguished \"a general object that can be aligned with the meaning of the utterance” from “a particular object of extensional reference.\" This \"suggests that he makes a distinction between sense and reference.\" \nThe principal basis of this claim is a quotation in Alexander of Aphrodisias's “Comments on Aristotle's 'Topics'” with a three-way distinction: \n\nThe sense-reference distinction is commonly confused with that between connotation and denotation, which originates with John Stuart Mill. According to Mill, a common term like 'white' \"denotes\" all white things, as snow, paper. But according to Frege, a common term does not refer to any individual white thing, but rather to an abstract Concept (\"Begriff\"). We must distinguish between the relation of reference, which holds between a proper name and the object it refers to, such as between the name 'Earth', and the planet Earth, and the relation of 'falling under', such as when the Earth falls under the concept \"planet\". The relation of a proper name to the object it designates is direct, whereas a word like 'planet' has no such direct relation at all to the Earth at all, but only to a concept that the Earth falls under. Moreover, judging \"of\" anything that it falls under this concept is not in any way part of our knowledge of what the word 'planet' means. The distinction between connotation and denotation is closer to that between Concept and Object, than to that between 'sense' and 'reference'.\n\n"}
{"id": "2985811", "url": "https://en.wikipedia.org/wiki?curid=2985811", "title": "Signed measure", "text": "Signed measure\n\nIn mathematics, signed measure is a generalization of the concept of measure by allowing it to have negative values. Some authors may call it a charge, by analogy with electric charge, which is a familiar distribution that takes on positive and negative values.\n\nThere are two slightly different concepts of a signed measure, depending on whether or not one allows it to take infinite values. In research papers and advanced books signed measures are usually only allowed to take finite values, while undergraduate textbooks often allow them to take infinite values. To avoid confusion, this article will call these two cases \"finite signed measures\" and \"extended signed measures\".\n\nGiven a measurable space (\"X\", Σ), that is, a set \"X\" with a sigma algebra Σ on it, an extended signed measure is a function\nsuch that formula_2 and formula_3 is sigma additive, that is, it satisfies the equality\nwhere the series on the right must converge absolutely, for any sequence \"A\", \"A\", ..., \"A\", ... of disjoint sets in Σ. One consequence is that any extended signed measure can take +∞ as value, or it can take −∞ as value, but both are not available. The expression ∞ − ∞ is undefined and must be avoided.\n\nA finite signed measure (aka. real measure) is defined in the same way, except that it is only allowed to take real values. That is, it cannot take +∞ or −∞.\n\nFinite signed measures form a vector space, while extended signed measures are not even closed under addition, which makes them rather hard to work with. On the other hand, measures are extended signed measures, but are not in general finite signed measures.\n\nConsider a nonnegative measure ν on the space (\"X\", Σ) and a measurable function \"f\":\"X\"→ R such that\n\nThen, a finite signed measure is given by\n\nfor all \"A\" in Σ.\n\nThis signed measure takes only finite values. To allow it to take +∞ as a value, one needs to replace the assumption about \"f\" being absolutely integrable with the more relaxed condition\n\nwhere \"f\"(\"x\") = max(−\"f\"(\"x\"), 0) is the negative part of \"f\".\n\nWhat follows are two results which will imply that an extended signed measure is the difference of two nonnegative measures, and a finite signed measure is the difference of two finite non-negative measures.\n\nThe Hahn decomposition theorem states that given a signed measure μ, there exist two measurable sets \"P\" and \"N\" such that:\n\nMoreover, this decomposition is unique up to adding to/subtracting μ-null sets from \"P\" and \"N\".\n\nConsider then two nonnegative measures μ and μ defined by\n\nand\n\nfor all measurable sets \"E\", that is, \"E\" in Σ.\n\nOne can check that both μ and μ are nonnegative measures, with one taking only finite values, and are called the \"positive part\" and \"negative part\" of μ, respectively. One has that μ = μ - μ. The measure |μ| = μ + μ is called the \"variation\" of μ, and its maximum possible value, ||μ|| = |μ|(\"X\"), is called the \"total variation\" of μ.\n\nThis consequence of the Hahn decomposition theorem is called the \"Jordan decomposition\". The measures μ, μ and |μ| are independent of the choice of \"P\" and \"N\" in the Hahn decomposition theorem.\n\nThe sum of two finite signed measures is a finite signed measure, as is the product of a finite signed measure by a real number: they are closed under linear combination. It follows that the set of finite signed measures on a measurable space (\"X\", Σ) is a real vector space; this is in contrast to positive measures, which are only closed under conical combination, and thus form a convex cone but not a vector space. Furthermore, the total variation defines a norm in respect to which the space of finite signed measures becomes a Banach space. This space has even more structure, in that it can be shown to be a Dedekind complete Banach lattice and in so doing the Radon–Nikodym theorem can be shown to be a special case of the Freudenthal spectral theorem.\n\nIf \"X\" is a compact separable space, then the space of finite signed Baire measures is the dual of the real Banach space of all continuous real-valued functions on \"X\", by the Riesz–Markov–Kakutani representation theorem.\n\n\n"}
{"id": "49535", "url": "https://en.wikipedia.org/wiki?curid=49535", "title": "Thought experiment", "text": "Thought experiment\n\nA thought experiment (, \"Gedanken-Experiment\", or \"Gedankenerfahrung\",) considers some hypothesis, theory, or principle for the purpose of thinking through its consequences. Given the structure of the experiment, it may not be possible to perform it, and even if it could be performed, there need not be an intention to perform it.\n\nThe common goal of a thought experiment is to explore the potential consequences of the principle in question:\n\nExamples of thought experiments include Schrödinger's cat, illustrating quantum indeterminacy through the manipulation of a perfectly sealed environment and a tiny bit of radioactive substance, and Maxwell's demon, which attempts to demonstrate the ability of a hypothetical finite being to violate the 2nd law of thermodynamics.\n\nThe ancient Greek δείκνυμι \"(transl.: deiknymi)\", or thought experiment, \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought-experiment. Perhaps the key experiment in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the 'thought experiment' technique. The 'experiment' is described by Galileo in \"Discorsi e dimostrazioni matematiche\" (1638) (literally, 'Discourses and Mathematical Demonstrations') thus:\n\nAlthough the extract does not convey the elegance and power of the 'demonstration' terribly well, it is clear that it is a 'thought' experiment, rather than a practical one. Strange then, as Cohen says, that philosophers and scientists alike refuse to acknowledge either Galileo in particular, or the thought experiment technique in general for its pivotal role in both science and philosophy. (The exception proves the rule — the iconoclastic philosopher of science, Paul Feyerabend, has also observed this methodological prejudice.)\n\nInstead, many philosophers prefer to consider 'Thought Experiments' to be merely the use of a hypothetical scenario to help understand the way things are.\n\nThought experiments have been used in a variety of fields, including philosophy, law, physics, and mathematics. In philosophy, they have been used at least since classical antiquity, some pre-dating Socrates. In law, they were well-known to Roman lawyers quoted in the Digest. In physics and other sciences, notable thought experiments date from the 19th and especially the 20th century, but examples can be found at least as early as Galileo.\n\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the Latin-German mixed term \"Gedankenexperiment\" (lit. thought experiment) circa 1812. Ørsted was also the first to use its entirely German equivalent, \"Gedankenversuch\", in 1820.\n\nMuch later, Ernst Mach used the term \"Gedankenexperiment\" in a different way, to denote exclusively the \"imaginary\" conduct of a \"real\" experiment that would be subsequently performed as a \"real physical experiment\" by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\n\nThe English term \"thought experiment\" was coined (as a calque) from Mach's \"Gedankenexperiment\", and it first appeared in the 1897 English translation of one of Mach’s papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time (for both scientists and philosophers). However, people had no way of categorizing it or speaking about it. This helps to explain the extremely wide and diverse range of the application of the term \"thought experiment\" once it had been introduced into English.\n\nThought experiments, which are well-structured, well-defined hypothetical questions that employ subjunctive reasoning (irrealis moods) – \"What might happen (or, what might have happened) if . . . \" – have been used to pose questions in philosophy at least since Greek antiquity, some pre-dating Socrates. In physics and other sciences many thought experiments date from the 19th and especially the 20th Century, but examples can be found at least as early as Galileo.\n\nIn thought experiments we gain new information by rearranging or reorganizing already known empirical data in a new way and drawing new (a priori) inferences from them or by looking at these data from a different and unusual perspective. In Galileo’s thought experiment, for example, the rearrangement of empirical experience consists in the original idea of combining bodies of different weight.\n\nThought experiments have been used in philosophy (especially ethics), physics, and other fields (such as cognitive psychology, history, political science, economics, social psychology, law, organizational studies, marketing, and epidemiology). In law, the synonym \"hypothetical\" is frequently used for such experiments.\n\nRegardless of their intended goal, all thought experiments display a patterned way of thinking that is designed to allow us to explain, predict and control events in a better and more productive way.\n\nIn terms of their theoretical consequences, thought experiments generally:\n\nThought experiments can produce some very important and different outlooks on previously unknown or unaccepted theories. However, they may make those theories themselves irrelevant, and could possibly create new problems that are just as difficult, or possibly more difficult to resolve.\n\nIn terms of their practical application, thought experiments are generally created to:\n\nScientists tend to use thought experiments as imaginary, \"proxy\" experiments prior to a real, \"physical\" experiment (Ernst Mach always argued that these gedankenexperiments were \"a necessary precondition for physical experiment\"). In these cases, the result of the \"proxy\" experiment will often be so clear that there will be no need to conduct a physical experiment at all.\n\nScientists also use thought experiments when particular physical experiments are impossible to conduct (Carl Gustav Hempel labeled these sorts of experiment \"theoretical experiments-in-imagination\"), such as Einstein's thought experiment of chasing a light beam, leading to special relativity. This is a unique use of a scientific thought experiment, in that it was never carried out, but led to a successful theory, proven by other empirical means.\n\nThe relation to real experiments can be quite complex, as can be seen again from an example going back to Albert Einstein. In 1935, with two coworkers, he published a paper on a newly created subject called later the EPR effect (EPR paradox). In this paper, starting from certain philosophical assumptions, on the basis of a rigorous analysis of a certain, complicated, but in the meantime assertedly realizable model, he came to the conclusion that \"quantum mechanics should be described as \"incomplete\"\". Niels Bohr asserted a refutation of Einstein's analysis immediately, and his view prevailed. After some decades, it was asserted that feasible experiments could prove the error of the EPR paper. These experiments tested the Bell inequalities published in 1964 in a purely theoretical paper. The above-mentioned EPR philosophical starting assumptions were considered to be falsified by empirical fact (e.g. by the optical \"real experiments\" of Alain Aspect).\n\nThus \"thought experiments\" belong to a theoretical discipline, usually to theoretical physics, but often to theoretical philosophy. In any case, it must be distinguished from a real experiment, which belongs naturally to the experimental discipline and has \"the final decision on \"true\" or \"not true\"\", at least in physics.\n\nThe first characteristic pattern that thought experiments display is their orientation\nin time. They are either:\n\nThe second characteristic pattern is their movement in time in relation to “the present\nmoment standpoint” of the individual performing the experiment; namely, in terms of:\n\nGenerally speaking, there are seven types of thought experiments in which one reasons from causes to effects, or effects to causes:\n\n\"Prefactual (before the fact) thought experiments\" — the term prefactual was coined by Lawrence J. Sanna in 1998 — speculate on possible future outcomes, given the present, and ask \"What will be the outcome if event E occurs?\"\n\n\"Counterfactual (contrary to established fact) thought experiments\" — the term \"counterfactual\" was coined by Nelson Goodman in 1947, extending Roderick Chisholm's (1946) notion of a \"contrary-to-fact conditional\" — speculate on the possible outcomes of a different past; and ask \"What might have happened if A had happened instead of B?\" (e.g., \"If Isaac Newton and Gottfried Leibniz had cooperated with each other, what would mathematics look like today?\").\n\nThe study of counterfactual speculation has increasingly engaged the interest of scholars in a wide range of domains such as philosophy, psychology, cognitive psychology, history, political science, economics, social psychology, law, organizational theory, marketing, and epidemiology.\n\n\"Semifactual thought experiments\" — the term \"semifactual\" was coined by Nelson Goodman in 1947 — speculate on the extent to which things might have remained the same, despite there being a different past; and asks the question Even though X happened instead of E, would Y have still occurred? (e.g., Even if the goalie had moved left, rather than right, could he have intercepted a ball that was traveling at such a speed?).\n\nSemifactual speculations are an important part of clinical medicine.\n\nThe activity of prediction attempts to project the circumstances of the present into the future. According to David Sarewitz and Roger Pielke (1999, p123), scientific prediction takes two forms:\n\nAlthough they perform different social and scientific functions, the only difference between the qualitatively identical activities of \"predicting\", \"forecasting,\" and \"nowcasting\" is the distance of the speculated future from the present moment occupied by the user. Whilst the activity of nowcasting, defined as “a detailed description of the current weather along with forecasts obtained by extrapolation up to 2 hours ahead”, is essentially concerned with describing the current state of affairs, it is common practice to extend the term “to cover very-short-range forecasting up to 12 hours ahead” (Browning, 1982, p.ix).\n\nThe activity of hindcasting involves running a forecast model after an event has happened in order to test whether the model's simulation is valid.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n\nThe activity of \"retrodiction\" (or \"postdiction\") involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (e.g., reverse engineering and forensics).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) the produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\", the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient.\n\nThe activity of \"backcasting\" — the term \"backcasting\" was coined by John Robinson in 1982 — involves establishing the description of a very definite and very specific future situation. It then involves an imaginary moving backwards in time, step-by-step, in as many stages as are considered necessary, from the future to the present to reveal the mechanism through which that particular specified future could be attained from the present.\n\nBackcasting is not concerned with predicting the future:\n\nAccording to Jansen (1994, p. 503:\n\nIn philosophy, a thought experiment typically presents an imagined scenario with the intention of eliciting an intuitive or reasoned response about the way things are in the thought experiment. (Philosophers might also supplement their thought experiments with theoretical reasoning designed to support the desired intuitive response.) The scenario will typically be designed to target a particular philosophical notion, such as morality, or the nature of the mind or linguistic reference. The response to the imagined scenario is supposed to tell us about the nature of that notion in any scenario, real or imagined.\n\nFor example, a thought experiment might present a situation in which an agent intentionally kills an innocent for the benefit of others. Here, the relevant question is not whether the action is moral or not, but more broadly whether a moral theory is correct that says morality is determined solely by an action's consequences (See Consequentialism). John Searle imagines a man in a locked room who receives written sentences in Chinese, and returns written sentences in Chinese, according to a sophisticated instruction manual. Here, the relevant question is not whether or not the man understands Chinese, but more broadly, whether a functionalist theory of mind is correct.\n\nIt is generally hoped that there is universal agreement about the intuitions that a thought experiment elicits. (Hence, in assessing their own thought experiments, philosophers may appeal to \"what we should say,\" or some such locution.) A successful thought experiment will be one in which intuitions about it are widely shared. But often, philosophers differ in their intuitions about the scenario.\n\nOther philosophical uses of imagined scenarios arguably are thought experiments also. In one use of scenarios, philosophers might imagine persons in a particular situation (maybe ourselves), and ask what they would do.\n\nFor example, in the veil of ignorance, John Rawls asks us to imagine a group of persons in a situation where they know nothing about themselves, and are charged with devising a social or political organization. The use of the state of nature to imagine the origins of government, as by Thomas Hobbes and John Locke, may also be considered a thought experiment. Søren Kierkegaard explored the possible ethical and religious implications of Abraham's binding of Isaac in \"Fear and Trembling\" Similarly, Friedrich Nietzsche, in \"On the Genealogy of Morals\", speculated about the historical development of Judeo-Christian morality, with the intent of questioning its legitimacy.\n\nAn early written thought experiment was Plato's allegory of the cave. Another historic thought experiment was Avicenna's \"Floating Man\" thought experiment in the 11th century. He asked his readers to imagine themselves suspended in the air isolated from all in order to demonstrate human self-awareness and self-consciousness, and the substantiality of the soul.\n\nIn many thought experiments, the scenario would be nomologically possible, or possible according to the laws of nature. John Searle's Chinese room is nomologically possible.\n\nSome thought experiments present scenarios that are not nomologically possible. In his Twin Earth thought experiment, Hilary Putnam asks us to imagine a scenario in which there is a substance with all of the observable properties of water (e.g., taste, color, boiling point), but is chemically different from water. It has been argued that this thought experiment is not nomologically possible, although it may be possible in some other sense, such as metaphysical possibility. It is debatable whether the nomological impossibility of a thought experiment renders intuitions about it moot.\n\nIn some cases, the hypothetical scenario might be considered metaphysically impossible, or impossible in any sense at all. David Chalmers says that we can imagine that there are zombies, or persons who are physically identical to us in every way but who lack consciousness. This is supposed to show that physicalism is false. However, some argue that zombies are inconceivable: we can no more imagine a zombie than we can imagine that 1+1=3. Others have claimed that the conceivability of a scenario may not entail its possibility.\n\nThe philosophical work of Stefano Gualeni focuses on the use of virtual worlds to materialize thought experiments and to playfully negotiate philosophical ideas. His arguments were originally presented in his 2015 book \"Virtual Worlds as Philosophical Tools\".\n\nGualeni's argument is that the history of philosophy has, until recently, merely been the history of written thought, and digital media can complement and enrich the limited and almost exclusively linguistic approach to philosophical thought. He considers virtual worlds to be philosophically viable and advantageous in contexts like those of thought experiments, when the recipients of a certain philosophical notion or perspective are expected to objectively test and evaluate different possible courses of action, or in cases where they are confronted with interrogatives concerning non-actual or non-human phenomenologies .\n\nAmong the most visible thought experiments designed by Stefano Gualeni:\n\nOther examples of playful, interactive thought experiments:\n\n\n\n\n\n\n\n"}
{"id": "55888", "url": "https://en.wikipedia.org/wiki?curid=55888", "title": "Trusted system", "text": "Trusted system\n\nIn the security engineering subspecialty of computer science, a trusted system is a system that is relied upon to a specified extent to enforce a specified security policy. This is equivalent to saying that a trusted system is one whose failure would break a security policy (if a policy exists that the trusted system is trusted to enforce).\n\nThe meaning of the word \"trust\" is critical, as it does not carry the meaning that might be expected in everyday usage. A system trusted by a user, is one that the user feels safe to use, and trusts to do tasks without secretly executing harmful or unauthorised programs; while trusted computing refers to whether programs can trust the platform to be unmodified from that expected, whether or not those programs are innocent, malicious or execute tasks that are undesired by the user.\n\nA subset of trusted systems (\"Division B\" and \"Division A\") implement mandatory access control (MAC) labels; as such, it is often assumed that they can be used for processing classified information. However, this is generally untrue. There are four modes in which one can operate a multilevel secure system: multilevel mode, compartmented mode, dedicated mode, and system-high mode. The National Computer Security Center's \"Yellow Book\" specifies that B3 and A1 systems can only be used for processing a strict subset of security labels, and only when operated according to a particularly strict configuration.\n\nCentral to the concept of U.S. Department of Defense-style \"trusted systems\" is the notion of a \"reference monitor\", which is an entity that occupies the logical heart of the system and is responsible for all access control decisions. Ideally, the reference monitor is (a) tamper-proof, (b) always invoked, and (c) small enough to be subject to independent testing, the completeness of which can be assured. Per the U.S. National Security Agency's 1983 Trusted Computer System Evaluation Criteria (TCSEC), or \"Orange Book\", a set of \"evaluation classes\" were defined that described the features and assurances that the user could expect from a trusted system.\n\nKey to the provision of the highest levels of assurance (B3 and A1) is the dedication of significant system engineering toward minimization of the complexity (not \"size\", as often cited) of the trusted computing base (TCB), defined as that combination of hardware, software, and firmware that is responsible for enforcing the system's security policy.\n\nAn inherent engineering conflict would appear to arise in higher-assurance systems in that, the smaller the TCB, the larger the set of hardware, software, and firmware that lies outside the TCB and is, therefore, untrusted. Although this may lead the more technically naive to sophists' arguments about the nature of trust, the argument confuses the issue of \"correctness\" with that of \"trustworthiness\".\n\nIn contrast to the TCSEC's precisely defined hierarchy of six evaluation classes—the highest of which, A1, is featurally identical to B3, differing only in documentation standards—the more recently introduced Common Criteria (CC)—which derive from a blend of more or less technically mature standards from various NATO countries—provide a more tenuous spectrum of seven \"evaluation classes\" that intermix features and assurances in an arguably non-hierarchical manner and lack the philosophic precision and mathematical stricture of the TCSEC. In particular, the CC tolerate very loose identification of the \"target of evaluation\" (TOE) and support—even encourage—an inter-mixture of security requirements culled from a variety of predefined \"protection profiles.\" While a strong case can be made that even the more seemingly arbitrary components of the TCSEC contribute to a \"chain of evidence\" that a fielded system properly enforces its advertised security policy, not even the highest (E7) level of the CC can truly provide analogous consistency and stricture of evidentiary reasoning.\n\nThe mathematical notions of trusted systems for the protection of classified information derive from two independent but interrelated corpora of work. In 1974, David Bell and Leonard LaPadula of MITRE, working under the close technical guidance and economic sponsorship of Maj. Roger Schell, Ph.D., of the U.S. Army Electronic Systems Command (Ft. Hanscom, MA), devised what is known as the Bell-LaPadula model, in which a more or less trustworthy computer system is modeled in terms of objects (passive repositories or destinations for data, such as files, disks, printers) and subjects (active entities—perhaps users, or system processes or threads operating on behalf of those users—that cause information to flow among objects). The entire operation of a computer system can indeed be regarded a \"history\" (in the serializability-theoretic sense) of pieces of information flowing from object to object in response to subjects' requests for such flows.\n\nAt the same time, Dorothy Denning at Purdue University was publishing her Ph.D. dissertation, which dealt with \"lattice-based information flows\" in computer systems. (A mathematical \"lattice\" is a partially ordered set, characterizable as a directed acyclic graph, in which the relationship between any two vertices is either \"dominates,\" \"is dominated by,\" or neither.) She defined a generalized notion of \"labels\"—corresponding more or less to the full security markings one encounters on classified military documents, \"e.g.\", TOP SECRET WNINTEL TK DUMBO—that are attached to entities. Bell and LaPadula integrated Denning's concept into their landmark MITRE technical report—entitled, \"Secure Computer System: Unified Exposition and Multics Interpretation\"—whereby labels attached to objects represented the sensitivity of data contained within the object (though there can be, and often is, a subtle semantic difference between the sensitivity of the data within the object and the sensitivity of the object itself), while labels attached to subjects represented the trustworthiness of the user executing the subject. The concepts are unified with two properties, the \"simple security property\" (a subject can only read from an object that it \"dominates\" [\"is greater than\" is a close enough—albeit mathematically imprecise—interpretation]) and the \"confinement property,\" or \"*-property\" (a subject can only write to an object that dominates it). (These properties are loosely referred to as \"no-read-up\" and \"no-write-down,\" respectively.) Jointly enforced, these properties ensure that information cannot flow \"downhill\" to a repository whence insufficiently trustworthy recipients may discover it. By extension, assuming that the labels assigned to subjects are truly representative of their trustworthiness, then the no-read-up and no-write-down rules rigidly enforced by the reference monitor are provably sufficient to constrain Trojan horses, one of the most general classes of attack (\"sciz.\", the popularly reported worms and viruses are specializations of the Trojan horse concept).\n\nThe Bell-LaPadula model technically only enforces \"confidentiality,\" or \"secrecy,\" controls, \"i.e.\", they address the problem of the sensitivity of objects and attendant trustworthiness of subjects to not inappropriately disclose it. The dual problem of \"integrity\"(i.e., the problem of accuracy, or even provenance of objects) and attendant trustworthiness of subjects to not inappropriately modify or destroy it, is addressed by mathematically affine models; the most important of which is named for its creator, K. J. Biba. Other integrity models include the Clark-Wilson model and Shockley and Schell's program integrity model, \"The SeaView Model\"\n\nAn important feature of MACs, is that they are entirely beyond the control of any user. The TCB automatically attaches labels to any subjects executed on behalf of users and files they access or modify. In contrast, an additional class of controls, termed discretionary access controls(DACs), are under the direct control of the system users. Familiar protection mechanisms such as permission bits (supported by UNIX since the late 1960s and—in a more flexible and powerful form—by Multics since earlier still) and access control lists (ACLs) are familiar examples of DACs.\n\nThe behavior of a trusted system is often characterized in terms of a mathematical model—which may be more or less rigorous depending upon applicable operational and administrative constraints—that takes the form of a finite state machine (FSM) with state criteria, state transition constraints, a set of \"operations\" that correspond to state transitions (usually, but not necessarily, one), and a descriptive top-level specification (DTLS) which entails a user-perceptible interface (\"e.g.\", an API, a set of system calls [in UNIX parlance] or system exits [in mainframe parlance]); each element of which engenders one or more model operations.\n\nThe Trusted Computing Group creates specifications that are meant to address particular requirements of trusted systems, including attestation of configuration and safe storage of sensitive information.\n\nTrusted systems in the context of national or homeland security, law enforcement, or social control policy are systems in which some conditional prediction about the behavior of people or objects within the system has been determined prior to authorizing access to system resources.\n\nFor example, trusted systems include the use of \"security envelopes\" in national security and counterterrorism applications, \"trusted computing\" initiatives in technical systems security, and the use of credit or identity scoring systems in financial and anti-fraud applications; in general, they include any system (i) in which probabilistic threat or risk analysis is used to assess \"trust\" for decision-making before authorizing access or for allocating resources against likely threats (including their use in the design of systems constraints to control behavior within the system), or (ii) in which deviation analysis or systems surveillance is used to ensure that behavior within systems complies with expected or authorized parameters.\n\nThe widespread adoption of these authorization-based security strategies (where the default state is DEFAULT=DENY) for counterterrorism, anti-fraud, and other purposes is helping accelerate the ongoing transformation of modern societies from a notional Beccarian model of criminal justice based on accountability for deviant actions after they occur – see Cesare Beccaria, On Crimes and Punishment (1764) – to a Foucauldian model based on authorization, preemption, and general social compliance through ubiquitous preventative surveillance and control through system constraints – see Michel Foucault, \"Discipline and Punish\" (1975, Alan Sheridan, tr., 1977, 1995).\n\nIn this emergent model, \"security\" is geared not towards policing but to risk management through surveillance, exchange of information, auditing, communication, and classification. These developments have led to general concerns about individual privacy and civil liberty and to a broader philosophical debate about the appropriate forms of social governance methodologies.\n\nTrusted systems in the context of information theory is based on the definition of trust as 'Trust is that which is essential to a communication channel but cannot be transferred from a source to a destination using that channel' by Ed Gerck.\n\nIn Information Theory, information has nothing to do with knowledge or meaning. In the context of Information Theory, information is simply that which is transferred from a source to a destination, using a communication channel. If, before transmission, the information is available at the destination then the transfer is zero. Information received by a party is that which the party does not expect—as measured by the uncertainty of the party as to what the message will be.\n\nLikewise, trust as defined by Gerck has nothing to do with friendship, acquaintances, employee-employer relationships, loyalty, betrayal and other overly-variable concepts. Trust is not taken in the purely subjective sense either, nor as a feeling or something purely personal or psychological—trust is understood as something potentially communicable. Further, this definition of trust is abstract, allowing different instances and observers in a trusted system to communicate based on a common idea of trust (otherwise communication would be isolated in domains), where all necessarily different subjective and intersubjective realizations of trust in each subsystem (man and machines) may coexist.\n\nTaken together in the model of Information Theory, information is what you do not expect and trust is what you know. Linking both concepts, trust is seen as qualified reliance on received information. In terms of trusted systems, an assertion of trust cannot be based on the record itself, but on information from other information channels. The deepening of these questions leads to complex conceptions of trust which have been thoroughly studied in the context of business relationships. It also leads to conceptions of information where the \"quality\" of information integrates trust or trustworthiness in the structure of the information itself and of the information system(s) in which it is conceived: higher quality in terms of particular definitions of accuracy and precision means higher trustworthiness.\n\nAn introduction to the calculus of trust (Example: 'If I connect two trusted systems, are they more or less trusted when taken together?') is given in.\n\nThe IBM Federal Software Group has suggested that provides the most useful definition of trust for application in an information technology environment, because it is related to other information theory concepts and provides a basis for measuring trust. In a network centric enterprise services environment, such notion of trust is considered to be requisite for achieving the desired collaborative, service-oriented architecture vision.\n\n\nSee also, The Trusted Systems Project, a part of the Global Information Society Project (GISP), a joint research project of the World Policy Institute (WPI) and the Center for Advanced Studies in Sci. & Tech. Policy (CAS).\n"}
{"id": "172990", "url": "https://en.wikipedia.org/wiki?curid=172990", "title": "Use–mention distinction", "text": "Use–mention distinction\n\nThe use–mention distinction is a foundational concept of analytic philosophy, according to which it is necessary to make a distinction between a word (or phrase) and it, and many philosophical works have been \"vitiated by a failure to distinguish use and mention\". The distinction is disputed by non-analytic philosophers.\n\nThe distinction between use and mention can be illustrated for the word \"cheese\":\n\nThe first sentence is a statement about the substance called \"cheese\"; it uses the word 'cheese' to refer to that substance. The second is a statement about the word 'cheese' as a signifier; it mentions the word without using it to refer to anything other than itself.\n\nIn written language, mentioned words or phrases often appear between quotation marks (as in Chicago' contains three vowels\") or in italics (as in \"When I say \"honey\", I mean the sweet stuff that bees make\"), and style authorities such as \"Strunk and White\" insist that mentioned words or phrases must always be made visually distinct in this manner. Used words or phrases (much more common than mentioned ones) do not bear any typographic distinction. In spoken language, or in absence of the use of stylistic cues such as quotation marks or italics in written language, the audience must identify mentioned words or phrases through semantic and pragmatic cues.\n\nIf quotation marks are used, it is sometimes the practice to distinguish between the quotation marks used for speech and those used for mentioned words, with double quotes in one place and single in the other:\n\nA few authorities recommend against such a distinction, and prefer one style of quotation mark to be used for both purposes.\n\nThe general phenomenon of a term's having different references in different contexts was called \"suppositio\" (substitution) by medieval logicians. It describes how one has to substitute a term in a sentence based on its meaning—that is, based on the term's referent. In general, a term can be used in several ways. For nouns, they are:\n\nThe last sentence contains a mention example.\n\nThe use–mention distinction is especially important in analytic philosophy. Failure to properly distinguish use from mention can produce false, misleading, or meaningless statements or category errors. For example, the following correctly distinguish between use and mention:\n\nThe first sentence, a mention example, is a statement about the word \"copper\" and not the chemical element. Notably, the word is composed of six letters, but not any kind of metal or other tangible thing. The second sentence, a use example, is a statement about the chemical element copper and not the word itself. Notably, the element is composed of 29 electrons and protons and a number of neutrons, but not any letters.\n\nStanisław Leśniewski was perhaps the first to make widespread use of this distinction and the fallacy that arises from overlooking it, seeing it all around in analytic philosophy of the time, for example in Russell and Whitehead's \"Principia Mathematica\". At the logical level, a use–mention mistake occurs when two heterogeneous levels of meaning or context are confused inadvertently.\nDonald Davidson told that in his student years, \"quotation was usually introduced as a somewhat shady device, and the introduction was accompanied by a stern sermon on the sin of confusing the use and mention of expressions\". He presented a class of sentences like\n\nwhich both use the meaning of the quoted words to complete the sentence, and mention them as they are attributed to W. V. Quine, to argue against his teachers' hard distinction. His claim was that quotations could not be analyzed as simple expressions that mention their content by means of naming it or describing its parts, as sentences like the above would lose their exact, twofold meaning.\n\nSelf-referential statements mention themselves or their components, often producing logical paradoxes, such as Quine's paradox. A mathematical analogy of self-referential statements lies at the core of Gödel's incompleteness theorem (diagonal lemma). There are many examples of self-reference and use–mention distinction in the works of Douglas Hofstadter, who makes the distinction thus:\n\nAlthough the standard notation for mentioning a term in philosophy and logic is to put the term in quotation marks, issues arise when the mention is itself of a mention. Notating using italics might require a potentially infinite number of typefaces, while putting quotation marks within quotation marks may lead to ambiguity.\n\nSome analytic philosophers have said the distinction \"may seem rather pedantic\".\n\nIn a 1977 response to analytic philosopher John Searle, Jacques Derrida mentioned the distinction as \"rather laborious and problematical\".\n\n\n\n\n"}
{"id": "449568", "url": "https://en.wikipedia.org/wiki?curid=449568", "title": "−1", "text": "−1\n\nIn mathematics, −1 is the additive inverse of 1, that is, the number that when added to 1 gives the additive identity element, 0. It is the negative integer greater than negative two (−2) and less than 0.\n\nNegative one bears relation to Euler's identity since \"e\" = −1.\n\nIn software development, −1 is a common initial value for integers and is also used to show that a variable contains no useful information.\n\nIn programming languages, −1 can be used to index the last (or 2nd last) item of an array, depending on whether 0 or 1 represents the first item.\n\nNegative one has some similar but slightly different properties to positive one.\n\nMultiplying a number by −1 is equivalent to changing the sign on the number. This can be proved using the distributive law and the axiom that 1 is the multiplicative identity: for \"x\" real, we have\n\nwhere we used the fact that any real \"x\" times 0 equals 0, implied by cancellation from the equation\n\nIn other words,\n\nso (−1) · \"x\", or −\"x\", is the arithmetic inverse of \"x\".\n\nThe square of −1, i.e. −1 multiplied by −1, equals 1. As a consequence, a product of two negative real numbers is positive.\n\nFor an algebraic proof of this result, start with the equation\n\nThe first equality follows from the above result. The second follows from the definition of −1 as additive inverse of 1: it is precisely that number that when added to 1 gives 0. Now, using the distributive law, we see that\n\nThe second equality follows from the fact that 1 is a multiplicative identity. But now adding 1 to both sides of this last equation implies\n\nThe above arguments hold in any ring, a concept of abstract algebra generalizing integers and real numbers.\n\nAlthough there are no real square roots of -1, the complex number \"i\" satisfies \"i\" = −1, and as such can be considered as a square root of −1. The only other complex number who's square is 1 is −\"i\". In the algebra of quaternions, which contain the complex plane, the equation \"x\" = −1 has infinite solutions.\n\nExponentiation of a non-zero real number can be extended to negative integers. We make the definition that \"x\" = , meaning that we define raising a number to the power −1 to have the same effect as taking its reciprocal. This definition is then extended to negative integers preserves the exponential law \"x\"\"x\" = \"x\" for real numbers \"a\" and \"b\".\n\nExponentiation to negative integers can be extended to invertible elements of a ring, by defining \"x\" as the multiplicative inverse of \"x\".\n\n−1 that appears next to functions or matrices does not mean raising them to the power −1 but their inverse functions or inverse matrices. For example, \"f\"(\"x\") is the inverse of \"f\"(\"x\"), or sin(\"x\") is a notation of arcsine function.\n\nMost computer systems represent negative integers using two's complement. In such systems, −1 is represented using a bit pattern of all ones. For example, an 8-bit signed integer using two's complement would represent −1 as the bitstring \"11111111\", or \"FF\" in hexadecimal (base 16). If interpreted as an unsigned integer, the same bitstring of \"n\" ones represents 2 − 1, the largest possible value that \"n\" bits can hold. For example, the 8-bit string \"11111111\" above represents 2 − 1 = 255.\n\nIn some programming languages, when used to index some data types (such as an array), then -1 can be used to identify the very last (or 2nd last) item, depending on whether 0 or 1 represents the first item.\nIf the first item is indexed by 0, then -1 identifies the last item.\nIf the first item is indexed by 1, then -1 identifies the second-to-last item.\n"}
