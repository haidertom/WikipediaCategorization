{"id": "6679056", "url": "https://en.wikipedia.org/wiki?curid=6679056", "title": "A priori and a posteriori", "text": "A priori and a posteriori\n\nThe Latin phrases a priori ( \"from the earlier\") and a posteriori ( \"from the later\") are philosophical terms of art popularized by Immanuel Kant's \"Critique of Pure Reason\" (first published in 1781, second edition in 1787), one of the most influential works in the history of philosophy. However, in their Latin forms they appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nThese terms are used with respect to reasoning (epistemology) to distinguish \"necessary conclusions from first premises\" (i.e., what must come before sense observation) from \"conclusions based on sense observation\" which must follow it. Thus, the two kinds of knowledge, justification, or argument, may be glossed:\n\n\nThere are many points of view on these two types of knowledge, and their relationship gives rise to one of the oldest problems in modern philosophy.\n\nThe terms \"a priori\" and \"a posteriori\" are primarily used as adjectives to modify the noun \"knowledge\" (for example, \"\"a priori\" knowledge\"). However, \"a priori\" is sometimes used to modify other nouns, such as \"truth\". Philosophers also may use \"apriority\" and \"aprioricity\" as nouns to refer (approximately) to the quality of being \"a priori\".\n\nAlthough definitions and use of the terms have varied in the history of philosophy, they have consistently labeled two separate epistemological notions. See also the related distinctions: deductive/inductive, analytic/synthetic, necessary/contingent.\n\nThe intuitive distinction between \"a priori\" and \"a posteriori\" knowledge (or justification) is best seen via examples, as below:\n\n\nSeveral philosophers reacting to Kant sought to explain \"a priori\" knowledge without appealing to, as Paul Boghossian (MD) explains, \"a special faculty ... that has never been described in satisfactory terms.\" One theory, popular among the logical positivists of the early 20th century, is what Boghossian calls the \"analytic explanation of the a priori.\" The distinction between analytic and synthetic propositions was first introduced by Kant. While Kant's original distinction was primarily drawn in terms of conceptual containment, the contemporary version of the distinction primarily involves, as the American philosopher W. V. O. Quine put it, the notions of \"true by virtue of meanings and independently of fact.\" \"Analytic\" propositions are thought to be true in virtue of their meaning alone, while \"a posteriori analytic\" propositions are thought to be true in virtue of their meaning \"and\" certain facts about the world. According to the analytic explanation of the \"a priori\", all \"a priori\" knowledge is analytic; so \"a priori\" knowledge need not require a special faculty of pure intuition, since it can be accounted for simply by one's ability to understand the meaning of the proposition in question. In short, proponents of this explanation claimed to have reduced a dubious metaphysical faculty of pure reason to a legitimate linguistic notion of analyticity.\n\nHowever, the analytic explanation of \"a priori\" knowledge has undergone several criticisms. Most notably, Quine argued that the analytic–synthetic distinction is illegitimate. Quine states: \"But for all its a priori reasonableness, a boundary between analytic and synthetic statements simply has not been drawn. That there is such a distinction to be drawn at all is an unempirical dogma of empiricists, a metaphysical article of faith.\" While the soundness of Quine's critique is highly disputed, it had a powerful effect on the project of explaining the \"a priori\" in terms of the analytic.\n\nThe metaphysical distinction between necessary and contingent truths has also been related to \"a priori\" and \"a posteriori\" knowledge. A proposition that is \"necessarily true\" is one whose negation is self-contradictory (thus, it is said to be true in every possible world). Consider the proposition that all bachelors are unmarried. Its negation, the proposition that some bachelors are married, is incoherent, because the concept of being unmarried (or the meaning of the word \"unmarried\") is part of the concept of being a bachelor (or part of the definition of the word \"bachelor\"). To the extent that contradictions are impossible, self-contradictory propositions are necessarily false, because it is impossible for them to be true. Thus, the negation of a self-contradictory proposition is supposed to be necessarily true. By contrast, a proposition that is \"contingently true\" is one whose negation is not self-contradictory (thus, it is said that it is \"not\" true in every possible world). As Jason Baehr states, it seems plausible that all necessary propositions are known \"a priori\", because \"[s]ense experience can tell us only about the actual world and hence about what is the case; it can say nothing about what must or must not be the case.\"\n\nFollowing Kant, some philosophers have considered the relationship between aprioricity, analyticity, and necessity to be extremely close. According to Jerry Fodor, \"Positivism, in particular, took it for granted that \"a priori\" truths must be necessary...\" However, since Kant, the distinction between analytic and synthetic propositions had slightly changed. Analytic propositions were largely taken to be \"true by virtue of meanings and independently of fact\", while synthetic propositions were not—one must conduct some sort of empirical investigation, looking to the world, to determine the truth-value of synthetic propositions.\n\nAprioricity, analyticity, and necessity have since been more clearly separated from each other. The American philosopher Saul Kripke (1972), for example, provided strong arguments against this position. Kripke argued that there are necessary \"a posteriori\" truths, such as the proposition that water is HO (if it is true). According to Kripke, this statement is necessarily true (since water and HO are the same thing, they are identical in every possible world, and truths of identity are logically necessary) and \"a posteriori\" (since it is known only through empirical investigation). Following such considerations of Kripke and others (such as Hilary Putnam), philosophers tend to distinguish more clearly the notion of aprioricity from that of necessity and analyticity.\n\nKripke's definitions of these terms, however, diverge in subtle ways from those of Kant. Taking these differences into account, Kripke's controversial analysis of naming as contingent and \"a priori\" would, according to Stephen Palmquist, best fit into Kant's epistemological framework by calling it \"analytic a posteriori\". Aaron Sloman presented a brief defence of Kant's three distinctions (analytic/synthetic, apriori/empirical and necessary/contingent) in . It did not assume \"possible world semantics\" for the third distinction, merely that some part of \"this\" world might have been different.\n\nThus, the relationship between aprioricity, necessity, and analyticity is not easy to discern. However, most philosophers at least seem to agree that while the various distinctions may overlap, the notions are clearly not identical: the \"a priori\"/\"a posteriori\" distinction is epistemological, the analytic/synthetic distinction is linguistic, and the necessary/contingent distinction is metaphysical.\n\nThe phrases \"\"a priori\" and \"a posteriori\"\" are Latin for \"from what comes before\" and \"from what comes later\" (or, less literally, \"from first principles, before experience\" and \"after experience\"). They appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nAn early philosophical use of what might be considered a notion of \"a priori\" knowledge (though not called by that name) is Plato's theory of recollection, related in the dialogue \"Meno\" (380 ), according to which something like \"a priori\" knowledge is knowledge inherent, intrinsic in the human mind.\n\nAlbert of Saxony, a 14th-century logician, wrote on both \"a priori\" and \"a posteriori\".\n\nG. W. Leibniz introduced a distinction between \"a priori\" and \"a posteriori\" criteria for the possibility of a notion in his (1684) short treatise \"Meditations on Knowledge, Truth, and Ideas\". \"A priori\" and \"a posteriori\" arguments for the existence of God appear in his \"Monadology\" (1714).\n\nGeorge Berkeley outlined the distinction in his 1710 work \"A Treatise Concerning the Principles of Human Knowledge\" (para. XXI).\n\nThe 18th-century German philosopher Immanuel Kant (1781) advocated a blend of rationalist and empiricist theories. Kant says, \"Although all our cognition begins with experience, it does not follow that it arises [is caused by] from experience\" According to Kant, \"a priori\" cognition is transcendental, or based on the \"form\" of all possible experience, while \"a posteriori\" cognition is empirical, based on the \"content\" of experience. Kant states, \"[…] it is quite possible that our empirical knowledge is a compound of that which we receive through impressions, and that which the faculty of cognition supplies from itself sensuous impressions [sense data] giving merely the \"occasion\" [opportunity for a cause to produce its effect].\" Contrary to contemporary usages of the term, Kant thinks that \"a priori\" knowledge is not entirely independent of the content of experience. And unlike the rationalists, Kant thinks that \"a priori\" cognition, in its pure form, that is without the admixture of any empirical content, is limited to the deduction of the conditions of possible experience. These \"a priori\", or transcendental conditions, are seated in one's cognitive faculties, and are not provided by experience in general or any experience in particular (although an argument exists that \"a priori\" intuitions can be \"triggered\" by experience). \n\nKant nominated and explored the possibility of a transcendental logic with which to consider the deduction of the \"a priori\" in its pure form. Space, time and causality are considered pure \"a priori\" intuitions. Kant reasoned that the pure \"a priori\" intuitions are established via his transcendental aesthetic and transcendental logic. He claimed that the human subject would not have the kind of experience that it has were these \"a priori\" forms not in some way constitutive of him as a human subject. For instance, a person would not experience the world as an orderly, rule-governed place unless time, space and causality were determinant functions in the form of perceptual faculties, i. e., there can be no experience in general without space, time or causality as particular determinants thereon. The claim is more formally known as Kant's transcendental deduction and it is the central argument of his major work, the \"Critique of Pure Reason\". The transcendental deduction argues that time, space and causality are ideal as much as real. In consideration of a possible logic of the \"a priori\", this most famous of Kant's deductions has made the successful attempt in the case for the fact of subjectivity, what constitutes subjectivity and what relation it holds with objectivity and the empirical.\n\nAfter Kant's death, a number of philosophers saw themselves as correcting and expanding his philosophy, leading to the various forms of German Idealism. One of these philosophers was Johann Fichte. His student (and critic), Arthur Schopenhauer, accused him of rejecting the distinction between \"a priori\" and \"a posteriori\" knowledge:\n\n\n\n\n"}
{"id": "38235255", "url": "https://en.wikipedia.org/wiki?curid=38235255", "title": "Affirmation and negation", "text": "Affirmation and negation\n\nIn linguistics and grammar, affirmation and negation (abbreviated respectively ' and ') are the ways that grammar encode negative and positive polarity in verb phrases, clauses, or other utterances. Essentially an affirmative (positive) form is used to express the validity or truth of a basic assertion, while a negative form expresses its falsity. Examples are the sentences \"Jane is here\" and \"Jane is not here\"; the first is affirmative, while the second is negative.\n\nThe grammatical category associated and with affirmative and negative is called polarity. This means that a sentence, verb phrase, etc. may be said to have either affirmative or negative polarity (its polarity may be either affirmative or negative). Affirmative is typically the unmarked polarity, whereas a negative statement is marked in some way, whether by a negating word or particle such as English \"not\", an affix such as Japanese -\"nai\", or by other means, which reverses the meaning of the predicate. The process of converting affirmative to negative is called negation – the grammatical rules for negation vary from language to language, and a given language may have more than one method of doing so.\n\nAffirmative and negative responses (especially, though not exclusively, to questions) are often expressed using particles or words such as \"yes\" and \"no\", where \"yes\" is the affirmative and \"no\" the negative particle.\n\nSpecial affirmative and negative words (particles) are often found in responses to questions, and sometimes to other assertions by way of agreement or disagreement. In English, these are \"yes\" and \"no\" respectively, in French \"oui\", \"si\" and \"non\", in Swedish \"ja\", \"jo\" and \"nej\", and so on. Not all languages make such common use of particles of this type; in some (such as Welsh) it is more common to repeat the verb or another part of the predicate, with or without negation accordingly.\n\nComplications sometimes arise in the case of responses to negative statements or questions; in some cases the response that confirms a negative statement is the negative particle (as in English: \"You're not going out? No.\"), but in some languages this is reversed. Some languages have a distinct form to answer a negative question, such as French \"si\" and Swedish \"jo\" (these serve to contradict the negative statement suggested by the first speaker).\n\nLanguages have a variety of grammatical rules for converting affirmative verb phrases or clauses into negative ones.\n\nIn many languages, an affirmative is made negative by the addition of a particle, meaning \"not\". This may be added before the verb phrase, as with the Spanish \"no\":\nOther examples of negating particles preceding the verb phrase include Italian \"non\", Russian не \"nye\" and Polish \"nie\" (they can also be found in constructed languages: \"ne\" in Esperanto and \"non\" in Interlingua). In some other languages the negating particle follows the verb or verb phrase, as in Dutch:\nParticles following the verb in this way include \"not\" in archaic and dialectal English (\"you remember not\"), \"nicht\" in German (\"ich schlafe nicht\", \"I am not sleeping\"), and \"inte\" in Swedish (\"han hoppade inte\", \"he did not jump\").\n\nIn French, particles are added both before the verb phrase (\"ne\") and after the verb (\"pas\"):\nHowever, in colloquial French the first particle is often omitted: \"Je sais pas\". Similar use of two negating particles can also be found in Afrikaans: \"Hy kan nie Afrikaans praat nie\" (\"He cannot speak Afrikaans\").\n\nIn standard Modern English, negation is achieved by adding \"not\" after an auxiliary verb (which here means one of a special grammatical class of verbs that also includes forms of the copula \"be\"; see English auxiliaries). If no such verb is present then the dummy auxiliary \"do\" (\"does\", \"did\") is introduced – see \"do\"-support. For example:\nDifferent rules apply in subjunctive, imperative and non-finite clauses. For more details see . (In Middle English, the particle \"not\" could follow any verb, e.g. \"I see not the horse.\")\n\nIn some languages, like Welsh, verbs have special inflections to be used in negative clauses. (In some language families, this may lead to reference to a negative mood.) An example is Japanese, which conjugates verbs in the negative after adding the suffix \"-nai\" (indicating negation), e.g. \"taberu\" (\"eat\") and \"tabenai\" (\"do not eat\"). It could be argued that English has joined the ranks of these languages, since negation requires the use of an auxiliary verb and a distinct syntax in most cases; the form of the basic verb can change on negation, as in \"he sings\" vs. \"he doesn't sing\". Zwicky and Pullum have shown that \"n't\" is an inflectional suffix, not a clitic or a derivational suffix.\n\nComplex rules for negation also apply in Finnish; see . In some languages negation may also affect the dependents of the verb; for example in some Slavic languages, such as Russian, the case of a direct object often changes from accusative to genitive when the verb is negated.\n\nNegation can be applied not just to whole verb phrases, clauses or sentences, but also to specific elements (such as adjectives and noun phrases) within sentences. Ways in which this can be done again depend on the grammar of the language in question. English generally places \"not\" before the negated element, as in \"I witnessed not a debate, but a war.\" There are also negating affixes, such as the English prefixes \"non-\", \"un-\", \"in-\", etc. Such elements are called privatives.\n\nThere also exist elements which carry a specialized negative meaning, including pronouns such as \"nobody\", \"none\" and \"nothing\", determiners such as \"no\" (as in \"no apples\"), and adverbs such as \"never\", \"no longer\" and \"nowhere\".\n\nAlthough such elements themselves have negative force, in some languages a clause in which they appear is additionally marked for ordinary negation. For example, in Russian, \"I see nobody\" is expressed as я никого́ не ви́жу \"ja nikovó nye vízhu\", literally \"I nobody not see\" – the ordinary negating particle не \"nye\" (\"not\") is used in addition to the negative pronoun никого́ \"nikovó\" (\"nobody\"). Italian behaves in a similar way: \"Non ti vede nessuno\", \"nobody can see you\", although \"Nessuno ti vede\" is also a possible clause with exactly the same meaning.\n\nIn Russian, all of the elements (\"not\", \"never\", \"nobody\", \"nowhere\") would appear together in the sentence in their negative form. In Italian, a clause works much as in Russian, but \"non\" does not have to be there, and can be there only before the verb if it precedes all other negative elements: \"Tu non porti mai nessuno da nessuna parte\". \"Nobody ever brings you anything here\", however, could be translated \"Nessuno qui ti porta mai niente\" or \"Qui non ti porta mai niente nessuno\". In French, where simple negation is performed using \"ne ... pas\" (see above), specialized negatives appear in combination with the first particle (\"ne\"), but \"pas\" is omitted:\nIn Ancient Greek, a simple negative (οὐ \"ou\" \"not\" or μή \"mḗ\" \"not (modal)\") following another simple or compound negative (e.g. οὐδείς \"oudeís\" \"nobody\") results in an affirmation, whereas a compound negative following a simple or compound negative strengthens the negation:\n\nSimple grammatical negation of a clause in principle has the effect of converting a proposition to its logical negation – replacing an assertion that something is the case by an assertion that it is not the case.\n\nIn some cases, however, particularly when a particular modality is expressed, the semantic effect of negation may be somewhat different. For example, in English, the meaning of \"you must not go\" is not in fact the exact negation of that of \"you must go\" – this would be expressed as \"you don't have to go\" or \"you needn't go\". The negation \"must not\" has a stronger meaning (the effect is to apply the logical negation to the following infinitive rather than to the full clause with \"must\"). For more details and other similar cases, see the relevant sections of English modal verbs.\n\nIn some cases, by way of irony, an affirmative statement may be intended to have the meaning of the corresponding negative, or vice versa. For examples see antiphrasis and sarcasm.\n\nFor the use of double negations or similar as understatements (\"not unappealing\", \"not bad\", etc.) see litotes.\n\n\n\n"}
{"id": "55063170", "url": "https://en.wikipedia.org/wiki?curid=55063170", "title": "Afro-pessimism", "text": "Afro-pessimism\n\nAfro-pessimism is a framework and critical idiom that describes the ongoing effects of racism, colonialism, and historical processes of enslavement including the Trans-Atlantic slave trade, and their impact on structural conditions as well as personal, subjective, and lived experience, and embodied reality. \n\nThe term was first coined in 1990 in an article in \"Jeune Afrique Economie\" by Francophone Cameroonian author Sony Lab'ou Tansi. Writer and intellectual Frank B. Wilderson III developed the term in his political memoir about his time spent teaching and participating in the African National Congress in South Africa during apartheid. \n\nAccording to Tansi, \"Afro-pessimism [is] a terrible word used to conceal the greatest mess of all time,\" which is the \"tragedy\" that Africa's position \"dooms us to construct and build garbage economies in the depths of the most cruel, unbearable, and inhuman form of indignity that humans can swallow\" (as translated by John Conteh-Morgan). According to Wilderson, afro-pessimism theorizes blackness as a position of \"accumulation and fungibility\" (Saidiya Hartman); that is, as condition—or relation—of ontological death, as opposed to a cultural identity or human subjectivity.\n\nWilderson, along with Hortense Spillers, Saidiya Hartman, Achille Mbembe, Jared Sexton, and others who have contributed to afro-pessimist thought, cite the Martinician psychiatrist, philosopher, and writer Frantz Fanon as a foundational figure in the tradition of Afro-pessimism. \n\nAfro-pessimism has been constructed in many ways and with different aims. But Afro-pessimism is chiefly approached a transcendent position, not as a negative or disaffected political attitude in the sense that pessimism might seemingly connote. The Black radical tradition has drawn upon the term as a way to acknowledge the power, depth, and vitality of the resilience and radical imagination of people of African descent. Within this same critique, some have used Afro-pessimism to articulate the subject-position of renunciation, refusal, distancing, dread, doubt and abjection in response to the multitude and ongoing effects and historical traumas of colonialism. This includes the view that dismantling white supremacy would mean dismantling much of the social and political institutions of the modern world.\n\nDiscussions of Afro-pessimism have manifest in an online context, and have continued in Afro-pessimist approaches to art, poetics, and computing.\n\nAfro-pessimist ideas have been part of ongoing conversations about pan-African identity, as an inclusionary concept of blackness among all people of African descent. Pan-African thought has drawn attention to the shared racial identity and also the particulars of the expression of African identity among the African Diaspora and peoples on the African continent. Pan-African thought has analyzed the ongoing struggles of African peoples, and the power of Afrocentricity as a move away from the colonialism and violence of Eurocentricity. The writings of Frantz Fanon, a Martinican psychiatrist, intellectual, and revolutionary, reflect pan-African and Afro-pessimistic approaches to decolonization and black liberation. \n\nThe Pan-African movement négritude represents pessimism as a kind of realist recognition of the historical traumas of colonialism, from an existentialist position. A key figure in the movement, Aimé Césaire, uses pessimism to consider transcendence and a recognition of the breadth of the cultural imagination and perseverance of people of African descent.\n\nAfro-pessimism has also been employed as a term describing a narrative in Western media and International relations theory that portrays post-colonial Africa as unlikely to achieve economic growth and democratic governance. This use of Afro-pessimism has nothing to with Wilderson's definition. This form of Afro-pessimism has been criticized as a Western construct regarding the ongoing portrayal of Africa and African people in Western media, overwhelmingly in terms of tragedy, doom, victimization, and victim-hood. Scholar Toussaint Nothias has characterized these discussions by the components, \"essentialism, racialization, selectivity, ranking framework, and prediction.\" From this Afro-pessimistic perspective, news media that portray Africa and African people by the trope of victimhood, mirror the Eurocentric and ethnocentric of the Western media, language, images, and rhetoric. In this ways the media tends to victimize and exoticize Africa for its going struggles with poverty, health-crisis, famine, and lack of modern development. The victimization is then visible in the humanitarian and development projects, which sometimes use the language of \"saving\" African people from such \"humanitarian disasters\".\n\n\n"}
{"id": "41099486", "url": "https://en.wikipedia.org/wiki?curid=41099486", "title": "Argument-deduction-proof distinctions", "text": "Argument-deduction-proof distinctions\n\nArgument-deduction-proof distinctions originated with logic itself. Naturally, the terminology evolved. \nAn argument, more fully a premise-conclusion argument, is a two-part system composed of premises and conclusion. An argument is \"valid\" if and only if its conclusion is a consequence of its premises. Every premise set has infinitely many consequences each giving rise to a valid argument. Some consequences are obviously so but most are not: most are hidden consequences. Most valid arguments are not yet known to be valid. To determine validity in non-obvious cases deductive reasoning is required. There is no deductive reasoning in an argument \"per se\"; such must come from the outside. \n\nEvery argument's conclusion is a premise of other arguments. The word \"constituent\" may be used for either a premise or conclusion.In the context of this article and in most classical contexts, all candidates for consideration as argument constituents fall under the category of truth-bearer: propositions, statements, sentences, judgments, etc.\n\nA deduction is a three-part system composed of premises, a conclusion, and chain of intermediates — steps of reasoning showing that its conclusion is a consequence of its premises. The reasoning in a deduction is by definition cogent. Such reasoning itself, or the chain of intermediates representing it, has also been called an argument, more fully a deductive argument. In many cases, an argument can be known to be valid by means of a deduction of its conclusion from its premises but non-deductive methods such as Venn diagrams and other graphic procedures have been proposed.\n\nA proof is a deduction whose premises are known truths. A proof of the Pythagorean theorem is a deduction that might use several premises — axioms, postulates, and definitions — and contain dozens of intermediate steps. As Alfred Tarski famously emphasized in accord with Aristotle, truths can be known by proof but proofs presuppose truths not known by proof.\nPremise-conclusion arguments do not require or produce either knowledge of validity or knowledge of truth. Premise sets may be chosen arbitrarily and conclusions may be chosen arbitrarily. \nDeductions require knowing how to reason but they do not require knowledge of truth of their premises. Deductions produce knowledge of the validity of arguments but ordinarily they do not produce knowledge of the truth of their conclusions.\nProofs require knowledge of the truth of their premises, they require knowledge of deductive reasoning, and they produce knowledge of their conclusions.\nModern logicians disagree concerning the nature of argument constituents.Quine devotes the first chapter of \"Philosophy of Logic\" to this issue. Historians have not even been able to agree on what Aristotle took as constituents.\nArgument-deduction-proof distinctions are inseparable from what have been called the \"consequence-deducibility\" distinction and the \"truth-and-consequence conception of proof\". Variations among argument-deduction-proof distinctions are not all terminological.\n\nLogician Alonzo Church never used the word \"argument\" in the above sense and had no synonym. Moreover, Church never explained that deduction is the process of producing knowledge of consequence and it never used the common noun \"deduction\" for an application of the deduction process. His primary focus in discussing proof was “conviction” produced by generation of chains of logical truths—not the much more widely applicable and more familiar general process of demonstration as found in pre-Aristotelian geometry and discussed by Aristotle. He did discuss deductions in the above sense but not by that name: he called them awkwardly “proofs from premises” — an expression he coined for the purpose.\n\nThe absence of argument-deduction-proof distinctions is entirely consonant with Church's avowed Platonistic logicism. Following Dummett's insightful remarks about Frege, which — \"mutatis mutandis\" — apply even more to Church, it might be possible to explain the today-surprising absence.\n"}
{"id": "33944016", "url": "https://en.wikipedia.org/wiki?curid=33944016", "title": "Canon (basic principle)", "text": "Canon (basic principle)\n\nThe concept of canon is very broad; in a general sense it refers to being a rule or a body of rules.\n\nThere are definitions that state it as: “the body of rules, principles, or standards accepted as axiomatic and universally binding in a field of study or art”. This can be related to such topics as literary canons or the canons of rhetoric, which is a topic within itself that describes the rules of giving a speech. There are five key principles, and when grouped together, are the principles set for giving speeches as seen with regard to Rhetoric. This is one such example of how the term canon is used in regard to rhetoric.\n\n"}
{"id": "33346439", "url": "https://en.wikipedia.org/wiki?curid=33346439", "title": "Conceptual design", "text": "Conceptual design\n\nConceptual Design is an early phase of the design process, in which the broad outlines of function and form of something are articulated. It includes the design of interactions, experiences, processes and strategies. It involves an understanding of people's needs - and how to meet them with products, services, & processes. Common artifacts of conceptual design are concept sketches and models.\n\n"}
{"id": "58267", "url": "https://en.wikipedia.org/wiki?curid=58267", "title": "Conceptual schema", "text": "Conceptual schema\n\nA 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.\n\nA conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization (\"entity classes\"), about which it is inclined to collect information, and characteristics of (\"attributes\") and associations between pairs of those things of significance (\"relationships\").\n\nBecause a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of \"external schema\" that each represent one person's view of the world around him or her. These are consolidated into a single \"conceptual schema\" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.\n\nThe model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a \"sub-type\" entity class is also an instance of the entity class's \"super-type\". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.\n\nSuper-type/sub-type relationships may be \"exclusive\" or not. A methodology may require that each instance of a super-type may \"only\" be an instance of \"one\" sub-type. Similarly, a super-type/sub-type relationship may be \"exhaustive\" or not. It is exhaustive if the methodology requires that each instance of a super-type \"must be\" an instance of a sub-type. A sub-type named other is often necessary.\n\n\nA data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.\n\n\n\n"}
{"id": "773298", "url": "https://en.wikipedia.org/wiki?curid=773298", "title": "Conceptual system", "text": "Conceptual system\n\nA conceptual system is a system that is composed of non-physical objects, i.e. ideas or concepts. In this context a system is taken to mean \"an interrelated, interworking set of objects\".\n\nA conceptual system is a conceptual model. Such systems may be related to any topic from formal science to individual imagination. Conceptual systems may be found within the human mind, as works of art and fiction, and within the academic world. Indeed, this article may be understood as a conceptual system because it includes a set of interrelated concepts.\n\nBroadly, when a conceptual system includes a range of values, ideas, and beliefs the conceptual system is said be a view of the world. In psychology and social work, a conceptual system may refer to an individual's mental model of the world. In humans, a conceptual system may be understood as kind of a metaphor for the world. In science, there are many forms of conceptual systems including laws, theories, and models. Those conceptual systems may be developed through inductive reasoning, deductive reasoning, and empirical analysis.\n\nThe idea that the human mind might contain conceptual systems goes back at least as far as Kelly's personal construct theory in 1955. More recently, many scholars discuss conceptual systems and the importance of understanding them (c.f. Bateson, Luhmann, Senge, Quine, Eco, Umpleby, and Wallis). On the personal level, the human mind is generally held to contain a wide range of conceptual systems although they are not well organized. Indeed, our minds are full of conflicting mental models which makes decision making unreliable - particularly in large-scale, complex situations.\n\nWithin the academic literature, each theory may be understood as a conceptual system. Conceptual systems are generally held to have greater value when they are more useful, based on more research, and are more systemically interrelated.\n\nGenerally, that validity may also be described in terms of its internal coherence and the correspondence between the conceptual system and other systems (e.g. social systems or physical systems). Coherence may be tested by Integrative complexity (for individuals) and by Integrative Propositional Analysis (for academic theories). Correspondence is generally tested by empirical analysis and conditions of falsifiability. The conceptual system may then be said to model the physical or social system and serve as a guide for individual behavior or academic research.\n\nExamples of conceptual systems include:\n\nA concept is an abstract idea or a mental symbol, typically associated with a corresponding representation in language or symbology, that denotes all of the objects in a given category or class of entities, interactions, phenomena, or relationships between them. Concepts are abstract in that they omit the differences of the things in extension, treating them as if they were identical. They are universal in that they apply equally to everything in their extension. Concepts are also the basic elements of propositions, much the same way a word is the basic semantic element of a sentence. Unlike perceptions, which are particular images of individual objects, concepts cannot be visualized. Because they are not, themselves, individual perceptions, concepts are discursive and result from reason. They can only be thought about, or designated, by means of a name. Words are not concepts. Words are signs for concepts.\n\nA conceptual model is a representation of some phenomenon, data or theory by logical and mathematical objects such as functions, relations, tables, stochastic processes, formulas, axiom systems, rules of inference etc. A conceptual model has an ontology, that is the set of expressions in the model which are \"intended\" to denote some aspect of the modeled object. Here we are deliberately vague as to how expressions are constructed in a model and particularly what the logical structure of formulas in a model actually is. In fact, we have made no assumption that models are encoded in any formal logical system at all, although we briefly address this issue below. Moreover, the definition given here is oblivious to whether two expressions really should denote the same thing. Note that this notion of ontology is different from (and weaker than) ontology as is sometimes understood in philosophy; in our sense there is no claim that the expressions actually denote anything which exists \"physically\" or \"spatio-temporally\" (to use W. Quine's formulation).\n\nFor example, a stochastic model of stock prices includes in its ontology a sample space, random variables, the mean and variance of stock prices, various regression coefficients etc. Models of quantum mechanics in which pure states are represented as unit vectors in a Hilbert space include in their ontologies observables, dynamics, measurement operators etc. It is possible that observables and states of quantum mechanics are as physically real as the electrons they model, but by adopting this purely formal notion of ontology we avoid altogether this question.\n\nA conceptual framework is used in research to outline possible courses of action or to present a preferred approach to a system analysis project. It has also been defined as the organization of ideas to achieve a purpose The framework is built from a set of concepts linked to a planned or existing system of methods, behaviors, functions, relationships, and objects. A conceptual framework might, in computing terms, be thought of as a relational model.\n\nFor example, a conceptual framework of accounting \"seeks to identify the nature, subject, purpose and broad content of general-purpose financial reporting and the qualitative characteristics that financial information should possess\".\n\n\n"}
{"id": "58690", "url": "https://en.wikipedia.org/wiki?curid=58690", "title": "Crystal structure", "text": "Crystal structure\n\nIn crystallography, crystal structure is a description of the ordered arrangement of atoms, ions or molecules in a crystalline material. Ordered structures occur from the intrinsic nature of the constituent particles to form symmetric patterns that repeat along the principal directions of three-dimensional space in matter.\n\nThe smallest group of particles in the material that constitutes the repeating pattern is the unit cell of the structure. The unit cell completely defines the symmetry and structure of the entire crystal lattice, which is built up by repetitive translation of the unit cell along its principal axes. The repeating patterns are said to be located at the points of the Bravais lattice.\n\nThe lengths of the principal axes, or edges, of the unit cell and the angles between them are the lattice constants, also called \"lattice parameters\". The symmetry properties of the crystal are described by the concept of space groups. All possible symmetric arrangements of particles in three-dimensional space may be described by the 230 space groups.\n\nThe crystal structure and symmetry play a critical role in determining many physical properties, such as cleavage, electronic band structure, and optical transparency.\n\nCrystal structure is described in terms of the geometry of arrangement of particles in the unit cell. The unit cell is defined as the smallest repeating unit having the full symmetry of the crystal structure. The geometry of the unit cell is defined as a parallelepiped, providing six lattice parameters taken as the lengths of the cell edges (\"a\", \"b\", \"c\") and the angles between them (α, β, γ). The positions of particles inside the unit cell are described by the fractional coordinates (\"x\", \"y\", \"z\") along the cell edges, measured from a reference point. It is only necessary to report the coordinates of a smallest asymmetric subset of particles. This group of particles may be chosen so that it occupies the smallest physical space, which means that not all particles need to be physically located inside the boundaries given by the lattice parameters. All other particles of the unit cell are generated by the symmetry operations that characterize the symmetry of the unit cell. The collection of symmetry operations of the unit cell is expressed formally as the space group of the crystal structure.\n\nVectors and planes in a crystal lattice are described by the three-value Miller index notation. This syntax uses the indices \"ℓ\", \"m\", and \"n\" as directional orthogonal parameters, which are separated by 90°.\n\nBy definition, the syntax (\"ℓmn\") denotes a plane that intercepts the three points \"a\"/\"ℓ\", \"a\"/\"m\", and \"a\"/\"n\", or some multiple thereof. That is, the Miller indices are proportional to the inverses of the intercepts of the plane with the unit cell (in the basis of the lattice vectors). If one or more of the indices is zero, it means that the planes do not intersect that axis (i.e., the intercept is \"at infinity\"). A plane containing a coordinate axis is translated so that it no longer contains that axis before its Miller indices are determined. The Miller indices for a plane are integers with no common factors. Negative indices are indicated with horizontal bars, as in (13). In an orthogonal coordinate system for a cubic cell, the Miller indices of a plane are the Cartesian components of a vector normal to the plane.\n\nConsidering only (\"ℓmn\") planes intersecting one or more lattice points (the \"lattice planes\"), the distance \"d\" between adjacent lattice planes is related to the (shortest) reciprocal lattice vector orthogonal to the planes by the formula\n\nThe crystallographic directions are geometric lines linking nodes (atoms, ions or molecules) of a crystal. Likewise, the crystallographic planes are geometric \"planes\" linking nodes. Some directions and planes have a higher density of nodes. These high density planes have an influence on the behavior of the crystal as follows:\n\n\n\nSome directions and planes are defined by symmetry of the crystal system. In monoclinic, rhombohedral, tetragonal, and trigonal/hexagonal systems there is one unique axis (sometimes called the principal axis) which has higher rotational symmetry than the other two axes. The basal plane is the plane perpendicular to the principal axis in these crystal systems. For triclinic, orthorhombic, and cubic crystal systems the axis designation is arbitrary and there is no principal axis.\n\nFor the special case of simple cubic crystals, the lattice vectors are orthogonal and of equal length (usually denoted \"a\"); similarly for the reciprocal lattice. So, in this common case, the Miller indices (\"ℓmn\") and [\"ℓmn\"] both simply denote normals/directions in Cartesian coordinates. For cubic crystals with lattice constant \"a\", the spacing \"d\" between adjacent (ℓmn) lattice planes is (from above):\nBecause of the symmetry of cubic crystals, it is possible to change the place and sign of the integers and have equivalent directions and planes:\n\n\nFor face-centered cubic (fcc) and body-centered cubic (bcc) lattices, the primitive lattice vectors are not orthogonal. However, in these cases the Miller indices are conventionally defined relative to the lattice vectors of the cubic supercell and hence are again simply the Cartesian directions.\n\nThe spacing d between adjacent (\"hkl\") lattice planes is given by:\n\nThe defining property of a crystal is its inherent symmetry, by which we mean that under certain 'operations' the crystal remains unchanged. All crystals have translational symmetry in three directions, but some have other symmetry elements as well. For example, rotating the crystal 180° about a certain axis may result in an atomic configuration that is identical to the original configuration. The crystal is then said to have a twofold rotational symmetry about this axis. In addition to rotational symmetries like this, a crystal may have symmetries in the form of mirror planes and translational symmetries, and also the so-called \"compound symmetries,\" which are a combination of translation and rotation/mirror symmetries. A full classification of a crystal is achieved when all of these inherent symmetries of the crystal are identified.\n\nThese lattice systems are a grouping of crystal structures according to the axial system used to describe their lattice. Each lattice system consists of a set of three axes in a particular geometric arrangement. There are seven lattice systems. They are similar to but not quite the same as the seven crystal systems. \nThe simplest and most symmetric, the cubic (or isometric) system, has the symmetry of a cube, that is, it exhibits four threefold rotational axes oriented at 109.5° (the tetrahedral angle) with respect to each other. These threefold axes lie along the body diagonals of the cube. The other six lattice systems, are hexagonal, tetragonal, rhombohedral (often confused with the trigonal crystal system), orthorhombic, monoclinic and triclinic.\n\nBravais lattices, also referred to as \"space lattices\", describe the geometric arrangement of the lattice points, and therefore the translational symmetry of the crystal. The three dimensions of space afford 14 distinct Bravais lattices describing the translational symmetry. All crystalline materials recognized today, not including quasicrystals, fit in one of these arrangements. The fourteen three-dimensional lattices, classified by lattice system, are shown above.\n\nThe crystal structure consists of the same group of atoms, the \"basis\", positioned around each and every lattice point. This group of atoms therefore repeats indefinitely in three dimensions according to the arrangement of one of the Bravais lattices. The characteristic rotation and mirror symmetries of the unit cell is described by its crystallographic point group.\n\nA crystal system is a set of point groups in which the point groups themselves and their corresponding space groups are assigned to a lattice system. Of the 32 point groups that exist in three dimensions, most are assigned to only one lattice system, in which case the crystal system and lattice system both have the same name. However, five point groups are assigned to two lattice systems, rhombohedral and hexagonal, because both lattice systems exhibit threefold rotational symmetry. These point groups are assigned to the trigonal crystal system. \n\nIn total there are seven crystal systems: triclinic, monoclinic, orthorhombic, tetragonal, trigonal, hexagonal, and cubic.\n\nThe crystallographic point group or \"crystal class\" is the mathematical group comprising the symmetry operations that leave at least one point unmoved and that leave the appearance of the crystal structure unchanged. These symmetry operations include\n\n\nRotation axes (proper and improper), reflection planes, and centers of symmetry are collectively called \"symmetry elements\". There are 32 possible crystal classes. Each one can be classified into one of the seven crystal systems.\n\nIn addition to the operations of the point group, the space group of the crystal structure contains translational symmetry operations. These include:\n\nThere are 230 distinct space groups.\n\nBy considering the arrangement of atoms relative to each other, their coordination numbers (or number of nearest neighbors), interatomic distances, types of bonding, etc., it is possible to form a general view of the structures and alternative ways of visualizing them.\n\nThe principles involved can be understood by considering the most efficient way of packing together equal-sized spheres and stacking close-packed atomic planes in three dimensions. For example, if plane A lies beneath plane B, there are two possible ways of placing an additional atom on top of layer B. If an additional layer was placed directly over plane A, this would give rise to the following series:\nThis arrangement of atoms in a crystal structure is known as hexagonal close packing (hcp).\n\nIf, however, all three planes are staggered relative to each other and it is not until the fourth layer is positioned directly over plane A that the sequence is repeated, then the following sequence arises:\nThis type of structural arrangement is known as cubic close packing (ccp).\n\nThe unit cell of a ccp arrangement of atoms is the face-centered cubic (fcc) unit cell. This is not immediately obvious as the closely packed layers are parallel to the {111} planes of the fcc unit cell. There are four different orientations of the close-packed layers.\n\nThe packing efficiency can be worked out by calculating the total volume of the spheres and dividing by the volume of the cell as follows:\n\nThe 74% packing efficiency is the maximum density possible in unit cells constructed of spheres of only one size. Most crystalline forms of metallic elements are hcp, fcc, or bcc (body-centered cubic). The coordination number of atoms in hcp and fcc structures is 12 and its atomic packing factor (APF) is the number mentioned above, 0.74. This can be compared to the APF of a bcc structure, which is 0.68.\n\nGrain boundaries are interfaces where crystals of different orientations meet. A grain boundary is a single-phase interface, with crystals on each side of the boundary being identical except in orientation. The term \"crystallite boundary\" is sometimes, though rarely, used. Grain boundary areas contain those atoms that have been perturbed from their original lattice sites, dislocations, and impurities that have migrated to the lower energy grain boundary.\n\nTreating a grain boundary geometrically as an interface of a single crystal cut into two parts, one of which is rotated, we see that there are five variables required to define a grain boundary. The first two numbers come from the unit vector that specifies a rotation axis. The third number designates the angle of rotation of the grain. The final two numbers specify the plane of the grain boundary (or a unit vector that is normal to this plane).\n\nGrain boundaries disrupt the motion of dislocations through a material, so reducing crystallite size is a common way to improve strength, as described by the Hall–Petch relationship. Since grain boundaries are defects in the crystal structure they tend to decrease the electrical and thermal conductivity of the material. The high interfacial energy and relatively weak bonding in most grain boundaries often makes them preferred sites for the onset of corrosion and for the precipitation of new phases from the solid. They are also important to many of the mechanisms of creep.\n\nGrain boundaries are in general only a few nanometers wide. In common materials, crystallites are large enough that grain boundaries account for a small fraction of the material. However, very small grain sizes are achievable. In nanocrystalline solids, grain boundaries become a significant volume fraction of the material, with profound effects on such properties as diffusion and plasticity. In the limit of small crystallites, as the volume fraction of grain boundaries approaches 100%, the material ceases to have any crystalline character, and thus becomes an amorphous solid.\n\nReal crystals feature defects or irregularities in the ideal arrangements described above and it is these defects that critically determine many of the electrical and mechanical properties of real materials. When one atom substitutes for one of the principal atomic components within the crystal structure, alteration in the electrical and thermal properties of the material may ensue. Impurities may also manifest as spin impurities in certain materials. Research on magnetic impurities demonstrates that substantial alteration of certain properties such as specific heat may be affected by small concentrations of an impurity, as for example impurities in semiconducting ferromagnetic alloys may lead to different properties as first predicted in the late 1960s. Dislocations in the crystal lattice allow shear at lower stress than that needed for a perfect crystal structure.\n\nThe difficulty of predicting stable crystal structures based on the knowledge of only the chemical composition has long been a stumbling block on the way to fully computational materials design. Now, with more powerful algorithms and high-performance computing, structures of medium complexity can be predicted using such approaches as evolutionary algorithms, random sampling, or metadynamics.\n\nThe crystal structures of simple ionic solids (e.g., NaCl or table salt) have long been rationalized in terms of Pauling's rules, first set out in 1929 by Linus Pauling, referred to by many since as the \"father of the chemical bond\". Pauling also considered the nature of the interatomic forces in metals, and concluded that about half of the five d-orbitals in the transition metals are involved in bonding, with the remaining nonbonding d-orbitals being responsible for the magnetic properties. He, therefore, was able to correlate the number of d-orbitals in bond formation with the bond length as well as many of the physical properties of the substance. He subsequently introduced the metallic orbital, an extra orbital necessary to permit uninhibited resonance of valence bonds among various electronic structures.\n\nIn the resonating valence bond theory, the factors that determine the choice of one from among alternative crystal structures of a metal or intermetallic compound revolve around the energy of resonance of bonds among interatomic positions. It is clear that some modes of resonance would make larger contributions (be more mechanically stable than others), and that in particular a simple ratio of number of bonds to number of positions would be exceptional. The resulting principle is that a special stability is associated with the simplest ratios or \"bond numbers\": , , , , , etc. The choice of structure and the value of the axial ratio (which determines the relative bond lengths) are thus a result of the effort of an atom to use its valency in the formation of stable bonds with simple fractional bond numbers.\n\nAfter postulating a direct correlation between electron concentration and crystal structure in beta-phase alloys, Hume-Rothery analyzed the trends in melting points, compressibilities and bond lengths as a function of group number in the periodic table in order to establish a system of valencies of the transition elements in the metallic state. This treatment thus emphasized the increasing bond strength as a function of group number. The operation of directional forces were emphasized in one article on the relation between bond hybrids and the metallic structures. The resulting correlation between electronic and crystalline structures is summarized by a single parameter, the weight of the d-electrons per hybridized metallic orbital. The \"d-weight\" calculates out to 0.5, 0.7 and 0.9 for the fcc, hcp and bcc structures respectively. The relationship between d-electrons and crystal structure thus becomes apparent.\n\nIn crystal structure predictions/simulations, the periodicity is usually applied, since the system is imagined as unlimited big in all directions. Starting from a triclinic structure with no further symmetry property assumed, the system may be driven to show some additional symmetry properties by applying Newton's Second Law on particles in the unit cell and a recently developed dynamical equation for the system period vectors\n\nPolymorphism is the occurrence of multiple crystalline forms of a material. It is found in many crystalline materials including polymers, minerals, and metals. According to Gibbs' rules of phase equilibria, these unique crystalline phases are dependent on intensive variables such as pressure and temperature. Polymorphism is related to allotropy, which refers to elemental solids. The complete morphology of a material is described by polymorphism and other variables such as crystal habit, amorphous fraction or crystallographic defects. Polymorphs have different stabilities and may spontaneously convert from a metastable form (or thermodynamically unstable form) to the stable form at a particular temperature. They also exhibit different melting points, solubilities, and X-ray diffraction patterns.\n\nOne good example of this is the quartz form of silicon dioxide, or SiO. In the vast majority of silicates, the Si atom shows tetrahedral coordination by 4 oxygens. All but one of the crystalline forms involve tetrahedral {SiO} units linked together by shared vertices in different arrangements. In different minerals the tetrahedra show different degrees of networking and polymerization. For example, they occur singly, joined together in pairs, in larger finite clusters including rings, in chains, double chains, sheets, and three-dimensional frameworks. The minerals are classified into groups based on these structures. In each of the 7 thermodynamically stable crystalline forms or polymorphs of crystalline quartz, only 2 out of 4 of each the edges of the {SiO} tetrahedra are shared with others, yielding the net chemical formula for silica: SiO.\n\nAnother example is elemental tin (Sn), which is malleable near ambient temperatures but is brittle when cooled. This change in mechanical properties due to existence of its two major allotropes, α- and β-tin. The two allotropes that are encountered at normal pressure and temperature, α-tin and β-tin, are more commonly known as \"gray tin\" and \"white tin\" respectively. Two more allotropes, γ and σ, exist at temperatures above 161 °C and pressures above several GPa. White tin is metallic, and is the stable crystalline form at or above room temperature. Below 13.2 °C, tin exists in the gray form, which has a diamond cubic crystal structure, similar to diamond, silicon or germanium. Gray tin has no metallic properties at all, is a dull gray powdery material, and has few uses, other than a few specialized semiconductor applications. Although the α–β transformation temperature of tin is nominally 13.2 °C, impurities (e.g. Al, Zn, etc.) lower the transition temperature well below 0 °C, and upon addition of Sb or Bi the transformation may not occur at all.\n\nTwenty of the 32 crystal classes are piezoelectric, and crystals belonging to one of these classes (point groups) display piezoelectricity. All piezoelectric classes lack a center of symmetry. Any material develops a dielectric polarization when an electric field is applied, but a substance that has such a natural charge separation even in the absence of a field is called a polar material. Whether or not a material is polar is determined solely by its crystal structure. Only 10 of the 32 point groups are polar. All polar crystals are pyroelectric, so the 10 polar crystal classes are sometimes referred to as the pyroelectric classes.\n\nThere are a few crystal structures, notably the perovskite structure, which exhibit ferroelectric behavior. This is analogous to ferromagnetism, in that, in the absence of an electric field during production, the ferroelectric crystal does not exhibit a polarization. Upon the application of an electric field of sufficient magnitude, the crystal becomes permanently polarized. This polarization can be reversed by a sufficiently large counter-charge, in the same way that a ferromagnet can be reversed. However, although they are called ferroelectrics, the effect is due to the crystal structure (not the presence of a ferrous metal).\n\n"}
{"id": "26167139", "url": "https://en.wikipedia.org/wiki?curid=26167139", "title": "Definitionism", "text": "Definitionism\n\nDefinitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.\n"}
{"id": "33548913", "url": "https://en.wikipedia.org/wiki?curid=33548913", "title": "Dehaene–Changeux model", "text": "Dehaene–Changeux model\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's \"global workspace model\" for consciousness.\n\nIt is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour of the brain's \"higher cognitive functions\" such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test.\n\nThe Dehaene–Changeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives. It would later be used to predict observable reaction times within the priming paradigm and in inattentional blindness.\n\nThe Dehaene–Changeux model is a meta neural network (i.e. a network of neural networks) composed of a very large number of integrate-and-fire neurons programmed in either a stochastic or deterministic way. The neurons are organised in complex thalamo-cortical columns with long-range connexions and a critical role played by the interaction between von Economo's areas. Each thalamo-cortical column is composed of pyramidal cells and inhibitory interneurons receiving a long-distance excitatory neuromodulation which could represent noradrenergic input.\n\nAmong others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.\n\nThe DCM exhibits several surcritical emergent behaviors such as multistability and a Hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various all-or-none behaviors which Dehaene et al. use to determine a testable taxonomy between different states of consciousness. \n\nThe Dehaene-Changeux Model contributed to the study of nonlinearity and self-organized criticality in particular as an explanatory model of the brain's emergent behaviors, including consciousness. Studying the brain's phase-locking and large-scale synchronization, Kitzbichler et al. (2011a) confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brain's physiological bandwidth.\n\nFurthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\".\n\nIt contributed to the study of phase transition in the brain under sedation, and notably GABA-ergic sedation such as that induced by propofol (Murphy et al. 2011, Stamatakis et al. 2010). The Dehaene-Changeux Model was contrasted and cited in the study of collective consciousness and its pathologies (Wallace et al. 2007). Boly et al. (2007) used the model for a reverse somatotopic study, demonstrating a correlation between baseline brain activity and somatosensory perception in humans. Boly et al. (2008) also used the DCM in a study of the baseline state of consciousness of the human brain's default network.\n\n\n"}
{"id": "21899301", "url": "https://en.wikipedia.org/wiki?curid=21899301", "title": "Disquotational principle", "text": "Disquotational principle\n\nThe disquotational principle is a philosophical theorem which holds that a rational speaker will accept \"p\" if and only if he or she believes \"p\". The quotes indicate that the statement \"p\" is being treated as a sentence, and not as a proposition. This principle is presupposed by claims that hold that substitution fails in certain intensional contexts.\n\nConsider the following argument:\n\nTo derive (3), we have to assume that when Sally accepts that \"Cicero was a famous orator\", she believes that Cicero was a famous orator. Then we can exchange Cicero for Tully, and derive (3). Bertrand Russell thought that this demonstrated the failure of substitutivity of identicals in intensional contexts.\n\nIn \"A Puzzle about Belief,\" Saul Kripke argues that the application of the disquotational theorem can yield a paradox on its own, without appeal to the substitution principle, and that this may show that the problem lies with the former, and not the latter. There are various formulations of this argument.\n\nSuppose that, Pierre, a Frenchman, comes to believe that (1) \"Londres est jolie\" (London is pretty), without ever having visited the city. Later in life, Pierre ends up living in London. He finds no French speakers there (he does not speak English yet), and everyone refers to the city as \"London,\" not \"Londres\". He finds this city decidedly unattractive, for the neighborhood he decides to live in is decidedly unattractive. Over time, he learns English, and formulates the belief that (2) \"London is not pretty\". Pierre never realizes that London is the English word for \"Londres\". Now with the disquotational principle, we can deduce from (1) that Pierre believes the proposition that \"Londres est jolie\". With a weak principle of translation (e.g., \"a proposition in language A is the same as a semantically identical proposition in language B\" [note that a proposition is not the same as a sentence]), we can now deduce that Pierre believes that London is pretty. But we can also deduce from (2) and the disquotational principle that Pierre believes that London is not pretty. These deductions can be made \"even though Pierre has made no logical blunders in forming his beliefs\". Without the disquotational principle, this contradiction could not be derived, because we would not be able to assume that (1) and (2) meant anything in particular.\n\nThis paradox can also be derived without appeal to another language. Suppose that Pierre assents to the proposition that \"Paderewski had musical talent\", perhaps having heard that this man was a famous pianist. With the disquotational principle, we can deduce that Pierre believes the proposition that Paderewski had musical talent. Now suppose that Pierre overhears a friend discussing the political exploits of a certain statesman, Paderewski, without knowing that the two Paderewskis are the same man. Pierre's background tells him that statesmen are generally not very gifted in music, and this leads him to the belief that Paderewski had no musical talent. The disquotation principle allows us to deduce that Pierre believes the proposition that Paderewski had no musical talent. Using this principle, we have now deduced that Pierre believes that Paderewski had musical talent, and does not believe that Paderewski had musical talent, \"even though Pierre's beliefs were formed logically\".\n\n"}
{"id": "15285866", "url": "https://en.wikipedia.org/wiki?curid=15285866", "title": "Experimental system", "text": "Experimental system\n\nIn scientific research, an experimental system is the physical, technical and procedural basis for an experiment or series of experiments. Historian of science Hans-Jörg Rheinberger defines an experimental system as: \"A basic unit of experimental activity combining local, technical, instrumental, institutional, social, and epistemic aspects.\" Scientists (particularly laboratory biologists) and historians and philosophers of biology have pointed to the development and spread of successful experimental systems, such as those based on popular model organism or scientific apparatus, as key elements in the history of science, particularly since the early 20th century. The choice of an appropriate experimental system is often seen as critical for a scientist's long-term success, as experimental systems can be very productive for some kinds of questions and less productive for others, acquiring a sort of momentum that takes research in unpredicted directions.\n\nA successful experimental system must be stable and reproducible enough for scientists to make sense of the system's behavior, but variable and unpredictable enough that it can produce useful results. In many cases, a well-understood experimental system can be \"black-boxed\" as a standard technique, which can then be a component of other experimental systems. Rheinberger divides experimental systems into two parts: the part under investigation (\"epistemic things\") and the well-understood part that provides a stable context for experimentation (\"technical objects\").\n\nThe development of experimental systems in biology often requires the \"domestication\" of a particular organism for the laboratory environment, including the creation of relatively homogeneous lines or strains and the tailoring of conditions to highlight the variable aspects that scientists are interested in. Scientific technologies, similarly, often require the development of a full experimental system to go from a viable concept to a technique that works in practice on a usefully consistent basis. For example, the invention of the polymerase chain reaction (PCR) is generally attributed to Kary Mullis, who came up with the concept in 1983, but the process of development of PCR into the revolutionary technology it became by the early 1990s took years of work by others at Cetus Corporation—and the basic components of the system had been known since the 1960s DNA synthesis work of Har Gobind Khorana—making \"who invented PCR?\" a complicated question.\n\n"}
{"id": "1973470", "url": "https://en.wikipedia.org/wiki?curid=1973470", "title": "Fuzzy concept", "text": "Fuzzy concept\n\nA fuzzy concept is a concept of which the boundaries of application can vary considerably according to context or conditions, instead of being fixed once and for all. This means the concept is vague in some way, lacking a fixed, precise meaning, without however being unclear or meaningless altogether. It has a definite meaning, which can be made more precise only through further elaboration and specification - including a closer definition of the context in which the concept is used. The study of the characteristics of fuzzy concepts and fuzzy language is called \"fuzzy semantics\". The inverse of a \"fuzzy concept\" is a \"crisp concept\" (i.e. a precise concept). \n\nA fuzzy concept is understood by scientists as a concept which is \"to an extent applicable\" in a situation. That means the concept has \"gradations\" of significance or \"unsharp\" (variable) boundaries of application. A fuzzy statement is a statement which is true \"to some extent\", and that extent can often be represented by a scaled value. The best known example of a fuzzy concept around the world is an amber traffic light, and indeed fuzzy concepts are widely used in traffic control systems. The term is also used these days in a more general, popular sense - in contrast to its technical meaning - to refer to a concept which is \"rather vague\" for any kind of reason.\n\nIn the past, the very idea of reasoning with fuzzy concepts faced considerable resistance from academic elites. They did not want to endorse the use of imprecise concepts in research or argumentation. Yet although people might not be aware of it, the use of fuzzy concepts has risen gigantically in all walks of life from the 1970s onward. That is mainly due to advances in electronic engineering, fuzzy mathematics and digital computer programming. The new technology allows very complex inferences about \"variations on a theme\" to be anticipated and fixed in a program. \n\nThe new neuro-fuzzy computational methods make it possible, to identify, to measure and respond to fine gradations of significance, with great precision. It means that practically useful concepts can be coded and applied to all kinds of tasks, even if, ordinarily, these concepts are never precisely defined. Nowadays engineers, statisticians and programmers often represent fuzzy concepts mathematically, using fuzzy logic, fuzzy values, fuzzy variables and fuzzy sets.\n\nProblems of vagueness and fuzziness have probably always existed in human experience. The boundary between different things can appear blurry. Sometimes people have to think, when they are not in the best frame of mind to do it, or, they have to talk about something out there, which just isn't sharply defined. Across time, however, philosophers and scientists began to reflect about those kinds of problems, in much more systematic ways.\n\nThe ancient Sorites paradox first raised the logical problem of how we could exactly define the threshold at which a change in quantitative gradation turns into a qualitative or categorical difference. With some physical processes this threshold is relatively easy to identify. For example, water turns into steam at 100 °C or 212 °F (the boiling point depends partly on atmospheric pressure, which decreases at higher altitudes). \n\nWith many other processes and gradations, however, the point of change is much more difficult to locate, and remains somewhat vague. Thus, the boundaries between qualitatively different things may be \"unsharp\": we know that there are boundaries, but we cannot define them exactly. \n\nAccording to the modern idea of the continuum fallacy, the fact that a statement is to an extent vague, does not automatically mean that it is invalid. The problem then becomes one of how we could ascertain the kind of validity that the statement does have.\n\nThe Nordic myth of Loki's wager suggested that concepts that lack precise meanings or precise boundaries of application cannot be usefully discussed at all. However, the 20th century idea of \"fuzzy concepts\" proposes that \"somewhat vague terms\" can be operated with, since we can explicate and define the variability of their application, by assigning numbers to gradations of applicability. This idea sounds simple enough, but it had large implications.\n\nThe intellectual origins of the species of fuzzy concepts as a logical category have been traced back to a diversity of famous and less well-known thinkers, including (among many others) Eubulides, Plato, Cicero, Georg Wilhelm Friedrich Hegel, Karl Marx and Friedrich Engels, Friedrich Nietzsche, Hugh MacColl, Charles S. Peirce, Max Black, Jan Łukasiewicz, Emil Leon Post, Alfred Tarski, Georg Cantor, Nicolai A. Vasiliev, Kurt Gödel, Stanisław Jaśkowski and Donald Knuth. \n\nAcross at least two and a half millennia, all of them had something to say about graded concepts with unsharp boundaries. This suggests at least that the awareness of the existence of concepts with \"fuzzy\" characteristics, in one form or another, has a very long history in human thought. Quite a few logicians and philosophers have also tried to \"analyze\" the characteristics of fuzzy concepts as a recognized species, sometimes with the aid of some kind of many-valued logic or substructural logic.\n\nAn early attempt in the post-WW2 era to create a theory of sets where set membership is a matter of degree was made by Abraham Kaplan and Hermann Schott in 1951. They intended to apply the idea to empirical research. Kaplan and Schott measured the degree of membership of empirical classes using real numbers between 0 and 1, and they defined corresponding notions of intersection, union, complementation and subset. However, at the time, their idea \"fell on stony ground\". J. Barkley Rosser Sr. published a treatise on many-valued logics in 1952, anticipating \"many-valued sets\". Another treatise was published in 1963 by Aleksandr A. Zinov'ev and others\n\nIn 1964, the American philosopher William Alston introduced the term \"degree vagueness\" to describe vagueness in an idea that results from the absence of a definite cut-off point along an implied scale (in contrast to \"combinatory vagueness\" caused by a term that has a number of logically independent conditions of application). \n\nThe German mathematician Dieter Klaua published a German-language paper on fuzzy sets in 1965, but he used a different terminology (he referred to \"many-valued sets\", not \"fuzzy sets\").\n\nTwo popular introductions to many-valued logic in the late 1960s were by Robert J. Ackermann and Nicholas Rescher respectively. Rescher’s book includes a bibliography on fuzzy theory up to 1965, which was extended by Robert Wolf for 1966-1974. Haack provides references to significant works after 1974. Bergmann provides a more recent (2008) introduction to fuzzy reasoning.\n\nUsually the Iranian-born American computer scientist Lotfi A. Zadeh (1921-2017) is credited with inventing the specific idea of a \"fuzzy concept\" in his seminal 1965 paper on fuzzy sets, because he gave a formal mathematical presentation of the phenomenon that was widely accepted by scholars.<ref>Lotfi A. Zadeh, \"Fuzzy sets\". In: \"Information and Control\", Vol. 8, June 1965, pp. 338–353.\n\n"}
{"id": "33920395", "url": "https://en.wikipedia.org/wiki?curid=33920395", "title": "General Group Problem Solving (GGPS) Model", "text": "General Group Problem Solving (GGPS) Model\n\nThe General Group Problem Solving (GGPS) Model is a problem solving methodology, in which a group of individuals will define the desired outcome, identify the gap between the current state and the target and generate ideas for closing the gap by brainstorming. The end result is list of actions needed to achieve the desired results.\n\nSally Fuller and Ramon Aldag argue that group decision-making models have been operating under too narrow of a focus due to the overemphasis of the groupthink phenomenon. In addition, according to them, group decision-making has often been framed in relative isolation, ignoring context and real-world circumstances, which is a likely consequence of testing group decision-making in laboratory studies. They claim that the groupthink model is overly deterministic and an unrealistically restrictive depiction of the group problem-solving process.” To address these problems, they propose a new model that incorporates elements of group decision-making processes from a broader, more comprehensive perspective, offering a more general and generalizable framework for future research. The model includes elements of Irving Janis's original model (1977), but only those that have been consistently supported by the literature. To understand the differences between the two models, we briefly summarize both Janis's model and the GGPS-model first.\n\nJanis defines groupthink as “the mode of thinking that persons engage in when concurrence-seeking becomes so dominant in a cohesive in-group that it tends to over-ride realistic appraisals of alternative courses of action.” In a subsequent article, he elaborates on this by saying: “I use the term \"groupthink\" as a quick and easy way to refer to a mode of thinking that people engage in when they are deeply involved in a cohesive in-group, when the members' strivings for unanimity override their motivation to realistically appraise alternative courses of action. Groupthink refers to a deterioration of mental efficiency, reality testing, and moral judgment that results from in-group pressures.”\nAll this suggests that the original groupthink model was proposed for a rather specific situation, and Janis states that we can only call a phenomenon groupthink if all the warning signs are present (see groupthink symptoms).\n\nThe GGPS-model (developed by Ramon Aldag and Sally Fuller) broadens the perspectives, incorporating elements of the original groupthink model, in a fashion that creates a more widely applicable schematic.\nTwo key differences should be noted in comparison to Janis’ model:\n\n\nThree sets of antecedents are proposed by GGPS: decision characteristics, group structure and decision-making context.\n\nElements belonging here are the importance of the decision, time pressure, structure, procedural requirements, and task characteristics.\n\n\"Examples\": whether the task is simple or complex will make a substantial difference in required member input, as well as in whether a directive leader is necessary. Group interaction is also altered if, given a task, a single correct answer exists and if it becomes obvious to any of the members, since subsequent group interaction will likely be reduced.\n\nElements are cohesiveness, members’ homogeneity, insulation of the group, leader impartiality, leader power, history of the group, probability of future interaction, stage of group development and type of group.\n\n\"Examples\": whether group members anticipate to work together again in the future can have a major impact on to what degree can political motives influence the process. If it’s unlikely that the group will come together again, political influence can be lessened. Stage of group development is important because members of mature group with a long history may feel more comfortable challenging each other’s ideas, thus cohesiveness results in quality decision making and positive outcomes.\n\nElements are organizational political norms, member political motives, prior discussion of issue, prior goal attainment, goal definition, and degree of stress from external threat.\n\n\"Examples\": whether group members identified and pursue a unitary goal or they have multiple, discrepant goals influences the rationality of the decision-making process. Members’ political motives also make a tremendous difference, if individuals have a vested interest in certain outcomes, or there are one or more coalitions present, behavior in the decision making process could be altered.\n\nThe model differentiates two categories of emergent group characteristics: group perceptions and processes.\n\nThese include members’ perceptions of the group’s vulnerability, the inherent morality of the group, member unanimity, and views of opposing groups.\n\nThese include the group’s response to negative feedback, treatment of dissenters, self-censorship, and use of mindguards.\n\nDecision process characteristics are grouped in terms of the first three stages of group problem-solving processes: problem identification, alternative generation and evaluation and choice. Implementation and control stages are not included because they follow the actual decision, but some variables preparing for those stages are indeed included (e.g. development of contingency plans and gathering of control-related information).\n\nElements of this stage are predecisional information search, survey of objectives, and explicit problem definition.\n\n\"Example\": if members of the group fail to explicitly or correctly define the problem, there is a chance that they will solve the wrong problem.\n\nElements are number of alternatives and quality of alternatives.\n\n\"Example\": in generating alternatives, it’s important to differentiate quality and quantity of alternative ideas generated. Some group processes, such as brainstorming, are directed towards generating large numbers of ideas, with the assumption that it will lead to a superior alternative. Defective processes, on the other hand, might lead to large numbers of low quality ideas.\n\nElements are information processing quality, the source of the initial selection of a preferred alternative, emergence of preferred alternative, group decision rule, timing of convergence, reexamination of preferred and rejected alternatives, source of the final solution, development of contingency plans, and gathering of control-related information.\n\n\"Example\": whether the group decides based on a majority rule or a consensus has to be reached, makes a great difference in the process. If a consensus is to be reached, dissent could be discouraged, because dissenters could elongate and jeopardize the process. With a majority rule, dissent is more acceptable.\n\nThe GGPS model includes an array of decision, political and affective outcomes.\n\nDecision outcomes: include acceptance of the decision by those affected by it and/or those who have to implement it, adherence to the decision, implementation success, and decision quality.\n\n\"Example\": if the leader of the group is not satisfied with the decision, he/she might unilaterally reverse it.\n\nPolitical outcomes: include future motivation of the leader, future motivation of the group and future use of the group.\n\n\"Example\": if the outcome did not satisfy the political agenda of the leader, he/she might use the group less or not at all in the future.\n\nAffective outcomes: include satisfaction with the leader, satisfaction with the group process and satisfaction with the decision.\n\n\"Example\": whether members are content with the fairness of the group process, whether trust was developed and preserved, or whether commitment to the decision is strong will greatly influence the group’s future functioning.\n"}
{"id": "20744016", "url": "https://en.wikipedia.org/wiki?curid=20744016", "title": "Health management system", "text": "Health management system\n\nThe health management system (HMS) is an evolutionary medicine regulative process proposed by Nicholas Humphrey in which actuarial assessment of fitness and economic-type cost–benefit analysis determines the body’s regulation of its physiology and health. This incorporation of cost–benefit calculations into body regulation provides a science grounded approach to mind–body phenomena such as placebos that are otherwise not explainable by low level, noneconomic, and purely feedback based homeostatic or allostatic theories.\n\n\nPlacebos are explained as the result of false information about the availability of external treatment and support that mislead the health management system into not deploying evolved self-treatments. This results in the placebo suppression of medical symptoms.\n\nSince Hippocrates, it has been recognized that the body has self-healing powers (vis medicatrix naturae). Modern evolutionary medicine identifies them with physiologically based self-treatments that provide the body with prophylactic, healing, or restorative capabilities against injuries, infections and physiological disruption. Examples include:\n\n\nThese evolved self-treatments deployed by the body are experienced by humans as unpleasant and unwanted illness symptoms.\n\nSuch self-treatments according to evolutionary medicine are deployed to increase an individual’s biological fitness.\n\nTwo factors affect their deployment.\n\nFirst, it is usually advantageous to deploy them on a precautionary basis. As a result, it will often turn out that they have been deployed apparently unnecessarily, though this has in fact been advantageous since in probabilistic terms they have provided an insurance against a potentially costly outcome. As Nesse notes: \"Vomiting, for example, may cost only a few hundred calories and a few minutes, whereas not vomiting may result in a 5% chance of death\" page 77.\n\nSecond, self-treatments are costly both in using energy, and also in their risk of damaging the body.\n\n\nOne factor in deployment is low level physiological control by proinflammatory cytokines such as IL-1 triggered by bacterial lipopolysaccharides (LPS).\n\nAnother is higher level control in which the brain takes into account what it learns about circumstances and how that makes it well and ill. Conditioning shows the existence of such learnt control: give saccharin paired in a drink with a drug that creates immunosuppression, and later on, giving saccharin alone will produce immunosuppression. Such conditioning happens both in experimental rodents and humans.\n\nEvolution, according to Nicholas Humphrey, has selected an internal health management system that uses cost benefit analysis upon whether the deployment of a self-treatment aids biological fitness, and so should be activated. \na specially designed procedure for “economic resource management” that is, I believe, one of the key features of the “natural health-care service” which has evolved in ourselves and other animals to help us deal throughout our lives with repeated bouts of sickness, injury, and other threats to our well-being.\n\nAn analogy is explicitly made with the health economics consideration used in management decisions involving external medical treatment.\n\nNow, if you wonder about this choice of managerial terminology for talking about biological healing systems, I should say that it is quite deliberate (and so is the pun on NHS.) With the phrase “natural health-care service” I do intend to evoke, at a biological level, all the economic connotations that are so much a part of modern health-care in society.\n\nExternal medications will affect the cost benefits advantages of deploying an evolved self-treatment. Some animals use external ones. Wild animals, including apes, do so in the form of ingested detoxifying clays, rough leaves that clear gut parasites, and pharmacologically active plants Complementary to this, research finds that animals have the ability to select and prefer substances that aid their recuperation from illness.\n\nThe welfare of social animals (including humans) depends upon other individuals (social buffering). The actuarial assessments of the costs and benefits of deploying a self-treatment therefore will depend upon the presence, or not, of other individuals. The presence of helpful others will affect, for example, the risk of predators when incapacitated, and—in those case in which animals do this (such as humans)—the provision of food, and care during sickness.\n\nThe health management system factors in the presence of such external treatment and social support as one aspect of the circumstances needed to determine whether it is advantageous to deploy or not an evolved self-treatment.\n\nAll humans societies use external medications, and some individuals exist that are considered to have special healing knowledge about illnesses and their treatments. Humans are also usually supportive to those in their group. The availability of these things will affect the cost benefits of the body deploying its own biological ones. This could, in turn, lead to the health management system (given its beliefs (information) about treatments and support) to deploy or not, or doing so differently, the body’s own treatments.\n\nNicholas Humphrey describes how the health management system explains placebos – an external treatment without direct physiological effects – as follows:\nSuppose, for example, a doctor gives someone who is suffering an infection a pill that she rightly believes to contain an antibiotic: because her hopes will be raised she will no doubt make appropriate adjustments to her health-management strategy – lowering her precautionary defences in anticipation of the sickness not lasting long.\n\nThe health management system, in other words, when faced with an infection is tricked into making a mistaken cost benefit analysis using false information. The effect of that false information is that the benefits of the self-treatment cease to outweigh its costs. As a result, it is not deployed, and an individual does not experience unwanted medical symptoms.\n\nFailure to deploy an evolved self-treatment need not put an individual at risk since evolution has advantaged their deployment on a precautionary basis. As Nicholas Humphrey notes:\n\nTherefore, not deploying an evolved self-treatment, and so not having a medical symptom due to placebo false information might be without consequence.\n\nThe health management system’s idea of a top down neural control of the body is also found in the idea that a central governor regulates muscle fatigue to protect the body from the harmful effects (such as anoxia and hyperglycemia) of over prolonged exercise.\n\nThe idea of a fatigue governor was first proposed in 1924 by the 1922 Nobel Prize winner Archibald Hill, and more recently, on the basis of modern research, by Tim Noakes.\n\nLike with the health management system, the central governor shares the idea that much of what is attributed to low level feedback homeostatic regulation is, in fact, due to top down control by the brain. The advantage of this top down management is that the brain can enhance such regulation by allowing it to be modified by information. For example, in endurance running, a cost benefit trade exists off between the advantages of continuing to run, and the risk if this is too prolonged that it might harm the body. Being able to regulate fatigue in terms of information about the benefits and costs of continued exercise would enhance biological fitness.\n\nLow level theories exist that suggest that fatigue is due mechanical failure of the exercising muscles (\"peripheral fatigue\"). However, such low level theories do not explain why running muscle fatigue is affected by information relevant to cost benefit trade offs. For example, marathon runners can carry on running longer if told they are near the finishing line, than far away. The existence of a central governor can explain this effect.\n\n\n"}
{"id": "37519566", "url": "https://en.wikipedia.org/wiki?curid=37519566", "title": "Hitchens's razor", "text": "Hitchens's razor\n\nHitchens's razor is an epistemological razor asserting that the burden of proof regarding the truthfulness of a claim lies with the one who makes the claim, and if this burden is not met, the claim is unfounded, and its opponents need not argue further in order to dismiss it.\n\nThe concept is named, echoing Occam's razor, for the journalist and writer Christopher Hitchens, who in a 2003 \"Slate\" article formulated it thus: \"What can be asserted without evidence can be dismissed without evidence\". The dictum also appears in \"God Is Not Great: How Religion Poisons Everything\", a book by Hitchens published in 2007.\n\nHitchens's razor is actually an English translation of the Latin proverb \"\" (\"What is freely asserted is freely dismissed\"), which was commonly used in the 19th century. It takes a stronger stance than the Sagan standard (\"Extraordinary claims require extraordinary evidence\"), instead applying to even non-extraordinary claims.\n\n"}
{"id": "369116", "url": "https://en.wikipedia.org/wiki?curid=369116", "title": "Hume's fork", "text": "Hume's fork\n\nHume's fork is an explanation, developed by later philosophers, of David Hume's 1730s division of \"relations of ideas\" from \"matters of fact and real existence\". A distinction is made between necessary versus contingent (concerning reality), versus (concerning knowledge), and analytic versus synthetic (concerning language). Relations of abstract ideas align on one side (necessary, \"a priori\", analytic), whereas concrete truths align on the other (contingent, \"a posteriori\", synthetic).\n\nThe \"necessary\" is generally true in all possible worlds—usually by mere logical validity—whereas the \"contingent\" hinges on the way the real world is. The \"a priori\" is knowable before or without, whereas the \"a posteriori\" is knowable only after or through, experience in an area of interest. The \"analytic\" is a statement true by virtue of its terms' meanings, and therefore a tautology—necessarily true but uninformative—whereas the \"synthetic\" is true by its terms' meanings in relation to a state of facts. In other words, analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. Philosophers have used the terms differently, and there is debate over whether there is a legitimate distinction. \n\nHume's strong empiricism, as in Hume's fork as well as Hume's problem of induction, was taken as a threat to Newton's theory of motion. Immanuel Kant responded with rationalism in his 1781 \"Critique of Pure Reason\", where Kant attributed to the mind a causal role in sensory experience by the mind's aligning the environmental input by arranging those sense data into the experience of space and time. Kant thus reasoned existence of the synthetic \"a priori\"—combining meanings of terms with states of facts, yet known true without experience of the particular instance—replacing the two prongs of Hume's fork with a three-pronged-fork thesis (Kant's pitchfork) and thus saving Newton's law of universal gravitation.\n\nIn 1919, Newton's theory fell to Einstein's general theory of relativity. In the late 1920s, the logical positivists rejected Kant's synthetic \"a priori\" and asserted Hume's fork, so called, while hinging it at language—the analytic/synthetic division—while presuming that by holding to analyticity, they could develop a logical syntax entailing, as a consequence of Hume's fork, both necessity and aprioricity, thus restricting science to claims verifiable as either false or true. In the early 1950s, Willard Van Orman Quine undermined the analytic/synthetic division by explicating ontological relativity, as every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. By the early 1970s, Saul Kripke established the necessary \"a posteriori\", since if the Morning Star and the Evening Star are the same star, they are the same star by necessity, but this is known true by a human only through relevant experience.\n\nHume's fork remains basic in Anglo-American philosophy. Many deceptions and confusions are foisted by surreptitious or unwitting conversion of a synthetic claim to an analytic claim, rendered true by necessity but merely a tautology, for instance the \"No true Scotsman\" move. Simply put, Hume's fork has limitations. Related concerns are Hume's distinction of demonstrative versus probable reasoning and Hume's law. Hume makes other, important two-category distinctions, such as beliefs versus desires and as impressions versus ideas.\n\nThe first distinction is between two different areas of human study:\n\nHume's fork is often stated in such a way that statements are divided up into two types:\n\n\nIn modern terminology, members of the first group are known as analytic propositions and members of the latter as synthetic propositions. This terminology comes from Kant (Introduction to \"Critique of Pure Reason\", Section IV).\n\nInto the first class fall statements such as \"all bodies are extended\", \"all bachelors are unmarried\", and truths of mathematics and logic. Into the second class fall statements like \"the sun rises in the morning\", and \"all bodies have mass\".\n\nHume wants to prove that certainty does not exist in science. First, Hume notes that statements of the second type can never be entirely certain, due to the fallibility of our senses, the possibility of deception (see e.g. the modern brain in a vat theory) and other arguments made by philosophical skeptics. It is always logically possible that any given statement about the world is false.\n\nSecond, Hume claims that our belief in cause-and-effect relationships between events is not grounded on reason, but rather arises merely by habit or custom. Suppose one states: \"Whenever someone on earth lets go of a stone it falls.\" While we can grant that in every instance thus far when a rock was dropped on Earth it went down, this does not make it logically necessary that in the future rocks will fall when in the same circumstances. Things of this nature rely upon the future conforming to the same principles which governed the past. But that isn't something that we can know based on past experience—all past experience could tell us is that in the past, the future has resembled the past.\n\nThird, Hume notes that relations of ideas can be used only to prove other relations of ideas, and mean nothing outside of the context of how they relate to each other, and therefore tell us nothing about the world. Take the statement \"An equilateral triangle has three sides of equal length.\" While some earlier philosophers (most notably Plato and Descartes) held that logical statements such as these contained the most formal reality, since they are always true and unchanging, Hume held that, while true, they contain no formal reality, because the truth of the statements rests on the definitions of the words involved, and not on actual things in the world, since there is no such thing as a true triangle or exact equality of length in the world. So for this reason, relations of ideas cannot be used to prove matters of fact.\n\nThe results claimed by Hume as consequences of his fork are drastic. According to him, relations of ideas can be proved with certainty (by using other relations of ideas), however, they don't really mean anything about the world. Since they don't mean anything about the world, relations of ideas cannot be used to prove matters of fact. Because of this, matters of fact have no certainty and therefore cannot be used to prove anything. Only certain things can be used to prove other things for certain, but only things about the world can be used to prove other things about the world. But since we can't cross the fork, nothing is both certain and about the world, only one or the other, and so it is impossible to prove something about the world with certainty.\n\nIf accepted, Hume's fork makes it pointless to try to prove the existence of God (for example) as a matter of fact. If God is not literally made up of physical matter, and does not have an observable effect on the world, making a statement about God is not a matter of fact. Therefore, a statement about God must be a relation of ideas. In this case if we prove the statement \"God exists,\" it doesn't really tell us anything about the world; it is just playing with words. It is easy to see how Hume's fork voids the causal argument and the ontological argument for the existence of a non-observable God. However, this does not mean that the validity of Hume's fork would imply that God definitely does not exist, only that it would imply that the existence of God cannot be proven as a matter of fact without worldly evidence. \n\nHume rejected the idea of any meaningful statement that did not fall into this schema, saying:\nIf we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, Does it contain any abstract reasoning concerning quantity or number? No. Does it contain any experimental reasoning concerning matter of fact and existence? No. Commit it then to the flames: for it can contain nothing but sophistry and illusion. — \"An Enquiry Concerning Human Understanding\"\n"}
{"id": "31938666", "url": "https://en.wikipedia.org/wiki?curid=31938666", "title": "Knowledge space (philosophy)", "text": "Knowledge space (philosophy)\n\nIn philosophy and media studies, a knowledge space is described as an emerging anthropological space in which the knowledge of individuals becomes the primary focus for social structure, values, and beliefs. The concept is put forward and explored by philosopher and media critic Pierre Lévy in his 1997 book \"Collective Intelligence\".\n\nLevy's notion of the \"knowledge space\" relies on his conception of anthropological spaces, which he defines as \"a system of proximity (space) unique to the world of humanity (anthropological), and thus dependent on human technologies, significations, language, culture, conventions, representations, and emotions\" (5). Building on the language of the philosophers Gilles Deleuze and Félix Guattari, he states that \"anthropological spaces in themselves are neither infrastructures nor superstructures but planes of existence, frequencies, velocities, determined within the social spectrum\" (147). Each space contains \"worlds of signification\" (149) by which humans come to understand and make sense of the world. Furthermore, although one space may dominate, many spaces can and do exist simultaneously.\n\nLevy describes three existing anthropological spaces. They are:\n\nThe knowledge space is an emerging anthropological space which, while it has always existed (139), is only now coming into fruition as a guiding space of humanity. In this space, singularities (individuals) are recognized as singularities and knowledge becomes the guiding value for humanity. Since all human experience represents unique knowledge, within the knowledge space all individuals are valued for their unique knowledge regardless of race (earth space), nationality (territorial space), or economic status (commodity space). Within this space static identity gives way to the \"quantum identities\" as individuals become participates and the distinction between of \"us\" and \"them\" disappears (159). Instead, humanity forms \"collective intelligences\" in which knowledge is valued and freely traded. What is \"real\" becomes \"that which implies the practical activity, intellectual and imaginary, of living subjects\" (168). Life, experiences, and knowledge become the underlying and ever changing guiding path for human societies.\n\nLevy's theories rely heavily on the technological developments of the 1990s, particularly the rise of biotechnology, nanotechnology, the Internet, new media and information technologies. In chapter 3, he describes how technologies have made a shift from the molar to the molecular (a move which makes literal a distinction by Delueze and Guattari) in that technologies now handle units as individuals (his term is \"singularities\") rather than in mass. He suggests that this mirrors our rising recognition of the individuals as singularities rather than massive conglomerated groups.\n"}
{"id": "1047584", "url": "https://en.wikipedia.org/wiki?curid=1047584", "title": "Man bites dog (journalism)", "text": "Man bites dog (journalism)\n\nThe phrase man bites dog is a shortened version of an aphorism in journalism which describes how an unusual, infrequent event (such as a man biting a dog) is more likely to be reported as news than an ordinary, everyday occurrence with similar consequences, such as a dog biting a man. An event is usually considered more newsworthy if there is something unusual about it; a commonplace event is less likely to be seen as newsworthy, even if the consequences of both events have objectively similar outcomes. The result is that rarer events more often appear as news stories, while more common events appear less often, thus distorting the perceptions of news consumers of what constitutes normal rates of occurrence.\n\nThe phenomenon is also described in the journalistic saying, \"You never read about a plane that did not crash\".\n\nThe phrase was coined by Alfred Harmsworth, 1st Viscount Northcliffe (1865–1922), a British newspaper magnate, but is also attributed to \"New York Sun\" editor John B. Bogart (1848–1921): \"When a dog bites a man, that is not news, because it happens so often. But if a man bites a dog, that is news.\" The quote is also attributed to Charles Anderson Dana (1819–1897).\n\nSome consider it a principle of yellow journalism.\n\nIn 2000, the \"Santa Cruz Sentinel\" ran a story titled \"Man bites dog\" about a San Francisco man who bit his own dog.\n\nReuters ran a story, \"It's News! Man Bites Dog\", about a man biting a dog in December 2007.\n\nA 2008 story of a boy biting a dog in Brazil had news outlets quoting the phrase.\n\nIn 2010, NBC Connecticut ran a story about a man who bit a police dog, prefacing it with, \"It's often said, if a dog bites a man it's not news, but if a man bites a dog, you've got a story. Well, here is that story.\"\n\nOn May 14, 2012, the \"Medway Messenger\", a British local newspaper, ran a front page story headlined \"MAN BITES DOG\" about a man who survived a vicious attack from a Staffordshire bull terrier by biting the dog back.\n\nOn September 27, 2012, the \"Toronto Star\", a Canadian newspaper, ran the story headlined \"Nearly Naked Man Bites Dog\", about a man that is alleged to have bitten a dog in Pembroke, Ontario.\n\nOn December 2, 2012, \"Sydney Morning Herald\" reported about a man that bit the dog and its unfortunate consequence; 'Man bites Dog, goes to hospital' \n\nOn May 5, 2013, \"Nine News\", an Australian news outlet, ran a story headlined \"Man bites dog to save wife\" about a man who bit a Labrador on the nose, after it attacked his wife and bit off her nose.\n\nOn March 12, 2014, Rosbalt, a Russian news agency, reported that a man in Lipetsk had burnt a bed in his apartment, run around the city in his underwear, and, finally, \"bit a fighting breed dog\" following an hours-long online debate about the situation in Ukraine.\n\nIn April 2014, CNN reported a mom bit a pit bull attacking her daughter.\n\nOn June 14, 2014, the \"South Wales Argus\" ran a front page teaser headlined \"Man Bites Dog\" about a man who has been accused of assaulting his partner and her pet dog. The Online version of this story was later amended to \"Man bites dog and escapes jail\".\n\nOn September 1, 2014 the \"Coventry Telegraph\" and the \"Daily Mirror\" ran an article about a man who had bitten a dog after it attacked his pet.\n\nOn December 17, 2014 the \"Cambridge News\" ran an article with a headline starting: \"Man bites dog then dies\".\n\nOn November 4, 2015 the \"Washington Post\" ran an article with the title \"Man bites dog. No, really.\"\n\nOn April 10, 2018 the \"Daily Telegraph\" ran such an article about a man biting a dog to defend his own dog.\n\nOn May 4, 2018, the \"Salt Lake Tribune\" ran an article about a man biting a police dog while being taken into custody.\n\nIn Terry Pratchett's novel \"The Truth\", protagonist and newspaper editor William DeWorde uncovers a plot against the ruler of the city by interviewing the sole witness, a dog, via an interpreter. DeWorde's resulting story is headlined \"Dog Bites Man\", and he notes with some amusement that he was able to make the phrase news-worthy.\n\nThere have also been a number of \"dog shoots man\" news stories.\n\nAs an example of a related phrase, a story titled \"Deer Shoots Hunter\" appeared in a 1947 issue of the Pittsburgh Press, mentioning a hunter that was shot by his own gun due to a reflex kick by the deer he had killed. And in 2005, in Michigan, there was a case of \"cat shoots man\".\n"}
{"id": "570963", "url": "https://en.wikipedia.org/wiki?curid=570963", "title": "Marginal propensity to save", "text": "Marginal propensity to save\n\nThe marginal propensity to save (MPS) is the fraction of an increase in income that is not spent on an increase in consumption. That is, the marginal propensity to save is the proportion of each additional dollar of household income that is used for saving. It is the slope of the line plotting saving against income. For example, if a household earns one extra dollar, and the marginal propensity to save is 0.35, then of that dollar, the household will spend 65 cents and save 35 cents. Likewise, it is the fractional decrease in saving that results from a decrease in income.\n\nThe MPS plays a central role in Keynesian economics as it quantifies the saving-income relation, which is the flip side of the consumption-income relation, and according to Keynes it reflects the fundamental psychological law. The marginal propensity to save is also a key variable in determining the value of the multiplier.\n\nMPS can be calculated as the change in savings divided by the change in income.\n\nOr mathematically, the marginal propensity to save (MPS) function is expressed as the derivative of the savings (S) function with respect to disposable income (Y).\n\nNow, MPS can be calculated as follows:\n\nMPS = (Change in savings) / (Change in income)\n\nThis implies that for each additional one unit of income, the savings increase by 0.4.\n\nThere are different implications of this above-mentioned formula.\n\n\nSince MPS is measured as ratio of change in savings to change in income, its value lies between 0 and 1.\nAlso, marginal propensity to save is opposite of marginal propensity to consume.\n\nMathematically, in a closed economy, MPS + MPC = 1, since an increase in one unit of income will be either consumed or saved.\n\nIn the above example, If MPS = 0.4, then MPC = 1 - 0.4 = 0.6.\n\nGenerally, it is assumed that value of marginal propensity to save for the richer is more than the marginal propensity to save for the poorer. If income increases for both parties by $1, then the propensity to save for a richer person would be more than that for the poorer person.\n\nMarginal propensity to save is also used as an alternative term for slope of saving line.\nThe slope of a saving line is given by the equation S = -a + (1-b)Y, where -a refers to autonomous savings and (1-b) refers to marginal propensity to save (here b refers to marginal propensity to consume but as MPC + MPS = 1, so (1-b) refers to MPS).\n\nIn this diagram, the savings function is an increasing function of disposable income i.e. savings increase as income increases.\n\nAn important implication of marginal propensity to save is measurement of the multiplier. A multiplier measures the magnified change in aggregate product i.e. the gross domestic product, resulting from a change in an autonomous variable (for example, government expenditure, investment expenditures, etc.).\n\nThe effect of a change in production creates a multiplied impact because it creates income which further creates consumption. However, the resulting consumption is also an expenditure which thus, generates more income, which creates more consumption. This next round of consumption leads to a further change in production, which generates even more income, and which induces even more consumption.\n\nAnd thus, as it goes on and on, it results in a magnified, multiplied change in aggregate production initially triggered by a change in autonomous variable, but amplified by the creation of more income and increase in consumption.\n\nMathematically, the above effect can be stated as:\n\nAnd it goes on and on.\nWe can express this as:\n\nThe end result is a magnified, multiplied change in aggregate production initially triggered by the change in investment, but amplified by the change in consumption i.e. the initial investment multiplied by the consumption coefficient (Marginal Propensity to consume).\n\nThe MPS enters into the process because it indicates the division of extra income between consumption and saving. It determines how much saving is induced with each change in production and income, and thus how much consumption is induced. If the MPS is smaller, then the multiplier process is also greater as less saving is induced, but more consumption is induced, with each round of activity.\n\nThus, in this highly simplified model, total magnified change in production due to change in an autonomous variable by $1\n\nThe effect of a multiplier effect can be measured as:\n\nIf the MPS is smaller, then the multiplier process is also greater as less saving is induced, and more consumption is induced with each round of activity.\n\nFor example, if MPS = 0.2, then multiplier effect is 5, and if MPS = 0.4, then the multiplier effect is 2.5. Thus, we can see that a lower propensity to save implies a higher multiplier effect.\n\n\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "4718632", "url": "https://en.wikipedia.org/wiki?curid=4718632", "title": "Mental representation", "text": "Mental representation\n\nA mental representation (or cognitive representation), in philosophy of mind, cognitive psychology, neuroscience, and cognitive science, is a hypothetical internal cognitive symbol that represents external reality, or else a mental process that makes use of such a symbol: \"a formal system for making explicit certain entities or types of information, together with a specification of how the system does this\".\n\nMental representation is the mental imagery of things that are not actually present to the senses. In contemporary philosophy, specifically in fields of metaphysics such as philosophy of mind and ontology, a mental representation is one of the prevailing ways of explaining and describing the nature of ideas and concepts.\n\nMental representations (or mental imagery) enable representing things that have never been experienced as well as things that do not exist. Think of yourself traveling to a place you have never visited before, or having a third arm. These things have either never happened or are impossible and do not exist, yet our brain and mental imagery allows us to imagine them. Although visual imagery is more likely to be recalled, mental imagery may involve representations in any of the sensory modalities, such as hearing, smell, or taste. Stephen Kosslyn proposes that images are used to help solve certain types of problems. We are able to visualize the objects in question and mentally represent the images to solve it.\n\nMental representations also allow people to experience things right in front of them—though the process of how the brain interprets the representational content is debated.\n\nRepresentationalism (also known as indirect realism) is the view that representations are the main way we access external reality. Another major prevailing philosophical theory posits that concepts are entirely abstract objects.\n\nThe representational theory of mind attempts to explain the nature of ideas, concepts and other mental content in contemporary philosophy of mind, cognitive science and experimental psychology. In contrast to theories of naive or direct realism, the representational theory of mind postulates the actual existence of mental representations which act as intermediaries between the observing subject and the objects, processes or other entities observed in the external world. These intermediaries stand for or represent to the mind the objects of that world.\n\nFor example, when someone arrives at the belief that his or her floor needs sweeping, the representational theory of mind states that he or she forms a mental representation that represents the floor and its state of cleanliness.\n\nThe original or \"classical\" representational theory probably can be traced back to Thomas Hobbes and was a dominant theme in classical empiricism in general. According to this version of the theory, the mental representations were images (often called \"ideas\") of the objects or states of affairs represented. For modern adherents, such as Jerry Fodor, Steven Pinker and many others, the representational system consists rather of an internal language of thought (i.e., mentalese). The contents of thoughts are represented in symbolic structures (the formulas of Mentalese) which, analogously to natural languages but on a much more abstract level, possess a syntax and semantics very much like those of natural languages. For the Spanish logician and cognitive scientist Luis M. Augusto, at this abstract, formal level, the syntax of thought is the set of symbol rules (i.e., operations, processes, etc. on and with symbol structures) and the semantics of thought is the set of symbol structures (concepts and propositions). Content (i.e., thought) emerges from the meaningful co-occurrence of both sets of symbols. For instance, \"8 x 9\" is a meaningful co-occurrence, whereas \"CAT x §\" is not; \"x\" is a symbol rule called for by symbol structures such as \"8\" and \"9\", but not by \"CAT\" and \"§\".\n\nThere are two types of representationalism, strong and weak. Strong representationalism attempts to reduce phenomenal character to intentional content. On the other hand, weak representationalism claims only that phenomenal character supervenes on intentional content. Strong representationalism aims to provide a theory about the nature of phenomenal character, and offers a solution to the hard problem of consciousness. In contrast to this, weak representationalism does not aim to provide a theory of consciousness, nor does it offer a solution to the hard problem of consciousness.\n\nStrong representationalism can be further broken down into restricted and unrestricted versions. The restricted version deals only with certain kinds of phenomenal states e.g. visual perception. Most representationalists endorse an unrestricted version of representationalism. According to the unrestricted version, for any state with phenomenal character that state’s phenomenal character reduces to its intentional content. Only this unrestricted version of representationalism is able to provide a general theory about the nature of phenomenal character, as well as offer a potential solution to the hard problem of consciousness. The successful reduction of the phenomenal character of a state to its intentional content would provide a solution to the hard problem of consciousness once a physicalist account of intentionality is worked out.\n\nWhen arguing against the unrestricted version of representationalism people will often bring up phenomenal mental states that appear to lack intentional content. The unrestricted version seeks to account for all phenomenal states. Thus, for it to be true, all states with phenomenal character must have intentional content to which that character is reduced. Phenomenal states without intentional content therefore serve as a counterexample to the unrestricted version. If the state has no intentional content its phenomenal character will not be reducible to that state’s intentional content, for it has none to begin with.\n\nA common example of this kind of state are moods. Moods are states with phenomenal character that are generally thought to not be directed at anything in particular. Moods are thought to lack directedness, unlike emotions, which are typically thought to be directed at particular things e.g. you are mad \"at\" your sibling, you are afraid \"of\" a dangerous animal. People conclude that because moods are undirected they are also nonintentional i.e. they lack intentionality or aboutness. Because they are not directed at anything they are not about anything. Because they lack intentionality they will lack any intentional content. Lacking intentional content their phenomenal character will not be reducible to intentional content, refuting the representational doctrine.\n\nThough emotions are typically considered as having directedness and intentionality this idea has also been called into question. One might point to emotions a person all of a sudden experiences that do not appear to be directed at or about anything in particular. Emotions elicited by listening to music are another potential example of undirected, nonintentional emotions. Emotions aroused in this way do not seem to necessarily be about anything, including the music that arouses them.\n\nIn response to this objection a proponent of representationalism might reject the undirected nonintentionality of moods, and attempt to identify some intentional content they might plausibly be thought to possess. The proponent of representationalism might also reject the narrow conception of intentionality as being directed at a particular thing, arguing instead for a broader kind of intentionality.\n\nThere are three alternative kinds of directedness/intentionality one might posit for moods. \nIn the case of outward directedness moods might be directed at either the world as a whole, a changing series of objects in the world, or unbound emotion properties projected by people onto things in the world. In the case of inward directedness moods are directed at the overall state of a person’s body. In the case of hybrid directedness moods are directed at some combination of inward and outward things.\n\nEven if one can identify some possible intentional content for moods we might still question whether that content is able to sufficiently capture the phenomenal character of the mood states they are a part of. Amy Kind contends that in the case of all the previously mentioned kinds of directedness (outward, inward, and hybrid) the intentional content supplied to the mood state is not capable of sufficiently capturing the phenomenal aspects of the mood states. In the case of inward directedness, the phenomenology of the mood does not seem tied to the state of one’s body, and even if one’s mood is reflected by the overall state of one’s body that person will not necessarily be aware of it, demonstrating the insufficiency of the intentional content to adequately capture the phenomenal aspects of the mood. In the case of outward directedness, the phenomenology of the mood and its intentional content do not seem to share the corresponding relation they should given that the phenomenal character is supposed to reduce to the intentional content. Hybrid directedness, if it can even get off the ground, faces the same objection.\n\nThere is a wide debate on what kinds of representations exist. There are several philosophers who bring about different aspects of the debate. Such philosophers include Alex Morgan, Gualtiero Piccinini, and Uriah Kriegel—though this is not an exhaustive list.\n\nThere are \"job description\" representations. That is representations that (1) represent something—have intentionality, (2) have a special relation—the represented object does not need to exist, and (3) content plays a causal role in what gets represented: e.g. saying \"hello\" to a friend, giving a glare to an enemy.\n\nStructural representations are also important. These types of representations are basically mental maps that we have in our minds that correspond exactly to those objects in the world (the intentional content). According to Morgan, structural representations are not the same as mental representations—there is nothing mental about them: plants can have structural representations.\n\nThere are also internal representations. These types of representations include those that involve future decisions, episodic memories, or any type of projection into the future.\n\nIn Gualtiero Piccinini's forthcoming work, he discusses topics on natural and nonnatural mental representations. He relies on the natural definition of mental representations given by Grice (1957) where \"P entails that P\". e.g. Those spots mean measles, entails that the patient has measles. Then there are nonnatural representations: \"P does not entail P\". e.g. The 3 rings on the bell of a bus mean the bus is full—the rings on the bell are independent of the fullness of the bus—we could have assigned something else (just as arbitrary) to signify that the bus is full.\n\nThere are also objective and subjective mental representations. Objective representations are closest to tracking theories—where the brain simply tracks what is in the environment. If there is a blue bird outside my window, the objective representation is that of the blue bird. Subjective representations can vary person-to-person. For example, if I am colorblind, that blue bird outside my window will not \"appear\" blue to me since I cannot represent the blueness of blue (i.e. I cannot see the color blue). The relationship between these two types of representation can vary.\n\nEliminativists think that subjective representations don't exist. Reductivists think subjective representations are reducible to objective. Non-reductivists think that subjective representations are real and distinct.\n\n\n"}
{"id": "7048926", "url": "https://en.wikipedia.org/wiki?curid=7048926", "title": "Negative-bias temperature instability", "text": "Negative-bias temperature instability\n\nNegative-bias temperature instability (NBTI) is a key reliability issue in MOSFETs. NBTI manifests as an increase in the threshold voltage and consequent decrease in drain current and transconductance of a MOSFET. The degradation exhibits a power-law dependence on time. It is of immediate concern in p-channel MOS devices (pMOS), since they almost always operate with negative gate-to-source voltage; however, the very same mechanism also affects nMOS transistors when biased in the accumulation regime, i.e. with a negative bias applied to the gate.\n\nThe details of the mechanisms of NBTI have been debated, but two effects are believed to contribute: trapping of positively charged holes, and generation of interface states. \n\n\nThe existence of two coexisting mechanisms has resulted in scientific controversy over the relative importance of each component, and over the mechanism of generation and recovery of interface states. \n\nIn sub-micrometer devices nitrogen is incorporated into the silicon gate oxide to reduce the gate leakage current density and prevent boron penetration. It is known that incorporating nitrogen enhances NBTI. For new technologies (45 nm and shorter nominal channel lengths), high-K metal gate stacks are used as an alternative to improve the gate current density for a given equivalent oxide thickness (EOT). Even with the introduction of new materials like hafnium oxides, NBTI remains.\n\nWith the introduction of high K metal gates, a new degradation mechanism appeared, referred to as PBTI (for positive bias temperature instabilities), which affects nMOS transistor when positively biased. In this case, no interface states are generated and 100% of the Vth degradation may be recovered.\n\n\n"}
{"id": "19471895", "url": "https://en.wikipedia.org/wiki?curid=19471895", "title": "Negative affectivity", "text": "Negative affectivity\n\nNegative affectivity (NA), or negative affect, is a personality variable that involves the experience of negative emotions and poor self-concept. Negative affectivity subsumes a variety of negative emotions, including anger, contempt, disgust, guilt, fear, and nervousness. Low negative affectivity is characterized by frequent states of calmness and serenity, along with states of confidence, activeness, and great enthusiasm.\n\nIndividuals differ in negative emotional reactivity. Trait negative affectivity roughly corresponds to the dominant personality factor of anxiety/neuroticism that is found within the Big Five personality traits as emotional stability. The Big Five are characterized as openness, conscientiousness, extraversion, agreeableness, and neuroticism. Neuroticism can plague an individual with severe mood swings, frequent sadness, worry, and being easily disturbed, and predicts the development and onset of all \"common\" mental disorders. Research shows that negative affectivity relates to different classes of variables: Self-reported stress and (poor) coping skills, health complaints, and frequency of unpleasant events. Weight gain and mental health complaints are often experienced as well.\n\nPeople who express high negative affectivity view themselves and a variety of aspects of the world around them in generally negative terms. Negative affectivity is strongly related to life satisfaction. Individuals high in negative affect will exhibit, on average, higher levels of distress, anxiety, and dissatisfaction, and tend to focus on the unpleasant aspects of themselves, the world, the future, and other people, and also evoke more negative life events. The similarities between these affective traits and life satisfaction have led some researchers to view both positive and negative affect with life satisfaction as specific indicators of the broader construct of subjective well-being.\n\nNegative affect arousal mechanisms can induce negative affective states as evidenced by a study conducted by Stanley S. Seidner on negative arousal and white noise. The study quantified reactions from Mexican and Puerto Rican participants in response to the devaluation of speakers from other ethnic origins.\n\nThere are many instruments that can be used to measure negative affectivity, including measures of related concepts, such as neuroticism and trait anxiety. Two frequently used are:\n\nPANAS – The Positive and Negative Affect Schedule incorporates a 10-item negative affect scale. The PANAS-X is an expanded version of PANAS that incorporates negative affect subscales for Fear, Sadness, Guilt, Hostility, and Shyness.\n\nI-PANAS-SF – The International Positive and Negative Affect Schedule Short Form is an extensively validated brief, cross-culturally reliable 10-item version of the PANAS. Negative Affect items are Afraid, Ashamed, Hostile, Nervous and Upset. Internal consistency reliabilities between .72 and .76 are reported. The I-PANAS-SF was developed to eliminate redundant and ambiguous items and thereby derive an efficient measure for general use in research situations where either time or space are limited, or where international populations are of interest but where English may not be the mother tongue.\n\nRecent studies indicate that negative affect has important, beneficial impacts on cognition and behavior. These developments are a remarkable departure from past psychological research, which is characterized by a unilateral emphasis on the benefits of positive affect. Both states of affect influence mental processes and behavior. Negative affect is regularly recognized as a \"stable, heritable trait tendency to experience a broad range of negative feelings, such as worry, anxiety, self-criticisms, and a negative self-view\". This allows one to feel every type of emotion, which is regarded as a normal part of life and human nature. So, while the emotions themselves are viewed as negative, the individual experiencing them should not be classified as a negative person or depressed. They are going through a normal process and are feeling something that many individuals may not be able to feel or process due to differing problems.\n\nThese findings complement evolutionary psychology theories that affective states serve adaptive functions in promoting suitable cognitive strategies to deal with environmental challenges. Positive affect is associated with assimilative, top-down processing used in response to familiar, benign environments. Negative affect is connected with accommodative, bottom-up processing in response to unfamiliar, or problematic environments. Thus, positive affectivity promotes simplistic heuristic approaches that rely on preexisting knowledge and assumptions. Conversely, negative affectivity promotes controlled, analytic approaches that rely on externally drawn information.\n\nBenefits of negative affect are present in areas of cognition including perception, judgment, memory and interpersonal personal relations. Since negative affect relies more on cautious processing than preexisting knowledge, people with negative affect tend to perform better in instances involving deception, manipulation, impression formation, and stereotyping. Negative affectivity's analytical and detailed processing of information leads to fewer reconstructive-memory errors, whereas positive mood relies on broader schematic to thematic information that ignores detail. Thus, information processing in negative moods reduces the misinformation effect and increases overall accuracy of details. People also exhibit less interfering responses to stimuli when given descriptions or performing any cognitive task.\n\nPeople are notoriously susceptible to forming inaccurate judgments based on biases and limited information. Evolutionary theories propose that negative affective states tend to increase skepticism and decrease reliance on preexisting knowledge. Consequently, judgmental accuracy is improved in areas such as impression formation, reducing fundamental attribution error, stereotyping, and gullibility. While sadness is normally associated with the hippocampus, it does not produce the same side effects that would be associated with feelings of pleasure or excitement. Sadness correlates with feeling blue or the creation of tears, while excitement may cause a spike in blood pressure and one's pulse. As far as judgment goes, most people think about how they themselves feel about a certain situation. They will jump right to their current mood when asked a question. However, some mistake this process when using their current mood to justify a reaction to a stimulus. If you're sad, yet only a little bit, chances are your reactions and input will be negative as a whole.\n\nFirst impressions are one of the most basic forms of judgments people make on a daily basis; yet judgment formation is a complex and fallible process. Negative affect is shown to decrease errors in forming impressions based on presuppositions. One common judgment error is the halo effect, or the tendency to form unfounded impressions of people based on known but irrelevant information. For instance, more attractive people are often attributed with more positive qualities. Research demonstrates that positive affect tends to increase the halo effect, whereas negative affect decreases it.\n\nA study involving undergraduate students demonstrated a halo effect in identifying a middle-aged man as more likely to be a philosopher than an unconventional, young woman. These halo effects were nearly eliminated when participants were in a negative affective state. In the study, researchers sorted participants into either happy or sad groups using an autobiographical mood induction task in which participants reminisced on sad or happy memories. Then, participants read a philosophical essay by a fake academic who was identified as either a middle-aged, bespectacled man or as a young, unorthodox-looking woman. The fake writer was evaluated on intelligence and competence. The positive affect group exhibited a strong halo effect, rating the male writer significantly higher than the female writer in competence. The negative affect group exhibited almost no halo effects rating the two equally. Researchers concluded that impression formation is improved by negative affect. Their findings support theories that negative affect results in more elaborate processing based upon external, available information.\n\nThe systematic, attentive approach caused by negative affect reduces fundamental attribution error, the tendency to inaccurately attribute behavior to a person's internal character without taking external, situational factors into account. The fundamental attribution error (FAE) is connected with positive affect since it occurs when people use top-down cognitive processing based on inferences. Negative affect stimulates bottom-up, systematic analysis that reduces fundamental attribution error.\n\nThis effect is documented in FAE research in which students evaluated a fake debater on attitude and likability based on an essay the \"debater\" wrote. After being sorted into positive or negative affect groups, participants read one of two possible essays arguing for one side or another on a highly controversial topic. Participants were informed that the debater was assigned a stance to take in the essay that did not necessarily reflect his views. Still, the positive affect groups rated debaters who argued unpopular views as holding the same attitude expressed in the essay. They were also rated as unlikeable compared to debaters with popular stances, thus, demonstrating FAE. In contrast, the data for the negative affect group displayed no significant difference in ratings for debaters with popular stance and debaters with unpopular stances. These results indicate that positive affect assimilation styles promote fundamental attribution error, and negative affect accommodation styles minimize the error in respect to judging people.\n\nNegative affect benefits judgment in diminishing the implicit use of stereotypes by promoting closer attention to stimuli. In one study, participants were less likely to discriminate against targets that appeared Muslim when in a negative affective state. After organizing participants into positive and negative affect groups, researchers had them play a computer game. Participants had to make rapid decisions to shoot only at targets carrying a gun. Some of the targets wore turbans making them appear Muslim. As expected, there was a significant bias against Muslim targets resulting in a tendency to shoot at them. However, this tendency decreased with subjects in negative affective states. Positive affect groups developed more aggressive tendencies toward Muslims. Researchers concluded that negative affect leads to less reliance on internal stereotypes, thus decreasing judgmental bias.\n\nMultiple studies have shown that negative affectivity has a beneficial role in increasing skepticism and decreasing gullibility. Because negative affective states increase external analysis and attention to details, people in negative states are better able to detect deception.\n\nResearchers have presented findings in which students in negative affective states had improved lie detection compared to students in positive affective states. In a study, students watched video clips of everyday people either lying or telling the truth. First, music was used to induce positive, negative, or neutral affect in participants. Then, experimenters played 14 video messages that had to be identified by participants as true or false. As expected, the negative affect group performed better in veracity judgments than the positive affect group who performed no better than chance. Researchers believe that the negative affect groups detected deception more successfully because they attended to stimulus details and systematically built inferences from those details.\n\nMemory has been found to have many failures that effect the accuracy of recalled memories. This has been especially pragmatic in criminal settings as eyewitness memories have been found to be less reliable than one would hope. However, the externally focused and accommodative processing of negative affect has a positive effect on the overall improvement of memory. This evidenced by reduction of the misinformation effect and the number of false memories reported. The knowledge implies that negative affect can be used to enhance eyewitness memory; however, additional research suggests that the extent to which memory is improved by negative affect does not sufficiently improve eyewitness testimonies to significantly reduce its error.\n\nNegative affect has been shown to decrease susceptibility of incorporating misleading information, which is related to the misinformation effect. The misinformation effect refers to the finding that misleading information presented between the encoding of an event and its subsequent recall influences a witness's memory. This corresponds to two types of memory failure:\n\nNegative mood is shown to decrease suggestibility error. This is seen through reduced amounts of incorporation of false memories when misleading information is present. On the other hand, positive affect has shown to increase susceptibility to misleading information. An experiment with undergraduate students supported these results. Participants began the study in a lecture hall and witnessed what they thought was an unexpected five-minute belligerent encounter between an intruder and the lecturer. A week later, these participants watched a 10-minute-long video that generated either a positive, negative or neutral mood. They then completed a brief questionnaire about the previous incident between the intruder and lecturer that they witnessed the week earlier. In this questionnaire half of the participants received questions with misleading information and the other half received questions without any misleading information. This manipulation was used to determine if participants were susceptible to suggestibility failure. After 45 minutes of unrelated distractors participants were given a set of true or false questions which tested for false memories. Participants experiencing negative moods reported fewer numbers of false memories, whereas those experiencing positive moods reported a greater amount of false memories. This implies that positive affect promotes integration of misleading details and negative affect reduces the misinformation effect.\n\nPeople who experience negative affectivity following an event report fewer reconstructive false memories. This was evidenced by two studies conducted around public events. The first surrounded the events of the televised O.J. Simpson trial. Participants were asked to fill out questionnaires three times: one week, two months and a year after the televised verdict. These questionnaires measured participant emotion towards the verdict and the accuracy of their recalled memory of what occurred during the trial. Overall the study found that although participant response to the event outcome did not affect the quantity of remembered information, it did influence the likelihood of false memory. Participants who were pleased with the verdict of the O.J. Simpson trial were more likely to falsely believe something occurred during the trial than those who were displeased with the verdict. Another experiment found the same findings with Red Sox fans and Yankees fans in their overall memory of events that occurred in the final game of a 2004 playoff series in which the Red Sox defeated the Yankees. The study found that the Yankees fans had better memory of events that occurred than the Red Sox fans. The results from both of these experiments are consistent with the findings that negative emotion can lead to fewer memory errors and thus increased memory accuracy of events.\n\nAlthough negative affect has been shown to decrease the misinformation effect, the degree to which memory is improved is not enough to make a significant effect on witness testimony. In fact, emotions, including negative affect, are shown to reduce accuracy in identifying perpetrators from photographic lineups. Researchers demonstrated this effect in an experiment in which participants watched a video that induced either negative emotion or a neutral mood. The two videos were deliberately similar except for the action of interest, which was either a mugging (negative emotion) or a conversation (neutral emotion). After watching one of the two videos participants are shown perpetrator lineups, which either contained the target perpetrator from the video or a foil, a person that looked similar to the target. The results revealed that the participants who watched the emotion-induced video were more likely to incorrectly identify the innocent foil than to correctly identify the perpetrator. Neutral participants were more likely to correctly identify the perpetrator in comparison to their emotional counterparts. This demonstrates that emotional affect in forensic settings decreases accuracy of eyewitness memory. These findings are consistent with prior knowledge that stress and emotion greatly impair eyewitness ability to recognitive perpetrators.\n\nNegative affectivity can produce several interpersonal benefits. It can cause subjects to be more polite and considerate with others. Unlike positive mood, which causes less assertive approaches, negative affectivity can, in many ways, cause a person to be more polite and elaborate when making requests.\n\nNegative affectivity increases the accuracy of social perceptions and inferences. Specifically, high negative-affectivity people have more negative, but accurate, perceptions of the impression they make to others. People with low negative affectivity form overly-positive, potentially inaccurate impression of others that can lead to misplaced trust.\n\nA research conducted by Forgas J.P studied how affectivity can influence intergroup discrimination. He measured affectivity by how people allocate rewards to in-group and out-group members. In the procedure, participants had to describe their interpretations after looking at patterns of judgments about people. Afterwards, participants were exposed to a mood induction process, where they had to watch videotapes designed to elicit negative or positive affectivity. Results showed that participants with positive affectivity were more negative and discriminated more than participants with negative affectivity. Also, happy participants were more likely to discriminate between in-group and out-group members than sad participants. Negative affect is often associated with team selection. It is viewed as a trait that could make selecting individuals for a team irrelevant, thus preventing knowledge from becoming known or predicted for current issues that may arise.\n\nNegative affectivity subconsciously signals a challenging social environment.\nNegative mood may increase a tendency to conform to social norms.\n\nIn a study, college students where exposed to a mood induction process. After the mood induction process, participants were required to watch a show with positive and negative elements. After watching the show, they were asked to engage on a hypothetical conversation in which they \"describe the episode (they) just observed to a friend\". Their speech was recorded and transcribed during this task. Results showed that speakers in a negative mood had a better quality descriptions and greater amount of information and details. These results show that negative mood can improve people's communication skills.\n\nA negative mood is closely linked to better conversation because it makes use of the hippocampus and different regions of the brain. When someone is upset, that individual may see or hear things differently than an individual who is very upbeat and happy all the time. The small details the negative individual picks up may be something completely overlooked before. Anxiety disorders are often associated with over-thinking and ruminating on topics that would seem irrelevant and pointless to an individual without a disorder. OCD is one common anxiety trait that allows the affected individual a different insight on how things may appear to be. A person that makes use of his or her negative affect has a different view of the world and what goes on in it, thus making their conversations different and interesting to others.\n\nResults of one study show that participants with negative affectivity were more careful with the information they shared with others, being more cautious with who they could trust or not. Researchers found that negative mood not only decreases intimacy levels but also increases caution in placing trust in others.\n\n\n"}
{"id": "10784136", "url": "https://en.wikipedia.org/wiki?curid=10784136", "title": "Negative base", "text": "Negative base\n\nA negative base (or negative radix) may be used to construct a non-standard positional numeral system. Like other place-value systems, each position holds multiples of the appropriate power of the system's base; but that base is negative—that is to say, the base is equal to for some natural number ().\n\nNegative-base systems can accommodate all the same numbers as standard place-value systems, but both positive and negative numbers are represented without the use of a minus sign (or, in computer representation, a sign bit); this advantage is countered by an increased complexity of arithmetic operations. The need to store the information normally contained by a negative sign often results in a negative-base number being one digit longer than its positive-base equivalent.\n\nThe common names for negative-base positional numeral systems are formed by prefixing \"nega-\" to the name of the corresponding positive-base system; for example, negadecimal (base −10) corresponds to decimal (base 10), negabinary (base −2) to binary (base 2), and negaternary (base −3) to ternary (base 3).\n\nConsider what is meant by the representation in the negadecimal system, whose base is −10:\n\nSince 10,000 + (−2,000) + 200 + (−40) + 3 = , the representation in negadecimal notation is equivalent to in decimal notation, while in decimal would be written in negadecimal.\n\nNegative numerical bases were first considered by Vittorio Grünwald in his work \"Giornale di Matematiche di Battaglini\", published in 1885. Grünwald gave algorithms for performing addition, subtraction, multiplication, division, root extraction, divisibility tests, and radix conversion. Negative bases were later independently rediscovered by A. J. Kempner in 1936 and Zdzisław Pawlak and A. Wakulicz in 1959.\n\nNegabinary was implemented in the early Polish computer BINEG (and UMC), built 1957–59, based on ideas by Z. Pawlak and A. Lazarkiewicz from the Mathematical Institute in Warsaw. Implementations since then have been rare.\n\nDenoting the base as , every integer can be written uniquely as\nwhere each digit is an integer from 0 to and the leading digit is (unless ). The base expansion of is then given by the string .\n\nNegative-base systems may thus be compared to signed-digit representations, such as balanced ternary, where the radix is positive but the digits are taken from a partially negative range. (In the table below the digit of value −1 is written as the single character T.)\n\nSome numbers have the same representation in base as in base . For example, the numbers from 100 to 109 have the same representations in decimal and negadecimal. Similarly,\nand is represented by 10001 in binary and 10001 in negabinary.\n\nSome numbers with their expansions in a number of positive and corresponding negative bases are:\n\nNote that the base expansions of negative integers have an even number of digits, while the base expansions of the non-negative integers have an odd number of digits.\n\nThe base expansion of a number can be found by repeated division by , recording the non-negative remainders of formula_3, and concatenating those remainders, starting with the last. Note that if , remainder , then and therefore . To arrive at the correct conversion, the value for must be chosen such that is non-negative and minimal. This is exemplified in the fourth line of the following example wherein –5 ÷ –3 must be chosen to equal 2 remainder 1 instead of 1 remainder –2.\n\nFor example, to convert 146 in decimal to negaternary:\nReading the remainders backward we obtain the negaternary representation of 146: 21102.\n\nNote that in most programming languages, the result (in integer arithmetic) of dividing a negative number by a negative number is rounded towards 0, usually leaving a negative remainder. In such a case we have . Because , is the positive remainder. Therefore, to get the correct result in such case, computer implementations of the above algorithm should add 1 and to the quotient and remainder respectively.\n\nPrivate Shared Function ToNegaternary(value As Integer) As String\n\nEnd Function\nfrom [-10 -2] interval:\n\n (if\n\nThe conversion from integer to some negative base:\n\nFunction toNegativeBase(Number As Integer , base As Integer) As System.Collections.Generic.List(Of Integer)\n\nend function\nThe conversion to \"negabinary\" (base −2; digits formula_5) allows a remarkable shortcut \n(C implementation):\nDue to D. Librik (Szudzik). The bitwise XOR portion is originally due to Schroeppel (1972).\n\nJavaScript port for the same shortcut calculation: \nThe conversion to \"negaquaternary\" (base −4; digits formula_6) allows a similar shortcut (C implementation):\nJavaScript port for the same shortcut calculation: \nThe following describes the arithmetic operations for negabinary; calculations in larger bases are similar.\n\nAdding negabinary numbers proceeds bitwise, starting from the least significant bits; the bits from each addend are summed with the (balanced ternary) carry from the previous bit (0 at the LSB). This sum is then decomposed into an output bit and carry for the next iteration as show in the table:\n\nThe second row of this table, for instance, expresses the fact that −1 = 1 + 1 × −2; the fifth row says 2 = 0 + −1 × −2; etc.\n\nAs an example, to add 1010101 (1 + 4 + 16 + 64 = 85) and 1110100 (4 + 16 − 32 + 64 = 52),\n\nso the result is 110011001 (1 − 8 + 16 − 128 + 256 = 137).\n\nWhile adding two negabinary numbers, every time a carry is generated an extra carry should be propagated to next bit. Consider same example as above\n\nA full adder circuit can be designed to add numbers in negabinary. The following logic is used to calculate the sum and carries:\n\nIncrementing a negabinary number can be done by using the following formula:\n\nTo subtract, multiply each bit of the second number by −1, and add the numbers, using the same table as above.\n\nAs an example, to compute 1101001 (1 − 8 − 32 + 64 = 25) minus 1110100 (4 + 16 − 32 + 64 = 52),\n\nso the result is 100101 (1 + 4 −32 = −27).\n\nUnary negation, , can be computed as binary subtraction from zero, .\n\nShifting to the left multiplies by −2, shifting to the right divides by −2.\n\nTo multiply, multiply like normal decimal or binary numbers, but using the negabinary rules for adding the carry, when adding the numbers.\n\nFor each column, add \"carry\" to \"number\", and divide the sum by −2, to get the new \"carry\", and the resulting bit as the remainder.\n\nIt is possible to compare negabinary numbers by slightly adjusting a normal unsigned binary comparator. When comparing the numbers formula_11 and formula_12, invert each odd positioned bit of both numbers.\nAfter this, compare formula_11 and formula_12 using a standard unsigned comparator.\nBase representation may of course be carried beyond the radix point, allowing the representation of non-integral numbers.\n\nAs with positive-base systems, terminating representations correspond to fractions where the denominator is a power of the base; repeating representations correspond to other rationals, and for the same reason.\n\nUnlike positive-base systems, where integers and terminating fractions have non-unique representations (for example, in decimal 0.999… = 1) in negative-base systems the integers have only a single representation. However, there do exist rationals with non-unique representations. For the digits {0, 1, …, t} with formula_15 the biggest digit and\nwe have\nSo every number formula_19 with a terminating fraction formula_20 added has two distinct representations.\n\nFor example, in negaternary, i.e. formula_21 and formula_22, there is\n\nSuch non-unique representations can be found by considering the largest and smallest possible representations with integral parts 0 and 1 respectively, and then noting that they are equal. (Indeed, this works with any integral-base system.) The rationals thus non-uniquely expressible are those of form\nwith formula_25\n\nJust as using a negative base allows the representation of negative numbers without an explicit negative sign, using an imaginary base allows the representation of Gaussian integers. Donald Knuth proposed the quater-imaginary base (base 2i) in 1955.\n\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "561582", "url": "https://en.wikipedia.org/wiki?curid=561582", "title": "Negative gearing", "text": "Negative gearing\n\nNegative gearing is a form of financial leverage whereby an investor borrows money to acquire an income-producing investment and the gross income generated by the investment (at least in the short term) is less than the cost of owning and managing the investment, including depreciation and interest charged on the loan (but excluding capital repayments). The investor may enter into a negatively geared investment expecting tax benefits or the capital gain on the investment, when the investment is ultimately disposed of, to exceed the accumulated losses of holding the investment. The investor would take into account the tax treatment of negative gearing, which may generate additional benefits to the investor in the form of tax benefits if the loss on a negatively geared investment is tax-deductible against the investor's other taxable income and if the capital gain on the sale is given a favourable tax treatment. \nNegative gearing is often discussed with regard to real estate, where rental income is less than mortgage interest costs, but may also apply to shares whose dividend income falls short of interest costs on a margin loan. The tax treatment may or may not be the same between the two.\n\nPositive gearing occurs when one borrows to invest in an income-producing asset and the returns (income) from that asset exceed the cost of borrowing. From then on, the investor must pay tax on the rental income profit until the asset is sold, when point the investor must pay capital gains tax on any profit.\n\nWhen the income generated covers the interest, it is simply a geared investment, which creates passive income. A negative gearing strategy makes a profit under any of the following circumstances:\n\n\nThe investor must be able to fund any shortfall until the asset is sold or until the investment becomes positively geared (income > interest). The different tax treatment of planned ongoing losses and possible future capital gains affects the investor's final return. In countries that tax capital gains at a lower rate than income, it is possible for an investor to make a loss overall before taxation but a small gain after taxpayer subsidies.\n\nSome countries, including Australia, Japan, and New Zealand, allow unrestricted use of negative gearing losses to offset income from other sources. Several other OECD countries, including the US, Germany, Sweden, Canada, and France, allow loss offsetting with some restrictions. Applying tax deductions from negatively geared investment housing to other income is not permitted in the UK or the Netherlands. With respect to investment decisions and market prices, other taxes such as stamp duties and capital gains tax may be more or less onerous in those countries, increasing or decreasing the attractiveness of residential property as an investment.\n\nA negatively-geared investment property will generally remain negatively geared for several years, when the rental income will have increased with inflation to the point that the investment is positively geared (the rental income is greater than the interest cost).\n\nThe tax treatment of negative gearing (also termed \"rental loss offset against other income\") varies. For example:\n\nAustralia allows the application of property losses arising from negative gearing against other types of income, such as wage or business income, with only a few limits or restrictions. Negative gearing by property investors reduced personal income tax revenue in Australia by $600 million in the 2001/02 tax year, $3.9 billion in 2004/05 and $13.2 billion in 2010/11.\n\nNegative gearing is a controversial political issue in Australia and was a major issue during the 2016 federal election, during which the Labor Party proposed restricting (but not eliminating) negative gearing and to halve the capital gains tax discount to 25%. Analysis found that negative gearing in Australia provides a greater benefit to wealthier Australians than the less wealthy. Then Federal Treasurer Scott Morrison, in defence of negative gearing, cited tax data that shows that numerous middle income groups (he mentioned teachers, nurses, and electricians) benefit in larger numbers from negative gearing than finance managers. (The raw numbers do not show the extent to which different professional groups benefit.)\n\nTraditionally, Australian taxpayers have been allowed to negatively gear their investment properties, in the strict sense of investing in property at an initial loss. Negative gearing was restricted by a prohibition on the transfer of contingent property income and the property losses could not offset income from labour. It is assumed this applied to losses as well as income, but this is unclear in the Income Tax Assessment Act 1936. \n\nA common method of bypassing the restrictions on property losses offsetting income from labour was to convert such income into another form through the use of partnerships and other legal mechanisms. As a result, this restriction may not have been significant. A partnership, trust or similar legal mechaninsm allowed an individual, or individuals, to pool their incomes and losses, thus allowing them to offset property losses against other incomes. This option was not available to individuals who derived their income from wages. \n\nIn 1983, the Victorian Deputy Commissioner of Taxation briefly denied Victorian property investors the deduction for interest in excess of the rental income, so losses could not be transferred nor moved to a future tax year. That ruling was quickly over-ruled by the federal tax commissioner.\n\nin 1985, under the Australian Labor Party Hawke government, negative gearing rules were changed so that property losses were no longer quarantined and instead could offset wage income. Six months later, following the tax summit in July 1985, the Hawke government undid this change, once more quarantining negative gearing interest expenses on new transactions so that they could be claimed only against rental income, not other income. (Any excess loss could be carried forward to offset property income in later years.) That ensured that at personal level and, more importantly, at a national level, property losses would not be subsidised by income from personal exertion. In applying the formula, all previous governments thereby isolated and consequently discouraged capital speculation being subsidised from the general income tax receipts pool.\n\nIn addition, a capital gains tax (CGT) was introduced in Australia on 20 September 1985. While a separate tax, it is often associated with negative gearing.\n\nThe Hawke government's reversion to the earlier system in which property losses could not offset income from labour was unpopular with property investors. These investors claimed this reversion had caused investment in rental accommodation to dry up and rents to rise substantially. This was unsupported by evidence other than localised increases in real rents in both Perth and Sydney, which also had the lowest vacancy rates of all capital cities at the time. \n\nHowever, in July 1987, after lobbying by the property industry, the Federal Labor government with Paul Keating as its Treasurer, reversed its decision once more, allowing negative gearing losses to be applied against income from labour.\n\nAustralian tax treatment of negative gearing is as follows:\n\n\nThe tax treatment of negative gearing and capital gains may benefit investors in a number of ways, including:\n\nHowever, in certain situations the tax rate applied to the capital gain may be higher than the rate of tax saving because of initial deductions such as for investors who have a low marginal tax rate while they make deductions but a high marginal rate in the year the capital gain is realised.\n\nIn contrast, the tax treatment of real estate by owner-occupiers differs from investment properties. Mortgage interest and upkeep expenses on a private property are not deductible, but any capital gain (or loss) made on disposal of a primary residence is tax-free. (Special rules apply on a change from private use to renting or vice versa and for what is considered a main residence.)\n\nThe economic and social effects of negative gearing in Australia are a matter of ongoing debate. Those in favour of negative gearing argue:\n\n\nOpponents of negative gearing argue:\n\n\nThe view that the temporary removal of negative gearing between 1985 and 1987 caused rents to rise has been challenged by Chief Economist of the ANZ Saul Eslake, who has been quoted as saying: It's true, according to Real Estate Institute data, that rents went up in Sydney and Perth. But the same data doesn't show any discernible increase in the other state capitals. I would say that, if negative gearing had been responsible for a surge in rents, then you should have observed it everywhere, not just two capitals. In fact, if you dig into other parts of the REI database, what you find is that vacancy rates were unusually low at that time before negative gearing was abolished.Eslake is referring to changes in inflation-adjusted rents (i.e., when CPI inflation is subtracted from the nominal rent increases). These are also known as real rent changes. Nominal rents nationally rose by over 25% during the two years that negative gearing was quarantined. They rose strongly in every Australian capital city, according to the official ABS CPI data. However, as nominal changes include inflation, they provide a less clear picture of how rents changed in effect, and of how changes such as disallowing property losses to offset other types of income affect rent.\n\nAdditionally, it is difficult to assess the impact no longer allowing such deductions had during those two years, given that these deductions had only been allowed for six months prior to their disallowance in July 1985.\n\nAn ABC Fact check report, posted on the 3rd of March 2016 and titled \"Fact check: Did abolishing negative gearing push up rents?\" provided the following, after inflation, rental cost changes, during the period negative gearing was abolished. (Rounded to whole numbers and listed by size of city). Sydney +2% to +4%, Melbourne +2% to 0%, Brisbane -3% to -4%, Perth 0% to +6% and Adelaide +1% to -3%. All values have the effects of inflation removed, which in 1986 was 9.18%. \n\nAs a comparison with house pricing, the medium house price in Sydney rose from $98000, in 1985, to $120025, in 1986, or after inflation, by 12.18%. In Brisbane the medium house price moved from $61550 to $63000, in the same period, or an after inflation, a change of -6.25%. The medium unit price in Sydney moved from $70500 to $72300, in the same period. An after inflation change of -6.07%. In brisbane the after inflation change for units was -0.05%.\n\nIn 2003, the Reserve Bank of Australia (RBA) stated in its submission to the Productivity Commission First Home Ownership Inquiry: there are no specific aspects of current tax arrangements designed to encourage investment in property relative to other investments in the Australian tax system. Nor is there any recent tax policy initiative we can point to that accounts for the rapid growth in geared property investment. But the fact is that when we observe the results, resources and finance are being disproportionately channelled into this area, and property promoters use tax effectiveness as an important selling point.They went on to say that \"the most sensible area to look for moderation of demand is among investors\", and that: the taxation treatment in Australia is more favourable to investors than is the case in other countries. In particular, the following areas appear worthy of further study by the Productivity Commission:i. ability to negatively gear an investment property when there is little prospect of the property being cash-flow positive for many years;ii. the benefit that investors receive by virtue of the fact that when property depreciation allowances are \"clawed back\" through the capital gains tax, the rate of tax is lower than the rate that applied when depreciation was allowed in the first place.iii. the general treatment of property depreciation, including the ability to claim depreciation on loss-making investments.\n\nIn 2008, the report of the Senate Select Committee on Housing Affordability in Australia echoed the findings of the 2004 Productivity Commission report. One recommendation to the enquiry suggested that negative gearing should be capped: \"There should not be unlimited access. Millionaires and billionaires should not be able to access it, and you should not be able to access it on your 20th investment property. There should be limits to it.\"\n\nA 2015 report from the Senate Economics References Committee argues that, while negative gearing has an influence on housing affordability, the primary issue is a mismatch between supply and demand. A submission to this committee from the Department of Social Services stated that:[while] demand for housing has increased significantly over the last 30 years, the supply of new dwellings has not responded, with average annual completions of new dwellings remaining around 150,000 since the mid-1980s.The effect of negative gearing on the supply side of dwelling construction is difficult to pin down. Commentary from Eslake and others has highlighted the preponderance of negatively-geared purchases in established suburbs where the probability of a lightly-taxed capital gain exists, challenging the idea that negative gearing leads to substantial amounts of new construction. Many economists have commented extensively on the tax subsidy being made available to speculative buyers in competition against homebuyers, who have no such tax subsidy, leading to significant social dislocation.\n\nAdditionally, the tax subsidy feeding into higher home prices adds to the wealth of those taking advantage of negative gearing. The process that crowds out domestic home owners by pushing up the price of housing also makes the successful user of negative gearing more asset rich due to the increase in land value. This allows these people to borrow further funds against equity in the previously acquired properties, resulting in further acquisitions under tax subsidy. This process can raise prices and thereby make it harder for people who wish to buy a house as an owner-occupier.\n\nWhile allowing for negative gearing in its basic form, the United Kingdom does not allow the transfer of one type of income (or loss) to another type of income. This is due to its schedular system of taxation. In this type of taxation system, the tax paid is dependent on income source. Therefore, an individual who received an income from labour and from land would pay two separate tax rates for the two relevant income sources.\n\nBetween 1997 and 2007, the Tax Law Rewrite Project changed this system by simplifying the schedules. As with the previous system, people would not be allowed to transfer incomes (or losses).\n\nA UK government online resource on renting out property in England and Wales outlines how to offset losses. It states that losses can be offset against \"future profits by carrying it forward to a later year\" or against \"profits from other properties (if you have them)\".\n\nNew Zealand allows negative gearing and the transfer of losses to other income streams, with some restrictions.\n\nThe Rental Income Guide states a loss can only be deducted against other incomes if the rental income is at market rate.\n\nThe Opposition Labour Party attempted to raise negative gearing in the 2011 election, but after their failure to win government the issue reduced in significance.\n\nIn principle, Canada does not allow the transfer of income streams. However, the most current Canadian tax form indicates this can occur in some circumstances. According to \"Line 221 - Carrying charges and interest expenses\", interest payments from an investment designed to generate an income can be deducted:Claim the following carrying charges and interest you paid to earn income from investments: [...] Most interest you pay on money you borrow for investment purposes, but generally only if you use it to try to earn investment income, including interest and dividends. However, if the only earnings your investment can produce are capital gains, you cannot claim the interest you paid.Other sources indicate the deduction must be reasonable and that people should contact the Canada Revenue Agency for more information. The \"Rental Income Includes Form T776\" states people can deduce rental losses from other sources of income: \"You have a rental loss if your rental expenses are more than your gross rental income. If you incur the expenses to earn income, you can deduct your rental loss against your other sources of income.\" However, there is a caveat: the rental loss must be reasonable. What is reasonable is not defined in the \"Rental Income Includes Form T776\" Guide.\n\nBased on these sources, claiming rental losses against other incomes in a given year is allowed as long as a profit is made over the life of the investment, excluding the effects of capital gains.\n\nIt should be noted that Canada has a Federal and Provincial income tax, and the above only relates to Federal income tax.\n\nIn principle, the US federal tax does not allow the transfer of income streams. In general, taxpayers can only deduct expenses of renting property from their rental income, as renting property out is usually considered a passive activity. However, if renters are considered to have actively participated in the activities, they can claim deductions from rental losses against their other \"nonpassive income\". A definition of \"active participation\" is outlined in the \"Reporting Rental Income, Expenses, and Losses\" guide:You actively participated in a rental real estate activity if you (and your spouse) owned at least 10% of the rental property and you made management decisions or arranged for others to provide services (such as repairs) in a significant and \"bona fide\" sense. Management decisions that may count as active participation include approving new tenants, deciding on rental terms, approving expenditures, and other similar decisions.It is possible deduct any loss against other incomes, depending on a range of factors.\n\nJapan allows tax payers to offset rental losses against other income.\n\nIndividuals can claim losses against rental loss with minimal restrictions, but if the property was owned through a partnership or trust there are restrictions.\n\nThere are a number of additional rules, such as restricting claims of losses due to Bad Debt. Additional information can be found in the Japan Tax Site.\n\nThe German tax system is complex, but within the bounds of standard federal income tax, Germany does not allow the transfer of income. Rental losses can only be offset against rental income. As stated on the Global Property Guide site, \"Owners can deduct any expenses from the gross receipts, which were incurred to produce, maintain and safeguard that income.\"\n\nGermany recognizes seven sources of income:\n\n\nThe income from each of these sources is calculated separately.\n\nRental income is taxed as income and is subject to the progressive tax rate. Interest on loans provided to finance real estate, expenses, and property-related cost (e.g., management fees, insurance) can be deducted from the taxable rental income.\n\nIn principle, the Dutch tax system does not allow the transfer of income. Most citizens calculate tax, separately, in 3 income groups:\n\nHowever, I am unable to identify a clear definition where rental income fits in these three categories in the government taxation laws. \n\nDutch resident and non-resident companies and partnerships owning Dutch property are in principle allowed to deduct interest expenses on loans from banks or affiliated companies, and property-related costs from their taxable income.\n\n\n"}
{"id": "6475900", "url": "https://en.wikipedia.org/wiki?curid=6475900", "title": "Negative pregnant", "text": "Negative pregnant\n\nA negative pregnant (sometimes called a pregnant denial) refers to a denial which implies its affirmative opposite by seeming to deny only a qualification of the allegation and not the allegation itself. For example, \"I deny that I owe the plaintiff five hundred dollars\" might imply that the person making the statement owes some other sum of money, and was only denying that they owe that particular amount. \n\nA negative pregnant which appears in pleadings will often elicit a request for further and better particulars, or an interrogatory. In order to avoid a negative pregnant in the above example, one might instead say, \"I deny that I owe the plaintiff five hundred dollars, or any other sum of money.\"\n\nThe issue can also arise in the context of statutory interpretation. For instance, Justice Thurgood Marshall argues in his dissent to \"EEOC v. Aramco\" that the presumption against extraterritoriality is rebutted by a negative inference from the alien-exemption provision of Title VII of the Civil Rights Act of 1964, which states that Title VII \"shall not apply to an employer with respect to the employment of aliens outside any State.\" Marshall concludes that \"Absent an intention that Title VII \"apply\" 'outside any State,' Congress would have had no reason to craft this extraterritorial exemption. And because only discrimination against aliens is exempted, employers remain accountable for discrimination against United States citizens abroad.\" \n\n"}
{"id": "26502557", "url": "https://en.wikipedia.org/wiki?curid=26502557", "title": "Negative room pressure", "text": "Negative room pressure\n\nNegative room pressure is an isolation technique used in hospitals and medical centers to prevent cross-contaminations from room to room. It includes a ventilation system that generates negative pressure to allow air to flow into the isolation room but not escape from the room, as air will naturally flow from areas with higher pressure to areas with lower pressure, thereby preventing contaminated air from escaping the room. This technique is used to isolate patients with airborne contagious diseases such as tuberculosis, measles, or chickenpox.\n\nNegative pressure is generated and maintained by a ventilation system that removes more exhaust air from the room than air is allowed into the room. Air is allowed into the room through a gap under the door (typically about one half-inch high). Except for this gap, the room should be as airtight as possible, allowing no air in through cracks and gaps, such as those around windows, light fixtures and electrical outlets. Leakage from these sources can compromise or eliminate room negative pressure.\n\nA smoke test can help determine whether a room is under negative pressure. A tube containing smoke is held near the bottom of the negative pressure room door, about 2 inches in front of the door. The smoke tube is held parallel to the door, and a small amount of smoke is then generated by gently squeezing the bulb. Care is taken to release the smoke from the tube slowly to ensure the velocity of the smoke from the tube does not overpower the air velocity. If the room is at negative pressure, the smoke will travel under the door and into the room. If the room is not a negative pressure, the smoke will be blown outward or will stay stationary.\n\n"}
{"id": "1969927", "url": "https://en.wikipedia.org/wiki?curid=1969927", "title": "Negative space", "text": "Negative space\n\nNegative space, in art, is the space around and between the subject(s) of an image. Negative space may be most evident when the space around a subject, not the subject itself, forms an interesting or artistically relevant shape, and such space occasionally is used to artistic effect as the \"real\" subject of an image.\n\nThe use of negative space is a key element of artistic composition. The Japanese word \"ma\" is sometimes used for this concept, for example in garden design.\n\nIn a two-tone, black-and-white image, a subject is normally depicted in black and the space around it is left blank (white), thereby forming a silhouette of the subject. Reversing the tones so that the space around the subject is printed black and the subject itself is left blank, however, causes the negative space to be apparent as it forms shapes around the subject. This is called figure-ground reversal.\n\nIn graphic design of printed or displayed materials, where effective communication is the objective, the use of negative space may be crucial. Not only within the typography, but in its placement in relation to the whole. It is the basis of why upper and lower case typography always is more legible than the use of all capital letters. Negative space varies around lower case letters, allowing the human eye to distinguish each word rapidly as one distinctive item, rather than having to parse out what the words are in a string of letters that all present the same overall profile as in all caps. The same judicious use of negative space drives the effectiveness of the entire design. Because of the long history of the use of black ink on white paper, \"white space\" is the term often used in graphics to identify the same separation.\n\nElements of an image that distract from the intended subject, or in the case of photography, objects in the same focal plane, are not considered negative space. Negative space may be used to depict a subject in a chosen medium by showing everything around the subject, but not the subject itself. Use of negative space will produce a silhouette of the subject. Most often, negative space is used as a neutral or contrasting background to draw attention to the main subject, which then is referred to as the positive space.\n\nConsidering and improving the balance between negative space and positive space in a composition is considered by many to enhance the design. This basic, but often overlooked, principle of design gives the eye a \"place to rest,\" increasing the appeal of a composition through subtle means.\n\nThe use of negative space in art may be analogous to silence in music, but only when it is juxtaposed with adjacent musical ideas. As such, there is a difference between inert and active silences in music, where the latter is more closely analogous to negative space in art.\n\n\n"}
{"id": "3921784", "url": "https://en.wikipedia.org/wiki?curid=3921784", "title": "Original camera negative", "text": "Original camera negative\n\nThe original camera negative (OCN) is the film in a traditional film-based movie camera which captures the original image. This is the film from which all other copies will be made. It is known as raw stock prior to exposure.\n\nThe size of a roll varies depending on the film gauge and whether or not a new roll, re-can, or short end was used. One hundred or 400 foot rolls are common in 16mm, while 400 or 1,000 foot (ft) rolls are used in 35mm work. While these are the most common sizes, other lengths such as 200, 800, or 1,200 ft may be commercially available from film stock manufacturers, usually by special order. Rolls of 100 and 200 ft are generally wound on spools for daylight-loading, while longer lengths are only wound around a plastic core. Core-wound stock has no exposure protection outside its packaging, and therefore must be loaded into a camera magazine within a darkroom or changing bag/tent in order to prevent the film being fogged.\n\nOriginal camera negative is of great value, as if lost or damaged it cannot be re-created without re-shooting the scene, something which is often impossible. It also contains the highest-quality version of the original image available, before any analog resolution and dynamic range loss from copying. For these reasons, original camera negative is handled with great care, and only by specialized trained people in dedicated film laboratories.\n\nAfter the film is processed by the film lab, camera rolls are assembled into lab rolls of 1,200 to 1,500 ft. Work prints may be made for viewing dailies or editing the picture on film.\n\nOnce film editing is finalized, a negative cutter will conform the negative using the Keykode on the edge of the film as a reference, cutting the original camera negative and incorporating any opticals (titles, dissolves, fades, and special effects), and cementing it together into several rolls. \n\nThe edited original negative is then copied to create a safety positive which can be used as a backup to create a usable negative. At this point, an answer print will be created from the original camera negative, and upon its approval, interpositives (IPs) and internegatives (INs) are created, from which the release prints are made. Generally speaking, the original camera negative is considered too important and delicate to be used for any processes more than necessary, as each pass through a lab process carries the risk of further degrading the quality of the negative by scratching the emulsion. Once an answer print is approved, the interpositives and internegatives are regarded as the earliest generation of the finished and graded film, and are almost always used for transfers to video or new film restorations. The original camera negatives is usually regarded as a last resort in the event that all of the intermediate elements have been compromised or lost.\n\nThe more popular a film is, the higher the likelihood that the original negative is in a worse shape, due to the need to return to the original camera negative to strike new interpositives to replace the exhausted ones, and thus create more internegatives and release prints. Before 1969, 35mm prints were struck directly from the original negative, often running into hundreds of copies, and causing further wear on the original.\n\nPhysical film stock is still occasionally used in film-making, particularly in prestige productions where the director and cinematographer have the power to require the extra cost, but as of 2016, it is becoming increasingly rare.\n\nIn modern cinematography, the camera is usually a digital camera, and no physical negative exists. However, the concept of \"camera original material\" is still used to describe camera image data. Camera original material that has not yet been ingested, duplicated, and archived is in a similar precarious state to original camera negative in a film process. One of the jobs of the digital imaging technician is to ensure that digital camera original material is backed up as soon as possible.\n"}
{"id": "51837768", "url": "https://en.wikipedia.org/wiki?curid=51837768", "title": "Positive and Negative Affect Schedule", "text": "Positive and Negative Affect Schedule\n\nThe Positive and Negative Affect Schedule (PANAS) is a self-report questionnaire that consists of two 10-item scales to measure both positive and negative affect. Each item is rated on a 5-point scale of 1 \"(not at all)\" to 5 \"(very much)\". The measure has been used mainly as a research tool in group studies, but can be utilized within clinical and non-clinical populations as well. Shortened, elongated, and children's versions of the PANAS have been developed, taking approximately 5–10 minutes to complete. Clinical and non-clinical studies have found the PANAS to be a reliable and valid instrument in the assessment of positive and negative affect.\n\nThe PANAS was developed in 1988 by researchers from the University of Minnesota and Southern Methodist University. Previous mood measures have shown correlations of variable strength between positive and negative affect, and these same measures have questionable reliability and validity. Watson, Clark, and Tellegen developed the PANAS in an attempt to provide a better, purer measure of each of these dimensions.\n\nThe researchers extracted 60 terms from the factor analyses of Michael Zevon and Tellegen shown to be relatively accurate markers of either positive or negative affect, but not both. They chose terms that met a strong correlation to one corresponding dimension but exhibited a weak correlation to the other. Through multiple rounds of elimination and preliminary analyses with a test population, the researchers arrived at 10 terms for each of the two scales, as follows:\n\nThe PANAS for Children (PANAS-C) was developed in an attempt to differentiate the affective expressions of anxiety and depression in children. The tripartite model on which this measure is based suggests that high levels of negative affect is present in those with anxiety and depression, but high levels of positive affect is not shared between the two. Previous mood scales for children have been shown to reliably capture the former relationship but not the latter; the PANAS-C was created as a tool with better discriminant validity for child assessment. Similar to the development of the original PANAS, the PANAS-C drew from terms of the PANAS-X and eliminated several terms with insufficient correlations between the term and the affective construct after preliminary analyses with a non-clinical sample of children. The final version of the measure consists of 27 items: 12 positive affect terms and 15 negative affect terms. Despite the purpose of its development, however, the measure’s discriminant validity is still wanting.\n\nThe PANAS-SF, comprises 10 items that were determined through the highest factor loadings on the exploratory factor analysis reported by Watson et al. (1988) in his original PANAS. Previous mood scales, such that of Bradburn, had low reliabilities and high correlations between subscales. Watson was able to address these concerns in his study of the original PANAS; however, his participants consisted mostly of student populations. The purpose of the PANAS-SF was not only to provide a shorter and more concise form of the PANAS, but to be able to apply the schedules to older clinical populations. Overall, it was reported that this modified model was consistent with Watson’s.\n\nSeparate from the PANAS-SF, Edmund Thompson created the international PANAS short form (I-PANAS-SF) in order to make a 10 item mood scale that can be implemented effectively on an international level, provide more clarity on the content of the items, reduce ambiguities, address the limitations of the original and the previous short form of the PANAS, and also to provide a shorter, yet dependable and valid scale. To determine the 10 items of the 20 original items, two focus groups were utilized to evaluate all of the original 20 PANAS items. They found that while some items were easily understood by the participant, certains items had different meanings or were too ambiguous. Items that had too much ambiguity were eliminated from the modified form. Researchers found that the I-PANAS-SF had high correlations with the original PANAS. Through multiple tests and studies, they were able to determine that the I-PANAS-SF was on par with the original scale and can be used as a reliable, valid, brief, and efficient instrument on an international scale.\n\nIn 1994, Watson and Clark developed an expanded form of the PANAS, called the PANAS-X, that consists of 60 items that can be completed in 10 minutes or less. The PANAS-X incorporates the original, higher order dimensions specified in the PANAS in addition to the measures of 11 lower order emotional states. These measures are broken down into three main categories: basic negative emotion scales consisting of fear, hostility, guilt, and sadness; basic positive emotion scales consisting of joviality, self-assurance, and attentiveness; and other affective states consisting of shyness, fatigue, serenity, and surprise. Through extensive analyses, all eleven affective states, with the exception of surprise, were shown to be stable, valid measures that assess how an individual’s emotional states fluctuate over time.\n\nReliability refers to whether the scores are reproducible. Unless otherwise specified, the reliability scores and values come from studies done with a United States population sample.\n\nMany forms of the PANAS (PANAS-C, PANAS-X, I-PANAS-SF, and among others) have shown that the PANAS has been widely employed. Recent studies have also shown that the PANAS can be administered in a large general adult population, as well as other populations. However, to date, the PANAS is mostly used as a research tool in group studies, but it has the potential to be utilized in clinical work with individuals. Furthermore, the PANAS has the potential to be used to evaluate mental illnesses, as shown in an experiment conducted by Dyck, Jolly, and Kramer, which demonstrated its effectiveness in distinguishing between depression and anxiety in clinical samples.  \n\nSince the PANAS is a self-report questionnaire, it can be difficult to assess people’s mood accurately, as people can overstate or understate their experience of their moods. In addition, the original PANAS had a limited sample size of college students, which concerns with wide applicability to other samples. Furthermore, some studies claim that the PANAS is too long or that its items are redundant. The PANAS does not encompass higher order mood states.\n\n"}
{"id": "24097", "url": "https://en.wikipedia.org/wiki?curid=24097", "title": "Principle of bivalence", "text": "Principle of bivalence\n\nIn logic, the semantic principle (or law) of bivalence states that every declarative sentence expressing a proposition (of a theory under inspection) has exactly one truth value, either true or false. A logic satisfying this principle is called a two-valued logic or bivalent logic.\n\nIn formal logic, the principle of bivalence becomes a property that a semantics may or may not possess. It is not the same as the law of excluded middle, however, and a semantics may satisfy that law without being bivalent.\n\nThe principle of bivalence is studied in philosophical logic to address the question of which natural-language statements have a well-defined truth value. Sentences which predict events in the future, and sentences which seem open to interpretation, are particularly difficult for philosophers who hold that the principle of bivalence applies to all declarative natural-language statements. Many-valued logics formalize ideas that a realistic characterization of the notion of consequence requires the admissibility of premises which, owing to vagueness, temporal or quantum indeterminacy, or reference-failure, cannot be considered classically bivalent. Reference failures can also be addressed by free logics.\n\nThe principle of bivalence is related to the law of excluded middle though the latter is a syntactic expression of the language of a logic of the form \"P ∨ ¬P\". The difference between the principle and the law is important because there are logics which validate the law but which do not validate the principle. For example, the three-valued Logic of Paradox (LP) validates the law of excluded middle, but not the law of non-contradiction, ¬(P ∧ ¬P), and its intended semantics is not bivalent. In classical two-valued logic both the law of excluded middle and the law of non-contradiction hold.\n\nMany modern logic programming systems replace the law of the excluded middle with the concept of negation as failure. The programmer may wish to add the law of the excluded middle by explicitly asserting it as true; however, it is not assumed \"a priori\".\n\nThe intended semantics of classical logic is bivalent, but this is not true of every semantics for classical logic. In Boolean-valued semantics (for classical propositional logic), the truth values are the elements of an arbitrary Boolean algebra, \"true\" corresponds to the maximal element of the algebra, and \"false\" corresponds to the minimal element. Intermediate elements of the algebra correspond to truth values other than \"true\" and \"false\". The principle of bivalence holds only when the Boolean algebra is taken to be the two-element algebra, which has no intermediate elements.\n\nAssigning Boolean semantics to classical predicate calculus requires that the model be a complete Boolean algebra because the universal quantifier maps to the infimum operation, and the existential quantifier maps to the supremum; this is called a Boolean-valued model. All finite Boolean algebras are complete.\n\nIn order to justify his claim that true and false are the only logical values, Suszko (1977) observes that every structural Tarskian many-valued propositional logic can be provided with a bivalent semantics.\n\nA famous example is the \"contingent sea battle\" case found in Aristotle's work, \"De Interpretatione\", chapter 9:\n\nThe principle of bivalence here asserts:\n\nAristotle to embrace bivalence for such future contingents; Chrysippus, the Stoic logician, did embrace bivalence for this and all other propositions. The controversy continues to be of central importance in both the philosophy of time and the philosophy of logic.\n\nOne of the early motivations for the study of many-valued logics has been precisely this issue. In the early 20th century, the Polish formal logician Jan Łukasiewicz proposed three truth-values: the true, the false and the \"as-yet-undetermined\". This approach was later developed by Arend Heyting and L. E. J. Brouwer; see Łukasiewicz logic.\n\nIssues such as this have also been addressed in various temporal logics, where one can assert that \"\"Eventually\", either there will be a sea battle tomorrow, or there won't be.\" (Which is true if \"tomorrow\" eventually occurs.)\n\nSuch puzzles as the Sorites paradox and the related continuum fallacy have raised doubt as to the applicability of classical logic and the principle of bivalence to concepts that may be vague in their application. Fuzzy logic and some other multi-valued logics have been proposed as alternatives that handle vague concepts better. Truth (and falsity) in fuzzy logic, for example, comes in varying degrees. Consider the following statement in the circumstance of sorting apples on a moving belt:\n\nUpon observation, the apple is an undetermined color between yellow and red, or it is motled both colors. Thus the color falls into neither category \" red \" nor \" yellow \", but these are the only categories available to us as we sort the apples. We might say it is \"50% red\". This could be rephrased: it is 50% true that the apple is red. Therefore, P is 50% true, and 50% false. Now consider:\n\nIn other words, P and not-P. This violates the law of noncontradiction and, by extension, bivalence. However, this is only a partial rejection of these laws because P is only partially true. If P were 100% true, not-P would be 100% false, and there is no contradiction because P and not-P no longer holds.\n\nHowever, the law of the excluded middle is retained, because P and not-P implies P or not-P, since \"or\" is inclusive. The only two cases where P and not-P is false (when P is 100% true or false) are the same cases considered by two-valued logic, and the same rules apply.\n\nExample of a 3-valued logic applied to vague (undetermined) cases: Kleene 1952 (§64, pp. 332–340) offers a 3-valued logic for the cases when algorithms involving partial recursive functions may not return values, but rather end up with circumstances \"u\" = undecided. He lets \"t\" = \"true\", \"f\" = \"false\", \"u\" = \"undecided\" and redesigns all the propositional connectives. He observes that:\n\nThe following are his \"strong tables\":\nFor example, if a determination cannot be made as to whether an apple is red or not-red, then the truth value of the assertion Q: \" This apple is red \" is \" u \". Likewise, the truth value of the assertion R \" This apple is not-red \" is \" u \". Thus the AND of these into the assertion Q AND R, i.e. \" This apple is red AND this apple is not-red \" will, per the tables, yield \" u \". And, the assertion Q OR R, i.e. \" This apple is red OR this apple is not-red \" will likewise yield \" u \".\n\n"}
{"id": "1931801", "url": "https://en.wikipedia.org/wiki?curid=1931801", "title": "Principle of charity", "text": "Principle of charity\n\nIn philosophy and rhetoric, the principle of charity or charitable interpretation requires interpreting a speaker's statements in the most rational way possible and, in the case of any argument, considering its best, strongest possible interpretation. In its narrowest sense, the goal of this methodological principle is to avoid attributing irrationality, logical fallacies, or falsehoods to the others' statements, when a coherent, rational interpretation of the statements is available. According to Simon Blackburn \"it constrains the interpreter to maximize the truth or rationality in the subject's sayings.\"\n\nNeil L. Wilson gave the principle its name in 1958–59. Its main area of application, by his lights, is determining the referent of a proper name:\nHow should we set about discovering the significance which a person attaches to a given name? […] Let us suppose that somebody (whom I am calling \"Charles\") makes just the following five assertions containing the name \"Caesar.\" […]\n(1) Caesar conquered Gaul. (Gc) \n(2) Caesar crossed the Rubicon. (Rc) \n(3) Caesar was murdered on the Ides of March. (Mc) \n[…] And so we act on what might be called the Principle of Charity. We select as designatum that individual which will make the largest possible number of Charles' statements true. […] We might say the designatum is that individual which satisfies more of the asserted matrices containing the word \"Caesar\" than does any other individual. \nWillard Van Orman Quine and Donald Davidson provide other formulations of the principle of charity. Davidson sometimes referred to it as \"the principle of rational accommodation\". He summarized it: \"We make maximum sense of the words and thoughts of others when we interpret in a way that optimises agreement\". The principle may be invoked to make sense of a speaker's utterances when one is unsure of their meaning. In particular, Quine's use of the principle gives it this latter, wide domain.\n\nSince the time of Quine \"et al.\", other philosophers have formulated at least four versions of the principle of charity. These alternatives may conflict with one another, so which principle to use may depend on the goal of the conversation. The four principles are:\n\n\nA related principle is the principle of humanity, which states that we must assume that another speaker's beliefs and desires are connected to each other and to reality in some way, and attribute to him or her \"the propositional attitudes one supposes one would have oneself in those circumstances\" (Daniel Dennett, \"Mid-Term Examination,\" in \"The Intentional Stance\", p. 343).\n\n\n"}
{"id": "1166245", "url": "https://en.wikipedia.org/wiki?curid=1166245", "title": "Principle of distributivity", "text": "Principle of distributivity\n\nThe principle of distributivity states that the algebraic distributive law is valid for classical logic, where both logical conjunction and logical disjunction are distributive over each other so that for any propositions \"A\", \"B\" and \"C\" the equivalences\nand\nhold.\n\nThe principle of distributivity is valid in classical logic, but invalid in quantum logic.\n\nThe article \"Is Logic Empirical?\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is inconsistent with a reasonable interpretation of quantum phenomena.\n"}
{"id": "1788944", "url": "https://en.wikipedia.org/wiki?curid=1788944", "title": "Principle of plenitude", "text": "Principle of plenitude\n\nThe principle of plenitude asserts that the universe contains all possible forms of existence. The historian of ideas Arthur Lovejoy was the first to trace the history of this philosophically important principle explicitly. Lovejoy distinguishes two versions of the principle: a static version, in which the universe displays a constant fullness and diversity, and a temporalized version, in which fullness and diversity gradually increase over time.\n\nLovejoy traces the principle of plenitude to the writings of Plato, finding in the \"Timaeus\" an insistence on \"the necessarily complete translation of all the ideal possibilities into actuality\". By contrast, he takes Aristotle to reject the principle in his \"Metaphysics\", when he writes that \"it is not necessary that everything that is possible should exist in actuality\".\n\nSince Plato, the principle of plenitude has had the following adherents:\n\n\n\n"}
{"id": "39812836", "url": "https://en.wikipedia.org/wiki?curid=39812836", "title": "Process reference models", "text": "Process reference models\n\nA process reference model is a model that has generic functionality and can be used more than once in different models. The creator of a process model benefits from existing process reference models by not needing to reinvent the process model but only reusing it as a starting point in creating a process model for a specific purpose.\n\nDuring the identification of processes ideal for reuse, the designer needs to (1) Get approval (2) Provide Organization Scope Context and (3) Identify Process Standardization Opportunities.\n"}
{"id": "1593030", "url": "https://en.wikipedia.org/wiki?curid=1593030", "title": "Product/process distinction", "text": "Product/process distinction\n\nThe product/process distinction is the distinction between the product information and the process information of a consumer good. Product information is information that pertains to a consumer good, namely to its price, quality, and safety (its proximate attributes). Process information is information that pertains to the means by which the consumer good is made i.e. the working conditions under which it comes into being, as well as the treatment of animals involved in its production chain (its peripheral attributes).\n\nThe product/process distinction is used by the World Trade Organization (WTO) as a way to determine whether or not a complaint filed by an importing nation is valid and warrants trade barriers against the exporting nation. Under WTO rules, an importing nation can lodge a complaint with the WTO that the exporting nation uses methods for obtaining or producing the good in question that the importing nation finds to be immoral or unethical. If the independent World Trade Organization Advisory Board, made up of a panel of international law and trade experts, finds that the importing nation has a legitimate complaint, enforces said ethical standards for domestic production, and isn't trying to merely skirt its free trade obligations, then the Board will rule that trade barriers are justified. Despite what World Trade Organization officials have said, in practice the World Trade Organization finds these complaints illegitimate the vast majority of the time.\n\nFor example, if the European Union (EU) wants to ban imports of cosmetics that were tested on laboratory animals on grounds that such testing is unethical, it can file a complaint with the World Trade Organization and, in theory, the WTO would allow the EU to enact trade barriers provided that the EU bans its own domestic cosmetic producers from testing on laboratory animals. \nIn these cases, however, the World Trade Organization has consistently ruled that such barriers are illegal because only the process is different, while the final product itself is not. Therefore, the WTO has made the product/process distinction an important factor in determining whether trade barriers are justified.\n\nThe World Trade Organization has stated that if nations were able to enact barriers merely because the importing nation's standards differ from their own, control could be lost and barriers could be enacted around the world for frivolous reasons. However, many complain that these rulings go against the stated intentions of the World Trade Organization, and prove that the organization often puts commercial interests above environmental, ethical, and human rights issues.\n"}
{"id": "48000439", "url": "https://en.wikipedia.org/wiki?curid=48000439", "title": "Records Continuum Model", "text": "Records Continuum Model\n\nThe Records Continuum Model (RCM) was created in the 1990s by Monash University academic Frank Upward with input from colleagues Sue McKemmish and Livia Iacovino as a response to evolving discussions about the challenges of managing digital records and archives in the discipline of Archival Science. The RCM was first published in Upward’s 1996 paper \"Structuring the Records Continuum – Part One: Postcustodial principles and properties\". Upward describes the RCM within the broad context of a continuum where activities and interactions transform documents into records, evidence and memory that are used for multiple purposes over time. Upward places the RCM within a post-custodial, postmodern and structuration conceptual framework. Australian academics and practitioners continue to explore, develop and extend the RCM and records continuum theory, along with international collaborators, via the Records Continuum Research Group (RCRG) at Monash University.\n\nThe RCM is an abstract conceptual model that helps to understand and explore recordkeeping activities (as interaction) in relation to multiple contexts over space and time (spacetime). Recordkeeping activities take place from before the records are created by identifying recordkeeping requirements in policies, systems, organizations, processes, laws, social mandates that impact on what is created and how it is managed over spacetime. In a continuum, recordkeeping processes, such as adding metadata, fix documents so that they can be managed as evidence. Those records deemed as having continuing value are retained and managed as an archive. The implication of an RCM approach to records and archives is that systems and processes can be designed and put in place before records are even created. A continuum approach therefore highlights that records are both current and archival at the point of creation.\n\nThe RCM is represented as a series of concentric rings (dimensions of \"Create\", \"Capture\", \"Organize\" and \"Pluralize\") and crossed axes (transactionality, evidentiality, recordkeeping and identity) with each axis labelled with a description of the activity or interaction that occurs at that intersection. \"Create\", \"Capture\", \"Organize\" and \"Pluralize\" represent recordkeeping activities that occur within spacetime. Activities that occur in these dimensions across the axes are explained in the table below:\n\nThe value of the RCM is that it can help to map where on a continuum recordkeeping activities are or can be placed. The RCM can then be used to explore the conceptual and practical assumptions that underpin the practice, in particular the dualisms inherent in the usage and practice of the terms \"records\" and \"archives\". This definition lends itself to a linear reading of the RCM – starting at \"Create\" as the initiating phase and working outwards towards \"Pluralization\" of recorded information. Another linear reading is to consider design first – the role that systems of \"Pluralization\" and \"Organization\" play in designing, planning and implementing recordkeeping and then considering the implications for \"Create\" and \"Capture\". However, these are just two of many ways to interpret the model as the dimensions and axes represent multiple realities that occur within spacetime, any of which can occur simultaneously, concurrently and sequentially in electronic or digital environments, and/or physical spaces.\n\nBy representing multiple realities, the RCM articulates the numerous and diverse perspectives that contribute to records and archives including individual, group, community, organizational, institutional and societal. These contexts reveal the need to take into account various stakeholders and co-contributors in relation to use, access and appraisal of records and archives. Over the lifespan of a record multiple decisions are made by various stakeholders of the records that include, but are not limited to records managers and archivists. Other stakeholders can be identified at various dimensions of interaction, including those involved in providing information (not only the person or organization who produced or captured it), as well as their family and community. Records are therefore not simply physical or digital representations of physical objects held and managed in an archive or repository, but are evidence of multiple perspectives, narratives and contexts that contributed to their formation.\n\nThe RCM is often described as being in contrast or at odds with the lifecycle records model. While the RCM is inclusive of multiple ways of conceptualizing and performing recordkeeping, including a lifecycle approach, there are some significant differences. Firstly, where the lifecycle approach shows clearly demarcated phases in the management of records, a continuum approach conceptualizes elements as continuous with no discernable parts. Secondly, the lifecycle approach identifies clear conceptual and procedural boundaries between active or current records and inactive or historical records, but a continuum approach sees records processes as more integrated across spacetime. In the continuum it is recordkeeping processes that carry records forward through spacetime to enable their use for multiple purposes. What this means is that records are always \"in a state of always becoming...\", and able to contribute new contexts via the recordkeeping processes that occur with them. Archival records are therefore not just historical, but are able to be re-interpreted, re-created, and re-contextualized according to their place and use in spacetime. In this way, archival institutions are nodes in the network of recorded information and its contexts, rather than the end point in a lifecycle stage for records that are managed as \"relics\".\n\nThe RCM is a representation of what is commonly referred to as records continuum theory, as well as Australian continuum thinking and/or approaches. These ideas were evolved as part of an Australian approach to archival management espoused by Ian Maclean, Chief Archivist of the Commonwealth Archives Office in Australia in the 1950s and 1960s. Maclean, whose ideas and practices were the subject of the first RCRG publication in 1994, referred in a 1959 \"American Archivist\" article to a \"continuum of (public) records administration\" from administrative efficiency through recordkeeping to the safe keeping of a \"cultural end-product\". Maclean’s vision challenged the divide between current recordkeeping and archival practice. Fellow contemporary at the Commonwealth Archives Office Peter Scott is also included as a core influence on Australian records continuum theory with his development of the Australian Series System, a registry system that helped identify and document the complex and multiple \"social, functional, provenancial, and documentary relationships\" involved in managing records and recordkeeping processes over spacetime.\n\nFurther influences on the RCRG group include archival professionals and researchers like David Bearman and his work on transactionality and systems thinking, and Terry Cook's ideas about postcustodialism and macroappraisal. Wider influencing ideas include those from philosophers and social theorists Jacques Lacan, Michel Foucault, Jacques Derrida, and Jean-François Lyotard, as well as sociologist Anthony Giddens, with structuration theory being a core component of understanding social interaction over spacetime. Canadian archivist Jay Atherton's critique of the division between records managers and archivists in the 1980s and use of the term \"records continuum\" re-commenced the conversation MacLean began during his career and helped to bring his ideas and this term to Australian records continuum thinking. Atherton's use of the term records continuum has several significant differences in conception, application and heritage when compared to Australian records continuum thinking.\n\nPost-custodiality as an archival concept plays a major role in how the RCM was conceived. This term was born from an identified and urgent need to address the complexities of computer technologies on records creation and management over time and space. Post-custodiality is discussed by Frank Upward and Sue McKemmish in 1994 as part of an exploration of changes in archival discourse commencing in the 1980s by Gerald Ham and expanded on by Terry Cook as part of a \"post-custodial paradigm shift\". Post-custodiality in relation to the RCM is explored by Upward and McKemmish as an entry point into a wider conversation about records and recordkeeping being part of a process in which archival institutions have a part to play beyond that of the archival authority handling, appraisal, describing and arranging physical objects in their custody.\n\nDrawing from the above theoretical foundations, the RCM as a framework acknowledges the central role that recordkeeping activities have on the creation, capture, organization and ongoing management of records over time and throughout spaces such as organizations and institutional archives. Recordkeeping is a practice and a concept clearly defined in the archival and records literature by continuum writers as \"a broad and inclusive concept of integrated recordkeeping and archiving processes for current, regulatory, and historical recordkeeping purposes\". Recordkeeping refers to the activities performed on records that add new contexts such as capturing a record into a system, adding metadata, or selecting it for an archive. In the RCM records are therefore not defined according to their status as objects. Rather, records are understood as being part of a continuum of activity related to known (as well as potentially unknown) contexts. A record (as well as records, collections and archives) are therefore part of larger social, cultural, political, legal and archival processes. It is these contexts that are vital to understanding the role, value and evidential qualities of records in and across spacetime (past, present and potential future).\n\nThe RCM is the most well-known of all the continuum models created, but does not exist in isolation. Several other complementary models have been created by RCM creator Frank Upward, and there are others created by continuum researchers that offer enhanced or alternative ways of understanding the continuum.\n\nThe series of continuum models created by Frank Upward include:\n\nModels created in collaboration:\n\nOther models:\n\n"}
{"id": "172990", "url": "https://en.wikipedia.org/wiki?curid=172990", "title": "Use–mention distinction", "text": "Use–mention distinction\n\nThe use–mention distinction is a foundational concept of analytic philosophy, according to which it is necessary to make a distinction between a word (or phrase) and it, and many philosophical works have been \"vitiated by a failure to distinguish use and mention\". The distinction is disputed by non-analytic philosophers.\n\nThe distinction between use and mention can be illustrated for the word \"cheese\":\n\nThe first sentence is a statement about the substance called \"cheese\"; it uses the word 'cheese' to refer to that substance. The second is a statement about the word 'cheese' as a signifier; it mentions the word without using it to refer to anything other than itself.\n\nIn written language, mentioned words or phrases often appear between quotation marks (as in Chicago' contains three vowels\") or in italics (as in \"When I say \"honey\", I mean the sweet stuff that bees make\"), and style authorities such as \"Strunk and White\" insist that mentioned words or phrases must always be made visually distinct in this manner. Used words or phrases (much more common than mentioned ones) do not bear any typographic distinction. In spoken language, or in absence of the use of stylistic cues such as quotation marks or italics in written language, the audience must identify mentioned words or phrases through semantic and pragmatic cues.\n\nIf quotation marks are used, it is sometimes the practice to distinguish between the quotation marks used for speech and those used for mentioned words, with double quotes in one place and single in the other:\n\nA few authorities recommend against such a distinction, and prefer one style of quotation mark to be used for both purposes.\n\nThe general phenomenon of a term's having different references in different contexts was called \"suppositio\" (substitution) by medieval logicians. It describes how one has to substitute a term in a sentence based on its meaning—that is, based on the term's referent. In general, a term can be used in several ways. For nouns, they are:\n\nThe last sentence contains a mention example.\n\nThe use–mention distinction is especially important in analytic philosophy. Failure to properly distinguish use from mention can produce false, misleading, or meaningless statements or category errors. For example, the following correctly distinguish between use and mention:\n\nThe first sentence, a mention example, is a statement about the word \"copper\" and not the chemical element. Notably, the word is composed of six letters, but not any kind of metal or other tangible thing. The second sentence, a use example, is a statement about the chemical element copper and not the word itself. Notably, the element is composed of 29 electrons and protons and a number of neutrons, but not any letters.\n\nStanisław Leśniewski was perhaps the first to make widespread use of this distinction and the fallacy that arises from overlooking it, seeing it all around in analytic philosophy of the time, for example in Russell and Whitehead's \"Principia Mathematica\". At the logical level, a use–mention mistake occurs when two heterogeneous levels of meaning or context are confused inadvertently.\nDonald Davidson told that in his student years, \"quotation was usually introduced as a somewhat shady device, and the introduction was accompanied by a stern sermon on the sin of confusing the use and mention of expressions\". He presented a class of sentences like\n\nwhich both use the meaning of the quoted words to complete the sentence, and mention them as they are attributed to W. V. Quine, to argue against his teachers' hard distinction. His claim was that quotations could not be analyzed as simple expressions that mention their content by means of naming it or describing its parts, as sentences like the above would lose their exact, twofold meaning.\n\nSelf-referential statements mention themselves or their components, often producing logical paradoxes, such as Quine's paradox. A mathematical analogy of self-referential statements lies at the core of Gödel's incompleteness theorem (diagonal lemma). There are many examples of self-reference and use–mention distinction in the works of Douglas Hofstadter, who makes the distinction thus:\n\nAlthough the standard notation for mentioning a term in philosophy and logic is to put the term in quotation marks, issues arise when the mention is itself of a mention. Notating using italics might require a potentially infinite number of typefaces, while putting quotation marks within quotation marks may lead to ambiguity.\n\nSome analytic philosophers have said the distinction \"may seem rather pedantic\".\n\nIn a 1977 response to analytic philosopher John Searle, Jacques Derrida mentioned the distinction as \"rather laborious and problematical\".\n\n\n\n\n"}
