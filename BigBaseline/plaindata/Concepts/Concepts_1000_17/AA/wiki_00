{"id": "31068", "url": "https://en.wikipedia.org/wiki?curid=31068", "title": "Analogy of the divided line", "text": "Analogy of the divided line\n\nThe Analogy of the Divided Line () is presented by the Greek philosopher Plato in the \"Republic\" (509d–511e). It is written as a dialogue between Glaucon and Socrates, in which the latter further elaborates upon the immediately preceding Analogy of the Sun at the former's request. Socrates asks Glaucon to not only envision this unequally bisected line but to imagine further bisecting each of the two segments. Socrates explains that the four resulting segments represent four separate 'affections' (παθήματα) of the psyche. The lower two sections are said to represent the visible while the higher two are said to represent the intelligible. These affections are described in succession as corresponding to increasing levels of reality and truth from conjecture (εἰκασία) to belief () to thought (διάνοια) and finally to understanding (). Furthermore, this analogy not only elaborates a theory of the psyche but also presents metaphysical and epistemological views.\n\nThis analogy is immediately followed by the Analogy of the Cave at 514a. Socrates returns once more to the elements of the divided line (533d-534a) as he summarizes his dialectic.\n\nIn \"The Republic\" (509d–510a), Plato describes the Divided Line this way:\n\nThus AB represents shadows and reflections of physical things, and BC the physical things themselves. These correspond to two kinds of knowledge, the illusion (εἰκασία \"eikasia\") of our ordinary, everyday experience, and belief (πίστις \"pistis\") about discrete physical objects which cast their shadows. In the \"Timaeus\", the category of illusion includes all the \"opinions of which the minds of ordinary people are full,\" while the natural sciences are included in the category of belief.\n\nAccording to some translations, the segment CE, representing the intelligible world, is divided into the same ratio as AC, giving the subdivisions CD and DE (it can be readily verified that CD must have the same length as BC:\n\nPlato describes CD, the \"lower\" of these, as involving mathematical reasoning (διάνοια \"dianoia\"), where abstract mathematical objects such as geometric lines are discussed. Such objects are outside the physical world (and are not to be confused with the \"drawings\" of those lines, which fall within the physical world BC). However, they are less important to Plato than the subjects of philosophical understanding (νόησις \"noesis\"), the \"higher\" of these two subdivisions (DE):\n\nPlato here is using the familiar relationship between ordinary objects and their shadows or reflections in order to illustrate the relationship between the physical world as a whole and the world of Ideas (Forms) as a whole. The former is made up of a series of passing reflections of the latter, which is eternal, more real and \"true.\" Moreover, the knowledge that we have of the Ideas – when indeed we do have it – is of a higher order than knowledge of the mere physical world. In particular, knowledge of the forms leads to a knowledge of the Idea (Form) of the Good.\n\nThe Allegory of the Divided Line is the cornerstone of Plato's metaphysical framework. This structure, well hidden in the middle of the \"Republic\", a complex, multi-layered dialogue, illustrates the grand picture of Plato's metaphysics, epistemology, and ethics, all in one. It is not enough for the philosopher to understand the Ideas (Forms), he must also understand the relation of Ideas to all four levels of the structure to be able to know anything at all. In the \"Republic\", the philosopher must understand the Idea of Justice to live a just life or to organize and govern a just state.\n\nThe Divided Line also serves as our guide for most past and future metaphysics. The lowest level, which represents \"the world of becoming and passing away\" (\"Republic\", 508d), is the metaphysical model for a Heraclitean philosophy of constant flux and for Protagorean philosophy of appearance and opinion. The second level, a world of fixed physical objects, also became Aristotle's metaphysical model. The third level might be a Pythagorean level of mathematics. The fourth level is Plato's ideal Parmenidean reality, the world of highest level Ideas.\n\nPlato holds a very strict notion of knowledge. For example, he does not accept expertise about a subject, nor direct perception (see \"Theaetetus\"), nor true belief about the physical world (the \"Meno\") as knowledge. It is not enough for the philosopher to understand the Ideas (Forms), he must also understand the relation of Ideas to all four levels of the structure to be able to know anything at all. For this reason, in most of the \"earlier Socratic\" dialogues, Socrates denies knowledge both to himself and others.\n\nFor the first level, \"the world of becoming and passing away,\" Plato expressly denies the possibility of knowledge. Constant change never stays the same, therefore, properties of objects must refer to different Ideas at different times. Note that for knowledge to be possible, which Plato believed, the other three levels must be unchanging. The third and fourth level, mathematics and Ideas, are already eternal and unchanging. However, to ensure that the second level, the objective, physical world, is also unchanging, Plato, in the \"Republic\", Book 4 introduces empirically derived axiomatic restrictions that prohibit both motion and shifting perspectives.\n\n\n"}
{"id": "6395956", "url": "https://en.wikipedia.org/wiki?curid=6395956", "title": "Analytic–synthetic distinction", "text": "Analytic–synthetic distinction\n\nThe analytic–synthetic distinction (also called the analytic–synthetic dichotomy) is a semantic distinction, used primarily in philosophy to distinguish propositions (in particular, statements that are affirmative subject–predicate judgments) into two types: analytic propositions and synthetic propositions. Analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. However, philosophers have used the terms in very different ways. Furthermore, philosophers have debated whether there is a legitimate distinction.\n\nThe philosopher Immanuel Kant uses the terms \"analytic\" and \"synthetic\" to divide propositions into two types. Kant introduces the analytic–synthetic distinction in the Introduction to his \"Critique of Pure Reason\" (1781/1998, A6–7/B10–11). There, he restricts his attention to statements that are affirmative subject-predicate judgments and defines \"analytic proposition\" and \"synthetic proposition\" as follows:\n\nExamples of analytic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nEach of these statements is an affirmative subject-predicate judgment, and, in each, the predicate concept is \"contained\" within the subject concept. The concept \"bachelor\" contains the concept \"unmarried\"; the concept \"unmarried\" is part of the definition of the concept \"bachelor\". Likewise, for \"triangle\" and \"has three sides\", and so on.\n\nExamples of synthetic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nAs with the previous examples classified as analytic propositions, each of these new statements is an affirmative subject–predicate judgment. However, in none of these cases does the subject concept contain the predicate concept. The concept \"bachelor\" does not contain the concept \"alone\"; \"alone\" is not a part of the \"definition\" of \"bachelor\". The same is true for \"creatures with hearts\" and \"have kidneys\"; even if every creature with a heart also has kidneys, the concept \"creature with a heart\" does not contain the concept \"has kidneys\".\n\nIn the Introduction to the \"Critique of Pure Reason\", Kant contrasts his distinction between analytic and synthetic propositions with another distinction, the distinction between \"a priori\" and \"a posteriori\" propositions. He defines these terms as follows:\n\nExamples of \"a priori\" propositions include:\n\nThe justification of these propositions does not depend upon experience: one need not consult experience to determine whether all bachelors are unmarried, nor whether . (Of course, as Kant would grant, experience is required to understand the concepts \"bachelor\", \"unmarried\", \"7\", \"+\" and so forth. However, the \"a priori\" / \"a posteriori\" distinction as employed here by Kant refers not to the \"origins\" of the concepts but to the \"justification\" of the propositions. Once we have the concepts, experience is no longer necessary.)\n\nExamples of \"a posteriori\" propositions include:\n\nBoth of these propositions are \"a posteriori\": any justification of them would require one's experience.\n\nThe analytic/synthetic distinction and the \"a priori\" / \"a posteriori\" distinction together yield four types of propositions:\n\nKant posits the third type as obviously self-contradictory. Ruling it out, he discusses only the remaining three types as components of his epistemological frameworkeach, for brevity's sake, becoming, respectively, \"analytic\", \"synthetic a priori\", and \"empirical\" or \"a posteriori\" propositions. This triad will account for all propositions possible.\n\nPart of Kant's argument in the Introduction to the \"Critique of Pure Reason\" involves arguing that there is no problem figuring out how knowledge of analytic propositions is possible. To know an analytic proposition, Kant argued, one need not consult experience. Instead, one needs merely to take the subject and \"extract from it, in accordance with the principle of contradiction, the required predicate\" (A7/B12). In analytic propositions, the predicate concept is contained in the subject concept. Thus, to know an analytic proposition is true, one need merely examine the concept of the subject. If one finds the predicate contained in the subject, the judgment is true.\n\nThus, for example, one need not consult experience to determine whether \"All bachelors are unmarried\" is true. One need merely examine the subject concept (\"bachelors\") and see if the predicate concept \"unmarried\" is contained in it. And in fact, it is: \"unmarried\" is part of the definition of \"bachelor\" and so is contained within it. Thus the proposition \"All bachelors are unmarried\" can be known to be true without consulting experience.\n\nIt follows from this, Kant argued, first: All analytic propositions are \"a priori\"; there are no \"a posteriori\" analytic propositions. It follows, second: There is no problem understanding how we can know analytic propositions; we can know them because we only need to consult our concepts in order to determine that they are true.\n\nAfter ruling out the possibility of analytic \"a posteriori\" propositions, and explaining how we can obtain knowledge of analytic \"a priori\" propositions, Kant also explains how we can obtain knowledge of synthetic \"a posteriori\" propositions. That leaves only the question of how knowledge of synthetic \"a priori\" propositions is possible. This question is exceedingly important, Kant maintains, because all important metaphysical knowledge is of synthetic \"a priori\" propositions. If it is impossible to determine which synthetic \"a priori\" propositions are true, he argues, then metaphysics as a discipline is impossible. The remainder of the \"Critique of Pure Reason\" is devoted to examining whether and how knowledge of synthetic \"a priori\" propositions is possible.\n\nOver a hundred years later, a group of philosophers took interest in Kant and his distinction between analytic and synthetic propositions: the logical positivists.\n\nPart of Kant's examination of the possibility of synthetic \"a priori\" knowledge involved the examination of mathematical propositions, such as\n\nKant maintained that mathematical propositions such as these are synthetic \"a priori\" propositions, and that we know them. That they are synthetic, he thought, is obvious: the concept \"equal to 12\" is not contained within the concept \"7 + 5\"; and the concept \"straight line\" is not contained within the concept \"the shortest distance between two points\". From this, Kant concluded that we have knowledge of synthetic \"a priori\" propositions.\n\nGottlob Frege's notion of analyticity included a number of logical properties and relations beyond containment: symmetry, transitivity, antonymy, or negation and so on. He had a strong emphasis on formality, in particular formal definition, and also emphasized the idea of substitution of synonymous terms. \"All bachelors are unmarried\" can be expanded out with the formal definition of bachelor as \"unmarried man\" to form \"All unmarried men are unmarried\", which is recognizable as tautologous and therefore analytic from its logical form: any statement of the form \"All \"X\" that are (\"F\" and \"G\") are \"F\"\". Using this particular expanded idea of analyticity, Frege concluded that Kant's examples of arithmetical truths are analytical \"a priori\" truths and \"not\" synthetic \"a priori\" truths.\n\nThe logical positivists agreed with Kant that we have knowledge of mathematical truths, and further that mathematical propositions are \"a priori\". However, they did not believe that any complex metaphysics, such as the type Kant supplied, are necessary to explain our knowledge of mathematical truths. Instead, the logical positivists maintained that our knowledge of judgments like \"all bachelors are unmarried\" and our knowledge of mathematics (and logic) are in the basic sense the same: all proceeded from our knowledge of the meanings of terms or the conventions of language.\n\nThus the logical positivists drew a new distinction, and, inheriting the terms from Kant, named it the \"analytic/synthetic distinction\". They provided many different definitions, such as the following:\n\nSynthetic propositions were then defined as:\n\nThese definitions applied to all propositions, regardless of whether they were of subject–predicate form. Thus, under these definitions, the proposition \"It is raining or it is not raining\" was classified as analytic, while for Kant it was analytic by virtue of its logical form. And the proposition \"\" was classified as analytic, while under Kant's definitions it was synthetic.\n\nTwo-dimensionalism is an approach to semantics in analytic philosophy. It is a theory of how to determine the sense and reference of a word and the truth-value of a sentence. It is intended to resolve a puzzle that has plagued philosophy for some time, namely: How is it possible to discover empirically that a necessary truth is true? Two-dimensionalism provides an analysis of the semantics of words and sentences that makes sense of this possibility. The theory was first developed by Robert Stalnaker, but it has been advocated by numerous philosophers since, including David Chalmers and Berit Brogaard.\n\nAny given sentence, for example, the words,\n\nis taken to express two distinct propositions, often referred to as a \"primary intension\" and a \"secondary intension\", which together compose its meaning.\n\nThe primary intension of a word or sentence is its sense, i.e., is the idea or method by which we find its referent. The primary intension of \"water\" might be a description, such as \"watery stuff\". The thing picked out by the primary intension of \"water\" could have been otherwise. For example, on some other world where the inhabitants take \"water\" to mean \"watery stuff\", but, where the chemical make-up of watery stuff is not HO, it is not the case that water is HO for that world.\n\nThe \"secondary intension\" of \"water\" is whatever thing \"water\" happens to pick out in \"this\" world, whatever that world happens to be. So if we assign \"water\" the primary intension \"watery stuff\" then the secondary intension of \"water\" is HO, since HO is \"watery stuff\" in this world. The secondary intension of \"water\" in our world is HO, which is HO in every world because unlike \"watery stuff\" it is impossible for HO to be other than HO. When considered according to its secondary intension, \"Water is HO\" is true in every world.\n\nIf two-dimensionalism is workable it solves some very important problems in the philosophy of language. Saul Kripke has argued that \"Water is HO\" is an example of the \"necessary a posteriori\", since we had to discover that water was HO, but given that it is true, it cannot be false. It would be absurd to claim that something that is water is not HO, for these are known to be \"identical\".\n\nRudolf Carnap was a strong proponent of the distinction between what he called \"internal questions\", questions entertained within a \"framework\" (like a mathematical theory), and \"external questions\", questions posed outside any framework – posed before the adoption of any framework. The \"internal\" questions could be of two types: \"logical\" (or analytic, or logically true) and \"factual\" (empirical, that is, matters of observation interpreted using terms from a framework). The \"external\" questions were also of two types: those that were confused pseudo-questions (\"one disguised in the form of a theoretical question\") and those that could be re-interpreted as practical, pragmatic questions about whether a framework under consideration was \"more or less expedient, fruitful, conducive to the aim for which the language is intended\". The adjective \"synthetic\" was not used by Carnap in his 1950 work \"Empiricism, Semantics, and Ontology\". Carnap did define a \"synthetic truth\" in his work \"Meaning and Necessity\": a sentence that is true, but not simply because \"the semantical rules of the system suffice for establishing its truth\".\n\nThe notion of a synthetic truth is of something that is true both because of what it means and because of the way the world is, whereas analytic truths are true in virtue of meaning alone. Thus, what Carnap calls internal \"factual\" statements (as opposed to internal \"logical\" statements) could be taken as being also synthetic truths because they require \"observations\", but some external statements also could be \"synthetic\" statements and Carnap would be doubtful about their status. The analytic–synthetic argument therefore is not identical with the internal–external distinction.\n\nIn 1951, Willard Van Orman Quine published the essay \"Two Dogmas of Empiricism\" in which he argued that the analytic–synthetic distinction is untenable. The argument at bottom is that there are no \"analytic\" truths, but all truths involve an empirical aspect. In the first paragraph, Quine takes the distinction to be the following:\n\nQuine's position denying the analytic-synthetic distinction is summarized as follows:\nTo summarize Quine's argument, the notion of an analytic proposition requires a notion of synonymy, but establishing synonymy inevitably leads to matters of fact – synthetic propositions. Thus, there is no non-circular (and so no tenable) way to ground the notion of analytic propositions.\n\nWhile Quine's rejection of the analytic–synthetic distinction is widely known, the precise argument for the rejection and its status is highly debated in contemporary philosophy. However, some (for example, Boghossian) argue that Quine's rejection of the distinction is still widely accepted among philosophers, even if for poor reasons.\n\nPaul Grice and P. F. Strawson criticized \"Two Dogmas\" in their 1956 article \"In Defense of a Dogma\". Among other things, they argue that Quine's skepticism about synonyms leads to a skepticism about meaning. If statements can have meanings, then it would make sense to ask \"What does it mean?\". If it makes sense to ask \"What does it mean?\", then synonymy can be defined as follows: Two sentences are synonymous if and only if the true answer of the question \"What does it mean?\" asked of one of them is the true answer to the same question asked of the other. They also draw the conclusion that discussion about correct or incorrect translations would be impossible given Quine's argument. Four years after Grice and Strawson published their paper, Quine's book \"Word and Object\" was released. In the book Quine presented his theory of indeterminacy of translation.\n\nIn \"Speech Acts\", John Searle argues that from the difficulties encountered in trying to explicate analyticity by appeal to specific criteria, it does not follow that the notion itself is void. Considering the way which we would test any proposed list of criteria, which is by comparing their extension to the set of analytic statements, it would follow that any explication of what analyticity means presupposes that we already have at our disposal a working notion of analyticity.\n\nIn \"'Two Dogmas' Revisited\", Hilary Putnam argues that Quine is attacking two different notions:\nAnalytic truth defined as a true statement derivable from a tautology by putting synonyms for synonyms is near Kant's account of analytic truth as a truth whose negation is a contradiction. Analytic truth defined as a truth confirmed no matter what, however, is closer to one of the traditional accounts of \"a priori\". While the first four sections of Quine's paper concern analyticity, the last two concern a priority. Putnam considers the argument in the two last sections as independent of the first four, and at the same time as Putnam criticizes Quine, he also emphasizes his historical importance as the first top rank philosopher to both reject the notion of a priority and sketch a methodology without it.\n\nJerrold Katz, a one-time associate of Noam Chomsky, countered the arguments of \"Two Dogmas\" directly by trying to define analyticity non-circularly on the syntactical features of sentences.\n\nIn \"Philosophical Analysis in the Twentieth Century, Volume 1 : The Dawn of Analysis\", Scott Soames has pointed out that Quine's circularity argument needs two of the logical positivists' central theses to be effective:\n\nIt is only when these two theses are accepted that Quine's argument holds. It is not a problem that the notion of necessity is presupposed by the notion of analyticity if necessity can be explained without analyticity. According to Soames, both theses were accepted by most philosophers when Quine published \"Two Dogmas\". Today, however, Soames holds both statements to be antiquated. He says: \"Very few philosophers today would accept either [of these assertions], both of which now seem decidedly antique.\"\n\nPhilosopher Leonard Peikoff, in his essay \"The Analytic-Synthetic Dichotomy\", expands upon Ayn Rand's analysis. He posits that:\n\nThe theory of the analytic-synthetic dichotomy presents men with the following choice: If your statement is proved, it says nothing about that which exists; if it is about existents, it cannot be proved. If it is demonstrated by logical argument, it represents a subjective convention; if it asserts a fact, logic cannot establish it. If you validate it by an appeal to the meanings of your \"concepts\", then it is cut off from reality; if you validate it by an appeal to your \"percepts\", then you cannot be certain of it.\n\nTo Peikoff, the critical question is: What is included in the meaning of a concept? He rejects the idea that some of the characteristics of a concept's referents are excluded from the concept. Applying Rand's theory that a concept is a \"mental integration\" of similar existents, treated as \"units\", he argues that concepts stand for and mean the actual existents, including all their characteristics, not just those used to pick out the referents or define the concept. He states,\n\nSince a concept is an integration of units, it has no content or meaning apart from its units. The meaning of a concept consists of the units — the existents — which it integrates, including all the characteristics of these units... The fact that certain characteristics are, at a given time, unknown to man, does not indicate that these characteristics are excluded from the entity — or from the concept.\n\nFurthermore, he argues that there is no valid distinction between \"necessary\" and \"contingent\" facts, and that all truths are learned and validated by the same process: the application of logic to perceptual data. Associated with the analytic-synthetic dichotomy are a cluster of other divisions that Objectivism also regards as false and artificial, such as logical truth vs. factual truth, logically possible vs. empirically possible, and a priori vs. the a posteriori.\n\n\n\n"}
{"id": "2235265", "url": "https://en.wikipedia.org/wiki?curid=2235265", "title": "Anna Karenina principle", "text": "Anna Karenina principle\n\nThe Anna Karenina principle states that a deficiency in any one of a number of factors dooms an endeavor to failure. Consequently, a successful endeavor (subject to this principle) is one where every possible deficiency has been avoided.\n\nThe name of the principle derives from Leo Tolstoy's book \"Anna Karenina\", which begins:\n\nAll happy families are alike; each unhappy family is unhappy in its own way.In other words: in order to be happy, a family must be successful on \"each and every one\" of \"a\" \"range\" of criteria e.g.: sexual attraction, money issues, parenting, religion, in-laws. Failure on only \"one\" of these counts leads to \"un\"happiness. Thus there are more ways for a family to be unhappy than happy.\n\nIn statistics, the term \"Anna Karenina principle\" is used to describe significance tests: there are any number of ways in which a dataset may violate the null hypothesis and only one in which all the assumptions are satisfied.\n\nThe Anna Karenina principle was popularized by Jared Diamond in his book \"Guns, Germs and Steel\". Diamond uses this principle to illustrate why so few wild animals have been successfully domesticated throughout history, as a deficiency in any one of a great number of factors can render a species undomesticable. Therefore, all successfully domesticated species are not so because of a particular positive trait, but because of a lack of any number of possible negative traits. In chapter 9, six groups of reasons for failed domestication of animals are defined:\n\n\nMoore describes applications of the \"Anna Karenina principle\" in ecology:\n\nSuccessful ecological risk assessments are all alike; every unsuccessful ecological risk assessment fails in its own way. Tolstoy posited a similar analogy in his novel Anna Karenina : \"Happy families are all alike; every unhappy family is unhappy in its own way.\" By that, Tolstoy meant that for a marriage to be happy, it had to succeed in several key aspects. Failure on even one of these aspects, and the marriage is doomed . . . the Anna Karenina principle also applies to ecological risk assessments involving multiple stressors.\n\nMuch earlier, \"Aristotle\" states the same principle in the \"Nicomachean Ethics\" (Book 2):\n\nAgain, it is possible to fail in many ways (for evil belongs to the class of the unlimited, as the Pythagoreans conjectured, and good to that of the limited), while to succeed is possible only in one way (for which reason also one is easy and the other difficult – to miss the mark easy, to hit it difficult); for these reasons also, then, excess and defect are characteristic of vice, and the mean of virtue; For men are good in but one way, but bad in many.\n\nMany experiments and observations of groups of humans, animals, trees, grassy plants, stockmarket prices, and changes in the banking sector proved the modified Anna Karenina principle.\n\nBy studying the dynamics of correlation and variance in many systems facing external, or environmental, factors, we can typically, even before obvious symptoms of crisis appear, predict when one might occur, as correlation between individuals increases, and, at the same time, variance (and volatility) goes up... All well-adapted systems are alike, all non-adapted systems experience maladaptation in their own way... But in the chaos of maladaptation, there is an order. It seems, paradoxically, that as systems become more different they actually become more correlated within limits.\n\nThis effect is proved for many systems: from the adaptation of healthy people to a change in climate conditions to the analysis of fatal outcomes in oncological and cardiological clinics. The same effect is found in the stock market. The applicability of these two statistical indicators of stress, simultaneous increase of variance and correlations, for diagnosis of social stress in large groups was examined in the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years.\n\nVladimir Arnold in his book \"Catastrophe Theory\" describes \"The Principle of Fragility of Good Things\" which in a sense supplements the Principle of Anna Karenina: good systems must meet simultaneously a number of requirements; therefore, they are more fragile:\n\n... for systems belonging to the singular part of the stability boundary a small change of the parameters is more likely to send the system into the unstable region than into the stable region. This is a manifestation of a general principle stating that all good things (e.g. stability) are more fragile than bad things. It seems that in good situations a number of requirements must hold simultaneously, while to call a situation bad even one failure suffices. \n"}
{"id": "39105", "url": "https://en.wikipedia.org/wiki?curid=39105", "title": "Boehm system", "text": "Boehm system\n\nThe Boehm system is a system of keywork for the flute, created by inventor and flautist Theobald Boehm between 1831 and 1847. \n\nPrior to the development of the Boehm system, flutes were most commonly made of wood, with an inverse conical bore, eight keys, and tone holes (the openings where the fingers are placed to produce specific notes) that were small in size, and thus easily covered by the fingertips. Boehm's work was inspired by an 1831 concert in London, given by soloist Charles Nicholson who, with his father in the 1820s, had introduced a flute constructed with larger tone holes than were used in previous designs. This large-holed instrument could produce greater volume of sound than other flutes, and Boehm set out to produce his own large-holed design.\n\nIn addition to large holes, Boehm provided his flute with \"full venting\", meaning that all keys were normally open (previously, several keys were normally closed, and opened only when the key was operated). Boehm also wanted to locate tone holes at acoustically optimal points on the body of the instrument, rather than locations conveniently covered by the player's fingers. To achieve these goals, Boehm adapted a system of axle-mounted keys with a series of \"open rings\" (called \"brille\", German for \"eyeglasses\", as they resembled the type of eyeglass frames common during the 19th century) that were fitted around other tone holes, such that the closure of one tone hole by a finger would also close a key placed over a second hole.\n\nIn 1832 Boehm introduced a new conical-bore flute, which achieved a fair degree of success. Boehm, however, continued to look for ways to improve the instrument. Finding that an increased volume of air produced a stronger and clearer tone, he replaced the conical bore with a cylindrical bore, finding that a parabolic contraction of the bore near the embouchure hole improved the instrument's low register. He also found that optimal tone was produced when the tone holes were too large to be covered by the fingertips, and he developed a system of finger plates to cover the holes. These new flutes were at first made of silver, although Boehm later produced wooden versions. \n\nThe cylindrical Boehm flute was introduced in 1847, with the instrument gradually being adopted almost universally by professional and amateur players in Europe and around the world during the second half of the 19th century. The instrument was adopted for the performance of orchestral and chamber music, opera and theater, wind ensembles (e.g., military and civic bands), and most other music which might be loosely described as relating to \"Western classical music\" (including, for example, jazz). Many further refinements have been made, and countless design variations are common among flutes today (the \"offset G\" key, addition of the low B foot, etc.) The concepts of the Boehm system have been applied across the range of flutes available, including piccolos, alto flutes, bass flutes, and so on, as well as other wind instruments. The material of the instrument may vary (many piccolos are made of wood, some very large flutes are wooden or even made of PVC).\n\nThe flute is perhaps the oldest musical instrument, other than the human voice itself. There are very many flutes, both traversely blown and end-blown \"fipple\" flutes, currently produced which are not built on the Boehm model.\n\nThe fingering system for the saxophone closely resembles the Boehm system. A key system inspired by Boehm's for the clarinet family is also known as the \"Boehm system\", although it was developed by Hyacinthe Klosé and not Boehm himself. The Boehm system was also adapted for a small number of flageolets. Boehm did work on a system for the bassoon, and Boehm-inspired oboes have been made, but non-Boehm systems remain predominant for these instruments. The Albert system is another key system for the clarinet.\n\n\n"}
{"id": "42415226", "url": "https://en.wikipedia.org/wiki?curid=42415226", "title": "Conceptual combination", "text": "Conceptual combination\n\nConceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as \"complex concepts.\" Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.\n\nConceptual combination is an important concept in the fields of cognitive psychology and cognitive science.\n\nThe mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.\n\nCognitive models attempt to functionally outline the mental computation involved in conceptual combination. \n\nConstraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. \"Diagnosticity\" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. \"Plausibility\" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. \"Informativeness\" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).\n\nThe spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.\n\nSpreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.\n\nWhen one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as \"priming.\"\n\nSpreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.\n\nThe features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept \"white jacket,\" if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of \"good for winter camouflage,\" despite the fact that this property is not closely attached to the component concepts \"white\" nor \"jacket.\" This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.\n\nThis model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.\n\nThe neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.\n\nOf particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.\n\nFurther support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.\n\nAs language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.\n\nA concept that can be expressed using a single word is called a \"lexical concept.\" A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.\n\nTwo lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as \"burnt toast,\" \"eat roughly,\" and \"readily loved.\" Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as \"sound wave,\" \"video game,\" and \"sleeping pill.\"\n\nIn addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is \"dual-process theory.\" Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. \"Relational interpretation\" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase \"snake mouse\" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. \"Property interpretation\" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase \"snake mouse\" might be interpreted as a mouse with poisonous fangs or an elongated body.\n\nThe second principal theory is known as the \"Competition in Relations among Nominals\" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, \"chocolate cat\" is usually interpreted as \"a cat made of chocolate\" rather than \"a chocolate-eating cat\" simply because the \"made of\" modifier is heavily conditioned to be associated with \"chocolate.\"\n\nExplanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, \"John spread butter on a bagel.\" In this sentence, the lexical concepts \"spread,\" \"butter,\" and \"bagel\" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, \"John baked a computer.\" Because \"baked\" and \"computer\" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.\n\nHowever, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence \"John saw an elephant cloud.\" \"Elephant\" and \"cloud\" do not shared a close association, but it takes little effort to comprehend that the term \"elephant cloud\" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.\n\nAlthough many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.\n\nWhen lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed \"sense generation.\" This includes all processes that would normally occur excepting those dependent on a conversation partner. The \"generation hypothesis\" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.\n\nThe \"anaphor resolution hypothesis\" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed \"anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.\n\nThe \"dual-process hypothesis\" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.\n\nCreativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.\n\nThe psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.\n"}
{"id": "58267", "url": "https://en.wikipedia.org/wiki?curid=58267", "title": "Conceptual schema", "text": "Conceptual schema\n\nA 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.\n\nA conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization (\"entity classes\"), about which it is inclined to collect information, and characteristics of (\"attributes\") and associations between pairs of those things of significance (\"relationships\").\n\nBecause a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of \"external schema\" that each represent one person's view of the world around him or her. These are consolidated into a single \"conceptual schema\" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.\n\nThe model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a \"sub-type\" entity class is also an instance of the entity class's \"super-type\". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.\n\nSuper-type/sub-type relationships may be \"exclusive\" or not. A methodology may require that each instance of a super-type may \"only\" be an instance of \"one\" sub-type. Similarly, a super-type/sub-type relationship may be \"exhaustive\" or not. It is exhaustive if the methodology requires that each instance of a super-type \"must be\" an instance of a sub-type. A sub-type named other is often necessary.\n\n\nA data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.\n\n\n\n"}
{"id": "1745389", "url": "https://en.wikipedia.org/wiki?curid=1745389", "title": "Cooperative principle", "text": "Cooperative principle\n\nIn social science generally and linguistics specifically, the cooperative principle describes how effective communication in conversation is achieved in common social situations, that is, how listeners and speakers must act cooperatively and mutually accept one another to be understood in a particular way. As phrased by Paul Grice, who introduced it, \"Make your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged.\" Though phrased as a prescriptive command, the principle is intended as a description of how people normally behave in conversation. Jeffries and McIntyre describe Grice's maxims as \"encapsulating the assumptions that we prototypically hold when we engage in conversation\".\n\nThe cooperative principle can be divided into four maxims, called the Gricean maxims, describing specific rational principles observed by people who obey the cooperative principle; these principles enable effective communication. Grice proposed four conversational maxims that arise from the pragmatics of natural language. Applying the Gricean maxims is a way to explain the link between utterances and what is understood from them.\n\n\n\n\nWith respect to this maxim, Grice writes, \"Though the maxim itself is terse, its formulation conceals a number of problems that exercise me a good deal: questions about what different kinds and focuses of relevance there may be, how these shift in the course of a talk exchange, how to allow for the fact that subjects of conversations are legitimately changed, and so on. I find the treatment of such questions exceedingly difficult, and I hope to revert to them in later work.\"\n\n\n\nThese maxims may also be understood as describing the assumptions listeners normally make about the way speakers will talk, rather than prescriptions for how one ought to talk. Philosopher Kent Bach writes:\n\n...[W]e need first to get clear on the character of Grice's maxims. They are not sociological generalizations about speech, nor they are moral prescriptions or proscriptions on what to say or communicate. Although Grice presented them in the form of guidelines for how to communicate successfully, I think they are better construed as presumptions about utterances, presumptions that we as listeners rely on and as speakers exploit (Bach 2005).\n\nGricean maxims generate implicatures. If the overt, surface meaning of a sentence does not seem to be consistent with the Gricean maxims, and yet the circumstances lead us to think that the speaker is nonetheless obeying the cooperative principle, we tend to look for other meanings that could be implied by the sentence.\n\nGrice did not, however, assume that all people should constantly follow these maxims. Instead, he found it interesting when these were not respected, namely either \"flouted\" (with the listener being expected to be able to understand the message) or \"violated\" (with the listener being expected to not note this). Flouting would imply some other, hidden meaning. The importance was in what was \"not\" said. For example, answering \"It's raining\" to someone who has suggested playing a game of tennis only disrespects the maxim of relation on the surface; the reasoning behind this \"fragment\" sentence is normally clear to the interlocutor (the maxim is just \"flouted\").\n\nIt is possible to flout a maxim and thereby convey a different meaning than what is literally spoken. Many times in conversation, this flouting is manipulated by a speaker to produce a negative pragmatic effect, as with sarcasm or irony. One can flout the maxim of quality to tell a clumsy friend who has just taken a bad fall that her gracefulness is impressive and obviously intend to mean the complete opposite. Likewise, flouting the maxim of quantity may result in ironic understatement, the maxim of relevance in blame by irrelevant praise, and the maxim of manner in ironic ambiguity. The Gricean maxims are therefore often purposefully flouted by comedians and writers, who may hide the complete truth and manipulate their words for the effect of the story and the sake of the reader's experience.\n\nGrice's theory is often disputed by arguing that cooperative conversation, like most social behaviour, is culturally determined, and therefore the Gricean maxims and the cooperative principle cannot be universally applied due to intercultural differences. Keenan claims that the Malagasy, for example, follow a completely opposite cooperative principle in order to achieve conversational cooperation. In their culture, speakers are reluctant to share information and flout the maxim of quantity by evading direct questions and replying on incomplete answers because of the risk of losing face by committing oneself to the truth of the information, as well as the fact that having information is a form of prestige. However, Harnish points out that Grice only claims his maxims hold in conversations where his cooperative principle is in effect. The Malagasy speakers choose not to be cooperative, valuing the prestige of information ownership more highly. \n\nAnother criticism is that the Gricean maxims can easily be misinterpreted to be a guideline for etiquette, instructing speakers on how to be moral, polite conversationalists. However, the Gricean maxims, despite their wording, are only meant to describe the commonly accepted traits of successful cooperative communication. Geoffrey Leech introduced the politeness maxims: tact, generosity, approbation, modesty, agreement, and sympathy.\n\n\n\n"}
{"id": "39228396", "url": "https://en.wikipedia.org/wiki?curid=39228396", "title": "Equivalence principle (geometric)", "text": "Equivalence principle (geometric)\n\nThe equivalence principle is one of the corner-stones of gravitation theory. Different formulations of the equivalence principle are labeled \"weakest\", \"weak\", \"middle-strong\" and \"strong.\" All of these formulations are based on the empirical equality of inertial mass, gravitational active and passive charges.\n\nThe \"weakest\" equivalence principle is restricted to the motion law of a probe point mass in a uniform gravitational field. Its localization is the \"weak\" equivalence principle that states the existence of a desired local inertial frame at a given world point. This is the case of equations depending on a gravitational field and its first order derivatives, e. g., the equations of mechanics of probe point masses, and the equations of electromagnetic and Dirac fermion fields. The \"middle-strong\" equivalence principle is concerned with any matter, except a gravitational field, while the \"strong\" one is applied to all physical laws.\n\nThe above-mentioned variants of the equivalence principle aim to guarantee the transition of General Relativity to Special Relativity in a certain reference frame. However, only the particular \"weakest\" and \"weak\" equivalence principles are true. \nTo overcome this difficulty, the equivalence principle can be formulated in geometric terms as follows.\n\nIn the spirit of Felix Klein's Erlanger program, Special Relativity can be characterized as the Klein geometry of Lorentz group invariants. Then the geometric equivalence principle is formulated to require the existence of Lorentz invariants on a world manifold formula_1. This requirement holds if the tangent bundle formula_2 of formula_1 admits an atlas with Lorentz transition functions, i.e., a structure group of the associated frame bundle formula_4 of linear tangent frames in formula_2 is reduced to the Lorentz group formula_6. By virtue of the well known theorem on structure group reduction, this reduction takes place if and only if the quotient bundle formula_7 possesses a global section, which is a pseudo-Riemannian metric on formula_1.\n\nThus the geometric equivalence principle provides the necessary and sufficient conditions of the existence of a pseudo-Riemannian metric, i.e., a gravitational field on a world manifold.\n\nBased on the geometric equivalence principle, gravitation theory is formulated as gauge theory where a gravitational field is described as a classical Higgs field responsible for spontaneous breakdown of space-time symmetries.\n\n\n"}
{"id": "14162696", "url": "https://en.wikipedia.org/wiki?curid=14162696", "title": "Fluid Concepts and Creative Analogies", "text": "Fluid Concepts and Creative Analogies\n\nFluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought is a 1995 book by Douglas Hofstadter and other members of the Fluid Analogies Research Group exploring the mechanisms of intelligence through computer modeling. It contends that the notions of analogy and fluidity are fundamental to explain how the human mind solves problems and to create computer programs that show intelligent behavior. It analyzes several computer programs that members of the group have created over the years to solve problems that require intelligence.\n\nIt was the first book ever sold by Amazon.com.\n\nThe book is a collection of revised articles that appeared in precedence, each preceded by an introduction by Hofstadter.\nThey describe the scientific work by him and his collaborators in the 1980s and 1990s.\nThe project started in the late 1970s at Indiana University.\nIn 1983 he took a sabbatical year at MIT, working in Marvin Minsky's Artificial Intelligence Lab.\nThere he met and collaborated with Melanie Mitchell, who then became his doctoral student.\nSubsequently, Hofstadter moved to the University of Michigan, where the FARG (Fluid Analogies Research Group) was founded.\nEventually he returned to Indiana University in 1988, continuing the FARG research there.\nThe book was written during a sabbatical year at the Istituto per la Ricerca Scientifica e Tecnologica in Trento, Italy.\n\nUpon publication, Jon Udell, a BYTE senior technical editor-at-large said:\nFifteen years ago, \"Gödel, Escher, Bach: An Eternal Golden Braid\" exploded on the literary scene, earning its author a Pulitzer prize and a monthly column in \"Scientific American\". Douglas Hofstadter's exuberant synthesis of math, music, and art, and his inspired thought experiments with \"tangled hierarchy,\" recursion, pattern recognition, figure/ground reversal, and self-reference, delighted armchair philosophers and AI theorists. But in the end, many people believed that these intellectual games yielded no useful model of cognition on which to base future AI research. Now \"Fluid Concepts and Creative Analogies\" presents that model, along with the computer programs Hofstadter and his associates have designed to test it. These programs work in stripped-down yet surprisingly rich microdomains.\n\nOn April 3, 1995, \"Fluid Concepts and Creative Analogies\" became the first book ordered online by an Amazon.com customer.\n\n\nThe first AI project by Hofstadter stemmed from his teenage fascination with number sequences.\nWhen he was 17, he studied the way that triangular and square numbers interleave, and eventually found a recursive relation describing it.\nIn his first course on AI, he set to the students and to himself the task of writing a program that could extrapolate the rule by which a numeric sequence is generated.\nHe discusses breadth-first and depth-first techniques, but eventually concludes that the results represent expert systems that incarnate a lot of technical knowledge but don't shine much light on the mental processes that humans use to solve such puzzles.\n\nInstead he devised a simplified version of the problem, called SeekWhence, where sequences are based on very simple basic rules not requiring advanced mathematical knowledge.\nHe argues that pattern recognition, analogy, and fluid working hypotheses are fundamental to understand how humans tackle such problems.\n\nJumbo is a program to solve jumbles, word puzzles consisting in five or six scrambled letters that need to be anagrammed to form an English word.\nThe resulting word does not need to be a real one but just to a plausible, that is, to consists of a sequence of letters that is normal in English.\n\nThe constituent elements of Jumbo are the following:\nA \"temperature\" is associated to the present state of the cytoplasm; it determines how probable it is that a destructive codelet is executed.\nThere is a \"freezing\" temperature at which no destruction can occur anymore: a solution has been found.\n\nNumbo is a program by Daniel Defays that tries to solve numerical problems similar to those used in the French game \"Le compte est bon\". The game consists in combining some numbers called \"bricks\", using the operations of multiplication, addition, and subtraction, to obtain a given result.\n\nThe program is modeled on Jumbo and Copycat and uses a permanent network of known mathematical facts, a working memory in the form of a cytoplasm, and a coderack containing codelets to produce free associations of bricks in order to arrive at the result.\n\nThe chapter subtitle \"A Critique of Artificial-intelligence Methodology\" indicates that this is a polemical article, in which David Chalmers, Robert French, and Hofstadter criticize most of the research going on at that time (the early '80s) as exaggerating results and missing the central features of human intelligence.\n\nSome of these AI projects, like the structure mapping engine (SME), claimed to model high faculties of the human mind and to be able to understand literary analogies and to rediscover important scientific breakthroughs.\nIn the introduction, Hofstadter warns about the Eliza effect that leads people to attribute understanding to a computer program that only uses a few stock phrases.\nThe authors claim that the input data for such impressive results are already heavily structured in the direction of the intended discovery and only a simple matching task is left to the computer.\n\nTheir main claim is that it is impossible to model high-level cognition without at the same time modeling low-level perception.\nWhile cognition is necessarily based on perception, they argue that it in turn influences perception itself.\nTherefore, a sound AI project should try to model the two together.\nIn a slogan repeated several times throughout the book: \"cognition is recognition\".\n\nSince human perception is too complex to be modelled by available technology, they favor the restriction of AI projects to limited domains like the one used for the Copycat project.\n\nThis chapter presents, as stated in the full title, \"A Model of Mental Fluidity and Analogy-making\".\nIt is a description of the architecture of the Copycat program, developed by Hofstadter and Melanie Mitchell.\nThe field of application of the program is a domain of short alphabetic sequences.\nA typical puzzle is: \"If abc were changed to abd, how would you change ijk in the same way?\".\nThe program tries to find an answer using a strategy supposedly similar to the way the human mind tackles the question.\n\nCopycat has three major components:\nThe resulting software displays emergent properties.\nIt works according to a \"parallel terraced scan\" that runs several possible processes at the same time.\nIt shows mental fluidity in that concepts may \"slip\" into similar ones.\nIt emulates human behavior in tending to find the most obvious solutions most of the time but being more satisfied (as witnessed by low temperature) by more clever and deep answers that it finds more rarely.\n\nThis chapter compares Copycat with other recent (at the time) work in artificial intelligence.\nSpecifically, it matches it with the claimed results from the structure mapping engine SME and the Analogical Constraint Mapping Engine (ACME).\nThe authors' judgment is that those programs suffer from two defects: Their input is pre-structured by the developers to highlight the analogies that the software is supposed to find; and the general architecture of the programs is serial and deterministic rather than parallel and stochastic like Copycat's, which they consider psychologically more plausible.\n\nSevere criticism is put on the claim that these tools can solve \"real-life\" problems.\nIn fact, only the terms used in the example suggest that the input to the programs comes from a concrete situation.\nThe logical structures don't actually imply any meaning for the term.\n\nFinally a more positive assessment is given to two other projects: Indurkhya' PAN model and Kokinov's AMBR system.\n\nThis chapter looks at those aspects of human creativity that are not yet modeled by Copycat and lays down a research plan for a future extension of the software.\nThe main missing element is the mind's ability to observe itself and reflect on its own thinking process.\nAlso important is the ability to learn and to remember the results of the mental activity.\n\nThe creativity displayed in finding analogies should be applicable at ever higher levels: making analogies between analogies (expression inspired by the title of a book by Stanislaw Ulam), analogies between these second-order analogies, and so on.\n\nAnother of Hofstadter's students, Robert French, was assigned the task of applying the architecture of Copycat to a different domain, consisting in analogies between objects lying on a table in a coffeehouse.\nThe resulting program was named Tabletop.\n\nThe authors present a different and vaster domain to justify the relevance of attacking such a trivial-seeming project.\nThe alternative domain is called Ob-Platte and consists in discovering analogies between geographical locations in different regions or countries.\n\nOnce again arguments are offered against a brute-force approach, which would work on the small Tabletop domain but would become unfeasible on the larger Ob-Platte domain.\nInstead a parallel non-deterministic architecture is used, similar to the one adopted by the Copycat project.\n\nIn the premise to the chapter, title \"The Knotty Problem of Evaluating Research\", Hofstadter considers the question of how research in AI should be assessed.\nHe argues against a strict adherence to a match between the results of an AI program with the average answer of human test subjects.\nHe gives two reasons for his rejection: the AI program is supposed to emulate creativity, while an average of human responses will delete any original insight by any of the single subjects; and the architecture of the program should be more important that its mere functional description.\n\nIn the main article, the architecture of Tabletop is described: it is strongly inspired by that of Copycat and consists of a Slipnet, a Workspace, and a Corerack.\n\nThis last chapter is about a more ambitious project that Hofstadter started with student Gary McGraw.\nThe microdomain used is that of grid fonts: typographic alphabets constructed using a rigid system of small rigid components.\nThe goal is to construct a program that, given only a few or just one letter from the grid font, can generate the whole alphabet \"in the same style\".\nThe difficulty lies in the ambiguity and undefinability of \"style\".\nThe projected program would have a structure very similar to that of Jumble, Numble, Copycat, and Tabletop.\n\nIn the concluding part of the book, Hofstadter analyses some AI projects with a critical eye.\nHe finds that today's AI is missing the gist of human creativity and is making exaggerated claims.\nThe project under scrutiny are the following.\n\nAARON, a computer artist that can draw images of people in outdoor settings in a distinctive style reminiscent of that of a human artist; criticism: the program doesn't have any understanding of the objects it draws, it just uses some graphical algorithms with some randomness thrown in to generate different scenes at every run and to give the style a more natural feel.\n\nRacter, a computer author that wrote a book entitled \"The Policeman's Beard Is Half Constructed\".\nAlthough some of the prose generated by the program is quite impressive, due in part to the Eliza effect, the computer does not have any notion of plot or of the meaning of the words it uses. Furthermore, the book is made up of selected texts from thousands produced by the computer over several years.\n\nAM, a computer mathematician that generates new mathematical concepts. It managed to produce by itself the notion of prime number and the Goldbach conjecture. As with Racter, the question is how much the programmer filtered the output of the program, keeping only the occasional interesting output.\nAlso, mathematics being a very specialized domain, it is doubtful whether the techniques used can be abstracted to general cognition.\n\nAnother mathematical program, called Geometry, was celebrated for making an insightful discovery of an original proof that an isosceles triangle has equal base angles. The proof is based on seeing the triangle in two different ways. However, the program generates all possible ways of seeing the triangle, not even knowing that it is the same triangle.\n\nHofstadter concludes with some methodological remarks on the Turing Test.\nIn his opinion it is still a good definition and he argues that by interacting with a program, a human may be able to have insight not just on its behaviour but also on its structure.\nHowever, he criticises the use that is made of it at present: it encourages the development of fancy natural-language interfaces instead of the investigation of deep cognitive faculties.\n"}
{"id": "12656", "url": "https://en.wikipedia.org/wiki?curid=12656", "title": "Godwin's law", "text": "Godwin's law\n\nGodwin's law (or Godwin's rule of Hitler analogies) is an Internet adage asserting that \"As an online discussion grows longer, the probability of a comparison involving Nazis or Hitler approaches 1\"; that is, if an online discussion (regardless of topic or scope) goes on long enough, sooner or later someone will compare someone or something to Adolf Hitler or his deeds, the point at which effectively the discussion or thread often ends. Promulgated by the American attorney and author Mike Godwin in 1990, Godwin's law originally referred specifically to Usenet newsgroup discussions. It is now applied to any threaded online discussion, such as Internet forums, chat rooms, and comment threads, as well as to speeches, articles, and other rhetoric where \"reductio ad Hitlerum\" occurs.\n\nThere are many corollaries to Godwin's law, some considered more canonical (by being adopted by Godwin himself) than others. For example, there is a tradition in many newsgroups and other Internet discussion forums that, when a Hitler comparison is made, the thread is finished and whoever made the comparison loses whatever debate is in progress. This principle is itself frequently referred to as Godwin's law.\n\nGodwin's law itself can be abused as a distraction, diversion or even as censorship, fallaciously miscasting an opponent's argument as hyperbole when the comparisons made by the argument are actually appropriate. Similar criticisms of the \"law\" (or \"at least the distorted version which purports to prohibit all comparisons to German crimes\") have been made by the American lawyer, journalist, and author Glenn Greenwald.\n\nGodwin's law does not claim to articulate a fallacy; it is instead framed as a memetic tool to reduce the incidence of inappropriate hyperbolic comparisons. \"Although deliberately framed as if it were a law of nature or of mathematics,\" Godwin wrote, \"its purpose has always been rhetorical and pedagogical: I wanted folks who glibly compared someone else to Hitler to think a bit harder about the Holocaust.\"\n\nGodwin has stated that he introduced Godwin's law in 1990 as an experiment in memetics.\n\nIn 2012, \"Godwin's law\" became an entry in the third edition of the \"Oxford English Dictionary\".\n\nIn December 2015, Godwin commented on the Nazi and fascist comparisons being made by several articles about Republican presidential candidate Donald Trump, saying: \"If you're thoughtful about it and show some real awareness of history, go ahead and refer to Hitler when you talk about Trump, or any other politician.\" In August 2017, Godwin made similar remarks on social networking websites Facebook and Twitter with respect to the two previous days' Unite the Right rally in Charlottesville, Virginia, endorsing and encouraging efforts to compare its alt-right organizers to Nazis.\n\nIn October 2018, Godwin made a similar statement when someone asked him, via Twitter, if it would be OK to call Brazilian presidential candidate Jair Bolsonaro a \"nazi\", answering a direct question (\"So, just to be clear, is it OK to call Bolsonaro a nazi?\") with the portuguese word \"Sim!\" (meaning \"yes\" in English). \n\n\n"}
{"id": "369116", "url": "https://en.wikipedia.org/wiki?curid=369116", "title": "Hume's fork", "text": "Hume's fork\n\nHume's fork is an explanation, developed by later philosophers, of David Hume's 1730s division of \"relations of ideas\" from \"matters of fact and real existence\". A distinction is made between necessary versus contingent (concerning reality), versus (concerning knowledge), and analytic versus synthetic (concerning language). Relations of abstract ideas align on one side (necessary, \"a priori\", analytic), whereas concrete truths align on the other (contingent, \"a posteriori\", synthetic).\n\nThe \"necessary\" is generally true in all possible worlds—usually by mere logical validity—whereas the \"contingent\" hinges on the way the real world is. The \"a priori\" is knowable before or without, whereas the \"a posteriori\" is knowable only after or through, experience in an area of interest. The \"analytic\" is a statement true by virtue of its terms' meanings, and therefore a tautology—necessarily true but uninformative—whereas the \"synthetic\" is true by its terms' meanings in relation to a state of facts. In other words, analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. Philosophers have used the terms differently, and there is debate over whether there is a legitimate distinction. \n\nHume's strong empiricism, as in Hume's fork as well as Hume's problem of induction, was taken as a threat to Newton's theory of motion. Immanuel Kant responded with rationalism in his 1781 \"Critique of Pure Reason\", where Kant attributed to the mind a causal role in sensory experience by the mind's aligning the environmental input by arranging those sense data into the experience of space and time. Kant thus reasoned existence of the synthetic \"a priori\"—combining meanings of terms with states of facts, yet known true without experience of the particular instance—replacing the two prongs of Hume's fork with a three-pronged-fork thesis (Kant's pitchfork) and thus saving Newton's law of universal gravitation.\n\nIn 1919, Newton's theory fell to Einstein's general theory of relativity. In the late 1920s, the logical positivists rejected Kant's synthetic \"a priori\" and asserted Hume's fork, so called, while hinging it at language—the analytic/synthetic division—while presuming that by holding to analyticity, they could develop a logical syntax entailing, as a consequence of Hume's fork, both necessity and aprioricity, thus restricting science to claims verifiable as either false or true. In the early 1950s, Willard Van Orman Quine undermined the analytic/synthetic division by explicating ontological relativity, as every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. By the early 1970s, Saul Kripke established the necessary \"a posteriori\", since if the Morning Star and the Evening Star are the same star, they are the same star by necessity, but this is known true by a human only through relevant experience.\n\nHume's fork remains basic in Anglo-American philosophy. Many deceptions and confusions are foisted by surreptitious or unwitting conversion of a synthetic claim to an analytic claim, rendered true by necessity but merely a tautology, for instance the \"No true Scotsman\" move. Simply put, Hume's fork has limitations. Related concerns are Hume's distinction of demonstrative versus probable reasoning and Hume's law. Hume makes other, important two-category distinctions, such as beliefs versus desires and as impressions versus ideas.\n\nThe first distinction is between two different areas of human study:\n\nHume's fork is often stated in such a way that statements are divided up into two types:\n\n\nIn modern terminology, members of the first group are known as analytic propositions and members of the latter as synthetic propositions. This terminology comes from Kant (Introduction to \"Critique of Pure Reason\", Section IV).\n\nInto the first class fall statements such as \"all bodies are extended\", \"all bachelors are unmarried\", and truths of mathematics and logic. Into the second class fall statements like \"the sun rises in the morning\", and \"all bodies have mass\".\n\nHume wants to prove that certainty does not exist in science. First, Hume notes that statements of the second type can never be entirely certain, due to the fallibility of our senses, the possibility of deception (see e.g. the modern brain in a vat theory) and other arguments made by philosophical skeptics. It is always logically possible that any given statement about the world is false.\n\nSecond, Hume claims that our belief in cause-and-effect relationships between events is not grounded on reason, but rather arises merely by habit or custom. Suppose one states: \"Whenever someone on earth lets go of a stone it falls.\" While we can grant that in every instance thus far when a rock was dropped on Earth it went down, this does not make it logically necessary that in the future rocks will fall when in the same circumstances. Things of this nature rely upon the future conforming to the same principles which governed the past. But that isn't something that we can know based on past experience—all past experience could tell us is that in the past, the future has resembled the past.\n\nThird, Hume notes that relations of ideas can be used only to prove other relations of ideas, and mean nothing outside of the context of how they relate to each other, and therefore tell us nothing about the world. Take the statement \"An equilateral triangle has three sides of equal length.\" While some earlier philosophers (most notably Plato and Descartes) held that logical statements such as these contained the most formal reality, since they are always true and unchanging, Hume held that, while true, they contain no formal reality, because the truth of the statements rests on the definitions of the words involved, and not on actual things in the world, since there is no such thing as a true triangle or exact equality of length in the world. So for this reason, relations of ideas cannot be used to prove matters of fact.\n\nThe results claimed by Hume as consequences of his fork are drastic. According to him, relations of ideas can be proved with certainty (by using other relations of ideas), however, they don't really mean anything about the world. Since they don't mean anything about the world, relations of ideas cannot be used to prove matters of fact. Because of this, matters of fact have no certainty and therefore cannot be used to prove anything. Only certain things can be used to prove other things for certain, but only things about the world can be used to prove other things about the world. But since we can't cross the fork, nothing is both certain and about the world, only one or the other, and so it is impossible to prove something about the world with certainty.\n\nIf accepted, Hume's fork makes it pointless to try to prove the existence of God (for example) as a matter of fact. If God is not literally made up of physical matter, and does not have an observable effect on the world, making a statement about God is not a matter of fact. Therefore, a statement about God must be a relation of ideas. In this case if we prove the statement \"God exists,\" it doesn't really tell us anything about the world; it is just playing with words. It is easy to see how Hume's fork voids the causal argument and the ontological argument for the existence of a non-observable God. However, this does not mean that the validity of Hume's fork would imply that God definitely does not exist, only that it would imply that the existence of God cannot be proven as a matter of fact without worldly evidence. \n\nHume rejected the idea of any meaningful statement that did not fall into this schema, saying:\nIf we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, Does it contain any abstract reasoning concerning quantity or number? No. Does it contain any experimental reasoning concerning matter of fact and existence? No. Commit it then to the flames: for it can contain nothing but sophistry and illusion. — \"An Enquiry Concerning Human Understanding\"\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "26279594", "url": "https://en.wikipedia.org/wiki?curid=26279594", "title": "Interpretation (philosophy)", "text": "Interpretation (philosophy)\n\nA philosophical interpretation is the assignment of meanings to various concepts, symbols, or objects under consideration. Two broad types of interpretation can be distinguished: interpretations of physical objects, and interpretations of concepts (Conceptual model).\n\nInterpretation is related to perceiving the things. An aesthetic interpretation is an explanation of the meaning of some work of art. An aesthetic interpretation expresses an understanding of a work of art, a poem, performance, or piece of literature. There may be different interpretations to same work by art by different people owing to their different perceptions or aims. All such interpretations are termed as 'aesthetic interpretations'. Some people, instead of interpreting work of art, believe in interpreting artist himself. It pretty much means \"how or what do I believe about (subject)\"\n\nA judicial interpretation is a conceptual interpretation that explains how the judiciary should interpret the law, particularly constitutional documents and legislation (see statutory interpretation).\n\nIn logic, an interpretation is an assignment of meaning to the symbols of a language. The formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called \"formal semantics\".\n\nReligious interpretation and similarly religious self-interpretation define a section of religion-related studies (theology, comparative religion, reason) where attention is given to aspects of perception—where religious symbolism and the self-image of all those who hold religious views have important bearing on how others perceive their particular belief system and its adherents.\n\nAn interpretation is a \"descriptive interpretation\" (also called a \"factual interpretation\") if at least one of the undefined symbols of its formal system becomes, in the interpretation, the name of a physical object, or observable property. A descriptive interpretation is a type of interpretation used in science and logic to talk about empirical entities.\n\nWhen scientists attempt to formalize the principles of the empirical sciences, they use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system that will serve as a conceptual model of reality. Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "563405", "url": "https://en.wikipedia.org/wiki?curid=563405", "title": "League system", "text": "League system\n\nA league system is a hierarchy of leagues in a sport. They are often called pyramids, due to their tendency to split into an increasing number of regional divisions further down the system. League systems of some sort are used in many sports in many countries.\n\nIn association football, rugby union and rugby league, league systems are usually connected by the process of promotion and relegation, in which teams from a lower division who finish at the top of the standings in their league are promoted (advanced to the next level of the system) while teams who finish lowest in their division are relegated (move down to a lower division). This process can be automatic each year, or can require playoffs.\n\nIn North America, league systems in the most popular sports do not use promotion or relegation. Most professional sports are divided into major and minor leagues. Baseball and association football (known as soccer in North America) have well-defined pyramid shapes to their minor league hierarchies, each managed by a governing body (Minor League Baseball, an organization under the authority of the Commissioner of Baseball, governs baseball leagues; the United States Soccer Federation designates the American soccer pyramid.) Ice hockey's professional minor league system is linear, with one league at most of the four levels of the game; the ice hockey league system in North America is governed by collective bargaining agreements and affiliation deals between the NHL, AHL and ECHL.\n\nGridiron football does not operate on a league system. Different professional leagues play by very different sets of rules in different seasons (the NFL plays 11-a-side on a 100-yard field in autumn and early winter, the CFL uses 12-a-side on a 110-yard field in summer and early fall, while arena football and the minor indoor leagues each play 8-a-side on a 50-yard field in the spring and early summer). There have been attempts at forming true minor leagues for the professional game (most recently with 2017's The Spring League); none so far have been able to balance the major leagues' requests with the ability to maintain financial solvency.\n\n"}
{"id": "34074829", "url": "https://en.wikipedia.org/wiki?curid=34074829", "title": "Lexical entrainment", "text": "Lexical entrainment\n\nLexical entrainment is the phenomenon in conversational linguistics of the process of the subject adopting the reference terms of their interlocutor. In practice, it acts as a mechanism of the cooperative principle in which both parties to the conversation employ lexical entrainment as a progressive system to develop \"conceptual pacts\" (a working temporary conversational terminology) to ensure maximum clarity of reference in the communication between the parties; this process is necessary to overcome the ambiguity inherent in the multitude of synonyms that exist in language. \n\nLexical entrainment arises by two cooperative mechanisms: \n\nOnce lexical entrainment has come to determine the phrasing for a referent, both parties will use that terminology for the referent for the duration, even if it proceeds to violate the Gricean maxim of quantity. For example, if one wants to refer to a brown loafer out of a set of shoes that consist of: the loafer, a sneaker, and a high-heeled shoe, they will not use \"the shoe\" to describe the object as this phrasing does not unambiguously describe one item in the set under consideration. They will also not call the object \"the brown loafer\" which would violate Grice's maxim of quantity. The speaker will settle on using the term \"the loafer\" as it is just informative enough without giving too much information. \n\nAnother important factor is lexical availability; the ease of conceptualizing a referent in a certain way and then retrieving and producing a label for it. For many objects the most available labels are basic nouns; for example, the word \"dog\". Instead of saying \"animal\" or \"husky\" for the referent, most subjects will default to \"dog\".\nIf in a set of objects one is to refer to either a husky, a table, and a poster, people are still most likely to use the word \"dog.\" This is technically a violation of Grice's maxim of quantity, as using the term \"animal\" is ideal.\n\nLexical entrainment has applications in natural language processing in computers as well as human–human interaction. Currently, the adaptability of computers to modify their referencing to the terms of their human interlocutor is limited, so the entrainment adaptation falls to the human operator; this phenomenon is readily demonstrated in Brennan's 1996 experiment.\n"}
{"id": "6365190", "url": "https://en.wikipedia.org/wiki?curid=6365190", "title": "Li (Confucianism)", "text": "Li (Confucianism)\n\nLi () is a classical Chinese word which is commonly used in Chinese philosophy, particularly within Confucianism. \"Li\" does not encompass a definitive object but rather a somewhat abstract idea and, as such, is translated in a number of different ways. Wing-tsit Chan explains that \"li\" originally meant \"a religious sacrifice,\" but has come to mean ceremony, ritual, decorum, rules of propriety, good form, good custom, etc., and has even been equated with Natural law.\"\n\nIn Chinese cosmology, human agency participates in the ordering of the universe by Li ('rites'). There are several Chinese definitions of a rite, one of the most common definitions is that it transforms the invisible to visible; through the performance of rites at appropriate occasions, humans make visible the underlying order. Performing the correct ritual focuses, links, orders, and moves the social, which is the human realm, in correspondence with the terrestrial and celestial realms to keep all three in harmony. This procedure has been described as centering, which used to be the duty of the Son of Tian, the emperor. But it was also done by all those who conducted state, ancestral, and life-cycle rites and, in another way, by Daoists who conducted the rites of local gods as a centering of the forces of exemplary history, of liturgical service, of the correct conduct of human relations, and of the arts of divination such as the earliest of all Chinese classics—the \"Book of Changes\" (\"Yi Jing\")—joining textual learning to bodily practices for health and the harmonized enhancement of circuits of energy (qi). \n\nThe rites of \"li\" are not rites in the Western conception of religious custom. Rather, \"li\" embodies the entire spectrum of interaction with humans, nature, and even material objects. Confucius includes in his discussions of \"li\" such diverse topics as learning, tea drinking, titles, mourning, and governance. Xunzi cites \"songs and laughter, weeping and lamentation...rice and millet, fish and meat...the wearing of ceremonial caps, embroidered robes, and patterned silks, or of fasting clothes and mourning clothes...unspacious rooms and very nonsecluded halls, hard mats, seats and flooring\" as vital parts of the fabric of \"li\".\n\nAmong the earliest historical discussions on \"li\" stands the 25th year of Zhao Gong () in the Zuo Zhuan.\n\n\"Li\" consists of the norms of proper social behavior as taught to others by fathers, village elders and government officials. The teachings of li promoted ideals such as filial piety, brotherliness, righteousness, good faith and loyalty. The influence of \"li\" guided public expectations, such as the loyalty to superiors and respect for elders in the community.\n\nContinuous with the emphasis on community, following \"li\" included the internalization of action, which both yields the comforting feeling of tradition and allows one to become \"more open to the panoply of sensations of the experience\" (Rosemont 2005). But it should also maintain a healthy practice of selflessness, both in the actions themselves and in the proper example which is set for one's brothers. Approaches in the community, as well as personal approaches together demonstrate how \"li\" pervades in all things, the broad and the detailed, the good and the bad, the form and the formlessness. This is the complete realization of \"li\".\n\nThe rituals and practices of \"li\" are dynamic in nature. \"Li\" practices have been revised and evaluated throughout time to reflect the emerging views and beliefs found in society. Although these practices may change, which happens very slowly over time, the fundamental ideals remain at the core of \"li\", which largely relate to social order.\n\nConfucius envisioned proper government being guided by the principles of \"li\". Some Confucians proposed the perfectibility of human beings with learning Li as an important part of that process. Overall, Confucians believed governments should place more emphasis on \"li\" and rely much less on penal punishment when they govern.\n\nConfucius stressed the importance of the rites as fundamental to proper governmental leadership. In his sayings, Confucius regarded feudal lords in China that adopted the Chinese rites as being just rulers of the Central States. Contrarily, feudal lords that did not adopt these rites were considered uncivilized, not worthy of being considered Chinese or part of the Central States (Spring and Autumn Annals).\n\n\"Li\" should be practiced by all members of the society. \"Li\" also involves the superior treating the inferior with propriety and respect. As Confucius said \"a prince should employ his minister according to the rules of propriety (Li); ministers should serve their prince with loyalty\" (Analects, 3:19).\n\n\"Li\" is \"one term by which the [traditional Chinese] historiographers could name all the principles of conservatism they advanced in the speeches of their characters.\" \n\n"}
{"id": "160970", "url": "https://en.wikipedia.org/wiki?curid=160970", "title": "Literal and figurative language", "text": "Literal and figurative language\n\nLiteral and figurative language is a distinction within some fields of language analysis, in particular stylistics, rhetoric, and semantics. \n\nLiteral usage confers meaning to words, in the sense of the meaning they have by themselves, outside any figure of speech. It maintains a consistent meaning regardless of the context, with \"the intended meaning corresponding exactly to the meaning\" of the individual words. Figurative use of language is the use of words or phrases that \"implies a non-literal meaning which does make sense or that could [also] be true\".\n\nAristotle and later the Roman Quintilian were among the early analysts of rhetoric who expounded on the differences between literal and figurative language.\n\nIn 1769, Frances Brooke's novel \"The History of Emily Montague\" was used in the earliest \"Oxford English Dictionary\" citation for the figurative sense of \"literally\"; the sentence from the novel used was, \"He is a fortunate man to be introduced to such a party of fine women at his arrival; it is literally \"to feed among the lilies\".\" This citation was also used in the OED's 2011 revision.\n\nWithin literary analysis, such terms are still used; but within the fields of cognition and linguistics, the basis for identifying such a distinction is no longer used.\n\nFigurative language can take multiple forms, such as simile or metaphor. \"Merriam-Webster's Encyclopedia Of Literature\" says that figurative language can be classified in five categories: resemblance or relationship, emphasis or understatement, figures of sound, verbal games, and errors. \n\nA simile is a comparison of two things, indicated by some connective, usually \"like\", \"as\", \"than\", or a verb such as \"resembles\" to show how they are similar.\n\nA metaphor is a figure of speech in which two \"essentially unlike things\" are shown to have a type of resemblance or create a new image. The similarities between the objects being compared may be implied rather than directly stated.\n\nAn extended metaphor is a metaphor that is continued over multiple sentences.\n\nOnomatopoeia is a word designed to be an imitation of a sound.\n\nPersonification is the attribution of a personal nature or character to inanimate objects or abstract notions, especially as a rhetorical figure.\n\nAn oxymoron is a figure of speech in which a pair of opposite or contradictory terms is used together for emphasis.\n\nA paradox is a statement or proposition which is self-contradictory, unreasonable, or illogical.\n\nHyperbole is a figure of speech which uses an extravagant or exaggerated statement to express strong feelings.\n\nAllusion is a reference to a famous character or event.\n\nAn idiom is an expression that has a figurative meaning unrelated to the literal meaning of the phrase.\n\nA pun is an expression intended for a humorous or rhetorical effect by exploiting different meanings of words.\n\nPrior to the 1980s, the \"standard pragmatic\" model of comprehension was widely believed. In that model, it was thought the recipient would first attempt to comprehend the meaning as if literal, but when an appropriate literal inference could not be made, the recipient would shift to look for a figurative interpretation that would allow comprehension. Since then, research has cast doubt on the model. In tests, figurative language was found to be comprehended at the same speed as literal language; and so the premise that the recipient was first attempting to process a literal meaning and discarding it before attempting to process a figurative meaning appears to be false.\n\nBeginning with the work of Michael Reddy in his 1979 work \"The Conduit Metaphor\", many linguists now reject that there is a valid way to distinguish between a \"literal\" and \"figurative\" mode of language.\n\n"}
{"id": "23014670", "url": "https://en.wikipedia.org/wiki?curid=23014670", "title": "Marginal utility", "text": "Marginal utility\n\nIn economics, utility is the satisfaction or benefit derived by consuming a product; thus the marginal utility of a good or service is the change in the utility from an increase in the consumption of that good or service. \n\nIn the context of cardinal utility, economists sometimes speak of a law of diminishing marginal utility, meaning that the first unit of consumption of a good or service yields more utility than the second and subsequent units, with a continuing reduction for greater amounts. Therefore, the fall in marginal utility as consumption increases is known as diminishing marginal utility. Mathematically:\n\nThe term \"marginal\" refers to a small change, starting from some baseline level. As Philip Wicksteed explained the term,\n\nMarginal considerations are considerations which concern a slight increase or diminution of the stock of anything which we possess or are considering\n\nFrequently the marginal change is assumed to start from the endowment, meaning the total resources available for consumption (see Budget constraint). This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made by the individual himself or herself and by others.\n\nFor reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. Under this assumption, marginal concepts, including marginal utility, may be expressed in terms of differential calculus. Marginal utility can then be defined as the first derivative of total utility—the total satisfaction obtained from consumption of a good or service—with respect to the amount of consumption of that good or service.\n\nIn practice the smallest relevant division may be quite large. Sometimes economic analysis concerns the marginal values associated with a change of one unit of a discrete good or service, such as a motor vehicle or a haircut. For a motor vehicle, the total number of motor vehicles produced is large enough for a continuous assumption to be reasonable: this may not be true for, say, an aircraft carrier.\n\nDepending on which theory of \"utility\" is used, the interpretation of marginal utility can be meaningful or not. Economists have commonly described utility as if it were \"quantifiable\", that is, as if different levels of utility could be compared along a numerical scale. This has affected the development and reception of theories of marginal utility. Quantitative concepts of utility allow familiar arithmetic operations, and further assumptions of continuity and differentiability greatly increase tractability.\n\nContemporary mainstream economic theory frequently defers metaphysical questions, and merely notes or assumes that preference structures conforming to certain rules can be usefully \"proxied\" by associating goods, services, or their uses with quantities, and \"defines\" \"utility\" as such a quantification.\n\nAnother conception is Benthamite philosophy, which equated usefulness with the production of pleasure and avoidance of pain, assumed subject to arithmetic operation. British economists, under the influence of this philosophy (especially by way of John Stuart Mill), viewed utility as \"the feelings of pleasure and pain\" and further as a \"\"quantity\" of feeling\" (emphasis added).\n\nThough generally pursued outside of the mainstream methods, there are conceptions of utility that do not rely on quantification.\nFor example, the Austrian school generally attributes value to \"the satisfaction of wants\", and sometimes rejects even the \"possibility\" of quantification.\nIt has been argued that the Austrian framework makes it possible to consider rational preferences that would otherwise be excluded.\n\nIn any standard framework, the same object may have different marginal utilities for different people, reflecting different preferences or individual circumstances.\n\nThe concept in cardinal utility theory that marginal utilities diminish across the ranges relevant to decision-making is called the \"law of diminishing marginal utility\" (and is also known as Gossen's First Law). This refers to the increase in utility an individual gains from increasing their consumption of a particular good. \"The law of diminishing marginal utility is at the heart of the explanation of numerous economic phenomena, including time preference and the value of goods ... The law says, first, that the marginal utility of each homogenous unit decreases as the supply of units increases (and vice versa); second, that the marginal utility of a larger-sized unit is greater than the marginal utility of a smaller-sized unit (and vice versa). The first law denotes the law of diminishing marginal utility, the second law denotes the law of increasing total utility.\"\n\nIn modern economics, choice under conditions of certainty at a single point in time is modeled via ordinal utility, in which the numbers assigned to the utility of a particular circumstance of the individual have no meaning by themselves, but which of two alternative circumstances has higher utility \"is\" meaningful. With ordinal utility, a person's preferences have no unique marginal utility, and thus whether or not marginal utility is diminishing is not meaningful. In contrast, the concept of diminishing marginal utility is meaningful in the context of cardinal utility, which in modern economics is used in analyzing intertemporal choice, choice under uncertainty, and social welfare.\n\nThe law of diminishing marginal utility is similar to the law of diminishing returns which states that as the amount of one factor of production increases as all other factors of production are held the same, the marginal return (extra output gained by adding an extra unit) decreases.\n\nAs the rate of commodity acquisition increases, \"marginal\" utility decreases. If commodity consumption continues to rise, marginal utility at some point may fall to zero, reaching maximum total utility. Further increase in consumption of units of commodities causes marginal utility to become negative; this signifies dissatisfaction. For example,\n\nDiminishing marginal utility is traditionally a microeconomic concept and often holds for an individual, although the marginal utility of a good or service might be \"increasing\" as well. For example:\n\nAs suggested elsewhere in this article, occasionally one may come across a situation in which marginal utility increases even at a macroeconomic level. For example, the provision of a service may only be viable if it accessible to most or all of the population, and the marginal utility of a raw material required to provide such a service will increase at the \"tipping point\" at which this occurs. This is similar to the position with very large items such as aircraft carriers: the numbers of these items involved are so small that marginal utility is no longer a helpful concept, as there is merely a simple \"yes\" or \"no\" decision.\n\nMarginalism explains choice with the hypothesis that people decide whether to effect any given change based on the marginal utility of that change, with rival alternatives being chosen based upon which has the greatest marginal utility.\n\nIf an individual possesses a good or service whose marginal utility to him is less than that of some other good or service for which he could trade it, then it is in his interest to effect that trade. Of course, as one thing is sold and another is bought, the respective marginal gains or losses from further trades will change. If the marginal utility of one thing is diminishing, and the other is not increasing, all else being equal, an individual will demand an increasing ratio of that which is acquired to that which is sacrificed. (One important way in which all else might not be equal is when the use of the one good or service complements that of the other. In such cases, exchange ratios might be constant.) If any trader can better his position by offering a trade more favorable to complementary traders, then he will do so.\n\nIn an economy with money, the marginal utility of a quantity is simply that of the best good or service that it could purchase. In this way it is useful for explaining supply and demand, as well as essential aspects of models of imperfect competition.\n\nThe \"paradox of water and diamonds\", usually most commonly associated with Adam Smith, though recognized by earlier thinkers, is the apparent contradiction that water possesses a value far lower than diamonds, even though water is far more vital to a human being.\n\nPrice is determined by both marginal utility and marginal cost, and here the key to the \"paradox\" is that the marginal cost of water is far lower than that of diamonds.\n\nThat is not to say that the price of any good or service is simply a function of the marginal utility that it has for any one individual nor for some ostensibly typical individual. Rather, individuals are willing to trade based upon the respective marginal utilities of the goods that they have or desire (with these marginal utilities being distinct for each potential trader), and prices thus develop constrained by these marginal utilities.\n\nUnder the special case in which usefulness can be quantified, the change in utility of moving from state formula_1 to state formula_2 is\nMoreover, if formula_1 and formula_2 are distinguishable by values of just one variable formula_6 which is itself quantified, then it becomes possible to speak of the ratio of the marginal utility of the change in formula_6 to the size of that change:\n\n(where “c.p.” indicates that the \"only\" independent variable to change is formula_6).\n\nMainstream neoclassical economics will typically assume that the limit\nexists, and use “marginal utility” to refer to the partial derivative\nAccordingly, diminishing marginal utility corresponds to the condition\n\nThe concept of marginal utility grew out of attempts by economists to explain the determination of price. The term “marginal utility”, credited to the Austrian economist Friedrich von Wieser by Alfred Marshall, was a translation of Wieser's term “Grenznutzen” (\"border-use\").\n\nPerhaps the essence of a notion of diminishing marginal utility can be found in Aristotle's \"Politics\", wherein he writes (There has been marked disagreement about the development and role of marginal considerations in Aristotle's value theory.)\n\nA great variety of economists have concluded that there is \"some\" sort of interrelationship between utility and rarity that affects economic decisions, and in turn informs the determination of prices. Diamonds are priced higher than water because their marginal utility is higher than water .\n\nEighteenth-century Italian mercantilists, such as Antonio Genovesi, Giammaria Ortes, Pietro Verri, Marchese Cesare di Beccaria, and Count Giovanni Rinaldo Carli, held that value was explained in terms of the general utility and of scarcity, though they did not typically work-out a theory of how these interacted. In \"Della moneta\" (1751), Abbé Ferdinando Galiani, a pupil of Genovesi, attempted to explain value as a ratio of two ratios, \"utility\" and \"scarcity\", with the latter component ratio being the ratio of quantity to use.\n\nAnne Robert Jacques Turgot, in \"Réflexions sur la formation et la distribution de richesse\" (1769), held that value derived from the general utility of the class to which a good belonged, from comparison of present and future wants, and from anticipated difficulties in procurement.\n\nLike the Italian mercantists, Étienne Bonnot, Abbé de Condillac, saw value as determined by utility associated with the class to which the good belong, and by estimated scarcity. In \"De commerce et le gouvernement\" (1776), Condillac emphasized that value is not based upon cost but that costs were paid because of value.\n\nThis last point was famously restated by the Nineteenth Century proto-marginalist, Richard Whately, who in \"Introductory Lectures on Political Economy\" (1832) wrote (Whatley's student Senior is noted below as an early marginalist.)\n\nThe first unambiguous published statement of any sort of theory of marginal utility was by Daniel Bernoulli, in “Specimen theoriae novae de mensura sortis”. This paper appeared in 1738, but a draft had been written in 1731 or in 1732. In 1728, Gabriel Cramer had produced fundamentally the same theory in a private letter. Each had sought to resolve the St. Petersburg paradox, and had concluded that the marginal desirability of money decreased as it was accumulated, more specifically such that the desirability of a sum were the natural logarithm (Bernoulli) or square root (Cramer) thereof. However, the more general implications of this hypothesis were not explicated, and the work fell into obscurity.\n\nIn “A Lecture on the Notion of Value as Distinguished Not Only from Utility, but also from Value in Exchange”, delivered in 1833 and included in \"Lectures on Population, Value, Poor Laws and Rent\" (1837), William Forster Lloyd explicitly offered a general marginal utility theory, but did not offer its derivation nor elaborate its implications. The importance of his statement seems to have been lost on everyone (including Lloyd) until the early 20th century, by which time others had independently developed and popularized the same insight.\n\nIn \"An Outline of the Science of Political Economy\" (1836), Nassau William Senior asserted that marginal utilities were the ultimate determinant of demand, yet apparently did not pursue implications, though some interpret his work as indeed doing just that.\n\nIn “De la mesure de l’utilité des travaux publics” (1844), Jules Dupuit applied a conception of marginal utility to the problem of determining bridge tolls.\n\nIn 1854, Hermann Heinrich Gossen published \"Die Entwicklung der Gesetze des menschlichen Verkehrs und der daraus fließenden Regeln für menschliches Handeln\", which presented a marginal utility theory and to a very large extent worked-out its implications for the behavior of a market economy. However, Gossen's work was not well received in the Germany of his time, most copies were destroyed unsold, and he was virtually forgotten until rediscovered after the so-called Marginal Revolution.\n\nMarginalism eventually found a foothold by way of the work of three economists, Jevons in England, Menger in Austria, and Walras in Switzerland.\n\nWilliam Stanley Jevons first proposed the theory in “A General Mathematical Theory of Political Economy” (PDF), a paper presented in 1862 and published in 1863, followed by a series of works culminating in his book \"The Theory of Political Economy\" in 1871 that established his reputation as a leading political economist and logician of the time. Jevons' conception of utility was in the utilitarian tradition of Jeremy Bentham and of John Stuart Mill, but he differed from his classical predecessors in emphasizing that \"value depends entirely upon utility\", in particular, on \"final utility upon which the theory of Economics will be found to turn.\" He later qualified this in deriving the result that in a model of exchange equilibrium, price ratios would be proportional not only to ratios of \"final degrees of utility,\" but also to costs of production.\n\nCarl Menger presented the theory in \"Grundsätze der Volkswirtschaftslehre\" (translated as \"Principles of Economics\") in 1871. Menger's presentation is peculiarly notable on two points. First, he took special pains to explain \"why\" individuals should be expected to rank possible uses and then to use marginal utility to decide amongst trade-offs. (For this reason, Menger and his followers are sometimes called “the Psychological School”, though they are more frequently known as “the Austrian School” or as “the Vienna School”.) Second, while his illustrative examples present utility as quantified, his essential assumptions do not. (Menger in fact crossed-out the numerical tables in his own copy of the published \"Grundsätze\".) Menger also developed the law of diminishing marginal utility. Menger's work found a significant and appreciative audience.\n\nMarie-Esprit-Léon Walras introduced the theory in \"Éléments d'économie politique pure\", the first part of which was published in 1874 in a relatively mathematical exposition. Walras's work found relatively few readers at the time but was recognized and incorporated two decades later in the work of Pareto and Barone.\n\nAn American, John Bates Clark, is sometimes also mentioned. But, while Clark independently arrived at a marginal utility theory, he did little to advance it until it was clear that the followers of Jevons, Menger, and Walras were revolutionizing economics. Nonetheless, his contributions thereafter were profound.\n\nAlthough the Marginal Revolution flowed from the work of Jevons, Menger, and Walras, their work might have failed to enter the mainstream were it not for a second generation of economists. In England, the second generation were exemplified by Philip Henry Wicksteed, by William Smart, and by Alfred Marshall; in Austria by Eugen von Böhm-Bawerk and by Friedrich von Wieser; in Switzerland by Vilfredo Pareto; and in America by Herbert Joseph Davenport and by Frank A. Fetter.\n\nThere were significant, distinguishing features amongst the approaches of Jevons, Menger, and Walras, but the second generation did not maintain distinctions along national or linguistic lines. The work of von Wieser was heavily influenced by that of Walras. Wicksteed was heavily influenced by Menger. Fetter referred to himself and Davenport as part of “the American Psychological School”, named in imitation of the Austrian “Psychological School”. (And Clark's work from this period onward similarly shows heavy influence by Menger.) William Smart began as a conveyor of Austrian School theory to English-language readers, though he fell increasingly under the influence of Marshall.\n\nBöhm-Bawerk was perhaps the most able expositor of Menger's conception. He was further noted for producing a theory of interest and of profit in equilibrium based upon the interaction of diminishing marginal utility with diminishing marginal productivity of time and with time preference. (This theory was adopted in full and then further developed by Knut Wicksell and, with modifications including formal disregard for time-preference, by Wicksell's American rival Irving Fisher.)\n\nMarshall was the second-generation marginalist whose work on marginal utility came most to inform the mainstream of neoclassical economics, especially by way of his \"Principles of Economics\", the first volume of which was published in 1890. Marshall constructed the demand curve with the aid of assumptions that utility was quantified, and that the marginal utility of money was constant (or nearly so). Like Jevons, Marshall did not see an explanation for supply in the theory of marginal utility, so he synthesized an explanation of demand thus explained with supply explained in a more classical manner, determined by costs which were taken to be objectively determined. (Marshall later actively mischaracterized the criticism that these costs were themselves ultimately determined by marginal utilities.)\n\nKarl Marx acknowledged that \"nothing can have value, without being an object of utility\", but, in his analysis, \"use-value as such lies outside the sphere of investigation of political economy\", with labor being the principal determinant of value under capitalism.\n\nThe doctrines of marginalism and the Marginal Revolution are often interpreted as somehow a response to Marxist economics. However the first volume of \"Das Kapital\" was not published until July 1867, after the works of Jevons, Menger, and Walras were written or well under way (In 1874 Walras published Éléments d'économie politique pure and Carl Menger published Principles of Economics in 1871) ; and Marx was still a relatively minor figure when these works were completed. It is unlikely that any of them knew anything of him. (On the other hand, Hayek or Bartley has suggested that Marx, voraciously reading at the British Museum, may have come across the works of one or more of these figures, and that his inability to formulate a viable critique may account for his failure to complete any further volumes of \"Kapital\" before his death.)\n\nNonetheless, it is not unreasonable to suggest that the generation who followed the preceptors of the Revolution succeeded partly because they could formulate straightforward responses to Marxist economic theory. The most famous of these was that of Böhm-Bawerk, \"Zum Abschluss des Marxschen Systems\" (1896), but the first was Wicksteed's \"The Marxian Theory of Value. \"Das Kapital\": a criticism\" (1884, followed by \"The Jevonian criticism of Marx: a rejoinder\" in 1885). Initially there were only a few Marxist responses to marginalism, of which the most famous were Rudolf Hilferding's \"Böhm-Bawerks Marx-Kritik\" (1904) and \"Politicheskoy ekonomni rante\" (1914) by Никола́й Ива́нович Буха́рин (Nikolai Bukharin). However, over the course of the 20th century a considerable literature developed on the conflict between marginalism and the labour theory of value, with the work of the neo-Ricardian economist Piero Sraffa providing an important critique of marginalism.\n\nIt might also be noted that some followers of Henry George similarly consider marginalism and neoclassical economics a reaction to \"Progress and Poverty\", which was published in 1879.\n\nIn the 1980s John Roemer and other analytical Marxists have worked to rebuild Marxian theses on a marginalist foundation.\n\nIn his 1881 work \"Mathematical Psychics\", Francis Ysidro Edgeworth presented the indifference curve, deriving its properties from marginalist theory which assumed utility to be a differentiable function of quantified goods and services. Later work attempted to generalize to the indifference curve formulations of utility and marginal utility in avoiding unobservable measures of utility.\n\nIn 1915, Eugen Slutsky derived a theory of consumer choice solely from properties of indifference curves. Because of the World War, the Bolshevik Revolution, and his own subsequent loss of interest, Slutsky's work drew almost no notice, but similar work in 1934 by John Richard Hicks and R. G. D. Allen derived much the same results and found a significant audience. (Allen subsequently drew attention to Slutsky's earlier accomplishment.)\n\nAlthough some of the third generation of Austrian School economists had by 1911 rejected the quantification of utility while continuing to think in terms of marginal utility, most economists presumed that utility must be a sort of quantity. Indifference curve analysis seemed to represent a way to dispense with presumptions of quantification, albeit that a seemingly arbitrary assumption (admitted by Hicks to be a \"rabbit out of a hat\") about decreasing marginal rates of substitution would then have to be introduced to have convexity of indifference curves.\n\nFor those who accepted that indifference curve analysis superseded earlier marginal utility analysis, the latter became at best perhaps pedagogically useful, but \"old fashioned\" and observationally unnecessary.\n\nWhen Cramer and Bernoulli introduced the notion of diminishing marginal utility, it had been to address a paradox of gambling, rather than the paradox of value. The marginalists of the revolution, however, had been formally concerned with problems in which there was neither risk nor uncertainty. So too with the indifference curve analysis of Slutsky, Hicks, and Allen.\n\nThe expected utility hypothesis of Bernoulli and others was revived by various 20th century thinkers, with early contributions by Ramsey (1926), von Neumann and Morgenstern (1944), and Savage (1954). Although this hypothesis remains controversial, it brings not only utility, but a quantified conception of utility (cardinal utility), back into the mainstream of economic thought.\n\nA major reason why quantified models of utility are influential today is that risk and uncertainty have been recognized as central topics in contemporary economic theory. Quantified utility models simplify the analysis of risky decisions because, under quantified utility, diminishing marginal utility implies risk aversion. In fact, many contemporary analyses of saving and portfolio choice require stronger assumptions than diminishing marginal utility, such as the assumption of prudence, which means convex marginal utility.\n\nMeanwhile, the Austrian School continued to develop its ordinalist notions of marginal utility analysis, formally demonstrating that from them proceed the decreasing marginal rates of substitution of indifference curves.\n\n"}
{"id": "221419", "url": "https://en.wikipedia.org/wiki?curid=221419", "title": "Marginalism", "text": "Marginalism\n\nMarginalism is a theory of economics that attempts to explain the discrepancy in the value of goods and services by reference to their secondary, or marginal, utility. The reason why the price of diamonds is higher than that of water, for example, owes to the greater additional satisfaction of the diamonds over the water. Thus, while the water has greater total utility, the diamond has greater marginal utility.\n\nAlthough the central concept of marginalism is that of marginal utility, marginalists, following the lead of Alfred Marshall, drew upon the idea of marginal physical productivity in explanation of cost. The neoclassical tradition that emerged from British marginalism abandoned the concept of utility and gave marginal rates of substitution a more fundamental role in analysis. Marginalism is an integral part of mainstream economic theory.\n\nFor issues of marginality, constraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change.\n\nNeoclassical economics usually assumes that marginal changes are infinitesimals or limits. (Though this assumption makes the analysis less robust, it increases tractability.) One is therefore often told that \"marginal\" is synonymous with \"very small\", though in more general analysis this may not be operationally true (and would not in any case be literally true). Frequently, economic analysis concerns the marginal values associated with a change of one unit of a resource, because decisions are often made in terms of units; marginalism seeks to explain unit prices in terms of such marginal values.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nMarginalism assumes, for any given agent, economic rationality and an ordering of possible states-of-the-world, such that, for any given set of constraints, there is an attainable state which is best in the eyes of that agent. Descriptive marginalism asserts that choice amongst the specific means by which various anticipated specific states-of-the-world (outcomes) might be affected is governed only by the distinctions amongst those specific outcomes; prescriptive marginalism asserts that such choice \"ought\" to be so governed.\n\nOn such assumptions, each increase would be put to the specific, feasible, previously unrealized use of greatest priority, and each decrease would result in abandonment of the use of lowest priority amongst the uses to which the good or service had been put.\n\nThe marginal utility of a good or service is the utility of its marginal use. Under the assumption of economic rationality, it is the utility of its least urgent possible use \"from\" the best feasible combination of actions in which its use is included.\n\nIn 20th century mainstream economics, the term \"utility\" has come to be formally defined as a \"quantification\" capturing preferences by assigning greater quantities to states, goods, services, or applications that are of higher priority. But marginalism and the concept of marginal utility predate the establishment of this convention within economics. The more general conception of utility is that of \"use\" or \"usefulness\", and this conception is at the heart of marginalism; the term \"marginal utility\" arose from translation of the German \"Grenznutzen\", which literally means \"border use\", referring directly to the marginal use, and the more general formulations of marginal utility do not treat quantification as an \"essential\" feature. On the other hand, none of the early marginalists insisted that utility were \"not\" quantified, some indeed treated quantification as an essential feature, and those who did not still used an assumption of quantification for expository purposes. In this context, it is not surprising to find many presentations that fail to recognize a more general approach.\n\nUnder the special case in which usefulness can be quantified, the change in utility of moving from state formula_1 to state formula_2 is\nMoreover, if formula_1 and formula_2 are distinguishable by values of just one variable formula_6 which is itself quantified, then it becomes possible to speak of the ratio of the marginal utility of the change in formula_6 to the size of that change:\n(where “c.p.” indicates that the \"only\" independent variable to change is formula_6).\n\nMainstream neoclassical economics will typically assume that\nis well defined, and use “marginal utility” to refer to a partial derivative\n\nThe \"law\" of diminishing marginal utility (also known as a \"Gossen's First Law\") is that, \"ceteris paribus\", as additional amounts of a good or service are added to available resources, their marginal utilities are decreasing. This \"law\" is sometimes treated as a tautology, sometimes as something proven by introspection, or sometimes as a mere instrumental assumption, adopted only for its perceived predictive efficacy. Actually, it is not quite any of these things, though it may have aspects of each. The \"law\" does not hold under all circumstances, so it is neither a tautology nor otherwise proveable; but it has a basis in prior observation.\n\nAn individual will typically be able to partially order the potential uses of a good or service. If there is scarcity, then a rational agent will satisfy wants of highest possible priority, so that no want is avoidably sacrificed to satisfy a want of \"lower\" priority. In the absence of complementarity across the uses, this will imply that the priority of use of any additional amount will be lower than the priority of the established uses, as in this famous example:\n\nHowever, if there \"is\" a complementarity across uses, then an amount added can bring things past a desired tipping point, or an amount subtracted cause them to fall short. In such cases, the marginal utility of a good or service might actually be \"increasing\".\n\nWithout the presumption that utility is quantified, the \"diminishing\" of utility should not be taken to be itself an arithmetic subtraction. It is the movement from use of higher to lower priority, and may be no more than a purely ordinal change.\n\nWhen quantification of utility is assumed, diminishing marginal utility corresponds to a utility function whose \"slope\" is continually or continuously decreasing. In the latter case, if the function is also smooth, then the “law” may be expressed\nNeoclassical economics usually supplements or supplants discussion of marginal utility with indifference curves, which were originally derived as the level curves of utility functions, or can be produced without presumption of quantification, but are often simply treated as axiomatic. In the absence of complementarity of goods or services, diminishing marginal utility implies convexity of indifference curves (though such convexity would also follow from quasiconcavity of the utility function).\n\nThe \"rate of substitution\" is the \"least favorable\" rate at which an agent is willing to exchange units of one good or service for units of another. The marginal rate of substitution (\"MRS\") is the rate of substitution at the margin – in other words, given some constraint(s).\n\nWhen goods and services are discrete, the least favorable rate at which an agent would trade A for B will usually be different from that at which she would trade B for A:\nBut, when the goods and services are continuously divisible, in the limiting case\nand the marginal rate of substitution is the slope of the indifference curve (multiplied by formula_15).\n\nIf, for example, Lisa will not trade a goat for anything less than two sheep, then her\nAnd if she will not trade a sheep for anything less than two goats, then her\nBut if she would trade one gram of banana for one ounce of ice cream \"and vice versa\", then\n\nWhen indifference curves (which are essentially graphs of instantaneous rates of substitution) and the convexity of those curves are not taken as given, the \"law\" of diminishing marginal utility is invoked to explain diminishing marginal rates of substitution – a willingness to accept fewer units of good or service formula_19 in substitution for formula_20 as one's holdings of formula_19 grow relative to those of formula_20. If an individual has a stock or flow of a good or service whose marginal utility is less than would be that of some other good or service for which he or she could trade, then it is in his or her interest to effect that trade. Of course, as one thing is traded-away and another is acquired, the respective marginal gains or losses from further trades are now changed. On the assumption that the marginal utility of one is diminishing, and the other is not increasing, all else being equal, an individual will demand an increasing ratio of that which is acquired to that which is sacrificed. (One important way in which all else might not be equal is when the use of the one good or service complements that of the other. In such cases, exchange ratios might be constant.) If any trader can better his or her own marginal position by offering an exchange more favorable to other traders with desired goods or services, then he or she will do so.\n\nAt the highest level of generality, a marginal cost is a marginal opportunity cost. In most contexts, however, \"marginal cost\" will refer to marginal \"pecuniary\" cost – that is to say marginal cost measured by forgone money.\n\nA thorough-going marginalism sees marginal cost as increasing under the \"law\" of diminishing marginal utility, because applying resources to one application reduces their availability to other applications. Neoclassical economics tends to disregard this argument, but to see marginal costs as increasing in consequence of diminishing returns.\n\nMarginalism and neoclassical economics typically explain price formation broadly through the interaction of curves or schedules of supply and demand. In any case buyers are modelled as pursuing typically lower quantities, and sellers offering typically higher quantities, as price is increased, with each being willing to trade until the marginal value of what they would trade-away exceeds that of the thing for which they would trade.\n\nDemand curves are explained by marginalism in terms of marginal rates of substitution.\n\nAt any given price, a prospective buyer has some marginal rate of substitution of money for the good or service in question. Given the \"law\" of diminishing marginal utility, or otherwise given convex indifference curves, the rates are such that the willingness to forgo money for the good or service decreases as the buyer would have ever more of the good or service and ever less money. Hence, any given buyer has a demand schedule that generally decreases in response to price (at least until quantity demanded reaches zero). The aggregate quantity demanded by all buyers is, at any given price, just the sum of the quantities demanded by individual buyers, so it too decreases as price increases.\n\nBoth neoclassical economics and thorough-going marginalism could be said to explain supply curves in terms of marginal cost; however, there are marked differences in conceptions of that cost.\n\nMarginalists in the tradition of Marshall and neoclassical economists tend to represent the supply curve for any producer as a curve of marginal pecuniary costs objectively determined by physical processes, with an upward slope determined by diminishing returns.\n\nA more thorough-going marginalism represents the supply curve as a \"complementary demand curve\" – where the demand is \"for\" money and the purchase is made \"with\" a good or service. The shape of that curve is then determined by marginal rates of substitution of money for that good or service.\n\nBy confining themselves to limiting cases in which sellers or buyers are both \"price takers\" – so that demand functions ignore supply functions or \"vice versa\" – Marshallian marginalists and neoclassical economists produced tractable models of \"pure\" or \"perfect\" competition and of various forms of \"imperfect\" competition, which models are usually captured by relatively simple graphs. Other marginalists have sought to present what they thought of as more realistic explanations, but this work has been relatively uninfluential on the mainstream of economic thought.\n\nThe \"law\" of diminishing marginal utility is said to explain the \"paradox of water and diamonds\", most commonly associated with Adam Smith (though recognized by earlier thinkers). Human beings cannot even survive without water, whereas diamonds, in Smith's day, were ornamentation or engraving bits. Yet water had a very small price, and diamonds a very large price. Marginalists explained that it is the \"marginal\" usefulness of any given quantity that matters, rather than the usefulness of a \"class\" or of a \"totality\". For most people, water was sufficiently abundant that the loss or gain of a gallon would withdraw or add only some very minor use if any, whereas diamonds were in much more restricted supply, so that the loss or gain was much greater.\n\nThat is not to say that the price of any good or service is simply a function of the marginal utility that it has for any one individual nor for some ostensibly typical individual. Rather, individuals are willing to trade based upon the respective marginal utilities of the goods that they have or desire (with these marginal utilities being distinct for each potential trader), and prices thus develop constrained by these marginal utilities.\n\nPerhaps the essence of a notion of diminishing marginal utility can be found in Aristotle's \"Politics\", wherein he writes \n\nA great variety of economists concluded that there was \"some\" sort of inter-relationship between utility and rarity that effected economic decisions, and in turn informed the determination of prices.\n\nEighteenth-century Italian mercantilists, such as Antonio Genovesi, Giammaria Ortes, Pietro Verri, Cesare Beccaria, and Giovanni Rinaldo, held that value was explained in terms of the general utility and of scarcity, though they did not typically work-out a theory of how these interacted. In \"Della Moneta\" (1751), Abbé Ferdinando Galiani, a pupil of Genovesi, attempted to explain value as a ratio of two ratios, \"utility\" and \"scarcity\", with the latter component ratio being the ratio of quantity to use.\n\nAnne Robert Jacques Turgot, in \"Réflexions sur la formation et la distribution de richesse\" (1769), held that value derived from the general utility of the class to which a good belonged, from comparison of present and future wants, and from anticipated difficulties in procurement.\n\nLike the Italian mercantilists, Étienne Bonnot de Condillac saw value as determined by utility associated with the class to which the good belongs, and by estimated scarcity. In \"De commerce et le gouvernement\" (1776), Condillac emphasized that value is not based upon cost but that costs were paid because of value.\n\nThis last point was famously restated by the Nineteenth Century proto-marginalist, Richard Whately, who in \"Introductory Lectures on Political Economy\" (1832) wrote (Whately's student Nassau William Senior is noted below as an early marginalist.)\n\nFrédéric Bastiat in chapters V and XI of his \"Economic Harmonies\" (1850) also develops a theory of value as ratio between services that increment utility, rather than between total utility.\n\nThe first unambiguous published statement of any sort of theory of marginal utility was by Daniel Bernoulli, in \"Specimen theoriae novae de mensura sortis\". This paper appeared in 1738, but a draft had been written in 1731 or in 1732. In 1728, Gabriel Cramer produced fundamentally the same theory in a private letter. Each had sought to resolve the St. Petersburg paradox, and had concluded that the marginal desirability of money decreased as it was accumulated, more specifically such that the desirability of a sum were the natural logarithm (Bernoulli) or square root (Cramer) thereof. However, the more general implications of this hypothesis were not explicated, and the work fell into obscurity.\n\nIn \"A Lecture on the Notion of Value as Distinguished Not Only from Utility, but also from Value in Exchange\", delivered in 1833 and included in \"Lectures on Population, Value, Poor Laws and Rent\" (1837), William Forster Lloyd explicitly offered a general marginal utility theory, but did not offer its derivation nor elaborate its implications. The importance of his statement seems to have been lost on everyone (including Lloyd) until the early 20th century, by which time others had independently developed and popularized the same insight.\n\nIn \"An Outline of the Science of Political Economy\" (1836), Nassau William Senior asserted that marginal utilities were the ultimate determinant of demand, yet apparently did not pursue implications, though some interpret his work as indeed doing just that.\n\nIn \"De la mesure de l'utilité des travaux publics\" (1844), Jules Dupuit applied a conception of marginal utility to the problem of determining bridge tolls.\n\nIn 1854, Hermann Heinrich Gossen published \"Die Entwicklung der Gesetze des menschlichen Verkehrs und der daraus fließenden Regeln für menschliches Handeln\", which presented a marginal utility theory and to a very large extent worked-out its implications for the behavior of a market economy. However, Gossen's work was not well received in the Germany of his time, most copies were destroyed unsold, and he was virtually forgotten until rediscovered after the so-called Marginal Revolution.\n\nMarginalism as a formal theory can be attributed to the work of three economists, Jevons in England, Menger in Austria, and Walras in Switzerland. William Stanley Jevons first proposed the theory in articles in 1863 and 1871. Similarly, Carl Menger presented the theory in 1871. Menger explained why individuals use marginal utility to decide amongst trade-offs, but while his illustrative examples present utility as quantified, his essential assumptions do not.\nLéon Walras introduced the theory in \"Éléments d'économie politique pure\", the first part of which was published in 1874. (American John Bates Clark is also associated with the origins of Marginalism, but did little to advance the theory.\n\nAlthough the Marginal Revolution flowed from the work of Jevons, Menger, and Walras, their work might have failed to enter the mainstream were it not for a second generation of economists. In England, the second generation were exemplified by Philip Wicksteed, by William Smart, and by Alfred Marshall; in Austria by Eugen Böhm von Bawerk and by Friedrich von Wieser; in Switzerland by Vilfredo Pareto; and in America by Herbert Joseph Davenport and by Frank A. Fetter.\n\nThere were significant, distinguishing features amongst the approaches of Jevons, Menger, and Walras, but the second generation did not maintain distinctions along national or linguistic lines. The work of von Wieser was heavily influenced by that of Walras. Wicksteed was heavily influenced by Menger. Fetter referred to himself and Davenport as part of \"the American Psychological School\", named in imitation of the Austrian \"Psychological School\". (And Clark's work from this period onward similarly shows heavy influence by Menger.) William Smart began as a conveyor of Austrian School theory to English-language readers, though he fell increasingly under the influence of Marshall.\n\nBöhm-Bawerk was perhaps the most able expositor of Menger's conception. He was further noted for producing a theory of interest and of profit in equilibrium based upon the interaction of diminishing marginal utility with diminishing marginal productivity of time and with time preference. (This theory was adopted in full and then further developed by Knut Wicksell and, with modifications including formal disregard for time-preference, by Wicksell's American rival Irving Fisher.)\n\nMarshall was the second-generation marginalist whose work on marginal utility came most to inform the mainstream of neoclassical economics, especially by way of his \"Principles of Economics\", the first volume of which was published in 1890. Marshall constructed the demand curve with the aid of assumptions that utility was quantified, and that the marginal utility of money was constant (or nearly so). Like Jevons, Marshall did not see an explanation for supply in the theory of marginal utility, so he paired a marginal explanation of demand with a more classical explanation of supply, wherein costs were taken to be objectively determined. (Marshall later actively mischaracterized the criticism that these costs were themselves ultimately determined by marginal utilities.)\n\nThe doctrines of marginalism and the Marginal Revolution are often interpreted as a response to the rise of the worker's movement, Marxian economics and the earlier (Ricardian) socialist theories of the exploitation of labour. The first volume of \"Das Kapital\" was not published until July 1867, when marginalism was already developing, but before the advent of Marxian economics, proto-marginalist ideas such as those of Gossen had largely fallen on deaf ears. It was only in the 1880s, when Marxism had come to the fore as the main economic theory of the workers' movement, that Gossen found (posthumous) recognition.\n\nAside from the rise of Marxism, E. Screpanti and S. Zamagni point to a different 'external' reason for marginalism's success, which is its successful response to the Long Depression and the resurgence of class conflict in all developed capitalist economies after the 1848-1870 period of social peace. Marginalism, Screpanti and Zamagni argue, offered a theory of the free market as perfect, as performing optimal allocation of resources, while it allowed economists to blame any adverse effects of laissez-faire economics on the interference of workers' coalitions in the proper functioning of the market.\n\nScholars have suggested that the success of the generation who followed the preceptors of the Revolution was their ability to formulate straightforward responses to Marxist economic theory. The most famous of these was that of Böhm-Bawerk, “Zum Abschluss des Marxschen Systems” (1896), but the first was Wicksteed's “The Marxian Theory of Value. \"Das Kapital\": a criticism” (1884, followed by “The Jevonian criticism of Marx: a rejoinder” in 1885). The most famous early Marxist responses were Rudolf Hilferding's \"Böhm-Bawerks Marx-Kritik\" (1904) and \"The Economic Theory of the Leisure Class\" (1914) by Nikolai Bukharin.\n\nIn his 1881 work \"Mathematical Psychics\", Francis Ysidro Edgeworth presented the indifference curve, deriving its properties from marginalist theory which assumed utility to be a differentiable function of quantified goods and services. But it came to be seen that indifference curves could be considered as somehow \"given\", without bothering with notions of utility.\n\nIn 1915, Eugen Slutsky derived a theory of consumer choice solely from properties of indifference curves. Because of the World War, the Bolshevik Revolution, and his own subsequent loss of interest, Slutsky's work drew almost no notice, but similar work in 1934 by John Hicks and R. G. D. Allen derived much the same results and found a significant audience. (Allen subsequently drew attention to Slutsky's earlier accomplishment.)\n\nAlthough some of the third generation of Austrian School economists had by 1911 rejected the quantification of utility while continuing to think in terms of marginal utility, most economists presumed that utility must be a sort of quantity. Indifference curve analysis seemed to represent a way of dispensing with presumptions of quantification, albeït that a seemingly arbitrary assumption (admitted by Hicks to be a \"rabbit out of a hat\") about decreasing marginal rates of substitution would then have to be introduced to have convexity of indifference curves.\n\nFor those who accepted that superseded marginal utility analysis had been superseded by indifference curve analysis, the former became at best somewhat analogous to the Bohr model of the atom—perhaps pedagogically useful, but “old fashioned” and ultimately incorrect.\n\nWhen Cramer and Bernoulli introduced the notion of diminishing marginal utility, it had been to address a paradox of gambling, rather than the paradox of value. The marginalists of the revolution, however, had been formally concerned with problems in which there was neither risk nor uncertainty. So too with the indifference curve analysis of Slutsky, Hicks, and Allen.\n\nThe expected utility hypothesis of Bernoulli \"et alii\" was revived by various 20th century thinkers, including Frank Ramsey (1926), John von Neumann and Oskar Morgenstern (1944), and Leonard Savage (1954). Although this hypothesis remains controversial, it brings not merely utility but a quantified conception thereof back into the mainstream of economic thought, and would dispatch the Ockhamistic argument. (It should perhaps be noted that, in expected utility analysis, the “law” of diminishing marginal utility corresponds to what is called “risk aversion”.)\n\nKarl Marx died before marginalism became the interpretation of economic value accepted by mainstream economics. His theory was based on the labor theory of value, which distinguishes between exchange value and use value. In his \"Capital\" he rejected the explanation of long-term market values by supply and demand:\n\nIn his early response to marginalism, Nikolai Bukharin argued that \"the subjective evaluation from which price is to be derived really starts from this price\", concluding:\n\nSimilarly a later Marxist critic, Ernest Mandel, argued that marginalism was \"divorced from reality\", ignored the role of production, and that:\n\nMaurice Dobb argued that prices derived through marginalism depend on the distribution of income. The ability of consumers to express their preferences is dependent on their spending power. As the theory asserts that prices arise in the act of exchange, Dobb argues that it cannot explain how the distribution of income affects prices and consequently cannot explain prices.\n\nDobb also criticized the \"motives\" behind marginal utility theory. Jevons wrote, for example, \"so far as is consistent with the inequality of wealth in every community, all commodities are distributed by exchange so as to produce the maximum social benefit.\" (See Fundamental theorems of welfare economics.) Dobb contended that this statement indicated that marginalism is intended to insulate market economics from criticism by making prices the natural result of the given income distribution.\n\nSome economists strongly influenced by the Marxian tradition such as Oskar Lange, Włodzimierz Brus, and Michał Kalecki have attempted to integrate the insights of classical political economy, marginalism, and neoclassical economics. They believed that Marx lacked a sophisticated theory of prices, and neoclassical economics lacked a theory of the social frameworks of economic activity. Some other Marxists have also argued that on one level there is no conflict between marginalism and Marxism: one could employ a marginalist theory of supply and demand within the context of a “big picture” understanding of the Marxist notion that capitalists exploit surplus labor.\n\n\n"}
{"id": "315426", "url": "https://en.wikipedia.org/wiki?curid=315426", "title": "Mediocrity principle", "text": "Mediocrity principle\n\nThe mediocrity principle is the philosophical notion that \"if an item is drawn at random from one of several sets or categories, it's likelier to come from the most numerous category than from any one of the less numerous categories\". The principle has been taken to suggest that there is nothing very unusual about the evolution of the Solar System, Earth's history, the evolution of biological complexity, human evolution, or any one nation. It is a heuristic in the vein of the Copernican principle, and is sometimes used as a philosophical statement about the place of humanity. The idea is to assume mediocrity, rather than starting with the assumption that a phenomenon is special, privileged, exceptional, or even superior.\n\nThe mediocrity principle suggests, given the existence of life on Earth, that life typically exists on Earth-like planets throughout the universe.\n\nThe mediocrity principle is in contrast with the anthropic principle, which asserts that the presence of an intelligent observer (humans) limits the circumstances to bounds under which intelligent life can be observed to exist, no matter how improbable. Both stand in contrast to the fine-tuning hypothesis, which asserts that the natural conditions for intelligent life are implausibly rare.\n\nThe mediocrity principle implies that Earth-like environments are necessarily common, based in part on the evidence of any happening at all, whereas the anthropic principle suggests that no assertion can be made about the probability of intelligent life based on a sample set of one (self-described) example, who are necessarily capable of making such an assertion about themselves.\n\nIt is also possible to handle the Mediocrity Principle as a statistical problem, a case of a single Data point statistics, also present in the German tank problem.\n\nDavid Deutsch argues that the mediocrity principle is incorrect from a physical point of view, in reference to either humanity's part of the universe or to its species. Deutsch refers to Stephen Hawking's quote: \"The human race is just a chemical scum on a moderate-sized planet, orbiting around a very average star in the outer suburb of one among a hundred billion galaxies\". Deutsch wrote that Earth's neighborhood in the universe is not typical (80% of the universe's matter is dark matter) and that a concentration of mass such as the Solar System is an \"isolated, uncommon phenomenon\". He also disagrees with Richard Dawkins, who considers that humans, because of natural evolution, are limited to the capabilities of their species. Deutsch responds that even though evolution did not give humans the ability to detect neutrinos, scientists can currently detect them, which significantly expands their capabilities beyond what is available as a result of evolution.\n\n"}
{"id": "1684561", "url": "https://en.wikipedia.org/wiki?curid=1684561", "title": "Method of loci", "text": "Method of loci\n\nThe method of loci (\"loci\" being Latin for \"places\") is a method of memory enhancement which uses visualizations with the use of spatial memory, familiar information about one's environment, to quickly and efficiently recall information. The method of loci is also known as the memory journey, memory palace, or mind palace technique. This method is a mnemonic device adopted in ancient Roman and Greek rhetorical treatises (in the anonymous \"Rhetorica ad Herennium\", Cicero's \"De Oratore\", and Quintilian's \"Institutio Oratoria\"). Many memory contest champions claim to use this technique to recall faces, digits, and lists of words.\n\nThe term is most often found in specialised works on psychology, neurobiology, and memory, though it was used in the same general way at least as early as the first half of the nineteenth century in works on rhetoric, logic, and philosophy. John O'Keefe and Lynn Nadel refer to:'the method of loci', an imaginal technique known to the ancient Greeks and Romans and described by Yates (1966) in her book \"The Art of Memory\" as well as by Luria (1969). In this technique the subject memorizes the layout of some building, or the arrangement of shops on a street, or any geographical entity which is composed of a number of discrete loci. When desiring to remember a set of items the subject 'walks' through these loci in their imagination and commits an item to each one by forming an image between the item and any feature of that locus. Retrieval of items is achieved by 'walking' through the loci, allowing the latter to activate the desired items. The efficacy of this technique has been well established (Ross and Lawrence 1968, Crovitz 1969, 1971, Briggs, Hawkins and Crovitz 1970, Lea 1975), as is the minimal interference seen with its use.\n\nThe items to be remembered in this mnemonic system are mentally associated with specific physical locations. The method relies on memorized spatial relationships to establish order and recollect memorial content. It is also known as the \"Journey Method\", used for storing lists of related items, or the \"Roman Room\" technique, which is most effective for storing unrelated information.\n\nMany effective memorisers today use the \"method of loci\" to some degree. Contemporary memory competition, in particular the World Memory Championship, was initiated in 1991 and the first United States championship was held in 1997. Part of the competition requires committing to memory and recalling a sequence of digits, two-digit numbers, alphabetic letters, or playing cards. In a simple method of doing this, contestants, using various strategies well before competing, commit to long-term memory a unique vivid image associated with each item. They have also committed to long-term memory a familiar route with firmly established stop-points or loci. Then in the competition they need only deposit the image that they have associated with each item at the loci. To recall, they retrace the route, \"stop\" at each locus, and \"observe\" the image. They then translate this back to the associated item. For example, Ed Cooke, a World Memory Champion Competitor, describes to Josh Foer in his book \"Moonwalking with Einstein\" how he uses the method of loci. First, he describes a very familiar location where he can clearly remember many different smaller locations like his sink in his childhood home or his dog's bed. Cooke also advises that the more outlandish and vulgar the symbol used to memorize the material, the more likely it will stick.\n\nMemory champions elaborate on this by combining images. Eight-time World Memory Champion Dominic O'Brien uses this technique. The 2006 World Memory Champion, Clemens Mayer, used a 300-point-long journey through his house for his world record in \"number half marathon\", memorising 1040 random digits in a half-hour. Gary Shang has used the method of loci to memorise pi to over 65,536 (2) digits.\n\nUsing this technique a person with ordinary memorisation capabilities, after establishing the route stop-points and committing the associated images to long-term memory, with less than an hour of practice, can remember the sequence of a shuffled deck of cards. The world record for this is held by Simon Reinhard at 21.19 seconds.\n\nThe technique is taught as a metacognitive technique in learning-to-learn courses. It is generally applied to encoding the key ideas of a subject. Two approaches are:\n\nThe method of loci has also been shown to help sufferers of depression remember positive, self-affirming memories.\n\nA study at the University of Maryland evaluated participants ability to accurately recall two sets of familiar faces, using a traditional desktop, and with a head-mounted display. The study was designed to leverage the method of loci technique, with virtual environments resembling memory palaces. The study found an 8.8% recall improvement in favor of the head-mounted display, in part due to participants being able to leverage their vestibular and proprioceptive sensations.\n\nThe \"Rhetorica ad Herennium\" and most other sources recommend that the method of loci should be integrated with elaborative encoding (i.e., adding visual, auditory, or other details) to strengthen memory. However, due to the strength of spatial memory, simply mentally placing objects in real or imagined locations without further elaboration can be effective for simple associations.\n\nA variation of the \"method of loci\" involves creating imaginary locations (houses, palaces, roads, and cities) to which the same procedure is applied. It is accepted that there is a greater cost involved in the initial setup, but thereafter the performance is in line with the standard loci method. The purported advantage is to create towns and cities that each represent a topic or an area of study, thus offering an efficient filing of the information and an easy path for the regular review necessary for long term memory storage.\n\nSomething that is likely a reference to the \"method of loci\" techniques survives to this day in the common English phrases \"in the first place\", \"in the second place\", and so forth.\n\nThe technique is also used for second language vocabulary learning, as polyglot Timothy Doner described in his 2014 TED talk. The method is further described in Anthony Metiver's book \"How to learn and memorise German vocabulary\". What the author suggests is creating a memory palace for each letter of the German alphabet. Each memory palace then shall include a number of loci where an entry (a word or a phrase) can be stored and recalled whenever you need it.\n\nThe designation is not used with strict consistency. In some cases it refers broadly to what is otherwise known as the art of memory, the origins of which are related, according to tradition, in the story of Simonides of Ceos and the collapsing banquet hall. For example, after relating the story of how Simonides relied on remembered seating arrangements to call to mind the faces of recently deceased guests, Stephen M. Kosslyn remarks \"[t]his insight led to the development of a technique the Greeks called the method of loci, which is a systematic way of improving one's memory by using imagery.\" Skoyles and Sagan indicate that \"an ancient technique of memorization called Method of Loci, by which memories are referenced directly onto spatial maps\" originated with the story of Simonides. Referring to mnemonic methods, Verlee Williams mentions, \"One such strategy is the 'loci' method, which was developed by Simonides, a Greek poet of the fifth and sixth centuries BC.\" Loftus cites the foundation story of Simonides (more or less taken from Frances Yates) and describes some of the most basic aspects of the use of space in the art of memory. She states, \"This particular mnemonic technique has come to be called the \"method of loci\". While place or position certainly figured prominently in ancient mnemonic techniques, no designation equivalent to \"method of loci\" was used exclusively to refer to mnemonic schemes relying upon space for organization.\n\nIn other cases the designation is generally consistent, but more specific: \"The Method of Loci is a Mnemonic Device involving the creation of a Visual Map of one's house.\"\n\nThis term can be misleading: the ancient principles and techniques of the art of memory, hastily glossed in some of the works, cited above, depended equally upon images \"and\" places. The designator \"method of loci\" does not convey the equal weight placed on both elements. Training in the art or arts of memory as a whole, as attested in classical antiquity, was far more inclusive and comprehensive in the treatment of this subject.\n\nBrain scans of \"superior memorizers\", 90% of whom use the method of loci technique, have shown that it involves activation of regions of the brain involved in spatial awareness, such as the medial parietal cortex, retrosplenial cortex, and the right posterior hippocampus. The medial parietal cortex is most associated with encoding and retrieving of information. Patients who have medial parietal cortex damage have trouble linking landmarks with certain locations; many of these patients are unable to give or follow directions and often get lost. The retrosplenial cortex is also linked to memory and navigation. In one study on the effects of selective granular retrosplenial cortex lesions in rats, the researcher found that damage to the retrosplenial cortex led to impaired spatial learning abilities. Rats with damage to this area failed to recall which areas of the maze they had already visited, rarely explored different arms of the maze, almost never recalled the maze in future trials, and took longer to reach the end of the maze, as compared to rats with a fully working retrosplenial cortex.\n\nIn a classic study in cognitive neuroscience, O'Keefe and Nadel proposed \"that the hippocampus is the core of a neural memory system providing an objective spatial framework within which the items and events of an organism's experience are located and interrelated.\"\n\nIn a more recent study, memory champions during resting periods did not exhibit specific regional brain differences, but distributed functional brain network connectivity changes compared to control subjects. When volunteers trained use of the method of loci for six weeks, the training-induced changes in brain connectivity were similar to the brain network organization that distinguished memory champions from controls.\n\nFictional portrayals of the method of loci extend as far back as ancient Greek myths. The method of loci also features prominently in the BBC series \"Sherlock\", in which the titular main character uses a \"mind palace\" to store information. In the original Arthur Conan Doyle stories, Sherlock Holmes referred to his brain as an attic. In \"Hannibal Rising\" by Thomas Harris, a detailed description of Hannibal Lecter's memory palace is provided.\n"}
{"id": "57122", "url": "https://en.wikipedia.org/wiki?curid=57122", "title": "Multiplication table", "text": "Multiplication table\n\nIn mathematics, a multiplication table (sometimes, less formally, a times table) is a mathematical table used to define a multiplication operation for an algebraic system.\n\nThe decimal multiplication table was traditionally taught as an essential part of elementary arithmetic around the world, as it lays the foundation for arithmetic operations with base-ten numbers. Many educators believe it is necessary to memorize the table up to 9 × 9.\n\nThe oldest known multiplication tables were used by the Babylonians about 4000 years ago. However, they used a base of 60. The oldest known tables using a base of 10 are the Chinese decimal multiplication table on bamboo strips dating to about 305 BC, during China's Warring States period.\nThe multiplication table is sometimes attributed to the ancient Greek mathematician Pythagoras (570–495 BC). It is also called the Table of Pythagoras in many languages (for example French, Italian and at one point even Russian), sometimes in English. The Greco-Roman mathematician Nichomachus (60–120 AD), a follower of Neopythagoreanism, included a multiplication table in his \"Introduction to Arithmetic\", whereas the oldest surviving Greek multiplication table is on a wax tablet dated to the 1st century AD and currently housed in the British Museum.\n\nIn 493 AD, Victorius of Aquitaine wrote a 98-column multiplication table which gave (in Roman numerals) the product of every number from 2 to 50 times and the rows were \"a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144.\"\n\nIn his 1820 book \"The Philosophy of Arithmetic\", mathematician John Leslie published a multiplication table up to 99 × 99, which allows numbers to be multiplied in pairs of digits at a time. Leslie also recommended that young pupils memorize the multiplication table up to 25 × 25. The illustration below shows a table up to 12 × 12, which is a size commonly used in schools.\n\nThe traditional rote learning of multiplication was based on memorization of columns in the table, in a form like\n\n<poem>\n</poem>\nThis form of writing the multiplication table in columns with complete number sentences is still used in some countries, such as Bosnia and Herzegovina, instead of the modern grid above.\n\nThere is a pattern in the multiplication table that can help people to memorize the table more easily. It uses the figures below:\n\nFigure 1 is used for multiples of 1, 3, 7, and 9. Figure 2 is used for the multiples of 2, 4, 6, and 8. These patterns can be used to memorize the multiples of any number from 0 to 10, except 5. As you would start on the number you are multiplying, when you multiply by 0, you stay on 0 (0 is external and so the arrows have no effect on 0, otherwise 0 is used as a link to create a perpetual cycle). The pattern also works with multiples of 10, by starting at 1 and simply adding 0, giving you 10, then just apply every number in the pattern to the \"tens\" unit as you would normally do as usual to the \"ones\" unit.\nFor example, to recall all the multiples of 7:\n\n\nTables can also define binary operations on groups, fields, rings, and other algebraic systems. In such contexts they can be called Cayley tables. Here are the addition and multiplication tables for the finite field Z.\n\nFor every natural number \"n\", there are also addition and multiplication tables for the ring Z.\n\nFor other examples, see group, and octonion.\n\nThe Chinese multiplication table consists of eighty-one sentences with four or five Chinese characters per sentence, making it easy for children to learn by heart. A shorter version of the table consists of only forty-five sentences, as terms such as \"nine eights beget seventy-two\" are identical to \"eight nines beget seventy-two\" so there is no need to learn them twice.\n\nA bundle of 21 bamboo slips dated 305 BC in the Warring States period in the Tsinghua Bamboo Slips (清华简) collection is the world's earliest known example of a decimal multiplication table.\nIn 1989, the National Council of Teachers of Mathematics (NCTM) developed new standards which were based on the belief that all students should learn higher-order thinking skills, and which recommended reduced emphasis on the teaching of traditional methods that relied on rote memorization, such as multiplication tables. Widely adopted texts such as Investigations in Numbers, Data, and Space (widely known as TERC after its producer, Technical Education Research Centers) omitted aids such as multiplication tables in early editions. NCTM made it clear in their 2006 Focal Points that basic mathematics facts must be learned, though there is no consensus on whether rote memorization is the best method.\n\n"}
{"id": "161019", "url": "https://en.wikipedia.org/wiki?curid=161019", "title": "Negation", "text": "Negation\n\nIn logic, negation, also called the logical complement, is an operation that takes a proposition formula_1 to another proposition \"not formula_1\", written formula_3 (¬P), which is interpreted intuitively as being true when formula_1 is false, and false when formula_1 is true. Negation is thus a unary (single-argument) logical connective. It may be applied as an operation on notions, propositions, truth values, or semantic values more generally. In classical logic, negation is normally identified with the truth function that takes \"truth\" to \"falsity\" and vice versa. In intuitionistic logic, according to the Brouwer–Heyting–Kolmogorov interpretation, the negation of a proposition formula_1 is the proposition whose proofs are the refutations of formula_1.\n\nNo agreement exists as to the possibility of defining negation, as to its logical status, function, and meaning, as to its field of applicability..., and as to the interpretation of the negative judgment, (F.H. Heinemann 1944).\n\n\"Classical negation\" is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" when its operand is false and a value of \"false\" when its operand is true. So, if statement formula_1 is true, then formula_3 (pronounced \"not P\") would therefore be false; and conversely, if formula_3 is false, then formula_1 would be true.\n\nThe truth table of formula_3 is as follows:\n\nNegation can be defined in terms of other logical operations. For example, formula_3 can be defined as formula_14 (where formula_15 is logical consequence and formula_16 is absolute falsehood). Conversely, one can define formula_16 as formula_18 for any proposition formula_19 (where formula_20 is logical conjunction). The idea here is that any contradiction is false. While these ideas work in both classical and intuitionistic logic, they do not work in paraconsistent logic, where contradictions are not necessarily false. In classical logic, we also get a further identity, formula_21 can be defined as formula_22, where formula_23 is logical disjunction.\n\nAlgebraically, classical negation corresponds to complementation in a Boolean algebra, and intuitionistic negation to pseudocomplementation in a Heyting algebra. These algebras provide a semantics for classical and intuitionistic logic respectively.\n\nThe negation of a proposition formula_1 is notated in different ways in various contexts of discussion and fields of application. Among these variants are the following:\n\nThe notation N\"p\" is Łukasiewicz notation.\n\nIn set theory formula_25 is also used to indicate 'not member of': formula_26 is the set of all members of formula_27 that are not members of formula_28.\n\nNo matter how it is notated or symbolized, the negation formula_3 can be read as \"it is not the case that formula_1\", \"not that formula_1\", or usually more simply as \"not formula_1\".\n\nWithin a system of classical logic, double negation, that is, the negation of the negation of a proposition formula_1, is logically equivalent to formula_1. Expressed in symbolic terms, formula_35. In intuitionistic logic, a proposition implies its double negation but not conversely. This marks one important difference between classical and intuitionistic negation. Algebraically, classical negation is called an involution of period two.\n\nHowever, in intuitionistic logic we do have the equivalence of formula_36. Moreover, in the propositional case, a sentence is classically provable if its double negation is intuitionistically provable. This result is known as Glivenko's theorem.\n\nDe Morgan's laws provide a way of distributing negation over disjunction and conjunction :\n\nLet formula_39 denote the logical xor operation. In Boolean algebra, a linear function is one such that:\n\nIf there exists formula_40,\nformula_41,\nfor all formula_42.\n\nAnother way to express this is that each variable always makes a difference in the truth-value of the operation or it never makes a difference. Negation is a linear logical operator.\n\nIn Boolean algebra a self dual function is one such that:\n\nformula_43 for all\nformula_44.\nNegation is a self dual logical operator.\n\nThere are a number of equivalent ways to formulate rules for negation. One usual way to formulate classical negation in a natural deduction setting is to take as primitive rules of inference \"negation introduction\" (from a derivation of formula_1 to both formula_19 and formula_47, infer formula_3; this rule also being called \"reductio ad absurdum\"), \"negation elimination\" (from formula_1 and formula_3 infer formula_19; this rule also being called \"ex falso quodlibet\"), and \"double negation elimination\" (from formula_52 infer formula_1). One obtains the rules for intuitionistic negation the same way but by excluding double negation elimination.\n\nNegation introduction states that if an absurdity can be drawn as conclusion from formula_1 then formula_1 must not be the case (i.e. formula_1 is false (classically) or refutable (intuitionistically) or etc.). Negation elimination states that anything follows from an absurdity. Sometimes negation elimination is formulated using a primitive absurdity sign formula_16. In this case the rule says that from formula_1 and formula_3 follows an absurdity. Together with double negation elimination one may infer our originally formulated rule, namely that anything follows from an absurdity.\n\nTypically the intuitionistic negation formula_3 of formula_1 is defined as formula_14. Then negation introduction and elimination are just special cases of implication introduction (conditional proof) and elimination (modus ponens). In this case one must also add as a primitive rule \"ex falso quodlibet\".\n\nAs in mathematics, negation is used in computer science to construct logical statements.\n\nThe \"codice_1\" signifies logical NOT in B, C, and languages with a C-inspired syntax such as C++, Java, JavaScript, Perl, and PHP. \"codice_2\" is the operator used in ALGOL 60, BASIC, and languages with an ALGOL- or BASIC-inspired syntax such as Pascal, Ada, Eiffel and Seed7. Some languages (C++, Perl, etc.) provide more than one operator for negation. A few languages like PL/I and Ratfor use codice_3 for negation. Some modern computers and operating systems will display codice_3 as codice_1 on files encoded in ASCII. Most modern languages allow the above statement to be shortened from codice_6 to codice_7, which allows sometimes, when the compiler/interpreter is not able to optimize it, faster programs.\n\nIn computer science there is also \"bitwise negation\". This takes the value given and switches all the binary 1s to 0s and 0s to 1s. See bitwise operation. This is often used to create ones' complement or \"codice_8\" in C or C++ and two's complement (just simplified to \"codice_9\" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).\n\nTo get the absolute (positive equivalent) value of a given integer the following would work as the \"codice_9\" changes it from negative to positive (it is negative because \"codice_11\" yields true)\n\nTo demonstrate logical negation:\n\nInverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).\n\nThis convention occasionally surfaces in written speech, as computer-related slang for \"not\". The phrase codice_12, for example, means \"not voting\".\n\nIn Kripke semantics where the semantic values of formulae are sets of possible worlds, negation can be taken to mean set-theoretic complementation. (See also possible world semantics.)\n\n\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "32108559", "url": "https://en.wikipedia.org/wiki?curid=32108559", "title": "Negative hyperconjugation", "text": "Negative hyperconjugation\n\nIn organic chemistry, negative hyperconjugation is the donation of electron density from a filled π- or p-orbital to a neighboring σ-orbital. This phenomenon, a type of resonance, can stabilize the molecule or transition state. It also causes an elongation of the σ-bond by adding electron density to its antibonding orbital.\n\nNegative hyperconjugation is most commonly observed when the σ-orbital is located on certain C–F or C–O bonds, and does not occur to an appreciable extent with normal C–H bonds.\n\nIn negative hyperconjugation, the electron density flows in the \"opposite\" direction (from π- or p-orbital to empty σ-orbital) than it does in the more common hyperconjugation (from σ-orbital to empty p-orbital).\n\n"}
{"id": "13065509", "url": "https://en.wikipedia.org/wiki?curid=13065509", "title": "Paired opposites", "text": "Paired opposites\n\nPaired opposites are an ancient, pre-Socratic method of establishing thesis, antithesis and synthesis in terms of a standard for what is right and proper in natural philosophy.\n\nScalar ranges and coordinate systems are paired opposites within sets. Incorporating dimensions of positive and negative numbers and exponents, or expanding x, y and z coordinates, by adding a fourth dimension of time allows a resolution of position relative to the standard of the scale which is often taken as 0,0,0,0 with additional dimensions added as referential scales are expanded from space and time to mass and energy.\n\nAncient systems frequently scaled their degree of opposition by rate of increase or rate of decrease. Linear increase was enhanced by doubling systems. An acceleration in the rate of increase or decrease could be analyzed arithmetrically, geometrically, or through a wide range of other numerical and physical analysis. Arithmetic and geometric series, and other methods of rating proportionate expansion or contraction could be thought of as convergent or divergent toward a position.\n\nThough unit quantities were first defined by spatial dimensions, and then expanded by adding coordinates of time, the weight or mass a given spatial dimension could contain was also considered and even in antiquity, conditions under which the standard would be established such as at a given temperature, distance from sea level, or density were added.\n\nRates of change over time were then considered as either indexes of production or depletion\n\nPaired opposites are used as poetic diction meaning \"everything\". Common phrases incorporated paired opposites in English include \"all creatures great and small,\" \"working for the man every night and day,\" \"more things in heaven and Earth\" \"searching high and low\" \"in sickness and in health\". In Greek literature, Homer uses the device when he lets Telemachus say, \"I know all things, the good and the evil\" (Od.20:309-10).\nThe same phrase is used in Hebrew in text of Genesis, referring to the Tree of the knowledge of good and evil.\n\nIn quantum mechanics, as well as some fields of mathematics, conjugate variables are a form of paired opposites, in which knowledge of one precludes knowledge of the other. A standard example is the relation between position (x) and momentum (p), which can be expressed in terms of the uncertainty principle as formula_1.\n"}
{"id": "36606973", "url": "https://en.wikipedia.org/wiki?curid=36606973", "title": "Plastic Principle", "text": "Plastic Principle\n\nThe Plastic Principle is an idea introduced into Western thought by the English philosopher Ralph Cudworth (1617–1689) to explain the function of nature and life in the face of both the mechanism and materialism of the Enlightenment. It is a dynamic functional power that contains all of natural law, and is both sustentative and generative, organizing matter according to Platonic Ideas, that is, archetypes that lie beyond the physical realm coming from the Mind of God or Deity, the ground of Being.\n\nThe role of nature was one faced by philosophers in the Age of Reason or Enlightenment. The prevailing view was either that of the Church of a personal deity intervening in his creation, producing miracles, or an ancient pantheism (atheism relative to theism) – deity pervading all things and existing in all things. However, the \"ideas of an all-embracing providential care of the world and of one universal vital force capable of organizing the world from within.\" presented difficulties for philosophers of a spiritual as well as materialistic bent.\n\nThe Cartesian idea of nature as mechanical, and Hobbes' materialistic views were countered by the English philosopher, Ralph Cudworth (1617–1689), who, in his \"True intellectual system of the universe\" (1678), addressing the tension between theism and atheism, took both the Stoic idea of Divine Reason poured into the world, and the Platonic idea of the world soul (\"anima mundi\") to posit a power that was polaric – \"either as a ruling but separate mind or as an informing vital principle – either nous hypercosmios or nous enkosmios.\n\nCudworth was a member of the Cambridge Platonists, a group of English seventeenth-century thinkers associated with the University of Cambridge who were stimulated by Plato's teachings but also were aware or and influenced by Descartes, Hobbes, Bacon, Boyle and Spinoza. The other important philosopher of this group was Henry More (1614–1687). More held that spiritual substance or mind controlled inert matter. Out of his correspondence with Descartes, he developed the idea that everything, whether material or non, had extension, an example of the latter being space, which is infinite (Newton) and which then is correlative to the idea of God (set out in his Enchiridion metaphysicum 1667). In developing this idea, More also introduced a causal agent between God and substance, or Nature in his Hylarchic Principle, derived from Plato's \"anima mundi\" or world soul, and the Stoic's pneuma, which encapsulates the laws of nature, both for inert and vital nature, and involves a sympathetic resonance between soul (\"psyche\") and \"soma\".\n\nLike More, Cudworth put forward the idea of 'the Plastick Life of Nature', a formative principle that contains both substance and the laws of motion, as well as a nisus or direction that accounts for design and goal in the natural world. He was stimulated by the Cartesian idea of the mind as self-consciousness to see God as consciousness. He first analysed four forms of atheism from ancient times to present, and showed that all misunderstood the principle of life and knowledge, which involved unsentient activity and self-consciousness.\n\nAll of the atheistic approaches posted nature as unconscious, which for Cudworth was ontologically unsupportable, as a principle that was supposed to be the ultimate source of life and meaning could only be itself self-conscious and knowledgeable, that is, rational, otherwise creation or nature degenerates into inert matter set in motion by random external forces (Coleridge's 'chance whirlings of unproductive particles'). Cudworth saw nature as a vegetative power endowed with plastic (forming) and spermatic (generative) forces, but one with Mind, or a self-conscious knowledge. This idea would later emerge in the Romantic period in German science as Blumenbach's \"Bildungstreib\" (generative power) and the \"Lebenskraft\" (or \"Bildungskraft\").\n\nThe essence of atheism for Cudworth was the view that matter was self-active and self-sufficient, whereas for Cudworth the plastic power was unsentient and under the direct control of the universal Mind or \"Logos\". For him atheism, whether mechanical or material could not solve the \"phenomenon of nature.\" Henry More argued that atheism made each substance independent and self-acting such that it 'deified' matter. Cudworth argued that materialism/mechanism reduced \"substance to a corporeal entity, its activity to causal determinism, and each single thing to fleeting appearances in a system dominated by material necessity.\"\n\nCudworth had the idea of a general plastic nature of the world, containing natural laws to keep all of nature, inert and vital in orderly motion, and particular plastic natures in particular entities, which serve as 'Inward Principles' of growth and motion, but ascribes it to the Platonic tradition:\nFurther, Cudsworth's plastic principle was also a functional polarity. As he wrote:\n\nAs another historian notes in conclusion, \"Cudworth’s theory of plastic natures is offered as an alternative to the interpretation of all of nature as either governed by blind chance, or, on his understanding of the Malebranchean view, as micro-managed by God.\"\n\nCudworth's plastic principle also involves a theory of mind that is active, that is, God or the Supreme Mind is \"the spermatic reason\" which gives rise to individual mind and reason. Human mind can also create, and has access to spiritual or super-sensible 'Ideas' in the Platonic sense. Cudworth challenged Hobbesian determinism in arguing that will is not distinct from reason, but a power to act that is internal, and therefore, the voluntary will function involves self-determination, not external compulsion, though we have the power to act either in accordance with God's will or not. Cudworth's 'hegemonikon' (taken from Stoicism) is a function within the soul that combines the higher functions of the soul (voluntary will and reason) on the one hand with the lower animal functions (instinct), and also constitutes the whole person, thus bridging the Cartesian dualism of body and soul or \"psyche\" and \"soma\". This idea provided the basis for a concept of self-awareness and identity of an individual that is self-directed and autonomous, an idea that anticipates John Locke.\n\nLocke examined how man came to knowledge via stimulus (rather than seeing ideas as inherent), which approach led to his idea of the 'thinking' mind, which is both receptive and pro-active. The first involves receiving sensations ('simple ideas') and the second by reflection – \"observation of its own inner operations\" (inner sense which leads to complex ideas), with the second activity acting upon the first. Thought is set in motion by outer stimuli which 'simple ideas' are taken up by the mind's self-activity, an \"active power\" such that the outer world can only be real-ized as action (natural cause) by the activity of consciousness. Locke also took the issue of life as lying not in substance but in the capacity of the self for consciousness, to be able to organize (associate) disparate events, that is to participate life by means of the sense experiences, which have the capacity to produce every kind of experience in consciousness. These ideas of Locke were taken over by Fichte and influenced German Romantic science and medicine. (See Romantic medicine and Brunonian system of medicine).\nThomas Reid and his \"Common Sense\" philosophy, was also influenced by Cudworth, taking his influence into the Scottish Enlightenment.\n\nBerkeley later developed the idea of a plastic life principle with his idea of an 'aether' or 'aetherial medium' that causes 'vibrations' that animate all living beings. For Berkeley, it is the very nature of this medium that generates the 'attractions' of entities to each other.\n\nBerkeley meant this 'aether' to supplant Newton's gravity as the cause of motion (neither seeing the polarity involved between two forces, as Cudworth had in his plastic principle). However, in Berkeley's conception, aether is both the movement of spirit and the motion of nature.\n\nBoth Cudworth's views and those of Berkeley were taken up by Coleridge in his metaphor of the eolian harp in his 'Effusion XXXV' as one commentator noted: \"what we see in the first manuscript is the articulation of Cudworth’s principle of plastic nature, which is then transformed in the published version into a Berkeleyan expression of the causal agency of motion performed by God’s immanent activity.\"\n\nCudworth's idea of the plastic principle and that of mind will also be taken up in a new way in the idea of emergent evolution.\n"}
{"id": "8924176", "url": "https://en.wikipedia.org/wiki?curid=8924176", "title": "Principle of nonvacuous contrast", "text": "Principle of nonvacuous contrast\n\nThe principle of nonvacuous contrast is a logical or methodological principle which requires that a genuine predicate never refer to everything, or to nothing, within its universe of discourse.\n"}
{"id": "1137736", "url": "https://en.wikipedia.org/wiki?curid=1137736", "title": "Principle of sufficient reason", "text": "Principle of sufficient reason\n\nThe principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with \"ex nihilo nihil fit\". Hamilton identified the laws of inference modus ponens with the \"law of Sufficient Reason, or of Reason and Consequent\" and modus tollens with its contrapositive expression.\n\nThe principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\n\nA sufficient explanation may be understood either in terms of \"reasons\" or \"causes,\" for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.\n\nIt is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system\nThe principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).\n\nLeibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of contradiction): \"Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction.\" Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.\n\nLeibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:\nIn contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:\n\nLeibniz also used the principle of sufficient reason to refute the idea of absolute space:\n\nI say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.\n\nThe principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.\n\nA sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called \"necessary conditions\"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.\n\nHere is how Hamilton, circa 1837–1838, expressed his \"fourth law\" in his LECT. V. LOGIC. 60–61:\n\nAccording to Schopenhauer's \"On the Fourfold Root of the Principle of Sufficient Reason\", there are four distinct forms of the principle.\n\nFirst Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.\n\nSecond Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.\n\nThird Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; \"I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four.\"\n\n\"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness.\" \n\nFourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. \"Any judgment that does not follow its previously existing ground or reason\" or any state that cannot be explained away as falling under the three previous headings \"must be produced by an act of will which has a motive.\" As his proposition in 43 states, \"Motivation is causality seen from within.\"\n\nSeveral proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least \"in general\", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and \"directionality\" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).\n\nOnce it is agreed (e.g. from a kind of an \"arrow of time\") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), \"backwards\" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).\n\n\n"}
{"id": "33920294", "url": "https://en.wikipedia.org/wiki?curid=33920294", "title": "Psychotherapy and social action model", "text": "Psychotherapy and social action model\n\nThe psychotherapy and social action model is an approach to psychotherapy characterized by concentration on past and present personal, social, and political obstacles to mental health. In particular, the goal of this therapeutic approach is to acknowledge that individual symptoms are not unique, but rather shared by people similarly oppressed and marginalized. Ultimately, the psychotherapy and social action model aims to aid clients in overcoming mental illness through personal psychotherapy, group coping, and collective social action.\n\n The psychotherapy and social action model was initially proposed by Sue Holland, a psychotherapist with a background in community action. Holland developed this framework in 1980 following her experience working with women coping with psychological disorders at a housing estate in West London. At this estate, Holland observed the psychological difficulties experienced by women, noticing that their mental health was fundamentally tied to the social and economic obstacles they encountered as females in their society. In addition, Holland took issue with the way Depression (mood) was being treated at the shelter, believing that individualized treatment, especially with the use of psychotropic medication, was not successfully addressing the root of the dysfunction for these women. Instead, Holland posited a pathway from individual treatment to sociopolitical action that empowered women to deal with their mental dysfunction both privately and socially. As such, the psychotherapy and social action model is rooted in the ideals of both traditional psychotherapy and feminist empowerment.\n\nThe square model derives from the sociological theory of the four paradigms for the analysis of social theory. Outside the frame of the model, the dichotomy of individual versus social approaches to personal well-being is represented. The two bottom cells of the square delineate the changing of individuals to conform to social convention while the two top cells of the square represent the changing of social structures as opposed to the individual.\n\nThe four cells within the frame represent the four paradigms of social theory including functionalist, interpretive, radical humanist, and radical structuralist paradigms. Functionalism here is rooted in regulation and objective thinking, and represents the individual, status-quo approach to mental health. The interpretive paradigm is characterized by an approach to understanding the social world through subjective experience, and represents psychoeducation within the psychotherapy framework. The radical humanist paradigm is defined by a radial approach to change with an emphasis on “transcending limitations of existing social arrangements.” (Burrell & Morgan, 1979, p. 32). With respect to an approach to therapy, this stage is characterized by the adoption of a social self, such that healing occurs at a group or collective level. The radical structuralist paradigm concentrates on radical change through political or economic emancipation. This is the endpoint of therapy, at which time the client is empowered to challenge sociopolitical structures that foster the conditions perpetuating the manifestation of individual mental illness within an oppressed group.\n\nTaken from her 1992 publication entitled, “From Social Abuse to Social Action: a neighborhood psychotherapy and social action project for women,” Holland formulated her four step approach to mental health and social action for women in treatment for depression as follows:\n\nAt this stage, patients endorse the status-quo characterization of the “individualized patient.” As such, they treat their disorder passively with psychotropic medication and accept the label associated with their illness.\n\nThis stage represents the first alternative to the status-quo treatment of psychiatric disorders: talk therapy. At this stage, clients and therapists are able to explore the meaning of their psychopathology and pinpoint the potential causes through individual therapy.\n\nAt this stage, the client is able to move past the personal challenges that are acknowledged and addressed in psychotherapy and discover that the challenges are universal amongst similarly marginalized individuals. Together, clients aim to acknowledge what is best for the collective.\n\nThe final stage, as the name suggests, is the point at which the collective mobilizes to change the social structures enabling their common oppression. Having changed from an individual to a collective, the clients should feel empowered to undertake social change.\n\nIncluded in this framework is the assumption that only some of the clients in this therapy will traverse all three stages. In Holland’s words, “…many will be content enough with the relief from symptoms and the freedom to get on with their personal lives which the individual therapy gives them.” (Holland, 1992, p. 73). Thus, this framework is fluid based on the personal inclinations of the client throughout the therapeutic process.\n\n• Women’s Action for Mental Health (WAMH)\n• Men’s Advice Network (MAN)\n• Travers (1997)\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3224795", "url": "https://en.wikipedia.org/wiki?curid=3224795", "title": "Scientific modelling", "text": "Scientific modelling\n\nScientific modelling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usually commonly accepted knowledge. It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, and graphical models to visualize the subject. Modelling is an essential and inseparable part of many scientific disciplines, each of which have their own ideas about specific types of modelling.\n\nThere is also an increasing attention to scientific modelling in fields such as science education, philosophy of science, systems theory, and knowledge visualization. There is growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling.\n\nA scientific model seeks to represent empirical objects, phenomena, and physical processes in a logical and objective way. All models are \"in simulacra\", that is, simplified reflections of reality that, despite being approximations, can be extremely useful. Building and disputing models is fundamental to the scientific enterprise. Complete and true representation may be impossible, but scientific debate often concerns which is the better model for a given task, e.g., which is the more accurate climate model for seasonal forecasting.\n\nAttempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system that will not produce theoretical consequences that are contrary to what is found in reality. Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.\n\nFor the scientist, a model is also a way in which the human thought processes can be amplified. For instance, models that are rendered in software allow scientists to leverage computational power to simulate, visualize, manipulate and gain intuition about the entity, phenomenon, or process being represented. Such computer models are \"in silico\". Other types of scientific models are \"in vivo\" (living models, such as laboratory rats) and \"in vitro\" (in glassware, such as tissue culture).\n\nModels are typically used when it is either impossible or impractical to create experimental conditions in which scientists can directly measure outcomes. Direct measurement of outcomes under controlled conditions (see Scientific method) will always be more reliable than modelled estimates of outcomes.\n\nWithin modelling and simulation, a model is a task-driven, purposeful simplification and abstraction of a perception of reality, shaped by physical, legal, and cognitive constraints. It is task-driven, because a model is captured with a certain question or task in mind. Simplifications leave all the known and observed entities and their relation out that are not important for the task. Abstraction aggregates information that is important, but not needed in the same detail as the object of interest. Both activities, simplification and abstraction, are done purposefully. However, they are done based on a perception of reality. This perception is already a \"model\" in itself, as it comes with a physical constraint. There are also constraints on what we are able to legally observe with our current tools and methods, and cognitive constraints which limit what we are able to explain with our current theories. This model comprises the concepts, their behavior, and their relations in formal form and is often referred to as a conceptual model. In order to execute the model, it needs to be implemented as a computer simulation. This requires more choices, such as numerical approximations or the use of heuristics. Despite all these epistemological and computational constraints, simulation has been recognized as the third pillar of scientific methods: theory building, simulation, and experimentation.\n\nA simulation is the implementation of a model. A steady state simulation provides information about the system at a specific instant in time (usually at equilibrium, if such a state exists). A dynamic simulation provides information over time. A simulation brings a model to life and shows how a particular object or phenomenon will behave. Such a simulation can be useful for testing, analysis, or training in those cases where real-world systems or concepts can be represented by models.\n\nStructure is a fundamental and sometimes intangible notion covering the recognition, observation, nature, and stability of patterns and relationships of entities. From a child's verbal description of a snowflake, to the detailed scientific analysis of the properties of magnetic fields, the concept of structure is an essential foundation of nearly every mode of inquiry and discovery in science, philosophy, and art.\n\nA system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. In general, a system is a construct or collection of different elements that together can produce results not obtainable by the elements alone. The concept of an 'integrated whole' can also be stated in terms of a system embodying a set of relationships which are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. There are two types of system models: 1) discrete in which the variables change instantaneously at separate points in time and, 2) continuous where the state variables change continuously with respect to time.\n\nModelling is the process of generating a model as a conceptual representation of some phenomenon. Typically a model will deal with only some aspects of the phenomenon in question, and two models of the same phenomenon may be essentially different—that is to say, that the differences between them comprise more than just a simple renaming of components.\n\nSuch differences may be due to differing requirements of the model's end users, or to conceptual or aesthetic differences among the modellers and to contingent decisions made during the modelling process. Considerations that may influence the structure of a model might be the modeller's preference for a reduced ontology, preferences regarding statistical models versus deterministic models, discrete versus continuous time, etc. In any case, users of a model need to understand the assumptions made that are pertinent to its validity for a given use.\n\nBuilding a model requires abstraction. Assumptions are used in modelling in order to specify the domain of application of the model. For example, the special theory of relativity assumes an inertial frame of reference. This assumption was contextualized and further explained by the general theory of relativity. A model makes accurate predictions when its assumptions are valid, and might well not make accurate predictions when its assumptions do not hold. Such assumptions are often the point with which older theories are succeeded by new ones (the general theory of relativity works in non-inertial reference frames as well).\n\nThe term \"assumption\" is actually broader than its standard use, etymologically speaking. The Oxford English Dictionary (OED) and online Wiktionary indicate its Latin source as \"assumere\" (\"accept, to take to oneself, adopt, usurp\"), which is a conjunction of \"ad-\" (\"to, towards, at\") and \"sumere\" (to take). The root survives, with shifted meanings, in the Italian \"sumere\" and Spanish \"sumir\". In the OED, \"assume\" has the senses of (i) “investing oneself with (an attribute), ” (ii) “to undertake” (especially in Law), (iii) “to take to oneself in appearance only, to pretend to possess,” and (iv) “to suppose a thing to be.” Thus, \"assumption\" connotes other associations than the contemporary standard sense of “that which is assumed or taken for granted; a supposition, postulate,” and deserves a broader analysis in the philosophy of science.\n\nA model is evaluated first and foremost by its consistency to empirical data; any model inconsistent with reproducible observations must be modified or rejected. One way to modify the model is by restricting the domain over which it is credited with having high validity. A case in point is Newtonian physics, which is highly useful except for the very small, the very fast, and the very massive phenomena of the universe. However, a fit to empirical data alone is not sufficient for a model to be accepted as valid. Other factors important in evaluating a model include:\nPeople may attempt to quantify the evaluation of a model using a utility function.\n\nVisualization is any technique for creating images, diagrams, or animations to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of man. Examples from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci's revolutionary methods of technical drawing for engineering and scientific purposes.\n\nSpace mapping refers to a methodology that employs a \"quasi-global\" modeling formulation to link companion \"coarse\" (ideal or low-fidelity) with \"fine\" (practical or high-fidelity) models of different complexities. In engineering optimization, space mapping aligns (maps) a very fast coarse model with its related expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model. The alignment process iteratively refines a \"mapped\" coarse model (surrogate model).\n\n\n\n\nOne application of scientific modelling is the field of modelling and simulation, generally referred to as \"M&S\". M&S has a spectrum of applications which range from concept development and analysis, through experimentation, measurement and verification, to disposal analysis. Projects and programs may use hundreds of different simulations, simulators and model analysis tools.\nThe figure shows how Modelling and Simulation is used as a central part of an integrated program in a Defence capability development process.\n\nModel–based learning in education, particularly in relation to learning science involves students creating models for scientific concepts in order to:\n\nDifferent types of model based learning techniques include:\n\nModel–making in education is an iterative exercise with students refining, developing and evaluating their models over time. This shifts learning from the rigidity and monotony of traditional curriculum to an exercise of students' creativity and curiosity. This approach utilizes the constructive strategy of social collaboration and learning scaffold theory. Model based learning includes cognitive reasoning skills where existing models can be improved upon by construction of newer models using the old models as a basis.\n\n\"Model–based learning entails determining target models and a learning pathway that provide realistic chances of understanding.\" Model making can also incorporate blended learning strategies by using web based tools and simulators, thereby allowing students to:\n\n\"A well-designed simulation simplifies a real world system while heightening awareness of the complexity of the system. Students can participate in the simplified system and learn how the real system operates without spending days, weeks or years it would take to undergo this experience in the real world.\" \n\nThe teacher's role in the overall teaching and learning process is primarily that of a facilitator and arranger of the learning experience. He or she would assign the students, a model making activity for a particular concept and provide relevant information or support for the activity. For virtual model making activities, the teacher can also provide information on the usage of the digital tool and render troubleshooting support in case of glitches while using the same. The teacher can also arrange the group discussion activity between the students and provide the platform necessary for students to share their observations and knowledge extracted from the model making activity.\n\nModel–based learning evaluation could include the use of rubrics that assess the ingenuity and creativity of the student in the model construction and also the overall classroom participation of the student vis-a-vis the knowledge constructed through the activity.\n\nIt is important, however, to give due consideration to the following for successful model–based learning to occur:\n\n\nNowadays there are some 40 magazines about scientific modelling which offer all kinds of international forums. Since the 1960s there is a strong growing number of books and magazines about specific forms of scientific modelling. There is also a lot of discussion about scientific modelling in the philosophy-of-science literature. A selection:\n\n"}
{"id": "2985811", "url": "https://en.wikipedia.org/wiki?curid=2985811", "title": "Signed measure", "text": "Signed measure\n\nIn mathematics, signed measure is a generalization of the concept of measure by allowing it to have negative values. Some authors may call it a charge, by analogy with electric charge, which is a familiar distribution that takes on positive and negative values.\n\nThere are two slightly different concepts of a signed measure, depending on whether or not one allows it to take infinite values. In research papers and advanced books signed measures are usually only allowed to take finite values, while undergraduate textbooks often allow them to take infinite values. To avoid confusion, this article will call these two cases \"finite signed measures\" and \"extended signed measures\".\n\nGiven a measurable space (\"X\", Σ), that is, a set \"X\" with a sigma algebra Σ on it, an extended signed measure is a function\nsuch that formula_2 and formula_3 is sigma additive, that is, it satisfies the equality\nwhere the series on the right must converge absolutely, for any sequence \"A\", \"A\", ..., \"A\", ... of disjoint sets in Σ. One consequence is that any extended signed measure can take +∞ as value, or it can take −∞ as value, but both are not available. The expression ∞ − ∞ is undefined and must be avoided.\n\nA finite signed measure (aka. real measure) is defined in the same way, except that it is only allowed to take real values. That is, it cannot take +∞ or −∞.\n\nFinite signed measures form a vector space, while extended signed measures are not even closed under addition, which makes them rather hard to work with. On the other hand, measures are extended signed measures, but are not in general finite signed measures.\n\nConsider a nonnegative measure ν on the space (\"X\", Σ) and a measurable function \"f\":\"X\"→ R such that\n\nThen, a finite signed measure is given by\n\nfor all \"A\" in Σ.\n\nThis signed measure takes only finite values. To allow it to take +∞ as a value, one needs to replace the assumption about \"f\" being absolutely integrable with the more relaxed condition\n\nwhere \"f\"(\"x\") = max(−\"f\"(\"x\"), 0) is the negative part of \"f\".\n\nWhat follows are two results which will imply that an extended signed measure is the difference of two nonnegative measures, and a finite signed measure is the difference of two finite non-negative measures.\n\nThe Hahn decomposition theorem states that given a signed measure μ, there exist two measurable sets \"P\" and \"N\" such that:\n\nMoreover, this decomposition is unique up to adding to/subtracting μ-null sets from \"P\" and \"N\".\n\nConsider then two nonnegative measures μ and μ defined by\n\nand\n\nfor all measurable sets \"E\", that is, \"E\" in Σ.\n\nOne can check that both μ and μ are nonnegative measures, with one taking only finite values, and are called the \"positive part\" and \"negative part\" of μ, respectively. One has that μ = μ - μ. The measure |μ| = μ + μ is called the \"variation\" of μ, and its maximum possible value, ||μ|| = |μ|(\"X\"), is called the \"total variation\" of μ.\n\nThis consequence of the Hahn decomposition theorem is called the \"Jordan decomposition\". The measures μ, μ and |μ| are independent of the choice of \"P\" and \"N\" in the Hahn decomposition theorem.\n\nThe sum of two finite signed measures is a finite signed measure, as is the product of a finite signed measure by a real number: they are closed under linear combination. It follows that the set of finite signed measures on a measurable space (\"X\", Σ) is a real vector space; this is in contrast to positive measures, which are only closed under conical combination, and thus form a convex cone but not a vector space. Furthermore, the total variation defines a norm in respect to which the space of finite signed measures becomes a Banach space. This space has even more structure, in that it can be shown to be a Dedekind complete Banach lattice and in so doing the Radon–Nikodym theorem can be shown to be a special case of the Freudenthal spectral theorem.\n\nIf \"X\" is a compact separable space, then the space of finite signed Baire measures is the dual of the real Banach space of all continuous real-valued functions on \"X\", by the Riesz–Markov–Kakutani representation theorem.\n\n\n"}
{"id": "40592503", "url": "https://en.wikipedia.org/wiki?curid=40592503", "title": "Sure-thing principle", "text": "Sure-thing principle\n\nIn decision theory, the sure-thing principle states that a decision maker who would take a certain action if he knew that event \"E\" has occurred, and also if he knew that the negation of \"E\" has occurred, should also take that same action if he knows nothing about \"E\".\n\nThe principle was coined by L.J. Savage:\nHe formulated the principle as a dominance principle, but it can also be framed probabilistically. Jeffrey and later Pearl showed that Savage's principle is only valid when the probability of the event considered (e.g., the winner of the election) is unaffected by the action (buying the property). Under such conditions, the sure-thing principle is a theorem in the \"do\"-calculus (see Bayes networks). Blyth constructed a counterexample to the sure-thing principle using sequential sampling in the context of Simpson's paradox, but this example violates the required action-independence provision.\n\nThe principle is closely related to independence of irrelevant alternatives, and equivalent under the axiom of truth (everything the agent knows is true). It is similarly targeted by the Ellsberg and Allais paradoxes, in which actual people's choices seem to violate this principle. \n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
{"id": "37055344", "url": "https://en.wikipedia.org/wiki?curid=37055344", "title": "Triangle of opposition", "text": "Triangle of opposition\n\nIn the system of Aristotelian logic, the triangle of opposition is a diagram representing the different ways in which each of the three propositions of the system is logically related ('opposed') to each of the others. The system is also useful in the analysis of syllogistic logic, serving to identify the allowed logical conversions from one type to another.\n\n"}
{"id": "24709966", "url": "https://en.wikipedia.org/wiki?curid=24709966", "title": "Vicious circle principle", "text": "Vicious circle principle\n\nThe vicious circle principle is a principle that was endorsed by many predicativist mathematicians in the early 20th century to prevent contradictions. The principle states that no object or property may be introduced by a definition that depends on that object or property itself. In addition to ruling out definitions that are explicitly circular (like \"an object has property P iff it is not next to anything that has property P\"), this principle rules out definitions that quantify over domains which include the entity being defined. Thus, it blocks Russell's paradox, which defines a set S that contains all sets that don't contain themselves. This definition is blocked because it defines a new set in terms of the totality of all sets, of which this new set would itself be a member.\n\nHowever, it also blocks one standard definition of the natural numbers. First, we define a property as being \"hereditary\" if, whenever a number \"n\" has the property, so does \"n\" + 1. Then we say that \"x\" has the property of being a natural number if and \"only\" if it has every hereditary property that 0 has. This definition is blocked, because it defines \"natural number\" in terms of the totality of all hereditary properties, but \"natural number\" itself would be such a hereditary property, so the definition is circular in this sense.\n\nMost modern mathematicians and philosophers of mathematics think that this particular definition is not circular in any problematic sense, and thus they reject the vicious circle principle. But it was endorsed by many early 20th century researchers including Bertrand Russell and Henri Poincaré. On the other hand, Frank P. Ramsey and Rudolf Carnap accepted the ban on explicit circularity, but argued against the ban on circular quantification. After all, the definition \"let T be the tallest man in the room\" defines T by means of quantification over a domain (men in the room) of which T is a member. But this is not problematic, they suggest, because the definition doesn't actually create the person, but merely shows how to pick him out of the totality. Similarly, they suggest, definitions don't actually create sets or properties or objects, but rather just give one way of picking out the already existing entity from the collection of which it is a part. Thus, this sort of circularity in terms of quantification can't cause any problems.\n\nThis principle was the reason for Russell's development of the ramified theory of types rather than the theory of simple types. (See \"Ramified Hierarchy and Impredicative Principles\".)\nAn analysis of the paradoxes to be avoided shows that they all result from a kind of vicious circle. The vicious circles in question arise from supposing that a collection of objects may contain members which can only be defined by means of the collection as a whole. Thus, for example, the collection of propositions will be supposed to contain a proposition stating that “all propositions are either true or false.” It would seem, however, that such a statement could not be legitimate unless “all propositions” referred to some already definite collection, which it cannot do if new propositions are created by statements about “all propositions.” We shall, therefore, have to say that statements about “all propositions” are meaningless.… The principle which enables us to avoid illegitimate totalities may be stated as follows: “Whatever involves all of a collection must not be one of the collection”; or, conversely: “If, provided a certain collection had a total, it would have members only definable in terms of that total, then the said collection has no total.” We shall call this the “vicious-circle principle,” because it enables us to avoid the vicious circles involved in the assumption of illegitimate totalities. (Whitehead and Russell 1910, 37) (quoted in the Stanford Encyclopedia of Philosophy entry on Russell's Paradox)\n\n\n"}
