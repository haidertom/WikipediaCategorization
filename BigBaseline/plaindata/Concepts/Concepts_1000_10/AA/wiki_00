{"id": "55063170", "url": "https://en.wikipedia.org/wiki?curid=55063170", "title": "Afro-pessimism", "text": "Afro-pessimism\n\nAfro-pessimism is a framework and critical idiom that describes the ongoing effects of racism, colonialism, and historical processes of enslavement including the Trans-Atlantic slave trade, and their impact on structural conditions as well as personal, subjective, and lived experience, and embodied reality. \n\nThe term was first coined in 1990 in an article in \"Jeune Afrique Economie\" by Francophone Cameroonian author Sony Lab'ou Tansi. Writer and intellectual Frank B. Wilderson III developed the term in his political memoir about his time spent teaching and participating in the African National Congress in South Africa during apartheid. \n\nAccording to Tansi, \"Afro-pessimism [is] a terrible word used to conceal the greatest mess of all time,\" which is the \"tragedy\" that Africa's position \"dooms us to construct and build garbage economies in the depths of the most cruel, unbearable, and inhuman form of indignity that humans can swallow\" (as translated by John Conteh-Morgan). According to Wilderson, afro-pessimism theorizes blackness as a position of \"accumulation and fungibility\" (Saidiya Hartman); that is, as condition—or relation—of ontological death, as opposed to a cultural identity or human subjectivity.\n\nWilderson, along with Hortense Spillers, Saidiya Hartman, Achille Mbembe, Jared Sexton, and others who have contributed to afro-pessimist thought, cite the Martinician psychiatrist, philosopher, and writer Frantz Fanon as a foundational figure in the tradition of Afro-pessimism. \n\nAfro-pessimism has been constructed in many ways and with different aims. But Afro-pessimism is chiefly approached a transcendent position, not as a negative or disaffected political attitude in the sense that pessimism might seemingly connote. The Black radical tradition has drawn upon the term as a way to acknowledge the power, depth, and vitality of the resilience and radical imagination of people of African descent. Within this same critique, some have used Afro-pessimism to articulate the subject-position of renunciation, refusal, distancing, dread, doubt and abjection in response to the multitude and ongoing effects and historical traumas of colonialism. This includes the view that dismantling white supremacy would mean dismantling much of the social and political institutions of the modern world.\n\nDiscussions of Afro-pessimism have manifest in an online context, and have continued in Afro-pessimist approaches to art, poetics, and computing.\n\nAfro-pessimist ideas have been part of ongoing conversations about pan-African identity, as an inclusionary concept of blackness among all people of African descent. Pan-African thought has drawn attention to the shared racial identity and also the particulars of the expression of African identity among the African Diaspora and peoples on the African continent. Pan-African thought has analyzed the ongoing struggles of African peoples, and the power of Afrocentricity as a move away from the colonialism and violence of Eurocentricity. The writings of Frantz Fanon, a Martinican psychiatrist, intellectual, and revolutionary, reflect pan-African and Afro-pessimistic approaches to decolonization and black liberation. \n\nThe Pan-African movement négritude represents pessimism as a kind of realist recognition of the historical traumas of colonialism, from an existentialist position. A key figure in the movement, Aimé Césaire, uses pessimism to consider transcendence and a recognition of the breadth of the cultural imagination and perseverance of people of African descent.\n\nAfro-pessimism has also been employed as a term describing a narrative in Western media and International relations theory that portrays post-colonial Africa as unlikely to achieve economic growth and democratic governance. This use of Afro-pessimism has nothing to with Wilderson's definition. This form of Afro-pessimism has been criticized as a Western construct regarding the ongoing portrayal of Africa and African people in Western media, overwhelmingly in terms of tragedy, doom, victimization, and victim-hood. Scholar Toussaint Nothias has characterized these discussions by the components, \"essentialism, racialization, selectivity, ranking framework, and prediction.\" From this Afro-pessimistic perspective, news media that portray Africa and African people by the trope of victimhood, mirror the Eurocentric and ethnocentric of the Western media, language, images, and rhetoric. In this ways the media tends to victimize and exoticize Africa for its going struggles with poverty, health-crisis, famine, and lack of modern development. The victimization is then visible in the humanitarian and development projects, which sometimes use the language of \"saving\" African people from such \"humanitarian disasters\".\n\n\n"}
{"id": "41099486", "url": "https://en.wikipedia.org/wiki?curid=41099486", "title": "Argument-deduction-proof distinctions", "text": "Argument-deduction-proof distinctions\n\nArgument-deduction-proof distinctions originated with logic itself. Naturally, the terminology evolved. \nAn argument, more fully a premise-conclusion argument, is a two-part system composed of premises and conclusion. An argument is \"valid\" if and only if its conclusion is a consequence of its premises. Every premise set has infinitely many consequences each giving rise to a valid argument. Some consequences are obviously so but most are not: most are hidden consequences. Most valid arguments are not yet known to be valid. To determine validity in non-obvious cases deductive reasoning is required. There is no deductive reasoning in an argument \"per se\"; such must come from the outside. \n\nEvery argument's conclusion is a premise of other arguments. The word \"constituent\" may be used for either a premise or conclusion.In the context of this article and in most classical contexts, all candidates for consideration as argument constituents fall under the category of truth-bearer: propositions, statements, sentences, judgments, etc.\n\nA deduction is a three-part system composed of premises, a conclusion, and chain of intermediates — steps of reasoning showing that its conclusion is a consequence of its premises. The reasoning in a deduction is by definition cogent. Such reasoning itself, or the chain of intermediates representing it, has also been called an argument, more fully a deductive argument. In many cases, an argument can be known to be valid by means of a deduction of its conclusion from its premises but non-deductive methods such as Venn diagrams and other graphic procedures have been proposed.\n\nA proof is a deduction whose premises are known truths. A proof of the Pythagorean theorem is a deduction that might use several premises — axioms, postulates, and definitions — and contain dozens of intermediate steps. As Alfred Tarski famously emphasized in accord with Aristotle, truths can be known by proof but proofs presuppose truths not known by proof.\nPremise-conclusion arguments do not require or produce either knowledge of validity or knowledge of truth. Premise sets may be chosen arbitrarily and conclusions may be chosen arbitrarily. \nDeductions require knowing how to reason but they do not require knowledge of truth of their premises. Deductions produce knowledge of the validity of arguments but ordinarily they do not produce knowledge of the truth of their conclusions.\nProofs require knowledge of the truth of their premises, they require knowledge of deductive reasoning, and they produce knowledge of their conclusions.\nModern logicians disagree concerning the nature of argument constituents.Quine devotes the first chapter of \"Philosophy of Logic\" to this issue. Historians have not even been able to agree on what Aristotle took as constituents.\nArgument-deduction-proof distinctions are inseparable from what have been called the \"consequence-deducibility\" distinction and the \"truth-and-consequence conception of proof\". Variations among argument-deduction-proof distinctions are not all terminological.\n\nLogician Alonzo Church never used the word \"argument\" in the above sense and had no synonym. Moreover, Church never explained that deduction is the process of producing knowledge of consequence and it never used the common noun \"deduction\" for an application of the deduction process. His primary focus in discussing proof was “conviction” produced by generation of chains of logical truths—not the much more widely applicable and more familiar general process of demonstration as found in pre-Aristotelian geometry and discussed by Aristotle. He did discuss deductions in the above sense but not by that name: he called them awkwardly “proofs from premises” — an expression he coined for the purpose.\n\nThe absence of argument-deduction-proof distinctions is entirely consonant with Church's avowed Platonistic logicism. Following Dummett's insightful remarks about Frege, which — \"mutatis mutandis\" — apply even more to Church, it might be possible to explain the today-surprising absence.\n"}
{"id": "39105", "url": "https://en.wikipedia.org/wiki?curid=39105", "title": "Boehm system", "text": "Boehm system\n\nThe Boehm system is a system of keywork for the flute, created by inventor and flautist Theobald Boehm between 1831 and 1847. \n\nPrior to the development of the Boehm system, flutes were most commonly made of wood, with an inverse conical bore, eight keys, and tone holes (the openings where the fingers are placed to produce specific notes) that were small in size, and thus easily covered by the fingertips. Boehm's work was inspired by an 1831 concert in London, given by soloist Charles Nicholson who, with his father in the 1820s, had introduced a flute constructed with larger tone holes than were used in previous designs. This large-holed instrument could produce greater volume of sound than other flutes, and Boehm set out to produce his own large-holed design.\n\nIn addition to large holes, Boehm provided his flute with \"full venting\", meaning that all keys were normally open (previously, several keys were normally closed, and opened only when the key was operated). Boehm also wanted to locate tone holes at acoustically optimal points on the body of the instrument, rather than locations conveniently covered by the player's fingers. To achieve these goals, Boehm adapted a system of axle-mounted keys with a series of \"open rings\" (called \"brille\", German for \"eyeglasses\", as they resembled the type of eyeglass frames common during the 19th century) that were fitted around other tone holes, such that the closure of one tone hole by a finger would also close a key placed over a second hole.\n\nIn 1832 Boehm introduced a new conical-bore flute, which achieved a fair degree of success. Boehm, however, continued to look for ways to improve the instrument. Finding that an increased volume of air produced a stronger and clearer tone, he replaced the conical bore with a cylindrical bore, finding that a parabolic contraction of the bore near the embouchure hole improved the instrument's low register. He also found that optimal tone was produced when the tone holes were too large to be covered by the fingertips, and he developed a system of finger plates to cover the holes. These new flutes were at first made of silver, although Boehm later produced wooden versions. \n\nThe cylindrical Boehm flute was introduced in 1847, with the instrument gradually being adopted almost universally by professional and amateur players in Europe and around the world during the second half of the 19th century. The instrument was adopted for the performance of orchestral and chamber music, opera and theater, wind ensembles (e.g., military and civic bands), and most other music which might be loosely described as relating to \"Western classical music\" (including, for example, jazz). Many further refinements have been made, and countless design variations are common among flutes today (the \"offset G\" key, addition of the low B foot, etc.) The concepts of the Boehm system have been applied across the range of flutes available, including piccolos, alto flutes, bass flutes, and so on, as well as other wind instruments. The material of the instrument may vary (many piccolos are made of wood, some very large flutes are wooden or even made of PVC).\n\nThe flute is perhaps the oldest musical instrument, other than the human voice itself. There are very many flutes, both traversely blown and end-blown \"fipple\" flutes, currently produced which are not built on the Boehm model.\n\nThe fingering system for the saxophone closely resembles the Boehm system. A key system inspired by Boehm's for the clarinet family is also known as the \"Boehm system\", although it was developed by Hyacinthe Klosé and not Boehm himself. The Boehm system was also adapted for a small number of flageolets. Boehm did work on a system for the bassoon, and Boehm-inspired oboes have been made, but non-Boehm systems remain predominant for these instruments. The Albert system is another key system for the clarinet.\n\n\n"}
{"id": "29358535", "url": "https://en.wikipedia.org/wiki?curid=29358535", "title": "Comply or explain", "text": "Comply or explain\n\nComply or explain is a regulatory approach used in the United Kingdom, Germany, the Netherlands and other countries in the field of corporate governance and financial supervision. Rather than setting out binding laws, government regulators (in the UK, the Financial Reporting Council, in Germany, under the Aktiengesetz) set out a code, which listed companies may either comply with, or if they do not comply, explain publicly why they do not. The UK Corporate Governance Code, the German Corporate Governance Code (or Deutscher Corporate Governance Kodex) and the Dutch Corporate Governance Code 'Code Tabaksblat' () use this approach in setting minimum standards for companies in their audit committees, remuneration committees and recommendations for how good companies should divide authority on their boards.\n\nThe purpose of \"comply or explain\" is to \"let the market decide\" whether a set of standards is appropriate for individual companies. Since a company may deviate from the standard, this approach rejects the view that \"one size fits all\", but because of the requirement of disclosure of explanations to market investors, anticipates that if investors do not accept a company's explanations, then investors will sell their shares, hence creating a \"market sanction\", rather than a legal one. The concept was first introduced after the recommendations of the Cadbury Report of 1992.\n\n"}
{"id": "698226", "url": "https://en.wikipedia.org/wiki?curid=698226", "title": "Concept map", "text": "Concept map\n\nA concept map or conceptual diagram is a diagram that depicts suggested relationships between concepts. It is a graphical tool that instructional designers, engineers, technical writers, and others use to organize and structure knowledge.\n\nA concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as \"causes\", \"requires\", or \"contributes to\".\n\nThe technique for visualizing these relationships among different concepts is called \"concept mapping\". Concept maps have been used to define the ontology of computer systems, for example with the object-role modeling or Unified Modeling Language formalism.\n\nA concept map is a way of representing relationships between ideas, images, or words in the same way that a sentence diagram represents the grammar of a sentence, a road map represents the locations of highways and towns, and a circuit diagram represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.\n\nConcept maps were developed to enhance meaningful learning in the sciences. A well-made concept map grows within a \"context frame\" defined by an explicit \"focus question\", while a mind map often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on declarative memory content, which is also referred to as chunks or propositions. Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.\n\n\nConcept mapping was developed by Joseph D. Novak and his research team at Cornell University in the 1970s as a means of representing the emerging science knowledge of students. It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin in the learning movement called constructivism. In particular, constructivists hold that learners actively construct knowledge.\n\nNovak's work is based on the cognitive theories of David Ausubel, who stressed the importance of prior knowledge in being able to learn (or \"assimilate\") new concepts: \"The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly.\" Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as \"What is water?\" \"What causes the seasons?\" In his book \"Learning How to Learn\", Novak states that a \"meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures.\"\n\nVarious attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of \"off-loading\". In this 1998 paper, McAleese draws on the work of Sowa and a paper by Sweller & Chandler. In essence, McAleese suggests that the process of making knowledge explicit, using \"nodes\" and \"relationships\", allows the individual to become aware of what they know and as a result to be able to modify what they know. Maria Birbili applies that same idea to helping young children learn to think about what they know. The concept of the \"knowledge arena\" is suggestive of a virtual space where learners may explore what they know and what they do not know.\n\nConcept maps are used to stimulate the generation of ideas, and are believed to aid creativity. Concept mapping is also sometimes used for brain-storming. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.\n\nFormalized concept maps are used in software design, where a common usage is Unified Modeling Language diagramming amongst similar conventions and development methodologies.\n\nConcept mapping can also be seen as a first step in ontology-building, and can also be used flexibly to represent formal argument — similar to argument maps.\n\nConcept maps are widely used in education and business. Uses include:\n\n"}
{"id": "47792266", "url": "https://en.wikipedia.org/wiki?curid=47792266", "title": "Construction of Concept Map", "text": "Construction of Concept Map\n\nConcept is usually perceived as a regularity in events or objects or in their records. While constructing a concept map, it is essential to keep in mind that the concept be built with reference to a focus question. Hence, initially, the focus question to which we seek to answer is carefully chosen because learners usually tend to deviate from this question relating only to domains and thus, fails to answer the question.\n\nWith the selected domain and the focus question, the next step is to identify the key concepts which apply to this domain.About 15 to 25 concepts is sufficient which is usually ordered in a rank ordered list. Such list should be established from the most general and inclusive concept for the particular chosen problem. This list will assist in at least in the beginning of the construction of the concept map. The list is referred to as Parking Lot since the list is constantly shuffled to build up the required network of the concept. Two or more concepts are connected to each other using linking words or phrases which can only then complete a meaningful sentence.Another important characteristics of a concept map is the cross-links. This cross link acts as the relationship between two different domains used in the concept map.This helps in a clear representation of the knowledge contained in the concept and also gives a clear background with specific examples. \n"}
{"id": "7327", "url": "https://en.wikipedia.org/wiki?curid=7327", "title": "Copernican principle", "text": "Copernican principle\n\nIn physical cosmology, the Copernican principle is an alternative name for the principle of relativity, stating that humans, on the Earth or in the Solar system, are not privileged observers of the universe.\n\nNamed for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus's argument of a moving Earth. In some sense, it is equivalent to the mediocrity principle.\n\nHermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets can be explained by reference to an assumption that the Sun and not Earth is centrally located and stationary. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as \"demoting\" Earth from its central role it had in the Ptolemaic geocentric model, it was successors to Copernicus, notably the 16th century Giordano Bruno who adopted this new perspective. The earth's central position had been interpreted as being in the \"lowest and filthiest parts\". Instead, as Galileo said, the earth is part of the \"dance of the stars\" rather than the \"sump where the universe's filth and ephemera collect\". In the late 20th Century, Carl Sagan asked, \"Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.\"\n\nIn cosmology, if one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from our vantage point on Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle. In practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. It becomes more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 200 million parsecs. However, on scales comparable to the radius of the observable universe, we see systematic changes with distance from Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. While this might suggest that Earth is at the center of the universe, the Copernican principle requires us to interpret it as evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach us and show us the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.\n\nModern mathematical cosmology is based on the assumption that the Cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations.\n\nMichael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: \"It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe.\"\n\nBondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.\n\nSince the 1990s the term has been used (interchangeably with \"the Copernicus method\") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.\n\nThe Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the Cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\n\nBefore the term Copernican principle was even coined, Earth was repeatedly shown not to have any special location in the universe. The Copernican Revolution dethroned Earth to just one of many planets orbiting the Sun. Proper motion was mentioned by Halley. William Herschel found that the Solar System is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that our galaxy is just one of many galaxies in the universe. Examination of our galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.\n\nRecent and planned tests relevant to the cosmological and Copernican principles include:\n\nThe standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle and observations are largely consistent but there are always unsolved problems. Some cosmologists and theoretical physicists design models lacking the Cosmological or Copernican principles, to constrain the valid values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.\n\nA prominent example in this context is the observed accelerating universe and the cosmological constant issue. An alternative proposal to dark energy is that the universe is much more inhomogeneous than currently assumed, and specifically that we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.\n"}
{"id": "173937", "url": "https://en.wikipedia.org/wiki?curid=173937", "title": "Cosmological principle", "text": "Cosmological principle\n\nIn modern physical cosmology, the cosmological principle is the notion that the spatial distribution of matter in the universe is homogeneous and isotropic when viewed on a large enough scale, since the forces are expected to act uniformly throughout the universe, and should, therefore, produce no observable irregularities in the large-scale structuring over the course of evolution of the matter field that was initially laid down by the Big Bang.\n\nAstronomer William Keel explains:\n\nThe cosmological principle is usually stated formally as 'Viewed on a sufficiently large scale, the properties of the universe are the same for all observers.' This amounts to the strongly philosophical statement that the part of the universe which we can see is a fair sample, and that the same physical laws apply throughout. In essence, this in a sense says that the universe is knowable and is playing fair with scientists.\n\nThe cosmological principle depends on a definition of \"observer,\" and contains an implicit qualification and two testable consequences.\n\n\"Observers\" means any observer at any location in the universe, not simply any human observer at any location on Earth: as Andrew Liddle puts it, \"the cosmological principle [means that] the universe looks the same whoever and wherever you are.\"\n\nThe qualification is that variation in physical structures can be overlooked, provided this does not imperil the uniformity of conclusions drawn from observation: the Sun is different from the Earth, our galaxy is different from a black hole, some galaxies advance toward rather than recede from us, and the universe has a \"foamy\" texture of galaxy clusters and voids, but none of these different structures appears to violate the basic laws of physics.\n\nThe two testable structural consequences of the cosmological principle are homogeneity and isotropy. Homogeneity means that the same observational evidence is available to observers at different locations in the universe (\"the part of the universe which we can see is a fair sample\"). Isotropy means that the same observational evidence is available by looking in any direction in the universe (\"the same physical laws apply throughout\" ). The principles are distinct but closely related, because a universe that appears isotropic from any two (for a spherical geometry, three) locations must also be homogeneous.\n\nThe cosmological principle is first clearly asserted in the \"Philosophiæ Naturalis Principia Mathematica\" (1687) of Isaac Newton. In contrast to earlier classical or medieval cosmologies, in which Earth rested at the center of universe, Newton conceptualized the Earth as a sphere in orbital motion around the Sun within an empty space that extended uniformly in all directions to immeasurably large distances. He then showed, through a series of mathematical proofs on detailed observational data of the motions of planets and comets, that their motions could be explained by a single principle of \"universal gravitation\" that applied as well to the orbits of the Galilean moons around Jupiter, the Moon around the Earth, the Earth around the Sun, and to falling bodies on Earth. That is, he asserted the equivalent material nature of all bodies within the Solar System, the identical nature of the Sun and distant stars and thus the uniform extension of the physical laws of motion to a great distance beyond the observational location of Earth itself.\n\nObservations show that more distant galaxies are closer together and have lower content of chemical elements heavier than lithium. Applying the cosmological principle, this suggests that heavier elements were not created in the Big Bang but were produced by nucleosynthesis in giant stars and expelled across a series of supernovae explosions and new star formation from the supernovae remnants, which means heavier elements would accumulate over time. Another observation is that the furthest galaxies (earlier time) are often more fragmentary, interacting and unusually shaped than local galaxies (recent time), suggesting evolution in galaxy structure as well.\n\nA related implication of the cosmological principle is that the largest discrete structures in the universe are in mechanical equilibrium. Homogeneity and isotropy of matter at the largest scales would suggest that the largest discrete structures are parts of a single indiscrete form, like the crumbs which make up the interior of a cake. At extreme cosmological distances, the property of mechanical equilibrium in surfaces lateral to the line of sight can be empirically tested; however, under the assumption of the cosmological principle, it cannot be detected parallel to the line of sight (see timeline of the universe).\n\nCosmologists agree that in accordance with observations of distant galaxies, a universe must be non-static if it follows the cosmological principle. In 1923, Alexander Friedmann set out a variant of Einstein's equations of general relativity that describe the dynamics of a homogeneous isotropic universe. Independently, Georges Lemaître derived in 1927 the equations of an expanding universe from the General Relativity equations. Thus, a non-static universe is also implied, independent of observations of distant galaxies, as the result of applying the cosmological principle to general relativity.\n\nKarl Popper criticized the cosmological principle on the grounds that it makes \"our \"lack\" of knowledge a principle of \"knowing something\"\". He summarized his position as:\n\nAlthough the universe is inhomogeneous at smaller scales, it \"is\" statistically homogeneous on scales larger than 250 million light years. The cosmic microwave background is isotropic, that is to say that its intensity is about the same whichever direction we look at.\n\nHowever, recent findings have called this view into question. Data from the Planck Mission shows hemispheric bias in 2 respects: one with respect to average temperature (i.e. temperature fluctuations), the second with respect to larger variations in the degree of perturbations (i.e. densities). Therefore, the European Space Agency (the governing body of the Planck Mission) has concluded that these anisotropies are, in fact, statistically significant and can no longer be ignored.\n\nThe \"cosmological principle\" implies that at a sufficiently large scale, the universe is homogeneous. This means that different places will appear similar to one another, so sufficiently large structures cannot exist. Yadav and his colleagues have suggested a maximum scale of 260/h Mpc for structures within the universe according to this heuristic. Other authors have suggested values as low as 60/h Mpc. Yadav's calculation suggests that the maximum size of a structure can be about 370 Mpc.\n\nA number of observations conflict with predictions of maximal structure sizes:\n\n\nIn September 2016, however, studies of the expansion of the Universe that have used data taken by the \"Planck\" mission show it to be highly isotropical, reinforcing the cosmological principle\n\nThe perfect cosmological principle is an extension of the cosmological principle, and states that the universe is homogeneous and isotropic in space \"and\" time. In this view the universe looks the same everywhere (on the large scale), the same as it always has and always will. The perfect cosmological principle underpins Steady State theory and emerges from chaotic inflation theory.\n\n"}
{"id": "4110028", "url": "https://en.wikipedia.org/wiki?curid=4110028", "title": "Distinction (philosophy)", "text": "Distinction (philosophy)\n\nDistinction, the fundamental philosophical abstraction, involves the recognition of difference.\n\nIn classical philosophy, there were various ways in which things could be distinguished. The merely logical or virtual distinction, such as the difference between concavity and convexity, involves the mental apprehension of two definitions, but which cannot be realized outside the mind, as any concave line would be a convex line considered from another perspective. A real distinction involves a level of ontological separation, as when squirrels are distinguished from llamas (for no squirrel is a llama, and no llama is a squirrel). A real distinction is thus different than a merely conceptual one, in that in a real distinction, one of the terms can be realized in reality without the other being realized.\n\nLater developments include Duns Scotus's formal distinction, which developed in part out of the recognition in previous authors that there need to be an intermediary between logical and real distinctions.\n\nSome relevant distinctions to the history of Western philosophy include:\n\nWhile the there are anticipation of this distinction prior to Kant in the British Empiricists (and even further in Scholastic thought), it was Kant who introduced the terminology. The distinction concerns the relation of a subject to its predicate: analytic claims are those in which the subject contains the predicate, as in \"All bodies are extended.\" Synthetic claims bring two concepts together, as in \"All events are caused.\" The distinction was recently called into question by W.V.O. Quine, in his paper \"Two Dogmas of Empiricism.\"\n\nThe origins of the distinction are less clear, and it concerns the origins of knowledge. \"A posteriori\" knowledge arises from, or is caused by, experience. \"A priori\" knowledge may come temporally after experience, but its certainty is not derivable from the experience itself. Saul Kripke was the first major think to propose that there are analytic \"a posteriori\" knowledge claims.\n\nAristotle makes the distinction between actuality and potentiality. Actuality is a realization of the way a thing could be, while potency refers simply to the way a thing could be. There are two levels to each: matter itself can be anything, and becomes something actually by causes, making it something which then has the ability to be in a certain way, and that ability can then be realized. The matter of an ax can be an ax, then is made into an ax. The ax thereby is able to cut, and reaches a new form of actuality in actually cutting.\n\nThe major distinction Aquinas makes is that of essence and existence. It is a distinction already in Avicenna, but Aquinas maps the distinction onto the actuality/potentiality distinction of Aristotle, such that the essence of a thing is in potency to the existence of a thing, which is that thing's actuality.\n\nIn Kant, the distinction between appearance and thing-in-itself is foundational to his entire philosophical project. The distinction separates the way a thing appears to us on the one hand, and the way a thing really is.\n"}
{"id": "928779", "url": "https://en.wikipedia.org/wiki?curid=928779", "title": "First principle", "text": "First principle\n\nA first principle is a basic, foundational, self-evident proposition or assumption that cannot be deduced from any other proposition or assumption. In philosophy, first principles are taught by Aristotelians, and nuanced versions of first principles are referred to as postulates by Kantians. In mathematics, first principles are referred to as axioms or postulates. In physics and other sciences, theoretical work is said to be from first principles, or \"ab initio\", if it starts directly at the level of established science and does not make assumptions such as empirical model and parameter fitting.\n\nIn a formal logical system, that is, a set of propositions that are consistent with one another, it is possible that some of the statements can be deduced from other statements. For example, in the syllogism, \"All men are mortal; Socrates is a man; Socrates is mortal\" the last claim can be deduced from the first two.\n\nA first principle is an axiom that cannot be deduced from any other within that system. The classic example is that of Euclid's Elements; its hundreds of geometric propositions can be deduced from a set of definitions, postulates, and common notions: all three types constitute first principles.\n\nIn philosophy \"first principles\" are also commonly referred to as \"a priori\" terms and arguments, which are contrasted to \"a posteriori\" terms, reasoning or arguments, in that the former are simply assumed and exist prior to the reasoning process and the latter are deduced or inferred after the initial reasoning process. First principles are generally treated in the realm of philosophy known as epistemology, but are an important factor in any metaphysical speculation. \n\nIn philosophy \"first principles\" are often somewhat synonymous with \"a priori\", datum and axiomatic reasoning.\n\nTerence Irwin writes:\nProfoundly influenced by Euclid, Descartes was a rationalist who invented the foundationalist system of philosophy. He used the \"method of doubt\", now called Cartesian doubt, to systematically doubt everything he could possibly doubt, until he was left with what he saw as purely indubitable truths. Using these self-evident propositions as his axioms, or foundations, he went on to deduce his entire body of knowledge from them. The foundations are also called \"a priori\" truths. His most famous proposition is \"Je pense, donc je suis.\" (\"I think, therefore I am\", or \"Cogito ergo sum\")\n\nDescartes describes the concept of a first principle in the following excerpt from the preface to the \"Principles of Philosophy\" (1644):\nIn physics, a calculation is said to be \"from first principles\", or \"ab initio\", if it starts directly at the level of established laws of physics and does not make assumptions such as empirical model and fitting parameters.\n\nFor example, calculation of electronic structure using Schrödinger's equation within a set of approximations that do not include fitting the model to experimental data is an \"ab initio\" approach.\n\n\n"}
{"id": "12656", "url": "https://en.wikipedia.org/wiki?curid=12656", "title": "Godwin's law", "text": "Godwin's law\n\nGodwin's law (or Godwin's rule of Hitler analogies) is an Internet adage asserting that \"As an online discussion grows longer, the probability of a comparison involving Nazis or Hitler approaches 1\"; that is, if an online discussion (regardless of topic or scope) goes on long enough, sooner or later someone will compare someone or something to Adolf Hitler or his deeds, the point at which effectively the discussion or thread often ends. Promulgated by the American attorney and author Mike Godwin in 1990, Godwin's law originally referred specifically to Usenet newsgroup discussions. It is now applied to any threaded online discussion, such as Internet forums, chat rooms, and comment threads, as well as to speeches, articles, and other rhetoric where \"reductio ad Hitlerum\" occurs.\n\nThere are many corollaries to Godwin's law, some considered more canonical (by being adopted by Godwin himself) than others. For example, there is a tradition in many newsgroups and other Internet discussion forums that, when a Hitler comparison is made, the thread is finished and whoever made the comparison loses whatever debate is in progress. This principle is itself frequently referred to as Godwin's law.\n\nGodwin's law itself can be abused as a distraction, diversion or even as censorship, fallaciously miscasting an opponent's argument as hyperbole when the comparisons made by the argument are actually appropriate. Similar criticisms of the \"law\" (or \"at least the distorted version which purports to prohibit all comparisons to German crimes\") have been made by the American lawyer, journalist, and author Glenn Greenwald.\n\nGodwin's law does not claim to articulate a fallacy; it is instead framed as a memetic tool to reduce the incidence of inappropriate hyperbolic comparisons. \"Although deliberately framed as if it were a law of nature or of mathematics,\" Godwin wrote, \"its purpose has always been rhetorical and pedagogical: I wanted folks who glibly compared someone else to Hitler to think a bit harder about the Holocaust.\"\n\nGodwin has stated that he introduced Godwin's law in 1990 as an experiment in memetics.\n\nIn 2012, \"Godwin's law\" became an entry in the third edition of the \"Oxford English Dictionary\".\n\nIn December 2015, Godwin commented on the Nazi and fascist comparisons being made by several articles about Republican presidential candidate Donald Trump, saying: \"If you're thoughtful about it and show some real awareness of history, go ahead and refer to Hitler when you talk about Trump, or any other politician.\" In August 2017, Godwin made similar remarks on social networking websites Facebook and Twitter with respect to the two previous days' Unite the Right rally in Charlottesville, Virginia, endorsing and encouraging efforts to compare its alt-right organizers to Nazis.\n\nIn October 2018, Godwin made a similar statement when someone asked him, via Twitter, if it would be OK to call Brazilian presidential candidate Jair Bolsonaro a \"nazi\", answering a direct question (\"So, just to be clear, is it OK to call Bolsonaro a nazi?\") with the portuguese word \"Sim!\" (meaning \"yes\" in English). \n\n\n"}
{"id": "630398", "url": "https://en.wikipedia.org/wiki?curid=630398", "title": "Identity of indiscernibles", "text": "Identity of indiscernibles\n\nThe identity of indiscernibles is an ontological principle that states that there cannot be separate objects or entities that have all their properties in common. That is, entities \"x\" and \"y\" are identical if every predicate possessed by \"x\" is also possessed by \"y\" and vice versa; to suppose two things indiscernible is to suppose the same thing under two names. It states that no two distinct things (such as snowflakes) can be exactly alike, but this is intended as a metaphysical principle rather than one of natural science. A related principle is the indiscernibility of identicals, discussed below.\n\nA form of the principle is attributed to the German philosopher Gottfried Wilhelm Leibniz. It is one of his two great metaphysical principles, the other being the principle of sufficient reason. Both are famously used in his arguments with Newton and Clarke in the Leibniz–Clarke correspondence. Because of its association with Leibniz, the principle is sometimes known as Leibniz's law. \n\nSome philosophers have decided, however, that it is important to exclude certain predicates (or purported predicates) from the principle in order to avoid either triviality or contradiction. An example (detailed below) is the predicate that denotes whether an object is equal to \"x\" (often considered a valid predicate). As a consequence, there are a few different versions of the principle in the philosophical literature, of varying logical strength—and some of them are termed \"the strong principle\" or \"the weak principle\" by particular authors, in order to distinguish between them.\n\nWillard Van Orman Quine thought that the failure of substitutivity in intensional contexts (e.g., \"Sally believes that \"p\"\" or \"It is necessarily the case that \"q\"\") shows that modal logic is an impossible project. Saul Kripke holds that this failure may be the result of the use of the disquotational principle implicit in these proofs, and not a failure of substitutivity as such.\n\nThe identity of indiscernibles has been used to motivate notions of noncontextuality within quantum mechanics.\n\nAssociated with this principle is also the question as to whether it is a logical principle, or merely an empirical principle.\n\nThere are two principles here that must be distinguished (equivalent versions of each are given in the language of the predicate calculus). Note that these are all second-order expressions. Neither of these principles can be expressed in first-order logic (are nonfirstorderizable).\n\nPrinciple 1 doesn't entail reflexivity of = (or any other relation \"R\" substituted for it), but both properties together entail symmetry and transitivity (see proof box). Therefore, Principle 1 and reflexivity is sometimes used as a (second-order) axiomatization for the equality relation.\n\nPrinciple 1 is taken to be a logical truth and (for the most part) uncontroversial. Principle 2, on the other hand, is controversial; Max Black famously argued against it.\n\nThe above formulations are not satisfactory, however: the second principle should be read as having an implicit side-condition excluding any predicates that are equivalent (in some sense) to any of the following:\nIf all such predicates are included, then the second principle as formulated above can be trivially and uncontroversially shown to be a logical tautology: if \"x\" is non-identical to \"y\", then there will always be a putative \"property\" that distinguishes them, namely \"being identical to \"x\"\".\n\nOn the other hand, it is incorrect to exclude all predicates that are materially equivalent (i.e., contingently equivalent) to one or more of the four given above. If this is done, the principle says that in a universe consisting of two non-identical objects, because all distinguishing predicates are materially equivalent to at least one of the four given above (in fact, they are each materially equivalent to two of them), the two non-identical objects are identical—which is a contradiction.\n\nMax Black has argued against the identity of indiscernibles by counterexample. Notice that to show that the identity of indiscernibles is false, it is sufficient that one provides a model in which there are two distinct (numerically nonidentical) things that have all the same properties. He claimed that in a symmetric universe wherein only two symmetrical spheres exist, the two spheres are two distinct objects even though they have all their properties in common.\n\nBlack's argument appears significant because it shows that even relational properties (properties specifying distances between objects in space-time) fail to distinguish two identical objects in a symmetrical universe. Per his argument, two objects are, and will remain, equidistant from the universe's plane of symmetry and each other. Even bringing in an external observer to label the two spheres distinctly does not solve the problem, because it violates the symmetry of the universe.\n\nAs stated above, the principle of indiscernibility of identicals—that if two objects are in fact one and the same, they have all the same properties—is mostly uncontroversial. However, one famous application of the indiscernibility of identicals was by René Descartes in his \"Meditations on First Philosophy\". Descartes concluded that he could not doubt the existence of himself (the famous \"cogito\" argument), but that he \"could\" doubt the existence of his body.\n\nThis argument is criticized by some modern philosophers on the grounds that it allegedly derives a conclusion about what is true from a premise about what people know. What people know or believe about an entity, they argue, is not really a characteristic of that entity. A response may be that the argument in the \"Meditations on First Philosophy\" is that the inability of Descartes to doubt the existence of his mind is part of his mind's essence. One may then argue that identical things should have identical essences.\n\nNumerous counterexamples are given to debunk Descartes' reasoning via \"reductio ad absurdum\", such as the following argument based on a secret identity:\n\n\n\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "3108937", "url": "https://en.wikipedia.org/wiki?curid=3108937", "title": "Landauer's principle", "text": "Landauer's principle\n\nLandauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".\n\nAnother way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.\n\nA so-called logically-reversible computation, in which no information is erased, may in principle be carried out without releasing any heat. This has led to considerable interest in the study of reversible computing. Indeed, without reversible computing, increases in the number of computations-per-joule-of-energy-dissipated must come to a halt by about 2050: because the limit implied by Landauer's principle will be reached by then, according to Koomey's law. \n\nAt 20 °C (room temperature, or 293.15 K), the Landauer limit represents an energy of approximately 0.0172 eV, or 2.75 zJ. Theoretically, roomtemperature computer memory operating at the Landauer limit could be changed at a rate of one billion bits per second with energy being converted to heat in the memory media at the rate of only 2.85 trillionths of a watt (that is, at a rate of only 2.85 pJ/s). Modern computers use millions of times as much energy per second.\n\nRolf Landauer first proposed the principle in 1961 while working at IBM. He rigorously justified and stated important limits to an earlier conjecture by John von Neumann. For this reason, it is sometimes referred to as being simply the Landauer bound or Landauer limit.\n\nIn 2011, the principle was generalized to show that while information erasure requires an increase in entropy, that increase could theoretically occur at no energy cost. Instead, the cost can be taken in another conserved quantity, such as angular momentum.\n\nIn a 2012 article published in \"Nature\", a team of physicists from the École normale supérieure de Lyon, University of Augsburg and the University of Kaiserslautern described that for the first time they have measured the tiny amount of heat released when an individual bit of data is erased.\n\nIn 2014, physical experiments tested Landauer's principle and confirmed its predictions.\n\nIn 2016, researchers used a laser probe to measure the amount of energy dissipation that resulted when a nanomagnetic bit flipped from off to on. Flipping the bit required 15 millielectron volts (3 zeptojoules).\n\nA 2018 article published in \"Nature Physics\" features a Landauer erasure performed at cryogenic temperatures (T = 1K) on an array of high-spin (S = 10) quantum molecular magnets. The array is made to act as a spin register where each nanomagnet encodes a single bit of information. The experiment has laid the foundations for the extension of the validity of the Landauer principle to the quantum realm. Owing to the fast dynamics and low \"inertia\" of the single spins used in the experiment, the researchers also showed how an erasure operation can be carried out at the lowest possible thermodynamic cost — that imposed by the Landauer principle — and at a high speed. \n\nLandauer's principle can be understood to be a simple logical consequence of the second law of thermodynamics—which states that the entropy of an isolated system cannot decrease—together with the definition of thermodynamic temperature. For, if the number of possible logical states of a computation were to decrease as the computation proceeded forward (logical irreversibility), this would constitute a forbidden decrease of entropy, unless the number of possible physical states corresponding to each logical state were to simultaneously increase by at least a compensating amount, so that the total number of possible physical states was no smaller than it was originally (i.e. total entropy has not decreased).\n\nYet, an increase in the number of physical states corresponding to each logical state means that, for an observer who is keeping track of the logical state of the system but not the physical state (for example an \"observer\" consisting of the computer itself), the number of possible physical states has increased; in other words, entropy has increased from the point of view of this observer.\n\nThe maximum entropy of a bounded physical system is finite. (If the holographic principle is correct, then physical systems with finite surface area have a finite maximum entropy; but regardless of the truth of the holographic principle, quantum field theory dictates that the entropy of systems with finite radius and energy is finite due to the Bekenstein bound.) To avoid reaching this maximum over the course of an extended computation, entropy must eventually be expelled to an outside environment.\n\nLandauer's principle asserts that there is a minimum possible amount of energy required to erase one bit of information, known as the \"Landauer limit\":\nwhere \"k\" is the Boltzmann constant (approximately 1.38×10 J/K), \"T\" is the temperature of the heat sink in kelvins, and ln 2 is the natural logarithm of 2 (approximately 0.69315).\n\nFor an environment at temperature \"T\", energy \"E\" = \"ST\" must be emitted into that environment if the amount of added entropy is \"S\". For a computational operation in which 1 bit of logical information is lost, the amount of entropy generated is at least \"k\" ln 2, and so, the energy that must eventually be emitted to the environment is \"E\" ≥ \"kT\" ln 2.\n\nThe principle is widely accepted as physical law, but in recent years it has been challenged for using circular reasoning and faulty assumptions, notably in Earman and Norton (1998), and subsequently in Shenker (2000) and Norton (2004, 2011), and defended by Bennett (2003) and Ladyman et al. (2007).\n\n\n"}
{"id": "1722373", "url": "https://en.wikipedia.org/wiki?curid=1722373", "title": "Locard's exchange principle", "text": "Locard's exchange principle\n\nIn forensic science, Locard's exchange principle holds that the perpetrator of a crime will bring something into the crime scene and leave with something from it, and that both can be used as forensic evidence. Dr. Edmond Locard (13 December 1877 – 4 May 1966) was a pioneer in forensic science who became known as the Sherlock Holmes of France. He formulated the basic principle of forensic science as: \"Every contact leaves a trace\". Paul L. Kirk expressed the principle as follows:\n\nFragmentary or trace evidence is any type of material left at (or taken from) a crime scene, or the result of contact between two surfaces, such as shoes and the floor covering or soil, or fibers from where someone sat on an upholstered chair.\n\nWhen a crime is committed, fragmentary (or trace) evidence needs to be collected from the scene. A team of specialized police technicians goes to the scene of the crime and seals it off. They record video and take photographs of the crime scene, victim/s (if there are any) and items of evidence. If necessary, they undertake ballistics examinations. They check for foot, shoe, and tire mark impressions, plus hair as well as examine any vehicles and check for fingerprints - whole or partial.\n\nThe case studies below show how prevalent Locard's Exchange Principle is in each and every crime. The examples using Locard's Principle show not only how the transfer of trace evidence can tell the tale of what happened, but also how much care is required when collecting and evaluating trace evidence.\n\nKarola and Melanie Weimar, aged 5 and 7, lived with their parents, Reinhard and Monika, in Germany. They were reported missing on 4 August 1986. Their bodies were found on 7 August. They had been murdered.\n\nMonika first said the children had breakfast, then went to a playground. Three weeks later she said they were already dead when she returned home the previous night: Reinhard was sitting on the edge of Karola's bed, weeping and confused; he then disposed of the bodies.\n\nBoth parents were suspected, but Monika was having an affair, and was seen where Melanie's body was later found. She was convicted, but after serving her sentence, was released in 2006.\n\nInvestigators determined what clothes Monika was wearing on 3 and 4 August, but not Reinhard's clothes, so only fibers from her clothing were identified on the children's bodies, yet they were also constantly in contact with him.\n\nThe bedding contained 14 fibers from Karola's T-shirt. Frictionless tests, simulating a dead child, matched that figure better than the friction tests, simulating a live child, so Karola could have lain lifelessly in bed wearing her T-shirt, as stated by her mother.\n\n35 fibers from Monika's blouse were found on the back of Melanie's T-shirt, but only one on her bed sheet. In tests, between 6 and 10 fibers remained on the sheet. These higher numbers were thought to disprove Monika's claim that she gave her child a goodbye hug the previous day. However, there are several likely explanations. For example, the bedding was put in one bag, so fibers from the sheet could have been transferred to the cover and pillow. Only the central area of the top of the sheet was taped: it might have originally contained more than one blouse fiber, the others could have been transferred to the back or sides while in the bag.\n\nThe blouse fibers on Melanie's clothing were distributed evenly, not the clusters expected from carrying the body.\n\n265 fibers from the family car’s rear seat covers were found on Melanie's panties and the inside of her trousers, but only a small number of fibers from the front seats was found on the children. This helped disprove the theory that they were killed on the front seats.\n\nMelanie's clothes and hair were covered in 375 clinging fruits of goosegrass. As some of these itchy things were on the inside of her trousers and on her panties, the trousers must have been put on her after death.\n\nNo sand was found on the bodies or clothing (including socks and sandals) of either child, making the morning playground story unlikely.\n\nDanielle van Dam, aged 7, lived with her parents and brothers in San Diego, California. She was reported missing on 2 February 2002; her body was discovered on 27 February. Neighbor David Westerfield was almost immediately suspected, as he had gone camping in his RV, and he was convicted of her kidnapping and murder.\n\nHairs consistent with the van Dams’ dog were found in his RV, also carpet fibers consistent with Danielle's bedroom carpet. Danielle's nightly ritual was to wrestle with the dog after getting into her pajamas. The prosecution argued that those hairs and fibers got onto her pajamas through that contact, and were then carried on the pajamas to first Westerfield's house and then to his RV, when he kidnapped her from her bed. The alternative scenario is that they got onto her daytime clothes, and those of her mother and younger brother, and were carried to his house when they visited him earlier that week selling cookies. He said his laundry was out during that visit, so trace evidence from them could have got on it, and then been transferred to his bedroom and his RV (secondary Locard transfer). Also, his RV was often parked, sometimes unlocked, in the neighborhood streets, so Danielle could have sneaked inside, leaving behind that evidence.\n\nNo trace of Westerfield was found in the van Dam house.\n\n14 hairs consistent with Danielle's were found in his environment. All but one were compared on only mitochondrial DNA, so they might have come from her mother or a sibling. Most (21) of the hairs were in a dryer lint ball in his trash can, so they might have got in his laundry before the kidnapping.\n\nThere were 5 carpet fibers in his RV, but none in his house, suggesting those were deposited by someone going directly from her house to his RV, or they may have come from another house in that development.\n\nNo Danielle pajama or bedding fibers were reported in his environment. There was no trace evidence in his SUV (which casts doubt on the belief that she was transported from his house to his RV in his SUV). He vacuumed his RV after the kidnapping, but no trace evidence was in the vacuum cleaner.\n\nOne orange fiber with her body was consistent with about 200 in his house and 20 in his SUV (none in his RV), while 21 blue fibers with her body were consistent with 10 in his house and 46 in his RV (none in his SUV). Contrary to media reports, only a few items from her house were tested so that can’t be excluded as the source. In particular, the clothes of Danielle and her family during the cookie sale were not determined and eliminated. There were apparently two different types of the orange fibers, dull and very bright (so the number which matched might have been much less than 200). There were red fibers with her fingernails, and many other fibers with her body, which could not be matched to his environment. The only non-Danielle hair found with her body wasn’t his, nor was any desert sand reported with the body, and no soil or vegetation from the dump site was reported on his shoes, laundry, shovel or RV.\n\nTo explain why so much expected evidence was missing, the prosecution argued that he went on a cleaning frenzy, and tossed out evidence.\n\nIt is also mentioned in an episode of \"Hawaii Five-O\"\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "27079770", "url": "https://en.wikipedia.org/wiki?curid=27079770", "title": "Mental model theory of reasoning", "text": "Mental model theory of reasoning\n\nThe mental model theory of reasoning was developed by Philip Johnson-Laird and Ruth M.J. Byrne (Johnson-Laird and Byrne, 1991). It has been applied to the main domains of deductive inference including relational inferences such as spatial and temporal deductions; propositional inferences, such as conditional, disjunctive and negation deductions; quantified inferences such as syllogisms; and meta-deductive inferences.\n\nOngoing research on mental models and reasoning has led the theory to be extended to account for probabilistic inference (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).\n\n"}
{"id": "795103", "url": "https://en.wikipedia.org/wiki?curid=795103", "title": "Multiple drafts model", "text": "Multiple drafts model\n\nDaniel Dennett's multiple drafts model of consciousness is a physicalist theory of consciousness based upon cognitivism, which views the mind in terms of information processing. The theory is described in depth in his book, \"Consciousness Explained\", published in 1991. As the title states, the book proposes a high-level explanation of consciousness which is consistent with support for the possibility of strong AI.\n\nDennett describes the theory as \"first-person operationalism\". As he states it:\nDennett's thesis is that our modern understanding of consciousness is unduly influenced by the ideas of René Descartes. To show why, he starts with a description of the phi illusion. In this experiment, two different coloured lights, with an angular separation of a few degrees at the eye, are flashed in succession. If the interval between the flashes is less than a second or so, the first light that is flashed appears to move across to the position of the second light. Furthermore, the light seems to change colour as it moves across the visual field. A green light will appear to turn red as it seems to move across to the position of a red light. Dennett asks how we could see the light change colour \"before\" the second light is observed.\n\nDennett claims that conventional explanations of the colour change boil down to either \"Orwellian\" or \"Stalinesque\" hypotheses, which he says are the result of Descartes' continued influence on our vision of the mind. In an Orwellian hypothesis, the subject comes to one conclusion, then goes back and changes that memory in light of subsequent events. This is akin to George Orwell's \"Nineteen Eighty-Four\", where records of the past are routinely altered. In a Stalinesque hypothesis, the two events would be reconciled prior to entering the subject's consciousness, with the final result presented as fully resolved. This is akin to Joseph Stalin's show trials, where the verdict has been decided in advance and the trial is just a rote presentation.\nDennett argues that there is no principled basis for picking one of these theories over the other, because they share a common error in supposing that there is a special time and place where unconscious processing becomes consciously experienced, entering into what Dennett calls the \"Cartesian theatre\". Both theories require us to cleanly divide a sequence of perceptions and reactions into before and after the instant that they reach the seat of consciousness, but he denies that there is any such moment, as it would lead to infinite regress. Instead, he asserts that there is no privileged place in the brain where consciousness happens. Dennett states that, \"[t]here does not exist ... a process such as 'recruitment of consciousness' (into what?), nor any place where the 'vehicle's arrival' is recognized (by whom?)\"\n\nWith no theatre, there is no screen, hence no reason to re-present data after it has already been analysed. Dennett says that, \"the Multiple Drafts model goes on to claim that the brain does not bother 'constructing' any representations that go to the trouble of 'filling in' the blanks. That would be a waste of time and (shall we say?) paint. The judgement is already in so we can get on with other tasks!\"\n\nAccording to the model, there are a variety of sensory inputs from a given event and also a variety of interpretations of these inputs. The sensory inputs arrive in the brain and are interpreted at different times, so a given event can give rise to a succession of discriminations, constituting the equivalent of multiple drafts of a story. As soon as each discrimination is accomplished, it becomes available for eliciting a behaviour; it does not have to wait to be presented at the theatre.\n\nLike a number of other theories, the Multiple Drafts model understands conscious experience as taking time to occur, such that percepts do not instantaneously arise in the mind in their full richness. The distinction is that Dennett's theory denies any clear and unambiguous boundary separating conscious experiences from all other processing. According to Dennett, consciousness is to be found in the actions and flows of information from place to place, rather than some singular view containing our experience. There is no central experiencer who confers a durable stamp of approval on any particular draft.\n\nDifferent parts of the neural processing assert more or less control at different times. For something to reach consciousness is akin to becoming famous, in that it must leave behind consequences by which it is remembered. To put it another way, consciousness is the property of having enough influence to affect what the mouth will say and the hands will do. Which inputs are \"edited\" into our drafts is not an exogenous act of supervision, but part of the self-organizing functioning of the network, and at the same level as the circuitry that conveys information bottom-up.\n\nThe conscious self is taken to exist as an abstraction visible at the level of the intentional stance, akin to a body of mass having a \"centre of gravity\". Analogously, Dennett refers to the self as the \"centre of narrative gravity\", a story we tell ourselves about our experiences. Consciousness exists, but not independently of behaviour and behavioural disposition, which can be studied through heterophenomenology.\n\nThe origin of this operationalist approach can be found in Dennett's immediately preceding work. Dennett (1988) explains consciousness in terms of \"access consciousness\" alone, denying the independent existence of what Ned Block has labeled \"phenomenal consciousness\". He argues that \"Everything real has properties, and since I don't deny the reality of conscious experience, I grant that conscious experience has properties\". Having related all consciousness to properties, he concludes that they cannot be meaningfully distinguished from our judgements about them. He writes: \nIn other words, once we've explained a perception fully in terms of how it affects us, there is nothing left to explain. In particular, there is no such thing as a perception which may be considered in and of itself (a quale). Instead, the subject's honest reports of how things seem to them are inherently authoritative on how things seem to them, but not on the matter of how things actually are.\nThe key to the multiple drafts model is that, after removing qualia, explaining consciousness boils down to explaining the behaviour we recognise as conscious. Consciousness is as consciousness does.\n\nSome of the criticism of Dennett's theory is due to the perceived tone of his presentation. As one grudging supporter admits, \"there is much in this book that is disputable. And Dennett is at times aggravatingly smug and confident about the merits of his arguments ... All in all Dennett's book is annoying, frustrating, insightful, provocative and above all annoying\" (Korb 1993).\n\nBogen (1992) points out that the brain is bilaterally symmetrical. That being the case, if Cartesian materialism is true, there might be \"two\" Cartesian theatres, so arguments against only one are flawed. Velmans (1992) argues that the phi effect and the cutaneous rabbit illusion demonstrate that there is a delay whilst modelling occurs and that this delay was discovered by Libet.\n\nIt has also been claimed that the argument in the multiple drafts model does not support its conclusion.\n\nMuch of the criticism asserts that Dennett's theory attacks the wrong target, failing to explain what it claims to. Chalmers (1996) maintains that Dennett has produced no more than a theory of how subjects report events. Some even parody the title of the book as \"Consciousness Explained Away\", accusing him of greedy reductionism. Another line of criticism disputes the accuracy of Dennett's characterisations of existing theories:\nMultiple drafts is also attacked for making a claim to novelty. It may be the case, however, that such attacks mistake which features Dennett is claiming as novel. Korb states that, \"I believe that the central thesis will be relatively uncontentious for most cognitive scientists, but that its use as a cleaning solvent for messy puzzles will be viewed less happily in most quarters.\" (Korb 1993) In this way, Dennett uses uncontroversial ideas towards more controversial ends, leaving him open to claims of unoriginality when uncontroversial parts are focused upon.\n\nEven the notion of consciousness as drafts is not unique to Dennett. According to Hankins, Dieter Teichert suggests that Paul Ricoeur's theories agree with Dennett's on the notion that \"the self is basically a narrative entity, and that any attempt to give it a free-floating independent status is misguided.\" [Hankins] Others see Derrida's (1982) representationalism as consistent with the notion of a mind that has perceptually changing content without a definitive present instant.\n\nTo those who believe that consciousness entails something more than behaving in all ways conscious, Dennett's view is seen as eliminativist, since it denies the existence of qualia and the possibility of philosophical zombies. However, Dennett is not denying the existence of the mind or of consciousness, only what he considers a naive view of them. The point of contention is whether Dennett's own definitions are indeed more accurate: whether what we think of when we speak of perceptions and consciousness can be understood in terms of nothing more than their effect on behaviour.\n\nThe role of information processing in consciousness has been criticised by John Searle who, in his Chinese room argument, states that he cannot find anything that could be recognised as conscious experience in a system that relies solely on motions of things from place to place. Dennett sees this argument as misleading, arguing that consciousness is not to be found in a specific part of the system, but in the actions of the whole. In essence, he denies that consciousness requires something in addition to capacity for behaviour, saying that philosophers such as Searle, \"just can't imagine how understanding could be a property that emerges from lots of distributed quasi-understanding in a large system\" (p. 439).\n\n\n\n\n"}
{"id": "57122", "url": "https://en.wikipedia.org/wiki?curid=57122", "title": "Multiplication table", "text": "Multiplication table\n\nIn mathematics, a multiplication table (sometimes, less formally, a times table) is a mathematical table used to define a multiplication operation for an algebraic system.\n\nThe decimal multiplication table was traditionally taught as an essential part of elementary arithmetic around the world, as it lays the foundation for arithmetic operations with base-ten numbers. Many educators believe it is necessary to memorize the table up to 9 × 9.\n\nThe oldest known multiplication tables were used by the Babylonians about 4000 years ago. However, they used a base of 60. The oldest known tables using a base of 10 are the Chinese decimal multiplication table on bamboo strips dating to about 305 BC, during China's Warring States period.\nThe multiplication table is sometimes attributed to the ancient Greek mathematician Pythagoras (570–495 BC). It is also called the Table of Pythagoras in many languages (for example French, Italian and at one point even Russian), sometimes in English. The Greco-Roman mathematician Nichomachus (60–120 AD), a follower of Neopythagoreanism, included a multiplication table in his \"Introduction to Arithmetic\", whereas the oldest surviving Greek multiplication table is on a wax tablet dated to the 1st century AD and currently housed in the British Museum.\n\nIn 493 AD, Victorius of Aquitaine wrote a 98-column multiplication table which gave (in Roman numerals) the product of every number from 2 to 50 times and the rows were \"a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144.\"\n\nIn his 1820 book \"The Philosophy of Arithmetic\", mathematician John Leslie published a multiplication table up to 99 × 99, which allows numbers to be multiplied in pairs of digits at a time. Leslie also recommended that young pupils memorize the multiplication table up to 25 × 25. The illustration below shows a table up to 12 × 12, which is a size commonly used in schools.\n\nThe traditional rote learning of multiplication was based on memorization of columns in the table, in a form like\n\n<poem>\n</poem>\nThis form of writing the multiplication table in columns with complete number sentences is still used in some countries, such as Bosnia and Herzegovina, instead of the modern grid above.\n\nThere is a pattern in the multiplication table that can help people to memorize the table more easily. It uses the figures below:\n\nFigure 1 is used for multiples of 1, 3, 7, and 9. Figure 2 is used for the multiples of 2, 4, 6, and 8. These patterns can be used to memorize the multiples of any number from 0 to 10, except 5. As you would start on the number you are multiplying, when you multiply by 0, you stay on 0 (0 is external and so the arrows have no effect on 0, otherwise 0 is used as a link to create a perpetual cycle). The pattern also works with multiples of 10, by starting at 1 and simply adding 0, giving you 10, then just apply every number in the pattern to the \"tens\" unit as you would normally do as usual to the \"ones\" unit.\nFor example, to recall all the multiples of 7:\n\n\nTables can also define binary operations on groups, fields, rings, and other algebraic systems. In such contexts they can be called Cayley tables. Here are the addition and multiplication tables for the finite field Z.\n\nFor every natural number \"n\", there are also addition and multiplication tables for the ring Z.\n\nFor other examples, see group, and octonion.\n\nThe Chinese multiplication table consists of eighty-one sentences with four or five Chinese characters per sentence, making it easy for children to learn by heart. A shorter version of the table consists of only forty-five sentences, as terms such as \"nine eights beget seventy-two\" are identical to \"eight nines beget seventy-two\" so there is no need to learn them twice.\n\nA bundle of 21 bamboo slips dated 305 BC in the Warring States period in the Tsinghua Bamboo Slips (清华简) collection is the world's earliest known example of a decimal multiplication table.\nIn 1989, the National Council of Teachers of Mathematics (NCTM) developed new standards which were based on the belief that all students should learn higher-order thinking skills, and which recommended reduced emphasis on the teaching of traditional methods that relied on rote memorization, such as multiplication tables. Widely adopted texts such as Investigations in Numbers, Data, and Space (widely known as TERC after its producer, Technical Education Research Centers) omitted aids such as multiplication tables in early editions. NCTM made it clear in their 2006 Focal Points that basic mathematics facts must be learned, though there is no consensus on whether rote memorization is the best method.\n\n"}
{"id": "234029", "url": "https://en.wikipedia.org/wiki?curid=234029", "title": "Negative cache", "text": "Negative cache\n\nIn computer programming, negative cache is a cache that also stores \"negative\" responses, i.e. failures. This means that a program remembers the result indicating a failure even after the cause has been corrected. Usually negative cache is a design choice, but it can also be a software bug.\n\nConsider a web browser which attempts to load a page while the network is unavailable. The browser will receive an error code indicating the problem, and may display this error message to the user in place of the requested page. However, it is incorrect for the browser to place the error message in the page cache, as this would lead it to display the error again when the user tries to load the same page - even after the network is back up. The error message must not be cached under the page's URL; until the browser is able to successfully load the page, whenever the user tries to load the page, the browser must make a new attempt.\n\nA frustrating aspect of negative caches is that the user may put a great effort into troubleshooting the problem, and then after determining and removing the root cause, the error still does not vanish.\n\nThere are cases where failure-like states must be cached. For instance, DNS requires that caching nameservers remember negative responses as well as positive ones. If an authoritative nameserver returns a negative response, indicating that a name does not exist, this is cached. The negative response may be perceived as a failure at the application level; however, to the nameserver caching it, it is not a failure. The cache times for negative and positive caching may be tuned independently.\n\nA negative cache is normally only desired if failure is very expensive and the error condition arises automatically without user's action. It creates a situation where the user is unable to isolate the cause of the failure: despite fixing everything he/she can think of, the program still refuses to work. When a failure is cached, the program should provide a clear indication of what must be done to clear the cache, in addition to a description of the cause of the error. In such conditions a negative cache is an example of a design anti-pattern.\n\nNegative cache still may recover if the cached records expires.\n\n"}
{"id": "677516", "url": "https://en.wikipedia.org/wiki?curid=677516", "title": "Negative campaigning", "text": "Negative campaigning\n\nNegative campaigning or mudslinging is the process of deliberate spreading negative information about someone or something to worsen the public image of the described.\n\nDeliberate spreading of such information can be motivated either by honest desire of the campaigner to warn others against real dangers or deficiencies of the described, or by the campaigner's dishonest ideas on methods of winning in political, business or other spheres of competition against an honest rival.\n\nThe public image of an entity can be defined as reputation, esteem, respect, acceptance of the entity's appearance, values and behaviour by the general public of a given territory and/or a social group, possibly within time limits. As target groups of public and their values differ, so negativity or positivity of a public image is relative: e.g. while in most societies having an honest source of income is a positive value and stealing is discouraged, in the world of professional thieves honest work is frowned upon and stealing is encouraged. In polygamous societies monogamy is not viewed in the way it is valued in monogamous societies. Values of a society also change with time: e.g. homosexuality in Western culture was considered immoral and was criminally prosecuted until the sexual revolution of the second half of the 20 century.\nThus negative campaigning to be successful has to take into account current values of the group it addresses. The degree of strictness in practicing the group's values as opposed to its tolerance for violating the norms has also to be taken into consideration: e.g. while in the Old Testament and other traditional religious societies adultery and prostitution were outlawed and supposed to be punished by death, modern Western societies show much greater tolerance to these.\n\nIn United States politics, negative campaigning has been called \"as American as Mississippi mud\" and \"as American as apple pie\". Some research suggests negative campaigning is the norm in all political venues, mitigated only by the dynamics of a particular contest.\n\nThere are a number of techniques used in negative campaigning. Among the most effective is running advertisements attacking an opponent's personality, record, or opinion. There are two main types of ads used in negative campaigning: attack and contrast.\n\nAttack ads focus exclusively on the negative aspects of the opponent. There is no positive content in an attack ad, whether it is about the candidate or the opponent. Attack ads usually identify the risks associated with the opponent, often exploiting people’s fears to manipulate and lower the impression voters have of the opponent. Because attack ads have no positive content, they have the potential to be more influential than contrast ads in shaping voters’ views of the sponsoring candidate’s opponent.\n\nUnlike attack ads, contrast ads contain information about both the candidate and the opponent. The information about the candidate is positive, while the information about the opponent is negative. Contrast ads compare and contrast the candidate with the opponent, juxtaposing the positive information about the candidate with the negative information of the opponent. Because contrast ads must contain positive information, contrast ads are seen as less damaging to the political process than attack ads.\n\nOne of the most famous such ads was \"Daisy Girl\" by the campaign of Lyndon B. Johnson that successfully portrayed Republican Barry Goldwater as threatening nuclear war. Common negative campaign techniques include painting an opponent as soft on criminals, dishonest, corrupt, or a danger to the nation. One common negative campaigning tactic is attacking the other side for running a negative campaign.\n\nDirty tricks are also common in negative political campaigns. These generally involve secretly leaking damaging information to the media. This isolates a candidate from backlash and also does not cost any money. The material must be substantive enough to attract media interest, however, and if the truth is discovered it could severely damage a campaign. Other dirty tricks include trying to feed an opponent's team false information hoping they will use it and embarrass themselves.\n\nOften a campaign will use outside organizations, such as lobby groups, to launch attacks. These can be claimed to be coming from a neutral source and if the allegations turn out not to be true the attacking candidate will not be damaged if the links cannot be proven. Negative campaigning can be conducted by proxy. For instance, highly partisan ads were placed in the 2004 U.S. presidential election by allegedly independent bodies like MoveOn.org and Swift Boat Veterans for Truth.\n\nPush polls are attacks disguised as telephone polls. They might ask a question like \"How would you react if Candidate A was revealed to beat his wife?\", giving the impression that Candidate A might beat his wife. Members of the media and of the opposing party are deliberately not called making these tactics all but invisible and unprovable.\n\nG. Gordon Liddy played a major role in developing these tactics during the Nixon campaign playing an important advisory of rules that led to the campaign of 1972. James Carville, campaign manager of Bill Clinton's 1992 election, is also a major proponent of negative tactics. Lee Atwater, best known for being an advisor to presidents Ronald Reagan and George H.W. Bush, also pioneered many negative campaign techniques seen in political campaigns today.\n\nSponsors of overt negative campaigns often cite reasons to support mass communication of negative ideas. The Office of National Drug Control Policy uses negative campaigns to steer the public away from health risks. Similar negative campaigns have been used to rebut mass marketing by tobacco companies, or to discourage drunk driving. Those who conduct negative political campaigns sometimes say the public needs to know about the person he or she is voting for, even if it is bad. In other words, if a candidate’s opponent is a crook or a bad person, then he or she should be able to tell the public about it.\n\nMartin Wattenberg and Craig Brians, of the University of California, Irvine, considered in their study whether negative campaigning mobilizes or alienates voters. They concluded that data used by Stephen Ansolabehere in a 1994 American Political Science Review article to advance the hypothesis that negative campaigning demobilizes voters was flawed.\n\nA subsequent study done by Ansolabehere and Shanto Iyengar in 1995 corrected some of the previous study's flaws. This study concluded that negative advertising suppressed voter turnout, particularly for Independent voters. They speculated that campaigns tend to go negative only if the Independent vote is leaning toward the opponent. In doing so, they insure that the swing voters stay home, leaving the election up to base voters. They also found that negative ads have a greater impact on Democrats than on Republicans. According to them, base Republicans will vote no matter what (and will vote only for a Republican), but Democrats can be influenced to either stay home and not vote at all or to switch sides and vote for a Republican. This, combined with the effect negativity has on Independents, led them to conclude that Republicans benefit more from going negative than Democrats.\n\nOther researchers have found different, more positive outcomes from negative campaigns. Rick Farmer, PhD, an assistant professor of political science at the University of Akron found that negative ads are more memorable than positive ads when they reinforce a preexisting belief and are relevant to the central issues of a marketing campaign. Researchers at the University of Georgia found the impact of negative ads increases over time, while positive ads used to counteract negative ads lack the power of negative ads . Research also suggests negative campaigning introduces controversy and raises public awareness through additional news coverage .\n\nMost recently, Kyle Mattes and David P. Redlawsk in \"The Positive Case for Negative Campaigning\" show through surveys and experiments that negative campaigning provides informational benefits for voters. Without negativity, voters would not have full information about all of their choices, since no candidate will say anything bad about herself. They argue that candidates have to point out the flaws in their opponents for voters to be fully informed.\n\nSome strategists say that an effect of negative campaigning is that while it motivates the base of support it can alienate centrist and undecided voters from the political process, reducing voter turnout and radicalizing politics. \nIn a study done by Gina Garramone about how negative advertising affects the political process, it was found that a consequence of negative campaigning is greater image discrimination of the candidates and greater attitude polarization. While positive ads also contributed to the image discrimination and attitude polarization, Garramone found that negative campaigning played a more influential role in the discrimination and polarization than positive campaigning.\n\nNegative ads can produce a backlash. A disastrous ad was run by the Progressive Conservative Party of Canada in the 1993 Canadian federal election, apparently emphasizing Liberal Party of Canada leader Jean Chrétien's Bell's Palsy partial facial paralysis in a number of unflattering photos, with the subtext of criticizing his platforms. Chrétien took maximum advantage of the opportunity to gain the public's sympathy as a man who struggled with a physical disability and his party's subsequent overwhelming victory in the election helped reduce the governing Conservatives to two seats.\n\nA similar backlash happened to the Liberal Party in the 2006 federal election for running an attack ad that suggested that Conservative leader Stephen Harper would use Canadian soldiers to patrol Canadian cities, and impose some kind of martial law. The ad was only available from the Liberal Party's web site for a few hours prior to the release of the attack ads on television; nevertheless, it was picked up by the media and widely criticized for its absurdity, in particular the sentence \"we're not making this up; we're not allowed to make this stuff up\". Liberal MP Keith Martin expressed his disapproval of \"whoever the idiot who approved that ad was,\" shortly before Liberal leader Paul Martin (no relation) stated that he had personally approved them. The effect of the ads was to diminish the credibility of the party's other attack ads. It offended many Canadians, particularly those in the military, some of whom were fighting in Afghanistan at the time. (See Canadian federal election, 2006)\n\nMore recently, in the 2008 US Senate race in North Carolina, Republican incumbent Elizabeth Dole attempted an attack ad on Democratic challenger Kay Hagan, who had taken a small lead in polls, by tying her to atheists. Dole's campaign released an ad questioning Hagan's religion and it included a voice saying \"There is no God!\" over a picture of Kay Hagan's face. The voice was not Hagan's but it is believed the ad implied that it was. Initially, it was thought the ad would work as religion has historically been a very important issue to voters in the American south, but the ad produced a backlash across the state and Hagan responded forcefully with an ad saying that she was a Sunday school teacher and was a religious person. Hagan also claimed Dole was trying to change the subject from the economy (the ad appeared around the same time as the 2008 financial crisis). Hagan's lead in polls doubled and she won the race by a nine-point margin.\n\nBecause of the possible harm that can come from being seen as a negative campaigner, candidates often pledge to refrain from negative attacks. This pledge is usually abandoned when an opponent is perceived to be \"going negative,\" with the first retaliatory attack being, ironically, an accusation that the opponent is a negative campaigner.\n\nWhile some research has found advantages and other has found disadvantages, some studies find no difference between negative and positive approaches .\n\nResearch published in the Journal of Advertising found that negative political advertising makes the body want to turn away physically, but the mind remembers negative messages. The findings are based on research conducted by James Angelini, professor of communication at the University of Delaware, in collaboration with Samuel Bradley, assistant professor of advertising at Texas Tech University, and Sungkyoung Lee of Indiana University, which used ads that aired during the 2000 presidential election. During the study, the researchers placed electrodes under the eyes of willing participants and showed them a series of 30-second ads from both the George W. Bush and Al Gore campaigns. The electrodes picked up on the \"startle response,\" the automatic eye movement typically seen in response to snakes, spiders and other threats. Compared to positive or neutral messages, negative advertising prompted greater reflex reactions and a desire to move away.\n\n\n\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "17235432", "url": "https://en.wikipedia.org/wiki?curid=17235432", "title": "Negative elongation factor", "text": "Negative elongation factor\n\nIn molecular biology, NELF (negative elongation factor) is a four-subunit protein (NELF-A, NELF-B, NELF-C/NELF-D, and NELF-E) that negatively impacts transcription by RNA polymerase II (Pol II) by pausing about 20-60 nucleotides downstream from the transcription start site (TSS).\n\nThe NELF-A subunit is encoded by the gene WHSC2 (Wolf-Hirschhorn syndrome candidate 2). Microsequencing analysis demonstrated that NELF-B was the protein previously identified as the protein encoded by the gene COBRA1, and was shown to interact with BRCA1. It is unknown whether or not NELF-C and NELF-D are peptides resulting from the same mRNA with different translation initiation sites, possibly differing only in an extra 9 amino acids for NELF-C at the N-terminus, or peptides from different mRNAs entirely. A single NELF complex consists of either NELF-C or NELF-D but not both. NELF-E is also known as RDBP.\n\nNELF binds in a stable complex with DSIF and RNA polymerase II together, but not with either alone. P-TEFb (positive transcription elongation factor b) inhibits the effect of NELF and DSIF on Pol II elongation, via its phosphorylation of serine-2 of the C-terminal domain of Pol II, and the SPT5 subunit of DSIF, causing dissociation of NELF. NELF homologues exist in some metazoans (e.g. insects and vertebrates) but have not been found in plants, yeast, or nematode (worms).\n"}
{"id": "454151", "url": "https://en.wikipedia.org/wiki?curid=454151", "title": "Negative liberty", "text": "Negative liberty\n\nNegative liberty is freedom from interference by other people. Negative liberty is primarily concerned with freedom from external restraint and contrasts with positive liberty (the possession of the power and resources to fulfil one's own potential). The distinction was introduced by Isaiah Berlin in his 1958 lecture \"Two Concepts of Liberty\".\n\n\"Stanford Encyclopedia of Philosophy\" describes negative liberty:\n\"The negative concept of freedom ... is most commonly assumed in liberal defences of the constitutional liberties typical of liberal-democratic societies, such as freedom of movement, freedom of religion, and freedom of speech, and in arguments against paternalist or moralist state intervention. It is also often invoked in defences of the right to private property, although some have contested the claim that private property necessarily enhances negative liberty.\"\n\nAccording to Thomas Hobbes, \"a free man is he that in those things which by his strength and will he is able to do is not hindered to do what he hath the will to do\" (\"Leviathan\", Part 2, Ch. XXI; thus alluding to liberty in its negative sense).\n\nClaude Adrien Helvétius expressed the following point clearly: \"The free man is the man who is not in irons, nor imprisoned in a gaol, nor terrorized like a slave by the fear of punishment ... it is not lack of freedom, not to fly like an eagle or swim like a whale.\" Moreover, John Jay, in \"The Federalist\" paper No. 2, stated that: \"Nothing is more certain than the indispensable necessity of Government, and it is equally undeniable, that whenever and however it is instituted, the people must cede to it some of their natural rights, in order to vest it with requisite powers.\" Jay's meaning would be better expressed by substituting \"negative liberty\" in place of \"natural rights\", for the argument here is that the power or authority of a legitimate government derives in part from our accepting restrictions on negative liberty.\n\nAn idea that anticipates the distinction between negative and positive liberty was G. F. W. Hegel's \"sphere of abstract right\" (furthered in his \"Elements of the Philosophy of Right\"), which constitutes what now is called negative freedom and his subsequent distinction between \"abstract\" and \"positive liberty.\" \n\nIn the Anglophone analytic tradition the distinction between negative and positive liberty was introduced by Isaiah Berlin in his 1958 lecture \"Two Concepts of Liberty\". According to Berlin, the distinction is deeply embedded in the political tradition. In Berlin's words, \"liberty in the negative sense involves an answer to the question: 'What is the area within which the subject—a person or group of persons—is or should be left to do or be what he is able to do or be, without interference by other persons'.\" Restrictions on negative liberty are imposed by a person, not by natural causes or incapacity.\n\nFrankfurt School psychoanalyst and humanistic philosopher Erich Fromm drew a similar distinction between negative and positive freedom in his 1941 work, \"The Fear of Freedom\", that predates Berlin's essay by more than a decade. Fromm sees the distinction between the two types of freedom emerging alongside humanity's evolution away from the instinctual activity that characterizes lower animal forms. This aspect of freedom, he argues, \"is here used not in its positive sense of 'freedom to' but in its negative sense of 'freedom from', namely freedom from instinctual determination of his actions.\" For Fromm, then, negative freedom marks the beginning of humanity as a species conscious of its own existence free from base instinct.\n\nThe distinction between positive and negative liberty is considered specious by some socialist and Marxist political philosophers, who argue that positive and negative liberty are indistinguishable in practice, or that one cannot exist without the other. Although he is not a socialist nor a Marxist, Berlin argues: \"It follows that a frontier must be drawn between the area of private life and that of public authority. Where it is to be drawn is a matter of argument, indeed of haggling. Men are largely interdependent, and no man's activity is so completely private as never to obstruct the lives of others in any way. 'Freedom for the pike is death for the minnows'; the liberty of some must depend on the restraint of others.\"\n\nLibertarian thinker Tibor Machan defends negative liberty as \"required for moral choice and, thus, for human flourishing,\" claiming that it \"is secured when the rights of individual members of a human community to life, to voluntary action (or to liberty of conduct), and to property are universally respected, observed, and defended.\"\n\nOne might ask, \"How is men's desire for liberty to be reconciled with the assumed need for authority?\" Its answer by various thinkers provides a fault line for understanding their view on liberty but also a cluster of intersecting concepts such as authority, equality, and justice.\n\nHobbes and Locke give two influential and representative solutions to this question. As a starting point, both agree that a line must be drawn and a space sharply delineated where each individual can act unhindered according to their tastes, desires, and inclinations. This zone defines the sacrosanct space of personal liberty. But, they believe no society is possible without some authority, where the intended purpose of authority is to prevent collisions among the different ends and, thereby, to demarcate the boundaries where each person's zone of liberty begins and ends. Where Hobbes and Locke differ is the extent of the zone. Hobbes, who took a rather negative view of human nature, argued that a strong authority was needed to curb men's intrinsically wild, savage, and corrupt impulses. Only a powerful authority can keep at bay the permanent and always looming threat of anarchy. Locke believed, on the other hand, that men on the whole are more good than wicked and, accordingly, the area for individual liberty can be left rather at large.\n\nLocke is a slightly more ambiguous case than Hobbes because although his conception of liberty was largely negative (in terms of non-interference), he differed in that he courted the republican tradition of liberty by rejecting the notion that an individual could be free if he was under the arbitrary power of another:\n\n\"This \"freedom\" from absolute, arbitrary power, is so necessary to, and closely joined with a man's preservation, that he cannot part with it, but by what forfeits his preservation and life together: for a man, not having the power of his own life, cannot, by compact, or his own consent, enslave himself to any one, nor put himself under the absolute, arbitrary power of another, to take away his life, when he pleases. No body can give more power than he has himself; and he that cannot take away his own life, cannot give another power over it. Indeed, having by his fault forfeited his own life, by some act that deserves death; he, to whom he has forfeited it, may (when he has him in his power) delay to take it, and make use of him to his own service, and he does him no injury by it: for, whenever he finds the hardship of his slavery outweigh the value of his life, it is in his power, by resisting the will of his master, to draw on himself the death he desires.\"\n\nThis section outlines specific examples of governmental types which follow the concept of negative liberty.\n\nThomas Hobbes' \"Leviathan\" outlines a commonwealth based upon a monarchy to whom citizens have ceded their rights. The basic reasoning for Hobbes' assertion that this system was most ideal relates more to Hobbes' value of order and simplicity in government. The monarchy provides for its subjects, and its subjects go about their day-to-day lives without interaction with the government:\nThe commonwealth is instituted when all agree in the following manner: \"I authorise and give up my right of governing myself to this man, or to this assembly of men, on this condition; that thou give up, thy right to him, and authorise all his actions in like manner.\"\n\nThe sovereign has twelve principal rights:\n\n\nHobbes explicitly rejects the idea of \"Separation of Powers\", in particular the form that would later become the separation of powers under the United States Constitution. Part 6 is a perhaps under-emphasised feature of Hobbes's argument: his is explicitly in favour of censorship of the press and restrictions on the rights of free speech, should they be considered desirable by the sovereign in order to promote order.\nUpon closer inspection of Hobbes' \"Leviathan\", it becomes clear that Hobbes believed individual people in society must give up liberty to a sovereign. Whether that sovereign is an absolute monarch or other form was left open to debate, however Hobbes himself viewed the absolute monarch as the best of all options. Hobbes himself said,\n\nFor as amongst masterless men, there is perpetual war, of every man against his neighbour; no inheritance, to transmit to the son, nor to expect from the father; no propriety of goods, or lands; no security; but a full and absolute liberty in every particular man: so in states, and commonwealths not dependent on one another, every commonwealth, not every man, has an absolute liberty, to do what it shall judge, that is to say, what that man, or assembly that representeth it, shall judge most conducing to their benefit.\n\nFrom this quote it is clear that Hobbes contended that people in a state of nature ceded their individual rights to create sovereignty, retained by the state, in return for their protection and a more functional society. In essence, a social contract between the sovereign and citizens evolves out of pragmatic self-interest. Hobbes named the state \"Leviathan\", thus pointing to the artifice involved in the social contract. In this vein, Hobbes' concept of negative liberty was built upon the notion that the state would not act upon its subjects because its subjects had willingly relinquished their liberties.\n\n\n\n"}
{"id": "262606", "url": "https://en.wikipedia.org/wiki?curid=262606", "title": "Negative mass", "text": "Negative mass\n\nIn theoretical physics, negative mass is matter whose mass is of opposite sign to the mass of normal matter, e.g. −1 kg. Such matter would violate one or more energy conditions and show some strange properties, stemming from the ambiguity as to whether attraction should refer to force or the oppositely oriented acceleration for negative mass. It is used in certain speculative hypothesis, such as on the construction of traversable wormholes and the Alcubierre drive. Initially, the closest known real representative of such exotic matter is a region of negative pressure density produced by the Casimir effect.\n\nGeneral relativity describes gravity and the laws of motion for both positive and negative energy particles, hence negative mass, but does not include the other fundamental forces. On the other hand, the Standard Model describes elementary particles and the other fundamental forces, but it does not include gravity. A unified theory that explicitly includes gravity along with the other fundamental forces may be needed for a better understanding of the concept of negative mass.\n\nNegative mass is any region of space in which for some observers the mass density is measured to be negative. This could occur due to a region of space in which the stress component of the Einstein stress–energy tensor is larger in magnitude than the mass density. All of these are violations of one or another variant of the positive energy condition of Einstein's general theory of relativity; however, the positive energy condition is not a required condition for the mathematical consistency of the theory.\n\nEver since Newton first formulated his theory of gravity, there have been at least three conceptually distinct quantities called mass:\n\nEinstein’s equivalence principle postulates that inertial mass must equal passive gravitational mass. The law of conservation of momentum requires that active and passive gravitational mass be identical. All experimental evidence to date has found these are, indeed, always the same. In considering negative mass, it is important to consider which of these concepts of mass are negative. In most analyses of negative mass, it is assumed that the equivalence principle and conservation of momentum continue to apply, and therefore all three forms of mass are still the same.\n\nIn his 4th-prize essay for the 1951 Gravity Research Foundation competition, Joaquin Mazdak Luttinger considered the possibility of negative mass and how it would behave under gravitational and other forces.\n\nIn 1957, following Luttinger's idea, Hermann Bondi suggested in a paper in \"Reviews of Modern Physics\" that mass might be negative as well as positive. He pointed out that this does not entail a logical contradiction, as long as all three forms of mass are negative, but that the assumption of negative mass involves some counter-intuitive form of motion. For example, an object with negative inertial mass would be expected to accelerate in the opposite direction to that in which it was pushed (non-gravitationally).\n\nThere have been several other analyses of negative mass, such as the studies conducted by R. M. Price, however none addressed the question of what kind of energy and momentum would be necessary to describe non-singular negative mass. Indeed, the Schwarzschild solution for negative mass parameter has a naked singularity at a fixed spatial position. The question that immediately comes up is, would it not be possible to smooth out the singularity with some kind of negative mass density. The answer is yes, but not with energy and momentum that satisfies the dominant energy condition. This is because if the energy and momentum satisfies the dominant energy condition within a spacetime that is asymptotically flat, which would be the case of smoothing out the singular negative mass Schwarzschild solution, then it must satisfy the positive energy theorem, i.e. its ADM mass must be positive, which is of course not the case. However, it was noticed by Belletête and Paranjape that since the positive energy theorem does not apply to asymptotic de Sitter spacetime, it would actually be possible to smooth out, with energy–momentum that does satisfy the dominant energy condition, the singularity of the corresponding exact solution of negative mass Schwarzschild–de Sitter, which is the singular, exact solution of Einstein's equations with cosmological constant. In a subsequent article, Mbarek and Paranjape showed that it is in fact possible to obtain the required deformation through the introduction of the energy–momentum of a perfect fluid.\n\nAlthough no particles are known to have negative mass, physicists (primarily Hermann Bondi in 1957, William B. Bonnor in 1989, then Robert L. Forward) have been able to describe some of the anticipated properties such particles may have. Assuming that all three concepts of mass are equivalent the gravitational interactions between masses of arbitrary sign can be explored, based on the Einstein field equations and the equivalence principle:\n\n\nFor two positive masses, nothing changes and there is a gravitational pull on each other causing an attraction. Two negative masses would repel because of their negative inertial masses. For different signs however, there is a push that repels the positive mass from the negative mass, and a pull that attracts the negative mass towards the positive one at the same time.\n\nHence Bondi pointed out that two objects of equal and opposite mass would produce a constant acceleration of the system towards the positive-mass object, an effect called \"runaway motion\" by Bonnor who disregarded its physical existence, stating: \nSuch a couple of objects would accelerate without limit (except relativistic one); however, the total mass, momentum and energy of the system would remain 0.\n\nThis behavior is completely inconsistent with a common-sense approach and the expected behaviour of 'normal' matter; but is completely mathematically consistent and introduces no violation of conservation of momentum or energy. If the masses are equal in magnitude but opposite in sign, then the momentum of the system remains zero if they both travel together and accelerate together, no matter what their speed:\n\nAnd equivalently for the kinetic energy:\n\nHowever, this is perhaps not exactly valid if the energy in the gravitational field is taken into account.\n\nForward extended Bondi's analysis to additional cases, and showed that even if the two masses and are not the same, the conservation laws remain unbroken. This is true even when relativistic effects are considered, so long as inertial mass, not rest mass, is equal to gravitational mass.\n\nThis behaviour can produce bizarre results: for instance, a gas containing a mixture of positive and negative matter particles will have the positive matter portion increase in temperature without bound. However, the negative matter portion gains negative temperature at the same rate, again balancing out. Geoffrey A. Landis pointed out other implications of Forward's analysis, including noting that although negative mass particles would repel each other gravitationally, the electrostatic force would be attractive for like charges and repulsive for opposite charges.\n\nForward used the properties of negative-mass matter to create the concept of diametric drive, a design for spacecraft propulsion using negative mass that requires no energy input and no reaction mass to achieve arbitrarily high acceleration.\n\nForward also coined a term, \"nullification\", to describe what happens when ordinary matter and negative matter meet: they are expected to be able to cancel out or nullify each other's existence. An interaction between equal quantities of positive mass matter (hence of positive energy ) and negative mass matter (of negative energy ) would release no energy, but because the only configuration of such particles that has zero momentum (both particles moving with the same velocity in the same direction) does not produce a collision, all such interactions would leave a surplus of momentum, which is classically forbidden. So once this runaway phenomenon has been revealed, the scientific community considered negative mass could not exist in the universe.\n\nIn 1970, Jean-Marie Souriau demonstrated, through the complete Poincaré group of dynamic group theory, that reversing the energy of a particle (hence its mass, if the particle has one) is equal to reversing its arrow of time.\n\nThe universe according to general relativity is a Riemannian manifold associated to a metric tensor solution of Einstein’s field equations. In such a framework, the runaway motion prevents the existence of negative matter.\n\nSome bimetric theories of the universe propose that two parallel universes instead of one may exist with an opposite arrow of time, linked together by the Big Bang and interacting only through gravitation. The universe is then described as a manifold associated to two Riemannian metrics (one with positive mass matter and the other with negative mass matter). According to group theory, the matter of the conjugated metric would appear to the matter of the other metric as having opposite mass and arrow of time (though its proper time would remain positive). The coupled metrics have their own geodesics and are solutions of two coupled field equations:\n\nThe Newtonian approximation then provides the following interaction laws:\nThose laws are different to the laws described by Bondi and Bonnor, and solve the runaway paradox. The negative matter of the coupled metric, interacting with the matter of the other metric via gravity, could be an alternative candidate for the explanation of dark matter, dark energy, cosmic inflation and accelerating universe.\n\nIn electromagnetism one can derive the energy density of a field from Gauss's law, assuming the curl of the field is 0. Performing the same calculation using Gauss's law for gravity produces a negative energy density for a gravitational field.\n\nThe overwhelming consensus among physicists is that antimatter has positive mass and should be affected by gravity just like normal matter. Direct experiments on neutral antihydrogen have not been sensitive enough to detect any difference between the gravitational interaction of antimatter, compared to normal matter.\n\nBubble chamber experiments provide further evidence that antiparticles have the same inertial mass as their normal counterparts. In these experiments, the chamber is subjected to a constant magnetic field that causes charged particles to travel in helical paths, the radius and direction of which correspond to the ratio of electric charge to inertial mass. Particle–antiparticle pairs are seen to travel in helices with opposite directions but identical radii, implying that the ratios differ only in sign; but this does not indicate whether it is the charge or the inertial mass that is inverted. However, particle–antiparticle pairs are observed to electrically attract one another. This behavior implies that both have positive inertial mass and opposite charges; if the reverse were true, then the particle with positive inertial mass would be repelled from its antiparticle partner.\n\nPhysicist Peter Engels and a team of colleagues at Washington State University claimed to have observed negative mass behavior in rubidium atoms. On 10 April 2017 Engels team created negative \"effective\" mass by reducing the temperature of rubidium atoms to near absolute zero, generating a Bose–Einstein condensate. By using a laser-trap, the team were able to reverse the spin of some of the rubidium atoms in this state, and observed that once released from the trap, the atoms expanded and displayed properties of negative mass, in particular accelerating towards a pushing force instead of away from it.\nThis kind of negative effective mass is analogous to the well-known apparent negative effective mass of electrons in the upper part of the dispersion bands in solids. However, neither case is negative mass for the purposes of the stress–energy tensor.\n\nSome recent work with metamaterials suggests that some as-yet-undiscovered composite of superconductors, metamaterials and normal matter could exhibit signs of negative effective mass in much the same way as low temperature alloys melt at below the melting point of their components or some semiconductors have negative differential resistance.\nIn 1928, Paul Dirac's theory of elementary particles, now part of the Standard Model, already included negative solutions. The Standard Model is a generalization of quantum electrodynamics (QED) and negative mass is already built into the theory.\n\nMorris, Thorne and Yurtsever pointed out that the quantum mechanics of the Casimir effect can be used to produce a locally mass-negative region of space–time. In this article, and subsequent work by others, they showed that negative matter could be used to stabilize a wormhole. Cramer \"et al.\" argue that such wormholes might have been created in the early universe, stabilized by negative-mass loops of cosmic string. Stephen Hawking has proved that negative energy is a necessary condition for the creation of a closed timelike curve by manipulation of gravitational fields within a finite region of space; this proves, for example, that a finite Tipler cylinder cannot be used as a time machine.\n\nFor energy eigenstates of the Schrödinger equation, the wavefunction is wavelike wherever the particle's energy is greater than the local potential, and exponential-like (evanescent) wherever it is less. Naively, this would imply kinetic energy is negative in evanescent regions (to cancel the local potential). However, kinetic energy is an operator in quantum mechanics, and its expectation value is always positive, summing with the expectation value of the potential energy to yield the energy eigenvalue.\n\nFor wavefunctions of particles with zero rest mass (such as photons), this means that any evanescent portions of the wavefunction would be associated with a local negative mass–energy. However, the Schrödinger equation does not apply to massless particles; instead the Klein–Gordon equation is required.\n\nOne can achieve a negative mass independent of negative energy. According to mass–energy equivalence, mass is in proportion to energy and the coefficient of proportionality is . Actually, is still equivalent to although the coefficient is another constant such as . In this case, it is unnecessary to introduce a negative energy because the mass can be negative although the energy is positive. That is to say,\n\nUnder the circumstances,\n\nand so,\n\nWhen ,\n\nConsequently,\n\nwhere is invariant mass and invariant energy equals . The squared mass is still positive and the particle can be stable.\n\nFrom the above relation,\n\nThe negative momentum is applied to explain negative refraction, the inverse Doppler effect and the reverse Cherenkov effect observed in a negative index metamaterial. The radiation pressure in the metamaterial is also negative because the force is defined as . Negative pressure exists in dark energy too. Using these above equations, the energy–momentum relation should be\n\nSubstituting the Planck–Einstein relation and de Broglie's , we obtain the following dispersion relation \n\nwhen the wave consists of a stream of particles whose energy–momentum relation is formula_11 (wave–particle duality) and can be excited in a negative index metamaterial. The velocity of such a particle is equal to\n\nand range is from zero to infinity\n\nMoreover, the kinetic energy is also negative\n\nIn fact, negative kinetic energy exists in some models to describe dark energy (phantom energy) whose pressure is negative. In this way, the negative mass of exotic matter is now associated with negative momentum, negative pressure, negative kinetic energy and faster-than-light phenomena.\n\n"}
{"id": "3025363", "url": "https://en.wikipedia.org/wiki?curid=3025363", "title": "Negative relationship", "text": "Negative relationship\n\nIn statistics, there is a negative relationship or inverse relationship between two variables if higher values of one variable tend to be associated with lower values of the other. A negative relationship between two variables usually implies that the correlation between them is negative, or — what is in some contexts equivalent — that the slope in a corresponding graph is negative. A negative correlation between variables is also called anticorrelation or inverse correlation.\n\nNegative correlation can be seen geometrically when two normalized random vectors are viewed as points on a sphere, and the correlation between them is the cosine of the arc of separation of the points on the sphere. When this arc is more than a quarter-circle (θ > π/2), then the cosine is negative. Diametrically opposed points represent a correlation of –1 = cos(π). Any two points not in the same hemisphere have negative correlation.\n\nAn example would be a negative cross-sectional relationship between illness and vaccination, if it is observed that where the incidence of one is higher than average, the incidence of the other tends to be lower than average. Similarly, there would be a negative temporal relationship between illness and vaccination if it is observed in one location that times with a higher-than-average incidence of one tend to coincide with a lower-than-average incidence of the other.\n\nA particular inverse relationship is called inverse proportionality, and is given by formula_1 where \"k\" > 0 is a constant. In a Cartesian plane this relationship is displayed as a hyperbola with \"y\" decreasing as \"x\" increases.\n\nIn finance, an inverse correlation between the returns on two different assets enhances the risk-reduction effect of diversifying by holding them both in the same portfolio.\n\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "1969927", "url": "https://en.wikipedia.org/wiki?curid=1969927", "title": "Negative space", "text": "Negative space\n\nNegative space, in art, is the space around and between the subject(s) of an image. Negative space may be most evident when the space around a subject, not the subject itself, forms an interesting or artistically relevant shape, and such space occasionally is used to artistic effect as the \"real\" subject of an image.\n\nThe use of negative space is a key element of artistic composition. The Japanese word \"ma\" is sometimes used for this concept, for example in garden design.\n\nIn a two-tone, black-and-white image, a subject is normally depicted in black and the space around it is left blank (white), thereby forming a silhouette of the subject. Reversing the tones so that the space around the subject is printed black and the subject itself is left blank, however, causes the negative space to be apparent as it forms shapes around the subject. This is called figure-ground reversal.\n\nIn graphic design of printed or displayed materials, where effective communication is the objective, the use of negative space may be crucial. Not only within the typography, but in its placement in relation to the whole. It is the basis of why upper and lower case typography always is more legible than the use of all capital letters. Negative space varies around lower case letters, allowing the human eye to distinguish each word rapidly as one distinctive item, rather than having to parse out what the words are in a string of letters that all present the same overall profile as in all caps. The same judicious use of negative space drives the effectiveness of the entire design. Because of the long history of the use of black ink on white paper, \"white space\" is the term often used in graphics to identify the same separation.\n\nElements of an image that distract from the intended subject, or in the case of photography, objects in the same focal plane, are not considered negative space. Negative space may be used to depict a subject in a chosen medium by showing everything around the subject, but not the subject itself. Use of negative space will produce a silhouette of the subject. Most often, negative space is used as a neutral or contrasting background to draw attention to the main subject, which then is referred to as the positive space.\n\nConsidering and improving the balance between negative space and positive space in a composition is considered by many to enhance the design. This basic, but often overlooked, principle of design gives the eye a \"place to rest,\" increasing the appeal of a composition through subtle means.\n\nThe use of negative space in art may be analogous to silence in music, but only when it is juxtaposed with adjacent musical ideas. As such, there is a difference between inert and active silences in music, where the latter is more closely analogous to negative space in art.\n\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "39447080", "url": "https://en.wikipedia.org/wiki?curid=39447080", "title": "Non-extensive self-consistent thermodynamical theory", "text": "Non-extensive self-consistent thermodynamical theory\n\nIn experimental physics, researchers have proposed Non-extensive self-consistent thermodynamic theory to describe phenomena observed in the Large Hadron Collider (LHC). This theory investigates a fireball for high-energy particle collisions, while using Tsallis non-extensive thermodynamics. Fireballs lead to the bootstrap idea, or self-consistency principle, just as in the Boltzmann statistics used by Rolf Hagedorn. Assuming the distribution function gets variations, due to possible symmetrical change, Abdel Nasser Tawfik applied the non-extensive concepts of high-energy particle production.\n\nThe motivation to use the non-extensive statistics from Tsallis comes from the results obtained by Bediaga et al. They showed that with the substitution of the Boltzmann factor in Hagedorn's theory by the q-exponential function, it was possible to recover good agreement between calculation and experiment, even at energies as high as those achieved at the LHC, with q>1.\n\nThe starting point of the theory is entropy for a non-extensive quantum gas of bosons and fermions, as proposed by Conroy, Miller and Plastino, which is given by formula_1 where formula_2 is the non-extended version of the Fermi–Dirac entropy and formula_3 is the non-extended version of the Bose–Einstein entropy.\n\nThat group and also Clemens and Worku, the entropy just defined leads to occupation number formulas that reduce to Bediaga's. C. Beck, shows the power-like tails present in the distributions found in high energy physics experiments.\n\nUsing the entropy defined above, the partition function results are\nSince experiments have shown that formula_5, this restriction is adopted.\n\nAnother way to write the non-extensive partition function for a fireball is\nwhere formula_7 is the density of states of the fireballs.\n\nSelf-consistency implies that both forms of partition functions must be asymptotically equivalent and that the mass spectrum and the density of states must be related to each other by\nin the limit of formula_9 sufficiently large.\n\nThe self-consistency can be asymptotically achieved by choosing\nand\nwhere formula_12 is a constant and formula_13. Here, formula_14 are arbitrary constants. For formula_15 the two expressions above approach the corresponding expressions in Hagedorn's theory.\n\nWith the mass spectrum and density of states given above, the asymptotic form of the partition function is\nwhere\nwith\n\nOne immediate consequence of the expression for the partition function is the existence of a limiting temperature formula_19. This result is equivalent to Hagedorn's result. With these results, it is expected that at sufficiently high energy, the fireball presents a constant temperature and constant entropic factor.\n\nExperimental evidence of the existence of a limiting temperature and of a limiting entropic index can be found in J. Cleymans and collaborators, and I. Sena and A. Deppman.\n"}
{"id": "36797", "url": "https://en.wikipedia.org/wiki?curid=36797", "title": "Occam's razor", "text": "Occam's razor\n\nOccam's razor (also Ockham's razor or Ocham's razor; Latin: \"lex parsimoniae\" \"law of parsimony\") is the problem-solving principle that the simplest solution tends to be the correct one. When presented with competing hypotheses to solve a problem, one should select the solution with the fewest assumptions. The idea is attributed to English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.\n\nIn science, Occam's razor is used as an abductive heuristic in the development of theoretical models, rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with \"ad hoc\" hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.\n\nThe term \"Occam's razor\" did not appear until a few centuries after William of Ockham's death in 1347. Libert Froidmont, in his \"On Christian Philosophy of the Soul\", takes credit for the phrase, speaking of \"novacula occami\". Ockham did not invent this principle, but the \"razor\"—and its association with him—may be due to the frequency and effectiveness with which he used it. Ockham stated the principle in various ways, but the most popular version, \"Entities are not to be multiplied without necessity\" () was formulated by the Irish Franciscan philosopher John Punch in his 1639 commentary on the works of Duns Scotus.\n\nThe origins of what has come to be known as Occam's razor are traceable to the works of earlier philosophers such as John Duns Scotus (1265–1308), Robert Grosseteste (1175–1253), Maimonides (Moses ben-Maimon, 1138–1204), and even Aristotle (384–322 BC). Aristotle writes in his \"Posterior Analytics\", \"We may assume the superiority [other things being equal] of the demonstration which derives from fewer postulates or hypotheses.\" Ptolemy () stated, \"We consider it a good principle to explain the phenomena by the simplest hypothesis possible.\"\n\nPhrases such as \"It is vain to do with more what can be done with fewer\" and \"A plurality is not to be posited without necessity\" were commonplace in 13th-century scholastic writing. Robert Grosseteste, in \"Commentary on\" [Aristotle's] \"the Posterior Analytics Books\" (\"Commentarius in Posteriorum Analyticorum Libros\") (c. 1217–1220), declares: \"That is better and more valuable which requires fewer, other circumstances being equal... For if one thing were demonstrated from many and another thing from fewer equally known premises, clearly that is better which is from fewer because it makes us know quickly, just as a universal demonstration is better than particular because it produces knowledge from fewer premises. Similarly in natural science, in moral science, and in metaphysics the best is that which needs no premises and the better that which needs the fewer, other circumstances being equal.\"\n\nThe \"Summa Theologica\" of Thomas Aquinas (1225–1274) states that \"it is superfluous to suppose that what can be accounted for by a few principles has been produced by many.\" Aquinas uses this principle to construct an objection to God's existence, an objection that he in turn answers and refutes generally (cf. \"quinque viae\"), and specifically, through an argument based on causality. Hence, Aquinas acknowledges the principle that today is known as Occam's razor, but prefers causal explanations to other simple explanations (cf. also Correlation does not imply causation).\n\nWilliam of Ockham (\"circa\" 1287–1347) was an English Franciscan friar and theologian, an influential medieval philosopher and a nominalist. His popular fame as a great logician rests chiefly on the maxim attributed to him and known as Occam's razor. The term \"razor\" refers to distinguishing between two hypotheses either by \"shaving away\" unnecessary assumptions or cutting apart two similar conclusions.\n\nWhile it has been claimed that Occam's razor is not found in any of William's writings, one can cite statements such as (\"Plurality must never be posited without necessity\"), which occurs in his theological work on the \"Sentences of Peter Lombard\" (\"Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi\"; ed. Lugd., 1495, i, dist. 27, qu. 2, K).\n\nNevertheless, the precise words sometimes attributed to William of Ockham, (Entities must not be multiplied beyond necessity), are absent in his extant works; this particular phrasing comes from John Punch, who described the principle as a \"common axiom\" (\"axioma vulgare\") of the Scholastics. William of Ockham's contribution seems to restrict the operation of this principle in matters pertaining to miracles and God's power; so, in the Eucharist, a plurality of miracles is possible, simply because it pleases God.\n\nThis principle is sometimes phrased as (\"Plurality should not be posited without necessity\"). In his \"Summa Totius Logicae\", i. 12, William of Ockham cites the principle of economy, (\"It is futile to do with more things that which can be done with fewer\"; Thorburn, 1918, pp. 352–53; Kneale and Kneale, 1962, p. 243.)\n\nTo quote Isaac Newton, \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances. Therefore, to the same natural effects we must, as far as possible, assign the same causes.\"\n\nBertrand Russell offers a particular version of Occam's razor: \"Whenever possible, substitute constructions out of known entities for inferences to unknown entities.\"\n\nAround 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. This theory is a mathematical formalization of Occam's razor.\n\nAnother technical approach to Occam's razor is ontological parsimony. Parsimony means spareness and is also referred to as the Rule of Simplicity. This is considered a strong version of Occam's razor. A variation used in medicine is called the \"Zebra\": a doctor should reject an exotic medical diagnosis when a more commonplace explanation is more likely, derived from Theodore Woodward's dictum \"When you hear hoofbeats, think of horses not zebras\".\n\nErnst Mach formulated the stronger version of Occam's razor into physics, which he called the Principle of Economy stating: \"Scientists must use the simplest means of arriving at their results and exclude everything not perceived by the senses.\"\n\nThis principle goes back at least as far as Aristotle, who wrote \"Nature operates in the shortest way possible.\" The idea of parsimony or simplicity in deciding between theories, though not the intent of the original expression of Occam's razor, has been assimilated into our culture as the widespread layman's formulation that \"the simplest explanation is usually the correct one.\"\n\nPrior to the 20th century, it was a commonly held belief that nature itself was simple and that simpler hypotheses about nature were thus more likely to be true. This notion was deeply rooted in the aesthetic value that simplicity holds for human thought and the justifications presented for it often drew from theology. Thomas Aquinas made this argument in the 13th century, writing, \"If a thing can be done adequately by means of one, it is superfluous to do it by means of several; for we observe that nature does not employ two instruments [if] one suffices.\"\n\nBeginning in the 20th century, epistemological justifications based on induction, logic, pragmatism, and especially probability theory have become more popular among philosophers.\n\nOccam's razor has gained strong empirical support in helping to converge on better theories (see \"Applications\" section below for some examples).\n\nIn the related concept of overfitting, excessively complex models are affected by statistical noise (a problem also known as the bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, Bayesian inference, etc.).\n\nThe razor's statement that \"other things being equal, simpler explanations are generally better than more complex ones\" is amenable to empirical testing. Another interpretation of the razor's statement would be that \"simpler hypotheses are generally better than the complex ones\". The procedure to test the former interpretation would compare the track records of simple and comparatively complex explanations. If one accepts the first interpretation, the validity of Occam's razor as a tool would then have to be rejected if the more complex explanations were more often correct than the less complex ones (while the converse would lend support to its use). If the latter interpretation is accepted, the validity of Occam's razor as a tool could possibly be accepted if the simpler hypotheses led to correct conclusions more often than not.\n\nSome increases in complexity are sometimes necessary. So there remains a justified general bias toward the simpler of two competing explanations. To understand why, consider that for each accepted explanation of a phenomenon, there is always an infinite number of possible, more complex, and ultimately incorrect, alternatives. This is so because one can always burden a failing explanation with an ad hoc hypothesis. Ad hoc hypotheses are justifications that prevent theories from being falsified. Even other empirical criteria, such as consilience, can never truly eliminate such explanations as competition. Each true explanation, then, may have had many alternatives that were simpler and false, but also an infinite number of alternatives that were more complex and false. But if an alternative ad hoc hypothesis were indeed justifiable, its implicit conclusions would be empirically verifiable. On a commonly accepted repeatability principle, these alternative theories have never been observed and continue to escape observation. In addition, one does not say an explanation is true if it has not withstood this principle.\n\nPut another way, any new, and even more complex, theory can still possibly be true. For example, if an individual makes supernatural claims that leprechauns were responsible for breaking a vase, the simpler explanation would be that he is mistaken, but ongoing ad hoc justifications (e.g. \"... and that's not me on the film; they tampered with that, too\") successfully prevent outright falsification. This endless supply of elaborate competing explanations, called saving hypotheses, cannot be ruled out—except by using Occam's razor. A study of the predictive validity of Occam's razor found 32 published papers that included 97 comparisons of economic forecasts from simple and complex forecasting methods. None of the papers provided a balance of evidence that complexity of method improved forecast accuracy. In the 25 papers with quantitative comparisons, complexity increased forecast errors by an average of 27 percent.\n\nOne justification of Occam's razor is a direct result of basic probability theory. By definition, all assumptions introduce possibilities for error; if an assumption does not improve the accuracy of a theory, its only effect is to increase the probability that the overall theory is wrong.\n\nThere have also been other attempts to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book \"Information Theory, Inference, and Learning Algorithms\", where he emphasizes that a prior bias in favour of simpler models is not required.\n\nWilliam H. Jefferys and James O. Berger (1991) generalize and quantify the original formulation's \"assumptions\" concept as the degree to which a proposition is unnecessarily accommodating to possible observable data. They state, \"A hypothesis with fewer adjustable parameters will automatically have an enhanced posterior probability, due to the fact that the predictions it makes are sharp.\" The model they propose balances the precision of a theory's predictions against their sharpness—preferring theories that sharply make correct predictions over theories that accommodate a wide range of other possible results. This, again, reflects the mathematical relationship between key concepts in Bayesian inference (namely marginal probability, conditional probability, and posterior probability).\n\nThe bias–variance tradeoff is a framework that incorporates the Occam's razor principal in its balance between overfitting (i.e. variance minimization) and underfitting (i.e. bias minimization).\n\nKarl Popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations. Our preference for simplicity may be justified by its falsifiability criterion: we prefer simpler theories to more complex ones \"because their empirical content is greater; and because they are better testable\" (Popper 1992). The idea here is that a simple theory applies to more cases than a more complex one, and is thus more easily falsifiable. This is again comparing a simple theory to a more complex theory where both explain the data equally well.\n\nThe philosopher of science Elliott Sober once argued along the same lines as Popper, tying simplicity with \"informativeness\": The simplest theory is the more informative, in the sense that it requires less information to a question. He has since rejected this account of simplicity, purportedly because it fails to provide an epistemic justification for simplicity. He now believes that simplicity considerations (and considerations of parsimony in particular) do not count unless they reflect something more fundamental. Philosophers, he suggests, may have made the error of hypostatizing simplicity (i.e., endowed it with a \"sui generis\" existence), when it has meaning only when embedded in a specific context (Sober 1992). If we fail to justify simplicity considerations on the basis of the context in which we use them, we may have no non-circular justification: \"Just as the question 'why be rational?' may have no non-circular answer, the same may be true of the question 'why should simplicity be considered in evaluating the plausibility of hypotheses?'\"\n\nRichard Swinburne argues for simplicity on logical grounds:\n\nAccording to Swinburne, since our choice of theory cannot be determined by data (see Underdetermination and Duhem-Quine thesis), we must rely on some criterion to determine which theory to use. Since it is absurd to have no logical method for settling on one hypothesis amongst an infinite number of equally data-compliant hypotheses, we should choose the simplest theory: \"Either science is irrational [in the way it judges theories and predictions probable] or the principle of simplicity is a fundamental synthetic a priori truth.\" (Swinburne 1997).\n\nFrom the \"Tractatus Logico-Philosophicus\":\n\n\nand on the related concept of \"simplicity\":\n\n\nIn science, Occam's razor is used as a heuristic to guide scientists in developing theoretical models rather than as an arbiter between published models. In physics, parsimony was an important heuristic in Albert Einstein's formulation of special relativity, in the development and application of the principle of least action by Pierre Louis Maupertuis and Leonhard Euler, and in the development of quantum mechanics by Max Planck, Werner Heisenberg and Louis de Broglie.\n\nIn chemistry, Occam's razor is often an important heuristic when developing a model of a reaction mechanism. Although it is useful as a heuristic in developing models of reaction mechanisms, it has been shown to fail as a criterion for selecting among some selected published models. In this context, Einstein himself expressed caution when he formulated Einstein's Constraint: \"It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience\". An often-quoted version of this constraint (which cannot be verified as posited by Einstein himself) says \"Everything should be kept as simple as possible, but not simpler.\"\n\nIn the scientific method, parsimony is an epistemological, metaphysical or heuristic preference, not an irrefutable principle of logic or a scientific result. As a logical principle, Occam's razor would demand that scientists accept the simplest possible theoretical explanation for existing data. However, science has shown repeatedly that future data often support more complex theories than do existing data. Science prefers the simplest explanation that is consistent with the data available at a given time, but the simplest explanation may be ruled out as new data become available. That is, science is open to the possibility that future experiments might support more complex theories than demanded by current data and is more interested in designing experiments to discriminate between competing theories than favoring one theory over another based merely on philosophical principles.\n\nWhen scientists use the idea of parsimony, it has meaning only in a very specific context of inquiry. Several background assumptions are required for parsimony to connect with plausibility in a particular research problem. The reasonableness of parsimony in one research context may have nothing to do with its reasonableness in another. It is a mistake to think that there is a single global principle that spans diverse subject matter.\n\nIt has been suggested that Occam's razor is a widely accepted example of extraevidential consideration, even though it is entirely a metaphysical assumption. There is little empirical evidence that the world is actually simple or that simple accounts are more likely to be true than complex ones.\n\nMost of the time, Occam's razor is a conservative tool, cutting out \"crazy, complicated constructions\" and assuring \"that hypotheses are grounded in the science of the day\", thus yielding \"normal\" science: models of explanation and prediction. There are, however, notable exceptions where Occam's razor turns a conservative scientist into a reluctant revolutionary. For example, Max Planck interpolated between the Wien and Jeans radiation laws and used Occam's razor logic to formulate the quantum hypothesis, even resisting that hypothesis as it became more obvious that it was correct.\n\nAppeals to simplicity were used to argue against the phenomena of meteorites, ball lightning, continental drift, and reverse transcriptase. One can argue for atomic building blocks for matter, because it provides a simpler explanation for the observed reversibility of both mixing and chemical reactions as simple separation and rearrangements of atomic building blocks. At the time, however, the atomic theory was considered more complex because it implied the existence of invisible particles that had not been directly detected. Ernst Mach and the logical positivists rejected John Dalton's atomic theory until the reality of atoms was more evident in Brownian motion, as shown by Albert Einstein.\n\nIn the same way, postulating the aether is more complex than transmission of light through a vacuum. At the time, however, all known waves propagated through a physical medium, and it seemed simpler to postulate the existence of a medium than to theorize about wave propagation without a medium. Likewise, Newton's idea of light particles seemed simpler than Christiaan Huygens's idea of waves, so many favored it. In this case, as it turned out, neither the wave—nor the particle—explanation alone suffices, as light behaves like waves and like particles.\n\nThree axioms presupposed by the scientific method are realism (the existence of objective reality), the existence of natural laws, and the constancy of natural law. Rather than depend on provability of these axioms, science depends on the fact that they have not been objectively falsified. Occam's razor and parsimony support, but do not prove, these axioms of science. The general principle of science is that theories (or models) of natural law must be consistent with repeatable experimental observations. This ultimate arbiter (selection criterion) rests upon the axioms mentioned above.\n\nThere are examples where Occam's razor would have favored the wrong theory given the available data. Simplicity principles are useful philosophical preferences for choosing a more likely theory from among several possibilities that are all consistent with available data. A single instance of Occam's razor favoring a wrong theory falsifies the razor as a general principle. Michael Lee and others provide cases in which a parsimonious approach does not guarantee a correct conclusion and, if based on incorrect working hypotheses or interpretations of incomplete data, may even strongly support a false conclusion.\n\nIf multiple models of natural law make exactly the same testable predictions, they are equivalent and there is no need for parsimony to choose a preferred one. For example, Newtonian, Hamiltonian and Lagrangian classical mechanics are equivalent. Physicists have no interest in using Occam's razor to say the other two are wrong. Likewise, there is no demand for simplicity principles to arbitrate between wave and matrix formulations of quantum mechanics. Science often does not demand arbitration or selection criteria between models that make the same testable predictions.\n\nBiologists or philosophers of biology use Occam's razor in either of two contexts both in evolutionary biology: the units of selection controversy and systematics. George C. Williams in his book \"Adaptation and Natural Selection\" (1966) argues that the best way to explain altruism among animals is based on low-level (i.e., individual) selection as opposed to high-level group selection. Altruism is defined by some evolutionary biologists (e.g., R. Alexander, 1987; W. D. Hamilton, 1964) as behavior that is beneficial to others (or to the group) at a cost to the individual, and many posit individual selection as the mechanism that explains altruism solely in terms of the behaviors of individual organisms acting in their own self-interest (or in the interest of their genes, via kin selection). Williams was arguing against the perspective of others who propose selection at the level of the group as an evolutionary mechanism that selects for altruistic traits (e.g., D. S. Wilson & E. O. Wilson, 2007). The basis for Williams' contention is that of the two, individual selection is the more parsimonious theory. In doing so he is invoking a variant of Occam's razor known as Morgan's Canon: \"In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development.\" (Morgan 1903).\n\nHowever, more recent biological analyses, such as Richard Dawkins' \"The Selfish Gene\", have contended that Morgan's Canon is not the simplest and most basic explanation. Dawkins argues the way evolution works is that the genes propagated in most copies end up determining the development of that particular species, i.e., natural selection turns out to select specific genes, and this is really the fundamental underlying principle that automatically gives individual and group selection as emergent features of evolution.\n\nZoology provides an example. Muskoxen, when threatened by wolves, form a circle with the males on the outside and the females and young on the inside. This is an example of a behavior by the males that seems to be altruistic. The behavior is disadvantageous to them individually but beneficial to the group as a whole and was thus seen by some to support the group selection theory. Another interpretation is kin selection: if the males are protecting their offspring, they are protecting copies of their own alleles. Engaging in this behavior would be favored by individual selection if the cost to the male musk ox is less than half of the benefit received by his calf – which could easily be the case if wolves have an easier time killing calves than adult males. It could also be the case that male musk oxen would be individually less likely to be killed by wolves if they stood in a circle with their horns pointing out, regardless of whether they were protecting the females and offspring. That would be an example of regular natural selection – a phenomenon called \"the selfish herd\".\n\nSystematics is the branch of biology that attempts to establish patterns of genealogical relationship among biological taxa. It is also concerned with their classification. There are three primary camps in systematics: cladists, pheneticists, and evolutionary taxonomists. The cladists hold that genealogy alone should determine classification, pheneticists contend that overall similarity is the determining criterion, while evolutionary taxonomists say that both genealogy and similarity count in classification.\n\nIt is among the cladists that Occam's razor is to be found, although their term for it is \"cladistic parsimony\". Cladistic parsimony (or maximum parsimony) is a method of phylogenetic inference in the construction of types of phylogenetic trees (more specifically, cladograms). Cladograms are branching, tree-like structures used to represent hypotheses of relative degree of relationship, based on shared, derived character states. Cladistic parsimony is used to select as the preferred hypothesis of relationships the cladogram that requires the fewest implied character state transformations. Critics of the cladistic approach often observe that for some types of tree, parsimony consistently produces the wrong results, regardless of how much data is collected (this is called statistical inconsistency, or long branch attraction). However, this criticism is also potentially true for any type of phylogenetic inference, unless the model used to estimate the tree reflects the way that evolution actually happened. Because this information is not empirically accessible, the criticism of statistical inconsistency against parsimony holds no force. For a book-length treatment of cladistic parsimony, see Elliott Sober's \"Reconstructing the Past: Parsimony, Evolution, and Inference\" (1988). For a discussion of both uses of Occam's razor in biology, see Sober's article \"Let's Razor Ockham's Razor\" (1990).\n\nOther methods for inferring evolutionary relationships use parsimony in a more traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do simple hypotheses before researchers reject the simple hypotheses. Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.\n\nFrancis Crick has commented on potential limitations of Occam's razor in biology. He advances the argument that because biological systems are the products of (an ongoing) natural selection, the mechanisms are not necessarily optimal in an obvious sense. He cautions: \"While Ockham's razor is a useful tool in the physical sciences, it can be a very dangerous implement in biology. It is thus very rash to use simplicity and elegance as a guide in biological research.\"\n\nIn biogeography, parsimony is used to infer ancient migrations of species or populations by observing the geographic distribution and relationships of existing organisms. Given the phylogenetic tree, ancestral migrations are inferred to be those that require the minimum amount of total movement.\n\nIn the philosophy of religion, Occam's razor is sometimes applied to the existence of God. William of Ockham himself was a Christian. He believed in God, and in the authority of Scripture; he writes that \"nothing ought to be posited without a reason given, unless it is self-evident (literally, known through itself) or known by experience or proved by the authority of Sacred Scripture.\" Ockham believed that an explanation has no sufficient basis in reality when it does not harmonize with reason, experience, or the Bible. However, unlike many theologians of his time, Ockham did not believe God could be logically proven with arguments. To Ockham, science was a matter of discovery, but theology was a matter of revelation and faith. He states: \"only faith gives us access to theological truths. The ways of God are not open to reason, for God has freely chosen to create a world and establish a way of salvation within it apart from any necessary laws that human logic or rationality can uncover.\"\n\nSt. Thomas Aquinas, in the \"Summa Theologica\", uses a formulation of Occam's razor to construct an objection to the idea that God exists, which he refutes directly with a counterargument:\n\nFurther, it is superfluous to suppose that what can be accounted for by a few principles has been produced by many. But it seems that everything we see in the world can be accounted for by other principles, supposing God did not exist. For all natural things can be reduced to one principle which is nature; and all voluntary things can be reduced to one principle which is human reason, or will. Therefore there is no need to suppose God's existence.\n\nIn turn, Aquinas answers this with the \"quinque viae\", and addresses the particular objection above with the following answer:\n\nSince nature works for a determinate end under the direction of a higher agent, whatever is done by nature must needs be traced back to God, as to its first cause. So also whatever is done voluntarily must also be traced back to some higher cause other than human reason or will, since these can change or fail; for all things that are changeable and capable of defect must be traced back to an immovable and self-necessary first principle, as was shown in the body of the Article.\n\nRather than argue for the necessity of a god, some theists base their belief upon grounds independent of, or prior to, reason, making Occam's razor irrelevant. This was the stance of Søren Kierkegaard, who viewed belief in God as a leap of faith that sometimes directly opposed reason. This is also the doctrine of Gordon Clark's presuppositional apologetics, with the exception that Clark never thought the leap of faith was contrary to reason (see also Fideism).\n\nVarious arguments in favor of God establish God as a useful or even necessary assumption. Contrastingly some anti-theists hold firmly to the belief that assuming the existence of God introduces unnecessary complexity (Schmitt 2005, e.g., the Ultimate Boeing 747 gambit).\n\nAnother application of the principle is to be found in the work of George Berkeley (1685–1753). Berkeley was an idealist who believed that all of reality could be explained in terms of the mind alone. He invoked Occam's razor against materialism, stating that matter was not required by his metaphysic and was thus eliminable. One potential problem with this belief is that it's possible, given Berkeley's position, to find solipsism itself more in line with the razor than a God-mediated world beyond a single thinker.\n\nOccam's razor may also be recognized in the apocryphal story about an exchange between Pierre-Simon Laplace and Napoleon. It is said that in praising Laplace for one of his recent publications, the emperor asked how it was that the name of God, which featured so frequently in the writings of Lagrange, appeared nowhere in Laplace's. At that, he is said to have replied, \"It's because I had no need of that hypothesis.\" Though some point to this story as illustrating Laplace's atheism, more careful consideration suggests that he may instead have intended merely to illustrate the power of methodological naturalism, or even simply that the fewer logical premises one assumes, the stronger is one's conclusion.\n\nIn his article \"Sensations and Brain Processes\" (1959), J. J. C. Smart invoked Occam's razor with the aim to justify his preference of the mind-brain identity theory over spirit-body dualism. Dualists state that there are two kinds of substances in the universe: physical (including the body) and spiritual, which is non-physical. In contrast, identity theorists state that everything is physical, including consciousness, and that there is nothing nonphysical. Though it is impossible to appreciate the spiritual when limiting oneself to the physical, Smart maintained that identity theory explains all phenomena by assuming only a physical reality. Subsequently, Smart has been severely criticized for his use (or misuse) of Occam's razor and ultimately retracted his advocacy of it in this context. Paul Churchland (1984) states that by itself Occam's razor is inconclusive regarding duality. In a similar way, Dale Jacquette (1994) stated that Occam's razor has been used in attempts to justify eliminativism and reductionism in the philosophy of mind. Eliminativism is the thesis that the ontology of folk psychology including such entities as \"pain\", \"joy\", \"desire\", \"fear\", etc., are eliminable in favor of an ontology of a completed neuroscience.\n\nIn penal theory and the philosophy of punishment, parsimony refers specifically to taking care in the distribution of punishment in order to avoid excessive punishment. In the utilitarian approach to the philosophy of punishment, Jeremy Bentham's \"parsimony principle\" states that any punishment greater than is required to achieve its end is unjust. The concept is related but not identical to the legal concept of proportionality. Parsimony is a key consideration of the modern restorative justice, and is a component of utilitarian approaches to punishment, as well as the prison abolition movement. Bentham believed that true parsimony would require punishment to be individualised to take account of the sensibility of the individual—an individual more sensitive to punishment should be given a proportionately lesser one, since otherwise needless pain would be inflicted. Later utilitarian writers have tended to abandon this idea, in large part due to the impracticality of determining each alleged criminal's relative sensitivity to specific punishments.\n\nMarcus Hutter's universal artificial intelligence builds upon Solomonoff's mathematical formalization of the razor to calculate the expected value of an action.\n\nThere are various papers in scholarly journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, and using it to come up with criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and Kolmogorov complexity.\n\nOne of the problems with the original formulation of the razor is that it only applies to models with the same explanatory power (i.e., it only tells us to prefer the simplest of equally good models). A more general form of the razor can be derived from Bayesian model comparison, which is based on Bayes factors and can be used to compare models that don't fit the observations equally well. These methods can sometimes optimally balance the complexity and power of a model. Generally, the exact Occam factor is intractable, but approximations such as Akaike information criterion, Bayesian information criterion, Variational Bayesian methods, false discovery rate, and Laplace's method are used. Many artificial intelligence researchers are now employing such techniques, for instance through work on Occam Learning or more generally on the Free energy principle.\n\nStatistical versions of Occam's razor have a more rigorous formulation than what philosophical discussions produce. In particular, they must have a specific definition of the term \"simplicity\", and that definition can vary. For example, in the Kolmogorov–Chaitin minimum description length approach, the subject must pick a Turing machine whose operations describe the basic operations \"believed\" to represent \"simplicity\" by the subject. However, one could always choose a Turing machine with a simple operation that happened to construct one's entire theory and would hence score highly under the razor. This has led to two opposing camps: one that believes Occam's razor is objective, and one that believes it is subjective.\n\nThe minimum instruction set of a universal Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a \"natural\" Turing machine of small size as the proper basis for excluding arbitrarily complex instruction sets in the formulation of razors. Describing the program for the universal program as the \"hypothesis\", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that \"the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized.\" Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the minimum message length (MML) principle.\n\nOne possible conclusion from mixing the concepts of Kolmogorov complexity and Occam's razor is that an ideal data compressor would also be a scientific explanation/formulation generator. Some attempts have been made to re-derive known laws from considerations of simplicity or compressibility.\n\nAccording to Jürgen Schmidhuber, the appropriate mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's \"Foreword re C. S. Wallace\" for the subtle distinctions between the algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's \"MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness\" both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of decision tree induction, see Dowe and Needham's \"Message Length as an Effective Ockham's Razor in Decision Tree Induction\".\n\nOccam's razor is not an embargo against the positing of any kind of entity, or a recommendation of the simplest theory come what may. Occam's razor is used to adjudicate between theories that have already passed \"theoretical scrutiny\" tests and are equally well-supported by evidence. Furthermore, it may be used to prioritize empirical testing between two equally plausible but unequally testable hypotheses; thereby minimizing costs and wastes while increasing chances of falsification of the simpler-to-test hypothesis.\n\nAnother contentious aspect of the razor is that a theory can become more complex in terms of its structure (or syntax), while its ontology (or semantics) becomes simpler, or vice versa. Quine, in a discussion on definition, referred to these two perspectives as \"economy of practical expression\" and \"economy in grammar and vocabulary\", respectively.\n\nGalileo Galilei lampooned the \"misuse\" of Occam's razor in his \"Dialogue\". The principle is represented in the dialogue by Simplicio. The telling point that Galileo presented ironically was that if one really wanted to start from a small number of entities, one could always consider the letters of the alphabet as the fundamental entities, since one could construct the whole of human knowledge out of them.\n\nOccam's razor has met some opposition from people who have considered it too extreme or rash. Walter Chatton (c. 1290–1343) was a contemporary of William of Ockham (c. 1287–1347) who took exception to Occam's razor and Ockham's use of it. In response he devised his own \"anti-razor:\" \"If three things are not enough to verify an affirmative proposition about things, a fourth must be added, and so on.\" Although there have been a number of philosophers who have formulated similar anti-razors since Chatton's time, no one anti-razor has perpetuated in as much notability as Chatton's anti-razor, although this could be the case of the Late Renaissance Italian motto of unknown attribution \"Se non è vero, è ben trovato\" (\"Even if it is not true, it is well conceived\") when referred to a particularly artful explanation.\n\nAnti-razors have also been created by Gottfried Wilhelm Leibniz (1646–1716), Immanuel Kant (1724–1804), and Karl Menger (1902–1985). Leibniz's version took the form of a principle of plenitude, as Arthur Lovejoy has called it: the idea being that God created the most varied and populous of possible worlds. Kant felt a need to moderate the effects of Occam's razor and thus created his own counter-razor: \"The variety of beings should not rashly be diminished.\"\n\nKarl Menger found mathematicians to be too parsimonious with regard to variables, so he formulated his Law Against Miserliness, which took one of two forms: \"Entities must not be reduced to the point of inadequacy\" and \"It is vain to do with fewer what requires more.\" A less serious but (some might say) even more extremist anti-razor is 'Pataphysics, the \"science of imaginary solutions\" developed by Alfred Jarry (1873–1907). Perhaps the ultimate in anti-reductionism, \"'Pataphysics seeks no less than to view each event in the universe as completely unique, subject to no laws but its own.\" Variations on this theme were subsequently explored by the Argentine writer Jorge Luis Borges in his story/mock-essay \"Tlön, Uqbar, Orbis Tertius\". There is also Crabtree's Bludgeon, which cynically states that \"[n]o set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated.\"\n\n\n"}
{"id": "3792982", "url": "https://en.wikipedia.org/wiki?curid=3792982", "title": "Porphyrian tree", "text": "Porphyrian tree\n\nThe Tree of Porphyry is a classic device for illustrating what is also called a \"scale of being\". It was suggested—if not first, then most famously in the European philosophical tradition—by the 3rd century CE Greek neoplatonist philosopher and logician Porphyry. It is also known as \"scala praedicamentalis\".\n\nPorphyry suggests the Porphyrian tree in his introduction (in Greek, \"Isagoge\") to Aristotle's Categories. Porphyry presented Aristotle's classification of categories in a way that was later adopted into tree-like diagrams of dichotomous divisions, which indicate that a species is defined by a genus and a differentia and that this logical process continues until the lowest species is reached, which can no longer be so defined. No illustrations or diagrams occur in editions of Porphyry's original work. But, diagrams were eventually made, and became associated with the scheme that Porphyry describes, following Aristotle.\n\nPorphyry's \"Isagoge\" was originally written in Greek, but was translated into Latin in the early 6th century CE by Boethius. Boethius's translation became the standard philosophical logic textbook in the Middle Ages. Until the late 19th century, theories of categories based on Porphyry's work were still being taught to students of logic.\n\nThe following very helpful passage by philosopher James Franklin gives some hint as to the history of the Porphyrian tree:\n\nThus, the notion of the Porphyrian tree as an actual diagram comes later than Porphyry himself. Still, scholars do speak of Porphyry's tree as in the \"Isagoge\" and they mean by this only that the idea of dividing genera into species via differentiae is found in the \"Isagoge\". But, of course, Porphyry was only following what was already in Aristotle, and Aristotle was following what was already in his teacher, Plato.\n\nThe following Porphyrian tree consists of three columns of words; the middlemost (in boldface) contains the series of genera and species, and we can take as analogous to the trunk of a tree. The extremes (the terms that jut out to the left and right), containing the differentiae, we can take as analogous to the branches of a tree:\n\nThe diagram shows the highest genus to be substance. (Whether substance is a highest genus, really, is not in question here: right now we are only going to discuss what the diagram shows, not whether what it shows is true or false.) The technical term for a highest substance is \"\"summum genus\"\". So, substance is the \"summum genus\" as far as this diagram goes. The diagram shows that the genus substance to have two differentia, namely, \"thinking\" and \"extended\". This indicates that there are two species of the genus substance, thinking substance and extended substance. The diagram does not give a term for the species of thinking substance (this would be \"mind\"), but it does give the term for the species of extended substance, namely, body. That is, body is a species of the genus substance; body is that species of the genus substance that is extended.\n\nNow that we have seen body as a species of substance, we treat body as a genus itself. As a genus, it has two differentia of its own, inanimate and animate. So, there are two species of body, inanimate body and animate body. The diagram does not tell us what the term for inanimate body is, but it indicates a term for animate body, namely, animal. Animal is an animate species of the genus body.\n\nAnd, again, now that we have looked at animal as a species of the genus body, we look at animal now as a genus and consider its differentia, which are shown on the diagram to be irrational and rational. Thus, according to the diagram there are two species of the genus animal, irrational animal and rational animal. We are not told by the diagram what a term for irrational animal is, but the diagram indicates that a rational animal is a human. Thus, human is a rational species of the genus animal.\n\nBeneath human, however, there are no further species. \"This\" and \"that\" if they are considered differentiae, are of a special kind that map the species human not onto a new species but onto particular humans., The particular human Plato is named in the diagram. Plato is not a species (that is why his name is not in bold, unlike the species above). So, human is the lowest species in this diagram. The technical name for the lowest species in such a scheme is the \"\"infima species\"\". So, for this diagram, human is the \"infima species\".\n\n\n\n\n"}
{"id": "51837768", "url": "https://en.wikipedia.org/wiki?curid=51837768", "title": "Positive and Negative Affect Schedule", "text": "Positive and Negative Affect Schedule\n\nThe Positive and Negative Affect Schedule (PANAS) is a self-report questionnaire that consists of two 10-item scales to measure both positive and negative affect. Each item is rated on a 5-point scale of 1 \"(not at all)\" to 5 \"(very much)\". The measure has been used mainly as a research tool in group studies, but can be utilized within clinical and non-clinical populations as well. Shortened, elongated, and children's versions of the PANAS have been developed, taking approximately 5–10 minutes to complete. Clinical and non-clinical studies have found the PANAS to be a reliable and valid instrument in the assessment of positive and negative affect.\n\nThe PANAS was developed in 1988 by researchers from the University of Minnesota and Southern Methodist University. Previous mood measures have shown correlations of variable strength between positive and negative affect, and these same measures have questionable reliability and validity. Watson, Clark, and Tellegen developed the PANAS in an attempt to provide a better, purer measure of each of these dimensions.\n\nThe researchers extracted 60 terms from the factor analyses of Michael Zevon and Tellegen shown to be relatively accurate markers of either positive or negative affect, but not both. They chose terms that met a strong correlation to one corresponding dimension but exhibited a weak correlation to the other. Through multiple rounds of elimination and preliminary analyses with a test population, the researchers arrived at 10 terms for each of the two scales, as follows:\n\nThe PANAS for Children (PANAS-C) was developed in an attempt to differentiate the affective expressions of anxiety and depression in children. The tripartite model on which this measure is based suggests that high levels of negative affect is present in those with anxiety and depression, but high levels of positive affect is not shared between the two. Previous mood scales for children have been shown to reliably capture the former relationship but not the latter; the PANAS-C was created as a tool with better discriminant validity for child assessment. Similar to the development of the original PANAS, the PANAS-C drew from terms of the PANAS-X and eliminated several terms with insufficient correlations between the term and the affective construct after preliminary analyses with a non-clinical sample of children. The final version of the measure consists of 27 items: 12 positive affect terms and 15 negative affect terms. Despite the purpose of its development, however, the measure’s discriminant validity is still wanting.\n\nThe PANAS-SF, comprises 10 items that were determined through the highest factor loadings on the exploratory factor analysis reported by Watson et al. (1988) in his original PANAS. Previous mood scales, such that of Bradburn, had low reliabilities and high correlations between subscales. Watson was able to address these concerns in his study of the original PANAS; however, his participants consisted mostly of student populations. The purpose of the PANAS-SF was not only to provide a shorter and more concise form of the PANAS, but to be able to apply the schedules to older clinical populations. Overall, it was reported that this modified model was consistent with Watson’s.\n\nSeparate from the PANAS-SF, Edmund Thompson created the international PANAS short form (I-PANAS-SF) in order to make a 10 item mood scale that can be implemented effectively on an international level, provide more clarity on the content of the items, reduce ambiguities, address the limitations of the original and the previous short form of the PANAS, and also to provide a shorter, yet dependable and valid scale. To determine the 10 items of the 20 original items, two focus groups were utilized to evaluate all of the original 20 PANAS items. They found that while some items were easily understood by the participant, certains items had different meanings or were too ambiguous. Items that had too much ambiguity were eliminated from the modified form. Researchers found that the I-PANAS-SF had high correlations with the original PANAS. Through multiple tests and studies, they were able to determine that the I-PANAS-SF was on par with the original scale and can be used as a reliable, valid, brief, and efficient instrument on an international scale.\n\nIn 1994, Watson and Clark developed an expanded form of the PANAS, called the PANAS-X, that consists of 60 items that can be completed in 10 minutes or less. The PANAS-X incorporates the original, higher order dimensions specified in the PANAS in addition to the measures of 11 lower order emotional states. These measures are broken down into three main categories: basic negative emotion scales consisting of fear, hostility, guilt, and sadness; basic positive emotion scales consisting of joviality, self-assurance, and attentiveness; and other affective states consisting of shyness, fatigue, serenity, and surprise. Through extensive analyses, all eleven affective states, with the exception of surprise, were shown to be stable, valid measures that assess how an individual’s emotional states fluctuate over time.\n\nReliability refers to whether the scores are reproducible. Unless otherwise specified, the reliability scores and values come from studies done with a United States population sample.\n\nMany forms of the PANAS (PANAS-C, PANAS-X, I-PANAS-SF, and among others) have shown that the PANAS has been widely employed. Recent studies have also shown that the PANAS can be administered in a large general adult population, as well as other populations. However, to date, the PANAS is mostly used as a research tool in group studies, but it has the potential to be utilized in clinical work with individuals. Furthermore, the PANAS has the potential to be used to evaluate mental illnesses, as shown in an experiment conducted by Dyck, Jolly, and Kramer, which demonstrated its effectiveness in distinguishing between depression and anxiety in clinical samples.  \n\nSince the PANAS is a self-report questionnaire, it can be difficult to assess people’s mood accurately, as people can overstate or understate their experience of their moods. In addition, the original PANAS had a limited sample size of college students, which concerns with wide applicability to other samples. Furthermore, some studies claim that the PANAS is too long or that its items are redundant. The PANAS does not encompass higher order mood states.\n\n"}
{"id": "7295638", "url": "https://en.wikipedia.org/wiki?curid=7295638", "title": "Primary/secondary quality distinction", "text": "Primary/secondary quality distinction\n\nThe primary/secondary quality distinction is a conceptual distinction in epistemology and metaphysics, concerning the nature of reality. It is most explicitly articulated by John Locke in his \"Essay concerning Human Understanding\", but earlier thinkers such as Galileo and Descartes made similar distinctions.\n\nPrimary qualities are thought to be properties of objects that are independent of any observer, such as solidity, extension, motion, number and figure. These characteristics convey facts. They exist in the thing itself, can be determined with certainty, and do not rely on subjective judgments. For example, if an object is spherical, no one can reasonably argue that it is triangular.\n\nSecondary qualities are thought to be properties that produce sensations in observers, such as color, taste, smell, and sound. They can be described as the effect things have on certain people. Knowledge that comes from secondary qualities does not provide objective facts about things.\n\nPrimary qualities are measurable aspects of physical reality. Secondary qualities are subjective.\n\n\nLeibniz was an early critic of the distinction, writing in his 1686 \"Discourse on Metaphysics\" that \"[i]t is even possible to demonstrate that the ideas of size, figure and motion are not so distinctive as is imagined, and that they stand for something imaginary relative to our perceptions as do, although to a greater extent, the ideas of color, heat, and the other similar qualities in regard to which we may doubt whether they are actually to be found in the nature of the things outside of us.\"\n\nGeorge Berkeley wrote his famous critique of this distinction in his book Three Dialogues between Hylas and Philonous. Berkeley maintained that the ideas created by sensations are all that people can know for sure. As a result, what is perceived as real consists only of ideas in the mind. The crux of Berkeley's argument is that once an object is stripped of all its secondary qualities, it becomes very problematic to assign any acceptable meaning to the idea that \"there is\" some object. Not that one cannot picture to oneself (in one's mind) that some object could exist apart from any perceiver — one clearly can do this — but rather, that one cannot give any \"content\" to this idea. Suppose that someone says that a particular mind-independent object (meaning, an object free of all secondary qualities) exists at some particular spatio-temporal location (in Newtonian terms, in some particular place and at some particular time). Now, none of this particularly means anything if one cannot specify a place and time. In that case it's still a purely imaginary, empty idea. This is not generally thought to be a problem because realists imagine that they can, in fact, specify a place and time for a 'mind-independent' object. What is overlooked is that they can only specify a place and time in place and time \"as we experience them\". Berkeley did not doubt that one can do this, but that it is objective. One has simply related ideas to experiences (the idea of an \"object\" to our \"experiences of space and time\"). In this case there is no space and time, and therefore no objectivity. Space and time as we experience them are always piecemeal (even when the piece of space is big, as in some astronomical photos), it is only in imagination that they are total and all-encompassing, which is how we definitely imagine (!) 'real' space and time as being. This is why Berkeley argued that the materialist has merely an \"idea\" of an unperceived object: because people typically do take our imagining or picturing, as guaranteeing an objective reality to the 'existence' of 'something'. In no adequate way has it been specified nor given any acceptable meaning. As such Berkeley comes to his conclusion that having a compelling image in the mind, one which connects to no specifiable thing external to us, does not guarantee an objective existence.\n\nKant, in his \"Prolegomena to Any Future Metaphysics That Will Be Able to Present Itself as a Science\", claimed that primary, as well as secondary, qualities are subjective. They are both mere appearances that are located in the mind of a knowing observer. In § 13, Remark II, he wrote: \"Long before Locke's time, but assuredly since him, it has been generally assumed and granted without detriment to the actual existence of external things, that many of their predicates may be said to belong not to the things in themselves, but to their appearances, and to have no proper existence outside our representation. Heat, color, and taste, for instance, are of this kind. Now, if I go farther, and for weighty reasons rank as mere appearances the remaining qualities of bodies also, which are called primary, such as extension, place, and in general space, with all that which belongs to it (impenetrability or materiality, space, etc.)—no one in the least can adduce the reason of its being inadmissible.\"\n\n"}
{"id": "33112023", "url": "https://en.wikipedia.org/wiki?curid=33112023", "title": "Rationes seminales", "text": "Rationes seminales\n\nRationes seminales (Latin, from the Greek \"λόγοι σπερματικοὶ\" or \"logoi spermatikoi\"), translated variously as germinal or causal principles, primordial reasons, original factors, seminal reasons or virtues, or seedlike principles, is a theological theory on the origin of species. It is the doctrine that God created the world in seed form, with certain potentialities, which then developed or unfolded accordingly over time; what appears to be change is simply the realization of the preexisting potentialities. The theory is a metaphor of the growth of a plant: much like a planted seed eventually develops into a tree, so when God created the world he planted \"rationes seminales\", from which all life sprung. It is intended to reconcile the belief that God created all things, with the evident fact that new things are constantly developing.\n\nThe roots of this idea can be found within the Greek philosophy of the Stoics and Neoplatonism\nThe idea was incorporated into Christian thought through the writings of authors such as Athenagoras of Athens, Tertullian, Gregory of Nyssa, Augustine of Hippo, Bonaventure, Albertus Magnus, and Roger Bacon, until mostly rejected in the modern period. Evolution, though now it is seen to be compatible with evolution theories (cf \"Man incarnate spirit\" by Ramon Lucas Lucas). The idea of \"rationes seminales\" was also used as an explanation for spontaneous generation.\n\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "246976", "url": "https://en.wikipedia.org/wiki?curid=246976", "title": "Square of opposition", "text": "Square of opposition\n\nThe square of opposition is a diagram representing the relations between the four basic categorical propositions.\nThe origin of the square can be traced back to Aristotle making the distinction between two oppositions: contradiction and contrariety.\nBut Aristotle did not draw any diagram. This was done several centuries later by Apuleius and Boethius.\n\nIn traditional logic, a proposition (Latin: \"propositio\") is a spoken assertion (\"oratio enunciativa\"), not the meaning of an assertion, as in modern philosophy of language and logic. A \"categorical proposition\" is a simple proposition containing two terms, subject and predicate, in which the predicate is either asserted or denied of the subject.\n\nEvery categorical proposition can be reduced to one of four logical forms. These are:\n\nIn tabular form:\n\n\nAristotle states (in chapters six and seven of the \"Peri hermaneias\" (Περὶ Ἑρμηνείας, Latin \"De Interpretatione\", English 'On Interpretation')), that there are certain logical relationships between these four kinds of proposition. He says that to every affirmation there corresponds exactly one negation, and that every affirmation and its negation are 'opposed' such that always one of them must be true, and the other false. A pair of affirmative and negative statements he calls a 'contradiction' (in medieval Latin, \"contradictio\"). Examples of contradictories are 'every man is white' and 'not every man is white' (also read as 'some men are not white'), 'no man is white' and 'some man is white'.\n\n'Contrary' (medieval: \"contrariae\") statements, are such that both cannot at the same time be true. Examples of these are the universal affirmative 'every man is white', and the universal negative 'no man is white'. These cannot be true at the same time. However, these are not contradictories because both of them may be false. For example, it is false that every man is white, since some men are not white. Yet it is also false that no man is white, since there are some white men.\n\nSince every statement has a contradictory opposite, and since a contradictory is true when its opposite is false, it follows that the opposites of contraries (which the medievals called subcontraries, \"subcontrariae\") can both be true, but they cannot both be false. Since subcontraries are negations of universal statements, they were called 'particular' statements by the medieval logicians.\n\nAnother logical opposition implied by this, though not mentioned explicitly by Aristotle, is 'alternation' (\"alternatio\"), consisting of 'subalternation' and 'superalternation'. Alternation is a relation between a particular statement and a universal statement of the same quality such that the particular is implied by the other. The particular is the subaltern of the universal, which is the particular's superaltern. For example, if 'every man is white' is true, its contrary 'no man is white' is false. Therefore, the contradictory 'some man is white' is true. Similarly the universal 'no man is white' implies the particular 'not every man is white'.\n\nIn summary:\n\nThese relationships became the basis of a diagram originating with Boethius and used by medieval logicians to classify the logical relationships. The propositions are placed in the four corners of a square, and the relations represented as lines drawn between them, whence the name 'The Square of Opposition'.\n\nSubcontraries, which medieval logicians represented in the form 'quoddam A est B' (some particular A is B) and 'quoddam A non est B' (some particular A is not B) cannot both be false, since their universal contradictory statements (every A is B / no A is B) cannot both be true. This leads to a difficulty that was first identified by Peter Abelard. 'Some A is B' seems to imply 'something is A'. For example, 'Some man is white' seems to imply that at least one thing is a man, namely the man who has to be white, if 'some man is white' is true. But, 'some man is not white' also implies that something is a man, namely the man who is not white, if the statement 'some man is not white' is true. But Aristotelian logic requires that necessarily one of these statements is true. Both cannot be false. Therefore, (since both imply that something is a man) it follows that necessarily something is a man, i.e. men exist. But (as Abelard points out, in the Dialectica) surely men might not exist?\n\nAbelard also points out that subcontraries containing subject terms denoting nothing, such as 'a man who is a stone', are both false.\n\nTerence Parsons argues that ancient philosophers did not experience the problem of existential import as only the A and I forms had existential import.\n\nHe goes on to cite medieval philosopher William of Moerbeke\n\nAnd points to Boethius' translation of Aristotle's work as giving rise to the mistaken notion that the O form has existential import.\n\nIn the 19th century, George Boole argued for requiring existential import on both terms in particular claims (I and O), but allowing all terms of universal claims (A and E) to lack existential import. This decision made Venn diagrams particularly easy to use for term logic. The square of opposition, under this Boolean set of assumptions, is often called the modern Square of opposition. In the modern square of opposition, A and O claims are contradictories, as are E and I, but all other forms of opposition cease to hold; there are no contraries, subcontraries, or subalterns. Thus, from a modern point of view, it often makes sense to talk about 'the' opposition of a claim, rather than insisting as older logicians did that a claim has several different opposites, which are in different kinds of opposition with the claim.\n\nGottlob Frege's \"Begriffsschrift\" also presents a square of oppositions, organised in an almost identical manner to the classical square, showing the contradictories, subalternates and contraries between four formulae constructed from universal quantification, negation and implication.\n\nAlgirdas Julien Greimas' semiotic square was derived from Aristotle's work.\n\nThe traditional square of opposition is now often compared with squares based on inner- and outer-negation \n\nThe square of opposition has been extended to a logical hexagon which includes the relationships of six statements. It was discovered independently by both Augustin Sesmat and Robert Blanché. It has been proven that both the square and the hexagon, followed by a \"logical cube\", belong to a regular series of n-dimensional objects called \"logical bi-simplexes of dimension n.\" The pattern also goes even beyond this.\n\nThe logical square, also called square of opposition or square of Apuleius has its origin in the four marked sentences to be employed in syllogistic reasoning: Every man is bad, the universal affirmative and its negation Not every man is bad (or Some men are not bad), the particular negative on the one hand, Some men are bad, the particular affirmative and its negation No man is bad, the universal negative on the other. Robert Blanché published with Vrin his Structures intellectuelles in 1966 and since then many scholars think that the logical square or square of opposition representing four values should be replaced by the logical hexagon which by representing six values is a more potent figure because it has the power to explain more things about logic and natural language.\n\n\n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
{"id": "28547570", "url": "https://en.wikipedia.org/wiki?curid=28547570", "title": "Terminology model", "text": "Terminology model\n\nA terminology model is a refinement of a concept system. Within a terminology model the concepts (object types) of a specific problem or subject area are defined by subject matter experts in terms of concept (object type) definitions and definitions of subordinated concepts or characteristics (properties). Besides object types, the terminology model allows defining hierarchical classifications, definitions for object type and property behavior and definition of casual relations.\n\nThe terminology model is a means for subject matter experts to express their knowledge about the subject in subject specific terms. Since the terminology model is structured rather similar to an object-oriented database schema, is can be transformed without loss of information into an object-oriented database schema. Thus, the terminology model is a method for problem analysis on the one side and a mean of defining database schema on the other side.\n\nSeveral terminology models have been developed and published in the field of statistics:\n\n\n"}
{"id": "53256704", "url": "https://en.wikipedia.org/wiki?curid=53256704", "title": "Threatcasting", "text": "Threatcasting\n\nThreatcasting is a conceptual framework used to help multidisciplinary groups envision future scenarios. It is also a process that enables systematic planning against threats ten years in the future. Utilizing the threatcasting process, groups explore possible future threats and how to transform the future they desire into reality while avoiding undesired futures. Threatcasting is a continuous, multiple-step process with inputs from social science, technical research, cultural history, economics, trends, expert interviews, and science fiction storytelling. These inputs inform the exploration of potential visions of the future.\n\nOnce inputs are explored for impact and application, participants create a science fiction story (Science Fiction Prototyping) based ten years in the future to add context around human activity. Science Fiction Prototyping consists of a future story about a person in a place doing a thing. The threatcasting process results in creation of many potential futures scenarios - some futures are desirable while others are not. Identifying both types of futures (desirable and undesirable) will help the participant recognize which future to aim toward, and which to avoid. Utilizing the scenarios, participants plot actions necessary in the present and at various intervals working toward the ten year future scenario. These actions will help participants understand how to empower or disrupt the target future scenario. Flags (warning events) are also determined in order to map societal indicators onto the recommended path toward the targeted future. When identified flags appear in society, threatcasting participants map these back to the original forecast to see whether or not they are on track toward the target future scenario.\n\nThe notion of threatcasting can be traced back to Brian David Johnson, an applied futurist, who first began using threatcasting, also referred to as futurecasting, in 2011 and to George Hemingway of the Stratalis Group, who pioneered notion of futurecasting for corporate strategy and innovation industrial markets, including mining in the same year. Early adopters of threatcasting include the United States Air Force Academy, the Government of California, and the Army Cyber Institute at West Point Military Academy. Official use of the term threatcasting is attributed to Brian David Johnson in a 2014 Gazette article “Drones, smart hydrants considered by experts looking at future of firefighting.”\n\nThreatcasting is fundamentally different from traditional strategic planning and scenario building processes due to the identification of specific actions, indicators and concrete steps that can be taken today to disrupt, mitigate and recover from future threats.\n\nThe Army Cyber Institute at West Point in conjunction with Arizona State University's Global Securities Initiative and the School for the Future of Innovation in Society have established a Threatcasting Lab to host and manage a Cyber Threatcasting Project which looks to envision future cyber threats ten years in the future. The first session of this collaborative group was held at West Point, NY in August 2016.\n\n\n"}
{"id": "39090245", "url": "https://en.wikipedia.org/wiki?curid=39090245", "title": "Transient modelling", "text": "Transient modelling\n\nTransient modelling is a way of looking at a process with the primary criterion of time, observing the pattern of changes in the subject being studied over time. The obverse of it is Steady state, where you might know only the starting and ending figures but do not understand the process by which they were derived.\n\nTransient models will reveal the pattern of a process, which might be sinusoidal or another shape that will help to design a better system to manage that process. Transient models can be done on a spreadsheet with an ability to generate charts, or by any software that can handle data of inputs and outputs and generate some sort of a display. Transient modelling does not need a computer. It is a methodology that has worked for centuries, by observers noting patterns of change against time, analysing the result and proposing improved design solutions.\n\nA simple example is a garden water tank. This is being topped up by rainfall from the roof, but when the tank is full, the remaining water goes to the drain. When the gardener draws water off, the level falls. If the garden is large and the summer is hot, a steady state will occur in summer where the tank is nearly always empty in summer. If the season is wet, the garden is getting water from the sky, and the tank is not being emptied sufficiently, so in steady state it will be observed to be always full. If the gardener has a way of observing the level of water in the tank, and a record of daily rainfall and temperatures, and is precisely metering the amount of water being drawn off every day, the numbers and the dates can be recorded in spreadsheet at daily intervals. After enough samples are taken, a chart can be developed to model the rise and fall pattern over a year, or over 2 years. With a better understanding of the process, it might emerge that a 200litre water tank would run out 20–25 days a year, but a 400-litre water tank would never run out, and a 300-litre tank would run out only 1-2 day a year and therefore that would be an acceptable risk and it would be the most economical solution.\n\nOne of the best examples of transient modelling is transient climate simulation. The analysis of ice cores in glaciers to understand climate change. Ice cores have thousands of layers, each of which represents a winter season of snowfall, and trapped in these are bubbles of air, particle of space dust and pollen which reveal climatic data of the time. By mapping these to a time scale, scientists can analyse the fluctuations over time and make predictions for the future.\n\nTransient modelling is the basis of weather forecasting, of managing ecosystems, rail timetabling, managing the electricity grid, setting the national budget, floating currency, understanding traffic flows on a freeway, solar gains on glass fronted buildings, or even of checking the day-to-day transactions of one's monthly bank statement.\n\nWith the transient modelling approach, you understand the whole process better when the inputs and outputs are graphed against time.\n\n"}
{"id": "37055344", "url": "https://en.wikipedia.org/wiki?curid=37055344", "title": "Triangle of opposition", "text": "Triangle of opposition\n\nIn the system of Aristotelian logic, the triangle of opposition is a diagram representing the different ways in which each of the three propositions of the system is logically related ('opposed') to each of the others. The system is also useful in the analysis of syllogistic logic, serving to identify the allowed logical conversions from one type to another.\n\n"}
