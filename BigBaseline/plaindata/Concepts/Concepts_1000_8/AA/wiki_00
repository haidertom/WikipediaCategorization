{"id": "48624755", "url": "https://en.wikipedia.org/wiki?curid=48624755", "title": "Acceptability", "text": "Acceptability\n\nAcceptability is the characteristic of a thing being subject to acceptance for some purpose. A thing is acceptable if it is sufficient to serve the purpose for which it is provided, even if it is far less usable for this purpose than the ideal example. A thing is unacceptable (or has the characteristic of unacceptability) if it deviates so far from the ideal that it is no longer sufficient to serve the desired purpose, or if it goes against that purpose. From a logical perspective, a thing can be said to be acceptable if it has no characteristics that make it unacceptable:\n\nHungarian mathematician Imre Lakatos developed a concept of acceptability \"taken as \"a measure of the approximation to the truth\"\". This concept was criticized in its applicability to philosophy as requiring that better theories first be eliminated. Acceptability is also a key premise of negotiation, wherein opposing sides each begin from a point of seeking their ideal solution, and compromise until they reach a solution that both sides find acceptable:\n\nWhere an unacceptable proposal has been made, \"a counterproposal is generated if there are any acceptable ones that have had already been explored\". Since the acceptability of proposition to a participant in a negotiation is only known to that participant, the participant may act as though a proposal that is actually acceptable to them is not, in order to obtain a more favorable proposal. \n\nOne concept of acceptability that has been widely studied is acceptable risk in situations affecting human health. The idea of not increasing lifetime risk by more than one in a million has become commonplace in public health discourse and policy. It is a heuristic measure. It provides a numerical basis for establishing a negligible increase in risk.\n\nEnvironmental decision making allows some discretion for deeming individual risks potentially \"acceptable\" if less than one in ten thousand chance of increased lifetime risk. Low risk criteria such as these provide some protection for a case where individuals may be exposed to multiple chemicals e.g. pollutants, food additives or other chemicals. In practice, a true zero-risk is possible only with the suppression of the risk-causing activity.\n\nStringent requirements of 1 in a million may not be technologically feasible or may be so prohibitively expensive as to render the risk-causing activity unsustainable, resulting in the optimal degree of intervention being a balance between risks vs. benefit. For example, emissions from hospital incinerators result in a certain number of deaths per year. However, this risk must be balanced against the alternatives. There are public health risks, as well as economic costs, associated with all options. The risk associated with no incineration is potential spread of infectious diseases, or even no hospitals. Further investigation identifies options such as separating noninfectious from infectious wastes, or air pollution controls on a medical incinerator.\n\nAcceptable variance is the range of variance in any direction from the ideal value that remains acceptable. In project management, variance can be defined as \"the difference between what is planned and what is actually achieved\". Degrees of variance \"can be classified into negative variance, zero variance, acceptable variance, and unacceptable variance\". In software testing, for example, \"[g]enerally 0-5% is considered as acceptable variance\" from an ideal value.\n\nAcceptance testing is a practice used in chemical and engineering fields, intended to check ahead of time whether or not a thing will be acceptable.\n"}
{"id": "38235255", "url": "https://en.wikipedia.org/wiki?curid=38235255", "title": "Affirmation and negation", "text": "Affirmation and negation\n\nIn linguistics and grammar, affirmation and negation (abbreviated respectively ' and ') are the ways that grammar encode negative and positive polarity in verb phrases, clauses, or other utterances. Essentially an affirmative (positive) form is used to express the validity or truth of a basic assertion, while a negative form expresses its falsity. Examples are the sentences \"Jane is here\" and \"Jane is not here\"; the first is affirmative, while the second is negative.\n\nThe grammatical category associated and with affirmative and negative is called polarity. This means that a sentence, verb phrase, etc. may be said to have either affirmative or negative polarity (its polarity may be either affirmative or negative). Affirmative is typically the unmarked polarity, whereas a negative statement is marked in some way, whether by a negating word or particle such as English \"not\", an affix such as Japanese -\"nai\", or by other means, which reverses the meaning of the predicate. The process of converting affirmative to negative is called negation – the grammatical rules for negation vary from language to language, and a given language may have more than one method of doing so.\n\nAffirmative and negative responses (especially, though not exclusively, to questions) are often expressed using particles or words such as \"yes\" and \"no\", where \"yes\" is the affirmative and \"no\" the negative particle.\n\nSpecial affirmative and negative words (particles) are often found in responses to questions, and sometimes to other assertions by way of agreement or disagreement. In English, these are \"yes\" and \"no\" respectively, in French \"oui\", \"si\" and \"non\", in Swedish \"ja\", \"jo\" and \"nej\", and so on. Not all languages make such common use of particles of this type; in some (such as Welsh) it is more common to repeat the verb or another part of the predicate, with or without negation accordingly.\n\nComplications sometimes arise in the case of responses to negative statements or questions; in some cases the response that confirms a negative statement is the negative particle (as in English: \"You're not going out? No.\"), but in some languages this is reversed. Some languages have a distinct form to answer a negative question, such as French \"si\" and Swedish \"jo\" (these serve to contradict the negative statement suggested by the first speaker).\n\nLanguages have a variety of grammatical rules for converting affirmative verb phrases or clauses into negative ones.\n\nIn many languages, an affirmative is made negative by the addition of a particle, meaning \"not\". This may be added before the verb phrase, as with the Spanish \"no\":\nOther examples of negating particles preceding the verb phrase include Italian \"non\", Russian не \"nye\" and Polish \"nie\" (they can also be found in constructed languages: \"ne\" in Esperanto and \"non\" in Interlingua). In some other languages the negating particle follows the verb or verb phrase, as in Dutch:\nParticles following the verb in this way include \"not\" in archaic and dialectal English (\"you remember not\"), \"nicht\" in German (\"ich schlafe nicht\", \"I am not sleeping\"), and \"inte\" in Swedish (\"han hoppade inte\", \"he did not jump\").\n\nIn French, particles are added both before the verb phrase (\"ne\") and after the verb (\"pas\"):\nHowever, in colloquial French the first particle is often omitted: \"Je sais pas\". Similar use of two negating particles can also be found in Afrikaans: \"Hy kan nie Afrikaans praat nie\" (\"He cannot speak Afrikaans\").\n\nIn standard Modern English, negation is achieved by adding \"not\" after an auxiliary verb (which here means one of a special grammatical class of verbs that also includes forms of the copula \"be\"; see English auxiliaries). If no such verb is present then the dummy auxiliary \"do\" (\"does\", \"did\") is introduced – see \"do\"-support. For example:\nDifferent rules apply in subjunctive, imperative and non-finite clauses. For more details see . (In Middle English, the particle \"not\" could follow any verb, e.g. \"I see not the horse.\")\n\nIn some languages, like Welsh, verbs have special inflections to be used in negative clauses. (In some language families, this may lead to reference to a negative mood.) An example is Japanese, which conjugates verbs in the negative after adding the suffix \"-nai\" (indicating negation), e.g. \"taberu\" (\"eat\") and \"tabenai\" (\"do not eat\"). It could be argued that English has joined the ranks of these languages, since negation requires the use of an auxiliary verb and a distinct syntax in most cases; the form of the basic verb can change on negation, as in \"he sings\" vs. \"he doesn't sing\". Zwicky and Pullum have shown that \"n't\" is an inflectional suffix, not a clitic or a derivational suffix.\n\nComplex rules for negation also apply in Finnish; see . In some languages negation may also affect the dependents of the verb; for example in some Slavic languages, such as Russian, the case of a direct object often changes from accusative to genitive when the verb is negated.\n\nNegation can be applied not just to whole verb phrases, clauses or sentences, but also to specific elements (such as adjectives and noun phrases) within sentences. Ways in which this can be done again depend on the grammar of the language in question. English generally places \"not\" before the negated element, as in \"I witnessed not a debate, but a war.\" There are also negating affixes, such as the English prefixes \"non-\", \"un-\", \"in-\", etc. Such elements are called privatives.\n\nThere also exist elements which carry a specialized negative meaning, including pronouns such as \"nobody\", \"none\" and \"nothing\", determiners such as \"no\" (as in \"no apples\"), and adverbs such as \"never\", \"no longer\" and \"nowhere\".\n\nAlthough such elements themselves have negative force, in some languages a clause in which they appear is additionally marked for ordinary negation. For example, in Russian, \"I see nobody\" is expressed as я никого́ не ви́жу \"ja nikovó nye vízhu\", literally \"I nobody not see\" – the ordinary negating particle не \"nye\" (\"not\") is used in addition to the negative pronoun никого́ \"nikovó\" (\"nobody\"). Italian behaves in a similar way: \"Non ti vede nessuno\", \"nobody can see you\", although \"Nessuno ti vede\" is also a possible clause with exactly the same meaning.\n\nIn Russian, all of the elements (\"not\", \"never\", \"nobody\", \"nowhere\") would appear together in the sentence in their negative form. In Italian, a clause works much as in Russian, but \"non\" does not have to be there, and can be there only before the verb if it precedes all other negative elements: \"Tu non porti mai nessuno da nessuna parte\". \"Nobody ever brings you anything here\", however, could be translated \"Nessuno qui ti porta mai niente\" or \"Qui non ti porta mai niente nessuno\". In French, where simple negation is performed using \"ne ... pas\" (see above), specialized negatives appear in combination with the first particle (\"ne\"), but \"pas\" is omitted:\nIn Ancient Greek, a simple negative (οὐ \"ou\" \"not\" or μή \"mḗ\" \"not (modal)\") following another simple or compound negative (e.g. οὐδείς \"oudeís\" \"nobody\") results in an affirmation, whereas a compound negative following a simple or compound negative strengthens the negation:\n\nSimple grammatical negation of a clause in principle has the effect of converting a proposition to its logical negation – replacing an assertion that something is the case by an assertion that it is not the case.\n\nIn some cases, however, particularly when a particular modality is expressed, the semantic effect of negation may be somewhat different. For example, in English, the meaning of \"you must not go\" is not in fact the exact negation of that of \"you must go\" – this would be expressed as \"you don't have to go\" or \"you needn't go\". The negation \"must not\" has a stronger meaning (the effect is to apply the logical negation to the following infinitive rather than to the full clause with \"must\"). For more details and other similar cases, see the relevant sections of English modal verbs.\n\nIn some cases, by way of irony, an affirmative statement may be intended to have the meaning of the corresponding negative, or vice versa. For examples see antiphrasis and sarcasm.\n\nFor the use of double negations or similar as understatements (\"not unappealing\", \"not bad\", etc.) see litotes.\n\n\n\n"}
{"id": "34460605", "url": "https://en.wikipedia.org/wiki?curid=34460605", "title": "Andersen healthcare utilization model", "text": "Andersen healthcare utilization model\n\nThe Andersen Healthcare Utilization Model - is a conceptual model aimed at demonstrating the factors that lead to the use of health services. According to the model, usage of health services (including inpatient care, physician visits, dental care etc.) is determined by three dynamics: predisposing factors, enabling factors, and need. Predisposing factors can be characteristics such as race, age, and health beliefs. For instance, an individual who believes health services are an effective treatment for an ailment is more likely to seek care. Examples of enabling factors could be family support, access to health insurance, one's community etc. Need represents both perceived and actual need for health care services. The original model was developed by Ronald M. Andersen, a health services professor at UCLA, in 1968. The original model was expanded through numerous iterations and its most recent form models past the use of services to end at health outcomes and includes feedback loops.\n\nA major motivation for the development of the model was to offer measures of access. Andersen discusses four concepts within access that can be viewed through the conceptual framework. Potential access is the presence of enabling resources, allowing the individual to seek care if needed. Realized access is the actual use of care, shown as the outcome of interest in the earlier models. The Andersen framework also makes a distinction between equitable and inequitable access. Equitable access is driven by demographic characteristics and need whereas inequitable access is a result of social structure, health beliefs, and enabling resources.\n\nAndersen also introduces the concept of mutability of his factors. The idea here being that if a concept has a high degree of mutability (can be easily changed) perhaps policy would be justified in using its resources to do rather than a factor with low mutability. Characteristics that fall under demographics are quite difficult to change, however, enabling resources is assigned a high degree of mutability as the individual, community, or national policy can take steps to alter the level of enabling resources for an individual. For example, if the government decides to expand the Medicaid program an individual may experience an increase in enabling resources, which in turn may beget an increase in health services usage. The RAND Health Insurance Experiment (HIE) changed a highly mutable factor, out-of-pocket costs, which greatly changed individual rates of health services usage.\n\nThe initial behavior model was an attempt to study of why a family uses health services. However, due to the heterogeneity of family members the model focused on the individual rather than the family as the unit of analysis. Andersen also states that the model functions both to predict and explain use of health services.\n\nA second model was developed in the 1970s in conjunction with Aday and colleagues at the University of Chicago. This iteration includes systematic concepts of health care such as current policy, resources, and organization. The second generation model also extends the outcome of interest beyond utilization to consumer satisfaction.\n\nThe next generation of the model builds upon this idea by including health status (both perceived and evaluated) as outcomes alongside consumer satisfaction. Furthermore, this model include personal health practices as an antecedent to outcomes, acknowledging that it not solely use of health services that drives health and satisfaction. This model emphasizes a more public health approach of prevention, as advocated by Evans and Stoddart wherein personal health practices (i.e. smoking, diet, exercise) are included as a driving force towards health outcomes.\n\nThe 6th iteration of Andersen’s conceptual framework focuses on the individual as the unit of analysis and goes beyond health care utilization, adopting health outcomes as the endpoint of interest. This model is further differentiated from its predecessors by using a feedback loop to illustrate that health outcomes may affect aspects such as health beliefs, and need. It added genetic susceptibility as a predisposing determinant and quality of life as an outcome By using the framework’s relationships we can determine the directionality of the effect following a change in an individual’s characteristics or environment. For example, if one experiences an increase in need as a result of an infection, the Andersen model predicts this will lead to an increased use of services (all else equal). One potential change for a future iteration of this model is to add genetic information under predisposing characteristics. As genetic information becomes more readily available it seems likely this could impact health services usage, as well as health outcomes, beyond what is already accounted for in the current model.\n\nThe model has been criticized for not paying enough attention to culture and social interaction but Andersen argues this social structure is included in the \"predisposing characteristics\" component. Another criticism was the overemphasis of need and at the expense of health beliefs and social structure. However, Andersen argues need itself is a social construct. This is why need is split into perceived and evaluated. Where evaluated need represents a more measurable/objective need, perceived need is partly determined by health beliefs, such as whether or not people think their condition is serious enough to seek health services. Another limitation of the model is its emphasis on health care utilization or adopting health outcomes as a dichotomous factor, present or not present. Other help-seeking models also consider the type of help source, including informal sources. More recent work has taken help-seeking behaviors further, and more real-world, by including online and other non-face-to-face sources.\n"}
{"id": "5354120", "url": "https://en.wikipedia.org/wiki?curid=5354120", "title": "Apollonian and Dionysian", "text": "Apollonian and Dionysian\n\nThe Apollonian and Dionysian is a philosophical and literary concept, or also a dichotomy, based on Apollo and Dionysus in Greek mythology. Some Western philosophical and literary figures have invoked this dichotomy in critical and creative works, most notably Friedrich Nietzsche and later followers. \n\nIn Greek mythology, Apollo and Dionysus are both sons of Zeus. Apollo is the god of the sun, of rational thinking and order, and appeals to logic, prudence and purity. Dionysus is the god of wine and dance, of irrationality and chaos, and appeals to emotions and instincts. The Ancient Greeks did not consider the two gods to be opposites or rivals, although often the two deities were entwined by nature.\n\nAlthough the use of the concepts of the Apollonian and Dionysian is linked to Nietzsche's \"The Birth of Tragedy\", the terms were used before him in German culture. The poet Hölderlin spoke of them, while Winckelmann talked of Bacchus, the god of wine. After Nietzsche, others have continued to make use of the distinction. For example, Rudolf Steiner treated in depth the Apollonian and Dionysian and placed them in the general history and spiritual evolution of mankind.\n\nNietzsche's aesthetic usage of the concepts, which was later developed philosophically, first appeared in his book \"The Birth of Tragedy\", which was published in 1872. His major premise here was that the fusion of Dionysian and Apollonian \"Kunsttriebe\" (\"artistic impulses\") form dramatic arts, or tragedies. He goes on to argue that this fusion has not been achieved since the ancient Greek tragedians. Nietzsche is adamant that the works of Aeschylus and Sophocles represent the apex of artistic creation, the true realization of tragedy; it is with Euripides that tragedy begins its downfall (\"Untergang\"). Nietzsche objects to Euripides's use of Socratic rationalism (the dialectic) in his tragedies, claiming that the infusion of ethics and reason robs tragedy of its foundation, namely the fragile balance of the Dionysian and Apollonian.\n\nTo further the split, Nietzsche diagnoses the Socratic Dialectic as being diseased in the manner that it deals with looking at life. The scholarly dialectic is directly opposed to the concept of the Dionysian because it only seeks to negate life; it uses reason to always deflect, but never to create. Socrates rejects the intrinsic value of the senses and life for \"higher\" ideals. Nietzsche claims in \"The Gay Science\" that when Socrates drinks the hemlock, he sees the hemlock as the cure for life, proclaiming that he has been sick a long time. (Section 340.) In contrast, the Dionysian existence constantly seeks to affirm life. Whether in pain or pleasure, suffering or joy, the intoxicating revelry that Dionysus has for life itself overcomes the Socratic sickness and perpetuates the growth and flourishing of visceral life force—a great Dionysian 'Yes', to a Socratic 'No'. \nThe interplay between the Apollonian and Dionysian is apparent, Nietzsche claimed in \"The Birth of Tragedy\", from their use in Greek tragedy: the tragic hero of the drama, the main protagonist, struggles to make order of his unjust fate, though he dies unfulfilled in the end. For the audience of such a drama, Nietzsche claimed, this tragedy allows them to sense an underlying essence, what he called the \"Primordial Unity\", which revives our Dionysian nature—which is almost indescribably pleasurable. However, he later dropped this concept saying it was \"...burdened with all the errors of youth\" (Attempt at Self-Criticism, §2), the overarching theme was a sort of metaphysical solace or connection with the heart of creation.\n\nDifferent from Kant's idea of the sublime, the Dionysian is all-inclusive rather than alienating to the viewer as a sublimating experience. The sublime needs critical distance, while the Dionysian demands a closeness of experience. According to Nietzsche, the critical distance, which separates man from his closest emotions, originates in Apollonian ideals, which in turn separate him from his essential connection with self. The Dionysian embraces the chaotic nature of such experience as all-important; not just on its own, but as it is intimately connected with the Apollonian. The Dionysian magnifies man, but only so far as he realizes that he is one and the same with all ordered human experience. The godlike unity of the Dionysian experience is of utmost importance in viewing the Dionysian as it is related to the Apollonian, because it emphasizes the harmony that can be found within one's chaotic experience.\n\nNietzsche's idea has been interpreted as an expression of \"fragmented consciousness\" or existential instability by a variety of modern and post-modern writers, especially Martin Heidegger, Michel Foucault and Gilles Deleuze. According to Peter Sloterdijk, the Dionysian and the Apollonian form a dialectic; they are contrasting, but Nietzsche does not mean one to be valued more than the other. Truth being \"primordial pain\", our existential being is determined by the Dionysian/Apollonian dialectic.\n\nExtending the use of the Apollonian and Dionysian onto an argument on interaction between the mind and physical environment, Abraham Akkerman has pointed to masculine and feminine features of city form.\n\nAnthropologist Ruth Benedict used the terms to characterize cultures that value restraint and modesty (Apollonian) and ostentatiousness and excess (Dionysian). An example of an Apollonian culture in Benedict's analysis was the Zuñi people as opposed to the Dionysian Kwakiutl people. The theme was developed by Benedict in her main work \"Patterns of Culture\".\n\nAlbert Szent-Györgyi, who wrote that \"a discovery must be, by definition, at variance with existing knowledge\", divided scientists into two categories: the Apollonians and the Dionysians. He called scientific dissenters, who explored \"the fringes of knowledge\", Dionysians. He wrote, \"In science the Apollonian tends to develop established lines to perfection, while the Dionysian rather relies on intuition and is more likely to open new, unexpected alleys for research...The future of mankind depends on the progress of science, and the progress of science depends on the support it can find. Support mostly takes the form of grants, and the present methods of distributing grants unduly favor the Apollonian.\"\n\nAmerican humanities scholar Camille Paglia writes about the Apollonian and Dionysian in her 1990 bestseller \"Sexual Personae\". The broad outline of her concept has roots in Nietzschean discourse, an admitted influence, although Paglia's ideas diverge significantly.\n\nThe Apollonian and Dionysian concepts comprise a dichotomy that serves as the basis of Paglia's theory of art and culture. For Paglia, the Apollonian is light and structured while the Dionysian is dark and chthonic (she prefers \"Chthonic\" to Dionysian throughout the book, arguing that the latter concept has become all but synonymous with hedonism and is inadequate for her purposes, declaring that \"the Dionysian is no picnic.\"). The Chthonic is associated with females, wild/chaotic nature, and unconstrained sex/procreation. In contrast, the Apollonian is associated with males, clarity, celibacy and/or homosexuality, rationality/reason, and solidity, along with the goal of oriented progress: \"Everything great in western civilization comes from struggle against our origins.\"\n\nShe argues that there is a biological basis to the Apollonian/Dionysian dichotomy, writing: \"The quarrel between Apollo and Dionysus is the quarrel between the higher cortex and the older limbic and reptilian brains.\" Moreover, Paglia attributes all the progress of human civilization to masculinity revolting against the Chthonic forces of nature, and turning instead to the Apollonian trait of ordered creation. The Dionysian is a force of chaos and destruction, which is the overpowering and alluring chaotic state of wild nature. Rejection of – or combat with – Chthonianism by socially constructed Apollonian virtues accounts for the historical dominance of men (including asexual and homosexual men; and childless and/or lesbian-leaning women) in science, literature, arts, technology and politics. As an example, Paglia states: \"The male orientation of classical Athens was inseparable from its genius. Athens became great not despite but because of its misogyny.\"\n\n"}
{"id": "66975", "url": "https://en.wikipedia.org/wiki?curid=66975", "title": "Apophatic theology", "text": "Apophatic theology\n\nApophatic theology, also known as negative theology, is a form of theological thinking and religious practice which attempts to approach God, the Divine, by negation, to speak only in terms of what may not be said about the perfect goodness that is God. It forms a pair together with cataphatic theology, which approaches God or the Divine by affirmations or positive statements about what God \"is\".\n\nThe apophatic tradition is often, though not always, allied with the approach of mysticism, which aims at the vision of God, the perception of the divine reality beyond the realm of ordinary perception.\n\n\"Apophatic\", (adjective); from ἀπόφημι \"apophēmi\", meaning \"to deny\". From \"Online Etymology Dictionary\": \n\"Via negativa\" or \"via negationis\" (Latin), \"negative way\" or \"by way of denial\". The negative way forms a pair together with the \"kataphatic\" or positive way. According to Deirdre Carabine,\nAccording to Fagenblat, \"negative theology is as old as philosophy itself;\" elements of it can be found in Plato's \"unwritten doctrines,\" while it is also present in Neo-Platonic, Gnostic and early Christian writers. A tendency to apophatic thought can also be found in Philo of Alexandria.\n\nAccording to Carabine, \"apophasis proper\" in Greek thought starts with Neo-Platonism, with its speculations about the nature of the One, culminating in the works of Proclus. According to Carabine, there are two major points in the development of apophatic theology, namely the fusion of the Jewish tradition with Platonic philosophy in the writings of Philo, and the works of Dionysius the Pseudo-Areopagite, who infused Christian thought with Neo-Platonic ideas.\n\nThe Early Church Fathers were influenced by Philo, and Meredith even states that Philo \"is the real founder of the apophatic tradition.\" Yet, it was with Pseudo-Dionysius the Areopagite and Maximus the Confessor, whose writings shaped both Hesychasm, the contemplative tradition of the Eastern Orthodox Churches, and the mystical traditions of western Europe, that apophatic theology became a central element of Christian theology and contemplative practice.\n\nFor the ancient Greeks, knowledge of the gods was essential for proper worship. Poets had an important responsibility in this regard, and a central question was how knowledge of the Divine forms can be attained. Epiphany played an essential role in attaining this knowledge. Xenophanes (c. 570 – c. 475 BC) noted that the knowledge of the Divine forms is restrained by the human imagination, and Greek philosophers realized that this knowledge can only be mediated through myth and visual representations, which are culture-dependent.\n\nAccording to Herodotus (484–425 BCE), Homer and Hesiod (between 750 and 650 BC) taught the Greek the knowledge of the Divine bodies of the Gods. The ancient Greek poet Hesiod (between 750 and 650 BC) describes in his \"Theogony\" the birth of the gods and creation of the world, which became an \"ur-text for programmatic, first-person epiphanic narratives in Greek literature,\" but also \"explores the necessary limitations placed on human access to the divine.\" According to Platt, the statement of the Muses who grant Hesiod knowledge of the Gods \"actually accords better with the logic of apophatic religious thought.\"\n\nParmenides (fl. late sixth or early fifth century BC), in his poem \"On Nature\", gives an account of a revelation on two ways of inquiry. \"The way of conviction\" explores Being, true reality (\"what-is\"), which is \"What is ungenerated and deathless,/whole and uniform, and still and perfect.\" \"The way of opinion\" is the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. His distinction between unchanging Truth and shifting opinion is reflected in Plato's allegory of the Cave. Together with the Biblical story of Moses's ascent of Mount Sinai, it is used by Gregory of Nyssa and Pseudo-Dionysius the Areopagite to give a Christian account of the ascent of the soul toward God. Cook notes that Parmenides poem is a religious account of a mystical journey, akin to the mystery cults, giving a philosophical form to a religious outlook. Cook further notes that the philosopher's task is to \"attempt through 'negative' thinking to tear themselves loose from all that frustrates their pursuit of wisdom.\"\n\nPlato (428/427 or 424/423 – 348/347 BCE), \"deciding for Parmenides against Heraclitus\" and his theory of eternal change, had a strong influence on the development of apophatic thought.\n\nPlato further explored Parmenides's idea of timeless truth in his dialogue \"Parmenides\", which is a treatment of the eternal forms, \"Truth, Beauty and Goodness\", which are the real aims for knowledge. The Theory of Forms is Plato's answer to the problem \"how one unchanging reality or essential being can admit of many changing phenomena (and not just by dismissing them as being mere illusion).\"\n\nIn \"The Republic\", Plato argues that the \"real objects of knowledge are not the changing objects of the senses, but the immutable Forms,\" stating that the \"Form of the Good\" is the highest object of knowledge. His argument culminates in the Allegory of the Cave, in which he argues that humans are like prisoners in a cave, who can only see shadows of the Real, the \"Form of the Good\". Humans are to be educated to search for knowledge, by turning away from their bodily desires toward higher contemplation, culminating in an intellectual understanding or apprehension of the Forms, c.q. the \"first principles of all knowledge.\"\n\nAccording to Cook, the \"Theory of Forms\" has a theological flavour, and had a strong influence on the ideas of his Neo-Platonist interpreters Proclus and Plotinus. The pursuit of \"Truth, Beauty and Goodness\" became a central element in the apophatic tradition, but nevertheless, according to Carabine \"Plato himself cannot be regarded as the founder of the negative way.\" Carabine warns not to read later Neo-Platonic and Christian understandings into Plato, and notes that Plato did not identify his Forms with \"one transcendent source,\" an identification which his later interpreters made.\n\nMiddle Platonism (1st century BCE - 3rd century CE) further investigated Plato's \"Unwritten Doctrines,\" which drew on Pythagoras' first principles of the Monad and the Dyad (matter). Middle Platonism proposed a hierarchy of being, with God as its first principle at its top, identifying it with Plato's \"Form of the Good\". An influential proponent of Middle Platonism was Philo (c.25 BCE–c. 50 CE), who employed Middle Platonic philosophy in his interpretation of the Hebrew scriptures, and asserted a strong influence on early Christianity. According to Craig D. Allert, \"Philo made a monumental contribution to the creation of a vocabulary for use in negative statements about God.\" For Philo, God is undescribable, and he uses terms which emphasize God's transcendence.\n\nNeo-Platonism was a mystical or contemplative form of Platonism, which \"developed outside the mainstream of Academic Platonism.\" It started with the writings of Plotinus (204/5–270), and ended with the closing of the Platonic Academy by Emperor Justinian in 529 CE, when the pagan traditions were ousted. It is a product of Hellenistic syncretism, which developed due to the crossover between Greek thought and the Jewish scriptures, and also gave birth to Gnosticism. Proclus was the last head of the Platonic Academy; his student Pseudo-Dinosysius had a far-stretching Neo-Platonic influence on Christianity and Christian mysticism.\n\nPlotinus (204/5–270) was the founder of Neo-Platonism. In the Neo-Platonic philosophy of Plotinus and Proclus, the first principle became even more elevated as a radical unity, which was presented as an unknowable Absolute. For Plotinus, \"the One\" is the first principle, from which everything else emanates. He took it from Plato's writings, identifying the Good of the \"Republic\", as the cause of the other Forms, with \"the One\" of the first hypothesis of the second part of the \"Parmenides\". For Plotinus, \"the One\" precedes the Forms, and \"is beyond Mind and indeed beyond Being.\" From \"the One\" comes the Intellect, which contains all the Forms. \"The One\" is the principle of Being, while the Forms are the principle of the essence of beings, and the intelligibility which can recognize them as such. Plotinus's third principle is Soul, the desire for objects external to the person. The highest satisfaction of desire is the contemplation of \"the One\", which unites all existents \"as a single, all-pervasive reality.\"\n\n\"The One\" is radically simple, and does not even have self-knowledge, since self-knowledge would imply multiplicity. Nevertheless, Plotinus does urge for a search for the Absolute, turning inward and becoming aware of the \"presence of the intellect in the human soul,\" initiating an ascent of the soul by abstraction or \"taking away,\" culminating in a sudden appearance of \"the One\". In the \"Enneads\" Plotinus writes: \nCarabine notes that Plotinus' apophasis is not just a mental exercise, an acknowledgement of the unknowability of \"the One\", but a means to \"extasis\" and an ascent to \"the unapproachable light that is God.\" Pao-Shen Ho, investigating what are Plotinus' methods for reaching \"henosis\", concludes that \"Plotinus' mystical teaching is made up of two practices only, namely philosophy and negative theology.\" According to Moore, Plotinus appeals to the \"non-discursive, intuitive faculty of the soul,\" by \"calling for a sort of prayer, an invocation of the deity, that will permit the soul to lift itself up to the unmediated, direct, and intimate contemplation of that which exceeds it (V.1.6).\" Pao-Shen Ho further notes that \"for Plotinus, mystical experience is irreducible to philosophical arguments.\" The argumentation about \"henosis\" is preceded by the actual experience of it, and can only be understood when \"henosis\" has been attained. Ho further notes that Plotinus's writings have a didactic flavour, aiming to \"bring his own soul and \"the souls of others\" by way of Intellect to union with the One.\" As such, the \"Enneads\" as a spiritual or ascetic teaching device, akin to \"The Cloud of Unknowing\", demonstrating the methods of philosophical and apophatic inquiry. Ultimately, this leads to silence and the abandonment of all intellectual inquiry, leaving contemplation and unity.\n\nProclus (412-485) introduced the terminology which is being used in apophatic and cataphatic theology. He did this in the second book of his \"Platonic Theology\", arguing that Plato states that \"the One\" can be revealed \"through analogy,\" and that \"through negations [\"dia ton apophaseon\"] its transcendence over everything can be shown.\" For Proclus, apophatic and cataphonic theology form a contemplatory pair, with the apophatic approach corresponding to the manifestation of the world from \"the One\", and cataphonic theology corresponding to the return to \"the One\". The analogies are affirmations which direct us toward \"the One\", while the negations underlie the confirmations, being closer to \"the One\". According to Luz, Proclus also attracted students from other faiths, including the Samaritan Marinus. Luz notes that \"Marinus' Samaritan origins with its Abrahamic notion of a single ineffable Name of God () should also have been in many ways compatible with the school's ineffable and apophatic divine principle.\"\nThe Book of Revelation 8:1 mentions \"the silence of the perpetual choir in heaven.\" According to Dan Merkur,\nThe Early Church Fathers were influenced by Philo (c. 25 BCE – c. 50 CE), who saw Moses as \"the model of human virtue and Sinai as the archetype of man's ascent into the \"luminous darkness\" of God.\" His interpretation of Moses was followed by Clement of Alexandria, Origen, the Cappadocian Fathers, Pseudo-Dionysius, and Maximus the Confessor.\n\nGod's appearance to Moses in the burning bush was often elaborated on by the Early Church Fathers, especially Gregory of Nyssa (c. 335 – c. 395), realizing the fundamental unknowability of God; an exegesis which continued in the medieval mystical tradition. Their response is that, although God is unknowable, Jesus as person can be followed, since \"following Christ is the human way of seeing God.\"\n\nClement of Alexandria (c. 150 – c. 215) was an early proponent of apophatic theology. According to R.A. Baker, in Clement's writings the term \"theoria\" develops further from a mere intellectual \"seeing\" toward a spirutal form of contemplation. Clement's apophatic theology or philosophy is closely related to this kind of \"theoria\" and the \"mystic vision of the soul.\" For Clement, God is transcendent and immanent. According to Baker, Clement's apophaticism is mainly driven by Biblical texts, but by the Platonic tradition. His conception of an ineffable God is a synthesis of Plato and Philo, as seen from a Biblical perspective. According to Osborne, it is a synthesis in a Biblical framework; according to Baker, while the Platonic tradition accounts for the negative approach, the Biblical tradition accounts for the positive approach. \"Theoria\" and abstraction is the means to conceive of this ineffable God; it is preceded by dispassion.\n\nAccording to Tertullian (c. 155 – c. 240),\nSaint Cyril of Jerusalem (313-386), in his \"Catechetical Homilies\", states: \nAugustine of Hippo (354-430) defined God \"aliud, aliud valde\", meaning \"other, completely other\", in \"Confessions\" 7.10.16.\n\nApophatic theology found its most influential expression in the works of Pseudo-Dionysius the Areopagite (late 5th to early 6th century), a student of Proclus (412-485), combining a Christian worldview with Neo-Platonic ideas. He is a constant factor in the contemplative tradition of the eastern Orthodox Churches, and from the 9th century onwards his writings also had a strong impact on western mysticism.\n\nDionysius the Areopagite was a pseudonym, taken from Acts of the Apostles chapter 17, in which Paul gives a missionary speech to the court of the Areopagus in Athens. In Paul makes a reference to an altar-inscription, dedicated to the Unknown God, \"a safety measure honoring foreign gods still unknown to the Hellenistic world.\" For Paul, Jesus Christ is this unknown God, and as a result of Paul's speech Dionysius the Areopagite converts to Christianity. Yet, according to Stang, for Pseudo-Dionysius the Areopagite Athens is also the place of Neo-Platonic wisdom, and the term \"unknown God\" is a reversal of Paul's preaching toward an integration of Christianity with Neo-Platonism, and the union with the \"unknown God.\"\n\nAccording to Corrigan and Harrington, \"Dionysius' central concern is how a triune God, ... who is utterly unknowable, unrestricted being, beyond individual substances, beyond even goodness, can become manifest to, in, and through the whole of creation in order to bring back all things to the hidden darkness of their source.\" Drawing on Neo-Platonism, Pseudo-Dionysius described humans ascend to divinity as a process of purgation, illumination and union. Another Neo-Platonic influence was his description of the cosmos as a series of hierarchies, which overcome the distance between God and humans.\n\nIn Orthodox Christianity apophatic theology is taught as superior to cataphatic theology. The fourth-century Cappadocian Fathers stated a belief in the existence of God, but an existence unlike that of everything else: everything else that exists was created, but the Creator transcends this existence, is uncreated. The essence of God is completely unknowable; mankind can know God only through His energies. Gregory of Nyssa (c.335-c.395), John Chrysostom (c. 349 – 407), and Basil the Great (329-379) emphasized the importance of negative theology to an orthodox understanding of God. John of Damascus (c.675/676–749) employed negative theology when he wrote that positive statements about God reveal \"not the nature, but the things around the nature.\"\n\nMaximus the Confessor (580-622) took over Pseudo-Dionysius' ideas, and had a strong influence on the theology and contemplative practices of the Eastern Orthodox Churches. Gregory Palamas (1296–1359) formulated the definite theology of Hesychasm, the Orthodox practices of contemplative prayer and theosis, \"deification.\"\n\nInfluential modern Eastern Orthodox theologians are Vladimir Lossky, John Meyendorff, John S. Romanides and Georges Florovsky. Lossky argues, based on his reading of Dionysius and Maximus Confessor, that positive theology is always inferior to negative theology which is a step along the way to the superior knowledge attained by negation. This is expressed in the idea that mysticism is the expression of dogmatic theology \"par excellence\".\n\nAccording to Lossky, outside of directly revealed knowledge through Scripture and Sacred Tradition, such as the Trinitarian nature of God, God in His essence is beyond the limits of what human beings (or even angels) can understand. He is transcendent in essence (\"ousia\"). Further knowledge must be sought in a direct experience of God or His indestructible energies through \"theoria\" (vision of God). According to Aristotle Papanikolaou, in Eastern Christianity, God is immanent in his hypostasis or existences.\n\nNegative theology has a place in the Western Christian tradition as well. The 9th-century theologian John Scotus Erigena wrote: \n\nWhen he says \"\"He is not anything\" and \"God is not\"\", Scotus does not mean that there is no God, but that God cannot be said to exist in the way that creation exists, i.e. that God is uncreated. He is using apophatic language to emphasise that God is \"other\".\n\nTheologians like Meister Eckhart and Saint John of the Cross (San Juan de la Cruz) exemplify some aspects of or tendencies towards the apophatic tradition in the West. The medieval work, \"The Cloud of Unknowing\" and Saint John's \"Dark Night of the Soul\" are particularly well known. In 1215 apophatism became the official position of the Catholic Church, which, on the basis of Scripture and church tradition, during the Fourth Lateran Council formulated the following dogma:\nThomas Aquinas was born ten years later (1225-1274) and, although in his \"Summa Theologica\" he quotes Pseudo-Dionysius 1,760 times, his reading in a neo-Aristotelian key of the conciliar declaration overthrew its meaning inaugurating the \"analogical way\" as \"tertium\" between \"via negativa\" and \"via positiva\": the \"via eminentiae\" (see also \"analogia entis\"). According to Adrian Langdon,\nAccording to \"Catholic Encyclopedia\", the \"Doctor Angelicus\" and the scholastici declare [that] \nSince then Thomism has played a decisive role in resizing the negative or apophatic tradition of the magisterium.\n\nApophatic statements are still crucial to many modern theologians, restarting in 1800s by Søren Kierkegaard (see his concept of the infinite qualitative distinction) up to Rudolf Otto and Karl Barth (see their idea of \"Wholly Other\", i.e. \"ganz Andere\" or \"totaliter aliter\").\n\nC. S. Lewis, in his book \"Miracles\" (1947), advocates the use of negative theology when first thinking about God, in order to cleanse our minds of misconceptions. He goes on to say we must then refill our minds with the truth about God, untainted by mythology, bad analogies or false mind-pictures.\n\nThe mid-20th century Dutch philosopher Herman Dooyeweerd, who is often associated with a neo-Calvinistic tradition, provides a philosophical foundation for understanding why we can never absolutely know God, and yet, paradoxically, truly know something of God. Dooyeweerd made a sharp distinction between theoretical and pre-theoretical attitudes of thought. Most of the discussion of knowledge of God presupposes theoretical knowledge, in which we reflect and try to define and discuss. Theoretical knowing, by its very nature, is never absolute, always depends on religious presuppositions, and cannot grasp either God or the law side. Pre-theoretical knowing, on the other hand, is intimate engagement, and exhibits a diverse range of aspects. Pre-theoretical intuition, on the other hand, can grasp at least the law side. Knowledge of God, as God wishes to reveal it, is pre-theoretical, immediate and intuitive, never theoretical in nature. The philosopher Leo Strauss considered that the Bible, for example, should be treated as pre-theoretical (everyday) rather than theoretical in what it contains.\n\nIvan Illich (1926-2002), the historian and social critic, can be read as an apophatic theologian, according to a longtime collaborator, Lee Hoinacki, in a paper presented in memory of Illich, called \"Why Philia?\"\n\nAccording to Deirdre Carabine, negative theology has become a hot topic since the 1990s, resulting from a broad effort in the 19 and 20th century to portray Plato as a mysticist, which revived the interest in Neoplatonism and negative theology.\n\nKaren Armstrong, in her book \"The Case for God\" (2009), notices a recovery of apophatic theology in postmodern theology.\n\nThe Arabic term for \"negative theology\" is \"lahoot salbi\", which is a \"system of theology\" or \"nizaam al lahoot\" in Arabic. Different traditions/doctrine schools in Islam called Kalam schools (see Islamic schools and branches) use different theological approaches or \"nizaam al lahoot\" in approaching God in Islam (\"Allah\", Arabic الله) or the ultimate reality. The \"lahoot salbi\" or \"negative theology\" involves the use of \"ta'til\", which means \"negation,\" and the followers of the Mu'tazili school of Kalam, founded by Imam Wasil ibn Ata, are often called the \"Mu'attili\", because they are frequent users of the \"ta'tili\" methodology.\n\nRajab ʿAlī Tabrīzī, an Iranian and Shiat philosopher and mystic of the 17th century. instilled a radical apophatic theology in a generation of philosophers and theologians whose influence extended into the Qajar period. Mulla Rajab affirmed the completely unknowable,\nunqualifiable, and attributeless nature of God and upheld a general view concerning God’s attributes which can only be negatively ‘affirmed’, by means of the via\nnegativa.\n\nShia Islam adopted \"negative theology\". In the words of the Persian Ismaili missionary, Abu Yaqub al-Sijistani: \"There does not exist a tanzíh [\"transcendence\"] more brilliant and more splendid than that by which we establish the absolute transcendence of our Originator through the use of these phrases in which a negative and a negative of a negative apply to the thing denied.\" Early Sunni scholars who held to a literal reading of the Quran and hadith rejected this view, adhering to its opposite, believing that the Attributes of God such as \"Hand\", \"Foot\" etc... should be taken literally and that, therefore, God is like a human being. Today, most Sunnis, like the Ash'ari and Maturidi, adhere to a middle path between negation and anthropomorphism.\n\nMaimonides (1135/1138-1204) was \"the most influential medieval Jewish exponent of the \"via negativa\".\" Maimonides, but also Samuel ibn Tibbon, draw on Bahya ibn Paquda, who shows that our inability to describe God is related to the fact of His absolute unity. God, as the entity which is \"truly One\" (האחד האמת), must be free of properties and is thus unlike anything else and indescribable. According to Rabbi Yosef Wineberg, Maimonides stated that \"[God] is knowledge,\" and saw His Essence, Being and knowledge as completely one, \"a perfect unity and not a composite at all.\" Wineberg quotes Maimonides as stating\nIn \"The Guide for the Perplexed\" Maimonides stated:\nAccording to Fagenblat, it is only in the modern period that negative theology really gains importance in Jewish thought. Yeshayahu Leibowitz (1903-1994) was a prominent modern exponent of Jewish negative theology. According to Leibowitz, a person's faith is his commitment to obey God, meaning God's commandments, and this has nothing to do with a person’s image of God. This must be so because Leibowitz thought that God cannot be described, that God's understanding is not man's understanding, and thus all the questions asked of God are out of place.\n\nThere are interesting parallels in Indian thought, which developed largely separate from Western thought. Early Indian philosophical works which have apophatic themes include the Principal Upanishads (800 BCE to the start of common era) and the Brahma Sutras (from 450 BCE and 200 CE). An expression of negative theology is found in the Brihadaranyaka Upanishad, where Brahman is described as \"neti neti\" or \"neither this, nor that\". Further use of apophatic theology is found in the Brahma Sutras, which state:\n\nBuddhist philosophy has also strongly advocated the way of negation, beginning with the Buddha's own theory of anatta (not-atman, not-self) which denies any truly existent and unchanging essence of a person. Madhyamaka is a Buddhist philosophical school founded by Nagarjuna (2nd-3rd century CE), which is based on a fourfold negation of all assertions and concepts and promotes the theory of emptiness (shunyata). Apophatic assertions are also an important feature of Mahayana sutras, especially the prajñaparamita genre. These currents of negative theology are visible in all forms of Buddhism.\n\nApophatic movements in medieval Hindu philosophy are visible in the works of Shankara (8th century), a philosopher of Advaita Vedanta (non-dualism), and Bhartṛhari (5th century), a grammarian. While Shankara holds that the transcendent noumenon, Brahman, is realized by the means of negation of every phenomenon including language, Bhartṛhari theorizes that language has both phenomenal and noumenal dimensions, the latter of which manifests Brahman.\n\nIn Advaita, Brahman is defined as being Nirguna or without qualities. Anything imaginable or conceivable is not deemed to be the ultimate reality. The Taittiriya hymn speaks of Brahman as \"one where the mind does not reach\". Yet the Hindu scriptures often speak of Brahman's positive aspect. For instance, Brahman is often equated with bliss. These contradictory descriptions of Brahman are used to show that the attributes of Brahman are similar to ones experienced by mortals, but not the same. \n\nNegative theology also figures in the Buddhist and Hindu polemics. The arguments go something like this – Is Brahman an object of experience? If so, how do you convey this experience to others who have not had a similar experience? The only way possible is to relate this unique experience to common experiences while explicitly negating their sameness.\n\nEven though the \"via negativa\" essentially rejects theological understanding in and of itself as a path to God, some have sought to make it into an intellectual exercise, by describing God only in terms of what God is not. One problem noted with this approach is that there seems to be no fixed basis on deciding what God is not, unless the Divine is understood as an abstract experience of full aliveness unique to each individual consciousness, and universally, the perfect goodness applicable to the whole field of reality. Apophatic theology is often accused of being a version of atheism or agnosticism, since it cannot say truly that God exists. \"The comparison is crude, however, for conventional atheism treats the existence of God as a predicate that can be denied (“God is nonexistent”), whereas negative theology denies that God has predicates\". \"God or the Divine is\" without being able to attribute qualities about \"what He is\" would be the prerequisite of positive theology in negative theology that distinguishes theism from atheism. \"Negative theology is a complement to, not the enemy of, positive theology\". Since religious experience—or consciousness of the holy or sacred, is not reducible to other kinds of human experience, an abstract understanding of religious experience cannot be used as evidence or proof that religious discourse or praxis can have no meaning or value. In apophatic theology, the negation of theisms in the \"via negativa\" also requires the negation of their correlative atheisms if the dialectical method it employs is to maintain integrity.\n\n\n\n\n\n\n\n\n\n \n\n\n"}
{"id": "33944016", "url": "https://en.wikipedia.org/wiki?curid=33944016", "title": "Canon (basic principle)", "text": "Canon (basic principle)\n\nThe concept of canon is very broad; in a general sense it refers to being a rule or a body of rules.\n\nThere are definitions that state it as: “the body of rules, principles, or standards accepted as axiomatic and universally binding in a field of study or art”. This can be related to such topics as literary canons or the canons of rhetoric, which is a topic within itself that describes the rules of giving a speech. There are five key principles, and when grouped together, are the principles set for giving speeches as seen with regard to Rhetoric. This is one such example of how the term canon is used in regard to rhetoric.\n\n"}
{"id": "31799202", "url": "https://en.wikipedia.org/wiki?curid=31799202", "title": "Comstock–Needham system", "text": "Comstock–Needham system\n\nThe Comstock–Needham system is a naming system for insect wing veins, devised by John Comstock and George Needham in 1898. It was an important step in showing the homology of all insect wings. This system was based on Needham's \"pretracheation theory\" that was later discredited by Frederic Charles Fraser in 1938.\n\nThe Comstock and Needham system attributes different names to the veins on an insect's wing. From the anterior (leading) edge of the wing towards the posterior (rear), the major longitudinal veins are named:\n\nApart from the costal and the anal veins, each vein can be branched, in which case the branches are numbered from anterior to posterior. For example, the two branches of the subcostal vein will be called Sc and Sc.\n\nThe radius typically branches once near the base, producing anteriorly the R and posteriorly the \"radial sector\" Rs. The radial sector may fork twice.\n\nThe media may also fork twice, therefore having four branches reaching the wing margin.\n\nAccording to the Comstock–Needham system, the cubitus forks once, producing the cubital veins Cu and Cu. \nAccording to some other authorities, Cu may fork again, producing the Cu and Cu.\n\nAs there are several anal veins, they are called A1, A2, and so on. They are usually unforked.\n\nCrossveins link the longitudinal veins, and are named accordingly (for example, the medio-cubital crossvein is termed m-cu). Some crossveins have their own name, like the humeral crossvein h and the sectoral crossvein s.\n\nThe cells are named after the vein on the anterior side; for instance, the cell between Sc and R is called Sc.\n\nIn the case where two cells are separated by a crossvein but have the same anterior longitudinal vein, they should have the same name. To avoid this, they are attributed a number. For example, the R cell is divided in two by the radial cross vein: the basal cell is termed \"first R\", and the distal cell \"second R\".\n\nIf a cell is bordered anteriorly by a forking vein, such as R and R, the cell is named after the distal vein, in this case R.\n\n"}
{"id": "6978", "url": "https://en.wikipedia.org/wiki?curid=6978", "title": "Concept", "text": "Concept\n\nConcepts are mental representations, abstract objects or abilities that make up the fundamental building blocks of thoughts and beliefs. They play an important role in all aspects of cognition.\n\nIn contemporary philosophy, there are at least three prevailing ways to understand what a concept is:\n\n\nConcepts can be organized into a hierarchy, higher levels of which are termed \"superordinate\" and lower levels termed \"subordinate\". Additionally, there is the \"basic\" or \"middle\" level at which people will most readily categorize a concept. For example, a basic-level concept would be \"chair\", with its superordinate, \"furniture\", and its subordinate, \"easy chair\".\n\nA concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.\n\nConcepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word \"concept\" often just means any idea.\n\nWithin the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called \"mental representations\" (colloquially understood as \"ideas in the mind\"). Mental representations, in turn, are the building blocks of what are called \"propositional attitudes\" (colloquially understood as the stances or perspectives we take towards ideas, be it \"believing\", \"doubting\", \"wondering\", \"accepting\", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.\n\nA central question in the study of concepts is the question of what concepts \"are\". Philosophers construe this question as one about the ontology of concepts – what they are really like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.\n\nPlatonist views of the mind construe concepts as abstract objects,\n\nThere is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept \"dog\" is philosophically distinct from the things in the world grouped by this concept – or the reference class or extension. Concepts that can be equated to a single word are called \"lexical concepts\".\n\nStudy of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.\n\nIn the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word \"moon\" (a concept) is not the large, bright, shape-changing object up in the sky, but only \"represents\" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.\n\nKant declared that human minds possess pure or \"a priori\" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things \"in general\", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an \"a priori\" concept can relate to individual phenomena, in a manner analogous to an \"a posteriori\" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction \"a posteriori concepts\" (meaning concepts that arise out of experience). An empirical or an \"a posteriori\" concept is a general representation (\"Vorstellung\") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)\n\nA concept is a common feature or characteristic. Kant investigated the way that empirical \"a posteriori\" concepts are created.\nIn cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or \"recollections\", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.\n\nPlato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.\n\nGottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7).\n\nAccording to Carl Benjamin Boyer, in the introduction to his \"The History of the Calculus and its Conceptual Development\", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.\n\nIn a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.\n\nConcepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. (\"Sort\" is itself another word for concept, and \"sorting\" thus means to organise into concepts.)\n\nThe classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both \"necessary\" and \"sufficient\" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example \"bachelor\" is said to be defined by \"unmarried\" and \"man\". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the \"law of the excluded middle\", which means that there are no partial members of a class, you are either in or out.\n\nThe classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy – concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic \"Time Without Change\" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.\n\nGiven that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:\n\nPrototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as \"family resemblances\". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member – the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. According to Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain, some of these are; visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.\n\nTheory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory–Theory of concepts is responding to some of the issues of prototype theory and classic theory.\n\nAccording to the theory of ideasthesia (or \"sensing concepts\"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.\n\nThere is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.\n\nThe term \"concept\" is traced back to 1554–60 (Latin \"\" – \"something conceived\").\n\n"}
{"id": "698226", "url": "https://en.wikipedia.org/wiki?curid=698226", "title": "Concept map", "text": "Concept map\n\nA concept map or conceptual diagram is a diagram that depicts suggested relationships between concepts. It is a graphical tool that instructional designers, engineers, technical writers, and others use to organize and structure knowledge.\n\nA concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as \"causes\", \"requires\", or \"contributes to\".\n\nThe technique for visualizing these relationships among different concepts is called \"concept mapping\". Concept maps have been used to define the ontology of computer systems, for example with the object-role modeling or Unified Modeling Language formalism.\n\nA concept map is a way of representing relationships between ideas, images, or words in the same way that a sentence diagram represents the grammar of a sentence, a road map represents the locations of highways and towns, and a circuit diagram represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.\n\nConcept maps were developed to enhance meaningful learning in the sciences. A well-made concept map grows within a \"context frame\" defined by an explicit \"focus question\", while a mind map often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on declarative memory content, which is also referred to as chunks or propositions. Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.\n\n\nConcept mapping was developed by Joseph D. Novak and his research team at Cornell University in the 1970s as a means of representing the emerging science knowledge of students. It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin in the learning movement called constructivism. In particular, constructivists hold that learners actively construct knowledge.\n\nNovak's work is based on the cognitive theories of David Ausubel, who stressed the importance of prior knowledge in being able to learn (or \"assimilate\") new concepts: \"The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly.\" Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as \"What is water?\" \"What causes the seasons?\" In his book \"Learning How to Learn\", Novak states that a \"meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures.\"\n\nVarious attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of \"off-loading\". In this 1998 paper, McAleese draws on the work of Sowa and a paper by Sweller & Chandler. In essence, McAleese suggests that the process of making knowledge explicit, using \"nodes\" and \"relationships\", allows the individual to become aware of what they know and as a result to be able to modify what they know. Maria Birbili applies that same idea to helping young children learn to think about what they know. The concept of the \"knowledge arena\" is suggestive of a virtual space where learners may explore what they know and what they do not know.\n\nConcept maps are used to stimulate the generation of ideas, and are believed to aid creativity. Concept mapping is also sometimes used for brain-storming. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.\n\nFormalized concept maps are used in software design, where a common usage is Unified Modeling Language diagramming amongst similar conventions and development methodologies.\n\nConcept mapping can also be seen as a first step in ontology-building, and can also be used flexibly to represent formal argument — similar to argument maps.\n\nConcept maps are widely used in education and business. Uses include:\n\n"}
{"id": "178942", "url": "https://en.wikipedia.org/wiki?curid=178942", "title": "Conceptual art", "text": "Conceptual art\n\nConceptual art, sometimes simply called conceptualism, is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic, technical, and material concerns. Some works of conceptual art, sometimes called installations, may be constructed by anyone simply by following a set of written instructions. This method was fundamental to American artist Sol LeWitt's definition of Conceptual art, one of the first to appear in print:\n\nTony Godfrey, author of \"Conceptual Art (Art & Ideas)\" (1998), asserts that conceptual art questions the nature of art, a notion that Joseph Kosuth elevated to a definition of art itself in his seminal, early manifesto of conceptual art, \"Art after Philosophy\" (1969). The notion that art should examine its own nature was already a potent aspect of the influential art critic Clement Greenberg's vision of Modern art during the 1950s. With the emergence of an exclusively language-based art in the 1960s, however, conceptual artists such as Art & Language, Joseph Kosuth (who became the american editor of Art-Language), and Lawrence Weiner began a far more radical interrogation of art than was previously possible (see below). One of the first and most important things they questioned was the common assumption that the role of the artist was to create special kinds of material objects.\nThrough its association with the Young British Artists and the Turner Prize during the 1990s, in popular usage, particularly in the UK, \"conceptual art\" came to denote all contemporary art that does not practice the traditional skills of painting and sculpture. It could be said that one of the reasons why the term \"conceptual art\" has come to be associated with various contemporary practices far removed from its original aims and forms lies in the problem of defining the term itself. As the artist Mel Bochner suggested as early as 1970, in explaining why he does not like the epithet \"conceptual\", it is not always entirely clear what \"concept\" refers to, and it runs the risk of being confused with \"intention\". Thus, in describing or defining a work of art as conceptual it is important not to confuse what is referred to as \"conceptual\" with an artist's \"intention\".\n\nThe French artist Marcel Duchamp paved the way for the conceptualists, providing them with examples of prototypically conceptual works — the readymades, for instance. The most famous of Duchamp's readymades was \"Fountain\" (1917), a standard urinal-basin signed by the artist with the pseudonym \"R.Mutt\", and submitted for inclusion in the annual, un-juried exhibition of the Society of Independent Artists in New York (which rejected it). The artistic tradition does not see a commonplace object (such as a urinal) as art because it is not made by an artist or with any intention of being art, nor is it unique or hand-crafted. Duchamp's relevance and theoretical importance for future \"conceptualists\" was later acknowledged by US artist Joseph Kosuth in his 1969 essay, \"Art after Philosophy\", when he wrote: \"All art (after Duchamp) is conceptual (in nature) because art only exists conceptually\".\n\nIn 1956 the founder of Lettrism, Isidore Isou, developed the notion of a work of art which, by its very nature, could never be created in reality, but which could nevertheless provide aesthetic rewards by being contemplated intellectually. This concept, also called \"Art esthapériste\" (or \"infinite-aesthetics\"), derived from the infinitesimals of Gottfried Wilhelm Leibniz – quantities which could not actually exist except conceptually. The current incarnation () of the Isouian movement, Excoördism, self-defines as the art of the infinitely large and the infinitely small.\n\nIn 1961 the term \"concept art\", coined by the artist Henry Flynt in his article bearing the term as its title, appeared in a proto-Fluxus publication \"An Anthology of Chance Operations\". \nHowever, it assumed a different meaning when employed by Joseph Kosuth and by the English Art and Language group, who discarded the conventional art object in favour of a documented critical inquiry, that began in Art-Language The Journal of conceptual art in 1969, into the artist's social , philosophical and psychological status. By the mid-1970s they had produced publications, indices, performances, texts and paintings to this end. In 1970 \"Conceptual Art and Conceptual Aspects\", the first dedicated conceptual-art exhibition, took place at the New York Cultural Center.\n\nConceptual art emerged as a movement during the 1960s – in part as a reaction against formalism as then articulated by the influential New York art critic Clement Greenberg. According to Greenberg Modern art followed a process of progressive reduction and refinement toward the goal of defining the essential, formal nature of each medium. Those elements that ran counter to this nature were to be reduced. The task of painting, for example, was to define precisely what kind of object a painting truly is: what makes it a painting and nothing else. As it is of the nature of paintings to be flat objects with canvas surfaces onto which colored pigment is applied, such things as figuration, 3-D perspective illusion and references to external subject matter were all found to be extraneous to the essence of painting, and ought to be removed.\n\nSome have argued that conceptual art continued this \"dematerialization\" of art by removing the need for objects altogether,\nwhile others, including many of the artists themselves, saw conceptual art as a radical break with Greenberg's kind of formalist Modernism. Later artists continued to share a preference for art to be self-critical, as well as a distaste for illusion. However, by the end of the 1960s it was certainly clear that Greenberg's stipulations for art to continue within the confines of each medium and to exclude external subject matter no longer held traction.\nConceptual art also reacted against the commodification of art; it attempted a subversion of the gallery or museum as the location and determiner of art, and the art market as the owner and distributor of art. Lawrence Weiner said: \"Once you know about a work of mine you own it. There's no way I can climb inside somebody's head and remove it.\" Many conceptual artists' work can therefore only be known about through documentation which is manifested by it, e.g. photographs, written texts or displayed objects, which some might argue are not in themselves the art. It is sometimes (as in the work of Robert Barry, Yoko Ono, and Weiner himself) reduced to a set of written instructions describing a work, but stopping short of actually making it—emphasising the idea as more important than the artifact. This reveals an explicit preference for the \"art\" side of the ostensible dichotomy between art and craft, where art, unlike craft, takes place within and engages historical discourse: for example, Ono's \"written instructions\" make more sense alongside other conceptual art of the time.\n\nLanguage was a central concern for the first wave of conceptual artists of the 1960s and early 1970s. Although the utilisation of text in art was in no way novel, only in the 1960s did the artists Lawrence Weiner, Edward Ruscha, Joseph Kosuth, Robert Barry, and Art & Language begin to produce art by exclusively linguistic means. Where previously language was presented as one kind of visual element alongside others, and subordinate to an overarching composition (e.g. Synthetic Cubism), the conceptual artists used language in place of brush and canvas, and allowed it to signify in its own right. Of Lawrence Weiner's works Anne Rorimer writes, \"The thematic content of individual works derives solely from the import of the language employed, while presentational means and contextual placement play crucial, yet separate, roles.\"\n\nThe British philosopher and theorist of conceptual art Peter Osborne suggests that among the many factors that influenced the gravitation toward language-based art, a central role for conceptualism came from the turn to linguistic theories of meaning in both Anglo-American analytic philosophy, and structuralist and post structuralist Continental philosophy during the middle of the twentieth century. This linguistic turn \"reinforced and legitimized\" the direction the conceptual artists took. Osborne also notes that the early conceptualists were the first generation of artists to complete degree-based university training in art. Osborne later made the observation that contemporary art is \"post-conceptual\" in a public lecture delivered at the Fondazione Antonio Ratti, Villa Sucota in Como on July 9, 2010. It is a claim made at the level of the ontology of the work of art (rather than say at the descriptive level of style or movement).\n\nThe American art historian Edward A. Shanken points to the example of Roy Ascott who \"powerfully demonstrates the significant intersections between conceptual art and art-and-technology, exploding the conventional autonomy of these art-historical categories.\" Ascott, the British artist most closely associated with cybernetic art in England, was not included in Cybernetic Serendipity because his use of cybernetics was primarily conceptual and did not explicitly utilize technology. Conversely, although his essay on the application of cybernetics to art and art pedagogy, \"The Construction of Change\" (1964), was quoted on the dedication page (to Sol Lewitt) of Lucy R. Lippard's seminal \"Six Years: The Dematerialization of the Art Object from 1966 to 1972\", Ascott's anticipation of and contribution to the formation of conceptual art in Britain has received scant recognition, perhaps (and ironically) because his work was too closely allied with art-and-technology. Another vital intersection was explored in Ascott's use of the thesaurus in 1963 , which drew an explicit parallel between the taxonomic qualities of verbal and visual languages – a concept would be taken up in Joseph Kosuth's \"Second Investigation, Proposition 1\" (1968) and Mel Ramsden's \"Elements of an Incomplete Map\" (1968).\n\n\"By adopting language as their exclusive medium, Weiner, Barry, Wilson, Kosuth and Art & Language were able to sweep aside the vestiges of authorial presence manifested by formal invention and the handling of materials.\"\nAn important difference between conceptual art and more \"traditional\" forms of art-making goes to the question of artistic skill. Although skill in the handling of traditional media often plays little role in conceptual art, it is difficult to argue that no skill is required to make conceptual works, or that skill is always absent from them. John Baldessari, for instance, has presented realist pictures that he commissioned professional sign-writers to paint; and many conceptual performance artists (e.g. Stelarc, Marina Abramović) are technically accomplished performers and skilled manipulators of their own bodies. It is thus not so much an absence of skill or hostility toward tradition that defines conceptual art as an evident disregard for conventional, modern notions of authorial presence and of individual artistic expression.\n\nThe first wave of the \"conceptual art\" movement extended from approximately 1967 to 1978. Early \"concept\" artists like Henry Flynt, Robert Morris, and Ray Johnson influenced the later, widely accepted movement of conceptual art. Conceptual artists like Dan Graham, Hans Haacke, and Lawrence Weiner have proven very influential on subsequent artists, and well known contemporary artists such as Mike Kelley or Tracey Emin are sometimes labeled \"second- or third-generation\" conceptualists, or \"post-conceptual\" artists.\n\nMany of the concerns of the conceptual art movement have been taken up by contemporary artists. While they may or may not term themselves \"conceptual artists\", ideas such as anti-commodification, social and/or political critique, and ideas/information as medium continue to be aspects of contemporary art, especially among artists working with installation art, performance art, net.art and electronic/digital art.\n\n\n\nBooks\n\n\nEssays\n\n\nExhibition catalogues\n\n\n"}
{"id": "33346439", "url": "https://en.wikipedia.org/wiki?curid=33346439", "title": "Conceptual design", "text": "Conceptual design\n\nConceptual Design is an early phase of the design process, in which the broad outlines of function and form of something are articulated. It includes the design of interactions, experiences, processes and strategies. It involves an understanding of people's needs - and how to meet them with products, services, & processes. Common artifacts of conceptual design are concept sketches and models.\n\n"}
{"id": "555650", "url": "https://en.wikipedia.org/wiki?curid=555650", "title": "Conceptual framework", "text": "Conceptual framework\n\nA conceptual framework is an analytical tool with several variations and contexts. It can be applied in different categories of work where an overall picture is needed. It is used to make conceptual distinctions and organize ideas. Strong conceptual frameworks capture something real and do this in a way that is easy to remember and apply.\n\nIsaiah Berlin used the metaphor of a \"fox\" and a \"hedgehog\" to make conceptual distinctions in how important philosophers and authors view the world. Berlin describes hedgehogs as those who use a single idea or organizing principle to view the world (such as Dante Alighieri, Blaise Pascal, Fyodor Dostoyevsky, Plato, Henrik Ibsen and Georg Wilhelm Friedrich Hegel). Foxes, on the other hand, incorporate a type of pluralism and view the world through multiple, sometimes conflicting, lenses (examples include Johann Wolfgang von Goethe, James Joyce, William Shakespeare, Aristotle, Herodotus, Molière, and Honoré de Balzac).\n\nEconomists use the conceptual framework of \"supply\" and \"demand\" to distinguish between the behavior and incentive systems of firms and consumers. Like many conceptual frameworks, supply and demand can be presented through visual or graphical representations (see demand curve). Both political Science and economics use principal agent theory as a conceptual framework. The politics-administration dichotomy is a long standing conceptual framework used in public administration. All three of these cases are examples of a macro level conceptual framework.\n\nThe use of the term \"conceptual framework\" crosses both scale (large and small theories) and contexts (social science, marketing, applied science, art etc.). Its explicit definition and application can therefore vary.\n\nConceptual frameworks are particularly useful as organizing devices in empirical research. One set of scholars has applied the notion of conceptual framework to deductive, empirical research at the micro- or individual study level. They employ American football plays as a useful metaphor to clarify the meaning of \"conceptual framework\" (used in the context of a deductive empirical study).\n\nLikewise, conceptual frameworks are abstract representations, connected to the research project's goal that direct the collection and analysis of data (on the plane of observation – the ground). Critically, a football play is a \"plan of action\" tied to a particular, timely, purpose, usually summarized as long or short yardage. Shields and Rangarajan (2013) argue that it is this tie to \"purpose\" that make American football plays such a good metaphor. They define a conceptual framework as \"the way ideas are organized to achieve a research project's purpose\". Like football plays, conceptual frameworks are connected to a research purpose or aim. Explanation is the most common type of research purpose employed in empirical research. The formal hypothesis of a scientific investigation is the framework associated with explanation.\n\nExplanatory research usually focuses on \"why\" or \"what caused\" a phenomenon to occur. Formal hypotheses posit possible explanations (answers to the why question) that are tested by collecting data and assessing the evidence (usually quantitative using statistical tests). For example, Kai Huang wanted to determine what factors contributed to residential fires in U.S. cities. Three factors were posited to influence residential fires. These factors (environment, population and building characteristics) became the hypotheses or conceptual framework he used to achieve his purpose – explain factors that influenced home fires in U.S. cities.\n\nSeveral types of conceptual frameworks have been identified, and line up with a research purpose in the following ways:\n\nNote that Shields and Rangarajan (2013) do not claim that the above are the only framework-purpose pairing. Nor do they claim the system is applicable to inductive forms of empirical research. Rather, the conceptual framework-research purpose pairings they propose are useful and provide new scholars a point of departure to develop their own research design.\n\nFrameworks have also been used to explain conflict theory and the balance necessary to reach what amounts to resolution. Within these conflict frameworks, visible and invisible variables function under concepts of relevance. Boundaries form and within these boundaries, tensions regarding laws and chaos (or freedom) are mitigated. These frameworks often function like cells, with sub-frameworks, stasis, evolution and revolution. Anomalies may exist without adequate \"lenses\" or \"filters\" to see them and may become visible only when the tools exist to define them.\n\n\n"}
{"id": "21899301", "url": "https://en.wikipedia.org/wiki?curid=21899301", "title": "Disquotational principle", "text": "Disquotational principle\n\nThe disquotational principle is a philosophical theorem which holds that a rational speaker will accept \"p\" if and only if he or she believes \"p\". The quotes indicate that the statement \"p\" is being treated as a sentence, and not as a proposition. This principle is presupposed by claims that hold that substitution fails in certain intensional contexts.\n\nConsider the following argument:\n\nTo derive (3), we have to assume that when Sally accepts that \"Cicero was a famous orator\", she believes that Cicero was a famous orator. Then we can exchange Cicero for Tully, and derive (3). Bertrand Russell thought that this demonstrated the failure of substitutivity of identicals in intensional contexts.\n\nIn \"A Puzzle about Belief,\" Saul Kripke argues that the application of the disquotational theorem can yield a paradox on its own, without appeal to the substitution principle, and that this may show that the problem lies with the former, and not the latter. There are various formulations of this argument.\n\nSuppose that, Pierre, a Frenchman, comes to believe that (1) \"Londres est jolie\" (London is pretty), without ever having visited the city. Later in life, Pierre ends up living in London. He finds no French speakers there (he does not speak English yet), and everyone refers to the city as \"London,\" not \"Londres\". He finds this city decidedly unattractive, for the neighborhood he decides to live in is decidedly unattractive. Over time, he learns English, and formulates the belief that (2) \"London is not pretty\". Pierre never realizes that London is the English word for \"Londres\". Now with the disquotational principle, we can deduce from (1) that Pierre believes the proposition that \"Londres est jolie\". With a weak principle of translation (e.g., \"a proposition in language A is the same as a semantically identical proposition in language B\" [note that a proposition is not the same as a sentence]), we can now deduce that Pierre believes that London is pretty. But we can also deduce from (2) and the disquotational principle that Pierre believes that London is not pretty. These deductions can be made \"even though Pierre has made no logical blunders in forming his beliefs\". Without the disquotational principle, this contradiction could not be derived, because we would not be able to assume that (1) and (2) meant anything in particular.\n\nThis paradox can also be derived without appeal to another language. Suppose that Pierre assents to the proposition that \"Paderewski had musical talent\", perhaps having heard that this man was a famous pianist. With the disquotational principle, we can deduce that Pierre believes the proposition that Paderewski had musical talent. Now suppose that Pierre overhears a friend discussing the political exploits of a certain statesman, Paderewski, without knowing that the two Paderewskis are the same man. Pierre's background tells him that statesmen are generally not very gifted in music, and this leads him to the belief that Paderewski had no musical talent. The disquotation principle allows us to deduce that Pierre believes the proposition that Paderewski had no musical talent. Using this principle, we have now deduced that Pierre believes that Paderewski had musical talent, and does not believe that Paderewski had musical talent, \"even though Pierre's beliefs were formed logically\".\n\n"}
{"id": "16312085", "url": "https://en.wikipedia.org/wiki?curid=16312085", "title": "Hu Washizu principle", "text": "Hu Washizu principle\n\nIn continuum mechanics, and in particular in finite element analysis, the Hu-Washizu principle is a variational principle which says that the action\n\nis stationary, where formula_2 is the elastic stiffness tensor. The Hu-Washizu principle is used to develop mixed finite element methods. The principle is named after Hu Haichang and Kyuichiro Washizu.\n\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "26127533", "url": "https://en.wikipedia.org/wiki?curid=26127533", "title": "Marginal abatement cost", "text": "Marginal abatement cost\n\nAbatement cost is the cost of reducing environmental negatives such as pollution. Marginal cost is an economic concept that measures the cost of an additional unit. The marginal abatement cost (MAC), in general, measures the cost of reducing one more unit of pollution.\n\nAlthough marginal abatement costs can be negative, such as when the low carbon option is cheaper than the business-as-usual option, MACs often rise steeply as more pollution is reduced.\n\nMarginal abatement costs are typically used on a marginal abatement cost curve (MACC) or MAC curve, which shows the marginal cost of additional reductions in pollution.\n\nCarbon traders use MAC curves to derive the supply function for modelling carbon price fundamentals. Power companies may employ MAC curves to guide their decisions about long-term capital investment strategies to select among a variety of efficiency and generation options. Economists have used MAC curves to explain the economics of interregional carbon trading. Policy-makers use MAC curves as merit order curves, to analyze how much abatement can be done in an economy at what cost, and where policy should be directed to achieve the emission reductions.\n\nHowever, MAC curves should not be used as abatement supply curves (or merit order curves) to decide which measures to implement in order to achieve a given emission-reduction target. Indeed, the options they list would take decades to implement, and it may be optimal to implement expensive but high-potential measures before introducing cheaper measures.\n\nThe way that MAC curves are usually built has been criticized for lack of transparency and the poor treatment it makes of uncertainty, inter-temporal dynamics, interactions between sectors and ancillary benefits. There is also concern regarding the biased ranking that occurs if some included options have negative costs. \n\nVarious economists, research organizations, and consultancies have produced MAC curves. Bloomberg New Energy Finance and McKinsey & Company have produced economy wide analyses on greenhouse gas emissions reductions for the United States. ICF International produced a California specific curve following AB-32 legislation as have Sweeney and Weyant.\n\nThe Wuppertal Institute for Climate, Environment and Energy produced several marginal abatement cost curves for Germany (also called Cost Potential Curves), depending on the perspective (end-user, utilities, society).\n\nThe US Environmental Protection Agency has done work on a MAC curve for non carbon dioxide emissions such as methane, NO, and HFCs. Enerdata and LEPII-CNRS (France) produce MAC curves with the Prospective Outlook on Long-term Energy Systems (POLES) model for the 6 Kyoto Protocol gases. These curves have been used for various public and private actors either to assess carbon policies or through the use of a carbon market analysis tool.\n\nThe World Bank 2013 low-carbon energy development plan for Nigeria, prepared jointly with the World Bank, ulitizes MAC curves created in Analytica.\n\n"}
{"id": "1148564", "url": "https://en.wikipedia.org/wiki?curid=1148564", "title": "Marginal concepts", "text": "Marginal concepts\n\nIn economics, marginal concepts are associated with a \"specific change\" in the quantity used of a good or service, as opposed to some notion of the over-all significance of that class of good or service, or of some total quantity thereof.\n\nConstraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual himself or herself.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change, as large as the smallest relevant division of that good or service. For reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. In such context, a marginal change may be an infinitesimal change or a limit. However, strictly speaking, the smallest relevant division may be quite large.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nThe marginal utility of a good or service is the utility of the specific use to which an agent would put a given increase in that good or service, or of the specific use that would be abandoned in response to a given decrease. In other words, marginal utility is the utility of the marginal use.\n\nThe marginal rate of substitution is the rate of substitution is the least favorable rate, at the margin, at which an agent is willing to exchange units of one good or service for units of another.\n\nA marginal benefit is a benefit (howsoever ranked or measured) associated with a marginal change.\n\nThe term “marginal cost” may refer to an opportunity cost at the margin, or to marginal \"pecuniary\" cost — that is to say marginal cost measured by forgone money.\n\nOther marginal concepts include (but are not limited to):\n\nMarginalism is the use of marginal concepts to explain economic phenomena.\n\nThe related concept of elasticity is the ratio of the incremental percentage change in one variable with respect to an incremental percentage change in another variable.\n"}
{"id": "21582679", "url": "https://en.wikipedia.org/wiki?curid=21582679", "title": "Marginal product of labor", "text": "Marginal product of labor\n\nIn economics, the marginal product of labor (MP) is the change in output that results from employing an added unit of labor.\n\nThe marginal product of a factor of production is generally defined as the change in output associated with a change in that factor, holding other inputs into production constant.\n\nThe marginal product of labor is then the change in output (\"Y\") per unit change in labor (\"L\"). In discrete terms the marginal product of labor is:\n\nIn continuous terms, the \"MP\" is the first derivative of the production function:\n\nGraphically, the \"MP\" is the slope of the production function.\n\nThere is a factory which produces toys. When there are no workers in the factory, no toys are produced. When there is one worker in the factory, six toys are produced per hour. When there are two workers in the factory, eleven toys are produced per hour. There is a marginal product of labor of five when there are two workers in the factory compared to one. When the marginal product of labor is increasing, this is called increasing marginal returns. However, as the number of workers increases, the marginal product of labor may not increase indefinitely. When not scaled properly, the marginal product of labor may go down when the number of employees goes up, creating a situation known as diminishing marginal returns. When the marginal product of labor becomes negative, it is known as negative marginal returns.\n\nThe marginal product of labor is directly related to costs of production. Costs are divided between fixed and variable costs. Fixed costs are costs that relate to the fixed input, capital, or \"rK\", where \"r\" is the rental cost of capital and \"K\" is the quantity of capital. Variable costs (VC) are the costs of the variable input, labor, or \"wL\", where \"w\" is the wage rate and \"L\" is the amount of labor employed. Thus, VC = wL . Marginal cost (MC) is the change in total cost per unit change in output or ∆C/∆Q. In the short run, production can be varied only by changing the variable input. Thus only variable costs change as output increases: ∆C = ∆VC = ∆(wL). Marginal cost is ∆(Lw)/∆Q. Now, ∆L/∆Q is the reciprocal of the marginal product of labor (∆Q/∆L). Therefore, marginal cost is simply the wage rate w divided by the marginal product of labor\n\nThus if the marginal product of labor is rising then marginal costs will be falling and if the marginal product of labor is falling marginal costs will be rising (assuming a constant wage rate).\n\nThe average product of labor is the total product of labor divided by the number of units of labor employed, or \"Q/L\". The average product of labor is a common measure of labor productivity. The AP curve is shaped like an inverted “u”. At low production levels the AP tends to increase as additional labor is added. The primary reason for the increase is specialization and division of labor. At the point the AP reaches its maximum value AP equals the MP. Beyond this point the AP falls.\n\nDuring the early stages of production MP is greater than AP. When the MP is above the AP the AP will increase. Eventually the \"MP\" reaches it maximum value at the point of diminishing returns. Beyond this point MP will decrease. However, at the point of diminishing returns the MP is still above the AP and AP will continue to increase until MP equals AP. When MP is below AP, AP will decrease.\n\nGraphically, the \"AP\" curve can be derived from the total product curve by drawing secants from the origin that intersect (cut) the total product curve. The slope of the secant line equals the average product of labor, where the slope = dQ/dL. The slope of the curve at each intersection marks a point on the average product curve. The slope increases until the line reaches a point of tangency with the total product curve. This point marks the maximum average product of labor. It also marks the point where MP (which is the slope of the total product curve) equals the AP (the slope of the secant). Beyond this point the slope of the secants become progressively smaller as \"AP\" declines. The MP curve intersects the AP curve from above at the maximum point of the AP curve. Thereafter, the MP curve is below the AP curve.\n\nThe falling MP is due to the law of diminishing marginal returns. The law states, \"as units of one input are added (with all other inputs held constant) a point will be reached where the resulting additions to output will begin to decrease; that is marginal product will decline.\" The law of diminishing marginal returns applies regardless of whether the production function exhibits increasing, decreasing or constant returns to scale. The key factor is that the variable input is being changed while all other factors of production are being held constant. Under such circumstances diminishing marginal returns are inevitable at some level of production.\n\nDiminishing marginal returns differs from diminishing returns. Diminishing marginal returns means that the marginal product of the variable input is falling. Diminishing returns occur when the marginal product of the variable input is negative. That is when a unit increase in the variable input causes total product to fall. At the point that diminishing returns begin the MP is zero.\n\nThe general rule is that a firm maximizes profit by producing that quantity of output where marginal revenue equals marginal costs. The profit maximization issue can also be approached from the input side. That is, what is the profit maximizing usage of the variable input? To maximize profits the firm should increase usage \"up to the point where the input’s marginal revenue product equals its marginal costs\". So, mathematically the profit maximizing rule is MRP = MC. The marginal profit per unit of labor equals the marginal revenue product of labor minus the marginal cost of labor or Mπ = MRP − MCA firm maximizes profits where Mπ = 0.\n\nThe marginal revenue product is the change in total revenue per unit change in the variable input assume labor. That is, MRP = ∆TR/∆L. MRP is the product of marginal revenue and the marginal product of labor or MRP = MR × MP.\n\n\n • formula_4\n\n • Output price is $40 per unit.\n\n\nIn the aftermath of the marginal revolution in economics, a number of economists including John Bates Clark and Thomas Nixon Carver sought to derive an ethical theory of income distribution based on the idea that workers were morally entitled to receive a wage exactly equal to their marginal product. In the 20th century, marginal productivity ethics found few supporters among economists, being criticised not only by egalitarians but by economists associated with the Chicago school such as Frank Knight (in \"The Ethics of Competition\") and the Austrian School, such as Leland Yeager. However, marginal productivity ethics were defended by George Stigler.\n\n\n"}
{"id": "2695183", "url": "https://en.wikipedia.org/wiki?curid=2695183", "title": "Marginal propensity to import", "text": "Marginal propensity to import\n\nThe marginal propensity to import (MPI) is the fractional change in import expenditure that occurs with a change in disposable income (income after taxes and transfers). For example, if a household earns one extra dollar of disposable income, and the marginal propensity to import is 0.2, then the household will spend 20 cents of that dollar on imported goods and services.\n\nMathematically, the marginal propensity to import (MPM) function is expressed as the derivative of the import (I) function with respect to disposable income (Y).formula_1In other words, the marginal propensity to import is measured as the ratio of the change in imports to the change in income, thus giving us a figure between 0 and 1. \n\nImports are also considered to be automatic stabilisers that work to lessen fluctuations in real GDP.\n\nThe UK government assumes that UK citizens have a high marginal propensity to import and thus will use a decrease in disposable income as a tool to control the current account on the balance of payments.\n\n"}
{"id": "17886679", "url": "https://en.wikipedia.org/wiki?curid=17886679", "title": "Marginal return", "text": "Marginal return\n\nMarginal return is the rate of return for a marginal increase in investment; roughly, this is the additional output resulting from a one-unit increase in the use of a variable input, while other inputs are constant.\n\n"}
{"id": "15628625", "url": "https://en.wikipedia.org/wiki?curid=15628625", "title": "Marginal use", "text": "Marginal use\n\nAs defined by the Austrian School of economics the marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease. The useful\"ness\" of the marginal use thus corresponds to the marginal utility of the good or service.\n\nOn the assumption that an agent is economically rational, each increase would be put to the specific, feasible, previously unrealized use of greatest priority, and each decrease would result in abandonment of the use of lowest priority amongst the uses to which the good or service had been put. And, in the absence of a complementarity across uses, the “law” of diminishing marginal utility will obtain.\n\nThe Austrian School of economics explicitly arrives at its conception of marginal utility as the utility of the marginal use, and “Grenznutzen” (the Austrian School term from which “marginal utility” was originally derived in translation) literally means \"border-use\"; other schools usually do not make an explicit connection.\n\n"}
{"id": "4718632", "url": "https://en.wikipedia.org/wiki?curid=4718632", "title": "Mental representation", "text": "Mental representation\n\nA mental representation (or cognitive representation), in philosophy of mind, cognitive psychology, neuroscience, and cognitive science, is a hypothetical internal cognitive symbol that represents external reality, or else a mental process that makes use of such a symbol: \"a formal system for making explicit certain entities or types of information, together with a specification of how the system does this\".\n\nMental representation is the mental imagery of things that are not actually present to the senses. In contemporary philosophy, specifically in fields of metaphysics such as philosophy of mind and ontology, a mental representation is one of the prevailing ways of explaining and describing the nature of ideas and concepts.\n\nMental representations (or mental imagery) enable representing things that have never been experienced as well as things that do not exist. Think of yourself traveling to a place you have never visited before, or having a third arm. These things have either never happened or are impossible and do not exist, yet our brain and mental imagery allows us to imagine them. Although visual imagery is more likely to be recalled, mental imagery may involve representations in any of the sensory modalities, such as hearing, smell, or taste. Stephen Kosslyn proposes that images are used to help solve certain types of problems. We are able to visualize the objects in question and mentally represent the images to solve it.\n\nMental representations also allow people to experience things right in front of them—though the process of how the brain interprets the representational content is debated.\n\nRepresentationalism (also known as indirect realism) is the view that representations are the main way we access external reality. Another major prevailing philosophical theory posits that concepts are entirely abstract objects.\n\nThe representational theory of mind attempts to explain the nature of ideas, concepts and other mental content in contemporary philosophy of mind, cognitive science and experimental psychology. In contrast to theories of naive or direct realism, the representational theory of mind postulates the actual existence of mental representations which act as intermediaries between the observing subject and the objects, processes or other entities observed in the external world. These intermediaries stand for or represent to the mind the objects of that world.\n\nFor example, when someone arrives at the belief that his or her floor needs sweeping, the representational theory of mind states that he or she forms a mental representation that represents the floor and its state of cleanliness.\n\nThe original or \"classical\" representational theory probably can be traced back to Thomas Hobbes and was a dominant theme in classical empiricism in general. According to this version of the theory, the mental representations were images (often called \"ideas\") of the objects or states of affairs represented. For modern adherents, such as Jerry Fodor, Steven Pinker and many others, the representational system consists rather of an internal language of thought (i.e., mentalese). The contents of thoughts are represented in symbolic structures (the formulas of Mentalese) which, analogously to natural languages but on a much more abstract level, possess a syntax and semantics very much like those of natural languages. For the Spanish logician and cognitive scientist Luis M. Augusto, at this abstract, formal level, the syntax of thought is the set of symbol rules (i.e., operations, processes, etc. on and with symbol structures) and the semantics of thought is the set of symbol structures (concepts and propositions). Content (i.e., thought) emerges from the meaningful co-occurrence of both sets of symbols. For instance, \"8 x 9\" is a meaningful co-occurrence, whereas \"CAT x §\" is not; \"x\" is a symbol rule called for by symbol structures such as \"8\" and \"9\", but not by \"CAT\" and \"§\".\n\nThere are two types of representationalism, strong and weak. Strong representationalism attempts to reduce phenomenal character to intentional content. On the other hand, weak representationalism claims only that phenomenal character supervenes on intentional content. Strong representationalism aims to provide a theory about the nature of phenomenal character, and offers a solution to the hard problem of consciousness. In contrast to this, weak representationalism does not aim to provide a theory of consciousness, nor does it offer a solution to the hard problem of consciousness.\n\nStrong representationalism can be further broken down into restricted and unrestricted versions. The restricted version deals only with certain kinds of phenomenal states e.g. visual perception. Most representationalists endorse an unrestricted version of representationalism. According to the unrestricted version, for any state with phenomenal character that state’s phenomenal character reduces to its intentional content. Only this unrestricted version of representationalism is able to provide a general theory about the nature of phenomenal character, as well as offer a potential solution to the hard problem of consciousness. The successful reduction of the phenomenal character of a state to its intentional content would provide a solution to the hard problem of consciousness once a physicalist account of intentionality is worked out.\n\nWhen arguing against the unrestricted version of representationalism people will often bring up phenomenal mental states that appear to lack intentional content. The unrestricted version seeks to account for all phenomenal states. Thus, for it to be true, all states with phenomenal character must have intentional content to which that character is reduced. Phenomenal states without intentional content therefore serve as a counterexample to the unrestricted version. If the state has no intentional content its phenomenal character will not be reducible to that state’s intentional content, for it has none to begin with.\n\nA common example of this kind of state are moods. Moods are states with phenomenal character that are generally thought to not be directed at anything in particular. Moods are thought to lack directedness, unlike emotions, which are typically thought to be directed at particular things e.g. you are mad \"at\" your sibling, you are afraid \"of\" a dangerous animal. People conclude that because moods are undirected they are also nonintentional i.e. they lack intentionality or aboutness. Because they are not directed at anything they are not about anything. Because they lack intentionality they will lack any intentional content. Lacking intentional content their phenomenal character will not be reducible to intentional content, refuting the representational doctrine.\n\nThough emotions are typically considered as having directedness and intentionality this idea has also been called into question. One might point to emotions a person all of a sudden experiences that do not appear to be directed at or about anything in particular. Emotions elicited by listening to music are another potential example of undirected, nonintentional emotions. Emotions aroused in this way do not seem to necessarily be about anything, including the music that arouses them.\n\nIn response to this objection a proponent of representationalism might reject the undirected nonintentionality of moods, and attempt to identify some intentional content they might plausibly be thought to possess. The proponent of representationalism might also reject the narrow conception of intentionality as being directed at a particular thing, arguing instead for a broader kind of intentionality.\n\nThere are three alternative kinds of directedness/intentionality one might posit for moods. \nIn the case of outward directedness moods might be directed at either the world as a whole, a changing series of objects in the world, or unbound emotion properties projected by people onto things in the world. In the case of inward directedness moods are directed at the overall state of a person’s body. In the case of hybrid directedness moods are directed at some combination of inward and outward things.\n\nEven if one can identify some possible intentional content for moods we might still question whether that content is able to sufficiently capture the phenomenal character of the mood states they are a part of. Amy Kind contends that in the case of all the previously mentioned kinds of directedness (outward, inward, and hybrid) the intentional content supplied to the mood state is not capable of sufficiently capturing the phenomenal aspects of the mood states. In the case of inward directedness, the phenomenology of the mood does not seem tied to the state of one’s body, and even if one’s mood is reflected by the overall state of one’s body that person will not necessarily be aware of it, demonstrating the insufficiency of the intentional content to adequately capture the phenomenal aspects of the mood. In the case of outward directedness, the phenomenology of the mood and its intentional content do not seem to share the corresponding relation they should given that the phenomenal character is supposed to reduce to the intentional content. Hybrid directedness, if it can even get off the ground, faces the same objection.\n\nThere is a wide debate on what kinds of representations exist. There are several philosophers who bring about different aspects of the debate. Such philosophers include Alex Morgan, Gualtiero Piccinini, and Uriah Kriegel—though this is not an exhaustive list.\n\nThere are \"job description\" representations. That is representations that (1) represent something—have intentionality, (2) have a special relation—the represented object does not need to exist, and (3) content plays a causal role in what gets represented: e.g. saying \"hello\" to a friend, giving a glare to an enemy.\n\nStructural representations are also important. These types of representations are basically mental maps that we have in our minds that correspond exactly to those objects in the world (the intentional content). According to Morgan, structural representations are not the same as mental representations—there is nothing mental about them: plants can have structural representations.\n\nThere are also internal representations. These types of representations include those that involve future decisions, episodic memories, or any type of projection into the future.\n\nIn Gualtiero Piccinini's forthcoming work, he discusses topics on natural and nonnatural mental representations. He relies on the natural definition of mental representations given by Grice (1957) where \"P entails that P\". e.g. Those spots mean measles, entails that the patient has measles. Then there are nonnatural representations: \"P does not entail P\". e.g. The 3 rings on the bell of a bus mean the bus is full—the rings on the bell are independent of the fullness of the bus—we could have assigned something else (just as arbitrary) to signify that the bus is full.\n\nThere are also objective and subjective mental representations. Objective representations are closest to tracking theories—where the brain simply tracks what is in the environment. If there is a blue bird outside my window, the objective representation is that of the blue bird. Subjective representations can vary person-to-person. For example, if I am colorblind, that blue bird outside my window will not \"appear\" blue to me since I cannot represent the blueness of blue (i.e. I cannot see the color blue). The relationship between these two types of representation can vary.\n\nEliminativists think that subjective representations don't exist. Reductivists think subjective representations are reducible to objective. Non-reductivists think that subjective representations are real and distinct.\n\n\n"}
{"id": "4602393", "url": "https://en.wikipedia.org/wiki?curid=4602393", "title": "Models of scientific inquiry", "text": "Models of scientific inquiry\n\nIn the philosophy of science, models of scientific inquiry have two functions: first, to provide a descriptive account of \"how\" scientific inquiry is carried out in practice, and second, to provide an explanatory account of \"why\" scientific inquiry succeeds as well as it appears to do in arriving at genuine knowledge.\n\nThe search for scientific knowledge ends far back into antiquity. At some point in the past, at least by the time of Aristotle, philosophers recognized that a fundamental distinction should be drawn between two kinds of scientific knowledge—roughly, knowledge \"that\" and knowledge \"why\". It is one thing to know \"that\" each planet periodically reverses the direction of its motion with respect to the background of fixed stars; it is quite a different matter to know \"why\". Knowledge of the former type is descriptive; knowledge of the latter type is explanatory. It is explanatory knowledge that provides scientific understanding of the world. (Salmon, 2006, pg. 3)\n\n\"Scientific inquiry refers to the diverse ways in which scientists study the natural world and propose explanations based on the evidence derived from their work.\"\n\nThe classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.\n\nWesley Salmon (1989) began his historical survey of scientific explanation with what he called the \"received view\", as it was received from Hempel and Oppenheim in the years beginning with their \"Studies in the Logic of Explanation\" (1948) and culminating in Hempel's \"Aspects of Scientific Explanation\" (1965). Salmon summed up his analysis of these developments by means of the following Table.\n\nIn this classification, a deductive-nomological (D-N) explanation of an occurrence is a valid deduction whose conclusion states that the outcome to be explained did in fact occur. The deductive argument is called an \"explanation\", its premisses are called the \"explanans\" (L: \"explaining\") and the conclusion is called the \"explanandum\" (L: \"to be explained\"). Depending on a number of additional qualifications, an explanation may be ranked on a scale from \"potential\" to \"true\".\n\nNot all explanations in science are of the D-N type, however. An \"inductive-statistical\" (I-S) explanation accounts for an occurrence by subsuming it under statistical laws, rather than categorical or universal laws, and the mode of subsumption is itself inductive instead of deductive. The D-N type can be seen as a limiting case of the more general I-S type, the measure of certainty involved being complete, or probability 1, in the former case, whereas it is less than complete, probability < 1, in the latter case.\n\nIn this view, the D-N mode of reasoning, in addition to being used to explain particular occurrences, can also be used to explain general regularities, simply by deducing them from still more general laws.\n\nFinally, the \"deductive-statistical\" (D-S) type of explanation, properly regarded as a subclass of the D-N type, explains statistical regularities by deduction from more comprehensive statistical laws. (Salmon 1989, pp. 8–9).\n\nSuch was the \"received view\" of scientific explanation from the point of view of logical empiricism, that Salmon says \"held sway\" during the third quarter of the last century (Salmon, p. 10).\n\nDuring the course of history, one theory has succeeded another, and some have suggested further work while others have seemed content just to explain the phenomena. The reasons why one theory has replaced another are not always obvious or simple. The philosophy of science includes the question: \"What criteria are satisfied by a 'good' theory\". This question has a long history, and many scientists, as well as philosophers, have considered it. The objective is to be able to choose one theory as preferable to another without introducing cognitive bias. Several often proposed criteria were summarized by Colyvan. A good theory:\n\nStephen Hawking supports items 1–4, but does not mention fruitfulness. On the other hand, Kuhn emphasizes the importance of seminality.\n\nThe goal here is to make the choice between theories less arbitrary. Nonetheless, these criteria contain subjective elements, and are heuristics rather than part of scientific method. Also, criteria such as these do not necessarily decide between alternative theories. Quoting Bird:\nIt also is debatable whether existing scientific theories satisfy all these criteria, which may represent goals not yet achieved. For example, explanatory power over all existing observations (criterion 3) is satisfied by no one theory at the moment.\nThe desiderata of a \"good\" theory have been debated for centuries, going back perhaps even earlier than Occam's razor, which often is taken as an attribute of a good theory. Occam's razor might fall under the heading of \"elegance\", the first item on the list, but too zealous an application was cautioned by Albert Einstein: \"Everything should be made as simple as possible, but no simpler.\" It is arguable that \"parsimony\" and \"elegance\" \"typically pull in different directions\". The falsifiability item on the list is related to the criterion proposed by Popper as demarcating a scientific theory from a theory like astrology: both \"explain\" observations, but the scientific theory takes the risk of making predictions that decide whether it is right or wrong:\n\nThomas Kuhn argued that changes in scientists' views of reality not only contain subjective elements, but result from group dynamics, \"revolutions\" in scientific practice which result in paradigm shifts. As an example, Kuhn suggested that the heliocentric \"Copernican Revolution\" replaced the geocentric views of Ptolemy not because of empirical failures, but because of a new \"paradigm\" that exerted control over what scientists felt to be the more fruitful way to pursue their goals.\n\nDeductive logic and inductive logic are quite different in their approaches.\n\nDeductive logic is the reasoning of proof, or logical implication. It is the logic used in mathematics and other axiomatic systems such as formal logic. In a deductive system, there will be axioms (postulates) which are not proven. Indeed, they cannot be proven without circularity. There will also be primitive terms which are not defined, as they cannot be defined without circularity. For example, one can define a line as a set of points, but to then define a point as the intersection of two lines would be circular. Because of these interesting characteristics of formal systems, Bertrand Russell humorously referred to mathematics as \"the field where we don't know what we are talking about, nor whether or not what we say is true\". All theorems and corollaries are proven by exploring the implications of the axiomata and other theorems that have previously been developed. New terms are defined using the primitive terms and other derived definitions based on those primitive terms.\n\nIn a deductive system, one can correctly use the term \"proof\", as applying to a theorem. To say that a theorem is proven means that it is impossible for the axioms to be true and the theorem to be false. For example, we could do a simple syllogism such as the following:\n\n\nNotice that it is not possible (assuming all of the trivial qualifying criteria are supplied) to be in Arches and not be in Utah. However, one can be in Utah while not in Arches National Park. The implication only works in one direction. Statements (1) and (2) taken together imply statement (3). Statement (3) does not imply anything about statements (1) or (2). Notice that we have not proven statement (3), but we have shown that statements (1) and (2) together imply statement (3). In mathematics, what is proven is not the truth of a particular theorem, but that the axioms of the system imply the theorem. In other words, it is impossible for the axioms to be true and the theorem to be false. The strength of deductive systems is that they are sure of their results. The weakness is that they are abstract constructs which are, unfortunately, one step removed from the physical world. They are very useful, however, as mathematics has provided great insights into natural science by providing useful models of natural phenomena. One result is the development of products and processes that benefit mankind.\n\nLearning about the physical world requires the use of inductive logic. This is the logic of theory building. It is useful in such widely divergent enterprises as science and crime scene detective work. One makes a set of observations, and seeks to explain what one sees. The observer forms a hypothesis in an attempt to explain what he/she has observed. The hypothesis will have implications, which will point to certain other observations that would naturally result from either a repeat of the experiment or making more observations from a slightly different set of circumstances. If the predicted observations hold true, one feels excitement that they may be on the right track. However, the hypothesis has not been proven. The hypothesis implies that certain observations should follow, but positive observations do not imply the hypothesis. They only make it more believable. It is quite possible that some other hypothesis could also account for the known observations, and may do better with future experiments. The implication flows in only one direction, as in the syllogism used in the discussion on deduction. Therefore, it is never correct to say that a scientific principle or hypothesis/theory has been proven. (At least, not in the rigorous sense of proof used in deductive systems.)\n\nA classic example of this is the study of gravitation. Newton formed a law for gravitation stating that the force of gravitation is directly proportional to the product of the two masses and inversely proportional to the square of the distance between them. For over 170 years, all observations seemed to validate his equation. However, telescopes eventually became powerful enough to see a slight discrepancy in the orbit of Mercury. Scientists tried everything imaginable to explain the discrepancy, but they could not do so using the objects that would bear on the orbit of Mercury. Eventually, Einstein developed his theory of general relativity and it explained the orbit of Mercury and all other known observations dealing with gravitation. During the long period of time when scientists were making observations that seemed to validate Newton's theory, they did not, in fact, prove his theory to be true. However, it must have seemed at the time that they did. It only took one counterexample (Mercury's orbit) to prove that there was something wrong with his theory.\n\nThis is typical of inductive logic. All of the observations that seem to validate the theory, do not prove its truth. But one counter-example can prove it false. That means that deductive logic is used in the evaluation of a theory. In other words, if A implies B, then not B implies not A. Einstein's theory of General Relativity has been supported by many observations using the best scientific instruments and experiments. However, his theory now has the same status as Newton's theory of gravitation prior to seeing the problems in the orbit of Mercury. It is highly credible and validated with all we know, but it is not proven. It is only the best we have at this point in time.\n\nAnother example of correct scientific reasoning is shown in the current search for the Higgs boson. Scientists on the Compact Muon Solenoid experiment at the Large Hadron Collider have conducted experiments yielding data suggesting the existence of the Higgs boson. However, realizing that the results could possibly be explained as a background fluctuation and not the Higgs boson, they are cautious and waiting for further data from future experiments. Said Guido Tonelli:\n\nA brief overview of the scientific method would then contain these steps as a minimum:\n\n\nWhen a hypothesis has survived a sufficient number of tests, it may be promoted to a scientific theory. A theory is a hypothesis that has survived many tests and seems to be consistent with other established scientific theories. Since a theory is a promoted hypothesis, it is of the same 'logical' species and shares the same logical limitations. Just as a hypothesis cannot be proven but can be disproved, that same is true for a theory. It is a difference of degree, not kind.\n\nArguments from analogy are another type of inductive reasoning. In arguing from analogy, one infers that since two things are alike in several respects, they are likely to be alike in another respect. This is, of course, an assumption. It is natural to attempt to find similarities between two phenomena and wonder what one can learn from those similarities. However, to notice that two things share attributes in several respects does not imply any similarities in other respects. It is possible that the observer has already noticed all of the attributes that are shared and any other attributes will be distinct. Argument from analogy is an unreliable method of reasoning that can lead to erroneous conclusions, and thus cannot be used to establish scientific facts.\n\n\n\nFor interesting explanations regarding the orbit of Mercury and General Relativity, the following links are useful:\n"}
{"id": "795103", "url": "https://en.wikipedia.org/wiki?curid=795103", "title": "Multiple drafts model", "text": "Multiple drafts model\n\nDaniel Dennett's multiple drafts model of consciousness is a physicalist theory of consciousness based upon cognitivism, which views the mind in terms of information processing. The theory is described in depth in his book, \"Consciousness Explained\", published in 1991. As the title states, the book proposes a high-level explanation of consciousness which is consistent with support for the possibility of strong AI.\n\nDennett describes the theory as \"first-person operationalism\". As he states it:\nDennett's thesis is that our modern understanding of consciousness is unduly influenced by the ideas of René Descartes. To show why, he starts with a description of the phi illusion. In this experiment, two different coloured lights, with an angular separation of a few degrees at the eye, are flashed in succession. If the interval between the flashes is less than a second or so, the first light that is flashed appears to move across to the position of the second light. Furthermore, the light seems to change colour as it moves across the visual field. A green light will appear to turn red as it seems to move across to the position of a red light. Dennett asks how we could see the light change colour \"before\" the second light is observed.\n\nDennett claims that conventional explanations of the colour change boil down to either \"Orwellian\" or \"Stalinesque\" hypotheses, which he says are the result of Descartes' continued influence on our vision of the mind. In an Orwellian hypothesis, the subject comes to one conclusion, then goes back and changes that memory in light of subsequent events. This is akin to George Orwell's \"Nineteen Eighty-Four\", where records of the past are routinely altered. In a Stalinesque hypothesis, the two events would be reconciled prior to entering the subject's consciousness, with the final result presented as fully resolved. This is akin to Joseph Stalin's show trials, where the verdict has been decided in advance and the trial is just a rote presentation.\nDennett argues that there is no principled basis for picking one of these theories over the other, because they share a common error in supposing that there is a special time and place where unconscious processing becomes consciously experienced, entering into what Dennett calls the \"Cartesian theatre\". Both theories require us to cleanly divide a sequence of perceptions and reactions into before and after the instant that they reach the seat of consciousness, but he denies that there is any such moment, as it would lead to infinite regress. Instead, he asserts that there is no privileged place in the brain where consciousness happens. Dennett states that, \"[t]here does not exist ... a process such as 'recruitment of consciousness' (into what?), nor any place where the 'vehicle's arrival' is recognized (by whom?)\"\n\nWith no theatre, there is no screen, hence no reason to re-present data after it has already been analysed. Dennett says that, \"the Multiple Drafts model goes on to claim that the brain does not bother 'constructing' any representations that go to the trouble of 'filling in' the blanks. That would be a waste of time and (shall we say?) paint. The judgement is already in so we can get on with other tasks!\"\n\nAccording to the model, there are a variety of sensory inputs from a given event and also a variety of interpretations of these inputs. The sensory inputs arrive in the brain and are interpreted at different times, so a given event can give rise to a succession of discriminations, constituting the equivalent of multiple drafts of a story. As soon as each discrimination is accomplished, it becomes available for eliciting a behaviour; it does not have to wait to be presented at the theatre.\n\nLike a number of other theories, the Multiple Drafts model understands conscious experience as taking time to occur, such that percepts do not instantaneously arise in the mind in their full richness. The distinction is that Dennett's theory denies any clear and unambiguous boundary separating conscious experiences from all other processing. According to Dennett, consciousness is to be found in the actions and flows of information from place to place, rather than some singular view containing our experience. There is no central experiencer who confers a durable stamp of approval on any particular draft.\n\nDifferent parts of the neural processing assert more or less control at different times. For something to reach consciousness is akin to becoming famous, in that it must leave behind consequences by which it is remembered. To put it another way, consciousness is the property of having enough influence to affect what the mouth will say and the hands will do. Which inputs are \"edited\" into our drafts is not an exogenous act of supervision, but part of the self-organizing functioning of the network, and at the same level as the circuitry that conveys information bottom-up.\n\nThe conscious self is taken to exist as an abstraction visible at the level of the intentional stance, akin to a body of mass having a \"centre of gravity\". Analogously, Dennett refers to the self as the \"centre of narrative gravity\", a story we tell ourselves about our experiences. Consciousness exists, but not independently of behaviour and behavioural disposition, which can be studied through heterophenomenology.\n\nThe origin of this operationalist approach can be found in Dennett's immediately preceding work. Dennett (1988) explains consciousness in terms of \"access consciousness\" alone, denying the independent existence of what Ned Block has labeled \"phenomenal consciousness\". He argues that \"Everything real has properties, and since I don't deny the reality of conscious experience, I grant that conscious experience has properties\". Having related all consciousness to properties, he concludes that they cannot be meaningfully distinguished from our judgements about them. He writes: \nIn other words, once we've explained a perception fully in terms of how it affects us, there is nothing left to explain. In particular, there is no such thing as a perception which may be considered in and of itself (a quale). Instead, the subject's honest reports of how things seem to them are inherently authoritative on how things seem to them, but not on the matter of how things actually are.\nThe key to the multiple drafts model is that, after removing qualia, explaining consciousness boils down to explaining the behaviour we recognise as conscious. Consciousness is as consciousness does.\n\nSome of the criticism of Dennett's theory is due to the perceived tone of his presentation. As one grudging supporter admits, \"there is much in this book that is disputable. And Dennett is at times aggravatingly smug and confident about the merits of his arguments ... All in all Dennett's book is annoying, frustrating, insightful, provocative and above all annoying\" (Korb 1993).\n\nBogen (1992) points out that the brain is bilaterally symmetrical. That being the case, if Cartesian materialism is true, there might be \"two\" Cartesian theatres, so arguments against only one are flawed. Velmans (1992) argues that the phi effect and the cutaneous rabbit illusion demonstrate that there is a delay whilst modelling occurs and that this delay was discovered by Libet.\n\nIt has also been claimed that the argument in the multiple drafts model does not support its conclusion.\n\nMuch of the criticism asserts that Dennett's theory attacks the wrong target, failing to explain what it claims to. Chalmers (1996) maintains that Dennett has produced no more than a theory of how subjects report events. Some even parody the title of the book as \"Consciousness Explained Away\", accusing him of greedy reductionism. Another line of criticism disputes the accuracy of Dennett's characterisations of existing theories:\nMultiple drafts is also attacked for making a claim to novelty. It may be the case, however, that such attacks mistake which features Dennett is claiming as novel. Korb states that, \"I believe that the central thesis will be relatively uncontentious for most cognitive scientists, but that its use as a cleaning solvent for messy puzzles will be viewed less happily in most quarters.\" (Korb 1993) In this way, Dennett uses uncontroversial ideas towards more controversial ends, leaving him open to claims of unoriginality when uncontroversial parts are focused upon.\n\nEven the notion of consciousness as drafts is not unique to Dennett. According to Hankins, Dieter Teichert suggests that Paul Ricoeur's theories agree with Dennett's on the notion that \"the self is basically a narrative entity, and that any attempt to give it a free-floating independent status is misguided.\" [Hankins] Others see Derrida's (1982) representationalism as consistent with the notion of a mind that has perceptually changing content without a definitive present instant.\n\nTo those who believe that consciousness entails something more than behaving in all ways conscious, Dennett's view is seen as eliminativist, since it denies the existence of qualia and the possibility of philosophical zombies. However, Dennett is not denying the existence of the mind or of consciousness, only what he considers a naive view of them. The point of contention is whether Dennett's own definitions are indeed more accurate: whether what we think of when we speak of perceptions and consciousness can be understood in terms of nothing more than their effect on behaviour.\n\nThe role of information processing in consciousness has been criticised by John Searle who, in his Chinese room argument, states that he cannot find anything that could be recognised as conscious experience in a system that relies solely on motions of things from place to place. Dennett sees this argument as misleading, arguing that consciousness is not to be found in a specific part of the system, but in the actions of the whole. In essence, he denies that consciousness requires something in addition to capacity for behaviour, saying that philosophers such as Searle, \"just can't imagine how understanding could be a property that emerges from lots of distributed quasi-understanding in a large system\" (p. 439).\n\n\n\n\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "236895", "url": "https://en.wikipedia.org/wiki?curid=236895", "title": "Negative capability", "text": "Negative capability\n\nNegative capability was a phrase first used by Romantic poet John Keats in 1817 to characterise the capacity of the greatest writers (particularly Shakespeare) to pursue a vision of artistic beauty even when it leads them into intellectual confusion and uncertainty, as opposed to a preference for philosophical certainty over artistic beauty. The term has been used by poets and philosophers to describe the ability of the individual to perceive, think, and operate beyond any presupposition of a predetermined capacity of the human being.\n\nKeats used the phrase only briefly in a private letter, and it became known only after his correspondence was collected and published. In a letter to his brothers, George and Thomas, on 21 December 1817, Keats described a conversation he had been engaged in a few days previously: \nI had not a dispute but a disquisition with Dilke, upon various subjects; several things dove-tailed in my mind, and at once it struck me what quality went to form a Man of Achievement, especially in Literature, and which Shakespeare possessed so enormously—I mean Negative Capability, that is, when a man is capable of being in uncertainties, mysteries, doubts, without any irritable reaching after fact and reason—Coleridge, for instance, would let go by a fine isolated verisimilitude caught from the Penetralium of mystery, from being incapable of remaining content with half-knowledge. This pursued through volumes would perhaps take us no further than this, that with a great poet the sense of Beauty overcomes every other consideration, or rather obliterates all consideration.\nSamuel Taylor Coleridge was, by 1817, a frequent target of criticism by the younger poets of Keats's generation, often ridiculed for his infatuation with German idealistic philosophy. Against Coleridge's obsession with philosophical truth, Keats sets up the model of Shakespeare, whose poetry articulated various points of view and never advocated a particular vision of truth.\n\nKeats's ideas here, as was usually the case in his letters, were expressed tersely with no effort to fully expound what he meant, but passages from other letters enlarge on the same theme. In a letter to J.H. Reynolds in February, 1818, he wrote: \nWe hate poetry that has a palpable design upon us—and if we do not agree, seems to put its hand in its breeches pocket. Poetry should be great & unobtrusive, a thing which enters into one's soul, and does not startle it or amaze it with itself but with its subject.\nIn another letter to Reynolds the following May, he contrived the metaphor of 'the chamber of maiden thought' and the notion of the 'burden of mystery', which together express much the same idea as that of negative capability:\nI compare human life to a large Mansion of Many Apartments, two of which I can only describe, the doors of the rest being as yet shut upon me—The first we step into we call the infant or thoughtless Chamber, in which we remain as long as we do not think—We remain there a long while, and notwithstanding the doors of the second Chamber remain wide open, showing a bright appearance, we care not to hasten to it; but are at length imperceptibly impelled by the awakening of the thinking principle—within us—we no sooner get into the second Chamber, which I shall call the Chamber of Maiden-Thought, than we become intoxicated with the light and the atmosphere, we see nothing but pleasant wonders, and think of delaying there for ever in delight: However among the effects this breathing is father of is that tremendous one of sharpening one's vision into the heart and nature of Man—of convincing ones nerves that the World is full of Misery and Heartbreak, Pain, Sickness, and oppression—whereby This Chamber of Maiden Thought becomes gradually darken'd and at the same time on all sides of it many doors are set open—but all dark—all leading to dark passages—We see not the balance of good and evil. We are in a Mist—We are now in that state—We feel the 'burden of the Mystery,' To this point was Wordsworth come, as far as I can conceive when he wrote 'Tintern Abbey' and it seems to me that his Genius is explorative of those dark Passages. Now if we live, and go on thinking, we too shall explore them. he is a Genius and superior to us, in so far as he can, more than we, make discoveries, and shed a light in them—Here I must think Wordsworth is deeper than Milton[.]\nKeats understood Coleridge as searching for a single, higher-order truth or solution to the mysteries of the natural world. He went on to find the same fault in Dilke and Wordsworth. All these poets, he claimed, lacked objectivity and universality in their view of the human condition and the natural world. In each case, Keats found a mind which was a narrow private path, not a \"thoroughfare for all thoughts\". Lacking for Keats were the central and indispensable qualities requisite for flexibility and openness to the world, or what he referred to as negative capability.\n\nThis concept of Negative Capability is precisely a rejection of set philosophies and preconceived systems of nature. He demanded that the poet be receptive rather than searching for fact or reason, and to not seek absolute knowledge of every truth, mystery, or doubt.\n\nIt is not known why Keats settled on the phrase 'negative capability', but some scholars have hypothesized that Keats was influenced in his studies of medicine and chemistry, and that it refers to the negative pole of an electric current which is passive and receptive. In the same way that the negative pole receives the current from the positive pole, the poet receives impulses from a world that is full of mystery and doubt, which cannot be explained but which the poet can translate into art.\n\nRoberto Unger appropriated Keats' term in order to explain resistance to rigid social divisions and hierarchies. For Unger, \"negative capability\" is the \"denial of whatever in our contexts delivers us over to a fixed scheme of division and hierarchy and to an enforced choice between routine and rebellion.\" It is thus through \"negative capability\" that we can further empower ourselves against social and institutional constraints, and loosen the bonds that entrap us in a certain social station.\n\nAn example of negative capability can be seen at work in industrial innovation. In order to create an innovator's advantage and develop new forms of economic enterprise, the modern industrialist could not just become more efficient with surplus extraction based on pre-existing work roles, but rather needed to invent new styles of flexible labor, expertise, and capital management. The industrialist needed to bring people together in new and innovative ways and redefine work roles and workplace organization. The modern factory had to at once stabilize its productive environment by inventing new restraints upon labor, such as length of the work day and division of tasks, but at the same time could not be too severe or risk being at a disadvantage to competitors, e.g. not being able to shift production tasks or capacity. Those industrialists and managers who were able to break old forms of organizational arrangements exercised negative capability.\n\nThis thesis of \"negative capability\" is a key component in Unger's theory of false necessity and formative context. The theory of false necessity claims that our social worlds are the artifact of our own human endeavors. There is no pre-set institutional arrangement that our societies adhere to, and there is no necessary historical mold of development that they will follow. Rather we are free to choose and develop the forms and the paths that our societies will take through a process of conflicts and resolutions. However, there are groups of institutional arrangements that work together to bring out certain institutional forms, liberal democracy, for example. These forms are the basis of a social structure, and which Unger calls formative contexts. In order to explain how we move from one formative context to another without the conventional social theory constraints of historical necessity (e.g. feudalism to capitalism), and to do so while remaining true to the key insight of individual human empowerment and anti-necessitarian social thought, Unger recognized that there are an infinite number of ways of resisting social and institutional constraints, which can lead to an infinite number of outcomes. This variety of forms of resistance and empowerment (i.e. negative capability) make change possible.\n\nThis thesis of \"negative capability\" addresses the problem of agency in relation to structure. It recognizes the constraints of structure and its molding influence upon the individual, but at the same time finds the individual able to resist, deny, and transcend their context. Unlike other theories of structure and agency, \"negative capability\" does not reduce the individual to a simple actor possessing only the dual capacity of compliance or rebellion, but rather sees him as able to partake in a variety of activities of self empowerment.\n\nThe twentieth-century British psychoanalyst Wilfred Bion elaborated on Keats's term to illustrate an attitude of openness of mind which he considered of central importance, not only in the psychoanalytic session, but in life itself. For Bion, negative capability was the ability to tolerate the pain and confusion of not knowing, rather than imposing ready-made or omnipotent certainties upon an ambiguous situation or emotional challenge. His idea has been taken up more widely in the British Independent School, as well as elsewhere in psychoanalysis and psychotherapy.\n\nThe notion of negative capability has been associated with the Zen philosophy. Keats' man of negative capability had qualities that enabled him to \"lose his self-identity, his 'imaginative identification' with and submission to things, and his power to achieve a unity with life\". The Zen concept of satori is the outcome of passivity and receptivity, culminating in \"sudden insight into the character of the real\". Satori is reached without deliberate striving. The antecedent stages to satori: quest, search, ripening and explosion. The \"quest\" stage is accompanied by a strong feeling of uneasiness, resembling the capacity to practice negative capability while the mind is in a state of \"uncertainties, mysteries and doubts\". In the explosive stage (akin to Keats' 'chief intensity'), a man of negative capability effects a \"fellowship with essence\".\n\nStanley Fish has expressed strong reservations about the attempt to apply the concept of negative capability to social contexts. He has written in critique of Unger's early work as being unable to chart a route for the idea to pass into reality, which leaves history closed and the individual holding onto the concept while kicking against air. Fish finds the capability Unger invokes in his early works unimaginable and unmanufacturable that can only be expressed outright in blatant speech, or obliquely in concept. More generally, Fish finds the idea of radical culture as an oppositional ideal in which context is continuously refined or rejected impracticable at best, and impossible at worst. Unger has addressed these criticisms by developing a full theory of historical process in which negative capability is employed.\n\n"}
{"id": "17235432", "url": "https://en.wikipedia.org/wiki?curid=17235432", "title": "Negative elongation factor", "text": "Negative elongation factor\n\nIn molecular biology, NELF (negative elongation factor) is a four-subunit protein (NELF-A, NELF-B, NELF-C/NELF-D, and NELF-E) that negatively impacts transcription by RNA polymerase II (Pol II) by pausing about 20-60 nucleotides downstream from the transcription start site (TSS).\n\nThe NELF-A subunit is encoded by the gene WHSC2 (Wolf-Hirschhorn syndrome candidate 2). Microsequencing analysis demonstrated that NELF-B was the protein previously identified as the protein encoded by the gene COBRA1, and was shown to interact with BRCA1. It is unknown whether or not NELF-C and NELF-D are peptides resulting from the same mRNA with different translation initiation sites, possibly differing only in an extra 9 amino acids for NELF-C at the N-terminus, or peptides from different mRNAs entirely. A single NELF complex consists of either NELF-C or NELF-D but not both. NELF-E is also known as RDBP.\n\nNELF binds in a stable complex with DSIF and RNA polymerase II together, but not with either alone. P-TEFb (positive transcription elongation factor b) inhibits the effect of NELF and DSIF on Pol II elongation, via its phosphorylation of serine-2 of the C-terminal domain of Pol II, and the SPT5 subunit of DSIF, causing dissociation of NELF. NELF homologues exist in some metazoans (e.g. insects and vertebrates) but have not been found in plants, yeast, or nematode (worms).\n"}
{"id": "26502557", "url": "https://en.wikipedia.org/wiki?curid=26502557", "title": "Negative room pressure", "text": "Negative room pressure\n\nNegative room pressure is an isolation technique used in hospitals and medical centers to prevent cross-contaminations from room to room. It includes a ventilation system that generates negative pressure to allow air to flow into the isolation room but not escape from the room, as air will naturally flow from areas with higher pressure to areas with lower pressure, thereby preventing contaminated air from escaping the room. This technique is used to isolate patients with airborne contagious diseases such as tuberculosis, measles, or chickenpox.\n\nNegative pressure is generated and maintained by a ventilation system that removes more exhaust air from the room than air is allowed into the room. Air is allowed into the room through a gap under the door (typically about one half-inch high). Except for this gap, the room should be as airtight as possible, allowing no air in through cracks and gaps, such as those around windows, light fixtures and electrical outlets. Leakage from these sources can compromise or eliminate room negative pressure.\n\nA smoke test can help determine whether a room is under negative pressure. A tube containing smoke is held near the bottom of the negative pressure room door, about 2 inches in front of the door. The smoke tube is held parallel to the door, and a small amount of smoke is then generated by gently squeezing the bulb. Care is taken to release the smoke from the tube slowly to ensure the velocity of the smoke from the tube does not overpower the air velocity. If the room is at negative pressure, the smoke will travel under the door and into the room. If the room is not a negative pressure, the smoke will be blown outward or will stay stationary.\n\n"}
{"id": "244067", "url": "https://en.wikipedia.org/wiki?curid=244067", "title": "Negative sign (astrology)", "text": "Negative sign (astrology)\n\nIn astrology, a negative, ceptive, dispassive, yin, nocturnal or feminine sign refers to any of the six even-numbered signs of the zodiac: Taurus, Cancer, Virgo, Scorpio, Capricorn or Pisces.\n\nThese signs constitute the earth and water triplicities.\n\nIn astrology there are two groups: positive and negative. These two groups also\ninclude six individual signs that are called zodiac signs. The negative signs associated\nwith the zodiac are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. The positive\nsigns associated with the zodiac are Aries, Gemini, Leo, Libra, Sagittarius, and\nAquarius. The twelve signs are divided into two\ngroups based upon one's exact time and place of birth. The negative and positive signs\nalternate, starting with Aries as positive and Taurus as negative and continuing this pattern through the list of zodiac signs.\n\nThe signs negative and positive are referred to as a negative-sunsign or a\npositive-sunsign. There are many terms used in astrology to differentiate the\ntwo groups. In Chinese astrology, the two groups are categorized as yin and yang, corresponding respectively to negative and positive. Standen explains that different astrologers may refer to signs by different names. For example, an astrologer may refer to positive signs as masculine and negative signs as feminine. Each sign is divided into two main\ntypes: active and passive. As a general rule, the active type will be called masculine and the passive type will be called feminine.\n\nZodiac signs associated with the negative are Taurus, Cancer, Virgo, Scorpio, Capricorn, and Pisces. Note that there is no value judgment attached to the terms negative or positive. They may be likened to polarities in a magnet: one side is positive and one side is negative. Neither side is \"good\" nor \"bad\"—they are merely different.\n\nThe sunsign effect is a pattern of alternating high and low extraversion-scores for\nthe 12 signs. Introvert is a person who gains energy when alone and spends it when with other people. Extravert is a person who gains energy from socialization, expending it when alone. Jan J.F. van Rooij did an experiment on Introversion-Extraversion: astrology\nversus psychology, to see if those in the negative sunsign were introverted and those in\nthe positive sunsign were negative. Van Rooijs did this experiment on those with\nastrological knowledge and on those with no astrological knowledge. His results showed\nthat negative sunsign people are not that responsive to the outer world and are\naccordingly not influenced that easily by astrological information. They rely more on\ntheir own ideas and feelings, thus proving his point that people who are born with the\nnegative sunsign are introverted, and those born with the positive sunsign are\nextroverted.\n\nEarth and Water are the elements attached to those who are in the negative sign.\nEarth is the element of Taurus, Virgo, and Capricorn. Water is the element of Cancer,\nScorpio, and Pisces. Elements are the basic traits of the signs. They reveal the\nfundamental aspects of the personality.\n“Water signs are attuned to waves of emotion, and often seem to have a built-in\nsonar for reading a mood. This gives them a special sensitivity in relationships, knowing\nwhen to show warmth and when to hold back. At their best, they are a healing force that\nbring people together -- at their worst, they are psychic vampires, able to manipulate\nand drain the life force of the closest to them”. Hall explains that water signs are\nmore in tune with their emotions and are comfortable showing them. Water signs bring a\ncertain presence to a situation; they seek out the problem and fix it.\n“Earth signs are sensual, meaning they engage with life through the five senses.\nIt takes time to sense the dense physical world, and earth signs can operate at a slower,\nmore thorough pace than the other elements. Theyʼre oriented toward whatʼs real, and\noften this makes them very productive, able to create tangible results.”\nEarth signs are described by Hall as earthy people. These signs focus on the things\nthat connect us to the earth: things which bring peace, as opposed to focusing on the material world.\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "4003593", "url": "https://en.wikipedia.org/wiki?curid=4003593", "title": "Negative volume index", "text": "Negative volume index\n\nNearly 78 years have passed since Paul L. Dysart, Jr. invented the Negative Volume Index and Positive Volume Index indicators. The indicators remain useful to identify primary market trends and reversals.\n\nIn 1936, Paul L. Dysart, Jr. began accumulating two series of advances and declines distinguished by whether volume was greater or lesser than the prior day's volume. He called the cumulative series for the days when volume had been greater than the prior day's volume the Positive Volume Index (PVI), and the series for the days when volume had been lesser the Negative Volume Index (NVI).\n\nA native of Iowa, Dysart worked in Chicago's LaSalle Street during the 1920s. After giving up his Chicago Board of Trade membership, he published an advisory letter geared to short-term trading using advance-decline data. In 1933, he launched the \"Trendway\" weekly stock market letter and published it until 1969 when he died. Dysart also developed the 25-day Plurality Index, the 25-day total of the absolute difference between the number of advancing issues and the number of declining issues, and was a pioneer in using several types of volume of trading studies. Richard Russell, editor of Dow Theory Letters, in his January 7, 1976 letter called Dysart \"one of the most brilliant of the pioneer market technicians.\"\n\nThe daily volume of the New York Stock Exchange and the NYSE Composite Index's advances and declines drove Dysart's indicators. Dysart believed that “volume is the driving force in the market.” He began studying market breadth numbers in 1931, and was familiar with the work of Leonard P. Ayres and James F. Hughes, who pioneered the tabulation of advances and declines to interpret stock market movements.\n\nDysart calculated NVI as follows: 1) if today's volume is less than yesterday's volume, subtract declines from advances, 2) add the difference to the cumulative NVI beginning at zero, and 3) retain the current NVI reading for the days when volume is greater than the prior day's volume. He calculated PVI in the same manner but for the days when volume was greater than the prior day's volume. NVI and PVI can be calculated daily or weekly.\n\nInitially, Dysart believed that PVI would be the more useful series, but in 1967, he wrote that NVI had “proved to be the most valuable of all the breadth indexes.” He relied most on NVI, naming it AMOMET, the acronym of “A Measure Of Major Economic Trend.”\n\nDysart's theory, expressed in his 1967 Barron's article, was that “if volume advances and prices move up or down in accordance [with volume], the move is assumed to be a good movement - if it is sustained when the volume subsides.” In other words, after prices have moved up on positive volume days, \"if prices stay up when the volume subsides for a number of days, we can say that such a move is 'good'.\" If the market “holds its own on negative volume days after advancing on positive volume, the market is in a strong position.”\n\nHe called PVI the “majority” curve. Dysart distinguished between the actions of the “majority” and those of the “minority.” The majority tends to emulate the minority, but its timing is not as sharp as that of the minority. When the majority showed an appetite for stocks, the PVI was usually “into new high ground” as happened in 1961.\n\nIt is said that the two indicators assume that \"smart\" money is traded on quiet days (low volume) and that the crowd trades on very active days. Therefore, the negative volume index picks out days when the volume is lower than on the previous day, and the positive index picks out days with a higher volume.\n\nBesides an article he wrote for Barron's in 1967, not many of Dysart's writings are available. What can be interpreted about Dysart's NVI is that whenever it rises above a prior high, and the DJIA is trending up, a “Bull Market Signal” is given. When the NVI falls below a prior low, and the DJIA is trending down, a “Bear Market Signal” is given. The PVI is interpreted in reverse.\nHowever, not all movements above or below a prior NVI or PVI level generate signals, as Dysart also designated “bullish” and “bearish penetrations.” These penetrations could occur before or after a Bull or Bear Market Signal, and at times were called “reaffirmations” of a signal. In 1969, he articulated one rule: “signals are most authentic when the NVI has moved sideways for a number of months in a relatively narrow range.” Dysart cautioned that “there is no mathematical system devoid of judgment which will continuously work without error in the stock market.”\n\nAccording to Dysart, between 1946 and 1967, the NVI “rendered 17 significant signals,” of which 14 proved to be right (an average of 4.32% from the final high or low) and 3 wrong (average loss of 6.33%). However, NVI “seriously erred” in 1963-1964 and in 1968, which concerned him. In 1969, Dysart reduced the weight he had previously given to the NVI in his analyses because NVI was no longer a “decisive” indicator of the primary trend, although it retained an “excellent ability to give us ‘leading’ indications of short-term trend reversals.”\n\nA probable reason for the NVI losing its efficacy during the mid-1960s may have been the steadily higher NYSE daily volume due to the dramatic increase in the number of issues traded so that prices rose on declining volume. Dysart’s NVI topped out in 1955 and trended down until at least 1968, although the DJIA moved higher during that period.\nNorman G. Fosback has attributed the “long term increase in the number of issues traded” as a reason for a downward bias in a cumulative advance-decline line. Fosback was the next influential technician in the story of NVI and PVI.\n\nFosback studied NVI and PVI and in 1976 reported his findings in his classic Stock Market Logic. He did not elucidate on the indicators’ background or mentioned Dysart except for saying that “in the past Negative Volume Indexes have always [his emphasis] been constructed using advance-decline data….” He posited, “There is no good reason for this fixation on the A/D Line. In truth, a Negative Volume Index can be calculated with any market index - the Dow Jones Industrial Average, the S&P 500, or even ‘unweighted’ market measures... Somehow this point has escaped the attention of technicians to date.”\n\nThe point had not been lost on Dysart, who wrote in Barron’s, “we prefer to use the issues-traded data [advances and declines] rather than the price data of any average because it is more all-encompassing, and more truly represents what’s happening in the entire market.” Dysart was a staunch proponent of using advances and declines.\n\nFosback made three variations to NVI and PVI:\n\n1. He cumulated the daily percent change in the market index rather than the difference between advances and declines. On negative volume days, he calculated the price change in the index from the prior day and added it to the most recent NVI. His calculations are as follows:\n\nIf C and C denote the closing prices of today and yesterday, respectively, the NVI for today is calculated by\n\n\nand the PVI is calculated by:\n\n\n2. He suggested starting the cumulative count at a base index level such as 100.\n\n3. He derived buy or sell signals by whether the NVI or PVI was above or below its one-year moving average.\n\nFosback's versions of NVI and PVI are what are popularly described in books and posted on Internet financial sites. Often reported are his findings that whenever NVI is above its one-year moving average there is a 96% (PVI - 79%) probability that a bull market is in progress, and when it is below its one-year moving average, there is a 53% (PVI - 67%) probability that a bear market is in place. These results were derived using a 1941-1975 test period. Modern tests might reveal different probabilities.\n\nToday, NVI and PVI are commonly associated with Fosback's versions, and Dysart, their inventor, is forgotten. It cannot be said that one version is better than the other. While Fosback provided a more objective interpretation of these indicators, Dysart's versions offer value to identify primary trends and short-term trend reversals.\n\nAlthough some traders use Fosback's NVI and PVI to analyze individual stocks, the indicators were created to track, and have been tested, on major market indexes. NVI was Dysart's most invaluable breadth index, and Fosback found that his version of “the Negative Volume Index is an excellent indicator of the primary market trend.” Traders can benefit from both innovations.\n\n"}
{"id": "831689", "url": "https://en.wikipedia.org/wiki?curid=831689", "title": "Pontryagin's maximum principle", "text": "Pontryagin's maximum principle\n\nPontryagin's maximum (or minimum) principle is used in optimal control theory to find the best possible control for taking a dynamical system from one state to another, especially in the presence of constraints for the state or input controls. It was formulated in 1956 by the Russian mathematician Lev Pontryagin and his students. It has as a special case the Euler–Lagrange equation of the calculus of variations.\n\nThe principle states, informally, that the \"control Hamiltonian\" must take an extreme value over controls in the set of all permissible controls. Whether the extreme value is maximum or minimum depends both on the problem and on the sign convention used for defining the Hamiltonian. The normal convention, which is the one used in Hamiltonian, leads to a maximum hence \"maximum principle\" but the sign convention used in this article makes the extreme value a minimum.\n\nIf formula_1 is the set of values of permissible controls then the principle states that the optimal control formula_2 must satisfy:\nwhere formula_4 is the optimal state trajectory and formula_5 is the optimal costate trajectory.\n\nThe result was first successfully applied to minimum time problems where the input control is constrained, but it can also be useful in studying state-constrained problems.\n\nSpecial conditions for the Hamiltonian can also be derived. When the final time formula_6 is fixed and the Hamiltonian does not depend explicitly on time formula_7, then:\nand if the final time is free, then:\nMore general conditions on the optimal control are given below.\n\nWhen satisfied along a trajectory, Pontryagin's minimum principle is a necessary condition for an optimum. The Hamilton–Jacobi–Bellman equation provides a necessary and sufficient condition for an optimum, but this condition must be satisfied over the whole of the state space.\n\nWhile the Hamilton-Jacobi-Bellman equation admits a straightforward extension to stochastic optimal control problems, the minimum principle does not.\n\nThe principle was first known as \"Pontryagin's maximum principle\" and its proof is historically based on maximizing the Hamiltonian. The initial application of this principle was to the maximization of the terminal speed of a rocket. However, as it was subsequently mostly used for minimization of a performance index it has here been referred to as the \"minimum principle\". Pontryagin's book solved the problem of minimizing a performance index.\n\nIn what follows we will be making use of the following notation.\n\nHere the necessary conditions are shown for minimization of a functional. Take formula_15 to be the state of the dynamical system with input formula_16, such that\nwhere formula_1 is the set of admissible controls and formula_19 is the terminal (i.e., final) time of the system. The control formula_20 must be chosen for all formula_21 to minimize the objective functional formula_22 which is defined by the application and can be abstracted as\n\nThe constraints on the system dynamics can be adjoined to the Lagrangian formula_24 by introducing time-varying Lagrange multiplier vector formula_25, whose elements are called the costates of the system. This motivates the construction of the Hamiltonian formula_26 defined for all formula_21 by:\nwhere formula_29 is the transpose of formula_25.\n\nPontryagin's minimum principle states that the optimal state trajectory formula_31, optimal control formula_2, and corresponding Lagrange multiplier vector formula_33 must minimize the Hamiltonian formula_26 so that\n\nfor all time formula_21 and for all permissible control inputs formula_20. It must also be the case that\n\nAdditionally, the costate equations\n\nmust be satisfied. If the final state formula_40 is not fixed (i.e., its differential variation is not zero), it must also be that the terminal costates are such that\n\nThese four conditions in (1)-(4) are the necessary conditions for an optimal control. Note that (4) only applies when formula_40 is free. If it is fixed, then this condition is not necessary for an optimum.\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "952842", "url": "https://en.wikipedia.org/wiki?curid=952842", "title": "Principle of compositionality", "text": "Principle of compositionality\n\nIn mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. This principle is also called Frege's principle, because Gottlob Frege is widely credited for the first modern formulation of it. However, the idea appears already among Indian philosophers of grammar such as Yāska, and also in Plato's work such as in \"Theaetetus\".\nBesides, the principle was never explicitly stated by Frege, and it was arguably already assumed by George Boole decades before Frege’s work.\n\nThe principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence \"Socrates was a man\". Once the meaningful lexical items are taken away—\"Socrates\" and \"man\"—what is left is the pseudo-sentence, \"S was a M\". The task becomes a matter of describing what the connection is between S and M.\n\nIt is frequently taken to mean that every operation of the syntax should be associated with an operation of the semantics that acts on the meanings of the constituents combined by the syntactic operation. As a guideline for constructing semantic theories, this is generally taken, as in the influential work on the philosophy of language by Donald Davidson, to mean that every construct of the syntax should be associated by a clause of the T-schema with an operator in the semantics that specifies how the meaning of the whole expression is built from constituents combined by the syntactic rule. In some general mathematical theories (especially those in the tradition of Montague grammar), this guideline is taken to mean that the interpretation of a language is essentially given by a homomorphism between an algebra of syntactic representations and an algebra of semantic objects.\n\nThe principle of compositionality also exists in a similar form in the compositionality of programming languages.\n\nThe principle of compositionality has been the subject of intense debate. Indeed, there is no general agreement as to how the principle is to be interpreted, although there have been several attempts to provide formal definitions of it. (Szabó, 2012)\n\nScholars are also divided as to whether the principle should be regarded as a factual claim, open to empirical testing; an analytic truth, obvious from the nature of language and meaning; or a methodological principle to guide the development of theories of syntax and semantics. The Principle of Compositionality has been attacked in all three spheres, although so far none of the criticisms brought against it have been generally regarded as compelling. Most proponents of the principle, however, make certain exceptions for idiomatic expressions in natural language. (Szabó, 2012)\n\nFurther, in the context of the philosophy of language, the principle of compositionality does not explain all of meaning. For example, you cannot infer sarcasm purely on the basis of words and their composition, yet a phrase used sarcastically means something completely different from the same phrase uttered straightforwardly. Thus, some theorists argue that the principle has to be revised to take into account linguistic and extralinguistic context, which includes the tone of voice used, common ground between the speakers, the intentions of the speaker, and so on. (Szabó, 2012)\n\n\n"}
{"id": "249438", "url": "https://en.wikipedia.org/wiki?curid=249438", "title": "Principle of least action", "text": "Principle of least action\n\nThe principle of least action – or, more accurately, the principle of stationary action – is a variational principle that, when applied to the action of a mechanical system, can be used to obtain the equations of motion for that system. In relativity, a different action must be minimized or maximized. The principle can be used to derive Newtonian, Lagrangian and Hamiltonian equations of motion, and even general relativity (see Einstein–Hilbert action). The physicist Paul Dirac, and after him Julian Schwinger and Richard Feynman, demonstrated how this principle can also be used in quantum calculations.\nIt was historically called \"least\" because its solution requires finding the path that has the least value. Its classical mechanics and electromagnetic expressions are a consequence of quantum mechanics, but the stationary action method helped in the development of quantum mechanics.\n\nThe principle remains central in modern physics and mathematics, being applied in thermodynamics, fluid mechanics, the theory of relativity, quantum mechanics, particle physics, and string theory and is a focus of modern mathematical investigation in Morse theory. Maupertuis' principle and Hamilton's principle exemplify the principle of stationary action.\n\nThe action principle is preceded by earlier ideas in optics. In ancient Greece, Euclid wrote in his \"Catoptrica\" that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path was the shortest length and least time.\n\nScholars often credit Pierre Louis Maupertuis for formulating the principle of least action because he wrote about it in 1744 and 1746. However, Leonhard Euler discussed the principle in 1744, and evidence shows that Gottfried Leibniz preceded both by 39 years.\n\nIn 1933, Paul Dirac discerned the quantum mechanical underpinning of the principle in the quantum interference of amplitudes.\n\nThe starting point is the \"action\", denoted formula_1 (calligraphic S), of a physical system. It is defined as the integral of the Lagrangian \"L\" between two instants of time \"t\" and \"t\" - technically a functional of the \"N\" generalized coordinates q = (\"q\", \"q\", ... , \"q\") which define the configuration of the system:\n\nwhere the dot denotes the time derivative, and \"t\" is time.\n\nMathematically the principle is\n\nwhere \"δ\" (lowercase Greek delta) means a \"small\" change. In words this reads:\n\nIn applications the statement and definition of action are taken together:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nIn the 1600s, Pierre de Fermat postulated that \"\"light travels between two given points along the path of shortest time\",\" which is known as the principle of least time or Fermat's principle.\n\nCredit for the formulation of the principle of least action is commonly given to Pierre Louis Maupertuis, who felt that \"Nature is thrifty in all its actions\", and applied the principle broadly:\n\nThis notion of Maupertuis, although somewhat deterministic today, does capture much of the essence of mechanics.\n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"vis viva\",\n\nwhich is the integral of twice what we now call the kinetic energy \"T\" of the system.\n\nLeonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the \"Additamentum 2\" to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\". Beginning with the second paragraph:\n\nAs Euler states, ∫\"Mv\"d\"s\" is the integral of the momentum over distance travelled, which, in modern notation, equals the abbreviated or reduced action\n\nThus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows.\n\nMaupertuis' priority was disputed in 1751 by the mathematician Samuel König, who claimed that it had been invented by Gottfried Leibniz in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a \"copy\" of a 1707 letter from Leibniz to Jacob Hermann with the principle, but the \"original\" letter has been lost. In contentious proceedings, König was accused of forgery, and even the King of Prussia entered the debate, defending Maupertuis (the head of his Academy), while Voltaire defended König.\n\nEuler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752. The claims of forgery were re-examined 150 years later, and archival work by C.I. Gerhardt in 1898 and W. Kabitz in 1913 uncovered other copies of the letter, and three others cited by König, in the Bernoulli archives.\n\nEuler continued to write on the topic; in his \"Reflexions sur quelques loix generales de la nature\" (1748), he called the quantity \"effort\". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\nMuch of the calculus of variations was stated by Joseph-Louis Lagrange in 1760 and he proceeded to apply this to problems in dynamics. In \"Méchanique Analytique\" (1788) Lagrange derived the general equations of motion of a mechanical body. William Rowan Hamilton in 1834 and 1835 applied the variational principle to the classical Lagrangian function\n\nto obtain the Euler–Lagrange equations in their present form.\n\nIn 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian.\n\nOther extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.\n\nThe mathematical equivalence of the differential equations of motion and their integral\ncounterpart has important philosophical implications. The differential equations are statements about quantities localized to a single point in space or single moment of time. For example, Newton's second law\n\nstates that the \"instantaneous\" force F applied to a mass \"m\" produces an acceleration a at the same \"instant\". By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) an extended region of space. Moreover, in the usual formulation of classical action principles, the initial and final states of the system are fixed, e.g.,\n\nIn particular, the fixing of the \"final\" state has been interpreted as giving the action principle a teleological character which has been controversial historically. However, according to W. Yourgrau and S. Mandelstam, \"the teleological approach... presupposes that the variational principles themselves have mathematical characteristics which they \"de facto\" do not possess\" In addition, some critics maintain this apparent teleology occurs because of the way in which the question was asked. By specifying some but not all aspects of both the initial and final conditions (the positions but not the velocities) we are making some inferences about the initial conditions from the final conditions, and it is this \"backward\" inference that can be seen as a teleological explanation. Teleology can also be overcome if we consider the classical description as a limiting case of the quantum formalism of path integration, in which stationary paths are obtained as a result of interference of amplitudes along all possible paths.\n\nThe short story \"Story of Your Life\" by the speculative fiction writer Ted Chiang contains visual depictions of Fermat's Principle along with a discussion of its teleological dimension. Keith Devlin's \"The Math Instinct\" contains a chapter, \"Elvis the Welsh Corgi Who Can Do Calculus\" that discusses the calculus \"embedded\" in some animals as they solve the \"least time\" problem in actual situations.\n\n"}
{"id": "33920294", "url": "https://en.wikipedia.org/wiki?curid=33920294", "title": "Psychotherapy and social action model", "text": "Psychotherapy and social action model\n\nThe psychotherapy and social action model is an approach to psychotherapy characterized by concentration on past and present personal, social, and political obstacles to mental health. In particular, the goal of this therapeutic approach is to acknowledge that individual symptoms are not unique, but rather shared by people similarly oppressed and marginalized. Ultimately, the psychotherapy and social action model aims to aid clients in overcoming mental illness through personal psychotherapy, group coping, and collective social action.\n\n The psychotherapy and social action model was initially proposed by Sue Holland, a psychotherapist with a background in community action. Holland developed this framework in 1980 following her experience working with women coping with psychological disorders at a housing estate in West London. At this estate, Holland observed the psychological difficulties experienced by women, noticing that their mental health was fundamentally tied to the social and economic obstacles they encountered as females in their society. In addition, Holland took issue with the way Depression (mood) was being treated at the shelter, believing that individualized treatment, especially with the use of psychotropic medication, was not successfully addressing the root of the dysfunction for these women. Instead, Holland posited a pathway from individual treatment to sociopolitical action that empowered women to deal with their mental dysfunction both privately and socially. As such, the psychotherapy and social action model is rooted in the ideals of both traditional psychotherapy and feminist empowerment.\n\nThe square model derives from the sociological theory of the four paradigms for the analysis of social theory. Outside the frame of the model, the dichotomy of individual versus social approaches to personal well-being is represented. The two bottom cells of the square delineate the changing of individuals to conform to social convention while the two top cells of the square represent the changing of social structures as opposed to the individual.\n\nThe four cells within the frame represent the four paradigms of social theory including functionalist, interpretive, radical humanist, and radical structuralist paradigms. Functionalism here is rooted in regulation and objective thinking, and represents the individual, status-quo approach to mental health. The interpretive paradigm is characterized by an approach to understanding the social world through subjective experience, and represents psychoeducation within the psychotherapy framework. The radical humanist paradigm is defined by a radial approach to change with an emphasis on “transcending limitations of existing social arrangements.” (Burrell & Morgan, 1979, p. 32). With respect to an approach to therapy, this stage is characterized by the adoption of a social self, such that healing occurs at a group or collective level. The radical structuralist paradigm concentrates on radical change through political or economic emancipation. This is the endpoint of therapy, at which time the client is empowered to challenge sociopolitical structures that foster the conditions perpetuating the manifestation of individual mental illness within an oppressed group.\n\nTaken from her 1992 publication entitled, “From Social Abuse to Social Action: a neighborhood psychotherapy and social action project for women,” Holland formulated her four step approach to mental health and social action for women in treatment for depression as follows:\n\nAt this stage, patients endorse the status-quo characterization of the “individualized patient.” As such, they treat their disorder passively with psychotropic medication and accept the label associated with their illness.\n\nThis stage represents the first alternative to the status-quo treatment of psychiatric disorders: talk therapy. At this stage, clients and therapists are able to explore the meaning of their psychopathology and pinpoint the potential causes through individual therapy.\n\nAt this stage, the client is able to move past the personal challenges that are acknowledged and addressed in psychotherapy and discover that the challenges are universal amongst similarly marginalized individuals. Together, clients aim to acknowledge what is best for the collective.\n\nThe final stage, as the name suggests, is the point at which the collective mobilizes to change the social structures enabling their common oppression. Having changed from an individual to a collective, the clients should feel empowered to undertake social change.\n\nIncluded in this framework is the assumption that only some of the clients in this therapy will traverse all three stages. In Holland’s words, “…many will be content enough with the relief from symptoms and the freedom to get on with their personal lives which the individual therapy gives them.” (Holland, 1992, p. 73). Thus, this framework is fluid based on the personal inclinations of the client throughout the therapeutic process.\n\n• Women’s Action for Mental Health (WAMH)\n• Men’s Advice Network (MAN)\n• Travers (1997)\n"}
{"id": "48000439", "url": "https://en.wikipedia.org/wiki?curid=48000439", "title": "Records Continuum Model", "text": "Records Continuum Model\n\nThe Records Continuum Model (RCM) was created in the 1990s by Monash University academic Frank Upward with input from colleagues Sue McKemmish and Livia Iacovino as a response to evolving discussions about the challenges of managing digital records and archives in the discipline of Archival Science. The RCM was first published in Upward’s 1996 paper \"Structuring the Records Continuum – Part One: Postcustodial principles and properties\". Upward describes the RCM within the broad context of a continuum where activities and interactions transform documents into records, evidence and memory that are used for multiple purposes over time. Upward places the RCM within a post-custodial, postmodern and structuration conceptual framework. Australian academics and practitioners continue to explore, develop and extend the RCM and records continuum theory, along with international collaborators, via the Records Continuum Research Group (RCRG) at Monash University.\n\nThe RCM is an abstract conceptual model that helps to understand and explore recordkeeping activities (as interaction) in relation to multiple contexts over space and time (spacetime). Recordkeeping activities take place from before the records are created by identifying recordkeeping requirements in policies, systems, organizations, processes, laws, social mandates that impact on what is created and how it is managed over spacetime. In a continuum, recordkeeping processes, such as adding metadata, fix documents so that they can be managed as evidence. Those records deemed as having continuing value are retained and managed as an archive. The implication of an RCM approach to records and archives is that systems and processes can be designed and put in place before records are even created. A continuum approach therefore highlights that records are both current and archival at the point of creation.\n\nThe RCM is represented as a series of concentric rings (dimensions of \"Create\", \"Capture\", \"Organize\" and \"Pluralize\") and crossed axes (transactionality, evidentiality, recordkeeping and identity) with each axis labelled with a description of the activity or interaction that occurs at that intersection. \"Create\", \"Capture\", \"Organize\" and \"Pluralize\" represent recordkeeping activities that occur within spacetime. Activities that occur in these dimensions across the axes are explained in the table below:\n\nThe value of the RCM is that it can help to map where on a continuum recordkeeping activities are or can be placed. The RCM can then be used to explore the conceptual and practical assumptions that underpin the practice, in particular the dualisms inherent in the usage and practice of the terms \"records\" and \"archives\". This definition lends itself to a linear reading of the RCM – starting at \"Create\" as the initiating phase and working outwards towards \"Pluralization\" of recorded information. Another linear reading is to consider design first – the role that systems of \"Pluralization\" and \"Organization\" play in designing, planning and implementing recordkeeping and then considering the implications for \"Create\" and \"Capture\". However, these are just two of many ways to interpret the model as the dimensions and axes represent multiple realities that occur within spacetime, any of which can occur simultaneously, concurrently and sequentially in electronic or digital environments, and/or physical spaces.\n\nBy representing multiple realities, the RCM articulates the numerous and diverse perspectives that contribute to records and archives including individual, group, community, organizational, institutional and societal. These contexts reveal the need to take into account various stakeholders and co-contributors in relation to use, access and appraisal of records and archives. Over the lifespan of a record multiple decisions are made by various stakeholders of the records that include, but are not limited to records managers and archivists. Other stakeholders can be identified at various dimensions of interaction, including those involved in providing information (not only the person or organization who produced or captured it), as well as their family and community. Records are therefore not simply physical or digital representations of physical objects held and managed in an archive or repository, but are evidence of multiple perspectives, narratives and contexts that contributed to their formation.\n\nThe RCM is often described as being in contrast or at odds with the lifecycle records model. While the RCM is inclusive of multiple ways of conceptualizing and performing recordkeeping, including a lifecycle approach, there are some significant differences. Firstly, where the lifecycle approach shows clearly demarcated phases in the management of records, a continuum approach conceptualizes elements as continuous with no discernable parts. Secondly, the lifecycle approach identifies clear conceptual and procedural boundaries between active or current records and inactive or historical records, but a continuum approach sees records processes as more integrated across spacetime. In the continuum it is recordkeeping processes that carry records forward through spacetime to enable their use for multiple purposes. What this means is that records are always \"in a state of always becoming...\", and able to contribute new contexts via the recordkeeping processes that occur with them. Archival records are therefore not just historical, but are able to be re-interpreted, re-created, and re-contextualized according to their place and use in spacetime. In this way, archival institutions are nodes in the network of recorded information and its contexts, rather than the end point in a lifecycle stage for records that are managed as \"relics\".\n\nThe RCM is a representation of what is commonly referred to as records continuum theory, as well as Australian continuum thinking and/or approaches. These ideas were evolved as part of an Australian approach to archival management espoused by Ian Maclean, Chief Archivist of the Commonwealth Archives Office in Australia in the 1950s and 1960s. Maclean, whose ideas and practices were the subject of the first RCRG publication in 1994, referred in a 1959 \"American Archivist\" article to a \"continuum of (public) records administration\" from administrative efficiency through recordkeeping to the safe keeping of a \"cultural end-product\". Maclean’s vision challenged the divide between current recordkeeping and archival practice. Fellow contemporary at the Commonwealth Archives Office Peter Scott is also included as a core influence on Australian records continuum theory with his development of the Australian Series System, a registry system that helped identify and document the complex and multiple \"social, functional, provenancial, and documentary relationships\" involved in managing records and recordkeeping processes over spacetime.\n\nFurther influences on the RCRG group include archival professionals and researchers like David Bearman and his work on transactionality and systems thinking, and Terry Cook's ideas about postcustodialism and macroappraisal. Wider influencing ideas include those from philosophers and social theorists Jacques Lacan, Michel Foucault, Jacques Derrida, and Jean-François Lyotard, as well as sociologist Anthony Giddens, with structuration theory being a core component of understanding social interaction over spacetime. Canadian archivist Jay Atherton's critique of the division between records managers and archivists in the 1980s and use of the term \"records continuum\" re-commenced the conversation MacLean began during his career and helped to bring his ideas and this term to Australian records continuum thinking. Atherton's use of the term records continuum has several significant differences in conception, application and heritage when compared to Australian records continuum thinking.\n\nPost-custodiality as an archival concept plays a major role in how the RCM was conceived. This term was born from an identified and urgent need to address the complexities of computer technologies on records creation and management over time and space. Post-custodiality is discussed by Frank Upward and Sue McKemmish in 1994 as part of an exploration of changes in archival discourse commencing in the 1980s by Gerald Ham and expanded on by Terry Cook as part of a \"post-custodial paradigm shift\". Post-custodiality in relation to the RCM is explored by Upward and McKemmish as an entry point into a wider conversation about records and recordkeeping being part of a process in which archival institutions have a part to play beyond that of the archival authority handling, appraisal, describing and arranging physical objects in their custody.\n\nDrawing from the above theoretical foundations, the RCM as a framework acknowledges the central role that recordkeeping activities have on the creation, capture, organization and ongoing management of records over time and throughout spaces such as organizations and institutional archives. Recordkeeping is a practice and a concept clearly defined in the archival and records literature by continuum writers as \"a broad and inclusive concept of integrated recordkeeping and archiving processes for current, regulatory, and historical recordkeeping purposes\". Recordkeeping refers to the activities performed on records that add new contexts such as capturing a record into a system, adding metadata, or selecting it for an archive. In the RCM records are therefore not defined according to their status as objects. Rather, records are understood as being part of a continuum of activity related to known (as well as potentially unknown) contexts. A record (as well as records, collections and archives) are therefore part of larger social, cultural, political, legal and archival processes. It is these contexts that are vital to understanding the role, value and evidential qualities of records in and across spacetime (past, present and potential future).\n\nThe RCM is the most well-known of all the continuum models created, but does not exist in isolation. Several other complementary models have been created by RCM creator Frank Upward, and there are others created by continuum researchers that offer enhanced or alternative ways of understanding the continuum.\n\nThe series of continuum models created by Frank Upward include:\n\nModels created in collaboration:\n\nOther models:\n\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
{"id": "1241988", "url": "https://en.wikipedia.org/wiki?curid=1241988", "title": "System of measurement", "text": "System of measurement\n\nA system of measurement is a collection of units of measurement and rules relating them to each other. Systems of measurement have historically been important, regulated and defined for the purposes of science and commerce. Systems of measurement in modern use include the metric system, the imperial system, and United States customary units.\n\nThe French Revolution gave rise to the metric system, and this has spread around the world, replacing most customary units of measure. In most systems, length (distance), mass, and time are \"base quantities\".\n\nLater science developments showed that either electric charge or electric current could be added to extend the set of base quantities by which many other metrological units could be easily defined. (However, electrical units are not necessary for such a set. Gaussian units, for example, have only length, mass, and time as base quantities, and the ampere is defined in terms of other units.) Other quantities, such as power and speed, are derived from the base set: for example, speed is distance per unit time. Historically a wide range of units was used for the same type of quantity: in different contexts, length was measured in inches, feet, yards, fathoms, rods, chains, furlongs, miles, nautical miles, stadia, leagues, with conversion factors which were not powers of ten. Such arrangements were satisfactory in their own contexts.\n\nThe preference for a more universal and consistent system (based on more rational base units) only gradually spread with the growth of science. Changing a measurement system has substantial financial and cultural costs which must be offset against the advantages to be obtained from using a more rational system. However pressure built up, including from scientists and engineers for conversion to a more rational, and also internationally consistent, basis of measurement.\n\nIn antiquity, \"systems of measurement\" were defined locally: the different units might be defined independently according to the length of a king's thumb or the size of his foot, the length of stride, the length of arm, or maybe the weight of water in a keg of specific size, perhaps itself defined in \"hands\" and \"knuckles\". The unifying characteristic is that there was some definition based on some standard. Eventually \"cubits\" and \"strides\" gave way to \"customary units\" to meet the needs of merchants and scientists.\n\nIn the metric system and other recent systems, a single basic unit is used for each base quantity. Often secondary units (multiples and submultiples) are derived from the basic units by multiplying by powers of ten, i.e. by simply moving the decimal point. Thus the basic metric unit of length is the metre; a distance of 1.234 m is 1,234 millimetres, or 0.001234 kilometres.\n\nMetrication is complete or nearly complete in almost all countries. US customary units are heavily used in the United States and to some degree in Liberia. Traditional Burmese units of measurement are used in Burma. U.S. units are used in limited contexts in Canada due to the large volume of trade; there is also considerable use of Imperial weights and measures, despite \"de jure\" Canadian conversion to metric.\n\nA number of other jurisdictions have laws mandating or permitting other systems of measurement in some or all contexts, such as the United Kingdom – whose road signage legislation, for instance, only allows distance signs displaying imperial units (miles or yards) – or Hong Kong.\n\nIn the United States, metric units are used almost universally in science, widely in the military, and partially in industry, but customary units predominate in household use. At retail stores, the liter is a commonly used unit for volume, especially on bottles of beverages, and milligrams, rather than grains, are used for medications. Some other standard non-SI units are still in international use, such as nautical miles and knots in aviation and shipping.\n\nMetric systems of units have evolved since the adoption of the first well-defined system in France in 1795. During this evolution the use of these systems has spread throughout the world, first to non-English-speaking countries, and then to English speaking countries.\n\nMultiples and submultiples of metric units are related by powers of ten and their names are formed with prefixes. This relationship is compatible with the decimal system of numbers and it contributes greatly to the convenience of metric units.\n\nIn the early metric system there were two base units, the metre for length and the gram for mass. The other units of length and mass, and all units of area, volume, and derived units such as density were derived from these two base units.\n\nMesures usuelles (French for \"customary measurements\") were a system of measurement introduced as a compromise between the metric system and traditional measurements. It was used in France from 1812 to 1839.\n\nA number of variations on the metric system have been in use. These include gravitational systems, the centimetre–gram–second systems (cgs) useful in science, the metre–tonne–second system (mts) once used in the USSR and the metre–kilogram–second system (mks).\n\nThe current international standard metric system is the International System of Units (\"Système international d'unités\" or SI) It is an mks system based on the metre, kilogram and second as well as the kelvin, ampere, candela, and mole.\n\nThe SI includes two classes of units which are defined and agreed internationally. The first of these classes includes the seven SI base units for length, mass, time, temperature, electric current, luminous intensity and amount of substance. The second class consists of the SI derived units. These derived units are defined in terms of the seven base units. All other quantities (e.g. work, force, power) are expressed in terms of SI derived units.\n\nBoth imperial units and US customary units derive from earlier English units. Imperial units were mostly used in the former British Empire and the British Commonwealth, but in all these countries they have been largely supplanted by the metric system. They are still used for some applications in the United Kingdom but have been mostly replaced by the metric system in commercial, scientific, and industrial applications.\nUS customary units, however, are still the main system of measurement in the United States. While some steps towards metrication have been made (mainly in the late 1960s and early 1970s), the customary units have a strong hold due to the vast industrial infrastructure and commercial development.\n\nWhile imperial and US customary systems are closely related, there are a number of differences between them. Units of length and area (the inch, foot, yard, mile etc.) are identical except for surveying purposes. The Avoirdupois units of mass and weight differ for units larger than a pound (lb). The imperial system uses a stone of 14 lb, a long hundredweight of 112 lb and a long ton of 2240 lb. The stone is not used in the US and the hundredweights and tons are short: 100 lb and 2000 lb respectively.\n\nWhere these systems most notably differ is in their units of volume. A US fluid ounce (fl oz), about 29.6 millilitres (ml), is slightly larger than the imperial fluid ounce (about 28.4 ml). However, as there are 16 US fl oz to a US pint and 20 imp fl oz per imperial pint, the imperial pint is about 20% larger. The same is true of quarts, gallons, etc. Six US gallons are a little less than five imperial gallons.\n\nThe Avoirdupois system served as the general system of mass and weight. In addition to this there are the Troy and the Apothecaries' systems. Troy weight was customarily used for precious metals, black powder and gemstones. The troy ounce is the only unit of the system in current use; it is used for precious metals. Although the troy ounce is larger than its Avoirdupois equivalent, the pound is smaller. The obsolete troy pound was divided into 12 ounces, rather than the 16 ounces per pound of the Avoirdupois system. The Apothecaries' system was traditionally used in pharmacology, but has now been replaced by the metric system; it shared the same pound and ounce as the troy system but with different further subdivisions.\n\nNatural units are physical units of measurement defined in terms of universal physical constants in such a manner that selected physical constants take on the numerical value of one when expressed in terms of those units. Natural units are so named because their definition relies on only properties of nature and not on any human construct. Various systems of natural units are possible.\n\nSome other examples are as follows:\n\nNon-standard measurement units, sometimes found in books, newspapers etc., include:\n\n\n\nA unit of measurement that applies to money is called a unit of account in economics and unit of measure in accounting. This is normally a currency issued by a country or a fraction thereof; for instance, the US dollar and US cent ( of a dollar), or the euro and euro cent.\n\nISO 4217 is the international standard describing three letter codes (also known as the currency code) to define the names of currencies established by the International Organization for Standardization (ISO).\n\nThroughout history, many official systems of measurement have been used. While no longer in official use, some of these customary systems are occasionally used in day-to-day life, for instance in cooking.\n\n\n\n"}
{"id": "49605051", "url": "https://en.wikipedia.org/wiki?curid=49605051", "title": "The Circumplex Model of Group Tasks", "text": "The Circumplex Model of Group Tasks\n\nThe Circumplex Model is a graphical representation of emotional states. Fundamentally, it is a circle with pleasant on the left, unpleasant on the right, activation on the top, and deactivation on the bottom. All the other emotions are placed around the circle as combinations of these four basic states. It is based on the theory that people experience emotions as overlapping and ambiguous. Group dynamics are the distinctive behaviors and attitudes observed by people in groups, and the study thereof. It is of most interest in the business world, the workforce, or any other setting where the performance of a group is important. Joseph E McGrath enlarged the circumplex model to include group dynamics, based on the work of Shaw, Carter, Hackman, Steiner, Shiflett, Taylor, Lorge, Davis, Laughlin, and others. There are four quadrants in this model representing: generating a task, choosing correct procedure, conflict resolution, and execution, and again there are subtypes distributed around the circle. He used this model as a research tool to evaluate group task performance.\n\nGroup dynamics involve the influential actions, processes and changes that exist both within and between groups. Group dynamics also involve the scientific study of group processes. Through extensive research in the field of group dynamics, it is now well known that all groups, despite their innumerable differences, possess common properties and dynamics. Social psychological researchers have attempted to organize these commonalities, in order to further understand the genuine nature of group processes.\n\nFor instance, social psychological research indicates that there are numerous goal-related interactions and activities that groups of all sizes undertake . These interactions have been categorized by Robert F. Bales, who spent his entire life attempting to find an answer to the question, \"What do people do when they are in groups?\". To simplify the understanding of group interactions, Bales concluded that all interactions within groups could be categorized as either a \"relationship interaction\" (or socioemotional interaction) or a \"task interaction\".\n\nJust as Bales was determined to identify the basic types of interactions involved in groups, Joseph E. McGrath was determined to identify the various goal-related activities that are regularly displayed by groups. McGrath contributed greatly to the understanding of group dynamics through the development of his circumplex model of group tasks. As intended, McGrath's model effectively organizes all group-related activities by distinguishing between four basic group goals. These goals are referred to as the circumplex model of group task's four quadrants, which are categorized based on the dominant performance process involved in a group's task of interest.\n\nThe four quadrants are as follows: \n\nTo further differentiate the various goal-related group activities, McGrath further sub-divides these four categories, resulting in eight categories in total. The breakdown of these categories is as follows:\n\n1. \"Generating ideas or plans\"\n2. \"Choosing a solution\"\n3. \"Negotiating a solution to a conflict\" \n4. \"Executing a task\" \n\nAccording to McGrath and Kravitz (1982), the four most commonly represented tasks in the group dynamics literature are intellective tasks, decision-making tasks, cognitive conflict tasks and mixed-motive tasks.\n\nThe circumplex model of group tasks takes the organization of goal-related activities a step further by distinguishing between tasks that involve cooperation between group members, cooperation tasks (Types 1, 2, 3 and 8) and tasks that often lead to conflict between group members, conflict tasks (Types 4, 5, 6 and 7). Additionally, McGrath's circumplex model of group tasks also distinguishes between tasks that require action (behavioural tasks) and tasks that require conceptual review (conceptual tasks). 'Behavioural tasks' include Types 1, 6, 7 and 8, while 'conceptual tasks' include Types 2, 3, 4 and 5.\n\nThe circumplex model of group tasks is, evidently, a very detailed and complex model. To allow for a more thorough understanding of its properties, a visual representation of the model has been developed. (Need a diagram of the model)\n\nSince the circumplex model of group tasks is quite detailed and complex, numerous social psychological researchers have attempted to describe the model in various ways to ensure readers obtain an optimal understanding of the model. For instance, according to Stratus and McGrath (1994), the four quadrants and the various task types with which they contain all relate to one another within a two-dimensional space. More specifically, Stratus and McGrath (1994) states that the horizontal dimension of the circumplex model of group tasks visual representation reflect the extent to which a task entails cognitive versus behavioural performance requirements. Likewise, the vertical dimension of the circumplex model of group tasks visual representation reflects the extent and form of interdependence among members.\n"}
