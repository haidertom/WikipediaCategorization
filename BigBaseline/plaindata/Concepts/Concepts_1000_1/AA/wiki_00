{"id": "8934226", "url": "https://en.wikipedia.org/wiki?curid=8934226", "title": "Basic limiting principle", "text": "Basic limiting principle\n\nA Basic Limiting Principle (B.L.P.) is a general principle that limits our explanations metaphysically or epistemologically, and which normally goes unquestioned or even unnoticed in our everyday or scientific thinking. The term was introduced by the philosopher C. D. Broad in his 1949 paper \"The Relevance of Psychical research to Philosophy\":\n\n\"There are certain limiting principles which we unhesitatingly take for granted as the framework within which all our practical activities and our scientific theories are confined. Some of these seem to be self-evident. Others are so overwhelmingly supported by all the empirical facts which fall within the range of ordinary experience and the scientific elaborations of it (including under this heading orthodox psychology) that it hardly enters our heads to question them. Let us call these Basic Limiting Principles.\"\n\nBroad offers nine examples of B.L.P.s, including the principle that there can be no backward causation, that there can be no action at a distance, and that one cannot perceive physical events or material things directly, unmediated by sensations.\n\n"}
{"id": "6672748", "url": "https://en.wikipedia.org/wiki?curid=6672748", "title": "Causal model", "text": "Causal model\n\nA causal model is a conceptual model that describes the causal mechanisms of a system. Causal models can improve study designs by providing clear rules for deciding which independent variables need to be included/controlled for. \n\nThey can allow some questions to be answered from existing observational data without the need for an interventional study such as a randomized controlled trial. Some interventional studies are inappropriate for ethical or practical reasons, meaning that without a causal model, some questions cannot be answered. \n\nCasual models can help with the question of external validity(whether results from one study apply to unstudied populations). Causal models can allow data from multiple studies to be merged (in certain circumstances) to answer questions that cannot be answered by any individual data set.\n\nCausal models are falsifiable, in that if they do not match data, they must be rejected as invalid.\n\nCausal models have found applications in signal processing, epidemiology and machine learning.\n\n Pearl defines a causal model as an ordered triple formula_1, where U is a set of exogenous variables whose values are determined by factors outside the model; V is a set of endogenous variables whose values are determined by factors within the model; and E is a set of structural equations that express the value of each endogenous variable as a function of the values of the other variables in U and V.\n\nAristotle defined a taxonomy of causality, including \"material\", \"formal\", \"efficient\" and \"final\" causes. Hume rejected Aristotle's taxonomy in favor of counterfactuals. At one point, he denied that objects have \"powers\" that make one a cause and another an effect. Later he adopted \"if the first object had not been, the second had never existed\" (\"but-for\" causation).\n\nIn the late 19th century, the discipline of statistics began to form. After a years-long effort to identify causal rules for domains such as biological inheritance, Galton introduced the concept of mean regression (epitomized by the sophomore slump in sports) which later led him to the non-causal concept of correlation. \n\nAs a positivist, Pearson expunged the notion of causality from much of science as an unprovable special case of association and introduced the correlation coefficient as the metric of association. He wrote, \"Force as a cause of motion is exactly the same as a tree god as a cause of growth\" and that causation was only a \"fetish among the inscrutable arcana of modern science\". Pearson founded \"Biometrika\" and the Biometrics Lab at University College London, which became the world leader in statistics.\n\nIn 1908 Hardy and Weinberg solved the problem of trait stability that had led Galton to abandon causality, by invoking Mendelian inheritance.\n\nIn 1921 Wright's path analysis became the theoretical ancestor of causal modeling and causal graphs. He developed this approach while attempting to untangle the relative impacts of heredity, development and environment on guinea pig coat patterns. He backed up his heretical claims by showing how such analyses could explain the relationship between guinea pig birth weight, in utero time and litter size. Opposition to these ideas by prominent statisticians led them to be ignored for the following 40 years (except among animal breeders). Instead scientists relied on correlations, partly at the behest of Wright's critic (and leading statistician), Fisher. One exception was Burks, a student who in 1926 was the first to apply path diagrams to represent a mediator and to assert that holding a mediator constant induces errors. She may have invented path diagrams independently. \n\nIn 1923, Neyman introduced the concept of a potential outcome, but his paper was not translated from Polish to English until 1990. \n\nIn 1958 Cox wrote warned that controlling for a variable Z is valid only if it is highly unlikely to be affected independent variables. \n\nIn the 1960s, Duncan, Blalock, Goldberger and others rediscovered path analysis. While reading Blalock's work on path diagrams, Duncan remembered a lecture by Ogburn twenty years earlier that mentioned a paper by Wright that mentioned Burk. \n\nSociologists called causal models structural equation modeling, but once it became a rote method, it lost its utility, leading some practitioners to reject any relationship to causality. Economists adopted the algebraic part of path analysis, calling it simultaneous equation modeling. However, economists still avoided attributing causal meaning to their equations.\n\nSixty years after his first paper, Wright published a piece that recapitulated it, following Karlin et al.'s critique, which objected that it handled only linear relationships and that robust, model-free presentations of data are more revealing.\n\nIn 1973 Lewis advocated replacing correlation with but-for causality (counterfactuals). He referred to humans' ability to envision alternative worlds in which a cause did or not occur and in which effect an appeared only following its cause. In 1974 Rubin introduced the notion of \"potential outcomes\" as a language for asking causal questions. \n\nIn 1983 Cartwright proposed that any factor that is \"causally relevant\" to an effect be conditioned on, moving beyond simple probability as the only guide. \n\nIn 1986 Baron and Kenny introduced principles for detecting and evaluating mediation in a system of linear equations. As of 2014 their paper was the 33rd most-cited of all time. That year Greenland and Robins introduced the \"exchangeability\" approach to handling confounding by considering a counterfactual. They proposed assessing what would have happened to the treatment group if they had not received the treatment and comparing that outcome to that of the control group. If they matched, confounding was said to be absent. \n\nPearl's causal metamodel involves a three-level abstraction he calls the ladder of causation. The lowest level, Association (seeing/observing), entails the sensing of regularities or patterns in the input data, expressed as correlations. The middle level, Intervention (doing), predicts the effects of deliberate actions, expressed as causal relationships. The highest level, Counterfactuals (imagining), involves constructing a theory of (part of) the world that explains why specific actions have specific effects and what happens in he absence of such actions.\n\nOne object is associated with another if observing one changes the probability of observing the other. Example: shoppers who buy toothpaste are more likely to also buy dental floss. Mathematically: \n\nor the probability of (purchasing) floss given (the purchase of) toothpaste. Associations can also be measured via computing the correlation of the two events. Associations have no causal implications. One event could cause the other, the reverse could be true, or both events could be caused by some third event (unhappy hygenist shames shopper into treating their mouth better ).\n\nThis level asserts specific causal relationships between events. Causality is assessed by experimentally performing some action that affects one of the events. Example: doubling the price of toothpaste (then what happens). Causality cannot be established by examining history (of price changes) because the price change may have been for some other reason that could itself affect the second event (a tariff that increases the price of both goods). Mathematically: \n\nwhere \"do\" is an operator that signals the experimental intervention (doubling the price).\n\nThe highest, counterfactual, level involves consideration of an alternate version of a past event. Example: What is the probability that If a store had doubled the price of floss, the toothpaste-purchasing shopper would still have bought it? Answering yes asserts the existence of a causal relationship. Models that can answer counterfactuals allow precise interventions whose consequences can be predicted. At the extreme, such models are accepted as physical laws (as in the laws of physics, e.g., inertia, which says that if force is not applied to a stationary object, it will not move).\n\nStatistics revolves around the analysis of relationships among multiple variables. Traditionally, these relationships are described as correlations, associations without any implied causal relationships. Causal models attempt to extend this framework by adding the notion of causal relationships, in which changes in one variable cause changes in others.\n\nTwentieth century definitions of causality relied purely on probabilities/associations. One event (X) was said to cause another if it raises the probability of the other (Y). Mathematically this is expressed as: \n\nA later definition attempted to address this ambiguity by conditioning on background factors. Mathematically: \n\nOther attempts to define causality include Granger causality, a statistical hypothesis test that causality (in economics) can be assessed by measuring the ability to predict the future values of one time series using prior values of another time series.\n\nA cause can be necessary, sufficient, contributory or some combination.\n\nFor \"x\" to be a necessary cause of \"y\", the presence of \"y\" must imply the prior occurrence of \"x\". The presence of \"x\", however, does not imply that \"y\" will occur. Necessary causes are also known as \"but-for\" causes, as in \"y\" would not have occurred but for the occurrence of \"x\". \n\nFor \"x\" to be a sufficient cause of \"y\", the presence of \"x\" must imply the subsequent occurrence of \"y\". However, another cause \"z\" may independently cause \"y\". Thus the presence of \"y\" does not require the prior occurrence of \"x\".\n\nFor \"x\" to be a contributory cause of \"y\", the presence of \"x\" must increase the likelihood of \"y\". If the likelihood is 100%, then \"x\" is instead called sufficient. A contributory cause may also be necessary.\n\nA causal diagram is a directed graph that displays causal relationships between variables in a causal model. A causal diagram includes a set of variables (or nodes). Each node is connected by an arrow to one or more other nodes upon which it has a causal influence. An arrowhead delineates the direction of causality, e.g., an arrow connecting variables A and B with the arrowhead at B indicates that a change in A causes a change in B (with an associated probability).\n\nCausal diagrams include causal loop diagrams, directed acyclic graphs, and Ishikawa diagrams.\n\nCasual diagrams are independent of the quantitative probabilities that inform them. Changes to those probabilities (e.g., due to technological improvements) do not require changes to the model.\n\nCausal models have formal structures with elements with specific properties.\n\nThe three types of connections of three nodes are linear chains, branching forks and merging colliders.\n\nChains are straight line connections with arrows pointing from cause to effect. In this model, B is a mediator in that it mediates the change that A would otherwise have on C. \n\nIn forks, one cause has multiple effects. The two effects have a common cause. Conditioning on B (for a specific value of B) reveals a positive correlation between A and C that is not causal. \n\nAn elaboration of a fork is the confounder:\n\nIn such models, B is a common cause of A and C (which also causes A), making B the confounder. \n\nIn colliders, multiple causes affect one outcome. Conditioning on B (for a specific value of B) often reveals a non-causal negative correlation between A and C. This negative correlation has been called collider bias and the \"explain-away\" effect as in, B explains away the correlation between A and C. The correlation can be positive in the case where contributions from both A and C are necessary to affect B. \n\nA mediator node modifies the effect of other causes on an outcome (as opposed to simply affecting the outcome). \n\nA confounder node affects multiple outcomes, creating a positive correlation among them.\n\nAn instrumental variable is one that: \n\n\nRegression coefficients can serve as estimates of the causal effect of an instrumental variable on an outcome as long as that effect is not confounded. In this way, instrumental variables allow causal factors to be quantified without data on confounders. \n\nFor example, given the model:\n\nZ is an independent variable, because it has a path to the outcome Y and is unconfounded, e.g., by U.\n\nDefinition: In the above example, if Z and X take binary values, then the assumption that Z = 0, X = 1 does not occur is called monotonicity. \n\nRefinements to the technique include creating an instrument by conditioning on other variable to block the paths between the instrument and the confounder and combining multiple variables to form a single instrument. \n\nDefinition: Mendelian randomization uses measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in observational studies.\n\nBecause genes vary randomly across populations, presence of a gene typically qualifies as an instrumental variable, implying that in many cases, causality can be quantified using regression on an observational study.\n\nIndependence conditions are rules for deciding whether two variables are independent of each other. Variables are independent if the values of one do not directly affect the values of the other. Multiple causal models can share independence conditions. For example, the models\n\nand \n\nhave the same independence conditions, because conditioning on B leaves A and C independent. However, the two models do not have the same meaning and can be falsified based on data. (If observations show an association between A and C after conditioning on B, then both models are incorrect). Conversely, data cannot show which of these two models are correct, because they have the same independence conditions. Conditioning on a variable is a mechanism for conducting hypothetical experiments. Conditioning on a variable involves analyzing the values of other variables for a given value of the conditioned variable. In the first example, conditioning on B implies that observations for a given value of B should show no correlation between A and C. If such a correlation exists, then the model is incorrect. Non-causal models cannot make such distinctions, because they do not make causal assertions. \n\nAn essential element of correlational study design is to identify potentially confounding influences on the variable under study, such as demographics. These variables are controlled for to eliminate those influences. However, the correct list of confounding variables cannot be determined \"a priori\". It is thus possible that a study may control for irrelevant variables or even (indirectly) the variable under study. \n\nCausal models offer a robust technique for identifying appropriate confounding variables. Formally, Z is a confounder if \"Y is associated with Z via paths not going through X\". These can often be determined using data collected for other studies. Mathematically, if \n\nthen X is a confounder for Y. \n\nEarlier, allegedly incorrect definitions include: \n\n\nThe latter is flawed in that given that in the model: \n\nZ matches the definition, but is a mediator, not a confounder, and is an example of controlling for the outcome.\n\nIn the model \n\nTraditionally, B was considered to be a confounder, because it is associated with X and with Y but is not on a causal path nor is it a descendant of anything on a causal path. Controlling for B causes it to become a confounder. This is known as M-bias. \n\nIn a causal model, the method for identifying all appropriate counfounders (deconfounding) is to block every noncausal path between X and Y without disrupting any causal paths. \n\nDefinition: a backdoor path between two variables X and Y is any path from X to Y that starts with an arrow pointing to X. \n\nX and Y are deconfounded if every backdoor path is blocked and no controlled-for variable Z is descended from X. It is not necessary to control for any variables other than the deconfounders. \n\nDefinition: the backdoor criterion is satisfied when all backdoor paths in a model are blocked.\n\nWhen the causal model is a plausible representation of reality and the backdoor criterion is satisfied, then partial regression coefficients can be used as (causal) path coefficients (for linear relationships). \n\nDefinition: a frontdoor path is a direct causal path for which data is available for all variables. \n\nThe following converts a do expression into a do-free expression by conditioning on the variables along the front-door path. \n\nPresuming data for these observable probabilities is available, the ultimate probability can be computed without an experiment, regardless of the existence of other confounding paths and without backdoor adjustment. \n\nQueries are questions asked based on a specific model. They are generally answered via performing experiments (interventions). Interventions take the form of fixing the value of one variable in a model and observing the result. Mathematically, such queries take the form (from the example): \n\nwhere the \"do\" operator indicates that the experiment explicitly modified the price of toothpaste. Graphically, this blocks any causal factors that would otherwise affect that variable. Diagramatically, this erases all causal arrows pointing at the experimental variable. \n\nMore complex queries are possible, in which the do operator is applied (the value is fixed) to multiple variables.\n\nThe do calculus is the set of manipulations that are available to transform one expression into another, with the general goal of transforming expressions that contain the do operator into expressions that do not. Expressions that do not include the do operator can be estimated from observational data alone, without the need for an experimental intervention, which might be expensive, lengthy or even unethical (e.g., asking subjects to take up smoking). The set of rules is complete (it can be used to derive every true statement in this system). An algorithm can determine whether, for a given model, a solution is computable in polynomial time. \n\nThe calculus includes three rules for the transformation of conditional probability expressions involving the do operator. \n\nRule 1 permits the addition or deletion of observations.:\n\nin the case that the variable set Z blocks all paths from W to Y and all arrows leading into X have been deleted. \n\nRule 2 permits the replacement of an intervention with an observation or vice versa.:\n\nin the case that Z satisfies the back-door criterion. \n\nRule 3 permits the deletion or addition of interventions.:\n\nin the case where no causal paths connect X and Y. \n\nThe rules do not imply that any query can have its do operators removed. In those cases, it may be possible to substitute a variable that is subject to manipulation (e.g., diet) in place of one that is not (e.g., blood cholesterol), which can then be transformed to remove the do. Example: \n\nCounterfactuals consider possibilities that are not found in data, such as whether a nonsmoker would have developed cancer had they instead been a heavy smoker. They are the highest step on Pearl's causality ladder. \n\nDefinition: A potential outcome for a variable Y is \"the value Y would have taken for individual \"u\", had X been assigned the value x\". Mathematically: \n\nThe potential outcome is defined at the level of the individual \"u.\" \n\nThe conventional approach to potential outcomes is data-, not model-driven, limiting its ability to untangle causal relationships. It treats causal questions as problems of missing data and gives incorrect answers to even standard scenarios. \n\nIn the context of causal models, potential outcomes are interpreted causally, rather than statistically.\n\nThe first law of causal inference states that the potential outcome \n\ncan be computed by modifying causal model M (by deleting arrows into X) and computing the outcome for some \"x\". Formally: \n\nExamining a counterfactual using a causal model involves three steps. The approach is valid regardless of the form of the model relationships (linear or otherwise) When the model relationships are fully specified, point values can be computed. In other cases, (e.g., when only probabilities are available) a probability-interval statement (non-smoker x would have a 10-20% chance of cancer) can be computed. \n\nGiven the model:\n\nthe equations for calculating the values of A and C derived from regression analysis or another technique can be applied, substituting known values from an observation and fixing the value of other variables (the counterfactual). \n\nApply abductive reasoning (logical inference that uses observation to find the simplest/most likely explanation) to estimate \"u\", the proxy for the unobserved variables on the specific observation that supports the counterfactual. \n\nFor a specific observation, use the do operator to establish the counterfactual (e.g., \"m\"=0), modifying the equations accordingly. \n\nCalculate the values of the output (\"y\") using the modified equations. \n\nDirect and indirect (mediated) causes can only be distinguished via conducting counterfactuals. Understanding mediation requires holding the mediator constant while intervening on the direct cause. In the model\n\nformula_28\n\nM mediates X's influence on Y, while X also has an unmediated effect on Y. Thus M is held constant, while do(X) is computed.\n\nThe Mediation Fallacy instead involves conditioning on the mediator if the mediator and the outcome are confounded, as they are in the above model.\n\nFor linear models, the indirect effect can be computed by taking the product of all the path coefficients along a mediated pathway. The total indirect effect is computed by the sum of the individual indirect effects. For linear models mediation is indicated when the coefficients of an equation fitted without including the mediator vary significantly from an equation that includes it. \n\nIn experiments on such a model, the controlled direct effect (CDE) is computed by forcing the value of the mediator M (do(M = 0)) and randomly assigning some subjects to each of the values of X (do(X=0), do(X=1), ...) and observing the resulting values of Y. \n\nEach value of the mediator has a corresponding CDE.\n\nHowever, a better experiment is to compute the natural direct effect. (NDE) This is the effect determined by leaving the relationship between X and M untouched while intervening on the relationship between X and Y. \n\nFor example, consider the direct effect of increasing dental hygenist visits (X) from every other year to every year, which encourages flossing (M). Gums (Y) get healthier, either because of the hygenist (direct) or the flossing (mediator/indirect). The experiment is to continue flossing while skipping the hygenist visit.\n\nThe indirect effect of X on Y is the \"increase we would see in Y while holding X constant and increasing M to whatever value M would attain under a unit increase in X\". \n\nIndirect effects cannot be \"controlled\" because the direct path cannot be disabled by holding another variable constant. The natural indirect effect (NIE) is the effect on gum health (Y) from flossing (M). The NIE is calculated as the sum of (floss and no-floss cases) of the difference between the probability of flossing given the hygenist and without the hygenist, or: \n\nThe above NDE calculation includes counterfactual subscripts (formula_32). For nonlinear models, the seemingly obvious equivalence \n\ndoes not apply because of anomalies such as threshold effects and binary values. However, \n\nworks for all model relationships (linear and nonlinear). It allows NDE to then be calculated directly from observational data, without interventions or use of counterfactual subscripts. \n\nCausal models provide a vehicle for integrating data across datasets, known as transport, even though the causal models (and the associated data) differ. E.g., survey data can be merged with randomized, controlled trial data. Transport offers a solution to the question of external validity, whether a study can be applied in a different context.\n\nWhere two models match on all relevant variables and data from one model is known to be unbiased, data from one population can be used to draw conclusions about the other. In other cases, where data is known to be biased, reweighting can allow the dataset to be transported. In a third case, conclusions can be drawn from an incomplete dataset. In some cases, data from studies of multiple populations can be combined (via transportation) to allow conclusions about an unmeasured population. In some cases, combining estimates (e.g., P(W|X)) from multiple studies can increase the precision of a conclusion. \n\nDo-calculus provides a general criterion for transport: A target variable can be transformed into another expression via a series of do-operations that does not involve any \"difference-producing\" variables (those that distinguish the two populations). An analogous rule applies to studies that have relevantly different participants. \n\nAny causal model can be implemented as a Bayesian network. Bayesian networks can be used to provide the inverse probability of an event (given an outcome, what are the probabilities of a specific cause). This requires preparation of a conditional probability table, showing all possible inputs and outcomes with their associated probabilities. \n\nFor example, given a two variable model of Disease and Test (for the disease) the conditional probability table takes the form: \nAccording to this table, when a patient does not have the disease, the probability of a positive test is 12%.\n\nWhile this is tractable for small problems, as the number of variables and their associated states increase, the probability table (and associated computation time) increases exponentially. \n\nBayesian networks are used commercially in applications such as wireless data error correction and DNA analysis. \n\n\n"}
{"id": "3699685", "url": "https://en.wikipedia.org/wiki?curid=3699685", "title": "Conceptual architecture", "text": "Conceptual architecture\n\nConceptual architecture is a form of architecture that utilizes conceptualism, characterized by an introduction of ideas or concepts from outside of architecture often as a means of expanding the discipline of architecture. This produces an essentially different kind of building than one produced by the widely held 'architect as a master-builder' model, in which craft and construction are the guiding principles. The finished building as product is less important in conceptual architecture, than the ideas guiding them, ideas represented primarily by texts, diagrams, or art installations. Architects that work in this vein are Diller + Scofidio, Bernard Tschumi, Peter Eisenman and Rem Koolhaas.\n\nConceptual architecture was studied in the essay, \"Notes on Conceptual Architecture: Towards a Definition\" by Peter Eisenman in 1970, and again by the Harvard Design Magazine in Fall of 2003 and Winter 2004, by a series of articles under the heading \"Architecture as Conceptual Art\". But the understanding of design as a construction of a concept was understood by many modernist architects as well. To quote Louis Kahn on Frank Lloyd Wright:\n\n\n"}
{"id": "17306829", "url": "https://en.wikipedia.org/wiki?curid=17306829", "title": "Conceptual necessity", "text": "Conceptual necessity\n\nConceptual necessity is a property of the certainty with which a state of affairs, as presented by a certain description, occurs: it occurs by conceptual necessity if and only if it occurs just by virtue of the meaning of the description. If someone is a bachelor, for instance, then he is bound to be unmarried by conceptual necessity, because the meaning of the word \"bachelor\" determines that he is.\n\nAlternatively, there is metaphysical necessity, which is a certainty determined, not by the meaning of a description, but instead by facts in the world described.\n\nHistorically, Baruch Spinoza was a subscriber to this belief.\n\n"}
{"id": "7327", "url": "https://en.wikipedia.org/wiki?curid=7327", "title": "Copernican principle", "text": "Copernican principle\n\nIn physical cosmology, the Copernican principle is an alternative name for the principle of relativity, stating that humans, on the Earth or in the Solar system, are not privileged observers of the universe.\n\nNamed for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus's argument of a moving Earth. In some sense, it is equivalent to the mediocrity principle.\n\nHermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets can be explained by reference to an assumption that the Sun and not Earth is centrally located and stationary. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as \"demoting\" Earth from its central role it had in the Ptolemaic geocentric model, it was successors to Copernicus, notably the 16th century Giordano Bruno who adopted this new perspective. The earth's central position had been interpreted as being in the \"lowest and filthiest parts\". Instead, as Galileo said, the earth is part of the \"dance of the stars\" rather than the \"sump where the universe's filth and ephemera collect\". In the late 20th Century, Carl Sagan asked, \"Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.\"\n\nIn cosmology, if one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from our vantage point on Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle. In practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. It becomes more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 200 million parsecs. However, on scales comparable to the radius of the observable universe, we see systematic changes with distance from Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. While this might suggest that Earth is at the center of the universe, the Copernican principle requires us to interpret it as evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach us and show us the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.\n\nModern mathematical cosmology is based on the assumption that the Cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations.\n\nMichael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: \"It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe.\"\n\nBondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.\n\nSince the 1990s the term has been used (interchangeably with \"the Copernicus method\") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.\n\nThe Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the Cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\n\nBefore the term Copernican principle was even coined, Earth was repeatedly shown not to have any special location in the universe. The Copernican Revolution dethroned Earth to just one of many planets orbiting the Sun. Proper motion was mentioned by Halley. William Herschel found that the Solar System is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that our galaxy is just one of many galaxies in the universe. Examination of our galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.\n\nRecent and planned tests relevant to the cosmological and Copernican principles include:\n\nThe standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle and observations are largely consistent but there are always unsolved problems. Some cosmologists and theoretical physicists design models lacking the Cosmological or Copernican principles, to constrain the valid values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.\n\nA prominent example in this context is the observed accelerating universe and the cosmological constant issue. An alternative proposal to dark energy is that the universe is much more inhomogeneous than currently assumed, and specifically that we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.\n"}
{"id": "59863", "url": "https://en.wikipedia.org/wiki?curid=59863", "title": "Correspondence principle", "text": "Correspondence principle\n\nIn physics, the correspondence principle states that the behavior of systems described by the theory of quantum mechanics (or by the old quantum theory) reproduces classical physics in the limit of large quantum numbers. In other words, it says that for large orbits and for large energies, quantum calculations must agree with classical calculations.\n\nThe principle was formulated by Niels Bohr in 1920, though he had previously made use of it as early as 1913 in developing his model of the atom.\n\nThe term codifies the idea that a new theory should reproduce under some conditions the results of older well-established theories in those domains where the old theories work. This concept is somewhat different from the requirement of a formal limit under which the new theory reduces to the older, thanks to the existence of a deformation parameter. \n\nClassical quantities appear in quantum mechanics in the form of expected values of observables, and as such the Ehrenfest theorem (which predicts the time evolution of the expected values) lends support to the correspondence principle.\n\nThe rules of quantum mechanics are highly successful in describing microscopic objects, atoms and elementary particles. But \"macroscopic systems,\" like springs and capacitors, are accurately described by classical theories like classical mechanics and classical electrodynamics. If quantum mechanics were to be applicable to macroscopic objects, there must be some limit in which quantum mechanics reduces to classical mechanics. \"Bohr's correspondence principle demands that classical physics and quantum physics give the same answer when the systems become large\". A. Sommerfeld (1924) referred to the principle as \"Bohrs Zauberstab\" (Bohr's magic wand).\n\nThe conditions under which quantum and classical physics agree are referred to as the correspondence limit, or the classical limit. Bohr provided a rough prescription for the correspondence limit: it occurs \"when the quantum numbers describing the system are large\". A more elaborated analysis of quantum-classical correspondence (QCC) in wavepacket spreading leads to the distinction between robust \"restricted QCC\" and fragile \"detailed QCC\". \"Restricted QCC\" refers to the first two moments of the probability distribution and is true even when the wave packets diffract, while \"detailed QCC\" requires smooth potentials which vary over scales much larger than the wavelength, which is what Bohr considered.\n\nThe post-1925 new quantum theory came in two different formulations. In matrix mechanics, the correspondence principle was built in and was used to construct the theory. In the Schrödinger approach classical behavior is not clear because the waves spread out as they move. Once the Schrödinger equation was given a probabilistic interpretation, Ehrenfest showed that Newton's laws hold on average: the quantum statistical expectation value of the position and momentum obey Newton's laws.\n\nThe correspondence principle is one of the tools available to physicists for selecting quantum theories corresponding to reality. The principles of quantum mechanics are broad: states of a physical system form a complex vector space and physical observables are identified with Hermitian operators that act on this Hilbert space. The correspondence principle limits the choices to those that reproduce classical mechanics in the correspondence limit.\n\nBecause quantum mechanics only reproduces classical mechanics in a statistical interpretation, and because the statistical interpretation only gives the probabilities of different classical outcomes, Bohr has argued that quantum physics does not reduce to classical mechanics similarly as classical mechanics emerges as an approximation of special relativity at small velocities. He argued that classical physics exists independently of quantum theory and cannot be derived from it. His position is that it is inappropriate to understand the experiences of observers using purely quantum mechanical notions such as wavefunctions because the different states of experience of an observer are defined classically, and do not have a quantum mechanical analog. The relative state interpretation of quantum mechanics is an attempt to understand the experience of observers using only quantum mechanical notions. Niels Bohr was an early opponent of such interpretations.\n\nMany of these conceptual problems, however, resolve in the phase-space formulation of quantum mechanics, where the \"same variables with the same interpretation\" are utilized to describe both quantum and classical mechanics.\n\nThe term \"correspondence principle\" is used in a more general sense to mean the reduction of a new scientific theory to an earlier scientific theory in appropriate circumstances. This requires that the new theory explain all the phenomena under circumstances for which the preceding theory was known to be valid, the \"correspondence limit\".\n\nFor example, \n\nIn order for there to be a correspondence, the earlier theory has to have a domain of validity—it must work under \"some\" conditions. Not all theories have a domain of validity. For example, there is no limit where Newton's mechanics reduces to Aristotle's mechanics because Aristotle's mechanics, although academically dominant for 18 centuries, does not have any domain of validity.\n\nIf an electron in an atom is moving on an orbit with period , classically the electromagnetic radiation will repeat itself every orbital period. If the coupling to the electromagnetic field is weak, so that the orbit doesn't decay very much in one cycle, the radiation will be emitted in a pattern which repeats every period, so that the Fourier transform will have frequencies which are only multiples of . This is the classical radiation law: the frequencies emitted are integer multiples of .\n\nIn quantum mechanics, this emission must be in quanta of light, of frequencies consisting of integer multiples of , so that classical mechanics is an approximate description at large quantum numbers. This means that the energy level corresponding to a classical orbit of period must have nearby energy levels which differ in energy by , and they should be equally spaced near that level,\n\nBohr worried whether the energy spacing 1/ should be best calculated with the period of the energy state formula_2, or formula_3, or some average—in hindsight, this model is only the leading semiclassical approximation.\n\nBohr considered circular orbits. Classically, these orbits must decay to smaller circles when photons are emitted. The level spacing between circular orbits can be calculated with the correspondence formula. For a Hydrogen atom, the classical orbits have a period determined by Kepler's third law to scale as . The energy scales as , so the level spacing formula amounts to\nIt is possible to determine the energy levels by recursively stepping down orbit by orbit, but there is a shortcut.\n\nThe angular momentum of the circular orbit scales as . The energy in terms of the angular momentum is then\n\nAssuming, with Bohr, that quantized values of are equally spaced, the spacing between neighboring energies is\nThis is as desired for equally spaced angular momenta. If one kept track of the constants, the spacing would be , so the angular momentum should be an integer multiple of ,\n\nThis is how Bohr arrived at his model. Since only the level \"spacing\" is determined heuristically by the correspondence principle, one could always add a small fixed offset to the quantum number— could just as well have been . \n\nBohr used his physical intuition to decide which quantities were best to quantize. It is a testimony to his skill that he was able to get so much from what is only the leading order approximation. A less heuristic treatment accounts for needed offsets in the ground state L, cf. Wigner–Weyl transform.\n\nBohr's correspondence condition can be solved for the level energies in a general one-dimensional potential. Define a quantity which is a function only of the energy, and has the property that\n\nThis is the analog of the angular momentum in the case of the circular orbits. The orbits selected by the correspondence principle are the ones that obey for integer, since\n\nThis quantity is canonically conjugate to a variable which, by the Hamilton equations of motion changes with time as the gradient of energy with . Since this is equal to the inverse period at all times, the variable increases steadily from 0 to 1 over one period.\n\nThe angle variable comes back to itself after 1 unit of increase, so the geometry of phase space in coordinates is that of a half-cylinder, capped off at = 0, which is the motionless orbit at the lowest value of the energy. These coordinates are just as canonical as , but the orbits are now lines of constant instead of nested ovoids in space.\n\nThe area enclosed by an orbit is invariant under canonical transformations, so it is the same in space as in . But in the coordinates, this area is the area of a cylinder of unit circumference between 0 and , or just . So is equal to the area enclosed by the orbit in coordinates too,\n\nThe quantization rule is that the action variable is an integer multiple of .\n\nBohr's correspondence principle provided a way to find the semiclassical quantization rule for a one degree of freedom system. It was an argument for the old quantum condition mostly independent from the one developed by Wien and Einstein, which focused on adiabatic invariance. But both pointed to the same quantity, the action.\n\nBohr was reluctant to generalize the rule to systems with many degrees of freedom. This step was taken by Sommerfeld, who proposed the general quantization rule for an integrable system,\n\nEach action variable is a separate integer, a separate quantum number.\n\nThis condition reproduces the circular orbit condition for two dimensional motion: let be polar coordinates for a central potential. Then is already an angle variable, and the canonical momentum conjugate is , the angular momentum. So the quantum condition for reproduces Bohr's rule:\n\nThis allowed Sommerfeld to generalize Bohr's theory of circular orbits to elliptical orbits, showing that the energy levels are the same. He also found some general properties of quantum angular momentum which seemed paradoxical at the time. One of these results was that the z-component of the angular momentum, the classical inclination of an orbit relative to the z-axis, could only take on discrete values, a result which seemed to contradict rotational invariance. This was called \"space quantization\" for a while, but this term fell out of favor with the new quantum mechanics since no quantization of space is involved.\n\nIn modern quantum mechanics, the principle of superposition makes it clear that rotational invariance is not lost. It is possible to rotate objects with discrete orientations to produce superpositions of other discrete orientations, and this resolves the intuitive paradoxes of the Sommerfeld model.\n\nHere is a demonstration\nof how large quantum numbers can give rise to classical (continuous) behavior.\n\nConsider the one-dimensional quantum harmonic oscillator. Quantum mechanics tells us that the total (kinetic and potential) energy of the oscillator, , has a set of discrete values,\nwhere is the angular frequency of the oscillator.\n\nHowever, in a classical harmonic oscillator such as a lead ball attached to the end of a spring, we do not perceive any discreteness. Instead, the energy of such a macroscopic system appears to vary over a continuum of values. We can verify that our idea of macroscopic systems fall within the correspondence limit. The energy of the classical harmonic oscillator with amplitude , is\n\nThus, the quantum number has the value\n\nIf we apply typical \"human-scale\" values = 1kg, = 1 rad/s, and = 1 m, then ≈ 4.74×10. This is a very large number, so the system is indeed in the correspondence limit.\n\nIt is simple to see why we perceive a continuum of energy in this limit. With = 1 rad/s, the difference between each energy level is ≈ 1.05 × 10J, well below what we normally resolve for macroscopic systems. One then describes this system through an emergent classical limit.\n\nHere we show that the expression of kinetic energy from special relativity becomes arbitrarily close to the classical expression, for speeds that are much slower than the speed of light, .\n\nEinstein's mass-energy equation\nwhere the velocity, is the velocity of the body relative to the observer, formula_17 is the \"rest\" mass (the observed mass of the body at zero velocity relative to the observer), and is the speed of light.\n\nWhen the velocity vanishes, the energy expressed above is not zero, and represents the \"rest\" energy,\n\nWhen the body is in motion relative to the observer, the total energy exceeds the rest energy by an amount that is, by definition, the \"kinetic\" energy,\n\nUsing the approximation\nwe get, when speeds are much slower than that of light, or , \n\nwhich is the Newtonian expression for kinetic energy.\n"}
{"id": "26167139", "url": "https://en.wikipedia.org/wiki?curid=26167139", "title": "Definitionism", "text": "Definitionism\n\nDefinitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.\n"}
{"id": "21899301", "url": "https://en.wikipedia.org/wiki?curid=21899301", "title": "Disquotational principle", "text": "Disquotational principle\n\nThe disquotational principle is a philosophical theorem which holds that a rational speaker will accept \"p\" if and only if he or she believes \"p\". The quotes indicate that the statement \"p\" is being treated as a sentence, and not as a proposition. This principle is presupposed by claims that hold that substitution fails in certain intensional contexts.\n\nConsider the following argument:\n\nTo derive (3), we have to assume that when Sally accepts that \"Cicero was a famous orator\", she believes that Cicero was a famous orator. Then we can exchange Cicero for Tully, and derive (3). Bertrand Russell thought that this demonstrated the failure of substitutivity of identicals in intensional contexts.\n\nIn \"A Puzzle about Belief,\" Saul Kripke argues that the application of the disquotational theorem can yield a paradox on its own, without appeal to the substitution principle, and that this may show that the problem lies with the former, and not the latter. There are various formulations of this argument.\n\nSuppose that, Pierre, a Frenchman, comes to believe that (1) \"Londres est jolie\" (London is pretty), without ever having visited the city. Later in life, Pierre ends up living in London. He finds no French speakers there (he does not speak English yet), and everyone refers to the city as \"London,\" not \"Londres\". He finds this city decidedly unattractive, for the neighborhood he decides to live in is decidedly unattractive. Over time, he learns English, and formulates the belief that (2) \"London is not pretty\". Pierre never realizes that London is the English word for \"Londres\". Now with the disquotational principle, we can deduce from (1) that Pierre believes the proposition that \"Londres est jolie\". With a weak principle of translation (e.g., \"a proposition in language A is the same as a semantically identical proposition in language B\" [note that a proposition is not the same as a sentence]), we can now deduce that Pierre believes that London is pretty. But we can also deduce from (2) and the disquotational principle that Pierre believes that London is not pretty. These deductions can be made \"even though Pierre has made no logical blunders in forming his beliefs\". Without the disquotational principle, this contradiction could not be derived, because we would not be able to assume that (1) and (2) meant anything in particular.\n\nThis paradox can also be derived without appeal to another language. Suppose that Pierre assents to the proposition that \"Paderewski had musical talent\", perhaps having heard that this man was a famous pianist. With the disquotational principle, we can deduce that Pierre believes the proposition that Paderewski had musical talent. Now suppose that Pierre overhears a friend discussing the political exploits of a certain statesman, Paderewski, without knowing that the two Paderewskis are the same man. Pierre's background tells him that statesmen are generally not very gifted in music, and this leads him to the belief that Paderewski had no musical talent. The disquotation principle allows us to deduce that Pierre believes the proposition that Paderewski had no musical talent. Using this principle, we have now deduced that Pierre believes that Paderewski had musical talent, and does not believe that Paderewski had musical talent, \"even though Pierre's beliefs were formed logically\".\n\n"}
{"id": "857235", "url": "https://en.wikipedia.org/wiki?curid=857235", "title": "Equivalence principle", "text": "Equivalence principle\n\nIn the theory of general relativity, the equivalence principle is the equivalence of gravitational and inertial mass, and Albert Einstein's observation that the gravitational \"force\" as experienced locally while standing on a massive body (such as the Earth) is the same as the \"pseudo-force\" experienced by an observer in a non-inertial (accelerated) frame of reference.\n\nSomething like the equivalence principle emerged in the early 17th century, when Galileo expressed experimentally that the acceleration of a test mass due to gravitation is independent of the amount of mass being accelerated.\n\nKepler, using Galileo's discoveries, showed knowledge of the equivalence principle by accurately describing what would occur if the moon were stopped in its orbit and dropped towards Earth. This can be deduced without knowing if or in what manner gravity decreases with distance, but requires assuming the equivalency between gravity and inertia.\n\nThe 1/54 ratio is Kepler's estimate of the Moon–Earth mass ratio, based on their diameters. The accuracy of his statement can be deduced by using Newton's inertia law F=ma and Galileo's gravitational observation that distance formula_1. Setting these accelerations equal for a mass is the equivalence principle. Noting the time to collision for each mass is the same gives Kepler's statement that D/D=M/M, without knowing the time to collision or how or if the acceleration force from gravity is a function of distance.\n\nNewton's gravitational theory simplified and formalized Galileo's and Kepler's ideas by recognizing Kepler's \"animal force or some other equivalent\" beyond gravity and inertia were not needed, deducing from Kepler's planetary laws how gravity reduces with distance.\n\nThe equivalence principle was properly introduced by Albert Einstein in 1907, when he observed that the acceleration of bodies towards the center of the Earth at a rate of 1\"\"g\"\" (\"g\" = 9.81 m/s being a standard reference of gravitational acceleration at the Earth's surface) is equivalent to the acceleration of an inertially moving body that would be observed on a rocket in free space being accelerated at a rate of 1\"g\". Einstein stated it thus:\n\nThat is, being on the surface of the Earth is equivalent to being inside a spaceship (far from any sources of gravity) that is being accelerated by its engines. The direction or vector of acceleration equivalence on the surface of the earth is \"up\" or directly opposite the center of the planet while the vector of acceleration in a spaceship is directly opposite from the mass ejected by its thrusters. From this principle, Einstein deduced that free-fall is inertial motion. Objects in free-fall do not experience being accelerated downward (e.g. toward the earth or other massive body) but rather weightlessness and no acceleration. In an inertial frame of reference bodies (and photons, or light) obey Newton's first law, moving at constant velocity in straight lines. Analogously, in a curved spacetime the world line of an inertial particle or pulse of light is \"as straight as possible\" (in space \"and\" time). Such a world line is called a geodesic and from the point of view of the inertial frame is a straight line. This is why an accelerometer in free-fall doesn't register any acceleration; there isn't any.\n\nAs an example: an inertial body moving along a geodesic through space can be trapped into an orbit around a large gravitational mass without ever experiencing acceleration. This is possible because spacetime is radically curved in close vicinity to a large gravitational mass. In such a situation the geodesic lines bend inward around the center of the mass and a free-floating (weightless) inertial body will simply follow those curved geodesics into an elliptical orbit. An accelerometer on-board would never record any acceleration.\n\nBy contrast, in Newtonian mechanics, gravity is assumed to be a force. This force draws objects having mass towards the center of any massive body. At the Earth's surface, the force of gravity is counteracted by the mechanical (physical) resistance of the Earth's surface. So in Newtonian physics, a person at rest on the surface of a (non-rotating) massive object is in an inertial frame of reference. These considerations suggest the following corollary to the equivalence principle, which Einstein formulated precisely in 1911:\n\nEinstein also referred to two reference frames, K and K'. K is a uniform gravitational field, whereas K' has no gravitational field but is uniformly accelerated such that objects in the two frames experience identical forces:\n\nThis observation was the start of a process that culminated in general relativity. Einstein suggested that it should be elevated to the status of a general principle, which he called the \"principle of equivalence\" when constructing his theory of relativity:\n\nEinstein combined (postulated) the equivalence principle with special relativity to predict that clocks run at different rates in a gravitational potential, and light rays bend in a gravitational field, even before he developed the concept of curved spacetime.\n\nSo the original equivalence principle, as described by Einstein, concluded that free-fall and inertial motion were physically equivalent. This form of the equivalence principle can be stated as follows. An observer in a windowless room cannot distinguish between being on the surface of the Earth, and being in a spaceship in deep space accelerating at 1g. This is not strictly true, because massive bodies give rise to tidal effects (caused by variations in the strength and direction of the gravitational field) which are absent from an accelerating spaceship in deep space. The room, therefore, should be small enough that tidal effects can be neglected.\n\nAlthough the equivalence principle guided the development of general relativity, it is not a founding principle of relativity but rather a simple consequence of the \"geometrical\" nature of the theory. In general relativity, objects in free-fall follow geodesics of spacetime, and what we perceive as the force of gravity is instead a result of our being unable to follow those geodesics of spacetime, because the mechanical resistance of matter prevents us from doing so.\n\nSince Einstein developed general relativity, there was a need to develop a framework to test the theory against other possible theories of gravity compatible with special relativity. This was developed by Robert Dicke as part of his program to test general relativity. Two new principles were suggested, the so-called Einstein equivalence principle and the strong equivalence principle, each of which assumes the weak equivalence principle as a starting point. They only differ in whether or not they apply to gravitational experiments.\n\nAnother clarification needed is that the equivalence principle assumes a constant acceleration of 1g without considering the mechanics of generating 1g. If we do consider the mechanics of it, then we must assume the aforementioned windowless room has a fixed mass. Accelerating it at 1g means there is a constant force being applied, which = m*g where m is the mass of the windowless room along with its contents (including the observer). Now, if the observer jumps inside the room, an object lying freely on the floor will decrease in weight momentarily because the acceleration is going to decrease momentarily due to the observer pushing back against the floor in order to jump. The object will then gain weight while the observer is in the air and the resulting decreased mass of the windowless room allows greater acceleration; it will lose weight again when the observer lands and pushes once more against the floor; and it will finally return to its initial weight afterwards. To make all these effects equal those we would measure on a planet producing 1g, the windowless room must be assumed to have the same mass as that planet. Additionally, the windowless room must not cause its own gravity, otherwise the scenario changes even further. These are technicalities, clearly, but practical ones if we wish the experiment to demonstrate more or less precisely the equivalence of 1g gravity and 1g acceleration.\n\nThree forms of the equivalence principle are in current use: weak (Galilean), Einsteinian, and strong.\n\nThe weak equivalence principle, also known as the universality of free fall or the Galilean equivalence principle can be stated in many ways. The strong EP includes (astronomic) bodies with gravitational binding energy (e.g., 1.74 solar-mass pulsar PSR J1903+0327, 15.3% of whose separated mass is absent as gravitational binding energy). The weak EP assumes falling bodies are bound by non-gravitational forces only. Either way:\n\nLocality eliminates measurable tidal forces originating from a radial divergent gravitational field (e.g., the Earth) upon finite sized physical bodies. The \"falling\" equivalence principle embraces Galileo's, Newton's, and Einstein's conceptualization. The equivalence principle does not deny the existence of measurable effects caused by a \"rotating\" gravitating mass (frame dragging), or bear on the measurements of light deflection and gravitational time delay made by non-local observers.\n\nBy definition of active and passive gravitational mass, the force on formula_2 due to the gravitational field of formula_3 is:\n\nLikewise the force on a second object of arbitrary mass due to the gravitational field of mass is:\n\nBy definition of inertial mass:\n\nIf formula_7 and formula_8 are the same distance formula_9 from formula_10 then, by the weak equivalence principle, they fall at the same rate (i.e. their accelerations are the same)\n\nHence:\n\nTherefore:\n\nIn other words, passive gravitational mass must be proportional to inertial mass for all objects.\n\nFurthermore, by Newton's third law of motion:\n\nmust be equal and opposite to\n\nIt follows that:\n\nIn other words, passive gravitational mass must be proportional to active gravitational mass for all objects.\n\nThe dimensionless Eötvös-parameter formula_17 is the difference of the ratios of gravitational and inertial masses divided by their average for the two sets of test masses \"A\" and \"B.\"\n\nTests of the weak equivalence principle are those that verify the equivalence of gravitational mass and inertial mass. An obvious test is dropping different objects, ideally in a vacuum environment, e.g., inside the Fallturm Bremen drop tower.\n\nSee:\n\nExperiments are still being performed at the University of Washington which have placed limits on the differential acceleration of objects towards the Earth, the Sun and towards dark matter in the galactic center. Future satellite experiments – STEP (Satellite Test of the Equivalence Principle), Galileo Galilei, and MICROSCOPE (MICROSatellite à traînée Compensée pour l'Observation du Principe d'Équivalence) – will test the weak equivalence principle in space, to much higher accuracy.\n\nWith the first successful production of antimatter, in particular anti-hydrogen, a new approach to test the weak equivalence principle has been proposed. Experiments to compare the gravitational behavior of matter and antimatter are currently being developed.\n\nProposals that may lead to a quantum theory of gravity such as string theory and loop quantum gravity predict violations of the weak equivalence principle because they contain many light scalar fields with long Compton wavelengths, which should generate fifth forces and variation of the fundamental constants. Heuristic arguments suggest that the magnitude of these equivalence principle violations could be in the 10 to 10 range. Currently envisioned tests of the weak equivalence principle are approaching a degree of sensitivity such that \"non-discovery\" of a violation would be just as profound a result as discovery of a violation. Non-discovery of equivalence principle violation in this range would suggest that gravity is so fundamentally different from other forces as to require a major reevaluation of current attempts to unify gravity with the other forces of nature. A positive detection, on the other hand, would provide a major guidepost towards unification.\n\nWhat is now called the \"Einstein equivalence principle\" states that the weak equivalence principle holds, and that:\nHere \"local\" has a very special meaning: not only must the experiment not look outside the laboratory, but it must also be small compared to variations in the gravitational field, tidal forces, so that the entire laboratory is freely falling. It also implies the absence of interactions with \"external\" fields \"other than the gravitational field\".\n\nThe principle of relativity implies that the outcome of local experiments must be independent of the velocity of the apparatus, so the most important consequence of this principle is the Copernican idea that dimensionless physical values such as the fine-structure constant and electron-to-proton mass ratio must not depend on where in space or time we measure them. Many physicists believe that any Lorentz invariant theory that satisfies the weak equivalence principle also satisfies the Einstein equivalence principle.\n\n\"Schiff's conjecture\" suggests that the weak equivalence principle implies the Einstein equivalence principle, but it has not been proven. Nonetheless, the two principles are tested with very different kinds of experiments. The Einstein equivalence principle has been criticized as imprecise, because there is no universally accepted way to distinguish gravitational from non-gravitational experiments (see for instance Hadley and Durand).\n\nIn addition to the tests of the weak equivalence principle, the Einstein equivalence principle can be tested by searching for variation of dimensionless constants and mass ratios. The present best limits on the variation of the fundamental constants have mainly been set by studying the naturally occurring Oklo natural nuclear fission reactor, where nuclear reactions similar to ones we observe today have been shown to have occurred underground approximately two billion years ago. These reactions are extremely sensitive to the values of the fundamental constants.\n\nThere have been a number of controversial attempts to constrain the variation of the strong interaction constant. There have been several suggestions that \"constants\" do vary on cosmological scales. The best known is the reported detection of variation (at the 10 level) of the fine-structure constant from measurements of distant quasars, see Webb et al. Other researchers dispute these findings. Other tests of the Einstein equivalence principle are gravitational redshift experiments, such as the Pound–Rebka experiment which test the position independence of experiments.\n\nThe strong equivalence principle suggests the laws of gravitation are independent of velocity and location. In particular,\nand\nThe first part is a version of the weak equivalence principle that applies to objects that exert a gravitational force on themselves, such as stars, planets, black holes or Cavendish experiments. The second part is the Einstein equivalence principle (with the same definition of \"local\"), restated to allow gravitational experiments and self-gravitating bodies. The freely-falling object or laboratory, however, must still be small, so that tidal forces may be neglected (hence \"local experiment\").\n\nThis is the only form of the equivalence principle that applies to self-gravitating objects (such as stars), which have substantial internal gravitational interactions. It requires that the gravitational constant be the same everywhere in the universe and is incompatible with a fifth force. It is much more restrictive than the Einstein equivalence principle.\n\nThe strong equivalence principle suggests that gravity is entirely geometrical by nature (that is, the metric alone determines the effect of gravity) and does not have any extra fields associated with it. If an observer measures a patch of space to be flat, then the strong equivalence principle suggests that it is absolutely equivalent to any other patch of flat space elsewhere in the universe. Einstein's theory of general relativity (including the cosmological constant) is thought to be the only theory of gravity that satisfies the strong equivalence principle. A number of alternative theories, such as Brans–Dicke theory, satisfy only the Einstein equivalence principle.\n\nThe strong equivalence principle can be tested by searching for a variation of Newton's gravitational constant \"G\" over the life of the universe, or equivalently, variation in the masses of the fundamental particles. A number of independent constraints, from orbits in the solar system and studies of big bang nucleosynthesis have shown that \"G\" cannot have varied by more than 10%.\n\nThus, the strong equivalence principle can be tested by searching for fifth forces (deviations from the gravitational force-law predicted by general relativity). These experiments typically look for failures of the inverse-square law (specifically Yukawa forces or failures of Birkhoff's theorem) behavior of gravity in the laboratory. The most accurate tests over short distances have been performed by the Eöt–Wash group. A future satellite experiment, SEE (Satellite Energy Exchange), will search for fifth forces in space and should be able to further constrain violations of the strong equivalence principle. Other limits, looking for much longer-range forces, have been placed by searching for the Nordtvedt effect, a \"polarization\" of solar system orbits that would be caused by gravitational self-energy accelerating at a different rate from normal matter. This effect has been sensitively tested by the Lunar Laser Ranging Experiment. Other tests include studying the deflection of radiation from distant radio sources by the sun, which can be accurately measured by very long baseline interferometry. Another sensitive test comes from measurements of the frequency shift of signals to and from the Cassini spacecraft. Together, these measurements have put tight limits on Brans–Dicke theory and other alternative theories of gravity.\n\nIn 2014, astronomers discovered a stellar triple system including a millisecond pulsar PSR J0337+1715 and two white dwarfs orbiting it. The system provided them a chance to test the strong equivalence principle in a strong gravitational field with high accuracy.\n\nOne challenge to the equivalence principle is the Brans–Dicke theory. Self-creation cosmology is a modification of the Brans–Dicke theory. The Fredkin Finite Nature Hypothesis is an even more radical challenge to the equivalence principle and has even fewer supporters.\n\nIn August 2010, researchers from the University of New South Wales, Swinburne University of Technology, and Cambridge University published a paper titled \"Evidence for spatial variation of the fine structure constant\", whose tentative conclusion is that, \"qualitatively, [the] results suggest a violation of the Einstein Equivalence Principle, and could infer a very large or infinite universe, within which our 'local' Hubble volume represents a tiny fraction.\"\n\nIn his book \"Einstein's Mistakes\", pages 226–227, Hans C. Ohanian describes several situations which falsify Einstein's Equivalence Principle. Inertial accelerative effects are analogous to, but not equivalent to, gravitational effects. Ohanian cites Ehrenfest for this same opinion.\n\nDutch physicist and string theorist Erik Verlinde has generated a self-contained, logical derivation of the equivalence principle based on the starting assumption of a holographic universe. Given this situation, gravity would not be a true fundamental force as is currently thought but instead an \"emergent property\" related to entropy. Verlinde's entropic gravity theory apparently leads naturally to the correct observed strength of dark energy; previous failures to explain its incredibly small magnitude have been called by such people as cosmologist Michael Turner (who is credited as having coined the term \"dark energy\") as \"the greatest embarrassment in the history of theoretical physics\". However, it should be noted that these ideas are far from settled and still very controversial.\n\n\n\n"}
{"id": "39228396", "url": "https://en.wikipedia.org/wiki?curid=39228396", "title": "Equivalence principle (geometric)", "text": "Equivalence principle (geometric)\n\nThe equivalence principle is one of the corner-stones of gravitation theory. Different formulations of the equivalence principle are labeled \"weakest\", \"weak\", \"middle-strong\" and \"strong.\" All of these formulations are based on the empirical equality of inertial mass, gravitational active and passive charges.\n\nThe \"weakest\" equivalence principle is restricted to the motion law of a probe point mass in a uniform gravitational field. Its localization is the \"weak\" equivalence principle that states the existence of a desired local inertial frame at a given world point. This is the case of equations depending on a gravitational field and its first order derivatives, e. g., the equations of mechanics of probe point masses, and the equations of electromagnetic and Dirac fermion fields. The \"middle-strong\" equivalence principle is concerned with any matter, except a gravitational field, while the \"strong\" one is applied to all physical laws.\n\nThe above-mentioned variants of the equivalence principle aim to guarantee the transition of General Relativity to Special Relativity in a certain reference frame. However, only the particular \"weakest\" and \"weak\" equivalence principles are true. \nTo overcome this difficulty, the equivalence principle can be formulated in geometric terms as follows.\n\nIn the spirit of Felix Klein's Erlanger program, Special Relativity can be characterized as the Klein geometry of Lorentz group invariants. Then the geometric equivalence principle is formulated to require the existence of Lorentz invariants on a world manifold formula_1. This requirement holds if the tangent bundle formula_2 of formula_1 admits an atlas with Lorentz transition functions, i.e., a structure group of the associated frame bundle formula_4 of linear tangent frames in formula_2 is reduced to the Lorentz group formula_6. By virtue of the well known theorem on structure group reduction, this reduction takes place if and only if the quotient bundle formula_7 possesses a global section, which is a pseudo-Riemannian metric on formula_1.\n\nThus the geometric equivalence principle provides the necessary and sufficient conditions of the existence of a pseudo-Riemannian metric, i.e., a gravitational field on a world manifold.\n\nBased on the geometric equivalence principle, gravitation theory is formulated as gauge theory where a gravitational field is described as a classical Higgs field responsible for spontaneous breakdown of space-time symmetries.\n\n\n"}
{"id": "24856902", "url": "https://en.wikipedia.org/wiki?curid=24856902", "title": "Evidential existentiality", "text": "Evidential existentiality\n\nThe principle of evidential existentiality in philosophy is a principle that explains and gives value to the existence of entities. The principle states that the reality of an entity's existence gives greater value to prove its existence than would be given through any outward studies. The principle has become a backbone of the God argument, stating that because God is a self-evident entity, His existence can only be shared by humans, thus proof of God is unnecessary and moot.\nIt appears that the existence is primarily evident to the self only. The God or Supreme self is perceivable to the self. So evidentially self perception is followed by God perception and so on.\n\n"}
{"id": "14162696", "url": "https://en.wikipedia.org/wiki?curid=14162696", "title": "Fluid Concepts and Creative Analogies", "text": "Fluid Concepts and Creative Analogies\n\nFluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought is a 1995 book by Douglas Hofstadter and other members of the Fluid Analogies Research Group exploring the mechanisms of intelligence through computer modeling. It contends that the notions of analogy and fluidity are fundamental to explain how the human mind solves problems and to create computer programs that show intelligent behavior. It analyzes several computer programs that members of the group have created over the years to solve problems that require intelligence.\n\nIt was the first book ever sold by Amazon.com.\n\nThe book is a collection of revised articles that appeared in precedence, each preceded by an introduction by Hofstadter.\nThey describe the scientific work by him and his collaborators in the 1980s and 1990s.\nThe project started in the late 1970s at Indiana University.\nIn 1983 he took a sabbatical year at MIT, working in Marvin Minsky's Artificial Intelligence Lab.\nThere he met and collaborated with Melanie Mitchell, who then became his doctoral student.\nSubsequently, Hofstadter moved to the University of Michigan, where the FARG (Fluid Analogies Research Group) was founded.\nEventually he returned to Indiana University in 1988, continuing the FARG research there.\nThe book was written during a sabbatical year at the Istituto per la Ricerca Scientifica e Tecnologica in Trento, Italy.\n\nUpon publication, Jon Udell, a BYTE senior technical editor-at-large said:\nFifteen years ago, \"Gödel, Escher, Bach: An Eternal Golden Braid\" exploded on the literary scene, earning its author a Pulitzer prize and a monthly column in \"Scientific American\". Douglas Hofstadter's exuberant synthesis of math, music, and art, and his inspired thought experiments with \"tangled hierarchy,\" recursion, pattern recognition, figure/ground reversal, and self-reference, delighted armchair philosophers and AI theorists. But in the end, many people believed that these intellectual games yielded no useful model of cognition on which to base future AI research. Now \"Fluid Concepts and Creative Analogies\" presents that model, along with the computer programs Hofstadter and his associates have designed to test it. These programs work in stripped-down yet surprisingly rich microdomains.\n\nOn April 3, 1995, \"Fluid Concepts and Creative Analogies\" became the first book ordered online by an Amazon.com customer.\n\n\nThe first AI project by Hofstadter stemmed from his teenage fascination with number sequences.\nWhen he was 17, he studied the way that triangular and square numbers interleave, and eventually found a recursive relation describing it.\nIn his first course on AI, he set to the students and to himself the task of writing a program that could extrapolate the rule by which a numeric sequence is generated.\nHe discusses breadth-first and depth-first techniques, but eventually concludes that the results represent expert systems that incarnate a lot of technical knowledge but don't shine much light on the mental processes that humans use to solve such puzzles.\n\nInstead he devised a simplified version of the problem, called SeekWhence, where sequences are based on very simple basic rules not requiring advanced mathematical knowledge.\nHe argues that pattern recognition, analogy, and fluid working hypotheses are fundamental to understand how humans tackle such problems.\n\nJumbo is a program to solve jumbles, word puzzles consisting in five or six scrambled letters that need to be anagrammed to form an English word.\nThe resulting word does not need to be a real one but just to a plausible, that is, to consists of a sequence of letters that is normal in English.\n\nThe constituent elements of Jumbo are the following:\nA \"temperature\" is associated to the present state of the cytoplasm; it determines how probable it is that a destructive codelet is executed.\nThere is a \"freezing\" temperature at which no destruction can occur anymore: a solution has been found.\n\nNumbo is a program by Daniel Defays that tries to solve numerical problems similar to those used in the French game \"Le compte est bon\". The game consists in combining some numbers called \"bricks\", using the operations of multiplication, addition, and subtraction, to obtain a given result.\n\nThe program is modeled on Jumbo and Copycat and uses a permanent network of known mathematical facts, a working memory in the form of a cytoplasm, and a coderack containing codelets to produce free associations of bricks in order to arrive at the result.\n\nThe chapter subtitle \"A Critique of Artificial-intelligence Methodology\" indicates that this is a polemical article, in which David Chalmers, Robert French, and Hofstadter criticize most of the research going on at that time (the early '80s) as exaggerating results and missing the central features of human intelligence.\n\nSome of these AI projects, like the structure mapping engine (SME), claimed to model high faculties of the human mind and to be able to understand literary analogies and to rediscover important scientific breakthroughs.\nIn the introduction, Hofstadter warns about the Eliza effect that leads people to attribute understanding to a computer program that only uses a few stock phrases.\nThe authors claim that the input data for such impressive results are already heavily structured in the direction of the intended discovery and only a simple matching task is left to the computer.\n\nTheir main claim is that it is impossible to model high-level cognition without at the same time modeling low-level perception.\nWhile cognition is necessarily based on perception, they argue that it in turn influences perception itself.\nTherefore, a sound AI project should try to model the two together.\nIn a slogan repeated several times throughout the book: \"cognition is recognition\".\n\nSince human perception is too complex to be modelled by available technology, they favor the restriction of AI projects to limited domains like the one used for the Copycat project.\n\nThis chapter presents, as stated in the full title, \"A Model of Mental Fluidity and Analogy-making\".\nIt is a description of the architecture of the Copycat program, developed by Hofstadter and Melanie Mitchell.\nThe field of application of the program is a domain of short alphabetic sequences.\nA typical puzzle is: \"If abc were changed to abd, how would you change ijk in the same way?\".\nThe program tries to find an answer using a strategy supposedly similar to the way the human mind tackles the question.\n\nCopycat has three major components:\nThe resulting software displays emergent properties.\nIt works according to a \"parallel terraced scan\" that runs several possible processes at the same time.\nIt shows mental fluidity in that concepts may \"slip\" into similar ones.\nIt emulates human behavior in tending to find the most obvious solutions most of the time but being more satisfied (as witnessed by low temperature) by more clever and deep answers that it finds more rarely.\n\nThis chapter compares Copycat with other recent (at the time) work in artificial intelligence.\nSpecifically, it matches it with the claimed results from the structure mapping engine SME and the Analogical Constraint Mapping Engine (ACME).\nThe authors' judgment is that those programs suffer from two defects: Their input is pre-structured by the developers to highlight the analogies that the software is supposed to find; and the general architecture of the programs is serial and deterministic rather than parallel and stochastic like Copycat's, which they consider psychologically more plausible.\n\nSevere criticism is put on the claim that these tools can solve \"real-life\" problems.\nIn fact, only the terms used in the example suggest that the input to the programs comes from a concrete situation.\nThe logical structures don't actually imply any meaning for the term.\n\nFinally a more positive assessment is given to two other projects: Indurkhya' PAN model and Kokinov's AMBR system.\n\nThis chapter looks at those aspects of human creativity that are not yet modeled by Copycat and lays down a research plan for a future extension of the software.\nThe main missing element is the mind's ability to observe itself and reflect on its own thinking process.\nAlso important is the ability to learn and to remember the results of the mental activity.\n\nThe creativity displayed in finding analogies should be applicable at ever higher levels: making analogies between analogies (expression inspired by the title of a book by Stanislaw Ulam), analogies between these second-order analogies, and so on.\n\nAnother of Hofstadter's students, Robert French, was assigned the task of applying the architecture of Copycat to a different domain, consisting in analogies between objects lying on a table in a coffeehouse.\nThe resulting program was named Tabletop.\n\nThe authors present a different and vaster domain to justify the relevance of attacking such a trivial-seeming project.\nThe alternative domain is called Ob-Platte and consists in discovering analogies between geographical locations in different regions or countries.\n\nOnce again arguments are offered against a brute-force approach, which would work on the small Tabletop domain but would become unfeasible on the larger Ob-Platte domain.\nInstead a parallel non-deterministic architecture is used, similar to the one adopted by the Copycat project.\n\nIn the premise to the chapter, title \"The Knotty Problem of Evaluating Research\", Hofstadter considers the question of how research in AI should be assessed.\nHe argues against a strict adherence to a match between the results of an AI program with the average answer of human test subjects.\nHe gives two reasons for his rejection: the AI program is supposed to emulate creativity, while an average of human responses will delete any original insight by any of the single subjects; and the architecture of the program should be more important that its mere functional description.\n\nIn the main article, the architecture of Tabletop is described: it is strongly inspired by that of Copycat and consists of a Slipnet, a Workspace, and a Corerack.\n\nThis last chapter is about a more ambitious project that Hofstadter started with student Gary McGraw.\nThe microdomain used is that of grid fonts: typographic alphabets constructed using a rigid system of small rigid components.\nThe goal is to construct a program that, given only a few or just one letter from the grid font, can generate the whole alphabet \"in the same style\".\nThe difficulty lies in the ambiguity and undefinability of \"style\".\nThe projected program would have a structure very similar to that of Jumble, Numble, Copycat, and Tabletop.\n\nIn the concluding part of the book, Hofstadter analyses some AI projects with a critical eye.\nHe finds that today's AI is missing the gist of human creativity and is making exaggerated claims.\nThe project under scrutiny are the following.\n\nAARON, a computer artist that can draw images of people in outdoor settings in a distinctive style reminiscent of that of a human artist; criticism: the program doesn't have any understanding of the objects it draws, it just uses some graphical algorithms with some randomness thrown in to generate different scenes at every run and to give the style a more natural feel.\n\nRacter, a computer author that wrote a book entitled \"The Policeman's Beard Is Half Constructed\".\nAlthough some of the prose generated by the program is quite impressive, due in part to the Eliza effect, the computer does not have any notion of plot or of the meaning of the words it uses. Furthermore, the book is made up of selected texts from thousands produced by the computer over several years.\n\nAM, a computer mathematician that generates new mathematical concepts. It managed to produce by itself the notion of prime number and the Goldbach conjecture. As with Racter, the question is how much the programmer filtered the output of the program, keeping only the occasional interesting output.\nAlso, mathematics being a very specialized domain, it is doubtful whether the techniques used can be abstracted to general cognition.\n\nAnother mathematical program, called Geometry, was celebrated for making an insightful discovery of an original proof that an isosceles triangle has equal base angles. The proof is based on seeing the triangle in two different ways. However, the program generates all possible ways of seeing the triangle, not even knowing that it is the same triangle.\n\nHofstadter concludes with some methodological remarks on the Turing Test.\nIn his opinion it is still a good definition and he argues that by interacting with a program, a human may be able to have insight not just on its behaviour but also on its structure.\nHowever, he criticises the use that is made of it at present: it encourages the development of fancy natural-language interfaces instead of the investigation of deep cognitive faculties.\n"}
{"id": "17367570", "url": "https://en.wikipedia.org/wiki?curid=17367570", "title": "Formal distinction", "text": "Formal distinction\n\nIn scholastic metaphysics, a formal distinction is a distinction intermediate between what is merely conceptual, and what is fully real or mind-independent. It was made by some realist philosophers of the Scholastic period in the thirteenth century, and particularly by Duns Scotus.\n\nMany realist philosophers of the period (such as Aquinas and Henry of Ghent), recognised the need for an intermediate distinction that was not merely conceptual, but not fully real or mind-independent either. Aquinas held that the difference between our concepts arises not just in the mind, but has a foundation in the thing (\"fundamentum in re\"). Henry held that there was an 'intentional' distinction (\"distinctio intentionalis\") such that 'intentions' (i.e. concepts) that are distinct in the mind, correspond to things which are potentially distinct in reality.\n\nScotus argued for a formal distinction (\"distinctio formalis a parte rei\"), which holds between entities which are inseparable and indistinct in reality, but whose definitions are not identical. For example, the personal properties of the Trinity are formally distinct from the Divine essence. Similarly, the distinction between the 'thisness' or \"haecceity\" of a thing and its existence is intermediate between a real and a conceptual distinction. There is also a formal distinction between the divine attributes and the powers of the soul.\n\nOckham was opposed to the idea, arguing that whenever there is any distinction or non-identity in reality, then two contradictory statements can be made. But contradictory statements cannot be truly asserted unless the realities they stand for are either (1) distinct real things (2) distinct concepts or (3) a thing and a concept. But if they all exist in reality, they are not distinct concepts, nor are they a real thing and a concept. Therefore, they are distinct in reality.\n\n"}
{"id": "26279594", "url": "https://en.wikipedia.org/wiki?curid=26279594", "title": "Interpretation (philosophy)", "text": "Interpretation (philosophy)\n\nA philosophical interpretation is the assignment of meanings to various concepts, symbols, or objects under consideration. Two broad types of interpretation can be distinguished: interpretations of physical objects, and interpretations of concepts (Conceptual model).\n\nInterpretation is related to perceiving the things. An aesthetic interpretation is an explanation of the meaning of some work of art. An aesthetic interpretation expresses an understanding of a work of art, a poem, performance, or piece of literature. There may be different interpretations to same work by art by different people owing to their different perceptions or aims. All such interpretations are termed as 'aesthetic interpretations'. Some people, instead of interpreting work of art, believe in interpreting artist himself. It pretty much means \"how or what do I believe about (subject)\"\n\nA judicial interpretation is a conceptual interpretation that explains how the judiciary should interpret the law, particularly constitutional documents and legislation (see statutory interpretation).\n\nIn logic, an interpretation is an assignment of meaning to the symbols of a language. The formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called \"formal semantics\".\n\nReligious interpretation and similarly religious self-interpretation define a section of religion-related studies (theology, comparative religion, reason) where attention is given to aspects of perception—where religious symbolism and the self-image of all those who hold religious views have important bearing on how others perceive their particular belief system and its adherents.\n\nAn interpretation is a \"descriptive interpretation\" (also called a \"factual interpretation\") if at least one of the undefined symbols of its formal system becomes, in the interpretation, the name of a physical object, or observable property. A descriptive interpretation is a type of interpretation used in science and logic to talk about empirical entities.\n\nWhen scientists attempt to formalize the principles of the empirical sciences, they use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system that will serve as a conceptual model of reality. Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.\n\n"}
{"id": "34644725", "url": "https://en.wikipedia.org/wiki?curid=34644725", "title": "Jurisprudence of concepts", "text": "Jurisprudence of concepts\n\nThe jurisprudence of concepts was the first \"sub-school\" of legal positivism, according to which, the written law must reflect concepts, when interpreted. Its main representatives were Ihering, Savigny and Puchta.\n\nThis school was, thus, the preceding trigger of the idea that law comes from a dogmatic source, imposition from man over man and not a \"natural\" consequence of other sciences or of metaphysical faith.\n\nAmong the main characters of the \"jurisprudence of concepts\" are:\n\nSo, according to this school, law should have prevailing sources based upon the legislative process, although needing to be proven by more inclusive ideas of a social sense.\n\n"}
{"id": "563405", "url": "https://en.wikipedia.org/wiki?curid=563405", "title": "League system", "text": "League system\n\nA league system is a hierarchy of leagues in a sport. They are often called pyramids, due to their tendency to split into an increasing number of regional divisions further down the system. League systems of some sort are used in many sports in many countries.\n\nIn association football, rugby union and rugby league, league systems are usually connected by the process of promotion and relegation, in which teams from a lower division who finish at the top of the standings in their league are promoted (advanced to the next level of the system) while teams who finish lowest in their division are relegated (move down to a lower division). This process can be automatic each year, or can require playoffs.\n\nIn North America, league systems in the most popular sports do not use promotion or relegation. Most professional sports are divided into major and minor leagues. Baseball and association football (known as soccer in North America) have well-defined pyramid shapes to their minor league hierarchies, each managed by a governing body (Minor League Baseball, an organization under the authority of the Commissioner of Baseball, governs baseball leagues; the United States Soccer Federation designates the American soccer pyramid.) Ice hockey's professional minor league system is linear, with one league at most of the four levels of the game; the ice hockey league system in North America is governed by collective bargaining agreements and affiliation deals between the NHL, AHL and ECHL.\n\nGridiron football does not operate on a league system. Different professional leagues play by very different sets of rules in different seasons (the NFL plays 11-a-side on a 100-yard field in autumn and early winter, the CFL uses 12-a-side on a 110-yard field in summer and early fall, while arena football and the minor indoor leagues each play 8-a-side on a 50-yard field in the spring and early summer). There have been attempts at forming true minor leagues for the professional game (most recently with 2017's The Spring League); none so far have been able to balance the major leagues' requests with the ability to maintain financial solvency.\n\n"}
{"id": "34074829", "url": "https://en.wikipedia.org/wiki?curid=34074829", "title": "Lexical entrainment", "text": "Lexical entrainment\n\nLexical entrainment is the phenomenon in conversational linguistics of the process of the subject adopting the reference terms of their interlocutor. In practice, it acts as a mechanism of the cooperative principle in which both parties to the conversation employ lexical entrainment as a progressive system to develop \"conceptual pacts\" (a working temporary conversational terminology) to ensure maximum clarity of reference in the communication between the parties; this process is necessary to overcome the ambiguity inherent in the multitude of synonyms that exist in language. \n\nLexical entrainment arises by two cooperative mechanisms: \n\nOnce lexical entrainment has come to determine the phrasing for a referent, both parties will use that terminology for the referent for the duration, even if it proceeds to violate the Gricean maxim of quantity. For example, if one wants to refer to a brown loafer out of a set of shoes that consist of: the loafer, a sneaker, and a high-heeled shoe, they will not use \"the shoe\" to describe the object as this phrasing does not unambiguously describe one item in the set under consideration. They will also not call the object \"the brown loafer\" which would violate Grice's maxim of quantity. The speaker will settle on using the term \"the loafer\" as it is just informative enough without giving too much information. \n\nAnother important factor is lexical availability; the ease of conceptualizing a referent in a certain way and then retrieving and producing a label for it. For many objects the most available labels are basic nouns; for example, the word \"dog\". Instead of saying \"animal\" or \"husky\" for the referent, most subjects will default to \"dog\".\nIf in a set of objects one is to refer to either a husky, a table, and a poster, people are still most likely to use the word \"dog.\" This is technically a violation of Grice's maxim of quantity, as using the term \"animal\" is ideal.\n\nLexical entrainment has applications in natural language processing in computers as well as human–human interaction. Currently, the adaptability of computers to modify their referencing to the terms of their human interlocutor is limited, so the entrainment adaptation falls to the human operator; this phenomenon is readily demonstrated in Brennan's 1996 experiment.\n"}
{"id": "8599305", "url": "https://en.wikipedia.org/wiki?curid=8599305", "title": "Logic model", "text": "Logic model\n\nLogic models are hypothesized descriptions of the chain of causes and effects (see Causality) leading to an outcome of interest (e.g. prevalence of cardiovascular diseases, annual traffic collision, etc). While they can be in a narrative form, logic model usually take form in a graphical depiction of the \"if-then\" (causal) relationships between the various elements leading to the outcome. However, the logic model is more than the graphical depiction: it is also the theories, scientific evidences, assumptions and beliefs that support it and the various processes behind it.\n\nLogic models are used by planners, funders, managers and evaluators of programs and interventions to plan, communicate, implement and evaluate them. They are being employed as well by health scientific community to organize and conduct literature reviews such as systematic reviews. Domains of application are various, e.g. waste management, poultry inspection, business education, heart disease and stroke prevention. Since they are used in various contexts and for different purposes, their typical components and levels of complexity varies in literature (compare for example the W.K. Kellogg Foundation presentation of logic model, mainly aimed for evaluation, and the numerous types of logic models in the Intervention Mapping framework)). In addition, depending of the purpose of the logic model, elements depicted and the relationships between them is more or less detailed.\n\nCiting Funnell and Rogers account, Joy A. Frechtling (2015) encyclopedic article traces logic model underpinnings in the 1950s. Patricia J. Rogers (2005) encyclopedic article rather trace it back to 1967 Edward A. Suchman book about evaluative research. Both encyclopedic article and LeCroy one (2018) mention an increasing interest, usage and publications about the subject.\n\nOne of the most important uses of the logic model is for program planning. It is suggested to use the logic model to focus on the intended outcomes of a particular program. The guiding questions change from \"what is being done?\" to \"what needs to be done\"? McCawley suggests that by using this new reasoning, a logic model for a program can be built by asking the following questions in sequence:\n\n\nBy placing the focus on ultimate outcomes or results, planners can \"think backward\" through the logic model to identify how best to achieve the desired results. Here it helps managers to 'plan with the end in mind', rather than just consider inputs (e.g. budgets, employees) or the tasks that must be done.\n\nThe logic model is often used in government or not-for-profit organizations, where the mission and vision are not aimed at achieving a financial benefit. Traditionally, government programs were described only in terms of their budgets. It is easy to measure the amount of money spent on a program, but this is a poor indicator of outcomes. Likewise it is relatively easy to measure the amount of work done (e.g. number of workers or number of years spent), but the workers may have just been 'spinning their wheels' without getting very far in terms of ultimate results or outcomes.\n\nHowever, nature of outcomes varies. To measure the progress toward outcomes, some initiatives may require an ad hoc measurement instrument. In addition, in programs such as in education or social programs, outcomes are usually in the long-term and may requires numerous intermediate changes (attitudes, social norm, industry practices, etc.) to advance progressively toward the outcomes.\n\nBy making clear the intended outcomes and the causal pathways leading to them, a program logic model provides the basis upon which planners and evaluators can develop a measurement plan and adequate instruments. Instead of only looking at the outcome progress, planners can open the \"black box\" and examine if the intermediate outcomes progress as planned. In addition, the pathways of numerous outcomes are still largely misunderstood due their complexity, their unpredictability and lack of scientific / practical evidences. Therefore, with proper research design, one may not only assess the progress of intermediate outcomes, but evaluate as well if the program theory of change is accurate, i.e. is successful change of an intermediate outcomes provokes the hypothesized subsequent effects in the causal pathway. Finally, outcomes may easily be achieved through processes independent of the program and an evaluation of those outcomes would suggest program success when in fact external outputs were responsible for the outcomes.\n\nMany authors and guides use the following template when speaking about logic model:\n\nMany refinements and variations have been added to the basic template. For example, many versions of logic models set out a series of outcomes/impacts, explaining in more detail the logic of how an intervention contributes to intended or observed results. Others often distinguish short-term, medium-term and long-term results, and between direct and indirect results.\n\nBartholomew et al. Intervention Mapping approach makes an extensive use of logic model through the whole life-cycle of a health promotion program. Since this method can start from as far as a vague desired outcomes (authors example is a city whose actors decide to address \"health issues\" of the city), planners goes through various steps in order to develop effective interventions and properly evaluate them (see Intervention Mapping entry for a more detailed account). Distinguishable but closely interweave logic models with different purposes are being developed through the process:\n\n\nEvaluators thereafter use the logic model of the intervention to design a proper evaluation plan to assess implementation, impact and efficiency.\n\nBy describing work in this way, managers have an easier way to define the work and measure it. Performance measures can be drawn from any of the steps. One of the key insights of the logic model is the importance of measuring final outcomes or results, because it is quite possible to waste time and money (inputs), \"spin the wheels\" on work activities, or produce outputs without achieving desired outcomes. It is these outcomes (impacts, long-term results) that are the only justification for doing the work in the first place. For commercial organizations, outcomes relate to profit. For not-for-profit or governmental organizations, outcomes relate to successful achievement of mission or program goals.\n\nThere are some potential disadvantages of logic models due to tendencies toward oversimplification. These include:\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "1148564", "url": "https://en.wikipedia.org/wiki?curid=1148564", "title": "Marginal concepts", "text": "Marginal concepts\n\nIn economics, marginal concepts are associated with a \"specific change\" in the quantity used of a good or service, as opposed to some notion of the over-all significance of that class of good or service, or of some total quantity thereof.\n\nConstraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual himself or herself.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change, as large as the smallest relevant division of that good or service. For reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. In such context, a marginal change may be an infinitesimal change or a limit. However, strictly speaking, the smallest relevant division may be quite large.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nThe marginal utility of a good or service is the utility of the specific use to which an agent would put a given increase in that good or service, or of the specific use that would be abandoned in response to a given decrease. In other words, marginal utility is the utility of the marginal use.\n\nThe marginal rate of substitution is the rate of substitution is the least favorable rate, at the margin, at which an agent is willing to exchange units of one good or service for units of another.\n\nA marginal benefit is a benefit (howsoever ranked or measured) associated with a marginal change.\n\nThe term “marginal cost” may refer to an opportunity cost at the margin, or to marginal \"pecuniary\" cost — that is to say marginal cost measured by forgone money.\n\nOther marginal concepts include (but are not limited to):\n\nMarginalism is the use of marginal concepts to explain economic phenomena.\n\nThe related concept of elasticity is the ratio of the incremental percentage change in one variable with respect to an incremental percentage change in another variable.\n"}
{"id": "2695183", "url": "https://en.wikipedia.org/wiki?curid=2695183", "title": "Marginal propensity to import", "text": "Marginal propensity to import\n\nThe marginal propensity to import (MPI) is the fractional change in import expenditure that occurs with a change in disposable income (income after taxes and transfers). For example, if a household earns one extra dollar of disposable income, and the marginal propensity to import is 0.2, then the household will spend 20 cents of that dollar on imported goods and services.\n\nMathematically, the marginal propensity to import (MPM) function is expressed as the derivative of the import (I) function with respect to disposable income (Y).formula_1In other words, the marginal propensity to import is measured as the ratio of the change in imports to the change in income, thus giving us a figure between 0 and 1. \n\nImports are also considered to be automatic stabilisers that work to lessen fluctuations in real GDP.\n\nThe UK government assumes that UK citizens have a high marginal propensity to import and thus will use a decrease in disposable income as a tool to control the current account on the balance of payments.\n\n"}
{"id": "1280458", "url": "https://en.wikipedia.org/wiki?curid=1280458", "title": "Marginal revenue", "text": "Marginal revenue\n\nIn microeconomics, marginal revenue (R') is the additional revenue that will be generated by increasing product sales by one unit. It can also be described as the unit revenue the last item sold has generated for the firm. In a perfectly competitive market, the additional revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good. This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price. However, a monopoly determines the entire industry's sales. As a result, it will have to lower the price of all units sold to increase sales by 1 unit. Therefore, the marginal revenue generated is always lower than the price the firm is able to charge for the unit sold, since each reduction in price causes unit revenue to decline on every good the firm sells. The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. \n\nA firms profits will be maximized when marginal revenue (MR) equals marginal cost (MC). If formula_1 then a firm should increase output for more profits, if formula_2 then a firm should decrease output for additional profits. A firm should choose the output level which is profit maximizing under perfect competition theory formula_3.\n\nMarginal revenue is equal to the ratio of the change in revenue for some change in quantity sold to that change in quantity sold. This can also be represented as a derivative when the change in quantity sold becomes arbitrarily small. More formally, define the revenue function to be the following\n\nBy the product rule, marginal revenue is then given by\n\nFor a firm facing perfect competition, price does not change with quantity sold (formula_6), so marginal revenue is equal to price. For a monopoly, the price decreases with quantity sold (formula_7), so marginal revenue is less than price (for positive formula_8).\n\nThe marginal revenue curve is affected by the same factors as the demand curve - changes in income, change in the prices of complements and substitutes, change in populations. These factors can cause the R curve to shift and rotate.\n\nThe relationship between marginal revenue and the elasticity of demand by the firm's customers can be derived as follows:\n\nwhere e is the price elasticity of demand. If demand is inelastic (e < 1) then R' will be negative, because to sell a marginal (infinitesimal) unit the firm would have to lower the selling price so much that it would lose more revenue on the pre-existing units than it would gain on the incremental unit. If demand is elastic (e > 1) R' will be positive, because the additional unit would not drive down the price by so much. If the firm is a perfect competitor, so that it is so small in the market that its quantity produced and sold has no effect on the price, then the price elasticity of demand is negative infinity, and marginal revenue simply equals the (market-determined) price.\n\nProfit maximization requires that a firm produces where marginal revenue equals marginal costs. Firm managers are unlikely to have complete information concerning their marginal revenue function or their marginal costs. Fortunately, the profit maximization conditions can be expressed in a “more easily applicable form” or rule of thumb.\n\nMarkup is the difference between price and marginal cost. The formula states that markup as a percentage of price equals the negative of the inverse of elasticity of demand. Alternatively, the relationship can be expressed as:\n\nThus if e is - 2 and mc is $5.00 then price is $10.00.\n\n(<R> - C')/ <R> = - 1/e is called the Lerner index after economist Abba Lerner. The Lerner index is a measure of market power - the ability of a firm to charge a price that exceeds marginal cost. The index varies from zero to 1. The greater the difference between price and marginal cost the closer the index value is to 1. The Lerner index increases as demand becomes less elastic.\n\nExample\nIf a company can sell 10 units at $20 each or 11 units at $19 each, then the marginal revenue from the eleventh unit is (11 × 19) - (10 × 20) = $9.\n\n\n"}
{"id": "3108261", "url": "https://en.wikipedia.org/wiki?curid=3108261", "title": "Marginal value", "text": "Marginal value\n\nA marginal value is\n(This third case is actually a special case of the second).\n\nIn the case of differentiability, at the limit, a marginal change is a mathematical differential, or the corresponding mathematical derivative.\n\nThese uses of the term “marginal” are especially common in economics, and result from conceptualizing constraints as \"borders\" or as \"margins\". The sorts of marginal values most common to economic analysis are those associated with \"unit\" changes of resources and, in mainstream economics, those associated with \"infinitesimal\" changes. Marginal values associated with units are considered because many decisions are made by unit, and marginalism explains \"unit price\" in terms of such marginal values. Mainstream economics uses infinitesimal values in much of its analysis for reasons of mathematical tractability.\n\nAssume a functional relationship\n\nIf the value of formula_2 is \"discretely\" changed from formula_3 to formula_4 while other independent variables remain unchanged, then the marginal value of the change in formula_2 is\nand the “marginal value” of formula_7 may refer to\nor to\n\nIf an individual saw her income increase from $50000 to $55000 per annum, and part of her response was to increase yearly purchases of amontillado from 2 casks to three casks, then\n\nIf \"infinitesimal\" values are considered, then a marginal value of formula_2 would be formula_11, and the “marginal value” of formula_7 would typically refer to\n\nAssume that, in some economy, aggregate consumption is well-approximated by\nwhere\nThen the \"marginal propensity to consume\" is\n\n"}
{"id": "315426", "url": "https://en.wikipedia.org/wiki?curid=315426", "title": "Mediocrity principle", "text": "Mediocrity principle\n\nThe mediocrity principle is the philosophical notion that \"if an item is drawn at random from one of several sets or categories, it's likelier to come from the most numerous category than from any one of the less numerous categories\". The principle has been taken to suggest that there is nothing very unusual about the evolution of the Solar System, Earth's history, the evolution of biological complexity, human evolution, or any one nation. It is a heuristic in the vein of the Copernican principle, and is sometimes used as a philosophical statement about the place of humanity. The idea is to assume mediocrity, rather than starting with the assumption that a phenomenon is special, privileged, exceptional, or even superior.\n\nThe mediocrity principle suggests, given the existence of life on Earth, that life typically exists on Earth-like planets throughout the universe.\n\nThe mediocrity principle is in contrast with the anthropic principle, which asserts that the presence of an intelligent observer (humans) limits the circumstances to bounds under which intelligent life can be observed to exist, no matter how improbable. Both stand in contrast to the fine-tuning hypothesis, which asserts that the natural conditions for intelligent life are implausibly rare.\n\nThe mediocrity principle implies that Earth-like environments are necessarily common, based in part on the evidence of any happening at all, whereas the anthropic principle suggests that no assertion can be made about the probability of intelligent life based on a sample set of one (self-described) example, who are necessarily capable of making such an assertion about themselves.\n\nIt is also possible to handle the Mediocrity Principle as a statistical problem, a case of a single Data point statistics, also present in the German tank problem.\n\nDavid Deutsch argues that the mediocrity principle is incorrect from a physical point of view, in reference to either humanity's part of the universe or to its species. Deutsch refers to Stephen Hawking's quote: \"The human race is just a chemical scum on a moderate-sized planet, orbiting around a very average star in the outer suburb of one among a hundred billion galaxies\". Deutsch wrote that Earth's neighborhood in the universe is not typical (80% of the universe's matter is dark matter) and that a concentration of mass such as the Solar System is an \"isolated, uncommon phenomenon\". He also disagrees with Richard Dawkins, who considers that humans, because of natural evolution, are limited to the capabilities of their species. Deutsch responds that even though evolution did not give humans the ability to detect neutrinos, scientists can currently detect them, which significantly expands their capabilities beyond what is available as a result of evolution.\n\n"}
{"id": "994704", "url": "https://en.wikipedia.org/wiki?curid=994704", "title": "Mental model", "text": "Mental model\n\nA mental model is an explanation of someone's thought process about how something works in the real world. It is a representation of the surrounding world, the relationships between its various parts and a person's intuitive perception about his or her own acts and their consequences. Mental models can help shape behaviour and set an approach to solving problems (similar to a personal algorithm) and doing tasks.\n\nA mental model is a kind of internal symbol or representation of external reality, hypothesized to play a major role in cognition, reasoning and decision-making. Kenneth Craik suggested in 1943 that the mind constructs \"small-scale models\" of reality that it uses to anticipate events.\n\nJay Wright Forrester defined general mental models as:\nThe image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system (Forrester, 1971).\n\nIn psychology, the term \"mental models\" is sometimes used to refer to mental representations or mental simulation generally. At other times it is used to refer to and to the mental model theory of reasoning developed by Philip Johnson-Laird and Ruth M.J. Byrne.\n\nThe term \"mental model\" is believed to have originated with Kenneth Craik in his 1943 book \"The Nature of Explanation\". in \"Le dessin enfantin\" (Children's drawings), published in 1927 by Alcan, Paris, argued that children construct internal models, a view that influenced, among others, child psychologist Jean Piaget.\n\nPhilip Johnson-Laird published \"Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness\" in 1983. In the same year, Dedre Gentner and Albert Stevens edited a collection of chapters in a book also titled \"Mental Models\". The first line of their book explains the idea further: \"One function of this chapter is to belabor the obvious; people's views of the world, of themselves, of their own capabilities, and of the tasks that they are asked to perform, or topics they are asked to learn, depend heavily on the conceptualizations that they bring to the task.\" (see the book: \"Mental Models\").\n\nSince then, there has been much discussion and use of the idea in human-computer interaction and usability by researchers including Donald Norman and Steve Krug (in his book \"Don't Make Me Think\"). Walter Kintsch and Teun A. van Dijk, using the term \"situation model\" (in their book \"Strategies of Discourse Comprehension\", 1983), showed the relevance of mental models for the production and comprehension of discourse.\n\nOne view of human reasoning is that it depends on mental models. In this view, mental models can be constructed from perception, imagination, or the comprehension of discourse (Johnson-Laird, 1983). Such mental models are similar to architects' models or to physicists' diagrams in that their structure is analogous to the structure of the situation that they represent, unlike, say, the structure of logical forms used in formal rule theories of reasoning. In this respect, they are a little like pictures in the picture theory of language described by philosopher Ludwig Wittgenstein in 1922. Philip Johnson-Laird and Ruth M.J. Byrne developed a theory of mental models which makes the assumption that reasoning depends, not on logical form, but on mental models (Johnson-Laird and Byrne, 1991).\n\nMental models are based on a small set of fundamental assumptions (axioms), which distinguish them from other proposed representations in the psychology of reasoning (Byrne and Johnson-Laird, 2009). Each mental model represents a possibility. A mental model represents one possibility, capturing what is common to all the different ways in which the possibility may occur (Johnson-Laird and Byrne, 2002). Mental models are iconic, i.e., each part of a model corresponds to each part of what it represents (Johnson-Laird, 2006). Mental models are based on a principle of truth: they typically represent only those situations that are possible, and each model of a possibility represents only what is true in that possibility according to the proposition. However, mental models can represent what is false, temporarily assumed to be true, for example, in the case of counterfactual conditionals and counterfactual thinking (Byrne, 2005).\n\nPeople infer that a conclusion is valid if it holds in all the possibilities. Procedures for reasoning with mental models rely on counter-examples to refute invalid inferences; they establish validity by ensuring that a conclusion holds over all the models of the premises. Reasoners focus on a subset of the possible models of multiple-model problems, often just a single model. The ease with which reasoners can make deductions is affected by many factors, including age and working memory (Barrouillet, et al., 2000). They reject a conclusion if they find a counterexample, i.e., a possibility in which the premises hold, but the conclusion does not (Schroyens, et al. 2003; Verschueren, et al., 2005).\n\nScientific debate continues about whether human reasoning is based on mental models, versus formal rules of inference (e.g., O'Brien, 2009), domain-specific rules of inference (e.g., Cheng & Holyoak, 2008; Cosmides, 2005), or probabilities (e.g., Oaksford and Chater, 2007). Many empirical comparisons of the different theories have been carried out (e.g., Oberauer, 2006).\n\nA mental model is generally:\n\nMental models are a fundamental way to understand organizational learning. Mental models, in popular science parlance, have been described as \"deeply held images of thinking and acting\". Mental models are so basic to understanding the world that people are hardly conscious of them.\n\nS.N. Groesser and M. Schaffernicht (2012) describe three basic methods which are typically used:\nThese methods allow showing a mental model of a dynamic system, as an explicit, written model about a certain system based on internal beliefs. Analyzing these graphical representations has been an increasing area of research across many social science fields. Additionally software tools that attempt to capture and analyze the structural and functional properties of individual mental models such as Mental Modeler, \"a participatory modeling tool based in fuzzy-logic cognitive mapping\", have recently been developed and used to collect/compare/combine mental model representations collected from individuals for use in social science research, collaborative decision-making, and natural resource planning.\n\nIn the simplification of reality, creating a model can find a sense of reality, seeking to overcome systemic thinking and system dynamics.\n\nThese two disciplines can help to construct a better coordination with the reality of mental models and simulate it accurately. They increase the probability that the consequences of how to decide and act in accordance with how to plan.\n\n\nAfter analyzing the basic characteristics, it is necessary to bring the process of changing the mental models, or the process of learning. Learning is a back-loop process, and feedback loops can be illustrated as: single-loop learning or double-loop learning.\n\nMental models affect the way that people work with information, and also how they determine the final decision. The decision itself changes, but the mental models remain the same. It is the predominant method of learning, because it is very convenient.\n\nDouble-loop learning (\"see diagram below\") is used when it is necessary to change the mental model on which a decision depends. Unlike single loops, this model includes a shift in understanding, from simple and static to broader and more dynamic, such as taking into account the changes in the surroundings and the need for expression changes in mental models.\n\n\n\n"}
{"id": "33742208", "url": "https://en.wikipedia.org/wiki?curid=33742208", "title": "Models of communication", "text": "Models of communication\n\nModels of communication are conceptual models used to explain the human communication process. The first major model for communication was developed in 1948 by Claude Elwood Shannon and published with an introduction by Warren Weaver for Bell Laboratories. Following the basic concept, communication is the process of sending and receiving messages or transferring information from one part (sender) to another (receiver).\n\nIn 1960, David Berlo expanded the linear transmission model with the Sender-Message-Channel-Receiver(SMCR) Model of Communication. Later, Wilbur Schramm introduced a model that identified multiple variables in communication which includes the transmitter, encoding, media, decoding, and receiver. \n\nElwood Shannon and Warren Weaver were engineers that worked for Bell Telephone Labs in the United States. Their goal was to make sure that the telephone cables and radio waves were working at the maximum efficiency. Therefore, they developed the Shannon-Weaver model which had an intention to expand a mathematical theory of communication. The Shannon–Weaver model was developed in 1949 which is referred as the 'mother of all models'. The model is well accepted as a main initial model for Communication Studies which has grown since then.\n\nAs well, the Shannon-Weaver model was designed to mirror the functioning of radio and telephone technology. Their initial model consisted of four primary parts: sender, message, channel, and receiver. The sender was the part of a telephone a person speaks into, the channel was the telephone itself, and the receiver was the part of the phone through which one can hear the person on the other end of the line. Shannon and Weaver also recognized that there may often be static or background sounds that interfere with the process of the other partner in a telephone conversation; they referred to this as noise. Certain types of background sounds can also indicate the absence of a signal.\n\nThe original model of Shannon and Weaver has five elements: information source, transmitter, channel, receiver, and destination. To illustrate the process of the communication the first step is the information source where the information is stored. Next, in order to send the information, the message is encoded into signals, so it can travel to its destination. After the message is encoded, it goes through the channel which the signals are adapted for the transmission. In addition, the channel carried the noise course which is any interference that might happen to lead to the signal receive a different information from the source. After the channel, the message arrives in the receiver step where the message reconstruct (decode) from the signal. Finally, the message arrives at the destination.\n\nIn a simple model, often referred to as \"the transmission model\" or \"standard view of communication\", information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emissor/ sender/ encoder to a destination/ receiver/ decoder. According to this common communication-related conception, communication is viewed as a means of sending and receiving information. The strengths of this model are its simplicity, generality, and quantifiability. The mathematicians Claude Shannon and Warren Weaver structured this model on the basis of the following elements:\n\n\nShannon and Weaver argued that this concept entails three levels of problems for communication:\n\n\nDaniel Chandler criticizes the transmission model in the following terms:\n\n\nIn 1960, David Berlo expanded Shannon and Weaver's 1949 linear model of communication and created the Sender-Message-Channel-Receiver (SMCR) Model of Communication. The SMCR Model of Communication separated the model into clear parts and has been expanded upon by other scholars.\n\nThe Berlo's communication process is a simple application for communication of person-to-person which include communication source, encoder, message, channel, decoder, and communication receiver. In addition, David Berlo presented some factors that influence the communication process between two people. The factors include communication skills, awareness level, social system, cultural system, and attitude.\n\nThe Berlo's Model of Communication process starts at the source. This is the part where determine the communication skills, attitude, knowledge, social system, and culture of the people involved in the communication. After the message is developed which is elements in a set of symbols. Then the encoder step beginning. The encoder process is where the motor skills take place by speaking or writing. The message goes through the channel which carries the message by hearing, seeing, touching, smelling, or tasting. Then the decoder process takes place. In this process, the receiver interpreter the message with her or him sensory skills. Finally, the communication receiver gets the whole message understood.\n\nCommunication is usually described along a few major dimensions: Message (what type of things are communicated), source / emissor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schramm (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).\n\nCommunication can be seen as processes of information transmission governed by three levels of semiotic rules:\n\nTherefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.\n\nIn light of these weaknesses, Barnlund (1970) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.\n\nIn a slightly more complex form, a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of \"noise\" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a [code-book], and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.\n\nTheories of co-regulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society. His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society.\n\nThere is an additional working definition of communication to consider that authors like Richard A. Lanham (2003) and as far back as Erving Goffman (1959) have highlighted. This is a progression from Lasswell's attempt to define human communication through to this century and revolutionized into the constructionist model. Constructionists believe that the process of communication is in itself the only messages that exist. The packaging can not be separated from the social and historical context from which it arose, therefore the substance to look at in communication theory is style for Richard Lanham and the performance of self for Erving Goffman.\n\nLanham chose to view communication as the rival to the over encompassing use of CBS model (which pursued to further the transmission model). CBS model argues that clarity, brevity, and sincerity are the only purpose to prose discourse, therefore communication. Lanham wrote: \"If words matter too, if the whole range of human motive is seen as animating prose discourse, then rhetoric analysis leads us to the essential questions about prose style\" (Lanham 10). This is saying that rhetoric and style are fundamentally important; they are not errors to what we actually intend to transmit. The process which we construct and deconstruct meaning deserves analysis.\n\nErving Goffman sees the performance of self as the most important frame to understand communication. Goffman wrote: \"What does seem to be required of the individual is that he learn enough pieces of expression to be able to 'fill in' and manage, more or less, any part that he is likely to be given\" (Goffman 73), highlighting the significance of expression.\n\nThe truth in both cases is the articulation of the message and the package as one. The construction of the message from social and historical context is the seed as is the pre-existing message is for the transmission model. Therefore, any look into communication theory should include the possibilities drafted by such great scholars as Richard A. Lanham and Goffman that style and performance is the whole process. lun\n\nCommunication stands so deeply rooted in human behaviors and the structures of society that scholars have difficulty thinking of it while excluding social or behavioral events. Because communication theory remains a relatively young field of inquiry and integrates itself with other disciplines such as philosophy, psychology, and sociology, one probably cannot expect a consensus conceptualization of communication across disciplines.\n\nCommunication Model Terms as provided by Rothwell (11-15):\n\nHumans act toward people or things on the basis of the meanings they assign to those people or things.\n-\"Language is the source of meaning\". \n-Meaning arises out of the social interaction people have with each other.\n\n-Meaning is not inherent in objects but it is negotiated through the use of language, hence the term symbolic interactionism.\nAs human beings, we have the ability to name things.\nSymbols, including names, are arbitrary signs.\nBy talking with others, we ascribe meaning to words and develop a universe of discourse\nA symbol is a stimulus that has a learned/shared meaning and a value for people\nSignificant symbols can be nonverbal as well as linguistic.\n\n-Negative responses can consequently reduce a person to nothing.\n-Our expectations evoke responses that confirm what we originally anticipated, resulting in a self-fulfilling prophecy.\n\nThis is a one-way model to communicate with others. It consists of the sender encoding a message and channeling it to the receiver in the presence of noise. In this model there is no feedback or response which may allow for a continuous \nexchange of information (F.N.S. Palma, 1993).\n\nThe linear model was first introduced by Shannon & Weaver in 1949. In the linear communication model, the message travels one direction from the start point to the endpoint. In other words, once the sender sends the message to the receiver the communication process ends. Many communications online use the linear communication model. For example, when you send an email, post a blog, or share something on social media. However, the linear model does not explain many other forms of communication including face-to-face conversation.\n\nIt is two linear models stacked on top of each other. The sender channels a message to the receiver and the receiver then becomes the sender and channels a message to the original sender. This model has added feedback, indicating that communication is not a one way but a two way process. It also has \"field of experience\" which includes our cultural background, ethnicity geographic location, extent of travel, and general personal experiences accumulated over the course of your lifetime. Draw backs – there is feedback but it is not simultaneous.\n\n\nCommunication theory can be seen from one of the following viewpoints:\n\n\nInspection of a particular theory on this level will provide a framework on the nature of communication as seen within the confines of that theory.\n\nTheories can also be studied and organized according to the ontological, epistemological, and axiological framework imposed by the theorist.\n\nOntology essentially poses the question of what, exactly, the theorist is examining. One must consider the very nature of reality. The answer usually falls in one of three realms depending on whether the theorist sees the phenomena through the lens of a realist, nominalist, or social constructionist. Realist perspective views the world objectively, believing that there is a world outside of our own experience and cognitions. Nominalists see the world subjectively, claiming that everything outside of one's cognitions is simply names and labels. Social constructionists straddle the fence between objective and subjective reality, claiming that reality is what we create together.\n\nEpistemology is an examination of the approaches and beliefs which inform particular modes of study of phenomena and domains of expertise. In positivist approaches to epistemology, objective knowledge is seen as the result of the empirical observation and perceptual experience. In the history of science, empirical evidence collected by way of pragmatic-calculation and the scientific method is believed to be the most likely to reflect truth in the findings. Such approaches are meant to predict a phenomenon. Subjective theory holds that understanding is based on situated knowledge, typically found using interpretative methodology such as ethnography and also interviews. Subjective theories are typically developed to explain or understand phenomena in the social world.\n\nAxiology is concerned with how values inform research and theory development. Most communication theory is guided by one of three axiological approaches. The first approach recognizes that values will influence theorists' interests but suggests that those values must be set aside once actual research begins. Outside replication of research findings is particularly important in this approach to prevent individual researchers' values from contaminating their findings and interpretations. The second approach rejects the idea that values can be eliminated from any stage of theory development. Within this approach, theorists do not try to divorce their values from inquiry. Instead, they remain mindful of their values so that they understand how those values contextualize, influence or skew their findings. The third approach not only rejects the idea that values can be separated from research and theory, but rejects the idea that they should be separated. This approach is often adopted by critical theorists who believe that the role of communication theory is to identify oppression and produce social change. In this axiological approach, theorists embrace their values and work to reproduce those values in their research and theory development.\n\n\n"}
{"id": "2059959", "url": "https://en.wikipedia.org/wiki?curid=2059959", "title": "Negative amortization", "text": "Negative amortization\n\nIn finance, negative amortization (also known as NegAm, deferred interest or graduated payment mortgage) occurs whenever the loan payment for any period is less than the interest charged over that period so that the outstanding balance of the loan increases. As an amortization method the shorted amount (difference between interest and repayment) is then added to the total amount owed to the lender. Such a practice would have to be agreed upon before shorting the payment so as to avoid default on payment. This method is generally used in an introductory period before loan payments exceed interest and the loan becomes self-amortizing. The term is most often used for mortgage loans; corporate loans with negative amortization are called PIK loans.\n\nAmortization refers to the process of paying off a debt (often from a loan or mortgage) through regular payments. A portion of each payment is for interest while the remaining amount is applied towards the principal balance. The percentage of interest versus principal in each payment is determined in an amortization schedule.\n\nNegative amortization only occurs in loans in which the periodic payment does not cover the amount of interest due for that loan period. The unpaid accrued interest is then capitalized monthly into the outstanding principal balance. The result of this is that the loan balance (or principal) increases by the amount of the unpaid interest on a monthly basis. The purpose of such a feature is most often for advanced cash management and/or more simply payment flexibility, but not to increase overall affordability.\n\nNeg-Ams also have what is called a recast period, and the recast principal balance cap is in the U.S. based on Federal and State legislation. The recast period is usually 60 months (5 years). The recast principal balance cap (also known as the \"neg am limit\") is usually up to a 25% increase of the amortized loan balance over the original loan amount. States and lenders can offer products with lesser recast periods and principal balance caps; but cannot issue loans that exceed their state and federal legislated requirements under penalty of law.\n\nA newer loan option has been introduced which allows for a 40-year loan term. This makes the minimum payment even lower than a comparable 30-year term.\n\n\nAll NegAM home loans eventually require full repayment of principal and interest according to the original term of the mortgage and note signed by the borrower. Most loans only allow NegAM to happen for no more than 5 years, and have terms to \"Recast\" (see below) the payment to a fully amortizing schedule if the borrower allows the principal balance to rise to a pre-specified amount.\n\nThis loan is written often in high cost areas, because the monthly mortgage payments will be lower than any other type of financing instrument.\n\nNegative amortization loans can be high risk loans for inexperienced investors. These loans tend to be safer in a falling rate market and riskier in a rising rate market.\n\nStart rates on negative amortization or minimum payment option loans can be as low as 1%. This is the payment rate, not the actual interest rate. The payment rate is used to calculate the minimum payment. Other minimum payment options include 1.95% or more.\n\nNegAM loans today are mostly straight adjustable rate mortgages (ARMs), meaning that they are fixed for a certain period and adjust every time that period has elapsed; e.g., one month fixed, adjusting every month. The NegAm loan, like all adjustable rate mortgages, is tied to a specific financial index which is used to determine the interest rate based on the current index and the margin (the markup the lender charges). Most NegAm loans today are tied to the Monthly Treasury Average, in keeping with the monthly adjustments of this loan. There are also Hybrid ARM loans in which there is a period of fixed payments for months or years, followed by an increased change cycle, such as six months fixed, then monthly adjustable.\n\nThe graduated payment mortgage is a \"fixed rate\" NegAm loan, but since the payment increases over time, it has aspects of the ARM loan until amortizing payments are required.\n\nThe most notable differences between the traditional payment option ARM and the hybrid payment option ARM are in the start rate, also known as the \"minimum payment\" rate. On a Traditional Payment Option Arm, the minimum payment is based on a principal and interest calculation of 1% - 2.5% on average.\n\nThe start rate on a hybrid payment option ARM is higher, yet still extremely competitive payment wise.\n\nOn a hybrid payment option ARM, the minimum payment is derived using the \"interest only\" calculation of the start rate. The start rate on the hybrid payment option ARM typically is calculated by taking the fully indexed rate (actual note rate), then subtracting 3%, which will give you the start rate.\n\nExample: 7.5% fully indexed rate − 3% = 4.5% (4.5% would be the start rate on a hybrid pay option ARM)\n\nThis guideline can vary among lenders.\n\nAliases the payment option ARM loans are known by:\n\n(In general Author is using time references that are relative to a time frame that is not defined. 'Today' which is?; 'last 5 years' from when, etc.)\n\nNegative-amortization loans, being relatively popular only in the last decade, have attracted a variety of criticisms:\n\nIn a very hot real estate market a buyer may use a negative-amortizing mortgage to purchase a property with the plan to sell the property at a higher price before the end of the \"negam\" period. Therefore, an informed investor could purchase several properties with minimal monthly obligations and make a great profit over a five-year plan in a rising real-estate market.\n\nHowever, if the property values decrease, it is likely that the borrower will owe more on the property than it is worth, known colloquially in the mortgage industry as \"being underwater\". In this situation, if the property owner cannot make the new monthly payment, he or she may be faced with foreclosure or having to refinance with a very high loan-to-value ratio, requiring additional monthly obligations, such as mortgage insurance, and higher rates and payments due to the adversity of a high loan-to-value ratio.\n\nIt is very easy for borrowers to ignore or misunderstand the complications of this product when being presented with minimal monthly obligations that could be from one half to one third what other, more predictable, mortgage products require.\n"}
{"id": "1092282", "url": "https://en.wikipedia.org/wiki?curid=1092282", "title": "Negative frequency", "text": "Negative frequency\n\nThe concept of negative and positive frequency can be as simple as a wheel rotating one way or the other way: a \"signed value\" of frequency can indicate both the rate and direction of rotation. The rate is expressed in units such as revolutions (a.k.a. \"cycles\") per second (hertz) or radian/second (where 1 cycle corresponds to 2\"π\" radians).\n\nLet \"ω\" be a nonnegative parameter with units of radians/sec. Then the angular function (angle vs. time) , has slope −\"ω\", which is called a negative frequency. But when the function is used as the argument of a cosine operator, the result is indistinguishable from .  Similarly, is indistinguishable from . Thus any sinusoids can be represented in terms of positive frequencies. The sign of the underlying phase slope is ambiguous.\n\nThe ambiguity is resolved when the cosine and sine operators can be observed simultaneously, because leads by 1/4 cycle (= \"π\"/2 radians) when , and lags by 1/4 cycle when .  Similarly, a vector, , rotates counter-clockwise at 1 radian/sec, and completes a circle every 2π seconds, and the vector rotates in the other direction.\n\nThe sign of \"ω\" is also preserved in the complex-valued function:\n\nsince R(\"t\") and I(\"t\") can be separately extracted and compared. Although formula_1  clearly contains more information than either of its components, a common interpretation is that it is a simpler function, because:\nwhich gives rise to the interpretation that cos(\"ωt\") comprises \"both\" positive and negative frequencies.  But the sum is actually a cancellation that contains less, not more, information. Any measure that indicates both frequencies includes a false positive, because \"ω\" can have only one sign.  The Fourier transform, for instance, merely tells us that cos(\"ωt\") correlates equally well with both and with .\n\nPerhaps the most well-known application of negative frequency is the calculation:\n\nwhich is a measure of the amount of frequency ω in the function \"x\"(\"t\") over the interval . When evaluated as a continuous function of \"ω\" for the theoretical interval , it is known as the Fourier transform of \"x\"(\"t\"). A brief explanation is that the product of two complex sinusoids is also a complex sinusoid whose frequency is the sum of the original frequencies. So when \"ω\" is positive, formula_4 causes all the frequencies of \"x\"(\"t\") to be reduced by amount \"ω\". Whatever part of \"x\"(\"t\") that was at frequency \"ω\" is changed to frequency zero, which is just a constant whose amplitude level is a measure of the strength of the original \"ω\" content. And whatever part of \"x\"(\"t\") that was at frequency zero is changed to a sinusoid at frequency −\"ω\". Similarly, all other frequencies are changed to non-zero values. As the interval increases, the contribution of the constant term grows in proportion. But the contributions of the sinusoidal terms only oscillate around zero. So \"X\"(\"ω\") improves as a relative measure of the amount of frequency \"ω\" in the function \"x\"(\"t\").\n\nThe Fourier transform of  formula_1  produces a non-zero response only at frequency \"ω\". The transform of formula_2 has responses at both \"ω\" and −\"ω\", as anticipated by .\n\n"}
{"id": "8499571", "url": "https://en.wikipedia.org/wiki?curid=8499571", "title": "Negative probability", "text": "Negative probability\n\nThe probability of the outcome of an experiment is never negative, although a quasiprobability distribution allows a negative probability, or quasiprobability for some events. These distributions may apply to unobservable events or conditional probabilities.\n\nIn 1942, Paul Dirac wrote a paper \"The Physical Interpretation of Quantum Mechanics\" where he introduced the concept of negative energies and negative probabilities:\n\nThe idea of negative probabilities later received increased attention in physics and particularly in quantum mechanics. Richard Feynman argued that no one objects to using negative numbers in calculations: although \"minus three apples\" is not a valid concept in real life, negative money is valid. Similarly he argued how negative probabilities as well as probabilities above unity possibly could be useful in probability calculations.\n\nMark Burgin gives another example:\nNegative probabilities have later been suggested to solve several problems and paradoxes. \"Half-coins\" provide simple examples for negative probabilities. These strange coins were introduced in 2005 by Gábor J. Székely. Half-coins have infinitely many sides numbered with 0,1,2... and the positive even numbers are taken with negative probabilities. Two half-coins make a complete coin in the sense that if we flip two half-coins then the sum of the outcomes is 0 or 1 with probability 1/2 as if we simply flipped a fair coin.\n\nIn \"Convolution quotients of nonnegative definite functions\" and \"Algebraic Probability Theory\" Imre Z. Ruzsa and Gábor J. Székely proved that if a random variable X has a signed or quasi distribution where some of the probabilities are negative then one can always find two random variables, Y and Z, with ordinary (not signed / not quasi) distributions such that X, Y are independent and X + Y = Z in distribution. Thus X can always be interpreted as the \"difference\" of two ordinary random variables, Z and Y. If Y is interpreted as a measurement error of X and the observed value is Z then the negative regions of the distribution of X are masked / shielded by the error Y.\n\nAnother example known as the Wigner distribution in phase space, introduced by Eugene Wigner in 1932 to study quantum corrections, often leads to negative probabilities. For this reason, it has later been better known as the Wigner quasiprobability distribution. In 1945, M. S. Bartlett worked out the mathematical and logical consistency of such negative valuedness. The Wigner distribution function is routinely used in physics nowadays, and provides the cornerstone of phase-space quantization. Its negative features are an asset to the formalism, and often indicate quantum interference. The negative regions of the distribution are shielded from direct observation by the quantum uncertainty principle: typically, the moments of such a non-positive-semidefinite quasiprobability distribution are highly constrained, and prevent \"direct measurability\" of the negative regions of the distribution. But these regions contribute negatively and crucially to the expected values of observable quantities computed through such distributions, nevertheless.\n\nConsider a double slit experiment with photons. The two waves exiting each slit can be written as:\n\nformula_1\n\nand\n\nformula_2\n\nwhere \"d\" is the distance to the detection screen, \"a\" is the separation between the two slits, \"x\" the distance to the center of the screen, \"λ\" the wavelength and \"dN/dt\" is the number of photons emitted per unit time at the source. The amplitude of measuring a photon at distance \"x\" from the center of the screen is the sum of these two amplitudes coming out of each hole, and therefore the probability that a photon is detected at position \"x\" will be given by the square of this sum:\n\nformula_3,\n\nThis should strike you as the well-known probability rule:\n\nformula_4\n\nwhatever the last term means. Indeed, if one closes either one of the holes forcing the photon to go through the other slit, the two corresponding intensities are\n\nformula_5 and formula_6.\n\nBut now, if one does interpret each of these terms in this way, the joint probability takes negative values roughly every formula_7 !formula_8\n\nHowever, these negative probabilities are never observed as one can't isolate the cases in which the photon \"goes through both slits\", but can hint at the existence of anti-particles.\n\nNegative probabilities have more recently been applied to mathematical finance. In quantitative finance most probabilities are not real probabilities but pseudo probabilities, often what is known as risk neutral probabilities. These are not real probabilities, but theoretical \"probabilities\" under a series of assumptions that helps simplify calculations by allowing such pseudo probabilities to be negative in certain cases as first pointed out by Espen Gaarder Haug in 2004.\n\nA rigorous mathematical definition of negative probabilities and their properties was recently derived by Mark Burgin and Gunter Meissner (2011). The authors also show how negative probabilities can be applied to financial option pricing.\n\nThe concept of negative probabilities have also been proposed for reliable facility location models where facilities are subject to negatively correlated disruption risks when facility locations, customer allocation, and backup service plans are determined simultaneously. Li et al. proposed a virtual station structure that transforms a facility network with positively correlated disruptions into an equivalent one with added virtual supporting stations, and these virtual stations were subject to independent disruptions. This approach reduces a problem from one with correlated disruptions to one without. Xie et al. later showed how negatively correlated disruptions can also be addressed by the same modeling framework, except that a supporting station now may be disrupted with a “failure propensity” which\n\n“... inherits all mathematical characteristics and properties of a failure probability except that we allow it to be larger than 1...”\n\nThis finding paves ways for using compact mixed-integer mathematical programs to optimally design reliable location of service facilities under site-dependent and positive/negative/mixed disruption correlations.\n\nThe proposed “propensity” concept in Xie et al. turns out to be what Feynman and others referred to as “quasi-probability.” Note that when a quasi-probability is larger than 1, then 1 minus this value gives a negative probability. The truly physically verifiable observation is the facility disruption states, and there is no direct information on the station states or their corresponding probabilities. Hence the failure probability of the stations, interpreted as “probabilities of imagined intermediary states,” could exceed unity.\n\n"}
{"id": "3778432", "url": "https://en.wikipedia.org/wiki?curid=3778432", "title": "Negative repetition", "text": "Negative repetition\n\nA negative repetition (negative rep) is the repetition of a technique in weight lifting in which the lifter performs the eccentric phase of a lift. Instead of pressing the weight up slowly, in proper form, a spotter generally aids in the concentric, or lifting, portion of the repetition while the lifter slowly performs the eccentric phase for 3–6 seconds. Negative reps are used to improve both muscular strength and power in subjects, this is commonly known as hypertrophy training.\n\nDue to its mechanical properties, this form of training can be used for both healthy individuals and individuals who are in rehabilitation. Studies have shown that negative repetitions or \"eccentric phase training\" combines a high amount of force on the muscle with a lower energy cost than normal concentric training, which requires 4–5 times the amount of energy. This justifies why this type of training is more beneficial and less of a risk to subjects rehabilitating or with a limited exercise capacity.\n\nEccentric training is often associated with the terms \"muscles soreness\" and \"muscle damage\". In 1902, Theodore Hough discovered and developed the term DOMS (delayed onset muscle soreness), after he found that exercises containing negative repetitions caused athletes to have sore muscles. Hough believed this was causing a rupture within the muscle; when he looked further into the subject, he found that when performing eccentric exercise that exhibited soreness, the muscle \"quickly adapts and becomes accustomed to the increase in applied stress\". The result of this was that the muscles' soreness not only decreased, but the muscular damage did too.\n\nIt has been proven that eccentric resistance training improves the functional mobility of older adults. Studies have shown that eccentric training of the lower body, in particular the , are essential in preventing falls in older adults and helping them maintain their independence. A study conducted focusing on eccentric training for the age group 65–87 years of age showed that, over 12 weeks, they had strengthened their knee extensors by up to 26%. With this evidence, it is reasonable to suggest that negative repetitions can help improve the health of older adults.\n\nStudies have shown that eccentric training may be successful in the treatment of certain tendonitis. Studies have shown that the use of eccentric training for twelve weeks may be an alternative to therapy for people suffering from Patellar Tendinopathy (Jumper's Knee). Eccentric training has also been proven successful in the treatment of chronic Achilles tendonitis, using a twelve-week eccentric calf muscle program various studies have shown the ability for people to return to normal pre-tendonitis levels. The reasoning behind the benefits of eccentric training for tendinopathy is still unclear.\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "4295487", "url": "https://en.wikipedia.org/wiki?curid=4295487", "title": "One-electron universe", "text": "One-electron universe\n\nThe one-electron universe postulate, proposed by John Wheeler in a telephone call to Richard Feynman in the spring of 1940, hypothesises that all electrons and positrons are actually manifestations of a single entity moving backwards and forwards in time. According to Feynman:\n\nThe idea is based on the world lines traced out across spacetime by every electron. Rather than have myriad such lines, Wheeler suggested that they could all be parts of one single line like a huge tangled knot, traced out by the one electron. Any given moment in time is represented by a slice across spacetime, and would meet the knotted line a great many times. Each such meeting point represents a real electron at that moment.\n\nAt those points, half the lines will be directed forward in time and half will have looped round and be directed backwards. Wheeler suggested that these backwards sections appeared as the antiparticle to the electron, the positron.\n\nMany more electrons have been observed than positrons, and electrons are thought to comfortably outnumber them. According to Feynman he raised this issue with Wheeler, who speculated that the missing positrons might be hidden within protons.\n\nFeynman was struck by Wheeler's insight that antiparticles could be represented by reversed world lines, and credits this to Wheeler, saying in his Nobel speech:\n\nFeynman later proposed this interpretation of the positron as an electron moving backward in time in his 1949 paper \"The Theory of Positrons\". Yoichiro Nambu later applied it to all production and annihilation of particle-antiparticle pairs, stating that \"the eventual creation and annihilation of pairs that may occur now and then is no creation or annihilation, but only a change of direction of moving particles, from past to future, or from future to past.\"\n\n"}
{"id": "2903306", "url": "https://en.wikipedia.org/wiki?curid=2903306", "title": "Paper negative", "text": "Paper negative\n\nThe paper negative process consists of using a negative printed on paper (either photographically or digitally) to create the final print of a photograph, as opposed to using a modern negative on a film base of cellulose acetate. The plastic acetate negative (which is what modern films produce) enables the printing of a very sharp image intended to be as close a representation of the actual subject as is possible. By using a negative based on paper instead, there is the possibility of creating a more ethereal image, simply by using a paper with a very visible grain, or by drawing on the paper or distressing it in some way. \n\nOne of the original forms of photography was based on the paper negative process. William Henry Fox Talbot's paper negative process, which was used to create his work \"The Pencil of Nature\", used a negative created on paper treated with silver salts, which was exposed in a camera obscura to create the negative and then contact printed on a similar paper to produce a positive image. \n\nWhen Talbot created this process it was intended to be a way to reproduce nature as accurately as possible (hence the name of his work, \"The Pencil of Nature\"). Through the years afterwards, however, better and more accurate ways of producing exact replicas of nature were developed, and these processes relegated the paper negative process to obsolescence. \n\nThe process of the paper negative is still relevant, though, in the realm of alternative-process photography. Photographers employing alternative processes reject the idea of the exact replica of nature and seek to use the inherent inexactness of antiquated processes to create a more personal and emotional image. The paper negative is an extremely versatile process that allows all manner of reworking and retouching of an image, and is the perfect medium to bridge the gap between camera operator and artist.\n\n\n\n"}
{"id": "6880483", "url": "https://en.wikipedia.org/wiki?curid=6880483", "title": "Philosophy of mind", "text": "Philosophy of mind\n\nPhilosophy of mind is a branch of philosophy that studies the nature of the mind. The mind–body problem is a paradigm issue in philosophy of mind, although other issues are addressed, such as the hard problem of consciousness, and the nature of particular mental states. Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness, the ontology of the mind, the nature of thought, and the relationship of the mind to the body.\n\nDualism and monism are the two central schools of thought on the mind–body problem, although nuanced views have arisen that do not fit one or the other category neatly. Dualism is found in both Eastern and Western traditions (in the Sankhya and Yoga schools of Hindu philosophy as well as Plato) but its entry into Western philosophy was thanks to René Descartes in the 17th century. Substance dualists like Descartes argue that the mind is an independently existing substance, whereas property dualists maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance.\n\nMonism is the position that mind and body are not ontologically distinct entities (independent substances). This view was first advocated in Western philosophy by Parmenides in the 5th century BCE and was later espoused by the 17th century rationalist Baruch Spinoza. Physicalists argue that only entities postulated by physical theory exist, and that mental processes will eventually be explained in terms of these entities as physical theory continues to evolve. Physicalists maintain various positions on the prospects of reducing mental properties to physical properties (many of whom adopt compatible forms of property dualism), and the ontological status of such mental properties remains unclear. Idealists maintain that the mind is all that exists and that the external world is either mental itself, or an illusion created by the mind. Neutral monists such as Ernst Mach and William James argue that events in the world can be thought of as either mental (psychological) or physical depending on the network of relationships into which they enter, and dual-aspect monists such as Spinoza adhere to the position that there is some other, neutral substance, and that both matter and mind are properties of this unknown substance. The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism.\n\nMost modern philosophers of mind adopt either a reductive or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences. Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states. Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science. Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms.\n\nThe mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.\n\nOur perceptual experiences depend on stimuli that arrive at our various sensory organs from the external world, and these stimuli cause changes in our mental states, ultimately causing us to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties.\n\nA related problem is how someone's propositional attitudes (e.g. beliefs and desires) cause that individual's neurons to fire and muscles to contract. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.\n\nDualism is a set of views about the relationship between mind and matter (or body). It begins with the claim that mental phenomena are, in some respects, non-physical. One of the earliest known formulations of mind–body dualism was expressed in the eastern Sankhya and Yoga schools of Hindu philosophy (c. 650 BCE), which divided the world into purusha (mind/spirit) and prakriti (material substance). Specifically, the Yoga Sutra of Patanjali presents an analytical approach to the nature of the mind.\n\nIn Western Philosophy, the earliest discussions of dualist ideas are in the writings of Plato who maintained that humans' \"intelligence\" (a faculty of the mind or soul) could not be identified with, or explained in terms of, their physical body. However, the best-known version of dualism is due to René Descartes (1641), and holds that the mind is a non-extended, non-physical substance, a \"res cogitans\". Descartes was the first to clearly identify the mind with consciousness and self-awareness, and to distinguish this from the brain, which was the seat of intelligence. He was therefore the first to formulate the mind–body problem in the form in which it still exists today.\n\nThe most frequently used argument in favor of dualism appeals to the common-sense intuition that conscious experience is distinct from inanimate matter. If asked what the mind is, the average person would usually respond by identifying it with their self, their personality, their soul, or some other such entity. They would almost certainly deny that the mind simply is the brain, or vice versa, finding the idea that there is just one ontological entity at play to be too mechanistic, or simply unintelligible. Many modern philosophers of mind think that these intuitions are misleading and that we should use our critical faculties, along with empirical evidence from the sciences, to examine these assumptions to determine whether there is any real basis to them.\n\nAnother important argument in favor of dualism is that the mental and the physical seem to have quite different, and perhaps irreconcilable, properties. Mental events have a subjective quality, whereas physical events do not. So, for example, one can reasonably ask what a burnt finger feels like, or what a blue sky looks like, or what nice music sounds like to a person. But it is meaningless, or at least odd, to ask what a surge in the uptake of glutamate in the dorsolateral portion of the prefrontal cortex feels like.\n\nPhilosophers of mind call the subjective aspects of mental events \"qualia\" or \"raw feels\". There is something that it is like to feel pain, to see a familiar shade of blue, and so on. There are qualia involved in these mental events that seem particularly difficult to reduce to anything physical. David Chalmers explains this argument by stating that we could conceivably know all the objective information about something, such as the brain states and wavelengths of light involved with seeing the color red, but still not know something fundamental about the situation – what it is like to see the color red.\n\nIf consciousness (the mind) can exist independently of physical reality (the brain), one must explain how physical memories are created concerning consciousness. Dualism must therefore explain how consciousness affects physical reality. One possible explanation is that of a miracle, proposed by Arnold Geulincx and Nicolas Malebranche, where all mind–body interactions require the direct intervention of God.\n\nAnother possible argument that has been proposed by C. S. Lewis is the Argument from Reason: if, as monism implies, all of our thoughts are the effects of physical causes, then we have no reason for assuming that they are also the consequent of a reasonable ground. Knowledge, however, is apprehended by reasoning from ground to consequent. Therefore, if monism is correct, there would be no way of knowing this—or anything else—we could not even suppose it, except by a fluke.\n\nThe zombie argument is based on a thought experiment proposed by Todd Moody, and developed by David Chalmers in his book \"The Conscious Mind\". The basic idea is that one can imagine one's body, and therefore conceive the existence of one's body, without any conscious states being associated with this body. Chalmers' argument is that it seems possible that such a being could exist because all that is needed is that all and only the things that the physical sciences describe about a zombie must be true of it. Since none of the concepts involved in these sciences make reference to consciousness or other mental phenomena, and any physical entity can be by definition described scientifically via physics, the move from conceivability to possibility is not such a large one. Others such as Dennett have argued that the notion of a philosophical zombie is an incoherent, or unlikely, concept. It has been argued under physicalism that one must either believe that anyone including oneself might be a zombie, or that no one can be a zombie—following from the assertion that one's own conviction about being (or not being) a zombie is a product of the physical world and is therefore no different from anyone else's. This argument has been expressed by Dennett who argues that \"Zombies think they are conscious, think they have qualia, think they suffer pains—they are just 'wrong' (according to this lamentable tradition) in ways that neither they nor we could ever discover!\"\nSee also the problem of other minds.\n\nInteractionist dualism, or simply interactionism, is the particular form of dualism first espoused by Descartes in the \"Meditations\". In the 20th century, its major defenders have been Karl Popper and John Carew Eccles. It is the view that mental states, such as beliefs and desires, causally interact with physical states.\n\nDescartes' famous argument for this position can be summarized as follows: Seth has a clear and distinct idea of his mind as a thinking thing that has no spatial extension (i.e., it cannot be measured in terms of length, weight, height, and so on). He also has a clear and distinct idea of his body as something that is spatially extended, subject to quantification and not able to think. It follows that mind and body are not identical because they have radically different properties.\n\nAt the same time, however, it is clear that Seth's mental states (desires, beliefs, etc.) have causal effects on his body and vice versa: A child touches a hot stove (physical event) which causes pain (mental event) and makes her yell (physical event), this in turn provokes a sense of fear and protectiveness in the caregiver (mental event), and so on.\n\nDescartes' argument crucially depends on the premise that what Seth believes to be \"clear and distinct\" ideas in his mind are necessarily true. Many contemporary philosophers doubt this. For example, Joseph Agassi suggests that several scientific discoveries made since the early 20th century have undermined the idea of privileged access to one's own ideas. Freud claimed that a psychologically-trained observer can understand a person's unconscious motivations better than the person himself does. Duhem has shown that a philosopher of science can know a person's methods of discovery better than that person herself does, while Malinowski has shown that an anthropologist can know a person's customs and habits better than the person whose customs and habits they are. He also asserts that modern psychological experiments that cause people to see things that are not there provide grounds for rejecting Descartes' argument, because scientists can describe a person's perceptions better than the person herself can.\n\nPsychophysical parallelism, or simply parallelism, is the view that mind and body, while having distinct ontological statuses, do not causally influence one another. Instead, they run along parallel paths (mind events causally interact with mind events and brain events causally interact with brain events) and only seem to influence each other. This view was most prominently defended by Gottfried Leibniz. Although Leibniz was an ontological monist who believed that only one type of substance, the monad, exists in the universe, and that everything is reducible to it, he nonetheless maintained that there was an important distinction between \"the mental\" and \"the physical\" in terms of causation. He held that God had arranged things in advance so that minds and bodies would be in harmony with each other. This is known as the doctrine of pre-established harmony.\n\nOccasionalism is the view espoused by Nicholas Malebranche as well as Islamic philosophers such as Abu Hamid Muhammad ibn Muhammad al-Ghazali that asserts that all supposedly causal relations between physical events, or between physical and mental events, are not really causal at all. While body and mind are different substances, causes (whether mental or physical) are related to their effects by an act of God's intervention on each specific occasion.\n\nProperty dualism is the view that the world is constituted of just one kind of substance – the physical kind – and there exist two distinct kinds of properties: physical properties and mental properties. In other words, it is the view that non-physical, mental properties (such as beliefs, desires and emotions) inhere in some physical bodies (at least, brains). How mental and physical properties relate causally depends on the variety of property dualism in question, and is not always a clear issue. Sub-varieties of property dualism include:\n\n\nDual aspect theory or dual-aspect monism is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. (Thus it is a mixed position, which is monistic in some respects). In modern philosophical writings, the theory's relationship to neutral monism has become somewhat ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements and the relationships into which they enter to determine whether the group can be thought of as mental, physical, both, or neither, dual-aspect theory suggests that the mental and the physical are manifestations (or aspects) of some underlying substance, entity or process that is itself neither mental nor physical as normally understood. Various formulations of dual-aspect monism also require the mental and the physical to be complementary, mutually irreducible and perhaps inseparable (though distinct).\n\nThis is a philosophy of mind that regards the degrees of freedom between mental and physical well-being as not necessarily synonymous thus implying an experiential dualism between body and mind. An example of these disparate degrees of freedom is given by Allan Wallace who notes that it is \"experientially apparent that one may be physically uncomfortable—for instance, while engaging in a strenuous physical workout—while mentally cheerful; conversely, one may be mentally distraught while experiencing physical comfort\". Experiential dualism notes that our subjective experience of merely seeing something in the physical world seems qualitatively different than mental processes like grief that comes from losing a loved one. This philosophy also is a proponent of causal dualism which is defined as the dual ability for mental states and physical states to affect one another. Mental states can cause changes in physical states and vice versa.\n\nHowever, unlike cartesian dualism or some other systems, experiential dualism does not posit two fundamental substances in reality: mind and matter. Rather, experiential dualism is to be understood as a conceptual framework that gives credence to the qualitative difference between the experience of mental and physical states. Experiential dualism is accepted as the conceptual framework of Madhyamaka Buddhism.\n\nMadhayamaka Buddhism goes even further, finding fault with the monist view of physicalist philosophies of mind as well in that these generally posit matter and energy as the fundamental substance of reality. Nonetheless, this does not imply that the cartesian dualist view is correct, rather Madhyamaka regards as error any affirming view of a fundamental substance to reality.In denying the independent self-existence of all the phenomena that make up the world of our experience, the Madhyamaka view departs from both the substance dualism of Descartes and the substance monism—namely, physicalism—that is characteristic of modern science. The physicalism propounded by many contemporary scientists seems to assert that the real world is composed of physical things-in-themselves, while all mental phenomena are regarded as mere appearances, devoid of any reality in and of themselves. Much is made of this difference between appearances and reality.\nIndeed, physicalism, or the idea that matter is the only fundamental substance of reality, is explicitly rejected by Buddhism.In the Madhyamaka view, mental events are no more or less real than physical events. In terms of our common-sense experience, differences of kind do exist between physical and mental phenomena. While the former commonly have mass, location, velocity, shape, size, and numerous other physical attributes, these are not generally characteristic of mental phenomena. For example, we do not commonly conceive of the feeling of affection for another person as having mass or location. These physical attributes are no more appropriate to other mental events such as sadness, a recalled image from one's childhood, the visual perception of a rose, or consciousness of any sort. Mental phenomena are, therefore, not regarded as being physical, for the simple reason that they lack many of the attributes that are uniquely characteristic of physical phenomena. Thus, Buddhism has never adopted the physicalist principle that regards only physical things as real.\n\nHylomorphism is a theory that originates with Aristotelian philosophy, which conceives being as a compound of matter and form. \"Hylomorphism\" is a 19th-century term formed from the Greek words ὕλη \"hyle\", \"wood, matter\", and μορφή, \"morphē\", \"form\".\n\nIn contrast to dualism, monism does not accept any fundamental divisions. The fundamentally disparate nature of reality has been central to forms of eastern philosophies for over two millennia. In Indian and Chinese philosophy, monism is integral to how experience is understood. Today, the most common forms of monism in Western philosophy are physicalist. Physicalistic monism asserts that the only existing substance is physical, in some sense of that term to be clarified by our best science. However, a variety of formulations (see below) are possible. Another form of monism, idealism, states that the only existing substance is mental. Although pure idealism, such as that of George Berkeley, is uncommon in contemporary Western philosophy, a more sophisticated variant called panpsychism, according to which mental experience and properties may be at the foundation of physical experience and properties, has been espoused by some philosophers such as Alfred North Whitehead and David Ray Griffin.\n\nPhenomenalism is the theory that representations (or sense data) of external objects are all that exist. Such a view was briefly adopted by Bertrand Russell and many of the logical positivists during the early 20th century. A third possibility is to accept the existence of a basic substance that is neither physical nor mental. The mental and physical would then both be properties of this neutral substance. Such a position was adopted by Baruch Spinoza and was popularized by Ernst Mach in the 19th century. This neutral monism, as it is called, resembles property dualism.\n\nBehaviorism dominated philosophy of mind for much of the 20th century, especially the first half. In psychology, behaviorism developed as a reaction to the inadequacies of introspectionism. Introspective reports on one's own interior mental life are not subject to careful examination for accuracy and cannot be used to form predictive generalizations. Without generalizability and the possibility of third-person examination, the behaviorists argued, psychology cannot be scientific. The way out, therefore, was to eliminate the idea of an interior mental life (and hence an ontologically independent mind) altogether and focus instead on the description of observable behavior.\n\nParallel to these developments in psychology, a philosophical behaviorism (sometimes called logical behaviorism) was developed. This is characterized by a strong verificationism, which generally considers unverifiable statements about interior mental life pointless. For the behaviorist, mental states are not interior states on which one can make introspective reports. They are just descriptions of behavior or dispositions to behave in certain ways, made by third parties to explain and predict another's behavior.\n\nPhilosophical behaviorism has fallen out of favor since the latter half of the 20th century, coinciding with the rise of cognitivism. Cognitivists reject behaviorism due to several perceived problems. For example, behaviorism could be said to be counterintuitive when it maintains that someone is talking about behavior in the event that a person is experiencing a painful headache.\n\nType physicalism (or type-identity theory) was developed by John Smart and Ullin Place as a direct reaction to the failure of behaviorism. These philosophers reasoned that, if mental states are something material, but not behavioral, then mental states are probably identical to internal states of the brain. In very simplified terms: a mental state \"M\" is nothing other than brain state \"B\". The mental state \"desire for a cup of coffee\" would thus be nothing more than the \"firing of certain neurons in certain brain regions\".\nDespite its initial plausibility, the identity theory faces a strong challenge in the form of the thesis of multiple realizability, first formulated by Hilary Putnam. For example, not only humans, but many different species of animals can experience pain. However, it seems highly unlikely that all of these diverse organisms with the same pain experience are in the identical brain state. And if this is the case, then pain cannot be identical to a specific brain state. The identity theory is thus empirically unfounded.\n\nOn the other hand, even granted the above, it does not follow that identity theories of all types must be abandoned. According to token identity theories, the fact that a certain brain state is connected with only one mental state of a person does not have to mean that there is an absolute correlation between types of mental state and types of brain state. The type–token distinction can be illustrated by a simple example: the word \"green\" contains four types of letters (g, r, e, n) with two tokens (occurrences) of the letter \"e\" along with one each of the others.\nThe idea of token identity is that only particular occurrences of mental events are identical with particular occurrences or tokenings of physical events. Anomalous monism (see below) and most other non-reductive physicalisms are token-identity theories. Despite these problems, there is a renewed interest in the type identity theory today, primarily due to the influence of Jaegwon Kim.\n\nFunctionalism was formulated by Hilary Putnam and Jerry Fodor as a reaction to the inadequacies of the identity theory. Putnam and Fodor saw mental states in terms of an empirical computational theory of the mind. At about the same time or slightly after, D.M. Armstrong and David Kellogg Lewis formulated a version of functionalism that analyzed the mental concepts of folk psychology in terms of functional roles. Finally, Wittgenstein's idea of meaning as use led to a version of functionalism as a theory of meaning, further developed by Wilfrid Sellars and Gilbert Harman. Another one, psychofunctionalism, is an approach adopted by the naturalistic philosophy of mind associated with Jerry Fodor and Zenon Pylyshyn.\n\nWhat all these different varieties of functionalism share in common is the thesis that mental states are characterized by their causal relations with other mental states and with sensory inputs and behavioral outputs. That is, functionalism abstracts away from the details of the physical implementation of a mental state by characterizing it in terms of non-mental functional properties. For example, a kidney is characterized scientifically by its functional role in filtering blood and maintaining certain chemical balances. From this point of view, it does not really matter whether the kidney be made up of organic tissue, plastic nanotubes or silicon chips: it is the role that it plays and its relations to other organs that define it as a kidney.\n\nNon-reductionist philosophers hold firmly to two essential convictions with regard to mind–body relations: 1) Physicalism is true and mental states must be physical states, but 2) All reductionist proposals are unsatisfactory: mental states cannot be reduced to behavior, brain states or functional states. Hence, the question arises whether there can still be a non-reductive physicalism. Donald Davidson's anomalous monism is an attempt to formulate such a physicalism. He \"thinks that when one runs across what are traditionally seen as absurdities of Reason, such as akrasia or self-deception, the personal psychology framework is not to be given up in favor of the subpersonal one, but rather must be enlarged or extended so that the rationality set out by the principle of charity can be found elsewhere.\"\n\nDavidson uses the thesis of supervenience: mental states supervene on physical states, but are not reducible to them. \"Supervenience\" therefore describes a functional dependence: there can be no change in the mental without some change in the physical–causal reducibility between the mental and physical without ontological reducibility.\n\nBecause non-reductive physicalist theories attempt to both retain the ontological distinction between mind and body and try to solve the \"surfeit of explanations puzzle\" in some way; critics often see this as a paradox and point out the similarities to epiphenomenalism, in that it is the brain that is seen as the root \"cause\" not the mind, and the mind seems to be rendered inert.\n\nEpiphenomenalism regards one or more mental states as the byproduct of physical brain states, having no influence on physical states. The interaction is one-way (solving the \"surfeit of explanations puzzle\") but leaving us with non-reducible mental states (as a byproduct of brain states) – causally reducible, but ontologically irreducible to physical states. Pain would be seen by epiphenomenaliasts as being caused by the brain state but as not having effects on other brain states, though it might have effects on other mental states (i.e. cause distress).\n\nWeak emergentism is a form of \"non-reductive physicalism\" that involves a layered view of nature, with the layers arranged in terms of increasing complexity and each corresponding to its own special science. Some philosophers hold that emergent properties causally interact with more fundamental levels, while others maintain that higher-order properties simply supervene over lower levels without direct causal interaction. The latter group therefore holds a less strict, or \"weaker\", definition of emergentism, which can be rigorously stated as follows: a property P of composite object O is emergent if it is metaphysically impossible for another object to lack property P if that object is composed of parts with intrinsic properties identical to those in O and has those parts in an identical configuration.\n\nSometimes emergentists use the example of water having a new property when Hydrogen H and Oxygen O combine to form HO (water). In this example there \"emerges\" a new property of a transparent liquid that would not have been predicted by understanding hydrogen and oxygen as gases. This is analogous to physical properties of the brain giving rise to a mental state. Emergentists try to solve the notorious mind–body gap this way. One problem for emergentism is the idea of \"causal closure\" in the world that does not allow for a mind-to-body causation.\n\nIf one is a materialist and believes that all aspects of our common-sense psychology will find reduction to a mature cognitive neuroscience, and that non-reductive materialism is mistaken, then one can adopt a final, more radical position: eliminative materialism.\n\nThere are several varieties of eliminative materialism, but all maintain that our common-sense \"folk psychology\" badly misrepresents the nature of some aspect of cognition. Eliminativists such as Patricia and Paul Churchland argue that while folk psychology treats cognition as fundamentally sentence-like, the non-linguistic vector/matrix model of neural network theory or connectionism will prove to be a much more accurate account of how the brain works.\n\nThe Churchlands often invoke the fate of other, erroneous popular theories and ontologies that have arisen in the course of history. For example, Ptolemaic astronomy served to explain and roughly predict the motions of the planets for centuries, but eventually this model of the solar system was eliminated in favor of the Copernican model. The Churchlands believe the same eliminative fate awaits the \"sentence-cruncher\" model of the mind in which thought and behavior are the result of manipulating sentence-like states called \"propositional attitudes\".\n\nIdealism is the form of monism that sees the world as consisting of minds, mental contents and or consciousness.\nIdealists are not faced with explaining how minds arise from bodies: rather, the world, bodies and objects are regarded as mere appearances held by minds. However, accounting for the mind–body problem is not usually the main motivation for idealism; rather, idealists tend to be motivated by skepticism, intentionality, and the unique nature of ideas.\nIdealism is prominent in Eastern religious and philosophical thought. It has gone through several cycles of popularity and neglect in the history of Western philosophy.\n\nDifferent varieties of idealism may hold that there are\n\nNeutral monism, in philosophy, is the metaphysical view that the mental and the physical are two ways of organizing or describing the same elements, which are themselves \"neutral\", that is, neither physical nor mental. This view denies that the mental and the physical are two fundamentally different things. Rather, neutral monism claims the universe consists of only one kind of stuff, in the form of neutral elements that are in themselves neither mental nor physical. These neutral elements might have the properties of color and shape, just as we experience those properties. But these shaped and colored elements do not exist in a mind (considered as a substantial entity, whether dualistically or physicalistically); they exist on their own.\n\nSome philosophers take an epistemic approach and argue that the mind–body problem is currently unsolvable, and perhaps will always remain unsolvable to human beings. This is usually termed New mysterianism. Colin McGinn holds that human beings are cognitively closed in regards to their own minds. According to McGinn human minds lack the concept-forming procedures to fully grasp how mental properties such as consciousness arise from their causal basis. An example would be how an elephant is cognitively closed in regards to particle physics.\n\nA more moderate conception has been expounded by Thomas Nagel, which holds that the mind–body problem is currently unsolvable at the present stage of scientific development and that it might take a future scientific paradigm shift or revolution to bridge the explanatory gap. Nagel posits that in the future a sort of \"objective phenomenology\" might be able to bridge the gap between subjective conscious experience and its physical basis.\n\nEach attempt to answer the mind–body problem encounters substantial problems. Some philosophers argue that this is because there is an underlying conceptual confusion. These philosophers, such as Ludwig Wittgenstein and his followers in the tradition of linguistic criticism, therefore reject the problem as illusory. They argue that it is an error to ask how mental and biological states fit together. Rather it should simply be accepted that human experience can be described in different ways—for instance, in a mental and in a biological vocabulary. Illusory problems arise if one tries to describe the one in terms of the other's vocabulary or if the mental vocabulary is used in the wrong contexts. This is the case, for instance, if one searches for mental states of the brain. The brain is simply the wrong context for the use of mental vocabulary—the search for mental states of the brain is therefore a category error or a sort of fallacy of reasoning.\n\nToday, such a position is often adopted by interpreters of Wittgenstein such as Peter Hacker. However, Hilary Putnam, the originator of functionalism, has also adopted the position that the mind–body problem is an illusory problem which should be dissolved according to the manner of Wittgenstein.\n\nWhere is the mind located? If the mind is a physical phenomenon of some kind, it has to be located somewhere. According to some, there are two possible options: either the mind is internal to the body (internalism) or the mind is external to it (externalism). More generally, either the mind depends only on events and properties taking place inside the subject's body or it depends also on factors external to it.\n\nProponents of internalism are committed to the view that neural activity is sufficient to produce the mind.\nProponents of externalism maintain that the surrounding world is in some sense constitutive of the mind.\n\nExternalism differentiates into several versions. The main ones are semantic externalism, cognitive externalism and phenomenal externalism. Each of these versions of externalism can further be divided into whether they refer only to the content or to the vehicles of the mind.\n\nSemantic externalism holds that the semantic content of the mind is totally or partially defined by a state of affairs external to the body of the subject. Hilary Putnam's Twin Earth thought experiment is a good example.\n\nCognitive externalism is a very broad collection of views that suggests the role of the environment, of tools, of development, and of the body in fleshing out cognition. Embodied cognition, the extended mind, and enactivism are good examples.\n\nPhenomenal externalism suggests that the phenomenal aspects of the mind are external to the body. Authors who addressed this possibility are Ted Honderich, Edwin Holt, Francois Tonneau, Kevin O'Regan, Riccardo Manzotti, Teed Rockwell and Max Velmans.\n\nThe thesis of physicalism is that the mind is part of the material (or physical) world. Such a position faces the problem that the mind has certain properties that no other material thing seems to possess. Physicalism must therefore explain how it is possible that these properties can nonetheless emerge from a material thing. The project of providing such an explanation is often referred to as the \"naturalization of the mental\". Some of the crucial problems that this project attempts to resolve include the existence of qualia and the nature of intentionality.\n\nMany mental states seem to be experienced subjectively in different ways by different individuals. And it is characteristic of a mental state that it has some experiential \"quality\", e.g. of pain, that it hurts. However, the sensation of pain between two individuals may not be identical, since no one has a perfect way to measure how much something hurts or of describing exactly how it feels to hurt. Philosophers and scientists therefore ask where these experiences come from. The existence of cerebral events, in and of themselves, cannot explain why they are accompanied by these corresponding qualitative experiences. The puzzle of why many cerebral processes occur with an accompanying experiential aspect in consciousness seems impossible to explain.\n\nYet it also seems to many that science will eventually have to explain such experiences. This follows from an assumption about the possibility of reductive explanations. According to this view, if an attempt can be successfully made to explain a phenomenon reductively (e.g., water), then it can be explained why the phenomenon has all of its properties (e.g., fluidity, transparency). In the case of mental states, this means that there needs to be an explanation of why they have the property of being experienced in a certain way.\n\nThe 20th-century German philosopher Martin Heidegger criticized the ontological assumptions underpinning such a reductive model, and claimed that it was impossible to make sense of experience in these terms. This is because, according to Heidegger, the nature of our subjective experience and its \"qualities\" is impossible to understand in terms of Cartesian \"substances\" that bear \"properties\". Another way to put this is that the very concept of qualitative experience is incoherent in terms of—or is semantically incommensurable with the concept of—substances that bear properties.\n\nThis problem of explaining introspective first-person aspects of mental states and consciousness in general in terms of third-person quantitative neuroscience is called the explanatory gap. There are several different views of the nature of this gap among contemporary philosophers of mind. David Chalmers and the early Frank Jackson interpret the gap as ontological in nature; that is, they maintain that qualia can never be explained by science because physicalism is false. There are two separate categories involved and one cannot be reduced to the other. An alternative view is taken by philosophers such as Thomas Nagel and Colin McGinn. According to them, the gap is epistemological in nature. For Nagel, science is not yet able to explain subjective experience because it has not yet arrived at the level or kind of knowledge that is required. We are not even able to formulate the problem coherently. For McGinn, on other hand, the problem is one of permanent and inherent biological limitations. We are not able to resolve the explanatory gap because the realm of subjective experiences is cognitively closed to us in the same manner that quantum physics is cognitively closed to elephants. Other philosophers liquidate the gap as purely a semantic problem. This semantic problem, of course, led to the famous \"Qualia Question\", which is: \"Does Red cause Redness\"?\n\nIntentionality is the capacity of mental states to be directed towards (\"about\") or be in relation with something in the external world. This property of mental states entails that they have contents and semantic referents and can therefore be assigned truth values. When one tries to reduce these states to natural processes there arises a problem: natural processes are not true or false, they simply happen. It would not make any sense to say that a natural process is true or false. But mental ideas or judgments are true or false, so how then can mental states (ideas or judgments) be natural processes? The possibility of assigning semantic value to ideas must mean that such ideas are about facts. Thus, for example, the idea that Herodotus was a historian refers to Herodotus and to the fact that he was a historian. If the fact is true, then the idea is true; otherwise, it is false. But where does this relation come from? In the brain, there are only electrochemical processes and these seem not to have anything to do with Herodotus.\n\nPhilosophy of perception is concerned with the nature of perceptual experience and the status of perceptual objects, in particular how perceptual experience relates to appearances and beliefs about the world. The main contemporary views within philosophy of perception include naive realism, enactivism and representational views.\n\nHumans are corporeal beings and, as such, they are subject to examination and description by the natural sciences. Since mental processes are intimately related to bodily processes, the descriptions that the natural sciences furnish of human beings play an important role in the philosophy of mind. There are many scientific disciplines that study processes related to the mental. The list of such sciences includes: biology, computer science, cognitive science, cybernetics, linguistics, medicine, pharmacology, and psychology.\n\nThe theoretical background of biology, as is the case with modern natural sciences in general, is fundamentally materialistic. The objects of study are, in the first place, physical processes, which are considered to be the foundations of mental activity and behavior. The increasing success of biology in the explanation of mental phenomena can be seen by the absence of any empirical refutation of its fundamental presupposition: \"there can be no change in the mental states of a person without a change in brain states.\"\n\nWithin the field of neurobiology, there are many subdisciplines that are concerned with the relations between mental and physical states and processes: Sensory neurophysiology investigates the relation between the processes of perception and stimulation. Cognitive neuroscience studies the correlations between mental processes and neural processes. Neuropsychology describes the dependence of mental faculties on specific anatomical regions of the brain. Lastly, evolutionary biology studies the origins and development of the human nervous system and, in as much as this is the basis of the mind, also describes the ontogenetic and phylogenetic development of mental phenomena beginning from their most primitive stages. Evolutionary biology furthermore places tight constraints on any philosophical theory of the mind, as the gene-based mechanism of natural selection does not allow any giant leaps in the development of neural complexity or neural software but only incremental steps over long time periods.\n\nThe methodological breakthroughs of the neurosciences, in particular the introduction of high-tech neuroimaging procedures, has propelled scientists toward the elaboration of increasingly ambitious research programs: one of the main goals is to describe and comprehend the neural processes which correspond to mental functions (see: neural correlate). Several groups are inspired by these advances.\n\nComputer science concerns itself with the automatic processing of information (or at least with physical systems of symbols to which information is assigned) by means of such things as computers. From the beginning, computer programmers have been able to develop programs that permit computers to carry out tasks for which organic beings need a mind. A simple example is multiplication. It is not clear whether computers could be said to have a mind. Could they, someday, come to have what we call a mind? This question has been propelled into the forefront of much philosophical debate because of investigations in the field of artificial intelligence (AI).\n\nWithin AI, it is common to distinguish between a modest research program and a more ambitious one: this distinction was coined by John Searle in terms of a weak AI and strong AI. The exclusive objective of \"weak AI\", according to Searle, is the successful simulation of mental states, with no attempt to make computers become conscious or aware, etc. The objective of strong AI, on the contrary, is a computer with consciousness similar to that of human beings. The program of strong AI goes back to one of the pioneers of computation Alan Turing. As an answer to the question \"Can computers think?\", he formulated the famous Turing test. Turing believed that a computer could be said to \"think\" when, if placed in a room by itself next to another room that contained a human being and with the same questions being asked of both the computer and the human being by a third party human being, the computer's responses turned out to be indistinguishable from those of the human. Essentially, Turing's view of machine intelligence followed the behaviourist model of the mind—intelligence is as intelligence does. The Turing test has received many criticisms, among which the most famous is probably the Chinese room thought experiment formulated by Searle.\n\nThe question about the possible sensitivity (qualia) of computers or robots still remains open. Some computer scientists believe that the specialty of AI can still make new contributions to the resolution of the \"mind–body problem\". They suggest that based on the reciprocal influences between software and hardware that takes place in all computers, it is possible that someday theories can be discovered that help us to understand the reciprocal influences between the human mind and the brain (wetware).\n\nPsychology is the science that investigates mental states directly. It uses generally empirical methods to investigate concrete mental states like joy, fear or obsessions. Psychology investigates the laws that bind these mental states to each other or with inputs and outputs to the human organism.\n\nAn example of this is the psychology of perception. Scientists working in this field have discovered general principles of the perception of forms. A law of the psychology of forms says that objects that move in the same direction are perceived as related to each other. This law describes a relation between visual input and mental perceptual states. However, it does not suggest anything about the nature of perceptual states. The laws discovered by psychology are compatible with all the answers to the mind–body problem already described.\n\nCognitive science is the interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does, and how it works. It includes research on intelligence and behavior, especially focusing on how information is represented, processed, and transformed (in faculties such as perception, language, memory, reasoning, and emotion) within nervous systems (human or other animal) and machines (e.g. computers). Cognitive science consists of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience, linguistics, anthropology, sociology, and education. It spans many levels of analysis, from low-level learning and decision mechanisms to high-level logic and planning; from neural circuitry to modular brain organisation. Rowlands argues that cognition is enactive, embodied, embedded, affective and (potentially) extended. The position is taken that the \"classical sandwich\" of cognition sandwiched between perception and action is artificial; cognition has to be seen as a product of a strongly coupled interaction that cannot be divided this way.\n\nMost of the discussion in this article has focused on one style or tradition of philosophy in modern Western culture, usually called analytic philosophy (sometimes described as Anglo-American philosophy). Many other schools of thought exist, however, which are sometimes subsumed under the broad (and vague) label of continental philosophy. In any case, though topics and methods here are numerous, in relation to the philosophy of mind the various schools that fall under this label (phenomenology, existentialism, etc.) can globally be seen to differ from the analytic school in that they focus less on language and logical analysis alone but also take in other forms of understanding human existence and experience. With reference specifically to the discussion of the mind, this tends to translate into attempts to grasp the concepts of thought and perceptual experience in some sense that does not merely involve the analysis of linguistic forms.\n\nImmanuel Kant's \"Critique of Pure Reason\", first published in 1781 and presented again with major revisions in 1787, represents a significant intervention into what will later become known as the philosophy of mind. Kant's first critique is generally recognized as among the most significant works of modern philosophy in the West. Kant is a figure whose influence is marked in both continental and analytic/Anglo-American philosophy. Kant's work develops an in-depth study of transcendental consciousness, or the life of the mind as conceived through universal categories of consciousness.\n\nIn Georg Wilhelm Friedrich Hegel's \"Philosophy of Mind\" (frequently translated as \"Philosophy of Spirit\" or Geist), the third part of his \"Encyclopedia of the Philosophical Sciences\", Hegel discusses three distinct types of mind: the \"subjective mind/spirit\", the mind of an individual; the \"objective mind/spirit\", the mind of society and of the State; and the \"Absolute mind/spirit\", the position of religion, art, and philosophy. See also Hegel's \"The Phenomenology of Spirit\". Nonetheless, Hegel's work differs radically from the style of Anglo-American philosophy of mind.\n\nIn 1896, Henri Bergson made in \"Matter and Memory\" \"Essay on the relation of body and spirit\" a forceful case for the ontological difference of body and mind by reducing the problem to the more definite one of memory, thus allowing for a solution built on the \"empirical test case\" of aphasia.\n\nIn modern times, the two main schools that have developed in response or opposition to this Hegelian tradition are phenomenology and existentialism. Phenomenology, founded by Edmund Husserl, focuses on the contents of the human mind (see noema) and how processes shape our experiences. Existentialism, a school of thought founded upon the work of Søren Kierkegaard, focuses on Human predicament and how people deal with the situation of being alive. Existential-phenomenology represents a major branch of continental philosophy (they are not contradictory), rooted in the work of Husserl but expressed in its fullest forms in the work of Martin Heidegger, Jean-Paul Sartre, Simone de Beauvoir and Maurice Merleau-Ponty. See Heidegger's \"Being and Time\", Merleau-Ponty's \"Phenomenology of Perception\", Sartre's \"Being and Nothingness\", and Simone de Beauvoir's \"The Second Sex\".\n\nSubstance Dualism is a common feature of several orthodox Hindu schools including the Sāṅkhya, Nyāya, Yoga and Dvaita Vedanta. In these schools a clear difference is drawn between matter and a non-material soul, which is eternal and undergoes samsara, a cycle of death and rebirth. The Nyāya school argued that qualities such as cognition and desire are inherent qualities which are not possessed by anything solely material, and therefore by process of elimination must belong to a non-material self, the atman. Many of these schools see their spiritual goal as moksha, liberation from the cycle of reincarnation.\n\nIn the Advaita Vedanta of the 8th century Indian philosopher Śaṅkara, the mind, body and world are all held to be the same unchanging eternal conscious entity called Brahman. Advaita, which means non-dualism, holds the view that all that exists is pure absolute consciousness. The fact that the world seems to be made up of changing entities is an illusion, or Maya. The only thing that exists is Brahman, which is described as Satchitananda (Being, consciousness and bliss). Advaita Vedanta is best described by a verse which states \"Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman.\"\n\nAnother form of monistic Vedanta is Vishishtadvaita (qualified non-dualism) as posited by the eleventh century philosopher Ramanuja. Ramanuja criticized Advaita Vedanta by arguing that consciousness is always intentional and that it is also always a property of something. Ramanuja's Brahman is defined by a multiplicity of qualities and properties in a single monistic entity. This doctrine is called \"samanadhikaranya\" (several things in a common substrate).\n\nArguably the first exposition of empirical materialism in the history of philosophy is in the Cārvāka school (also called Lokāyata). The Cārvāka school rejected the existence of anything but matter (which they defined as being made up of the four elements), including God and the soul. Therefore, they held that even consciousness was nothing but a construct made up of atoms. A section of the Cārvāka school believed in a material soul made up of air or breath, but since this also was a form of matter, it was not said to survive death.\n\nBuddhist teachings describe that the mind manifests moment-to-moment as sense impressions and mental phenomena that are continuously changing. The moment-by-moment manifestation of the mind-stream has been described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mind-stream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws.\n\nA salient feature of Buddhist philosophy which sets it apart from Indian orthodoxy is the centrality of the doctrine of not-self (Pāli. anatta, Skt. anātman). The Buddha's not-self doctrine sees humans as an impermanent composite of five psychological and physical aspects instead of a single fixed self. In this sense, what is called ego or the self is merely a convenient fiction, an illusion that does not apply to anything real but to an erroneous way of looking at the ever-changing stream of five interconnected aggregate factors. The relationship between these aggregates is said to be one of dependent-arising (pratītyasamutpāda). This means that all things, including mental events, arise co-dependently from a plurality of other causes and conditions. This seems to reject both causal determinist and epiphenomenalist conceptions of mind.\n\nThree centuries after the death of the Buddha (c. 150 BCE) saw the growth of a large body of literature called the Abhidharma in several contending Buddhist schools. In the Abhidharmic analysis of mind, the ordinary thought is defined as prapañca ('conceptual proliferation'). According to this theory, perceptual experience is bound up in multiple conceptualizations (expectations, judgments and desires). This proliferation of conceptualizations form our illusory superimposition of concepts like self and other upon an ever-changing stream of aggregate phenomena.\nIn this conception of mind no strict distinction is made between the conscious faculty and the actual sense perception of various phenomena. Consciousness is instead said to be divided into six sense modalities, five for the five senses and sixth for perception of mental phenomena. The arising of cognitive awareness is said to depend on sense perception, awareness of the mental faculty itself which is termed mental or 'introspective awareness' (\"manovijñāna\") and attention (\"āvartana\"), the picking out of objects out of the constantly changing stream of sensory impressions.\n\nRejection of a permanent agent eventually led to the philosophical problems of the seeming continuity of mind and also of explaining how rebirth and karma continue to be relevant doctrines without an eternal mind. This challenge was met by the Theravāda school by introducing the concept of mind as a factor of existence. This \"life-stream\" (Bhavanga-sota) is an undercurrent forming the condition of being. The continuity of a karmic \"person\" is therefore assured in the form of a mindstream (citta-santana), a series of flowing mental moments arising from the subliminal life-continuum mind (Bhavanga-citta), mental content, and attention.\n\nThe Sautrāntika school held a form of phenomenalism that saw the world as imperceptible. It held that external objects exist only as a support for cognition, which can only apprehend mental representations. This influenced the later Yogācāra school of Mahayana Buddhism. The Yogācāra school is often called the mind-only school because of its internalist stance that consciousness is the ultimate existing reality. The works of Vasubandhu have often been interpreted as arguing for some form of Idealism. Vasubandhu uses the dream argument and a mereological refutation of atomism to attack the reality of external objects as anything other than mental entities. Scholarly interpretations of Vasubandhu's philosophy vary widely, and include phenomenalism, neutral monism and realist phenomenology.\n\nThe Indian Mahayana schools were divided on the issue of the possibility of reflexive awareness (\"svasaṃvedana\"). Dharmakīrti accepted the idea of reflexive awareness as expounded by the Yogācāra school, comparing it to a lamp that illuminates itself while also illuminating other objects. This was strictly rejected by Mādhyamika scholars like Candrakīrti. Since in the philosophy of the Mādhyamika all things and mental events are characterized by emptiness, they argued that consciousness could not be an inherently reflexive ultimate reality since that would mean it was self-validating and therefore not characterized by emptiness. These views were ultimately reconciled by the 8th century thinker Śāntarakṣita. In Śāntarakṣita's synthesis he adopts the idealist Yogācāra views of reflexive awareness as a conventional truth into the structure of the two truths doctrine. Thus he states: \"By relying on the Mind-Only system, know that external entities do not exist. And by relying on this Middle Way system, know that no self exists at all, even in that [mind].\" \n\nThe Yogācāra school also developed the theory of the repository consciousness (\"ālayavijñāna\") to explain continuity of mind in rebirth and accumulation of karma. This repository consciousness acts as a storehouse for karmic seeds (bija) when all other senses are absent during the process of death and rebirth as well as being the causal potentiality of dharmic phenomena. Thus according to B. Alan Wallace: \nNo constituents of the body—in the brain or elsewhere—transform into mental states and processes. Such subjective experiences do not emerge from the body, but neither do they emerge from nothing. Rather, all objective mental appearances arise from the substrate, and all subjective mental states and processes arise from the substrate consciousness.\n\nTibetan Buddhist theories of mind evolved directly from the Indian Mahayana views. Thus the founder of the Gelug school, Je Tsongkhapa discusses the Yogācāra system of the Eight Consciousnesses in his \"Explanation of the Difficult Points\". He would later come to repudiate Śāntarakṣita's pragmatic idealism. \nAccording to the 14th Dalai Lama the mind can be defined \"as an entity that has the nature of mere experience, that is, 'clarity and knowing'. It is the knowing nature, or agency, that is called mind, and this is non-material.\" The simultaneously dual nature of mind is as follows:\nThe 14th Dalai Lama has also explicitly laid out his theory of mind as experiential dualism which is described above under the different types of dualism.\n\nBecause Tibetan philosophy of mind is ultimately soteriological, it focuses on meditative practices such as Dzogchen and Mahamudra that allow a practitioner to experience the true reflexive nature of their mind directly. This unobstructed knowledge of one's primordial, empty and non-dual Buddha nature is called rigpa. The mind's innermost nature is described among various schools as pure luminosity or \"clear light\" ('od gsal) and is often compared to a crystal ball or a mirror. Sogyal Rinpoche speaks of mind thus:\n\"Imagine a sky, empty, spacious, and pure from the beginning; its essence is like this. Imagine a sun, luminous, clear, unobstructed, and spontaneously present; its nature is like this.\"\n\nThe central issue in Chinese Zen philosophy of mind is in the difference between the pure and awakened mind and the defiled mind. Chinese Chan master Huangpo described the mind as without beginning and without form or limit while the defiled mind was that which was obscured by attachment to form and concepts. The pure Buddha-mind is thus able to see things \"as they truly are\", as absolute and non-dual \"thusness\" (Tathatā). This non-conceptual seeing also includes the paradoxical fact that there is no difference between a defiled and a pure mind, as well as no difference between samsara and nirvana.\n\nIn the Shobogenzo, the Japanese philosopher Dogen argued that body and mind are neither ontologically nor phenomenologically distinct but are characterized by a oneness called \"shin jin\" (bodymind). According to Dogen, \"casting off body and mind\" (\"Shinjin datsuraku\") in zazen will allow one to experience things-as-they-are (\"genjokoan\") which is the nature of original enlightenment (\"hongaku\").\n\nThere are countless subjects that are affected by the ideas developed in the philosophy of mind. Clear examples of this are the nature of death and its definitive character, the nature of emotion, of perception and of memory. Questions about what a person is and what his or her identity consists of also have much to do with the philosophy of mind. There are two subjects that, in connection with the philosophy of the mind, have aroused special attention: free will and the self.\n\nIn the context of philosophy of mind, the problem of free will takes on renewed intensity. This is certainly the case, at least, for materialistic determinists. According to this position, natural laws completely determine the course of the material world. Mental states, and therefore the will as well, would be material states, which means human behavior and decisions would be completely determined by natural laws. Some take this reasoning a step further: people cannot determine by themselves what they want and what they do. Consequently, they are not free.\n\nThis argumentation is rejected, on the one hand, by the compatibilists. Those who adopt this position suggest that the question \"Are we free?\" can only be answered once we have determined what the term \"free\" means. The opposite of \"free\" is not \"caused\" but \"compelled\" or \"coerced\". It is not appropriate to identify freedom with indetermination. A free act is one where the agent could have done otherwise if it had chosen otherwise. In this sense a person can be free even though determinism is true. The most important compatibilist in the history of the philosophy was David Hume. More recently, this position is defended, for example, by Daniel Dennett.\n\nOn the other hand, there are also many incompatibilists who reject the argument because they believe that the will is free in a stronger sense called libertarianism. These philosophers affirm the course of the world is either a) not completely determined by natural law where natural law is intercepted by physically independent agency, b) determined by indeterministic natural law only, or c) determined by indeterministic natural law in line with the subjective effort of physically non-reducible agency. Under Libertarianism, the will does not have to be deterministic and, therefore, it is potentially free. Critics of the second proposition (b) accuse the incompatibilists of using an incoherent concept of freedom. They argue as follows: if our will is not determined by anything, then we desire what we desire by pure chance. And if what we desire is purely accidental, we are not free. So if our will is not determined by anything, we are not free.\n\nThe philosophy of mind also has important consequences for the concept of \"self\". If by \"self\" or \"I\" one refers to an essential, immutable nucleus of the \"person\", some modern philosophers of mind, such as Daniel Dennett believe that no such thing exists. According to Dennett and other contemporaries, the self is considered an illusion. The idea of a self as an immutable essential nucleus derives from the idea of an immaterial soul. Such an idea is unacceptable to modern philosophers with physicalist orientations and their general skepticism of the concept of \"self\" as postulated by David Hume, who could never catch himself \"not\" doing, thinking or feeling anything. However, in the light of empirical results from developmental psychology, developmental biology and neuroscience, the idea of an essential inconstant, material nucleus—an integrated representational system distributed over changing patterns of synaptic connections—seems reasonable.\n\n"}
{"id": "447492", "url": "https://en.wikipedia.org/wiki?curid=447492", "title": "Policy debate", "text": "Policy debate\n\nPolicy debate is a form of debate competition in which teams of two advocate for and against a resolution that typically calls for policy change by the United States federal government. It is also referred to as cross-examination debate (sometimes shortened to Cross-X, CX, Cross-ex, or C-X) because of the 3-minute questioning period following each constructive speech. Affirmative teams generally present a \"plan\" as a proposal for implementation of the resolution.\n\nHigh school policy debate is sponsored by various organizations including the National Speech and Debate Association, National Association of Urban Debate Leagues, Catholic Forensic League, Stoa USA, and the National Christian Forensics and Communications Association, as well as many other regional speech organizations. Collegiate policy debates are generally competed under the guidelines of National Debate Tournament (NDT) and the Cross Examination Debate Association (CEDA), which have been joined at the collegiate level. A one-person policy format is sanctioned by the National Forensic Association (NFA) on the collegiate level as well.\n\nAcademic debate had its origins in intracollegiate debating societies, in which students would engage in (often public) debates against their classmates. Wake Forest University's debate program claims to have its origins in student literary societies founded on campus in the mid-1830s, which first presented joint \"orations\" in 1854. Many debating societies that were founded at least as early as the mid-nineteenth century are still active today, though they have generally shifted their focus to intercollegiate competitive debate. In addition to Wake Forest, the debate society at Northwestern University dates to 1855. Boston College's Fulton Debating Society, which was founded in 1868, continues to stage an annual public \"Fulton Prize Debate\" between teams of its own students after the intercollegiate debate season has ended. Other universities continue similar traditions.\n\nIntercollegiate debates have been held since at least as early as the 1890s. Historical records indicate that debates between teams from Wake Forest University and Trinity College (later Duke University) occurred beginning in 1897. Additionally, a debate between students from Boston College and Georgetown University occurred on May 1, 1895, in Boston. Whitman College debated Washington State University, Willamette University, and the University of Idaho in the late 1890s. Southwestern claims that the first debate held on its campus was between Southwestern and Fairmount College (which eventually became Wichita State University) but that debate could not have occurred prior to 1895, the year Fairmount College began classes.\n\nBy the mid-1970s, structured rules for lengths of speeches developed. Each side (affirmative and negative) was afforded two opening \"constructive\" speeches, and two closing \"rebuttal\" speeches, for a total of eight speeches per debate. Each speaker was cross-examined by an opponent for a period following his or her constructive speech. Traditionally rebuttals were half the length of constructives, but when a style of faster delivery speed became more standard in the late 1980s this time structure became problematic. Wake Forest University introduced reformed speech times in both its college (9‑6 instead of 10‑5) and high school (8‑5 instead of 8‑4) tournaments, which spread rapidly to become the new de facto standards.\n\nIn 2014, debaters Ameena Ruffin and Korey Johnson made history by being the first black women to win the Cross Examination Debate Association tournament, which is the largest college debate tournament.\n\nPolicy debaters' speed of delivery will vary from league to league and tournament to tournament. In many tournaments, debaters will speak very quickly in order to read as much evidence and make as many arguments as possible within the time-constrained speech. Speed reading or spreading is normal at the majority of national circuit policy debate tournaments.\n\nSome feel that the rapid-fire delivery makes debate harder to understand for the lay person. Rapid delivery is encouraged by those who believe that increased quantity and diversity of argumentation makes debates more educational. Others, citing scientific studies, claim that learning to speak faster also increases short- and long-term memory. A slower style is preferred by those who want debates to be understandable to lay people and those who claim that the pedagogical purpose of the activity is to train rhetorical skills. This is often a argument made by those who oppose in-round spreading - that the use of incomprehensible speeds makes debate less appealing to lay people. Many further claim that the increased speed encourages debaters to make several poor arguments, as opposed to a few high-quality ones. Most debaters will vary their rate of delivery depending upon the judge's preferences.\n\nDebaters utilize a specialized form of note taking, called \"flowing\", to keep track of the arguments presented during a debate. Conventionally, debater's flowing is divided into separate \"flows\" for each different argument in the debate round (kritiks, disads, topicalities, case, etc.). There are multiple methods of flowing but the most common style incorporates columns of arguments made in a given speech which allows the debater to match the next speaker's responses up with the original arguments. Certain shorthands for commonly used words are used to keep up with the rapid rate of delivery. The abbreviations or stand-in symbols vary between debaters.\n\nFlowing on a laptop has become more and more popular among high school and college debaters, despite the reservations of certain schools, tournaments, and judges. Some debaters use a basic computer spreadsheet; others use specialized flowing templates, which includes embedded shortcut keys for the most common formatting needs.\n\nAlthough there are many accepted standards in policy debate, there is no written formulation of rules. Sometimes debaters will in fact debate about how policy debate should work. These arguments are known as \"theory arguments\", and they are most often brought up when one team believes the actions of the other team are unfair and therefore warrant a loss. In more recent years, however, theory arguments have not been nearly as present in the Novice level as before.\n\nWhen the Affirmative team presents a plan, they take upon themselves the Burden of Proof to prove that their plan should be adopted. They must prove that their plan is an example of the resolution, and they must prove that the plan is a good idea. The Affirmative traditionally must uphold this burden using evidence from published sources, to avoid ridiculous cases.\n\nOne traditional way to judge policy debate states that the affirmative team must win certain issues, called the stock issues. They are generally interpreted to be as follows:\nHow many negative disadvantages will the plan have? Will its disadvantages outweigh its advantages?\nWill the plan solve the harms and can it even happen in the real world? How much of an impact will the plan have?\nWhat is the problem in the status quo to justify implementation of the plan? Is the plan important enough to even warrant consideration or make a difference?\nIs the affirmative's plan happening already, and if not, why?\nIs the plan an example of the resolution? Does the affirmative team's proposed policy comply with the wording of the resolution?\n\nThe \"stock issues\" are also commonly described as Significance, Harms, Inherency, Topicality, and Solvency. The \"stock issues\" are then taught collectively as \"The S.H.I.T.S.\" . Stock issues are taught extensively to novice debaters, but typically become less relevant as debaters move into more complex and less traditional arguments. Stock issues are strongly associated with traditional policy debate, and are typically stressed in advanced debates only if the judge is known to have traditionalist preferences. In some places (notably the West Coast), there is the additional stock issue of disadvantages; or a 'bad thing' that may occur as a result of passing the plan.\n\nMost affirmative teams today generally frame their case around advantages, which are good effects of their plan. The negative team will often present disadvantages which contend that the affirmative plan causes undesirable consequences. In an attempt to make sure that their advantages/disadvantages outweigh those of the other team, debaters often present extreme scenarios such as the extinction of the human race or a global nuclear war.\n\nNegation Theory contends that the negative need only negate the affirmative instead of having to negate the resolution. The acceptance of negation theory allows negative teams to run arguments such as topical counter plans that may affirm the resolution but still negate the affirmative's specific plan.\n\nAfter the affirmative presents its case, the negative can attack the case with many different arguments, which include:\n\nEvidence in debates is organized into units called \"cards\" (because such evidence was originally printed on note cards, though the practice has long been out of favor). Cards are designed to condense an author's argument so that debaters have an easy way to access the information. A card is composed of three parts: the tag, the cite, and the body. The \"tag\" is the debater's summary of the argument presented in the body. A tag is usually only one or two sentences. The \"cite\" contains all relevant citation information (that is, the author, date of publication, journal, title, etc.). Although every card should contain a complete citation, only the author's name and date of publication are typically spoken aloud in a speech. Some teams will also read the author's qualifications if they wish to emphasize this information. The \"body\" is a fragment of the author's original text. The length of a body can vary greatly—cards can be as short as a few sentences and as long as two or more pages. Most cards are between one and five paragraphs in length. The body of a card is often underlined or highlighted in order to eliminate unnecessary or redundant sentences when the card is read in a round. In a round, the tag is read first, followed by the cite and the body.\n\nAs pieces of evidence accumulate use, multiple colors of highlighting and different thicknesses of underlining often occur, sometimes making it difficult to determine which portion of the evidence was read. If debaters stop before finishing the underlined or highlighted portion of a card, it is considered good form to \"mark\" the card to show where one stopped reading. To otherwise misrepresent how much of a card was read—either by stopping early or by skipping underlined or highlighted sections—is known as \"cross-reading\" or \"clipping\" which is generally considered cheating. Although many judges overtly condemn the practice on their paradigms, it is hard to enforce, especially if judges permit debaters to be excessively unclear. Opponents will generally stand behind a debater whom they believe to be cross-reading or clipping, as if waiting to take a card (see below), and silently read along with them in an attempt to get their opponent to stop or the judge to notice.\n\nAs cards are read in round, it is common for an opponent to collect and examine even while a speech is still going on. This practice originated in part because cards are read at a rate faster than conversational speed but also because the un-underlined portions of cards are not read in round. Taking the cards during the speech allows the opponent to question the author's qualifications, the original context of the evidence, etc. in cross-examination. It is generally accepted whichever team is using preparation time has priority to read evidence read previously during a round by both teams. As a result, large amounts of evidence may change hands after the use of preparation time but before a speech. Most judges will not deduct from a team's preparation time for time spent finding evidence which the other team has misplaced.\n\nAfter a round, judges often \"call for cards\" to examine evidence whose merit was contested during the round or whose weight was emphasized during rebuttals so that they can read the evidence for themselves. Although widespread, this practice is explicitly banned at some tournaments, most notably National Catholic Forensic League nationals, and some judges refuse to call for cards because they believe the practice constitutes \"doing work for debaters that should have been done during round\". Judges may also call for evidence for the purpose of obtaining its citation information so that they can produce the evidence for their own school. Opponents and spectators are also generally allowed to collect citations in this manner, and some tournaments send scouts to rounds to facilitate the collection of cites for every team at the tournament, information which is sometimes published online.\n\nA judge refers to the individual responsible for determining the winner and loser of a policy debate round as well as assessing the relative merit of the participant speakers. Judges must resolve the complex issues presented in short time while, ideally, avoiding inserting their own personal beliefs that might cloud impartiality. \n\nSome circuits see \"lay\" or inexperienced judges recruited from the community as an important \"part of the game.\" Debaters in these circuits must be able to adapt from presentations to individuals with no debate experience at all, to judges who have themselves been debaters. This use of lay judges significantly impacts delivery and argumentation as the rapid-fire style and complex debate-theory arguments are frequently incomprehensible to lay judges. For this reason, other circuits restrict policy debate judging to qualified judges, generally ex-debaters. The use of lay judges, and its impact in speed, presentation and argumentation is a source of great controversy in the US high school debate community.\n\nThe judge is charged not only with selecting a winner, but also must allot points to each competitor. Known as \"speaker points\" or simply \"speaks\", its goal is to provide a numerical evaluation of the debaters' speaking skills. Speaker point schemes vary throughout local state and regional organizations particularly at the high school level. However, the method accepted by most national organizations such as the National Forensic League, Tournament of Champions, National Catholic Forensic League, Cross-Examination Debate Association, and National Debate Tournament, use values ranging from 1 to 30. In practice, within these organizations the standard variation is 26‑29, where 26's are given to extremely poor speakers, where a perfect score is considered incredibly rare and warranted only by an outstanding performance. Most tournaments accept halfpoint gradiations, for example 28.5s. Generally, speaker points are seen as secondary in importance to wins and losses, yet often correlate with a team's win/loss rate. In other words, the judge usually awards the winning team cumulatively higher speaker points than the losing team. If the judge does not, the decision is considered a \"low-point win\". Low-point wins simply mean that the team with better argumentation spoke worse, but are often rare, because judges will vote for teams that speak better, and award better speaker points to victorious teams.\n\nIn some smaller jurisdictions, the judge ranks the speakers 1‑4 instead of awarding them speaker points. Either speaker-point calculation may be used to break ties among teams with like records. Some areas also use speaker rankings in addition to speaker points in order to differentiate between speakers awarded the same number of points.\n\nAt a majority of tournaments, debaters also receive \"speaker awards\", which are awarded to the debaters who received the greatest number of speaker points. Many tournaments also drop the highest and lowest score received by each debater, in order to ensure that the speaker award calculations are fair and consistent, despite the preferences of different judges. The amount of speaker awards given out varies based on the number of debaters competing at any given tournament. For instance, a small local tournament might only award trophies or plaques to the top three debaters, whereas a widely attended \"national circuit\" tournament might give out awards to the top ten or fifteen speakers.\n\nExperienced debate judges (who were generally debaters in high school and/or college) generally carry a mindset that favors certain arguments and styles over others. Depending on what mindset, or paradigm, the judge uses, the debate can be drastically different. Because there is no one view of debate agreed upon by everyone, many debaters question a judge about their paradigm and/or their feelings on specific arguments before the round.\n\nNot every judge fits perfectly into one paradigm or another. A judge may say that they are \"tabula rasa\" or tab for short, or willing to listen to anything, but draw the line at arguments they consider to be offensive (such as arguments in favor of racism). Or, a judge might be a \"policymaker\", but still look at the debate in an offense/defense framework like a games-playing judge.\n\nExamples of paradigms include:\n\nMost high school debaters debate in local tournaments in their city, state or nearby states. Thousands of tournaments are held each year at high schools throughout the US.\n\nA small subset of high school debaters, mostly from elite public and private schools, travel around the country to tournaments in what is called the 'national circuit.' The championship of the national circuit is usually considered to be the Tournament of Champions, also called the T.O.C, at the University of Kentucky, which requires formal qualification in the form of two or more bids to the tournament. Bids are achieved by reaching a certain level of elimination rounds (for example, quarter-finals) at select, highly competitive, and carefully chosen tournaments across the country based upon the quality of debaters they attract and the diversity of locations from across the United States they represent.\n\nUrban debate leagues give students in urban school districts an opportunity to participate in policy debate. There are currently urban debate leagues in 24 of the largest cities in the United States. In total, more than 500 high schools participate in the league and more than 40,000 students have competed in urban debate.\n\nThere is some dispute over what constitutes the \"national championship\" in the United States per se, but two tournaments generally compete for the title:\nThe Tournament of Champions held at the University of Kentucky, and the National Speech and Debate tournament sponsored by the National Forensic League (now known as the National Speech & Debate Association). For the highest level of competition, the Tournament of Champions is generally considered to be the more prestigious title to hold.\n\nIn Texas, some debate occurs on the Texas Forensic Association (TFA) level. This organization includes mostly the progressive judging paradigms and favors many off topic arguments. TFA is geared towards the larger schools/programs who tend to be in the suburban areas in the major cities in the eastern part of the state. The other type of debate is UIL. UIL is open to all public schools throughout Texas.\n\nTFA State policy debate tends to favor a multitude of off-case arguments and teams that favor a progressive style; while UIL State tends to be more policy focused.\n\nThere is no single unified national championship in college debate; though the National Debate Tournament (NDT), the Cross Examination Debate Association (CEDA) and the American Debate Association (ADA) all host national tournaments. The NDT committee issues a ranking report of the top 16 teams in the country (\"first round bids\") for automatic advancement to the NDT in early February. The report roughly determines a regular season champion called the 'Copeland Award' for the team rated the highest over the course of the year through early February.\n\nWhile once attended by only highly competitive policy debaters, many high school students now attend debate institutes, which are typically held at colleges in the summer. Most institutes range from about two to seven weeks.\n\nMany institutes divide students into work groups, or \"labs\", based on skill level and experience. Many even offer specialized \"advanced\" or \"scholars\" workshops, to which acceptance is highly limited.\n\nA resolution or topic is a statement which the affirmative team affirms and the negative team negates. Resolutions are selected annually by affiliated schools. Most resolutions from the 1920s to 2005 have begun \"Resolved: that The United States federal government should\" although some variations from this structure have been apparent both before the NDT-CEDA merger and with the 2006–2007 college policy debate topic, which limited the affirmative agent to the United States Supreme Court.\n\nAt the college level, a number of topics are proposed and interested parties write \"topic papers\" discussing the pros and cons of that individual topic. Each school then gets one vote on the topic. The single topic area voted on then has a number of proposed topic wordings, one is chosen, and it is debated by affiliated students nationally for the entire season (standard academic school year).\n\nAt the high-school level, \"topic papers\" are also prepared but the voting procedure is different. These papers are then presented to a topic selection committee which rewords each topic and eventually narrows down the number of topics to five topics. Then the five resolutions are put to a two-tiered voting system. State forensic associations, the National Forensic League, and the National Catholic Forensic League all vote on the five topics, narrowing it down to two. Then the two topics are again put to a vote, and one topic is selected.\n\nResolved: The United States federal government should substantially reduce its military and/or police presence in one or more of the following: South Korea, Japan, Afghanistan, Kuwait, Iraq, Turkey.\n\nResolved: The United States federal government should substantially increase its exploration and/or development of space beyond the Mesosphere.\n\nResolved: The United States federal government should substantially increase its transportation infrastructure investment in the United States.\n\nResolved: The United States federal government should substantially increase its economic engagement toward Cuba, Mexico or Venezuela.\n\nResolved: The United States federal government should substantially increase its non-military exploration and/or development of the Earth's oceans.\n\nResolved: The United States federal government should substantially curtail its domestic surveillance.\nResolved: The United States federal government should substantially increase its economic and/or diplomatic engagement with the People's Republic of China.\nResolved: The United States federal government should substantially increase its funding and/or regulation of primary and/or secondary education in the United States.\nResolved: The United States federal government should substantially reduce its restrictions on legal immigration to the United States.\n\nThe times and speech order are generally as follows:\n\nIn addition to speeches, policy debates may allow for a certain amount of preparation time, or \"prep time,\" during a debate round. NFL rules call for 5 minutes of total prep time that can be used, although in practice high school debate tournaments usually give 8 minutes of prep time. College debates typically have 10 minutes of preparation time. The preparation time is used at each team's preference; they can use different amounts of preparation time before any of their speeches, or even none at all. Prep time can be allocated strategically to intimidate or inconvenience the other team: for instance, normally a 1AR requires substantial prep time, so a well-executed \"stand up 1AR,\" delivered after no prep time intimidates the negative team and takes away from time that the 2NR may have used to prepare the parts of his/her speech which do not rely on what the 1AR says.\n\n\n\n\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "24097", "url": "https://en.wikipedia.org/wiki?curid=24097", "title": "Principle of bivalence", "text": "Principle of bivalence\n\nIn logic, the semantic principle (or law) of bivalence states that every declarative sentence expressing a proposition (of a theory under inspection) has exactly one truth value, either true or false. A logic satisfying this principle is called a two-valued logic or bivalent logic.\n\nIn formal logic, the principle of bivalence becomes a property that a semantics may or may not possess. It is not the same as the law of excluded middle, however, and a semantics may satisfy that law without being bivalent.\n\nThe principle of bivalence is studied in philosophical logic to address the question of which natural-language statements have a well-defined truth value. Sentences which predict events in the future, and sentences which seem open to interpretation, are particularly difficult for philosophers who hold that the principle of bivalence applies to all declarative natural-language statements. Many-valued logics formalize ideas that a realistic characterization of the notion of consequence requires the admissibility of premises which, owing to vagueness, temporal or quantum indeterminacy, or reference-failure, cannot be considered classically bivalent. Reference failures can also be addressed by free logics.\n\nThe principle of bivalence is related to the law of excluded middle though the latter is a syntactic expression of the language of a logic of the form \"P ∨ ¬P\". The difference between the principle and the law is important because there are logics which validate the law but which do not validate the principle. For example, the three-valued Logic of Paradox (LP) validates the law of excluded middle, but not the law of non-contradiction, ¬(P ∧ ¬P), and its intended semantics is not bivalent. In classical two-valued logic both the law of excluded middle and the law of non-contradiction hold.\n\nMany modern logic programming systems replace the law of the excluded middle with the concept of negation as failure. The programmer may wish to add the law of the excluded middle by explicitly asserting it as true; however, it is not assumed \"a priori\".\n\nThe intended semantics of classical logic is bivalent, but this is not true of every semantics for classical logic. In Boolean-valued semantics (for classical propositional logic), the truth values are the elements of an arbitrary Boolean algebra, \"true\" corresponds to the maximal element of the algebra, and \"false\" corresponds to the minimal element. Intermediate elements of the algebra correspond to truth values other than \"true\" and \"false\". The principle of bivalence holds only when the Boolean algebra is taken to be the two-element algebra, which has no intermediate elements.\n\nAssigning Boolean semantics to classical predicate calculus requires that the model be a complete Boolean algebra because the universal quantifier maps to the infimum operation, and the existential quantifier maps to the supremum; this is called a Boolean-valued model. All finite Boolean algebras are complete.\n\nIn order to justify his claim that true and false are the only logical values, Suszko (1977) observes that every structural Tarskian many-valued propositional logic can be provided with a bivalent semantics.\n\nA famous example is the \"contingent sea battle\" case found in Aristotle's work, \"De Interpretatione\", chapter 9:\n\nThe principle of bivalence here asserts:\n\nAristotle to embrace bivalence for such future contingents; Chrysippus, the Stoic logician, did embrace bivalence for this and all other propositions. The controversy continues to be of central importance in both the philosophy of time and the philosophy of logic.\n\nOne of the early motivations for the study of many-valued logics has been precisely this issue. In the early 20th century, the Polish formal logician Jan Łukasiewicz proposed three truth-values: the true, the false and the \"as-yet-undetermined\". This approach was later developed by Arend Heyting and L. E. J. Brouwer; see Łukasiewicz logic.\n\nIssues such as this have also been addressed in various temporal logics, where one can assert that \"\"Eventually\", either there will be a sea battle tomorrow, or there won't be.\" (Which is true if \"tomorrow\" eventually occurs.)\n\nSuch puzzles as the Sorites paradox and the related continuum fallacy have raised doubt as to the applicability of classical logic and the principle of bivalence to concepts that may be vague in their application. Fuzzy logic and some other multi-valued logics have been proposed as alternatives that handle vague concepts better. Truth (and falsity) in fuzzy logic, for example, comes in varying degrees. Consider the following statement in the circumstance of sorting apples on a moving belt:\n\nUpon observation, the apple is an undetermined color between yellow and red, or it is motled both colors. Thus the color falls into neither category \" red \" nor \" yellow \", but these are the only categories available to us as we sort the apples. We might say it is \"50% red\". This could be rephrased: it is 50% true that the apple is red. Therefore, P is 50% true, and 50% false. Now consider:\n\nIn other words, P and not-P. This violates the law of noncontradiction and, by extension, bivalence. However, this is only a partial rejection of these laws because P is only partially true. If P were 100% true, not-P would be 100% false, and there is no contradiction because P and not-P no longer holds.\n\nHowever, the law of the excluded middle is retained, because P and not-P implies P or not-P, since \"or\" is inclusive. The only two cases where P and not-P is false (when P is 100% true or false) are the same cases considered by two-valued logic, and the same rules apply.\n\nExample of a 3-valued logic applied to vague (undetermined) cases: Kleene 1952 (§64, pp. 332–340) offers a 3-valued logic for the cases when algorithms involving partial recursive functions may not return values, but rather end up with circumstances \"u\" = undecided. He lets \"t\" = \"true\", \"f\" = \"false\", \"u\" = \"undecided\" and redesigns all the propositional connectives. He observes that:\n\nThe following are his \"strong tables\":\nFor example, if a determination cannot be made as to whether an apple is red or not-red, then the truth value of the assertion Q: \" This apple is red \" is \" u \". Likewise, the truth value of the assertion R \" This apple is not-red \" is \" u \". Thus the AND of these into the assertion Q AND R, i.e. \" This apple is red AND this apple is not-red \" will, per the tables, yield \" u \". And, the assertion Q OR R, i.e. \" This apple is red OR this apple is not-red \" will likewise yield \" u \".\n\n"}
{"id": "1166245", "url": "https://en.wikipedia.org/wiki?curid=1166245", "title": "Principle of distributivity", "text": "Principle of distributivity\n\nThe principle of distributivity states that the algebraic distributive law is valid for classical logic, where both logical conjunction and logical disjunction are distributive over each other so that for any propositions \"A\", \"B\" and \"C\" the equivalences\nand\nhold.\n\nThe principle of distributivity is valid in classical logic, but invalid in quantum logic.\n\nThe article \"Is Logic Empirical?\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is inconsistent with a reasonable interpretation of quantum phenomena.\n"}
{"id": "1788944", "url": "https://en.wikipedia.org/wiki?curid=1788944", "title": "Principle of plenitude", "text": "Principle of plenitude\n\nThe principle of plenitude asserts that the universe contains all possible forms of existence. The historian of ideas Arthur Lovejoy was the first to trace the history of this philosophically important principle explicitly. Lovejoy distinguishes two versions of the principle: a static version, in which the universe displays a constant fullness and diversity, and a temporalized version, in which fullness and diversity gradually increase over time.\n\nLovejoy traces the principle of plenitude to the writings of Plato, finding in the \"Timaeus\" an insistence on \"the necessarily complete translation of all the ideal possibilities into actuality\". By contrast, he takes Aristotle to reject the principle in his \"Metaphysics\", when he writes that \"it is not necessary that everything that is possible should exist in actuality\".\n\nSince Plato, the principle of plenitude has had the following adherents:\n\n\n\n"}
{"id": "23556881", "url": "https://en.wikipedia.org/wiki?curid=23556881", "title": "Psychological continuum model", "text": "Psychological continuum model\n\nThe psychological continuum model (PCM) is a framework to organise prior literature from various academic disciplines to explain sport and event consumer behaviour. \nThe framework suggests four stages – awareness, attraction, attachment and allegiance to describe how sport and event involvement progressively develops with corresponding behaviours (e.g., playing, watching, buying). The PCM uses a vertical framework to characterise various psychological connections that individuals form with objects to explain the role of attitude formation and change that directs behaviours across a variety of consumption activities. Explaining the \"how\" and \"why\" of sport and event consumer behaviour, it discusses how personal, psychological and environmental factors influence a wide range of sport consumption activities.\n\nThe figure shows the four stages of the PCM - awareness, attraction, attachment and allegiance. On each stage, there is a horizontal decision making process. Inputs (green arrows) influence the internal processing (blue boxes) that creates outputs (yellow arrows). The outcomes are shown in the four different stages of the PCM (grey boxes). The unique decision making process is based upon the level of involvement of the consumer towards a sport/team/event. The following sequence is shown in each stage:\n\nInputs --> Internal Processing <--> Output\n\nThe PCM framework states that, through the processing of internal and external inputs, individuals progress upward along the four psychological connection stages. The overall evaluation of an object at a specific stage is the product of the processing of personal, psychological and environmental factors.\n\nAwareness stands for the notion when an individual first learns that a certain sport, event or team exists. In this stage the individual has not formed a preference or favourite. The PCM suggests that awareness of sport, teams and events stems from formal and informal channels, for examples parents, friends, school and media. In most cases awareness begins during childhood, but can also derive from other socializing agents. The value placed on the specific sport and event from a societal perspective is important in the awareness stage. The examples of \"I know about football\" and \"I know about Liverpool FC\" illustrate the awareness stage box.\n\nIn the attraction stage, the individual has a favourite sport, event, team or leisure hobby. Attraction is based upon a number of extrinsic and intrinsic motives. In other words, the sport, event, or leisure hobby provides the opportunity to satisfy needs and receive benefits. The motives stem from a combination of personal, psychological and environmental factors. The Attraction processing creates outcomes of positive affect and intentions, as well as engaging in consumption behaviour related to the sport and event. The examples of \"I like football\" and \"I like Liverpool FC\" illustrate the attraction stage box.\n\nIn the attachment stage the benefits and the sport object are internalised taking on a collective emotional, functional, and symbolic meaning. The psychological connection towards a sport, event, team or leisure hobby strengthens. Internal processes become more important and the influence of socializing agents decreases. Examples for the attachment stage are \"I am a football player\" or \"I am a Liverpool Fan\".\n\nAs the attachment processing continues, the internal collective meaning becomes more durable in terms of persistence and resistance and has greater impact on activities and behaviour. This is noted by the examples of \"I live for football\" and \"I live for Liverpool FC\" within the allegiance stage.\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2985811", "url": "https://en.wikipedia.org/wiki?curid=2985811", "title": "Signed measure", "text": "Signed measure\n\nIn mathematics, signed measure is a generalization of the concept of measure by allowing it to have negative values. Some authors may call it a charge, by analogy with electric charge, which is a familiar distribution that takes on positive and negative values.\n\nThere are two slightly different concepts of a signed measure, depending on whether or not one allows it to take infinite values. In research papers and advanced books signed measures are usually only allowed to take finite values, while undergraduate textbooks often allow them to take infinite values. To avoid confusion, this article will call these two cases \"finite signed measures\" and \"extended signed measures\".\n\nGiven a measurable space (\"X\", Σ), that is, a set \"X\" with a sigma algebra Σ on it, an extended signed measure is a function\nsuch that formula_2 and formula_3 is sigma additive, that is, it satisfies the equality\nwhere the series on the right must converge absolutely, for any sequence \"A\", \"A\", ..., \"A\", ... of disjoint sets in Σ. One consequence is that any extended signed measure can take +∞ as value, or it can take −∞ as value, but both are not available. The expression ∞ − ∞ is undefined and must be avoided.\n\nA finite signed measure (aka. real measure) is defined in the same way, except that it is only allowed to take real values. That is, it cannot take +∞ or −∞.\n\nFinite signed measures form a vector space, while extended signed measures are not even closed under addition, which makes them rather hard to work with. On the other hand, measures are extended signed measures, but are not in general finite signed measures.\n\nConsider a nonnegative measure ν on the space (\"X\", Σ) and a measurable function \"f\":\"X\"→ R such that\n\nThen, a finite signed measure is given by\n\nfor all \"A\" in Σ.\n\nThis signed measure takes only finite values. To allow it to take +∞ as a value, one needs to replace the assumption about \"f\" being absolutely integrable with the more relaxed condition\n\nwhere \"f\"(\"x\") = max(−\"f\"(\"x\"), 0) is the negative part of \"f\".\n\nWhat follows are two results which will imply that an extended signed measure is the difference of two nonnegative measures, and a finite signed measure is the difference of two finite non-negative measures.\n\nThe Hahn decomposition theorem states that given a signed measure μ, there exist two measurable sets \"P\" and \"N\" such that:\n\nMoreover, this decomposition is unique up to adding to/subtracting μ-null sets from \"P\" and \"N\".\n\nConsider then two nonnegative measures μ and μ defined by\n\nand\n\nfor all measurable sets \"E\", that is, \"E\" in Σ.\n\nOne can check that both μ and μ are nonnegative measures, with one taking only finite values, and are called the \"positive part\" and \"negative part\" of μ, respectively. One has that μ = μ - μ. The measure |μ| = μ + μ is called the \"variation\" of μ, and its maximum possible value, ||μ|| = |μ|(\"X\"), is called the \"total variation\" of μ.\n\nThis consequence of the Hahn decomposition theorem is called the \"Jordan decomposition\". The measures μ, μ and |μ| are independent of the choice of \"P\" and \"N\" in the Hahn decomposition theorem.\n\nThe sum of two finite signed measures is a finite signed measure, as is the product of a finite signed measure by a real number: they are closed under linear combination. It follows that the set of finite signed measures on a measurable space (\"X\", Σ) is a real vector space; this is in contrast to positive measures, which are only closed under conical combination, and thus form a convex cone but not a vector space. Furthermore, the total variation defines a norm in respect to which the space of finite signed measures becomes a Banach space. This space has even more structure, in that it can be shown to be a Dedekind complete Banach lattice and in so doing the Radon–Nikodym theorem can be shown to be a special case of the Freudenthal spectral theorem.\n\nIf \"X\" is a compact separable space, then the space of finite signed Baire measures is the dual of the real Banach space of all continuous real-valued functions on \"X\", by the Riesz–Markov–Kakutani representation theorem.\n\n\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
{"id": "14934822", "url": "https://en.wikipedia.org/wiki?curid=14934822", "title": "Type–token distinction", "text": "Type–token distinction\n\nThe type–token distinction is used in disciplines such as logic, linguistics, metalogic, typography, and computer programming to clarify what words mean.\n\nThe sentence \"they drive the same car\" is ambiguous. Do they drive the same \"type\" of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type–token distinction separates types (abstract descriptive concepts) from tokens (objects that instantiate concepts).\n\nFor example: \"bicycle\" represents a type: the concept of a bicycle; whereas \"my bicycle\" represents a token of that type: an object that instantiates that type. In the sentence \"the bicycle is becoming more popular\" the word \"bicycle\" represents a type that is a concept; whereas in the sentence \"the bicycle is in the garage\" the word \"bicycle\" represents a token: a particular object.\n\nThe words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is \"thorny\", \"flowering\" and \"bushy\". You might say a rose bush \"instantiates\" these three types, or \"embodies\" these three concepts, or \"exhibits\" these three properties, or \"possesses\" these three qualities, features or attributes.\n\nProperty types (e.g \"height in metres\" or \"thorny\") are often understood ontologically as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.\n\nSome say types exist in descriptions of objects, but not as tangible physical objects. They say one can show someone a particular bicycle, but cannot show someone the type \"bicycle\", as in \"\"the bicycle\" is popular.\". However types do exist in the sense that they appear in mental and documented models.\n\nSome say tokens are objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can be intangible objects of types such as \"thought\", \"tennis match\", \"government\" and \"act of kindness\".\n\nThere is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an occurrence of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: \"A rose is a rose is a rose\". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: \"rose\", \"is\" and \"a\". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, 'a', 'is' and 'rose', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type 'a', two of 'is' and three of 'rose'.\n\nThe need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them. Reflection on the simple case of occurrences of numerals is often helpful.\n\nIn typography, the type–token distinction is used to determine the presence of a text printed by movable type:\n\nThe word 'letters' was used three times in the above paragraph, each time in a different meaning. The word 'letters' is one of many words having \"type–token ambiguity\". This section disambiguates 'letters' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher Charles Sanders Peirce in 1906 using terminology that he established.\n\nThe letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.\n\nPeirce's type–token distinction, also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or concatenation theory. There is only one word type spelled el-ee-tee-tee-ee-ar, namely, 'letter'; but every time that word type is written, a new word token has been created.\n\nSome logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.\n\nThe word type 'letter' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type 'letter' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type 'letter'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.\n\nPeirce's original words are the following.\n\"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty 'thes' on a page, and, of course, they count as twenty words. In another sense of the word 'word,' however, there is but one word 'the' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice ... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. ... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies.\" – Peirce 1906, Ogden-Richards, 1923, 280-1.\n\nThese distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.\n\n\n"}
