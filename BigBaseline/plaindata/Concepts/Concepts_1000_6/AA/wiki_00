{"id": "6978", "url": "https://en.wikipedia.org/wiki?curid=6978", "title": "Concept", "text": "Concept\n\nConcepts are mental representations, abstract objects or abilities that make up the fundamental building blocks of thoughts and beliefs. They play an important role in all aspects of cognition.\n\nIn contemporary philosophy, there are at least three prevailing ways to understand what a concept is:\n\n\nConcepts can be organized into a hierarchy, higher levels of which are termed \"superordinate\" and lower levels termed \"subordinate\". Additionally, there is the \"basic\" or \"middle\" level at which people will most readily categorize a concept. For example, a basic-level concept would be \"chair\", with its superordinate, \"furniture\", and its subordinate, \"easy chair\".\n\nA concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.\n\nConcepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word \"concept\" often just means any idea.\n\nWithin the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called \"mental representations\" (colloquially understood as \"ideas in the mind\"). Mental representations, in turn, are the building blocks of what are called \"propositional attitudes\" (colloquially understood as the stances or perspectives we take towards ideas, be it \"believing\", \"doubting\", \"wondering\", \"accepting\", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.\n\nA central question in the study of concepts is the question of what concepts \"are\". Philosophers construe this question as one about the ontology of concepts – what they are really like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.\n\nPlatonist views of the mind construe concepts as abstract objects,\n\nThere is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept \"dog\" is philosophically distinct from the things in the world grouped by this concept – or the reference class or extension. Concepts that can be equated to a single word are called \"lexical concepts\".\n\nStudy of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.\n\nIn the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word \"moon\" (a concept) is not the large, bright, shape-changing object up in the sky, but only \"represents\" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.\n\nKant declared that human minds possess pure or \"a priori\" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things \"in general\", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an \"a priori\" concept can relate to individual phenomena, in a manner analogous to an \"a posteriori\" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction \"a posteriori concepts\" (meaning concepts that arise out of experience). An empirical or an \"a posteriori\" concept is a general representation (\"Vorstellung\") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)\n\nA concept is a common feature or characteristic. Kant investigated the way that empirical \"a posteriori\" concepts are created.\nIn cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or \"recollections\", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.\n\nPlato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.\n\nGottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7).\n\nAccording to Carl Benjamin Boyer, in the introduction to his \"The History of the Calculus and its Conceptual Development\", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.\n\nIn a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.\n\nConcepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. (\"Sort\" is itself another word for concept, and \"sorting\" thus means to organise into concepts.)\n\nThe classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both \"necessary\" and \"sufficient\" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example \"bachelor\" is said to be defined by \"unmarried\" and \"man\". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the \"law of the excluded middle\", which means that there are no partial members of a class, you are either in or out.\n\nThe classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy – concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic \"Time Without Change\" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.\n\nGiven that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:\n\nPrototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as \"family resemblances\". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member – the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. According to Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain, some of these are; visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.\n\nTheory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory–Theory of concepts is responding to some of the issues of prototype theory and classic theory.\n\nAccording to the theory of ideasthesia (or \"sensing concepts\"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.\n\nThere is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.\n\nThe term \"concept\" is traced back to 1554–60 (Latin \"\" – \"something conceived\").\n\n"}
{"id": "42415226", "url": "https://en.wikipedia.org/wiki?curid=42415226", "title": "Conceptual combination", "text": "Conceptual combination\n\nConceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as \"complex concepts.\" Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.\n\nConceptual combination is an important concept in the fields of cognitive psychology and cognitive science.\n\nThe mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.\n\nCognitive models attempt to functionally outline the mental computation involved in conceptual combination. \n\nConstraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. \"Diagnosticity\" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. \"Plausibility\" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. \"Informativeness\" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).\n\nThe spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.\n\nSpreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.\n\nWhen one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as \"priming.\"\n\nSpreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.\n\nThe features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept \"white jacket,\" if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of \"good for winter camouflage,\" despite the fact that this property is not closely attached to the component concepts \"white\" nor \"jacket.\" This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.\n\nThis model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.\n\nThe neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.\n\nOf particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.\n\nFurther support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.\n\nAs language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.\n\nA concept that can be expressed using a single word is called a \"lexical concept.\" A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.\n\nTwo lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as \"burnt toast,\" \"eat roughly,\" and \"readily loved.\" Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as \"sound wave,\" \"video game,\" and \"sleeping pill.\"\n\nIn addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is \"dual-process theory.\" Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. \"Relational interpretation\" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase \"snake mouse\" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. \"Property interpretation\" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase \"snake mouse\" might be interpreted as a mouse with poisonous fangs or an elongated body.\n\nThe second principal theory is known as the \"Competition in Relations among Nominals\" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, \"chocolate cat\" is usually interpreted as \"a cat made of chocolate\" rather than \"a chocolate-eating cat\" simply because the \"made of\" modifier is heavily conditioned to be associated with \"chocolate.\"\n\nExplanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, \"John spread butter on a bagel.\" In this sentence, the lexical concepts \"spread,\" \"butter,\" and \"bagel\" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, \"John baked a computer.\" Because \"baked\" and \"computer\" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.\n\nHowever, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence \"John saw an elephant cloud.\" \"Elephant\" and \"cloud\" do not shared a close association, but it takes little effort to comprehend that the term \"elephant cloud\" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.\n\nAlthough many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.\n\nWhen lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed \"sense generation.\" This includes all processes that would normally occur excepting those dependent on a conversation partner. The \"generation hypothesis\" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.\n\nThe \"anaphor resolution hypothesis\" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed \"anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.\n\nThe \"dual-process hypothesis\" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.\n\nCreativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.\n\nThe psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.\n"}
{"id": "25707018", "url": "https://en.wikipedia.org/wiki?curid=25707018", "title": "Concision", "text": "Concision\n\nConcision (alternatively brevity, laconicism, terseness, or conciseness) is the cutting out of unnecessary words while conveying an idea. It aims to enhance communication by eliminating redundancy without omitting important information. Concision has been described as one of the elementary principles of writing. The related concept of succinctness is the opposite of verbosity.\n\nConcision means to be economical with words, expressing what's needed using the fewest words necessary. That may involve removing redundant or unnecessary phrases or replacing them with shorter ones. It is described in \"The Elements of Style\" by Strunk and White as follows:\n\nConcision has also been described as \"eliminat[ing] words that take up space without saying much.\" Simple examples include replacing \"\" with \"because\" or \"at this point in time\" with \"now\" or \"currently.\"\n\nAn example sentence, with explanation:\n\nThe following example is taken from:\n\nThe source suggests this replacement:\n\nIn the second quote, the same information is communicated in less than half the length. However, it could be more concisely rewritten and communicate the same information:\n\nConcise expression, particularly in writing, is considered one of the basic goals of teaching the English language. Techniques to achieve concise writing are taught for students at all levels, from the introduction to writing to the preparation of PhD dissertations, and legal writing for law students.\n\nIt has been argued that although \"in expository prose English places a high value on conciseness... [t]he value placed on conciseness... is not shared by all cultures\", with, for example, the Thai culture as one where redundancy is prized as an opportunity to use additional words to demonstrate the writer's command of the language. This may lead to a tendency for people from those cultures to use repetitive or redundant phrasing when learning English.\n\nThe related concept of succinctness is a characteristic of speech, writing, data structure, algorithmic games, and thought in general, exhibiting both clarity and brevity. It is the opposite of verbosity, in which there is an excess of words.\n\nBrevity in succinctness is not achieved by shortening original material by coding or compressing it, but rather by omitting redundant material from it.\n\n\n"}
{"id": "21899301", "url": "https://en.wikipedia.org/wiki?curid=21899301", "title": "Disquotational principle", "text": "Disquotational principle\n\nThe disquotational principle is a philosophical theorem which holds that a rational speaker will accept \"p\" if and only if he or she believes \"p\". The quotes indicate that the statement \"p\" is being treated as a sentence, and not as a proposition. This principle is presupposed by claims that hold that substitution fails in certain intensional contexts.\n\nConsider the following argument:\n\nTo derive (3), we have to assume that when Sally accepts that \"Cicero was a famous orator\", she believes that Cicero was a famous orator. Then we can exchange Cicero for Tully, and derive (3). Bertrand Russell thought that this demonstrated the failure of substitutivity of identicals in intensional contexts.\n\nIn \"A Puzzle about Belief,\" Saul Kripke argues that the application of the disquotational theorem can yield a paradox on its own, without appeal to the substitution principle, and that this may show that the problem lies with the former, and not the latter. There are various formulations of this argument.\n\nSuppose that, Pierre, a Frenchman, comes to believe that (1) \"Londres est jolie\" (London is pretty), without ever having visited the city. Later in life, Pierre ends up living in London. He finds no French speakers there (he does not speak English yet), and everyone refers to the city as \"London,\" not \"Londres\". He finds this city decidedly unattractive, for the neighborhood he decides to live in is decidedly unattractive. Over time, he learns English, and formulates the belief that (2) \"London is not pretty\". Pierre never realizes that London is the English word for \"Londres\". Now with the disquotational principle, we can deduce from (1) that Pierre believes the proposition that \"Londres est jolie\". With a weak principle of translation (e.g., \"a proposition in language A is the same as a semantically identical proposition in language B\" [note that a proposition is not the same as a sentence]), we can now deduce that Pierre believes that London is pretty. But we can also deduce from (2) and the disquotational principle that Pierre believes that London is not pretty. These deductions can be made \"even though Pierre has made no logical blunders in forming his beliefs\". Without the disquotational principle, this contradiction could not be derived, because we would not be able to assume that (1) and (2) meant anything in particular.\n\nThis paradox can also be derived without appeal to another language. Suppose that Pierre assents to the proposition that \"Paderewski had musical talent\", perhaps having heard that this man was a famous pianist. With the disquotational principle, we can deduce that Pierre believes the proposition that Paderewski had musical talent. Now suppose that Pierre overhears a friend discussing the political exploits of a certain statesman, Paderewski, without knowing that the two Paderewskis are the same man. Pierre's background tells him that statesmen are generally not very gifted in music, and this leads him to the belief that Paderewski had no musical talent. The disquotation principle allows us to deduce that Pierre believes the proposition that Paderewski had no musical talent. Using this principle, we have now deduced that Pierre believes that Paderewski had musical talent, and does not believe that Paderewski had musical talent, \"even though Pierre's beliefs were formed logically\".\n\n"}
{"id": "152902", "url": "https://en.wikipedia.org/wiki?curid=152902", "title": "Dormant Commerce Clause", "text": "Dormant Commerce Clause\n\nThe Dormant Commerce Clause, or Negative Commerce Clause, in American constitutional law, is a legal doctrine that courts in the United States have inferred from the Commerce Clause in Article I of the US Constitution. The Dormant Commerce Clause is used to prohibit state legislation that discriminates against interstate or international commerce.\n\nFor example, it is lawful for Michigan to require food labels that specifically identify certain animal parts, if they are present in the product, because the state law applies to food produced in Michigan as well as food imported from other states and foreign countries; the state law would violate the Commerce Clause if it applied only to imported food or if it was otherwise found to favor domestic over imported products. Likewise, California law requires milk sold to contain a certain percentage of milk solids that federal law does not require, which is allowed under the Dormant Commerce Clause doctrine because California's stricter requirements apply equally to California-produced milk and imported milk and so does not discriminate against or inappropriately burden interstate commerce.\n\nThe idea that regulation of interstate commerce may to some extent be an exclusive Federal power was discussed even before adoption of the Constitution, but the framers did not use the word \"dormant\". On September 15, 1787, the Framers of the Constitution debated in Philadelphia whether to guarantee states the ability to lay duties of tonnage without Congressional interference so that the states could finance the clearing of harbors and the building of lighthouses. James Madison believed that the mere existence of the Commerce Clause would bar states from imposing any duty of tonnage: \"He was more and more convinced that the regulation of Commerce was in its nature indivisible and ought to be wholly under one authority.\"\n\nRoger Sherman disagreed: \"The power of the United States to regulate trade being supreme can control interferences of the State regulations when such interferences happen; so that there is no danger to be apprehended from a concurrent jurisdiction.\" Sherman saw the commerce power as similar to the tax power, the latter being one of the concurrent powers shared by the federal and state governments. Ultimately, the Philadelphia Convention decided upon the present language about duties of tonnage in , which says: \"No state shall, without the consent of Congress, lay any duty of tonnage ...\"\n\nThe word \"dormant,\" in connection with the Commerce Clause, originated in dicta of Chief Justice John Marshall. For example, in the case of \"Gibbons v. Ogden\", , he wrote that the power to regulate interstate commerce \"can never be exercised by the people themselves, but must be placed in the hands of agents, or lie dormant.\" Concurring Justice William Johnson was even more emphatic that the Constitution is \"altogether in favor of the exclusive grants to Congress of power over commerce.\"\n\nLater, in the case of \"Willson v. Black-Bird Creek Marsh Co.\", , Marshall wrote: \"We do not think that the [state] act empowering the Black Bird Creek Marsh Company to place a dam across the creek, can, under all the circumstances of the case, be considered as repugnant to the power to regulate commerce in its dormant state, or as being in conflict with any law passed on the subject.\"\n\nIf Marshall was suggesting that the power over interstate commerce is an exclusive federal power, the Dormant Commerce Clause doctrine eventually developed very differently: it treats regulation that does not discriminate against or unduly burden interstate commerce as a concurrent power, rather than an exclusive federal power, and it treats regulation that does so as an exclusive federal power. Thus, the modern doctrine says that congressional power over interstate commerce is somewhat exclusive but \"not absolutely exclusive\". The approach began in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court: \"Either absolutely to affirm, or deny that the nature of this [commerce] power requires exclusive legislation by Congress, is to lose sight of the nature of the subjects of this power, and to assert concerning all of them, what is really applicable but to a part.\" The first clear holding of the Supreme Court striking down a state law under the Dormant Commerce Clause came in 1873.\n\nJustice Anthony Kennedy has written that: \"The central rationale for the rule against discrimination is to prohibit state or municipal laws whose object is local economic protectionism, laws that would excite those jealousies and retaliatory measures the Constitution was designed to prevent.\" In order to determine whether a law violates a so-called \"dormant\" aspect of the Commerce Clause, the court first asks whether it discriminates on its face against interstate commerce. In this context, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter.\n\nThus, in a dormant Commerce Clause case, a court is initially concerned with whether the law facially discriminates against out-of-state actors or has the effect of favoring in-state economic interests over out-of-state interests. Discriminatory laws motivated by \"simple economic protectionism\" are subject to a \"virtually per se rule of invalidity\", \"City of Philadelphia v. New Jersey\" 437 U.S. 617 (1978), \"Dean Milk Co. v. City of Madison, Wisconsin\", 340 U.S. 349 (1951), \"Hunt v. Washington State Apple Advertising Comm.\", 432 U.S. 333 (1977) which can only be overcome by a showing that the State has no other means to advance a legitimate local purpose, \"Maine v. Taylor\", 477 U.S. 131(1986). See also \"Brown-Forman Distillers v. New York State Liquor Authority\", .\n\nOn the other hand, when a law is \"directed to legitimate local concerns, with effects upon interstate commerce that are only incidental\" (United Haulers Association, Inc.), that is, where other legislative objectives are credibly advanced and there is no patent discrimination against interstate trade, the Court has adopted a much more flexible approach, the general contours of which were outlined in \"Pike v. Bruce Church, Inc.\", 397 U.S. 137, 142 (1970) and \"City of Philadelphia v. New Jersey\", 437 U.S. at 624. If the law is not outright or intentionally discriminatory or protectionist, but still has some impact on interstate commerce, the court will evaluate the law using a balancing test. The Court determines whether the interstate burden imposed by a law outweighs the local benefits. If such is the case, the law is usually deemed unconstitutional. See \"Pike v. Bruce Church, Inc.\", . In the Pike case, the Court explained that a state regulation having only \"incidental\" effects on interstate commerce \"will be upheld unless the burden imposed on such commerce is clearly excessive in relation to the putative local benefits\". 397 U.S. at 142, 90 S.Ct. at 847. When weighing burdens against benefits, a court should consider both \"the nature of the local interest involved, and ... whether it could be promoted as well with a lesser impact on interstate activities\". Id. Thus regulation designed to implement public health and safety, or serve other legitimate state interests, but impact interstate commerce as an incident to that purpose, are subject to a test akin to the rational basis test, a minimum level of scrutiny. See \"Bibb v. Navajo Freight Lines, Inc.\" In USA Recycling, Inc. v. Town of Babylon, 66 F.3d 1272, 1281 (C.A.2 (N.Y.), 1995), the court explained:\n\nIf the state activity constitutes \"regulation\" of interstate commerce, then the court must proceed to a second inquiry: whether the activity regulates evenhandedly with only \"incidental\" effects on interstate commerce, or discriminates against interstate commerce. As we use the term here, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter. The party challenging the validity of a state statute or municipal ordinance bears the burden of showing that it discriminates against, or places some burden on, interstate commerce. \"Hughes v. Oklahoma\", 441 U.S. 322, 336, 99 S.Ct. 1727, 1736, 60 L.Ed.2d 250 (1979). If discrimination is established, the burden shifts to the state or local government to show that the local benefits of the statute outweigh its discriminatory effects, and that the state or municipality lacked a nondiscriminatory alternative that could have adequately protected the relevant local interests. If the challenging party cannot show that the statute is discriminatory, then it must demonstrate that the statute places a burden on interstate commerce that \"is clearly excessive in relation to the putative local benefits.\" \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456, 471(1981) (quoting Pike, 397 U.S. at 142, 90 S.Ct. at 847).\n\nOver the years, the Supreme Court has consistently held that the language of the Commerce Clause contains a further, negative command prohibiting certain state taxation even when Congress has failed to legislate on the subject. Examples of such cases are \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992); \"Northwestern States Portland Cement Co. v. Minnesota\", 358 U.S. 450, 458 (1959) and \"H.P. Hood & Sons, Inc. v. Du Mond\", 336 U.S. 525 (1949).\n\nMore recently, in the 2015 case of \"Comptroller of Treasury of MD. v. Wynne\", the Court addressed Maryland's unusual practice of taxing personal income earned in Maryland, and taxing personal income of its citizens earned outside Maryland, \"without\" any tax credit for income tax paid to other states. The Court held this sort of double-taxation to be a violation of the dormant Commerce Clause. The Court faulted Justice Antonin Scalia's criticism of the dormant Commerce Clause doctrine by saying that he failed to \"explain why, under his interpretation of the Constitution, the Import-Export Clause \nwould not lead to the same result that we reach under the dormant Commerce Clause\".\n\nApplication of the dormant commerce clause to state taxation is another manifestation of the Court's holdings that the Commerce Clause prevents a State from retreating into economic isolation or jeopardizing the welfare of the Nation as a whole, as it would do if it were free to place burdens on the flow of commerce across its borders that commerce wholly within those borders would not bear. The Court's taxation decisions thus \"reflected a central concern of the Framers that was an immediate reason for calling the Constitutional Convention: the conviction that in order to succeed, the new Union would have to avoid the tendencies toward economic Balkanization that had plagued relations among the Colonies and later among the States under the Articles of Confederation.\" \"Wardair Canada, Inc. v. Florida Dept. of Revenue\", 477 U.S. 1 (1986); \"Hughes v. Oklahoma\", 441 U.S. 322 (1979); \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995).\n\nAs with the Court's application of the dormant commerce clause to discriminatory regulation, the pre-New Deal Court attempted to apply a formalistic approach to state taxation alleged to interfere with interstate commerce. The history is described in \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995):\n\nThe command has been stated more easily than its object has been attained, however, and the Court's understanding of the dormant Commerce Clause has taken some turns. In its early stages, the Court held the view that interstate commerce was wholly immune from state taxation \"in any form\", \"even though the same amount of tax should be laid on (intrastate) commerce\". This position gave way in time to a less uncompromising but formal approach, according to which, for example, the Court would invalidate a state tax levied on gross receipts from interstate commerce, or upon the \"freight carried\" in interstate commerce, but would allow a tax merely measured by gross receipts from interstate commerce as long as the tax was formally imposed upon franchises, or \"'in lieu of all taxes upon (the taxpayer's) property,'\" Dissenting from this formal approach in 1927, Justice Stone remarked that it was \"too mechanical, too uncertain in its application, and too remote from actualities, to be of value.\" \n\nAccompanying the revolution in approach in the Court's Congressional powers jurisprudence, the New Deal Court began to change its approach to state taxation as well. The Jefferson Lines decision continues:\n\nIn 1938, the old formalism began to give way with Justice Stone's opinion in \"Western Live Stock v. Bureau of Revenue\", 303 U.S. 250, which examined New Mexico's franchise tax, measured by gross receipts, as applied to receipts from out-of-state advertisers in a journal produced by taxpayers in New Mexico but circulated both inside and outside the State. Although the assessment could have been sustained solely on prior precedent, Justice Stone added a dash of the pragmatism that, with a brief interlude, has since become our aspiration in this quarter of the law. ... The Court explained that \"[i]t was not the purpose of the commerce clause to relieve those engaged in interstate commerce from their just share of state tax burden even though it increases the cost of doing the business.\" \n\nDuring the transition period, some taxes were upheld based on a careful review of the actual economic impact of the tax, and other taxes were reviewed based on the kind of tax involved, whether the tax had a nefarious impact on commerce or not. Under this formalistic approach, a tax might be struck down, and then re-passed with exactly the same economic incidence, but under another name, and then withstand review.\n\nThe absurdity of this approach was made manifest in the two Railway Express cases. In the first, a tax imposed by the state of Virginia on American business concerns operating within the state was struck down because it was a business privilege tax imposed on the privilege of doing business in interstate commerce. But then, in the second, Virginia revised the wording of its statute to impose a \"franchise tax\" on \"intangible property\" in the form of \"going concern\" value as measured by gross receipts.\n\nThe Court upheld the reworded statute as not violative of the prohibition on privilege taxes, even though the impact of the old tax and new were essentially identical. There was no real economic difference between the statutes in Railway Express I and Railway Express II. The Court long since had recognized that interstate commerce may be made to pay its way. Yet under the Spector rule, the economic realities in Railway Express I became irrelevant. The Spector rule (against privilege taxes) had come to operate only as a rule of draftsmanship, and served only to distract the courts and parties from their inquiry into whether the challenged tax produced results forbidden by the Commerce Clause.\n\nThe death knell of formalism occurred in \"Complete Auto Transit, Inc v. Brady\", 430 U.S. 274 (1977), which approved a Mississippi privilege tax upon a Michigan company engaged in the business of shipping automobiles to Mississippi dealers. The Court there explained:\n\nAppellant's attack is based solely on decisions of this Court holding that a tax on the \"privilege\" of engaging in an activity in the State may not be applied to an activity that is part of interstate commerce. See, e. g., \"Spector Motor Service v. O'Connor\", 340 U.S. 602 (1951); \"Freeman v. Hewit\", 329 U.S. 249 (1946). This rule looks only to the fact that the incidence of the tax is the \"privilege of doing business\"; it deems irrelevant any consideration of the practical effect of the tax. The rule reflects an underlying philosophy that interstate commerce should enjoy a sort of \"free trade\" immunity from state taxation. \n\nComplete Auto Transit is the last in a line of cases that gradually rejected a per se approach to state taxation challenges under the commerce clause. In overruling prior decisions which struck down privilege taxes per se, the Court noted the following, in what has become a central component of commerce clause state taxation jurisprudence:\n\nWe note again that no claim is made that the activity is not sufficiently connected to the State to justify a tax, or that the tax is not fairly related to benefits provided the taxpayer, or that the tax discriminates against interstate commerce, or that the tax is not fairly apportioned.\n\nThese four factors, nexus, relationship to benefits, discrimination, and apportionment, have come to be regarded as the four Complete Auto Transit factors applied repeatedly in subsequent cases. Complete Auto Transit must be recognized as the culmination of the Court's emerging commerce clause approach, not just in taxation, but in all of its aspects. Application of Complete Auto Transit to State taxation remains a highly technical and specialized venture, requiring the application of commerce clause principles to an understanding of specialized tax law.\n\nIn addition to satisfying the four-prong test in \"Complete Auto Transit\", the Supreme Court has held state taxes which burden international commerce cannot create a substantial risk of multiple taxations and must not prevent the federal government from \"speaking with one voice when regulating commercial relations with foreign governments\". \"Japan Lines, Ltd. v. County of Los Angeles\", 441 U.S. 434 (1979).\n\nIn \"Kraft Gen. Foods, Inc. v. Iowa Dept. of Revenue and Finance\", 505 U.S. 71 (1992), the Supreme Court considered a case in which Iowa taxed dividends from foreign subsidiaries, without allowing a credit for taxes paid to foreign governments, but not dividends from domestic subsidiaries operating outside Iowa. This differential treatment arose from Iowa's adoption of the definition of \"net income\" used by the Internal Revenue Service. For federal income tax purposes, dividends from domestic subsidiaries are allowed to be exempted from the parent corporations income to avoid double taxation. The Iowa Supreme Court rejected a Commerce Clause claim because Kraft failed to show \"that Iowa businesses receive a commercial advantage over foreign commerce due to Iowa's taxing scheme.\" Considering an Equal Protection Clause challenge, the Iowa Supreme Court held that the use of the federal government's definitions of income were convenient for the state and was \"rationally related to the goal of administrative efficiency\". The Supreme Court rejected the notion that administrative convenience was a sufficient defense for subjecting foreign commerce to a higher tax burden than interstate commerce. The Supreme Court held that \"a State's preference for domestic commerce over foreign commerce is inconsistent with the Commerce Clause even if the State's own economy is not a direct beneficiary of the discrimination.\"\n\nDiscrimination in the flow of interstate commerce has arisen in a variety of contexts. A line of important cases has dealt with local processing requirements. Under the local processing requirement, a municipality seeks to force the local processing of raw materials before they are shipped in interstate commerce.\n\nThe basic idea of the local processing ordinance was to provide favored access to local processors of locally produced raw materials. Examples of Supreme Court decisions in this vein are set out in its Carbone decision. They include \"Minnesota v. Barber\", 136 U.S. 313, (1890) (striking down a Minnesota statute that required any meat sold within the State, whether originating within or without the State, to be examined by an inspector within the State); \"Foster-Fountain Packing Co. v. Haydel\", 278 U.S. 1 (1928) (striking down a Louisiana statute that forbade shrimp to be exported unless the heads and hulls had first been removed within the State); \"Johnson v. Haydel\", 278 U.S. 16 (1928) (striking down analogous Louisiana statute for oysters); \"Toomer v. Witsell\", 334 U.S. 385 (1948) (striking down South Carolina statute that required shrimp fishermen to unload, pack, and stamp their catch before shipping it to another State); \"Pike v. Bruce Church, Inc.\", supra (striking down Arizona statute that required all Arizona-grown cantaloupes to be packaged within the State prior to export); \"South-Central Timber Development, Inc. v. Wunnicke\", 467 U.S. 82 (1984) (striking down an Alaska regulation that required all Alaska timber to be processed within the State prior to export). The Court has defined \"protectionist\" state legislation as \"regulatory measures designed to benefit in-state economic interests by burdening out-of-state competitors\". \"New Energy Co. of Indiana v. Limbach\", 486 U.S. 269, 273–74 (1988).\n\nIn the 1980s, spurred by RCRA's emphasis on comprehensive local planning, many states and municipalities sought to promote investment in more costly disposal technologies, such as waste-to-energy incinerators, state-of-the-art landfills, composting and recycling. Some states and localities sought to promote private investment in these costly technologies by guaranteeing a longterm supply of customers. See Phillip Weinberg, Congress, the Courts, and Solid Waste Transport: Good Fences Don't Always Make Good Neighbors, 25 Envtl. L. 57 (1995); Atlantic Coast Demolition & Recycling, Inc., 112 F.3d 652, 657 (3d Cir. 1997). For about a decade, the use of regulation to channel private commerce to designated private disposal sites was greatly restricted as the result of the Carbone decision discussed below.\n\nFlow control laws typically came in various designs. One common theme was the decision to fund local infrastructure by guaranteeing a minimum volume of business for privately constructed landfills, incinerators, composters or other costly disposal sites. In some locales, choice of the flow control device was driven by state bonding laws, or municipal finance concerns. If a county or other municipality issued general obligation bonds for construction of a costly incinerator, for example, state laws might require a special approval process. If approval could be obtained, the bonds themselves would be counted against governmental credit limitations, or might impact the governmental body's credit rating: in either instance the ability to bond for other purposes might be impaired. But by guaranteeing customers for a privately constructed and financed facility, a private entity could issue its own bonds, privately, on the strength of the public's waste assurance.\n\nThe private character of flow control regimens can thus be explained in part by the desire to utilize particular kinds of public financing devices. It can also be explained by significant encouragement at the national level, in national legislation as well as in federal executive policy to achieve environmental objectives utilizing private resources. Ironically, these public-private efforts often took the form of local processing requirements which ultimately ran afoul of the commerce clause.\n\nThe Town of Clarkstown had decided that it wanted to promote waste assurance through a local private transfer station. The transfer station would process waste and then forward the waste to the disposal site designated by the Town. The ordinance had the following features:\n\nWaste hauling in the Town of Clarkstown was accomplished by private haulers, subject to local regulation. The scheme had the following aspects: (A) The Town promoted the financing of a privately owned transfer station through a waste assurance agreement with the private company. Thus the designated facility was a private company. (B) The Town of Clarkstown forced private haulers to bring their solid waste for local processing at the designated transfer station, even if the ultimate destination of solid waste was an out-of-state disposal site. (C) The primary rationale for forcing in-state waste into the designated private transfer station was financial; it was seen as a device to raise revenue to finance the transfer station.\n\nThe Town of Clarkstown's ordinance was designed and written right in the teeth of the long line of Supreme Court cases which had historically struck down local processing requirements. In short, it was as if the authors of the ordinance had gone to a treatise on the commerce clause and intentionally chosen a device which had been traditionally prohibited. A long line of Supreme Court case law had struck down local processing requirements when applied to goods or services in interstate commerce. As the Court in Carbone wrote:\n\nWe consider a so-called flow control ordinance, which requires all solid waste to be processed at a designated transfer station before leaving the municipality. The avowed purpose of the ordinance is to retain the processing fees charged at the transfer station to amortize the cost of the facility. Because it attains this goal by depriving competitors, including out-of-state firms, of access to a local market, we hold that the flow control ordinance violates the Commerce Clause.\n\nThe Court plainly regarded the decision as a relatively unremarkable decision, not a bold stroke. As the Court wrote: \"The case decided today, while perhaps a small new chapter in that course of decisions, rests nevertheless upon well-settled principles of our Commerce Clause jurisprudence.\" And, the Court made it plain, that the problem with Clarkstown's ordinance was that it created a local processing requirement protective of a local private processing company:\n\nIn this light, the flow control ordinance is just one more instance of local processing requirements that we long have held invalid ... The essential vice in laws of this sort is that they bar the import of the processing service. Out-of-state meat inspectors, or shrimp hullers, or milk pasteurizers, are deprived of access to local demand for their services. Put another way, the offending local laws hoard a local resource—be it meat, shrimp, or milk—for the benefit of local businesses that treat it. 511 U.S. at 392–393.\n\nThe Court's 2007 decision in \"United Haulers Association v. Oneida-Herkimer Solid Waste Management Authority\" starkly illustrates the difference in result when the Court finds that local regulation is not discriminatory. The Court dealt with a flow control regimen quite similar to that considered in Carbone. The \"only salient difference is that the laws at issue here require haulers to bring waste to facilities owned and operated by a state-created public benefit corporation.\" The Court decided that the balancing test should apply, because the regulatory scheme favored the government owned facility, but treated all private facilities equally.\n\nCompelling reasons justify treating these laws differently from laws favoring particular private businesses over their competitors. \"Conceptually, of course, any notion of discrimination assumes a comparison of substantially similar entities.\" \"General Motors Corp. v. Tracy\", 519 U.S. 278 (1997). But States and municipalities are not private businesses—far from it. Unlike private enterprise, government is vested with the responsibility of protecting the health, safety, and welfare of its citizens. See \"Metropolitan Life Ins. Co. v. Massachusetts\", 471 U.S. 724 (1985) ... These important responsibilities set state and local government apart from a typical private business.\n\nThe Court's United Haulers decision demonstrates an understanding of the regulatory justifications for flow control starkly missing in the Carbone decision:\n\nBy the 1980s, the Counties confronted what they could credibly call a solid waste \" 'crisis.' \"... Many local landfills were operating without permits and in violation of state regulations. Sixteen were ordered to close and remediate the surrounding environment, costing the public tens of millions of dollars. These environmental problems culminated in a federal clean-up action against a landfill in Oneida County; the defendants in that case named over local businesses and several municipalities and school districts as third-party defendants The \"crisis\" extended beyond health and safety concerns. The Counties had an uneasy relationship with local waste management companies, enduring price fixing, pervasive overcharging, and the influence of organized crime. Dramatic price hikes were not uncommon: In 1986, for example, a county contractor doubled its waste disposal rate on six weeks' notice\n\nThe Court would not interfere with local government's efforts to solve an important public and safety problem.\n\nThe contrary approach of treating public and private entities the same under the dormant Commerce Clause would lead to unprecedented and unbounded interference by the courts with state and local government. The dormant Commerce Clause is not a roving license for federal courts to decide what activities are appropriate for state and local government to undertake, and what activities must be the province of private market competition. In this case, the citizens of Oneida and Herkimer Counties have chosen the government to provide waste management services, with a limited role for the private sector in arranging for transport of waste from the curb to the public facilities. The citizens could have left the entire matter for the private sector, in which case any regulation they undertook could not discriminate against interstate commerce. But it was also open to them to vest responsibility for the matter with their government, and to adopt flow control ordinances to support the government effort. It is not the office of the Commerce Clause to control the decision of the voters on whether government or the private sector should provide waste management services. \"The Commerce Clause significantly limits the ability of States and localities to regulate or otherwise burden the flow of interstate commerce, but it does not elevate free trade above all other values.\"\n\nThe history of commerce clause jurisprudence evidences a distinct difference in approach where the state is seeking to exercise its public health and safety powers, on the one hand, as opposed to attempting to regulate the flow of commerce. The exact dividing line between the two interests, the right of states to exercise regulatory control over their public health and safety, and the interest of the national government in unfettered interstate commerce is not always easy to discern. One Court has written as follows:\n\nNot surprisingly, the Court's effort to preserve a national market has, on numerous occasions, come into conflict with the states' traditional power to \"legislat[e] on all subjects relating to the health, life, and safety of their citizens.\" \"Huron Portland Cement Co. v. City of Detroit\", 362 U.S. 440, 443 (1960). On these occasions, the Supreme Court has \"struggled (to put it nicely) to develop a set of rules by which we may preserve a national market without needlessly intruding upon the States' police powers, each exercise of which no doubt has some effect on the commerce of the Nation.\" \"Camps Newfound/Owatonna v. Town of Harrison\", 520 U.S. 564, 596 (1997) (Scalia, J., dissenting) (citing \"Okla. Tax Comm'n v. Jefferson Lines\", 514 U.S. 175, 180–83 (1995)); see generally Boris I. Bittker, Regulation of Interstate and Foreign Commerce § 6.01[A], at 6–5 (\"[T]he boundaries of the [State's] off-limits area are, and always have been, enveloped in a haze.\"). Those rules are \"simply stated, if not simply applied.\" Camps Newfound/Owatonna, 520 U.S. at 596 (Scalia, J., dissenting).\n\nA frequently cited example of the deference afforded to the powers of state and local government may be found in \"Exxon Corp. v. Maryland\", 437 U.S. 117 (1978), where the State of Maryland barred producers of petroleum products from operating retail service stations in the state. It is difficult to imagine a regimen which might have greater impact on the way in which markets are organized. Yet, the Court found the legislation constitutionally permitted: \"The fact that the burden of a state regulation falls on some interstate companies does not, by itself establish a claim of discrimination against interstate commerce,\" the Court wrote. The \"Clause protects interstate market, not particular interstate firms, from prohibitive or burdensome regulations.\"\n\nSimilarly, in \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456 (1981) the Court upheld a state law that banned nonreturnable milk containers made of plastic but permitted other nonreturnable milk containers. The Court found that the existence of a burden on out-of-state plastic industry was not 'clearly excessive' in comparison to the state's interest in promoting conservation. And the court continued:\n\nIn Exxon, the Court stressed that the Commerce Clause protects the interstate market, not particular interstate firms, from prohibitive or burdensome regulations. A nondiscriminatory regulation serving substantial state purpose is not invalid simply because it causes some business to shift from a predominantly out-of-state industry to a predominantly in-state industry. Only if the burden on interstate commerce clearly outweighs the State's legitimate purpose does such a regulation violate the commerce clause. When a state statute regarding safety matters applies equally to interstate and intrastate commerce, the courts are generally reluctant to invalidate it even if it may have some impact on interstate commerce. In \"Bibb v. Navajo Freight Lines\" 359 U.S. 520, 524 (1959), the United States Supreme Court stated: 'These safety measures carry a strong presumption of validity when challenged in court. If there are alternative ways of solving a problem, we do not sit to determine which of them is best suited to achieve a valid state objective. Policy decisions are for the state legislature, absent federal entry into the field. Unless we can conclude on the whole record that \"the total effect of the law as a safety measure in reducing accidents and casualties is so slight or problematical as not to outweigh the national interest in keeping interstate commerce free from interferences which seriously impede it\" we must uphold the statute.\n\nThere are two notable exceptions to the dormant Commerce Clause doctrine that can permit state laws or actions that otherwise violate the Dormant Commerce Clause to survive court challenges.\n\nThe first exception occurs when Congress has legislated on the matter. See \"Western & Southern Life Ins. v. State Board of California\", . In this case the Dormant Commerce Clause is no longer \"dormant\" and the issue is a Commerce Clause issue, requiring a determination of whether Congress has approved, preempted, or left untouched the state law at issue.\n\nThe second exception is \"market participation exception\". This occurs when the state is acting \"in the market\", like a business or customer, rather than as a \"market regulator\". For example, when a state is contracting for the construction of a building or selling maps to state parks, rather than passing laws governing construction or dictating the price of state park maps, it is acting \"in the market\". Like any other business in such cases, a state may favor or shun certain customers or suppliers.\n\nThe Supreme Court introduced the market participant doctrine in \"Hughes v. Alexandria Scrap Corp.\", 426 U.S. 794 (1976), which upheld a Maryland program that offered bounties to scrap processors to destroy abandoned automobile hulks. See also \"Wisconsin Dep't of Indus., Labor & Human Relations v. Gould Inc.\", 475 U.S. 282, 289 (1986); \"Reeves, Inc. v. Stake\", 447 U.S. 429, 437 (1980). Because Maryland required out-of-state processors, but not in-state processors, to submit burdensome documentation to claim their bounties, the state effectively favored in-state processors over out-of-state processors. The Court held that because the state was merely attaching conditions to its expenditure of state funds, the Maryland program affected the market no differently than if Maryland were a private company bidding up the price of auto hulks. Because the state was not \"regulating\" the market, its economic activity was not subject to the anti-discrimination principles underlying the dormant Commerce Clause—and the state could impose different paperwork burdens on out-of-state processors. \"Nothing in the purposes animating the Commerce Clause prohibits a State, in the absence of congressional action, from participating in the market and exercising the right to favor its own citizens over others.\"\n\nAnother important case is \"White v. Massachusetts Council of Constr. Employers, Inc.\", in which the Supreme Court held that the City of Boston could require its building contractors to hire at least fifty percent of their workforce from among Boston residents. 460 U.S. at 214–15. Because all of the employees covered by that mandate were \"in a substantial if informal sense, 'working for the city,' \" Boston was considered to be simply favoring its own residents through the expenditures of municipal funds. The Supreme Court stated, \"when a state or local government enters the market as a participant it is not subject to the restraints of the Commerce Clause.\" Id. at 208. Nothing in the Constitution precludes a local government from hiring a local company precisely because it is local.\n\nOther important cases enunciating the market participation exception principle are \"Reeves, Inc. v. Stake\", and \"South-Central Timber Development, Inc. v. Wunnicke\", . The \"Reeves\" case outlines the market participation exception test. In this case state-run cement co-ops were allowed to make restrictive rules (e.g. rules not to sell out-of-state). Here, this government-sponsored business was acting restrictively like an individually owned business and this action was held to be constitutional. \"South-Central Timber\" is important because it limits the market exception. \"South-Central Timber\" holds that the market-participant doctrine is limited in allowing a State to impose burdens on commerce within the market in which it is a participant, but allows it to go no further. The State may not impose conditions that have a substantial regulatory effect outside of that particular market.\n\nThe \"market participation exception\" to the dormant Commerce Clause does not give states unlimited authority to favor local interests, because limits from other laws and Constitutional limits still apply. In \"United Building & Construction Trades Council v. Camden\", , the city of Camden, New Jersey had passed an ordinance requiring that at least forty percent of the employees of contractors and subcontractors on city projects be Camden residents. The Supreme Court found that while the law was not infirm because of the Dormant Commerce Clause, it violated the Privileges and Immunities Clause of Article IV of the Constitution. Justice Rehnquist's opinion distinguishes the market-participant doctrine from the privileges and immunities doctrine. Similarly, Congress has the power itself under the Commerce Clause to regulate and sanction states acting as \"market participants\", but it lacks power to legislate in ways that violate Article IV.\n\nIn the 21st century, the dormant Commerce Clause has been a frequent legal issue in cases arising under state laws regulating some aspects of Internet activity. Because of the interstate, and often international, nature of Internet communications, state laws addressing internet-related subjects such as spam, online sales or online pornography can often trigger Dormant Commerce Clause issues.\n\nA \"negative\" or \"dormant\" component to the Commerce Clause has been the subject of scholarly discussion for many decades. Both Supreme Court Justices Antonin Scalia and Clarence Thomas have rejected the notion of a Dormant Commerce Clause. They believe that such a doctrine is inconsistent with an originalist interpretation of the Constitution—so much so that they believe the doctrine is a \"judicial fraud\".\n\nA number of earlier Supreme Court justices also expressed dissatisfaction with the dormant Commerce Clause doctrine. For example, Chief Justice Taney said this in 1847:\n\nIf it was intended to forbid the States from making any regulations of commerce, it is difficult to account for the omission to prohibit it, when that prohibition has been so carefully and distinctly inserted in relation to other powers ... [T]he legislation of Congress and the States has conformed to this construction from the foundation of the government ... The decisions of this court will also, in my opinion, when carefully examined, be found to sanction the construction I am maintaining.\n\nHowever, that statement by Taney in 1847 was before the doctrine morphed in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court that the Commerce Clause does not always require \"exclusive legislation by Congress\".\n\nIn \"Trailer Marine Transport Corp. v. Rivera Vázquez\", 977 F.2d 1, 7-8 (1st Cir. 1992), the First Circuit held that the dormant Commerce Clause applies to Puerto Rico.\n\n\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "18562346", "url": "https://en.wikipedia.org/wiki?curid=18562346", "title": "Gossen's laws", "text": "Gossen's laws\n\nGossen's laws, named for Hermann Heinrich Gossen (1810 – 1858), are three laws of economics:\n\nThe citation referenced is the translation by Nicholas Georgescu-Roegen in which the traslator names only two laws: 1) ”If an enjoyment is experienced uninterruptedly, the corresponding intensity of pleasure decreases continuously until satiety is ultimately reached, at which point the intensity becomes nil.\" and, 2) \"A similar decrease of the intensity of pleasure takes place if a previous enjoyment of the same kind of pleasure is repeated. Not only does the initial intensity of pleasure become smaller but also the duration of the enjoyment becomes shorter, so that satiety is reached sooner. Moreover, the sooner the repetition, the smaller becomes the initial intensity as well as the duration of the enjoyment.\" (p.lxxx)\n\n\n"}
{"id": "18562589", "url": "https://en.wikipedia.org/wiki?curid=18562589", "title": "Gossen's second law", "text": "Gossen's second law\n\nGossen's Second “Law”, named for Hermann Heinrich Gossen (1810–1858), is the assertion that an economic agent will allocate his or her expenditures such that the ratio of the marginal utility of each good or service to its price (the marginal expenditure necessary for its acquisition) is equal to that for every other good or service. Formally,\nwhere\n\nImagine that an agent has spent money on various sorts of goods or services. If the last unit of currency spent on goods or services of one sort bought a quantity with \"less\" marginal utility than that which would have been associated with the quantity of another sort that could have been bought with the money, then the agent would have been \"better off\" instead buying more of that other good or service. Assuming that goods and services are continuously divisible, the only way that it is possible that the marginal expenditure on one good or service should not yield more utility than the marginal expenditure on the other (or \"vice versa\") is if the marginal expenditures yield \"equal\" utility.\n\nAssume that utility, goods, and services have the requisite properties so that formula_7 is well defined for each good or service. An agent then optimizes\nsubject to a budget constraint\nwhere\nUsing the method of Lagrange multipliers, one constructs the function\nand finds the first-order conditions for optimization as\n(which simply implies that all of formula_10 will be spent) and\nso that\nwhich is algebraically equivalent to\nSince every such ratio is equal to formula_17, the ratios are all equal one to another:\n\n\n"}
{"id": "7949372", "url": "https://en.wikipedia.org/wiki?curid=7949372", "title": "Humanitarian principles", "text": "Humanitarian principles\n\nThere are a number of meanings for the term humanitarian. Here humanitarian pertains to the practice of saving lives and alleviating suffering. It is usually related to emergency response (also called humanitarian response) whether in the case of a natural disaster or a man-made disaster such as war or other armed conflict. Humanitarian principles govern the way humanitarian response is carried out.\n\nThe principle of humanity means that all humankind shall be treated humanely and equally in all circumstances by saving lives and alleviating suffering, while ensuring respect for the individual. It is the fundamental principle of humanitarian response.\n\nThe Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief (RC/NGO Code) introduces the concept of the humanitarian imperative which expands the principle of humanity to include the right to receive and to give humanitarian assistance. It states the obligation of the international community \"to provide humanitarian assistance wherever it is needed.\"\n\nProvision of humanitarian assistance must be impartial and not based on nationality, race, religion, or political point of view. It must be based on need alone.\n\nFor most non-governmental humanitarian agencies (NGHAs), the principle of impartiality is unambiguous even if it is sometimes difficult to apply, especially in rapidly changing situations. However, it is no longer clear which organizations can claim to be humanitarian. For example, companies like PADCO, a USAID subcontractor, is sometimes seen as a humanitarian NGO. However, for the UN agencies, particularly where the UN is involved in peace keeping activities as the result of a Security Council resolution, it is not clear if the UN is in position to act in an impartial manner if one of the parties is in violation of terms of the UN Charter.\n\nHumanitarian agencies must formulate and implement their own policies independently of government policies or actions.\n\nProblems may arise because most NGHAs rely in varying degrees on government donors. Thus for some organizations it is difficult to maintain independence from their donors and not be confused in the field with governments who may be involved in the hostilities. The ICRC, has set the example for maintaining its independence (and neutrality) by raising its funds from governments through the use of separate annual appeals for headquarters costs and field operations.\n\nThe core principles are defining characteristics, the necessary conditions for humanitarian response. Organizations such as military forces and for-profit companies may deliver assistance to communities affected by disaster in order to save lives and alleviate suffering, but they are not considered by the humanitarian sector as humanitarian agencies as their response is not based on the core principles.\n\nIn addition to the core principles, there are other principles that govern humanitarian response for specific types of humanitarian agencies such as UN agencies, the Red Cross and Red Crescent Movement, and NGOs.\n\nThe International Red Cross and Red Crescent Movement follows, in addition to the above core principles, the principle of neutrality. For the Red Cross, neutrality means not to take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.\n\nThe principle of neutrality was specifically addressed to the Red Cross Movement to prevent it from not only taking sides in a conflict, but not to \"engage at any time in controversies of a political, racial, religious or ideological nature.\" The principle of neutrality was left out of the Red Cross/NGO code because some of the NGHAs, while committed to giving impartial assistance, were not ready to forgo their lobbying on justice issues related to political and ideological questions.\n\nUnited Nations General Assembly Resolution 46/182 lists the principle of neutrality, alongside the principles of humanity and impartiality in its annex as a guide to the provision of humanitarian assistance. The resolution is designed to strengthen human response of the UN system, and it clearly applies to the UN agencies.\n\nNeutrality can also apply to humanitarian actions of a state. \"Neutrality remains closely linked with the definition which introduced the concept into international law to designate the status of a State which decided to stand apart from an armed conflict. Consequently, its applications under positive law still depend on the criteria of abstention and impartiality which have characterized neutrality from the outset.\"\n\nThe application of the word neutrality to humanitarian aid delivered by UN agencies or even governments can be confusing. GA Resolution 46/182 proclaims the principle of neutrality, yet as an inter-governmental political organization, the UN is often engaged in controversies of a political nature. According to this interpretation, the UN agency or a government can provide neutral humanitarian aid as long as it does it impartially, based upon need alone.\n\nToday, the word neutrality is widely used within the humanitarian community, usually to mean the provision of humanitarian aid in an impartial and independent manner, based on need alone. Few international NGOs have curtailed work on justice or human rights issues because of their commitment to neutrality.\n\nThe provision of aid must not exploit the vulnerability of victims and be used to further political or religious creeds. All of the major non-governmental humanitarian agencies (NGHAs) by signing up to the RC/NGO Code of Conduct have committed themselves not to use humanitarian response to further political or religious creeds.\n\nAll of the above principles are important requirements for effective field operations. They are based on widespread field experience of agencies engaged in humanitarian response. In conflict situations, their breach may drastically affect the ability of agencies to respond to the needs of the victims.\n\nIf a warring party believes, for example, that an agency is favoring the other side, or that it is an agent of the enemy, access to the victims may be blocked and the lives of humanitarian workers may be put in danger. If one of the parties perceives that an agency is trying to spread another religious faith, there may be a hostile reaction to their activities.\n\nThe core principles, found in the Red Cross/NGO Code of Conduct and in GA Resolution 46/182 are derived from the Fundamental Principles of the Red Cross, particularly principles I (humanity), II (impartiality), III (neutrality—in the case of the UN), and IV (independence).\n\nAccountability has been defined as: \"the processes through which an organisation makes a \ncommitment to respond to and balance the needs of stakeholders in its decision making processes and activities, and delivers against this commitment.\" Humanitarian Accountability Partnership International adds: \"Accountability is about using power responsibly.\"\n\nArticle 9 of the Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief states:\n\"We hold ourselves accountable to both those we seek to assist and those from whom we accept resources;\" and thus identifies the two major stake holders: donors and beneficiaries. However, traditionally humanitarian agencies have tended to practice mainly \"upward accountability\", i.e. to their donors.\n\nThe experience of many humanitarian agencies during the Rwandan Genocide, led to a number of initiatives designed to improve humanitarian assistance and accountability, particularly with respect to the beneficiaries. Examples include the Sphere Project, ALNAP, Compas, the People In Aid Code of Good Practice, and the Humanitarian Accountability Partnership International, which runs a \"global quality insurance scheme for humanitarian agencies.\"\n\nThe RC/NGO Code also lists a number of more aspirational principles which are derived from experience with development assistance.\n\nThe Sphere Project Humanitarian Charter uses the language of human rights to remind that the right to life which is proclaimed in both the Universal Declaration of Human Rights and the International Convention on Civil and Political Rights is related to human dignity.\n\nHumanitarian principles are mainly focused on the behavior of organizations. However a humane response implies that humanitarian workers are not to take advantage of the vulnerabilities of those affected by war and violence. Agencies have the responsibility for developing rules of staff conduct which prevent abuse of the beneficiaries.\n\nOne of the most problematic areas is related to the issue of sexual exploitation and abuse of beneficiaries by humanitarian workers. In an emergency where victims have lost everything, women and girls are particularly vulnerable to sexual abuse.\n\nA number of reports which identified the sexual exploitation of refugees in west Africa prodded the humanitarian community to work together in examining the problem and to take measures to prevent abuses. In July 2002, the UN's Interagency Standing Committee (IASC) adopted a plan of action which stated: Sexual exploitation and abuse by humanitarian workers constitute acts of gross misconduct and are therefore grounds for termination of employment. The plan explicitly prohibited the \"Exchange of money, employment, goods, or services for sex, including sexual favours or other forms of humiliating, degrading or exploitative behaviour.\" The major NGHAs as well the UN agencies engaged in humanitarian response committed themselves to setting up internal structures to prevent sexual exploitation and abuse of beneficiaries.\n\nSubstantial efforts have been made in the humanitarian sector to monitor compliance with humanitarian principles. Such efforts include The People In Aid Code of Good Practice, an internationally recognised management tool that helps humanitarian and development organisations enhance the quality of their human resources management. The NGO, Humanitarian Accountability Partnership International, is also working to make humanitarian organizations more accountable, especially to the beneficiaries.\n\nStructures internal to the Red Cross Movement monitor compliance to the Fundamental Principles of the Red Cross.\n\nThe RC/NGO Code is self-enforcing. The SCHR carries out peer reviews among its members which look in part at the issue of compliance with principles set out in the RC/NGO Code\n\n"}
{"id": "30679665", "url": "https://en.wikipedia.org/wiki?curid=30679665", "title": "Inherent bad faith model", "text": "Inherent bad faith model\n\nThe inherent bad faith model of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. \n\nIt is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. An example is John Foster Dulles' position regarding the Soviet Union.\n\n"}
{"id": "6365190", "url": "https://en.wikipedia.org/wiki?curid=6365190", "title": "Li (Confucianism)", "text": "Li (Confucianism)\n\nLi () is a classical Chinese word which is commonly used in Chinese philosophy, particularly within Confucianism. \"Li\" does not encompass a definitive object but rather a somewhat abstract idea and, as such, is translated in a number of different ways. Wing-tsit Chan explains that \"li\" originally meant \"a religious sacrifice,\" but has come to mean ceremony, ritual, decorum, rules of propriety, good form, good custom, etc., and has even been equated with Natural law.\"\n\nIn Chinese cosmology, human agency participates in the ordering of the universe by Li ('rites'). There are several Chinese definitions of a rite, one of the most common definitions is that it transforms the invisible to visible; through the performance of rites at appropriate occasions, humans make visible the underlying order. Performing the correct ritual focuses, links, orders, and moves the social, which is the human realm, in correspondence with the terrestrial and celestial realms to keep all three in harmony. This procedure has been described as centering, which used to be the duty of the Son of Tian, the emperor. But it was also done by all those who conducted state, ancestral, and life-cycle rites and, in another way, by Daoists who conducted the rites of local gods as a centering of the forces of exemplary history, of liturgical service, of the correct conduct of human relations, and of the arts of divination such as the earliest of all Chinese classics—the \"Book of Changes\" (\"Yi Jing\")—joining textual learning to bodily practices for health and the harmonized enhancement of circuits of energy (qi). \n\nThe rites of \"li\" are not rites in the Western conception of religious custom. Rather, \"li\" embodies the entire spectrum of interaction with humans, nature, and even material objects. Confucius includes in his discussions of \"li\" such diverse topics as learning, tea drinking, titles, mourning, and governance. Xunzi cites \"songs and laughter, weeping and lamentation...rice and millet, fish and meat...the wearing of ceremonial caps, embroidered robes, and patterned silks, or of fasting clothes and mourning clothes...unspacious rooms and very nonsecluded halls, hard mats, seats and flooring\" as vital parts of the fabric of \"li\".\n\nAmong the earliest historical discussions on \"li\" stands the 25th year of Zhao Gong () in the Zuo Zhuan.\n\n\"Li\" consists of the norms of proper social behavior as taught to others by fathers, village elders and government officials. The teachings of li promoted ideals such as filial piety, brotherliness, righteousness, good faith and loyalty. The influence of \"li\" guided public expectations, such as the loyalty to superiors and respect for elders in the community.\n\nContinuous with the emphasis on community, following \"li\" included the internalization of action, which both yields the comforting feeling of tradition and allows one to become \"more open to the panoply of sensations of the experience\" (Rosemont 2005). But it should also maintain a healthy practice of selflessness, both in the actions themselves and in the proper example which is set for one's brothers. Approaches in the community, as well as personal approaches together demonstrate how \"li\" pervades in all things, the broad and the detailed, the good and the bad, the form and the formlessness. This is the complete realization of \"li\".\n\nThe rituals and practices of \"li\" are dynamic in nature. \"Li\" practices have been revised and evaluated throughout time to reflect the emerging views and beliefs found in society. Although these practices may change, which happens very slowly over time, the fundamental ideals remain at the core of \"li\", which largely relate to social order.\n\nConfucius envisioned proper government being guided by the principles of \"li\". Some Confucians proposed the perfectibility of human beings with learning Li as an important part of that process. Overall, Confucians believed governments should place more emphasis on \"li\" and rely much less on penal punishment when they govern.\n\nConfucius stressed the importance of the rites as fundamental to proper governmental leadership. In his sayings, Confucius regarded feudal lords in China that adopted the Chinese rites as being just rulers of the Central States. Contrarily, feudal lords that did not adopt these rites were considered uncivilized, not worthy of being considered Chinese or part of the Central States (Spring and Autumn Annals).\n\n\"Li\" should be practiced by all members of the society. \"Li\" also involves the superior treating the inferior with propriety and respect. As Confucius said \"a prince should employ his minister according to the rules of propriety (Li); ministers should serve their prince with loyalty\" (Analects, 3:19).\n\n\"Li\" is \"one term by which the [traditional Chinese] historiographers could name all the principles of conservatism they advanced in the speeches of their characters.\" \n\n"}
{"id": "939578", "url": "https://en.wikipedia.org/wiki?curid=939578", "title": "List of eponymous laws", "text": "List of eponymous laws\n\nThis list of eponymous laws provides links to articles on laws, principles, adages, and other succinct observations or predictions named after a person. In some cases the person named has coined the law – such as Parkinson's law. In others, the work or publications of the individual have led to the law being so named – as is the case with Moore's law. There are also laws ascribed to individuals by others, such as Murphy's law; or given eponymous names despite the absence of the named person.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1722373", "url": "https://en.wikipedia.org/wiki?curid=1722373", "title": "Locard's exchange principle", "text": "Locard's exchange principle\n\nIn forensic science, Locard's exchange principle holds that the perpetrator of a crime will bring something into the crime scene and leave with something from it, and that both can be used as forensic evidence. Dr. Edmond Locard (13 December 1877 – 4 May 1966) was a pioneer in forensic science who became known as the Sherlock Holmes of France. He formulated the basic principle of forensic science as: \"Every contact leaves a trace\". Paul L. Kirk expressed the principle as follows:\n\nFragmentary or trace evidence is any type of material left at (or taken from) a crime scene, or the result of contact between two surfaces, such as shoes and the floor covering or soil, or fibers from where someone sat on an upholstered chair.\n\nWhen a crime is committed, fragmentary (or trace) evidence needs to be collected from the scene. A team of specialized police technicians goes to the scene of the crime and seals it off. They record video and take photographs of the crime scene, victim/s (if there are any) and items of evidence. If necessary, they undertake ballistics examinations. They check for foot, shoe, and tire mark impressions, plus hair as well as examine any vehicles and check for fingerprints - whole or partial.\n\nThe case studies below show how prevalent Locard's Exchange Principle is in each and every crime. The examples using Locard's Principle show not only how the transfer of trace evidence can tell the tale of what happened, but also how much care is required when collecting and evaluating trace evidence.\n\nKarola and Melanie Weimar, aged 5 and 7, lived with their parents, Reinhard and Monika, in Germany. They were reported missing on 4 August 1986. Their bodies were found on 7 August. They had been murdered.\n\nMonika first said the children had breakfast, then went to a playground. Three weeks later she said they were already dead when she returned home the previous night: Reinhard was sitting on the edge of Karola's bed, weeping and confused; he then disposed of the bodies.\n\nBoth parents were suspected, but Monika was having an affair, and was seen where Melanie's body was later found. She was convicted, but after serving her sentence, was released in 2006.\n\nInvestigators determined what clothes Monika was wearing on 3 and 4 August, but not Reinhard's clothes, so only fibers from her clothing were identified on the children's bodies, yet they were also constantly in contact with him.\n\nThe bedding contained 14 fibers from Karola's T-shirt. Frictionless tests, simulating a dead child, matched that figure better than the friction tests, simulating a live child, so Karola could have lain lifelessly in bed wearing her T-shirt, as stated by her mother.\n\n35 fibers from Monika's blouse were found on the back of Melanie's T-shirt, but only one on her bed sheet. In tests, between 6 and 10 fibers remained on the sheet. These higher numbers were thought to disprove Monika's claim that she gave her child a goodbye hug the previous day. However, there are several likely explanations. For example, the bedding was put in one bag, so fibers from the sheet could have been transferred to the cover and pillow. Only the central area of the top of the sheet was taped: it might have originally contained more than one blouse fiber, the others could have been transferred to the back or sides while in the bag.\n\nThe blouse fibers on Melanie's clothing were distributed evenly, not the clusters expected from carrying the body.\n\n265 fibers from the family car’s rear seat covers were found on Melanie's panties and the inside of her trousers, but only a small number of fibers from the front seats was found on the children. This helped disprove the theory that they were killed on the front seats.\n\nMelanie's clothes and hair were covered in 375 clinging fruits of goosegrass. As some of these itchy things were on the inside of her trousers and on her panties, the trousers must have been put on her after death.\n\nNo sand was found on the bodies or clothing (including socks and sandals) of either child, making the morning playground story unlikely.\n\nDanielle van Dam, aged 7, lived with her parents and brothers in San Diego, California. She was reported missing on 2 February 2002; her body was discovered on 27 February. Neighbor David Westerfield was almost immediately suspected, as he had gone camping in his RV, and he was convicted of her kidnapping and murder.\n\nHairs consistent with the van Dams’ dog were found in his RV, also carpet fibers consistent with Danielle's bedroom carpet. Danielle's nightly ritual was to wrestle with the dog after getting into her pajamas. The prosecution argued that those hairs and fibers got onto her pajamas through that contact, and were then carried on the pajamas to first Westerfield's house and then to his RV, when he kidnapped her from her bed. The alternative scenario is that they got onto her daytime clothes, and those of her mother and younger brother, and were carried to his house when they visited him earlier that week selling cookies. He said his laundry was out during that visit, so trace evidence from them could have got on it, and then been transferred to his bedroom and his RV (secondary Locard transfer). Also, his RV was often parked, sometimes unlocked, in the neighborhood streets, so Danielle could have sneaked inside, leaving behind that evidence.\n\nNo trace of Westerfield was found in the van Dam house.\n\n14 hairs consistent with Danielle's were found in his environment. All but one were compared on only mitochondrial DNA, so they might have come from her mother or a sibling. Most (21) of the hairs were in a dryer lint ball in his trash can, so they might have got in his laundry before the kidnapping.\n\nThere were 5 carpet fibers in his RV, but none in his house, suggesting those were deposited by someone going directly from her house to his RV, or they may have come from another house in that development.\n\nNo Danielle pajama or bedding fibers were reported in his environment. There was no trace evidence in his SUV (which casts doubt on the belief that she was transported from his house to his RV in his SUV). He vacuumed his RV after the kidnapping, but no trace evidence was in the vacuum cleaner.\n\nOne orange fiber with her body was consistent with about 200 in his house and 20 in his SUV (none in his RV), while 21 blue fibers with her body were consistent with 10 in his house and 46 in his RV (none in his SUV). Contrary to media reports, only a few items from her house were tested so that can’t be excluded as the source. In particular, the clothes of Danielle and her family during the cookie sale were not determined and eliminated. There were apparently two different types of the orange fibers, dull and very bright (so the number which matched might have been much less than 200). There were red fibers with her fingernails, and many other fibers with her body, which could not be matched to his environment. The only non-Danielle hair found with her body wasn’t his, nor was any desert sand reported with the body, and no soil or vegetation from the dump site was reported on his shoes, laundry, shovel or RV.\n\nTo explain why so much expected evidence was missing, the prosecution argued that he went on a cleaning frenzy, and tossed out evidence.\n\nIt is also mentioned in an episode of \"Hawaii Five-O\"\n\n"}
{"id": "1148564", "url": "https://en.wikipedia.org/wiki?curid=1148564", "title": "Marginal concepts", "text": "Marginal concepts\n\nIn economics, marginal concepts are associated with a \"specific change\" in the quantity used of a good or service, as opposed to some notion of the over-all significance of that class of good or service, or of some total quantity thereof.\n\nConstraints are conceptualized as a \"border\" or \"margin\". The location of the margin for any individual corresponds to his or her \"endowment\", broadly conceived to include opportunities. This endowment is determined by many things including physical laws (which constrain how forms of energy and matter may be transformed), accidents of nature (which determine the presence of natural resources), and the outcomes of past decisions made both by others and by the individual himself or herself.\n\nA value that holds true given particular constraints is a \"marginal\" value. A change that would be affected as or by a specific loosening or tightening of those constraints is a \"marginal\" change, as large as the smallest relevant division of that good or service. For reasons of tractability, it is often assumed in neoclassical analysis that goods and services are continuously divisible. In such context, a marginal change may be an infinitesimal change or a limit. However, strictly speaking, the smallest relevant division may be quite large.\n\nThe marginal use of a good or service is the specific use to which an agent would put a given increase, or the specific use of the good or service that would be abandoned in response to a given decrease.\n\nThe marginal utility of a good or service is the utility of the specific use to which an agent would put a given increase in that good or service, or of the specific use that would be abandoned in response to a given decrease. In other words, marginal utility is the utility of the marginal use.\n\nThe marginal rate of substitution is the rate of substitution is the least favorable rate, at the margin, at which an agent is willing to exchange units of one good or service for units of another.\n\nA marginal benefit is a benefit (howsoever ranked or measured) associated with a marginal change.\n\nThe term “marginal cost” may refer to an opportunity cost at the margin, or to marginal \"pecuniary\" cost — that is to say marginal cost measured by forgone money.\n\nOther marginal concepts include (but are not limited to):\n\nMarginalism is the use of marginal concepts to explain economic phenomena.\n\nThe related concept of elasticity is the ratio of the incremental percentage change in one variable with respect to an incremental percentage change in another variable.\n"}
{"id": "17886679", "url": "https://en.wikipedia.org/wiki?curid=17886679", "title": "Marginal return", "text": "Marginal return\n\nMarginal return is the rate of return for a marginal increase in investment; roughly, this is the additional output resulting from a one-unit increase in the use of a variable input, while other inputs are constant.\n\n"}
{"id": "1280458", "url": "https://en.wikipedia.org/wiki?curid=1280458", "title": "Marginal revenue", "text": "Marginal revenue\n\nIn microeconomics, marginal revenue (R') is the additional revenue that will be generated by increasing product sales by one unit. It can also be described as the unit revenue the last item sold has generated for the firm. In a perfectly competitive market, the additional revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good. This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price. However, a monopoly determines the entire industry's sales. As a result, it will have to lower the price of all units sold to increase sales by 1 unit. Therefore, the marginal revenue generated is always lower than the price the firm is able to charge for the unit sold, since each reduction in price causes unit revenue to decline on every good the firm sells. The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. \n\nA firms profits will be maximized when marginal revenue (MR) equals marginal cost (MC). If formula_1 then a firm should increase output for more profits, if formula_2 then a firm should decrease output for additional profits. A firm should choose the output level which is profit maximizing under perfect competition theory formula_3.\n\nMarginal revenue is equal to the ratio of the change in revenue for some change in quantity sold to that change in quantity sold. This can also be represented as a derivative when the change in quantity sold becomes arbitrarily small. More formally, define the revenue function to be the following\n\nBy the product rule, marginal revenue is then given by\n\nFor a firm facing perfect competition, price does not change with quantity sold (formula_6), so marginal revenue is equal to price. For a monopoly, the price decreases with quantity sold (formula_7), so marginal revenue is less than price (for positive formula_8).\n\nThe marginal revenue curve is affected by the same factors as the demand curve - changes in income, change in the prices of complements and substitutes, change in populations. These factors can cause the R curve to shift and rotate.\n\nThe relationship between marginal revenue and the elasticity of demand by the firm's customers can be derived as follows:\n\nwhere e is the price elasticity of demand. If demand is inelastic (e < 1) then R' will be negative, because to sell a marginal (infinitesimal) unit the firm would have to lower the selling price so much that it would lose more revenue on the pre-existing units than it would gain on the incremental unit. If demand is elastic (e > 1) R' will be positive, because the additional unit would not drive down the price by so much. If the firm is a perfect competitor, so that it is so small in the market that its quantity produced and sold has no effect on the price, then the price elasticity of demand is negative infinity, and marginal revenue simply equals the (market-determined) price.\n\nProfit maximization requires that a firm produces where marginal revenue equals marginal costs. Firm managers are unlikely to have complete information concerning their marginal revenue function or their marginal costs. Fortunately, the profit maximization conditions can be expressed in a “more easily applicable form” or rule of thumb.\n\nMarkup is the difference between price and marginal cost. The formula states that markup as a percentage of price equals the negative of the inverse of elasticity of demand. Alternatively, the relationship can be expressed as:\n\nThus if e is - 2 and mc is $5.00 then price is $10.00.\n\n(<R> - C')/ <R> = - 1/e is called the Lerner index after economist Abba Lerner. The Lerner index is a measure of market power - the ability of a firm to charge a price that exceeds marginal cost. The index varies from zero to 1. The greater the difference between price and marginal cost the closer the index value is to 1. The Lerner index increases as demand becomes less elastic.\n\nExample\nIf a company can sell 10 units at $20 each or 11 units at $19 each, then the marginal revenue from the eleventh unit is (11 × 19) - (10 × 20) = $9.\n\n\n"}
{"id": "1402030", "url": "https://en.wikipedia.org/wiki?curid=1402030", "title": "Marginal revenue productivity theory of wages", "text": "Marginal revenue productivity theory of wages\n\nThe marginal revenue productivity theory of wages is a theory in neoclassical economics stating that wages are paid at a level equal to the marginal revenue product of labor, MRP (the value of the marginal product of labor), which is the increment to revenues caused by the increment to output produced by the last laborer employed. In a model, this is justified by an assumption that the firm is profit-maximizing and thus would employ labor only up to the point that marginal labor costs equal the marginal revenue generated for the firm. \n\nThe marginal revenue product (MRP) of a worker is equal to the product of the marginal product of labour (MP) (the increment to output from an increment to labor used) and the marginal revenue (MR) (the increment to sales revenue from an increment to output): MRP = MP × MR. The theory states that workers will be hired up to the point when the marginal revenue product is equal to the wage rate. If the marginal revenue brought by the worker is less than the wage rate, then employing that laborer would cause a decrease in profit.\n\nThe idea that payments to factors of production equal their marginal productivity had been laid out by John Bates Clark and Knut Wicksell, in simpler models. Much of the MRP theory stems from Wicksell's model.\n\nThe marginal revenue product of labour MRP is the increase in revenue per unit increase in the variable input = ∆TR/∆L \n\nThe change in output is not limited to that directly attributable to the additional worker. Assuming that the firm is operating with diminishing marginal returns then the addition of an extra worker reduces the average productivity of every other worker (and every other worker affects the marginal productivity of the additional worker).\n\nAs above noted the firm will continue to add units of labor until the MRP equals the wage rate \"w\"—mathematically until\n\nUnder perfect competition, marginal revenue product is equal to marginal physical product (extra unit produced as a result of a new employment) multiplied by price.\n\nThis is because the firm in perfect competition is a price taker. It does not have to lower the price in order to sell additional units of the good.\n\nFirms operating as monopolies or in imperfect competition face downward-sloping demand curves. To sell extra units of output, they would have to lower their output's price. Under such market conditions, marginal revenue product will not equal MPP×Price. This is because the firm is not able to sell output at a fixed price per unit. Thus the MRP curve of a firm in monopoly or in imperfect competition will slope downwards, when plotted against labor usage, at a faster rate than in perfect specific competition.\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "348044", "url": "https://en.wikipedia.org/wiki?curid=348044", "title": "Meta-system", "text": "Meta-system\n\nMeta-systems have several definitions. In general, they link the concepts \"system\" and \"meta-\". A \"meta-system\" is about other systems, such as describing, generalizing, modelling, or analyzing the other system(s).\n\nAccording to Turchin and Joslyn (1997), this \"natural\" systemic definition is not sufficient for their Theory of Meta-system Transition, it also is not equivalent to the definition of \"system of systems\" in Autopoietic Systems Theory.\n\nIn economics, meta-systems are like what Bataille calls general economies as opposed to the restricted economies of systems.\n\nA book about the difference between general and restricted economies is \"Complementarity\" by Arshad Naim. In this case \"meta\" is defined as what is beyond: the meta-system is what lies beyond the system.\n\nIn mathematics, biology and psychology, many variables have occurred within structures and systems that determined the results, discoveries, rates and value(s) of sets, systems, and developments within systems, structures, systems within structures and sets of structures.\n\nA mathematical-modelling rule system for a domain D is an example of a meta-system in mathematics and science, for similar and consistency of concrete or frequency found in models within a domain. \nThese are all modes or models; where commonalities are more consistent with consecutive scores or values within a ranged order and are good indicators for gauging probabilities, traits (psychology) or properties (biology).\n\nMeta-systems in cultural studies and sociology refer to contexts, milieux, situations, ecosystems, environments and the biological process with the use of commonalities in behavioral traits and human developments found surrounding a social or scientific system which the system must interact with in order to remain viable. Meta-systems have different structures and also are complementary to other structures of such systems. Without this complementarity in the values, bondings, or tact, the systems could not remain productive, viable or operational.\n\nThe term \"meta-system\" (or \"metasystem\") in cybernetics is synonymous with management system or control system. Stafford Beer, who founded management cybernetics with his viable system model, speaks of metasystems that apply metalanguages which are able to find means of making decisions when necessary improvements cannot be made. In computer science this is known as the halting problem. Here metalanguage works in a larger context than the language it describes and has more variety.\n\n"}
{"id": "33742208", "url": "https://en.wikipedia.org/wiki?curid=33742208", "title": "Models of communication", "text": "Models of communication\n\nModels of communication are conceptual models used to explain the human communication process. The first major model for communication was developed in 1948 by Claude Elwood Shannon and published with an introduction by Warren Weaver for Bell Laboratories. Following the basic concept, communication is the process of sending and receiving messages or transferring information from one part (sender) to another (receiver).\n\nIn 1960, David Berlo expanded the linear transmission model with the Sender-Message-Channel-Receiver(SMCR) Model of Communication. Later, Wilbur Schramm introduced a model that identified multiple variables in communication which includes the transmitter, encoding, media, decoding, and receiver. \n\nElwood Shannon and Warren Weaver were engineers that worked for Bell Telephone Labs in the United States. Their goal was to make sure that the telephone cables and radio waves were working at the maximum efficiency. Therefore, they developed the Shannon-Weaver model which had an intention to expand a mathematical theory of communication. The Shannon–Weaver model was developed in 1949 which is referred as the 'mother of all models'. The model is well accepted as a main initial model for Communication Studies which has grown since then.\n\nAs well, the Shannon-Weaver model was designed to mirror the functioning of radio and telephone technology. Their initial model consisted of four primary parts: sender, message, channel, and receiver. The sender was the part of a telephone a person speaks into, the channel was the telephone itself, and the receiver was the part of the phone through which one can hear the person on the other end of the line. Shannon and Weaver also recognized that there may often be static or background sounds that interfere with the process of the other partner in a telephone conversation; they referred to this as noise. Certain types of background sounds can also indicate the absence of a signal.\n\nThe original model of Shannon and Weaver has five elements: information source, transmitter, channel, receiver, and destination. To illustrate the process of the communication the first step is the information source where the information is stored. Next, in order to send the information, the message is encoded into signals, so it can travel to its destination. After the message is encoded, it goes through the channel which the signals are adapted for the transmission. In addition, the channel carried the noise course which is any interference that might happen to lead to the signal receive a different information from the source. After the channel, the message arrives in the receiver step where the message reconstruct (decode) from the signal. Finally, the message arrives at the destination.\n\nIn a simple model, often referred to as \"the transmission model\" or \"standard view of communication\", information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emissor/ sender/ encoder to a destination/ receiver/ decoder. According to this common communication-related conception, communication is viewed as a means of sending and receiving information. The strengths of this model are its simplicity, generality, and quantifiability. The mathematicians Claude Shannon and Warren Weaver structured this model on the basis of the following elements:\n\n\nShannon and Weaver argued that this concept entails three levels of problems for communication:\n\n\nDaniel Chandler criticizes the transmission model in the following terms:\n\n\nIn 1960, David Berlo expanded Shannon and Weaver's 1949 linear model of communication and created the Sender-Message-Channel-Receiver (SMCR) Model of Communication. The SMCR Model of Communication separated the model into clear parts and has been expanded upon by other scholars.\n\nThe Berlo's communication process is a simple application for communication of person-to-person which include communication source, encoder, message, channel, decoder, and communication receiver. In addition, David Berlo presented some factors that influence the communication process between two people. The factors include communication skills, awareness level, social system, cultural system, and attitude.\n\nThe Berlo's Model of Communication process starts at the source. This is the part where determine the communication skills, attitude, knowledge, social system, and culture of the people involved in the communication. After the message is developed which is elements in a set of symbols. Then the encoder step beginning. The encoder process is where the motor skills take place by speaking or writing. The message goes through the channel which carries the message by hearing, seeing, touching, smelling, or tasting. Then the decoder process takes place. In this process, the receiver interpreter the message with her or him sensory skills. Finally, the communication receiver gets the whole message understood.\n\nCommunication is usually described along a few major dimensions: Message (what type of things are communicated), source / emissor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schramm (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).\n\nCommunication can be seen as processes of information transmission governed by three levels of semiotic rules:\n\nTherefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.\n\nIn light of these weaknesses, Barnlund (1970) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.\n\nIn a slightly more complex form, a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of \"noise\" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a [code-book], and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.\n\nTheories of co-regulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society. His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society.\n\nThere is an additional working definition of communication to consider that authors like Richard A. Lanham (2003) and as far back as Erving Goffman (1959) have highlighted. This is a progression from Lasswell's attempt to define human communication through to this century and revolutionized into the constructionist model. Constructionists believe that the process of communication is in itself the only messages that exist. The packaging can not be separated from the social and historical context from which it arose, therefore the substance to look at in communication theory is style for Richard Lanham and the performance of self for Erving Goffman.\n\nLanham chose to view communication as the rival to the over encompassing use of CBS model (which pursued to further the transmission model). CBS model argues that clarity, brevity, and sincerity are the only purpose to prose discourse, therefore communication. Lanham wrote: \"If words matter too, if the whole range of human motive is seen as animating prose discourse, then rhetoric analysis leads us to the essential questions about prose style\" (Lanham 10). This is saying that rhetoric and style are fundamentally important; they are not errors to what we actually intend to transmit. The process which we construct and deconstruct meaning deserves analysis.\n\nErving Goffman sees the performance of self as the most important frame to understand communication. Goffman wrote: \"What does seem to be required of the individual is that he learn enough pieces of expression to be able to 'fill in' and manage, more or less, any part that he is likely to be given\" (Goffman 73), highlighting the significance of expression.\n\nThe truth in both cases is the articulation of the message and the package as one. The construction of the message from social and historical context is the seed as is the pre-existing message is for the transmission model. Therefore, any look into communication theory should include the possibilities drafted by such great scholars as Richard A. Lanham and Goffman that style and performance is the whole process. lun\n\nCommunication stands so deeply rooted in human behaviors and the structures of society that scholars have difficulty thinking of it while excluding social or behavioral events. Because communication theory remains a relatively young field of inquiry and integrates itself with other disciplines such as philosophy, psychology, and sociology, one probably cannot expect a consensus conceptualization of communication across disciplines.\n\nCommunication Model Terms as provided by Rothwell (11-15):\n\nHumans act toward people or things on the basis of the meanings they assign to those people or things.\n-\"Language is the source of meaning\". \n-Meaning arises out of the social interaction people have with each other.\n\n-Meaning is not inherent in objects but it is negotiated through the use of language, hence the term symbolic interactionism.\nAs human beings, we have the ability to name things.\nSymbols, including names, are arbitrary signs.\nBy talking with others, we ascribe meaning to words and develop a universe of discourse\nA symbol is a stimulus that has a learned/shared meaning and a value for people\nSignificant symbols can be nonverbal as well as linguistic.\n\n-Negative responses can consequently reduce a person to nothing.\n-Our expectations evoke responses that confirm what we originally anticipated, resulting in a self-fulfilling prophecy.\n\nThis is a one-way model to communicate with others. It consists of the sender encoding a message and channeling it to the receiver in the presence of noise. In this model there is no feedback or response which may allow for a continuous \nexchange of information (F.N.S. Palma, 1993).\n\nThe linear model was first introduced by Shannon & Weaver in 1949. In the linear communication model, the message travels one direction from the start point to the endpoint. In other words, once the sender sends the message to the receiver the communication process ends. Many communications online use the linear communication model. For example, when you send an email, post a blog, or share something on social media. However, the linear model does not explain many other forms of communication including face-to-face conversation.\n\nIt is two linear models stacked on top of each other. The sender channels a message to the receiver and the receiver then becomes the sender and channels a message to the original sender. This model has added feedback, indicating that communication is not a one way but a two way process. It also has \"field of experience\" which includes our cultural background, ethnicity geographic location, extent of travel, and general personal experiences accumulated over the course of your lifetime. Draw backs – there is feedback but it is not simultaneous.\n\n\nCommunication theory can be seen from one of the following viewpoints:\n\n\nInspection of a particular theory on this level will provide a framework on the nature of communication as seen within the confines of that theory.\n\nTheories can also be studied and organized according to the ontological, epistemological, and axiological framework imposed by the theorist.\n\nOntology essentially poses the question of what, exactly, the theorist is examining. One must consider the very nature of reality. The answer usually falls in one of three realms depending on whether the theorist sees the phenomena through the lens of a realist, nominalist, or social constructionist. Realist perspective views the world objectively, believing that there is a world outside of our own experience and cognitions. Nominalists see the world subjectively, claiming that everything outside of one's cognitions is simply names and labels. Social constructionists straddle the fence between objective and subjective reality, claiming that reality is what we create together.\n\nEpistemology is an examination of the approaches and beliefs which inform particular modes of study of phenomena and domains of expertise. In positivist approaches to epistemology, objective knowledge is seen as the result of the empirical observation and perceptual experience. In the history of science, empirical evidence collected by way of pragmatic-calculation and the scientific method is believed to be the most likely to reflect truth in the findings. Such approaches are meant to predict a phenomenon. Subjective theory holds that understanding is based on situated knowledge, typically found using interpretative methodology such as ethnography and also interviews. Subjective theories are typically developed to explain or understand phenomena in the social world.\n\nAxiology is concerned with how values inform research and theory development. Most communication theory is guided by one of three axiological approaches. The first approach recognizes that values will influence theorists' interests but suggests that those values must be set aside once actual research begins. Outside replication of research findings is particularly important in this approach to prevent individual researchers' values from contaminating their findings and interpretations. The second approach rejects the idea that values can be eliminated from any stage of theory development. Within this approach, theorists do not try to divorce their values from inquiry. Instead, they remain mindful of their values so that they understand how those values contextualize, influence or skew their findings. The third approach not only rejects the idea that values can be separated from research and theory, but rejects the idea that they should be separated. This approach is often adopted by critical theorists who believe that the role of communication theory is to identify oppression and produce social change. In this axiological approach, theorists embrace their values and work to reproduce those values in their research and theory development.\n\n\n"}
{"id": "4602393", "url": "https://en.wikipedia.org/wiki?curid=4602393", "title": "Models of scientific inquiry", "text": "Models of scientific inquiry\n\nIn the philosophy of science, models of scientific inquiry have two functions: first, to provide a descriptive account of \"how\" scientific inquiry is carried out in practice, and second, to provide an explanatory account of \"why\" scientific inquiry succeeds as well as it appears to do in arriving at genuine knowledge.\n\nThe search for scientific knowledge ends far back into antiquity. At some point in the past, at least by the time of Aristotle, philosophers recognized that a fundamental distinction should be drawn between two kinds of scientific knowledge—roughly, knowledge \"that\" and knowledge \"why\". It is one thing to know \"that\" each planet periodically reverses the direction of its motion with respect to the background of fixed stars; it is quite a different matter to know \"why\". Knowledge of the former type is descriptive; knowledge of the latter type is explanatory. It is explanatory knowledge that provides scientific understanding of the world. (Salmon, 2006, pg. 3)\n\n\"Scientific inquiry refers to the diverse ways in which scientists study the natural world and propose explanations based on the evidence derived from their work.\"\n\nThe classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.\n\nWesley Salmon (1989) began his historical survey of scientific explanation with what he called the \"received view\", as it was received from Hempel and Oppenheim in the years beginning with their \"Studies in the Logic of Explanation\" (1948) and culminating in Hempel's \"Aspects of Scientific Explanation\" (1965). Salmon summed up his analysis of these developments by means of the following Table.\n\nIn this classification, a deductive-nomological (D-N) explanation of an occurrence is a valid deduction whose conclusion states that the outcome to be explained did in fact occur. The deductive argument is called an \"explanation\", its premisses are called the \"explanans\" (L: \"explaining\") and the conclusion is called the \"explanandum\" (L: \"to be explained\"). Depending on a number of additional qualifications, an explanation may be ranked on a scale from \"potential\" to \"true\".\n\nNot all explanations in science are of the D-N type, however. An \"inductive-statistical\" (I-S) explanation accounts for an occurrence by subsuming it under statistical laws, rather than categorical or universal laws, and the mode of subsumption is itself inductive instead of deductive. The D-N type can be seen as a limiting case of the more general I-S type, the measure of certainty involved being complete, or probability 1, in the former case, whereas it is less than complete, probability < 1, in the latter case.\n\nIn this view, the D-N mode of reasoning, in addition to being used to explain particular occurrences, can also be used to explain general regularities, simply by deducing them from still more general laws.\n\nFinally, the \"deductive-statistical\" (D-S) type of explanation, properly regarded as a subclass of the D-N type, explains statistical regularities by deduction from more comprehensive statistical laws. (Salmon 1989, pp. 8–9).\n\nSuch was the \"received view\" of scientific explanation from the point of view of logical empiricism, that Salmon says \"held sway\" during the third quarter of the last century (Salmon, p. 10).\n\nDuring the course of history, one theory has succeeded another, and some have suggested further work while others have seemed content just to explain the phenomena. The reasons why one theory has replaced another are not always obvious or simple. The philosophy of science includes the question: \"What criteria are satisfied by a 'good' theory\". This question has a long history, and many scientists, as well as philosophers, have considered it. The objective is to be able to choose one theory as preferable to another without introducing cognitive bias. Several often proposed criteria were summarized by Colyvan. A good theory:\n\nStephen Hawking supports items 1–4, but does not mention fruitfulness. On the other hand, Kuhn emphasizes the importance of seminality.\n\nThe goal here is to make the choice between theories less arbitrary. Nonetheless, these criteria contain subjective elements, and are heuristics rather than part of scientific method. Also, criteria such as these do not necessarily decide between alternative theories. Quoting Bird:\nIt also is debatable whether existing scientific theories satisfy all these criteria, which may represent goals not yet achieved. For example, explanatory power over all existing observations (criterion 3) is satisfied by no one theory at the moment.\nThe desiderata of a \"good\" theory have been debated for centuries, going back perhaps even earlier than Occam's razor, which often is taken as an attribute of a good theory. Occam's razor might fall under the heading of \"elegance\", the first item on the list, but too zealous an application was cautioned by Albert Einstein: \"Everything should be made as simple as possible, but no simpler.\" It is arguable that \"parsimony\" and \"elegance\" \"typically pull in different directions\". The falsifiability item on the list is related to the criterion proposed by Popper as demarcating a scientific theory from a theory like astrology: both \"explain\" observations, but the scientific theory takes the risk of making predictions that decide whether it is right or wrong:\n\nThomas Kuhn argued that changes in scientists' views of reality not only contain subjective elements, but result from group dynamics, \"revolutions\" in scientific practice which result in paradigm shifts. As an example, Kuhn suggested that the heliocentric \"Copernican Revolution\" replaced the geocentric views of Ptolemy not because of empirical failures, but because of a new \"paradigm\" that exerted control over what scientists felt to be the more fruitful way to pursue their goals.\n\nDeductive logic and inductive logic are quite different in their approaches.\n\nDeductive logic is the reasoning of proof, or logical implication. It is the logic used in mathematics and other axiomatic systems such as formal logic. In a deductive system, there will be axioms (postulates) which are not proven. Indeed, they cannot be proven without circularity. There will also be primitive terms which are not defined, as they cannot be defined without circularity. For example, one can define a line as a set of points, but to then define a point as the intersection of two lines would be circular. Because of these interesting characteristics of formal systems, Bertrand Russell humorously referred to mathematics as \"the field where we don't know what we are talking about, nor whether or not what we say is true\". All theorems and corollaries are proven by exploring the implications of the axiomata and other theorems that have previously been developed. New terms are defined using the primitive terms and other derived definitions based on those primitive terms.\n\nIn a deductive system, one can correctly use the term \"proof\", as applying to a theorem. To say that a theorem is proven means that it is impossible for the axioms to be true and the theorem to be false. For example, we could do a simple syllogism such as the following:\n\n\nNotice that it is not possible (assuming all of the trivial qualifying criteria are supplied) to be in Arches and not be in Utah. However, one can be in Utah while not in Arches National Park. The implication only works in one direction. Statements (1) and (2) taken together imply statement (3). Statement (3) does not imply anything about statements (1) or (2). Notice that we have not proven statement (3), but we have shown that statements (1) and (2) together imply statement (3). In mathematics, what is proven is not the truth of a particular theorem, but that the axioms of the system imply the theorem. In other words, it is impossible for the axioms to be true and the theorem to be false. The strength of deductive systems is that they are sure of their results. The weakness is that they are abstract constructs which are, unfortunately, one step removed from the physical world. They are very useful, however, as mathematics has provided great insights into natural science by providing useful models of natural phenomena. One result is the development of products and processes that benefit mankind.\n\nLearning about the physical world requires the use of inductive logic. This is the logic of theory building. It is useful in such widely divergent enterprises as science and crime scene detective work. One makes a set of observations, and seeks to explain what one sees. The observer forms a hypothesis in an attempt to explain what he/she has observed. The hypothesis will have implications, which will point to certain other observations that would naturally result from either a repeat of the experiment or making more observations from a slightly different set of circumstances. If the predicted observations hold true, one feels excitement that they may be on the right track. However, the hypothesis has not been proven. The hypothesis implies that certain observations should follow, but positive observations do not imply the hypothesis. They only make it more believable. It is quite possible that some other hypothesis could also account for the known observations, and may do better with future experiments. The implication flows in only one direction, as in the syllogism used in the discussion on deduction. Therefore, it is never correct to say that a scientific principle or hypothesis/theory has been proven. (At least, not in the rigorous sense of proof used in deductive systems.)\n\nA classic example of this is the study of gravitation. Newton formed a law for gravitation stating that the force of gravitation is directly proportional to the product of the two masses and inversely proportional to the square of the distance between them. For over 170 years, all observations seemed to validate his equation. However, telescopes eventually became powerful enough to see a slight discrepancy in the orbit of Mercury. Scientists tried everything imaginable to explain the discrepancy, but they could not do so using the objects that would bear on the orbit of Mercury. Eventually, Einstein developed his theory of general relativity and it explained the orbit of Mercury and all other known observations dealing with gravitation. During the long period of time when scientists were making observations that seemed to validate Newton's theory, they did not, in fact, prove his theory to be true. However, it must have seemed at the time that they did. It only took one counterexample (Mercury's orbit) to prove that there was something wrong with his theory.\n\nThis is typical of inductive logic. All of the observations that seem to validate the theory, do not prove its truth. But one counter-example can prove it false. That means that deductive logic is used in the evaluation of a theory. In other words, if A implies B, then not B implies not A. Einstein's theory of General Relativity has been supported by many observations using the best scientific instruments and experiments. However, his theory now has the same status as Newton's theory of gravitation prior to seeing the problems in the orbit of Mercury. It is highly credible and validated with all we know, but it is not proven. It is only the best we have at this point in time.\n\nAnother example of correct scientific reasoning is shown in the current search for the Higgs boson. Scientists on the Compact Muon Solenoid experiment at the Large Hadron Collider have conducted experiments yielding data suggesting the existence of the Higgs boson. However, realizing that the results could possibly be explained as a background fluctuation and not the Higgs boson, they are cautious and waiting for further data from future experiments. Said Guido Tonelli:\n\nA brief overview of the scientific method would then contain these steps as a minimum:\n\n\nWhen a hypothesis has survived a sufficient number of tests, it may be promoted to a scientific theory. A theory is a hypothesis that has survived many tests and seems to be consistent with other established scientific theories. Since a theory is a promoted hypothesis, it is of the same 'logical' species and shares the same logical limitations. Just as a hypothesis cannot be proven but can be disproved, that same is true for a theory. It is a difference of degree, not kind.\n\nArguments from analogy are another type of inductive reasoning. In arguing from analogy, one infers that since two things are alike in several respects, they are likely to be alike in another respect. This is, of course, an assumption. It is natural to attempt to find similarities between two phenomena and wonder what one can learn from those similarities. However, to notice that two things share attributes in several respects does not imply any similarities in other respects. It is possible that the observer has already noticed all of the attributes that are shared and any other attributes will be distinct. Argument from analogy is an unreliable method of reasoning that can lead to erroneous conclusions, and thus cannot be used to establish scientific facts.\n\n\n\nFor interesting explanations regarding the orbit of Mercury and General Relativity, the following links are useful:\n"}
{"id": "161019", "url": "https://en.wikipedia.org/wiki?curid=161019", "title": "Negation", "text": "Negation\n\nIn logic, negation, also called the logical complement, is an operation that takes a proposition formula_1 to another proposition \"not formula_1\", written formula_3 (¬P), which is interpreted intuitively as being true when formula_1 is false, and false when formula_1 is true. Negation is thus a unary (single-argument) logical connective. It may be applied as an operation on notions, propositions, truth values, or semantic values more generally. In classical logic, negation is normally identified with the truth function that takes \"truth\" to \"falsity\" and vice versa. In intuitionistic logic, according to the Brouwer–Heyting–Kolmogorov interpretation, the negation of a proposition formula_1 is the proposition whose proofs are the refutations of formula_1.\n\nNo agreement exists as to the possibility of defining negation, as to its logical status, function, and meaning, as to its field of applicability..., and as to the interpretation of the negative judgment, (F.H. Heinemann 1944).\n\n\"Classical negation\" is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" when its operand is false and a value of \"false\" when its operand is true. So, if statement formula_1 is true, then formula_3 (pronounced \"not P\") would therefore be false; and conversely, if formula_3 is false, then formula_1 would be true.\n\nThe truth table of formula_3 is as follows:\n\nNegation can be defined in terms of other logical operations. For example, formula_3 can be defined as formula_14 (where formula_15 is logical consequence and formula_16 is absolute falsehood). Conversely, one can define formula_16 as formula_18 for any proposition formula_19 (where formula_20 is logical conjunction). The idea here is that any contradiction is false. While these ideas work in both classical and intuitionistic logic, they do not work in paraconsistent logic, where contradictions are not necessarily false. In classical logic, we also get a further identity, formula_21 can be defined as formula_22, where formula_23 is logical disjunction.\n\nAlgebraically, classical negation corresponds to complementation in a Boolean algebra, and intuitionistic negation to pseudocomplementation in a Heyting algebra. These algebras provide a semantics for classical and intuitionistic logic respectively.\n\nThe negation of a proposition formula_1 is notated in different ways in various contexts of discussion and fields of application. Among these variants are the following:\n\nThe notation N\"p\" is Łukasiewicz notation.\n\nIn set theory formula_25 is also used to indicate 'not member of': formula_26 is the set of all members of formula_27 that are not members of formula_28.\n\nNo matter how it is notated or symbolized, the negation formula_3 can be read as \"it is not the case that formula_1\", \"not that formula_1\", or usually more simply as \"not formula_1\".\n\nWithin a system of classical logic, double negation, that is, the negation of the negation of a proposition formula_1, is logically equivalent to formula_1. Expressed in symbolic terms, formula_35. In intuitionistic logic, a proposition implies its double negation but not conversely. This marks one important difference between classical and intuitionistic negation. Algebraically, classical negation is called an involution of period two.\n\nHowever, in intuitionistic logic we do have the equivalence of formula_36. Moreover, in the propositional case, a sentence is classically provable if its double negation is intuitionistically provable. This result is known as Glivenko's theorem.\n\nDe Morgan's laws provide a way of distributing negation over disjunction and conjunction :\n\nLet formula_39 denote the logical xor operation. In Boolean algebra, a linear function is one such that:\n\nIf there exists formula_40,\nformula_41,\nfor all formula_42.\n\nAnother way to express this is that each variable always makes a difference in the truth-value of the operation or it never makes a difference. Negation is a linear logical operator.\n\nIn Boolean algebra a self dual function is one such that:\n\nformula_43 for all\nformula_44.\nNegation is a self dual logical operator.\n\nThere are a number of equivalent ways to formulate rules for negation. One usual way to formulate classical negation in a natural deduction setting is to take as primitive rules of inference \"negation introduction\" (from a derivation of formula_1 to both formula_19 and formula_47, infer formula_3; this rule also being called \"reductio ad absurdum\"), \"negation elimination\" (from formula_1 and formula_3 infer formula_19; this rule also being called \"ex falso quodlibet\"), and \"double negation elimination\" (from formula_52 infer formula_1). One obtains the rules for intuitionistic negation the same way but by excluding double negation elimination.\n\nNegation introduction states that if an absurdity can be drawn as conclusion from formula_1 then formula_1 must not be the case (i.e. formula_1 is false (classically) or refutable (intuitionistically) or etc.). Negation elimination states that anything follows from an absurdity. Sometimes negation elimination is formulated using a primitive absurdity sign formula_16. In this case the rule says that from formula_1 and formula_3 follows an absurdity. Together with double negation elimination one may infer our originally formulated rule, namely that anything follows from an absurdity.\n\nTypically the intuitionistic negation formula_3 of formula_1 is defined as formula_14. Then negation introduction and elimination are just special cases of implication introduction (conditional proof) and elimination (modus ponens). In this case one must also add as a primitive rule \"ex falso quodlibet\".\n\nAs in mathematics, negation is used in computer science to construct logical statements.\n\nThe \"codice_1\" signifies logical NOT in B, C, and languages with a C-inspired syntax such as C++, Java, JavaScript, Perl, and PHP. \"codice_2\" is the operator used in ALGOL 60, BASIC, and languages with an ALGOL- or BASIC-inspired syntax such as Pascal, Ada, Eiffel and Seed7. Some languages (C++, Perl, etc.) provide more than one operator for negation. A few languages like PL/I and Ratfor use codice_3 for negation. Some modern computers and operating systems will display codice_3 as codice_1 on files encoded in ASCII. Most modern languages allow the above statement to be shortened from codice_6 to codice_7, which allows sometimes, when the compiler/interpreter is not able to optimize it, faster programs.\n\nIn computer science there is also \"bitwise negation\". This takes the value given and switches all the binary 1s to 0s and 0s to 1s. See bitwise operation. This is often used to create ones' complement or \"codice_8\" in C or C++ and two's complement (just simplified to \"codice_9\" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).\n\nTo get the absolute (positive equivalent) value of a given integer the following would work as the \"codice_9\" changes it from negative to positive (it is negative because \"codice_11\" yields true)\n\nTo demonstrate logical negation:\n\nInverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).\n\nThis convention occasionally surfaces in written speech, as computer-related slang for \"not\". The phrase codice_12, for example, means \"not voting\".\n\nIn Kripke semantics where the semantic values of formulae are sets of possible worlds, negation can be taken to mean set-theoretic complementation. (See also possible world semantics.)\n\n\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "3558732", "url": "https://en.wikipedia.org/wiki?curid=3558732", "title": "Negative and positive rights", "text": "Negative and positive rights\n\nNegative and positive rights are rights that oblige either action (\"positive rights\") or inaction (\"negative rights\"). These obligations may be of either a legal or moral character. The notion of positive and negative rights may also be applied to liberty rights.\n\nTo take an example involving two parties in a court of law: Adrian has a \"negative right to x\" against Clay if and only if Clay is \"prohibited\" from acting upon Adrian in some way regarding \"x\". In contrast, Adrian has a \"positive right to x\" against Clay if and only if Clay is obliged to act upon Adrian in some way regarding \"x\". A case in point, if Adrian has a \"negative right to life\" against Clay, then Clay is required to refrain from killing Adrian; while if Adrian has a \"positive right to life\" against Clay, then Clay is required to act as necessary to preserve the life of Adrian.\n\nRights considered \"negative rights\" may include civil and political rights such as freedom of speech, life, private property, freedom from violent crime, freedom of religion, \"habeas corpus\", a fair trial, and freedom from slavery.\n\nRights considered \"positive rights\", as initially proposed in 1979 by the Czech jurist Karel Vasak, may include other civil and political rights such as police protection of person and property and the right to counsel, as well as economic, social and cultural rights such as food, housing, public education, employment, national security, military, health care, social security, internet access, and a minimum standard of living. In the \"three generations\" account of human rights, negative rights are often associated with the first generation of rights, while positive rights are associated with the second and third generations.\n\nSome philosophers (see criticisms) disagree that the negative-positive rights distinction is useful or valid.\n\nUnder the theory of positive and negative rights, a negative right is a right \"not to be\" subjected to an action of another person or group—a government, for example—usually in the form of abuse or coercion. As such, negative rights exist unless someone acts to \"negate\" them. A positive right is a right \"to be\" subjected to an action of another person or group. In other words, for a positive right to be exercised, someone else's actions must be \"added\" to the equation. In theory, a negative right forbids others from acting against the right holder, while a positive right obligates others to act with respect to the right holder. In the framework of the Kantian categorical imperative, negative rights can be associated with perfect duties while positive rights can be connected to imperfect duties.\n\nBelief in a distinction between positive and negative rights is usually maintained, or emphasized, by libertarians, who believe that positive rights do not exist until they are created by contract. The United Nations Universal Declaration of Human Rights lists both positive and negative rights (but does not identify them as such). The constitutions of most liberal democracies guarantee negative rights, but not all include positive rights. Nevertheless, positive rights are often guaranteed by other laws, and the majority of liberal democracies provide their citizens with publicly funded education, health care, social security and unemployment benefits.\n\nRights are often spoken of as inalienable and sometimes even absolute. However, in practice this is often taken as graded absolutism; rights are ranked by degree of importance, and violations of lesser ones are accepted in the course of preventing violations of greater ones. Thus, even if the right not to be killed is inalienable, the corresponding obligation on others to refrain from killing is generally understood to have at least one exception: self-defense. Certain widely accepted negative obligations (such as the obligations to refrain from theft, murder, etc.) are often considered prima facie, meaning that the legitimacy of the obligation is accepted \"on its face\"; but even if not questioned, such obligations may still be ranked for ethical analysis.\n\nThus a thief may have a negative obligation not to steal, and a police officer may have a negative obligation not to tackle people—but a police officer tackling the thief easily meets the burden of proof that he acted justifiably, since his was a breach of a lesser obligation and negated the breach of a greater obligation. Likewise a shopkeeper or other passerby may also meet this burden of proof when tackling the thief. But if any of those individuals pulled a gun and shot the (unarmed) thief for stealing, most modern societies would not accept that the burden of proof had been met. The obligation not to kill—being universally regarded as one of the highest, if not the highest obligation—is so much greater than the obligation not to steal that a breach of the latter does not justify a breach of the former. Most modern societies insist that other, very serious ethical questions need come into play before stealing could justify killing.\n\nPositive obligations confer duty. But as we see with the police officer, exercising a duty may violate negative obligations (e.g. not to overreact and kill). For this reason, in ethics positive obligations are almost never considered \"prima facie\". The greatest negative obligation may have just one exception—one higher obligation of self-defense—but even the greatest positive obligations generally require more complex ethical analysis. For example, one could easily justify failing to help, not just one, but a great many injured children quite ethically in the case of triage after a disaster. This consideration has led ethicists to agree in a general way that positive obligations are usually junior to negative obligations because they are not reliably \"prima facie\". Some critics of positive rights implicitly suggest that because positive obligations are not reliably \"prima facie\" they must always be agreed to through contract.\n\nNineteenth-century philosopher Frédéric Bastiat summarized the conflict between these negative and positive rights by saying:\nAccording to Jan Narveson, the view of some that there is no distinction between negative and positive rights on the ground that negative rights require police and courts for their enforcement is \"mistaken\". He says that the question between what one has a right to do and who if anybody enforces it are separate issues. If rights are only negative then it simply means no one has a duty to enforce them, although individuals have a right to use any non-forcible means to gain the cooperation of others in protecting those rights. Therefore, he says \"the distinction between negative and positive is quite robust.\" Libertarians hold that positive rights, which would include a right to be protected, do not exist until they are created by contract. However, those who hold this view do not mean that police, for example, are not obligated to protect the rights of citizens. Since they contract with their employers to defend citizens from violence, then they have created that obligation to their employer. A negative right to life allows an individual to defend his life from others trying to kill him, or obtain voluntary assistance from others to defend his life—but he may not force others to defend him, because he has no natural right to be provided with defense. To force a person to defend one's own negative rights, or the negative rights of a third party, would be to violate that person's negative rights.\n\nOther advocates of the view that there is a distinction between negative and positive rights argue that the presence of a police force or army is not due to any positive right to these services that citizens claim, but rather because they are natural monopolies or public goods—features of any human society that arise naturally, even while adhering to the concept of negative rights only. Robert Nozick discusses this idea at length in his book \"Anarchy, State, and Utopia\".\n\nIn the field of medicine, positive rights of patients often conflict with negative rights of physicians. In controversial areas such as abortion and assisted suicide, medical professionals may not wish to offer certain services for moral or philosophical reasons. If enough practitioners opt out as a result of conscience, a right granted by conscience clause statutes in many jurisdictions, patients may not have any means of having their own positive rights fulfilled. Such was the case of Janet Murdock, a Montana woman who could not find any physician to assist her suicide in 2009. This controversy over positive and negative rights in medicine has become a focal point in the ongoing public debate between conservative ethicist Wesley J. Smith and bioethicist Jacob M. Appel. In discussing \"Baxter v. Montana\", Appel has written:\nSmith replies that this is \"taking the duty to die and transforming it into a duty to kill\", which he argues \"reflects a profound misunderstanding of the government’s role\".\n\nPresumably, if a person has positive rights it implies that other people have positive duties (to take certain actions); whereas negative rights imply that others have negative duties (to avoid certain other actions). Philosopher Henry Shue is skeptical; he believes that all rights (regardless of whether they seem more \"negative\" or \"positive\") requires both kinds of duties at once. In other words, Shue says that honouring a right will require avoidance (a \"negative\" duty) but also protective or reparative actions (\"positive\" duties). The negative positive distinction may be a matter of emphasis; it is therefore unhelpful to describe \"any right\" as though it requires only one of the two types of duties.\n\nTo Shue, rights can always be understood as confronting \"standard threats\" against humanity. Dealing with standard threats requires all kinds of duties, which may be divided across time (e.g. \"if avoiding the harmful behaviour fails, begin to repair the damages\"), but also divided across people. The point is that every right provokes all 3 types of behaviour (avoidance, protection, repair) to some degree. Dealing with a threat like murder, for instance, will require one individual to practice avoidance (e.g. the potential murderer must stay calm), others to protect (e.g. the police officer, who must stop the attack, or the bystander, who may be obligated to call the police), and others to repair (e.g. the doctor who must resuscitate a person who has been attacked). Thus, even the negative right not to be killed can only be guaranteed with the help of some positive duties. Shue goes further, and maintains that the negative and positive rights distinction can be harmful, because it may result in the neglect of necessary duties.\n\nJames P. Sterba makes similar criticisms. He holds that any right can be made to appear either positive or negative depending on the language used to define it. He writes:\n\nSterba has rephrased the traditional \"positive right\" to provisions, and put it in the form of a sort of \"negative right\" \"not to be prevented\" from taking the resources on their own.. Thus, all rights may not only require both \"positive\" and \"negative\" duties, but it seems that rights that do not involve forced labor can be phrased positively or negatively at will. The distinction between positive and negative may not be very useful, or justified, as rights requiring the provision of labor can be rephrased from \"right to education\" or \"right to health care\" to \"right to take surplus money to pay teachers\" or \"right to take surplus money to pay doctors\".\n\n\n\n"}
{"id": "5205483", "url": "https://en.wikipedia.org/wiki?curid=5205483", "title": "Negative free bid", "text": "Negative free bid\n\nNegative free bid is a contract bridge treatment whereby a free bid by responder over an opponent's overcall shows a long suit in a weak hand and is not forcing. This is in contrast with standard treatment, where a free bid can show unlimited values and is unconditionally forcing. The treatment is a relatively recent invention, and has become quite popular, especially in expert circles.\n\nNegative free bids resolve relatively frequent situations where the responder holds a long suit with which he would like to compete for a partscore, but is deprived from bidding it by opponent's overcall.\n\nFor example, if South holds: , partner opens 1 and East overcalls 1, he couldn't bid 2 in standard methods, as it would show 10+ high-card points, and a negative double would be too off-shape. With NFB treatment in effect though, he can bid 2 which the partner may pass (unless he has extra values and support, or an excellent suit of its own without tolerance for hearts).\n\nHowever, as a corollary, negative free bids affect the scope of negative double; if the hand is suitable for \"standard\" forcing free bid (10-11+ points), a negative double has to be made first and the suit bid only in the next round. Thus, the negative double can be made with the following types of hand:\nThis can sometimes allow the opponents to preempt effectively. \nFor example, West, holding: , after this auction is in an awkward situation — he doesn't know whether partner has spades or not; whether South was bidding to make or to sacrifice — is it correct to double, bid 4 or pass?\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "577248", "url": "https://en.wikipedia.org/wiki?curid=577248", "title": "New riddle of induction", "text": "New riddle of induction\n\nGrue and bleen are examples of logical predicates coined by Nelson Goodman in \"Fact, Fiction, and Forecast\" to illustrate the \"new riddle of induction\". These predicates are unusual because their application is time-dependent; many have tried to solve the new riddle on those terms, but Hilary Putnam and others have argued such time-dependency depends on the language adopted, and in some languages it is equally true for natural-sounding predicates such as \"green.\" For Goodman they illustrate the problem of projectible predicates and ultimately, which empirical generalizations are law-like and which are not.\nGoodman's construction and use of \"grue\" and \"bleen\" illustrates how philosophers use simple examples in conceptual analysis.\n\nGoodman defined grue relative to an arbitrary but fixed time \"t\" as follows: An object is grue if and only if it is observed before \"t\" and is green, or else is not so observed and is blue. An object is bleen if and only if it is observed before \"t\" and is blue, or else is not so observed and is green.\n\nTo understand the problem Goodman posed, it is helpful to imagine some arbitrary future time \"t\", say January 1, 10. For all green things we observe up to time \"t\", such as emeralds and well-watered grass, both the predicates \"green\" and \"grue\" apply. Likewise for all blue things we observe up to time \"t\", such as bluebirds or blue flowers, both the predicates \"blue\" and \"bleen\" apply. On January 2, 10, however, emeralds and well-watered grass are \"bleen\" and bluebirds or blue flowers are \"grue\". Clearly, the predicates \"grue\" and \"bleen\" are not the kinds of predicates we use in everyday life or in science, but the problem is that they apply in just the same way as the predicates \"green\" and \"blue\" up until some future time \"t\". From our current perspective (i.e., before time \"t\"), how can we say which predicates are more projectible into the future: \"green\" and \"blue\" or \"grue\" and \"bleen\"?\n\nIn this section, Goodman's new riddle of induction is outlined in order to set the context for his introduction of the predicates \"grue\" and \"bleen\" and thereby illustrate their philosophical importance.\n\nGoodman poses Hume's problem of induction as a problem of the validity of the predictions we make. Since predictions are about what has yet to be observed and because there is no necessary connection between what has been observed and what will be observed, what is the justification for the predictions we make? We cannot use deductive logic to infer predictions about future observations based on past observations because there are no valid rules of deductive logic for such inferences. Hume's answer was that our observations of one kind of event following another kind of event result in our minds forming habits of regularity (i.e., associating one kind of event with another kind). The predictions we make are then based on these regularities or habits of mind we have formed.\n\nGoodman takes Hume's answer to be a serious one. He rejects other philosophers' objection that Hume is merely explaining the origin of our predictions and not their justification. His view is that Hume has identified something deeper. To illustrate this, Goodman turns to the problem of justifying a system of rules of deduction. For Goodman, the validity of a deductive system is justified by its conformity to good deductive practice. The justification of rules of a deductive system depends on our judgements about whether to reject or accept specific deductive inferences. Thus, for Goodman, the problem of induction dissolves into the same problem as justifying a deductive system and while, according to Goodman, Hume was on the right track with habits of mind, the problem is more complex than Hume realized.\n\nIn the context of justifying rules of induction, this becomes the problem of confirmation of generalizations for Goodman. However, the confirmation is not a problem of justification but instead it is a problem of precisely defining how evidence confirms generalizations. It is with this turn that \"grue\" and \"bleen\" have their philosophical role in Goodman's view of induction.\n\nThe new riddle of induction, for Goodman, rests on our ability to distinguish \"lawlike\" from \"non-lawlike\" generalizations. \"Lawlike\" generalizations are capable of confirmation while \"non-lawlike\" generalizations are not. \"Lawlike\" generalizations are required for making predictions. Using examples from Goodman, the generalization that all copper conducts electricity is capable of confirmation by a particular piece of copper whereas the generalization that all men in a given room are third sons is not \"lawlike\" but accidental. The generalization that all copper conducts electricity is a basis for predicting that this piece of copper will conduct electricity. The generalization that all men in a given room are third sons, however, is not a basis for predicting that a given man in that room is a third son.\n\nWhat then makes some generalizations \"lawlike\" and others accidental? This, for Goodman, becomes a problem of determining which predicates are projectible (i.e., can be used in \"lawlike\" generalizations that serve as predictions) and which are not. Goodman argues that this is where the fundamental problem lies. This problem, known as \"Goodman's paradox\", is as follows. Consider the evidence that all emeralds examined thus far have been green. This leads us to conclude (by induction) that all future emeralds will be green. However, whether this prediction is \"lawlike\" or not depends on the predicates used in this prediction. Goodman observed that (assuming \"t\" has yet to pass) it is equally true that every emerald that has been observed is \"grue\". Thus, by the same evidence we can conclude that all future emeralds will be \"grue\". The new problem of induction becomes one of distinguishing projectible predicates such as \"green\" and \"blue\" from non-projectible predicates such as \"grue\" and \"bleen\".\n\nHume, Goodman argues, missed this problem. We do not, by habit, form generalizations from all associations of events we have observed but only some of them. All past observed emeralds were green, and we formed a habit of thinking the next emerald will be green, but they were equally grue, and we do not form habits concerning grueness. \"Lawlike\" predictions (or projections) ultimately are distinguishable by the predicates we use. Goodman's solution is to argue that \"lawlike\" predictions are based on projectible predicates such as \"green\" and \"blue\" and not on non-projectible predicates such as \"grue\" and \"bleen\" and what makes predicates projectible is their \"entrenchment\", which depends on their successful past projections. Thus, \"grue\" and \"bleen\" function in Goodman's arguments to both illustrate the new riddle of induction and to illustrate the distinction between projectible and non-projectible predicates via their relative entrenchment.\n\nThe most obvious response is to point to the artificially disjunctive definition of grue. The notion of predicate \"entrenchment\" is not required. Goodman, however, noted that this move will not work. If we take \"grue\" and \"bleen\" as primitive predicates, we can define green as \"\"grue\" if first observed before \"t\" and \"bleen\" otherwise\", and likewise for blue. To deny the acceptability of this disjunctive definition of green would be to beg the question.\n\nAnother proposed resolution of the paradox (which Goodman addresses and rejects) that does not require predicate \"entrenchment\" is that \"\"x\" is grue\" is not solely a predicate of \"x\", but of \"x\" and a time \"t\"—we can know that an object is green without knowing the time \"t\", but we cannot know that it is grue. If this is the case, we should not expect \"\"x\" is grue\" to remain true when the time changes. However, one might ask why \"\"x\" is green\" is \"not\" considered a predicate of a particular time \"t\"—the more common definition of \"green\" does not require any mention of a time \"t\", but the definition \"grue\" does. As we have just seen, this response also begs the question because \"blue\" can be defined in terms of \"grue\" and \"bleen\", which explicitly refer to time.\n\nRichard Swinburne gets past the objection that green may be redefined in terms of \"grue\" and \"bleen\" by making a distinction based on how we test for the applicability of a predicate in a particular case. He distinguishes between qualitative and locational predicates. Qualitative predicates, like green, \"can\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event. Locational predicates, like \"grue\", \"cannot\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event, in this case whether \"x\" is being observed before or after time \"t\". Although green can be given a definition in terms of the locational predicates \"grue\" and \"bleen\", this is irrelevant to the fact that green meets the criterion for being a qualitative predicate whereas \"grue\" is merely locational. He concludes that if some \"x\"'s under examination—like emeralds—satisfy both a qualitative and a locational predicate, but projecting these two predicates yields conflicting predictions, namely, whether emeralds examined after time \"t\" shall appear blue or green, we should project the qualitative predicate, in this case green.\n\nRudolf Carnap responded to Goodman's 1946 article. Carnap's approach to inductive logic is based on the notion of \"degree of confirmation\" \"c\"(\"h\",\"e\") of a given hypothesis \"h\" by a given evidence \"e\". Both \"h\" and \"e\" are logical formulas expressed in a simple language \"L\" which allows for\nThe universe of discourse consists of denumerably many individuals, each of which is designated by its own constant symbol; such individuals are meant to be regarded as positions (\"like space-time points in our actual world\") rather than extended physical bodies. A state description is a (usually infinite) conjunction containing every possible ground atomic sentence, either negated or unnegated; such a conjunction describes a possible state of the whole universe. Carnap requires the following semantic properties:\nCarnap distinguishes three kinds of properties:\nTo illuminate this taxonomy, let \"x\" be a variable and \"a\" a constant symbol; then an example of 1. could be \"\"x\" is blue or \"x\" is non-warm\", an example of 2. \"\"x\" = \"a\", and an example of 3. \"x\" is red and not \"x\" = \"a\"\".\n\nBased on his theory of inductive logic sketched above, Carnap formalizes Goodman's notion of projectibility of a property \"W\" as follows: the higher the relative frequency of \"W\" in an observed sample, the higher is the probability that a non-observed individual has the property \"W\". Carnap suggests \"as a tentative answer\" to Goodman, that all purely qualitative properties are projectible, all purely positional properties are non-projectible, and mixed properties require further investigation.\n\nWillard Van Orman Quine discusses an approach to consider only \"natural kinds\" as projectible predicates.\nHe first relates Goodman's grue paradox to Hempel's raven paradox by defining two predicates \"F\" and \"G\" to be (simultaneously) projectible if all their shared instances count toward confirmation of the claim \"each \"F\" is a \"G\"\". Then Hempel's paradox just shows that the complements of projectible predicates (such as \"is a raven\", and \"is black\") need not be projectible, while Goodman's paradox shows that \"is green\" is projectible, but \"is grue\" is not.\n\nNext, Quine reduces projectibility to the subjective notion of \"similarity\". Two green emeralds are usually considered more similar than two grue ones if only one of them is green. Observing a green emerald makes us expect a similar observation (i.e., a green emerald) next time. Green emeralds are a \"natural kind\", but grue emeralds are not. Quine investigates \"the dubious scientific standing of a general notion of similarity, or of kind\". Both are basic to thought and language, like the logical notions of e.g. identity, negation, disjunction. However, it remains unclear how to relate the logical notions to \"similarity\" or \"kind\"; Quine therefore tries to relate at least the latter two notions to each other.\n\nRelation between similarity and kind\n\nAssuming finitely many \"kinds\" only, the notion of \"similarity\" can be defined by that of \"kind\": an object \"A\" is more similar to \"B\" than to \"C\" if \"A\" and \"B\" belong jointly to more kinds than \"A\" and \"C\" do.\n\nVice versa, it remains again unclear how to define \"kind\" by \"similarity\". Defining e.g. the kind of red things as the set of all things that are more similar to a fixed \"paradigmatical\" red object than this is to another fixed \"foil\" non-red object (cf. left picture) isn't satisfactory, since the degree of overall similarity, including e.g. shape, weight, will afford little evidence of degree of redness. (In the picture, the yellow paprika might be considered more similar to the red one than the orange.)\n\nAn alternative approach inspired by Carnap defines a natural kind to be a set whose members are more similar to each other than each non-member is to at least one member. \nHowever, Goodman argued, that this definition would make the set of all red round things, red wooden things, and round wooden things (cf. right picture) meet the proposed definition of a natural kind, while \"surely it is not what anyone means by a kind\".\n\nWhile neither of the notions of similarity and kind can be defined by the other, they at least vary together: if \"A\" is reassessed to be more similar to \"C\" than to \"B\" rather than the other way around, the assignment of \"A\", \"B\", \"C\" to kinds will be permuted correspondingly; and conversely.\n\nBasic importance of similarity and kind\n\nIn language, every general term owes its generality to some resemblance of the things referred to. Learning to use a word depends on a double resemblance, viz. between the present and past circumstances in which the word was used, and between the present and past phonetic utterances of the word.\n\nEvery reasonable expectation depends on resemblance of circumstances, together with our tendency to expect similar causes to have similar effects. This includes any scientific experiment, since it can be reproduced only under similar, but not under completely identical, circumstances. Already Heraclitus' famous saying \"No man ever steps in the same river twice\" highlighted the distinction between similar and identical circumstances.\n\nGenesis of similarity and kind\n\nIn a behavioral sense, humans and other animals have an innate standard of similarity. It is part of our animal birthright, and characteristically animal in its lack of intellectual status, e.g. its alieness to mathematics and logic, cf. bird example.\n\nInduction itself is essentially animal expectation or habit formation.\nOstensive learning\nis a case of induction, and a curiously comfortable one, since each man's spacing of qualities and kind is enough like his neighbor's.\nIn contrast, the \"brute irrationality of our sense of similarity\" offers little reason to expect it being somehow in tune with the unanimated nature, which we never made.\nWhy inductively obtained theories about it should be trusted is the perennial philosophical problem of induction. Quine, following Watanabe,\nsuggests Darwin's theory as an explanation: if people's innate spacing of qualities is a gene-linked trait, then the spacing that has made for the most successful inductions will have tended to predominate through natural selection.\nHowever, this cannot account for the human ability to dynamically refine one's spacing of qualities in the course of getting acquainted with a new area.\n\nIn his book \"Wittgenstein on Rules and Private Language\", Saul Kripke proposed a related argument that leads to skepticism about meaning rather than skepticism about induction, as part of his personal interpretation (nicknamed \"Kripkenstein\" by some) of the private language argument. He proposed a new form of addition, which he called \"quus\", which is identical with \"+\" in all cases except those in which either of the numbers added are equal to or greater than 57; in which case the answer would be 5, i.e.:\n\nHe then asks how, given certain obvious circumstances, anyone could know that previously when I thought I had meant \"+\", I had not actually meant \"quus\". Kripke then argues for an interpretation of Wittgenstein as holding that the meanings of words are not individually contained mental entities.\n\n\n"}
{"id": "2903306", "url": "https://en.wikipedia.org/wiki?curid=2903306", "title": "Paper negative", "text": "Paper negative\n\nThe paper negative process consists of using a negative printed on paper (either photographically or digitally) to create the final print of a photograph, as opposed to using a modern negative on a film base of cellulose acetate. The plastic acetate negative (which is what modern films produce) enables the printing of a very sharp image intended to be as close a representation of the actual subject as is possible. By using a negative based on paper instead, there is the possibility of creating a more ethereal image, simply by using a paper with a very visible grain, or by drawing on the paper or distressing it in some way. \n\nOne of the original forms of photography was based on the paper negative process. William Henry Fox Talbot's paper negative process, which was used to create his work \"The Pencil of Nature\", used a negative created on paper treated with silver salts, which was exposed in a camera obscura to create the negative and then contact printed on a similar paper to produce a positive image. \n\nWhen Talbot created this process it was intended to be a way to reproduce nature as accurately as possible (hence the name of his work, \"The Pencil of Nature\"). Through the years afterwards, however, better and more accurate ways of producing exact replicas of nature were developed, and these processes relegated the paper negative process to obsolescence. \n\nThe process of the paper negative is still relevant, though, in the realm of alternative-process photography. Photographers employing alternative processes reject the idea of the exact replica of nature and seek to use the inherent inexactness of antiquated processes to create a more personal and emotional image. The paper negative is an extremely versatile process that allows all manner of reworking and retouching of an image, and is the perfect medium to bridge the gap between camera operator and artist.\n\n\n\n"}
{"id": "39098", "url": "https://en.wikipedia.org/wiki?curid=39098", "title": "Physical law", "text": "Physical law\n\nA physical law or a law of physics is a statement \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present.\" Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors.\n\nThe distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from \"physis\", the Greek word (translated into Latin as \"natura\") for \"nature\".\n\nSeveral general properties of physical laws have been identified. Physical laws are:\n\n\nSome of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his \"Philosophiae Naturalis Principia Mathematica\", and in Albert Einstein's theory of relativity. Other examples of laws of nature include Boyle's law of gases, conservation laws, the four laws of thermodynamics, etc.\n\nMany scientific laws are couched in mathematical terms (e.g. Newton's Second law \"F\" = , or the uncertainty principle, or the principle of least action, or causality). While these scientific laws explain what our senses perceive, they are still empirical, and so are not \"mathematical\" laws. (Mathematical laws can be proved purely by mathematics and not by scientific experiment.)\n\nOther laws reflect mathematical symmetries found in Nature (say, Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, Lorentz transformations reflect rotational symmetry of space–time). Laws are constantly being checked experimentally to higher and higher degrees of precision. This is one of the main goals of science. Just because laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed.\n\nWell-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies can be said to generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations (see below), to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.\n\nMany fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.\n\nThe inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.\n\nOne strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.\n\nSome laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.\n\nCompared to pre-modern accounts of causality, laws of nature fill the role played by divine causality on the one hand, and accounts such as Plato's theory of forms on the other.\n\nThe observation that there are underlying regularities in nature dates from prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws \"per se\", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.\n\nIn Europe, systematic theorizing about nature (\"physis\") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.\n\nFor the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's \"Natural Questions\", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.\n\nThe precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton were influenced by a religious view which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of \"The World\", René Descartes described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\" The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).\n\nSome mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.\n\nExamples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.\n\n\n\n"}
{"id": "36606973", "url": "https://en.wikipedia.org/wiki?curid=36606973", "title": "Plastic Principle", "text": "Plastic Principle\n\nThe Plastic Principle is an idea introduced into Western thought by the English philosopher Ralph Cudworth (1617–1689) to explain the function of nature and life in the face of both the mechanism and materialism of the Enlightenment. It is a dynamic functional power that contains all of natural law, and is both sustentative and generative, organizing matter according to Platonic Ideas, that is, archetypes that lie beyond the physical realm coming from the Mind of God or Deity, the ground of Being.\n\nThe role of nature was one faced by philosophers in the Age of Reason or Enlightenment. The prevailing view was either that of the Church of a personal deity intervening in his creation, producing miracles, or an ancient pantheism (atheism relative to theism) – deity pervading all things and existing in all things. However, the \"ideas of an all-embracing providential care of the world and of one universal vital force capable of organizing the world from within.\" presented difficulties for philosophers of a spiritual as well as materialistic bent.\n\nThe Cartesian idea of nature as mechanical, and Hobbes' materialistic views were countered by the English philosopher, Ralph Cudworth (1617–1689), who, in his \"True intellectual system of the universe\" (1678), addressing the tension between theism and atheism, took both the Stoic idea of Divine Reason poured into the world, and the Platonic idea of the world soul (\"anima mundi\") to posit a power that was polaric – \"either as a ruling but separate mind or as an informing vital principle – either nous hypercosmios or nous enkosmios.\n\nCudworth was a member of the Cambridge Platonists, a group of English seventeenth-century thinkers associated with the University of Cambridge who were stimulated by Plato's teachings but also were aware or and influenced by Descartes, Hobbes, Bacon, Boyle and Spinoza. The other important philosopher of this group was Henry More (1614–1687). More held that spiritual substance or mind controlled inert matter. Out of his correspondence with Descartes, he developed the idea that everything, whether material or non, had extension, an example of the latter being space, which is infinite (Newton) and which then is correlative to the idea of God (set out in his Enchiridion metaphysicum 1667). In developing this idea, More also introduced a causal agent between God and substance, or Nature in his Hylarchic Principle, derived from Plato's \"anima mundi\" or world soul, and the Stoic's pneuma, which encapsulates the laws of nature, both for inert and vital nature, and involves a sympathetic resonance between soul (\"psyche\") and \"soma\".\n\nLike More, Cudworth put forward the idea of 'the Plastick Life of Nature', a formative principle that contains both substance and the laws of motion, as well as a nisus or direction that accounts for design and goal in the natural world. He was stimulated by the Cartesian idea of the mind as self-consciousness to see God as consciousness. He first analysed four forms of atheism from ancient times to present, and showed that all misunderstood the principle of life and knowledge, which involved unsentient activity and self-consciousness.\n\nAll of the atheistic approaches posted nature as unconscious, which for Cudworth was ontologically unsupportable, as a principle that was supposed to be the ultimate source of life and meaning could only be itself self-conscious and knowledgeable, that is, rational, otherwise creation or nature degenerates into inert matter set in motion by random external forces (Coleridge's 'chance whirlings of unproductive particles'). Cudworth saw nature as a vegetative power endowed with plastic (forming) and spermatic (generative) forces, but one with Mind, or a self-conscious knowledge. This idea would later emerge in the Romantic period in German science as Blumenbach's \"Bildungstreib\" (generative power) and the \"Lebenskraft\" (or \"Bildungskraft\").\n\nThe essence of atheism for Cudworth was the view that matter was self-active and self-sufficient, whereas for Cudworth the plastic power was unsentient and under the direct control of the universal Mind or \"Logos\". For him atheism, whether mechanical or material could not solve the \"phenomenon of nature.\" Henry More argued that atheism made each substance independent and self-acting such that it 'deified' matter. Cudworth argued that materialism/mechanism reduced \"substance to a corporeal entity, its activity to causal determinism, and each single thing to fleeting appearances in a system dominated by material necessity.\"\n\nCudworth had the idea of a general plastic nature of the world, containing natural laws to keep all of nature, inert and vital in orderly motion, and particular plastic natures in particular entities, which serve as 'Inward Principles' of growth and motion, but ascribes it to the Platonic tradition:\nFurther, Cudsworth's plastic principle was also a functional polarity. As he wrote:\n\nAs another historian notes in conclusion, \"Cudworth’s theory of plastic natures is offered as an alternative to the interpretation of all of nature as either governed by blind chance, or, on his understanding of the Malebranchean view, as micro-managed by God.\"\n\nCudworth's plastic principle also involves a theory of mind that is active, that is, God or the Supreme Mind is \"the spermatic reason\" which gives rise to individual mind and reason. Human mind can also create, and has access to spiritual or super-sensible 'Ideas' in the Platonic sense. Cudworth challenged Hobbesian determinism in arguing that will is not distinct from reason, but a power to act that is internal, and therefore, the voluntary will function involves self-determination, not external compulsion, though we have the power to act either in accordance with God's will or not. Cudworth's 'hegemonikon' (taken from Stoicism) is a function within the soul that combines the higher functions of the soul (voluntary will and reason) on the one hand with the lower animal functions (instinct), and also constitutes the whole person, thus bridging the Cartesian dualism of body and soul or \"psyche\" and \"soma\". This idea provided the basis for a concept of self-awareness and identity of an individual that is self-directed and autonomous, an idea that anticipates John Locke.\n\nLocke examined how man came to knowledge via stimulus (rather than seeing ideas as inherent), which approach led to his idea of the 'thinking' mind, which is both receptive and pro-active. The first involves receiving sensations ('simple ideas') and the second by reflection – \"observation of its own inner operations\" (inner sense which leads to complex ideas), with the second activity acting upon the first. Thought is set in motion by outer stimuli which 'simple ideas' are taken up by the mind's self-activity, an \"active power\" such that the outer world can only be real-ized as action (natural cause) by the activity of consciousness. Locke also took the issue of life as lying not in substance but in the capacity of the self for consciousness, to be able to organize (associate) disparate events, that is to participate life by means of the sense experiences, which have the capacity to produce every kind of experience in consciousness. These ideas of Locke were taken over by Fichte and influenced German Romantic science and medicine. (See Romantic medicine and Brunonian system of medicine).\nThomas Reid and his \"Common Sense\" philosophy, was also influenced by Cudworth, taking his influence into the Scottish Enlightenment.\n\nBerkeley later developed the idea of a plastic life principle with his idea of an 'aether' or 'aetherial medium' that causes 'vibrations' that animate all living beings. For Berkeley, it is the very nature of this medium that generates the 'attractions' of entities to each other.\n\nBerkeley meant this 'aether' to supplant Newton's gravity as the cause of motion (neither seeing the polarity involved between two forces, as Cudworth had in his plastic principle). However, in Berkeley's conception, aether is both the movement of spirit and the motion of nature.\n\nBoth Cudworth's views and those of Berkeley were taken up by Coleridge in his metaphor of the eolian harp in his 'Effusion XXXV' as one commentator noted: \"what we see in the first manuscript is the articulation of Cudworth’s principle of plastic nature, which is then transformed in the published version into a Berkeleyan expression of the causal agency of motion performed by God’s immanent activity.\"\n\nCudworth's idea of the plastic principle and that of mind will also be taken up in a new way in the idea of emergent evolution.\n"}
{"id": "4595966", "url": "https://en.wikipedia.org/wiki?curid=4595966", "title": "Pollyanna principle", "text": "Pollyanna principle\n\nThe Pollyanna principle (also called Pollyannaism or positivity bias) is the tendency for people to remember pleasant items more accurately than unpleasant ones. Research indicates that at the subconscious level, the mind has a tendency to focus on the optimistic; while at the conscious level, it has a tendency to focus on the negative. This subconscious bias towards the positive is often described as the Pollyanna principle and is similar to the Forer effect.\n\nThe name derives from the 1913 novel \"Pollyanna\" by Eleanor H. Porter describing a girl who plays the \"glad game\"—trying to find something to be glad about in every situation. The novel has been adapted to film several times, most famously in 1920 and 1960. An early use of the name \"Pollyanna\" in psychological literature was in 1969 by Boucher and Osgood who described a \"Pollyanna hypothesis\" as a universal human tendency to use evaluatively positive words more frequently and diversely than evaluatively negative words in communicating. Empirical evidence for this tendency has been provided by computational analyses of large corpora of text.\n\nThe \"Pollyanna principle\" was described by Matlin and Stang in 1978 using the archetype of Pollyanna more specifically as a psychological principle which portrays the positive bias people have when thinking of the past. According to the Pollyanna Principle, the brain processes information that is pleasing and agreeable in a more precise and exact manner as compared to unpleasant information. We actually tend to remember past experiences as more rosy than they actually occurred.\n\nResearchers Margaret Matlin and David Stang provided substantial evidence of the Pollyanna Principle. They found that people expose themselves to positive stimuli and avoid negative stimuli, they take longer to recognize what is unpleasant or threatening than what is pleasant and safe, and they report that they encounter positive stimuli more frequently than they actually do. Matlin and Stang also determined that selective recall was a more likely occurrence when recall was delayed: the longer the delay, the more selective recall that occurred.\n\nThe Pollyanna principle has been observed on online social networks as well. For example, Twitter users preferentially share more, and are emotionally affected more frequently by, positive information.\n\nHowever, the Pollyanna principle does not always apply to individuals suffering from depression or anxiety, who tend to either have more depressive realism or a negative bias.\n\n\n"}
{"id": "831689", "url": "https://en.wikipedia.org/wiki?curid=831689", "title": "Pontryagin's maximum principle", "text": "Pontryagin's maximum principle\n\nPontryagin's maximum (or minimum) principle is used in optimal control theory to find the best possible control for taking a dynamical system from one state to another, especially in the presence of constraints for the state or input controls. It was formulated in 1956 by the Russian mathematician Lev Pontryagin and his students. It has as a special case the Euler–Lagrange equation of the calculus of variations.\n\nThe principle states, informally, that the \"control Hamiltonian\" must take an extreme value over controls in the set of all permissible controls. Whether the extreme value is maximum or minimum depends both on the problem and on the sign convention used for defining the Hamiltonian. The normal convention, which is the one used in Hamiltonian, leads to a maximum hence \"maximum principle\" but the sign convention used in this article makes the extreme value a minimum.\n\nIf formula_1 is the set of values of permissible controls then the principle states that the optimal control formula_2 must satisfy:\nwhere formula_4 is the optimal state trajectory and formula_5 is the optimal costate trajectory.\n\nThe result was first successfully applied to minimum time problems where the input control is constrained, but it can also be useful in studying state-constrained problems.\n\nSpecial conditions for the Hamiltonian can also be derived. When the final time formula_6 is fixed and the Hamiltonian does not depend explicitly on time formula_7, then:\nand if the final time is free, then:\nMore general conditions on the optimal control are given below.\n\nWhen satisfied along a trajectory, Pontryagin's minimum principle is a necessary condition for an optimum. The Hamilton–Jacobi–Bellman equation provides a necessary and sufficient condition for an optimum, but this condition must be satisfied over the whole of the state space.\n\nWhile the Hamilton-Jacobi-Bellman equation admits a straightforward extension to stochastic optimal control problems, the minimum principle does not.\n\nThe principle was first known as \"Pontryagin's maximum principle\" and its proof is historically based on maximizing the Hamiltonian. The initial application of this principle was to the maximization of the terminal speed of a rocket. However, as it was subsequently mostly used for minimization of a performance index it has here been referred to as the \"minimum principle\". Pontryagin's book solved the problem of minimizing a performance index.\n\nIn what follows we will be making use of the following notation.\n\nHere the necessary conditions are shown for minimization of a functional. Take formula_15 to be the state of the dynamical system with input formula_16, such that\nwhere formula_1 is the set of admissible controls and formula_19 is the terminal (i.e., final) time of the system. The control formula_20 must be chosen for all formula_21 to minimize the objective functional formula_22 which is defined by the application and can be abstracted as\n\nThe constraints on the system dynamics can be adjoined to the Lagrangian formula_24 by introducing time-varying Lagrange multiplier vector formula_25, whose elements are called the costates of the system. This motivates the construction of the Hamiltonian formula_26 defined for all formula_21 by:\nwhere formula_29 is the transpose of formula_25.\n\nPontryagin's minimum principle states that the optimal state trajectory formula_31, optimal control formula_2, and corresponding Lagrange multiplier vector formula_33 must minimize the Hamiltonian formula_26 so that\n\nfor all time formula_21 and for all permissible control inputs formula_20. It must also be the case that\n\nAdditionally, the costate equations\n\nmust be satisfied. If the final state formula_40 is not fixed (i.e., its differential variation is not zero), it must also be that the terminal costates are such that\n\nThese four conditions in (1)-(4) are the necessary conditions for an optimal control. Note that (4) only applies when formula_40 is free. If it is fixed, then this condition is not necessary for an optimum.\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "1875925", "url": "https://en.wikipedia.org/wiki?curid=1875925", "title": "Pythagorean Method of Memorization", "text": "Pythagorean Method of Memorization\n\nPythagorean Method of Memorization (PYMOM), also known as Triangular Movement Cycle (TMC), is a game-based, educational methodology or associative-learning technique that primarily uses corresponding information, such as terms and definitions on opposing sides, displayed on cue cards, to exploit psychological retention of information for academic study and language acquisition. PYMOM is named such because of the shape the cue-cards form during the progression of the game, a right-angled or Pythagorean triangle.\n\nIt is a theoretical educational method that is made up of several established and tested educational methods that have been in use for decades.\n\nPYMOM is a composite body of techniques that claims, in its digital form, to incorporate (to a greater or lesser degree): spaced repetition, non-failure redundant subroutine, chromatics, positive reinforcement, the Von Restorff effect, picture association, selective musical tonality, kinesthetics, the serial-position effect and meditation. There are two branches of this methodology:\n\nAs with both branches, there is only one variable in the game or learning method: a correct or incorrect answer. The initial movement cycle also remains largely unchanged.\n\nThe movement cycle which is most crucial to the methodology and reinforces the spaced repetition, begins with either 3, 4 or 5 cards; 3 cards for a 6-card session, 4 cards for a 10-card session and 5 cards for the most advanced 15-card session. Because two-sided associative cue-cards are being used, all cards are presented with a congruent side up, either all \"terms\" or \"definitions,\" not mixed.\n\nOnce cards have been answered correctly, the predominant row has reached its maximum and a card must be graduated out of this row to continue the game. Thus the card to the far right comes into play. Routines are repeated as each row reaches its maximum. A cue-card is finally eliminated from the game session by being answered correctly once more, after it has graduated to the top tier or row.\n\nThe first manifestation, referred to as the \"Triangular Movement Cycle\" or TMC, was a simple paper-based learning technique that was primarily a manual movement cycle using physical cue-cards, which allowed for manual-spaced repetition to elicit psychological retention of information. Its origins, however, are not very clear. Using TMC, teachers would move the cards for the student in a one-on-one setting according to either the correct or incorrect feedback from the student.This presented challenges for the teacher or tutor using this method. The first challenge lies in the fact that, although TMC lent itself well to a two-party learning group (i.e. teacher & student), it could also be done by the student themselves on their own. It was a very easy system to utilize once learned, however, it was found exceptionally difficult to teach the complex movement cycle and principles behind such to students, especially where a linguistic barrier was present. The second challenge lay in that the educator needed to create and remember innumerable cue-cards or create custom master lists in order to know the correct answers — and properly guide the student, thus progressing or digressing the card in play. TMC often failed to keep the attention of many students owing to the fact that cards were not very visually appealing. To make them so required tremendous effort — and was very time consuming.\n\nThe term \"Pythagorean Method of Memorization\" was coined in 2013 and officially copyrighted in October 2014 by a Canadian company named You Learn Educational Solutions & Linguistics Inc. PYMOM takes the movement cycle from TMC and remedied the challenge of teaching the movement cycle itself to students by providing a software-based solution to handling cycles by means of sub-routines prompted by the user’s input.\n\nPYMOM wove established educational theory into the fabric of TMC to create a viable educational platform for academic and linguistic study by several means. Because spaced repetition is intrinsically part of the movement-cycle subroutines, it adds to the content and surrounding experience making it into a platform. The developers of PYMOM describe it as an “organic learning experience.” The tenets that truly allow a learning system to be a PYMOM-based system are enumerated thusly: The Von Restorff effect: for example, where it features a language, this method is employed to further aid in memory retention of the highlighted word in the phrase.\n\n"}
{"id": "11153041", "url": "https://en.wikipedia.org/wiki?curid=11153041", "title": "Saint-Venant's principle", "text": "Saint-Venant's principle\n\nSaint-Venant's principle, named after Adhémar Jean Claude Barré de Saint-Venant, a French elasticity theorist, may be expressed as follows:\n\nThe original statement was published in French by Saint-Venant in 1855. Although this informal statement of the principle is well known among structural and mechanical engineers, more recent mathematical literature gives a rigorous interpretation in the context of partial differential equations. An early such interpretation was made by von Mises in 1945.\n\nThe Saint-Venant's principle allows elasticians to replace complicated stress distributions or weak boundary conditions with ones that are easier to solve, as long as that boundary is geometrically short. Quite analogous to the electrostatics, where the electric field due to the \"i\"-th moment of the load (with 0th being the net charge, 1st the dipole, 2nd the quadrupole) decays as formula_1 over space, Saint-Venant's principle states that high order momentum of mechanical load (moment with order higher than torque) decays so fast that they never need to be considered for regions far from the short boundary. Therefore, the Saint-Venant's principle can be regarded as a statement on the asymptotic behavior of the Green's function by a point-load.\n\n"}
{"id": "21647661", "url": "https://en.wikipedia.org/wiki?curid=21647661", "title": "Self model", "text": "Self model\n\nThe self-model is the central concept in the theory of consciousness called the self-model theory of subjectivity (SMT). This concept comprises experiences of ownership, of first person perspective, and of a long-term unity of beliefs and attitudes. These features are instantiated in the prefrontal cortex. This theory is an interdisciplinary approach to understanding and explaining the phenomenology of consciousness and the self. This theory has two core contents, the phenomenal self-model (PSM) and the phenomenal model of the intentionality relation (PMIR). Thomas Metzinger advanced the theory in his 1993 book \"Subjekt und Selbstmodell\" (Subject and self-model).\n\nThe PSM is an entity that “actually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain”. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, “a higher order property of particular forms of phenomenal content,” or the idea of ownership. The second is perspectivalness, which is “a global, structural property of phenomenal space as a whole”. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is “the phenomenal target property” or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the “existence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject”. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls naïve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are “transparent” so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a “conscious mental model, and its content is an ongoing, episodic subject-object relation”. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.\n\nThe prefrontal cortex is implicated in all the functions of the human self model. The following functions all require communication with the prefrontal cortex; agency and association areas of the cortex; spatial perspectivity and the parietal lobes, unity and the temporal lobes.\n\nDisorders of the self model are implicated in several disorders including schizophrenia, autism, and depersonalization. According to this theory, long-term unity is impaired in autism, similar to theory of mind deficits and weak central coherence theory. Individuals with autism are thought to be impaired in assigning mental states to other people, an ability that probably codevelops with long-term unity of self. Weak central coherence, that is, the inability to assemble information into a cohesive whole, reflects the same problems with creating a unified sense of self and benific sense extreme in narcissism.\n\n"}
{"id": "246976", "url": "https://en.wikipedia.org/wiki?curid=246976", "title": "Square of opposition", "text": "Square of opposition\n\nThe square of opposition is a diagram representing the relations between the four basic categorical propositions.\nThe origin of the square can be traced back to Aristotle making the distinction between two oppositions: contradiction and contrariety.\nBut Aristotle did not draw any diagram. This was done several centuries later by Apuleius and Boethius.\n\nIn traditional logic, a proposition (Latin: \"propositio\") is a spoken assertion (\"oratio enunciativa\"), not the meaning of an assertion, as in modern philosophy of language and logic. A \"categorical proposition\" is a simple proposition containing two terms, subject and predicate, in which the predicate is either asserted or denied of the subject.\n\nEvery categorical proposition can be reduced to one of four logical forms. These are:\n\nIn tabular form:\n\n\nAristotle states (in chapters six and seven of the \"Peri hermaneias\" (Περὶ Ἑρμηνείας, Latin \"De Interpretatione\", English 'On Interpretation')), that there are certain logical relationships between these four kinds of proposition. He says that to every affirmation there corresponds exactly one negation, and that every affirmation and its negation are 'opposed' such that always one of them must be true, and the other false. A pair of affirmative and negative statements he calls a 'contradiction' (in medieval Latin, \"contradictio\"). Examples of contradictories are 'every man is white' and 'not every man is white' (also read as 'some men are not white'), 'no man is white' and 'some man is white'.\n\n'Contrary' (medieval: \"contrariae\") statements, are such that both cannot at the same time be true. Examples of these are the universal affirmative 'every man is white', and the universal negative 'no man is white'. These cannot be true at the same time. However, these are not contradictories because both of them may be false. For example, it is false that every man is white, since some men are not white. Yet it is also false that no man is white, since there are some white men.\n\nSince every statement has a contradictory opposite, and since a contradictory is true when its opposite is false, it follows that the opposites of contraries (which the medievals called subcontraries, \"subcontrariae\") can both be true, but they cannot both be false. Since subcontraries are negations of universal statements, they were called 'particular' statements by the medieval logicians.\n\nAnother logical opposition implied by this, though not mentioned explicitly by Aristotle, is 'alternation' (\"alternatio\"), consisting of 'subalternation' and 'superalternation'. Alternation is a relation between a particular statement and a universal statement of the same quality such that the particular is implied by the other. The particular is the subaltern of the universal, which is the particular's superaltern. For example, if 'every man is white' is true, its contrary 'no man is white' is false. Therefore, the contradictory 'some man is white' is true. Similarly the universal 'no man is white' implies the particular 'not every man is white'.\n\nIn summary:\n\nThese relationships became the basis of a diagram originating with Boethius and used by medieval logicians to classify the logical relationships. The propositions are placed in the four corners of a square, and the relations represented as lines drawn between them, whence the name 'The Square of Opposition'.\n\nSubcontraries, which medieval logicians represented in the form 'quoddam A est B' (some particular A is B) and 'quoddam A non est B' (some particular A is not B) cannot both be false, since their universal contradictory statements (every A is B / no A is B) cannot both be true. This leads to a difficulty that was first identified by Peter Abelard. 'Some A is B' seems to imply 'something is A'. For example, 'Some man is white' seems to imply that at least one thing is a man, namely the man who has to be white, if 'some man is white' is true. But, 'some man is not white' also implies that something is a man, namely the man who is not white, if the statement 'some man is not white' is true. But Aristotelian logic requires that necessarily one of these statements is true. Both cannot be false. Therefore, (since both imply that something is a man) it follows that necessarily something is a man, i.e. men exist. But (as Abelard points out, in the Dialectica) surely men might not exist?\n\nAbelard also points out that subcontraries containing subject terms denoting nothing, such as 'a man who is a stone', are both false.\n\nTerence Parsons argues that ancient philosophers did not experience the problem of existential import as only the A and I forms had existential import.\n\nHe goes on to cite medieval philosopher William of Moerbeke\n\nAnd points to Boethius' translation of Aristotle's work as giving rise to the mistaken notion that the O form has existential import.\n\nIn the 19th century, George Boole argued for requiring existential import on both terms in particular claims (I and O), but allowing all terms of universal claims (A and E) to lack existential import. This decision made Venn diagrams particularly easy to use for term logic. The square of opposition, under this Boolean set of assumptions, is often called the modern Square of opposition. In the modern square of opposition, A and O claims are contradictories, as are E and I, but all other forms of opposition cease to hold; there are no contraries, subcontraries, or subalterns. Thus, from a modern point of view, it often makes sense to talk about 'the' opposition of a claim, rather than insisting as older logicians did that a claim has several different opposites, which are in different kinds of opposition with the claim.\n\nGottlob Frege's \"Begriffsschrift\" also presents a square of oppositions, organised in an almost identical manner to the classical square, showing the contradictories, subalternates and contraries between four formulae constructed from universal quantification, negation and implication.\n\nAlgirdas Julien Greimas' semiotic square was derived from Aristotle's work.\n\nThe traditional square of opposition is now often compared with squares based on inner- and outer-negation \n\nThe square of opposition has been extended to a logical hexagon which includes the relationships of six statements. It was discovered independently by both Augustin Sesmat and Robert Blanché. It has been proven that both the square and the hexagon, followed by a \"logical cube\", belong to a regular series of n-dimensional objects called \"logical bi-simplexes of dimension n.\" The pattern also goes even beyond this.\n\nThe logical square, also called square of opposition or square of Apuleius has its origin in the four marked sentences to be employed in syllogistic reasoning: Every man is bad, the universal affirmative and its negation Not every man is bad (or Some men are not bad), the particular negative on the one hand, Some men are bad, the particular affirmative and its negation No man is bad, the universal negative on the other. Robert Blanché published with Vrin his Structures intellectuelles in 1966 and since then many scholars think that the logical square or square of opposition representing four values should be replaced by the logical hexagon which by representing six values is a more potent figure because it has the power to explain more things about logic and natural language.\n\n\n"}
{"id": "40592503", "url": "https://en.wikipedia.org/wiki?curid=40592503", "title": "Sure-thing principle", "text": "Sure-thing principle\n\nIn decision theory, the sure-thing principle states that a decision maker who would take a certain action if he knew that event \"E\" has occurred, and also if he knew that the negation of \"E\" has occurred, should also take that same action if he knows nothing about \"E\".\n\nThe principle was coined by L.J. Savage:\nHe formulated the principle as a dominance principle, but it can also be framed probabilistically. Jeffrey and later Pearl showed that Savage's principle is only valid when the probability of the event considered (e.g., the winner of the election) is unaffected by the action (buying the property). Under such conditions, the sure-thing principle is a theorem in the \"do\"-calculus (see Bayes networks). Blyth constructed a counterexample to the sure-thing principle using sequential sampling in the context of Simpson's paradox, but this example violates the required action-independence provision.\n\nThe principle is closely related to independence of irrelevant alternatives, and equivalent under the axiom of truth (everything the agent knows is true). It is similarly targeted by the Ellsberg and Allais paradoxes, in which actual people's choices seem to violate this principle. \n"}
{"id": "49605051", "url": "https://en.wikipedia.org/wiki?curid=49605051", "title": "The Circumplex Model of Group Tasks", "text": "The Circumplex Model of Group Tasks\n\nThe Circumplex Model is a graphical representation of emotional states. Fundamentally, it is a circle with pleasant on the left, unpleasant on the right, activation on the top, and deactivation on the bottom. All the other emotions are placed around the circle as combinations of these four basic states. It is based on the theory that people experience emotions as overlapping and ambiguous. Group dynamics are the distinctive behaviors and attitudes observed by people in groups, and the study thereof. It is of most interest in the business world, the workforce, or any other setting where the performance of a group is important. Joseph E McGrath enlarged the circumplex model to include group dynamics, based on the work of Shaw, Carter, Hackman, Steiner, Shiflett, Taylor, Lorge, Davis, Laughlin, and others. There are four quadrants in this model representing: generating a task, choosing correct procedure, conflict resolution, and execution, and again there are subtypes distributed around the circle. He used this model as a research tool to evaluate group task performance.\n\nGroup dynamics involve the influential actions, processes and changes that exist both within and between groups. Group dynamics also involve the scientific study of group processes. Through extensive research in the field of group dynamics, it is now well known that all groups, despite their innumerable differences, possess common properties and dynamics. Social psychological researchers have attempted to organize these commonalities, in order to further understand the genuine nature of group processes.\n\nFor instance, social psychological research indicates that there are numerous goal-related interactions and activities that groups of all sizes undertake . These interactions have been categorized by Robert F. Bales, who spent his entire life attempting to find an answer to the question, \"What do people do when they are in groups?\". To simplify the understanding of group interactions, Bales concluded that all interactions within groups could be categorized as either a \"relationship interaction\" (or socioemotional interaction) or a \"task interaction\".\n\nJust as Bales was determined to identify the basic types of interactions involved in groups, Joseph E. McGrath was determined to identify the various goal-related activities that are regularly displayed by groups. McGrath contributed greatly to the understanding of group dynamics through the development of his circumplex model of group tasks. As intended, McGrath's model effectively organizes all group-related activities by distinguishing between four basic group goals. These goals are referred to as the circumplex model of group task's four quadrants, which are categorized based on the dominant performance process involved in a group's task of interest.\n\nThe four quadrants are as follows: \n\nTo further differentiate the various goal-related group activities, McGrath further sub-divides these four categories, resulting in eight categories in total. The breakdown of these categories is as follows:\n\n1. \"Generating ideas or plans\"\n2. \"Choosing a solution\"\n3. \"Negotiating a solution to a conflict\" \n4. \"Executing a task\" \n\nAccording to McGrath and Kravitz (1982), the four most commonly represented tasks in the group dynamics literature are intellective tasks, decision-making tasks, cognitive conflict tasks and mixed-motive tasks.\n\nThe circumplex model of group tasks takes the organization of goal-related activities a step further by distinguishing between tasks that involve cooperation between group members, cooperation tasks (Types 1, 2, 3 and 8) and tasks that often lead to conflict between group members, conflict tasks (Types 4, 5, 6 and 7). Additionally, McGrath's circumplex model of group tasks also distinguishes between tasks that require action (behavioural tasks) and tasks that require conceptual review (conceptual tasks). 'Behavioural tasks' include Types 1, 6, 7 and 8, while 'conceptual tasks' include Types 2, 3, 4 and 5.\n\nThe circumplex model of group tasks is, evidently, a very detailed and complex model. To allow for a more thorough understanding of its properties, a visual representation of the model has been developed. (Need a diagram of the model)\n\nSince the circumplex model of group tasks is quite detailed and complex, numerous social psychological researchers have attempted to describe the model in various ways to ensure readers obtain an optimal understanding of the model. For instance, according to Stratus and McGrath (1994), the four quadrants and the various task types with which they contain all relate to one another within a two-dimensional space. More specifically, Stratus and McGrath (1994) states that the horizontal dimension of the circumplex model of group tasks visual representation reflect the extent to which a task entails cognitive versus behavioural performance requirements. Likewise, the vertical dimension of the circumplex model of group tasks visual representation reflects the extent and form of interdependence among members.\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
