{"id": "2235265", "url": "https://en.wikipedia.org/wiki?curid=2235265", "title": "Anna Karenina principle", "text": "Anna Karenina principle\n\nThe Anna Karenina principle states that a deficiency in any one of a number of factors dooms an endeavor to failure. Consequently, a successful endeavor (subject to this principle) is one where every possible deficiency has been avoided.\n\nThe name of the principle derives from Leo Tolstoy's book \"Anna Karenina\", which begins:\n\nAll happy families are alike; each unhappy family is unhappy in its own way.In other words: in order to be happy, a family must be successful on \"each and every one\" of \"a\" \"range\" of criteria e.g.: sexual attraction, money issues, parenting, religion, in-laws. Failure on only \"one\" of these counts leads to \"un\"happiness. Thus there are more ways for a family to be unhappy than happy.\n\nIn statistics, the term \"Anna Karenina principle\" is used to describe significance tests: there are any number of ways in which a dataset may violate the null hypothesis and only one in which all the assumptions are satisfied.\n\nThe Anna Karenina principle was popularized by Jared Diamond in his book \"Guns, Germs and Steel\". Diamond uses this principle to illustrate why so few wild animals have been successfully domesticated throughout history, as a deficiency in any one of a great number of factors can render a species undomesticable. Therefore, all successfully domesticated species are not so because of a particular positive trait, but because of a lack of any number of possible negative traits. In chapter 9, six groups of reasons for failed domestication of animals are defined:\n\n\nMoore describes applications of the \"Anna Karenina principle\" in ecology:\n\nSuccessful ecological risk assessments are all alike; every unsuccessful ecological risk assessment fails in its own way. Tolstoy posited a similar analogy in his novel Anna Karenina : \"Happy families are all alike; every unhappy family is unhappy in its own way.\" By that, Tolstoy meant that for a marriage to be happy, it had to succeed in several key aspects. Failure on even one of these aspects, and the marriage is doomed . . . the Anna Karenina principle also applies to ecological risk assessments involving multiple stressors.\n\nMuch earlier, \"Aristotle\" states the same principle in the \"Nicomachean Ethics\" (Book 2):\n\nAgain, it is possible to fail in many ways (for evil belongs to the class of the unlimited, as the Pythagoreans conjectured, and good to that of the limited), while to succeed is possible only in one way (for which reason also one is easy and the other difficult – to miss the mark easy, to hit it difficult); for these reasons also, then, excess and defect are characteristic of vice, and the mean of virtue; For men are good in but one way, but bad in many.\n\nMany experiments and observations of groups of humans, animals, trees, grassy plants, stockmarket prices, and changes in the banking sector proved the modified Anna Karenina principle.\n\nBy studying the dynamics of correlation and variance in many systems facing external, or environmental, factors, we can typically, even before obvious symptoms of crisis appear, predict when one might occur, as correlation between individuals increases, and, at the same time, variance (and volatility) goes up... All well-adapted systems are alike, all non-adapted systems experience maladaptation in their own way... But in the chaos of maladaptation, there is an order. It seems, paradoxically, that as systems become more different they actually become more correlated within limits.\n\nThis effect is proved for many systems: from the adaptation of healthy people to a change in climate conditions to the analysis of fatal outcomes in oncological and cardiological clinics. The same effect is found in the stock market. The applicability of these two statistical indicators of stress, simultaneous increase of variance and correlations, for diagnosis of social stress in large groups was examined in the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years.\n\nVladimir Arnold in his book \"Catastrophe Theory\" describes \"The Principle of Fragility of Good Things\" which in a sense supplements the Principle of Anna Karenina: good systems must meet simultaneously a number of requirements; therefore, they are more fragile:\n\n... for systems belonging to the singular part of the stability boundary a small change of the parameters is more likely to send the system into the unstable region than into the stable region. This is a manifestation of a general principle stating that all good things (e.g. stability) are more fragile than bad things. It seems that in good situations a number of requirements must hold simultaneously, while to call a situation bad even one failure suffices. \n"}
{"id": "28231080", "url": "https://en.wikipedia.org/wiki?curid=28231080", "title": "Archiv für Begriffsgeschichte", "text": "Archiv für Begriffsgeschichte\n\nArchiv für Begriffsgeschichte ('Archive for Conceptual History') is a German peer-reviewed academic journal. It was founded by Erich Rothacker, and is published by Christian Bermes, Ulrich Dierse and Michael Erler. The editor is Annika Hand.\n\nThe journal publishes works on concepts of the history of philosophy and sciences, both from the European and from non-European traditions, on mythological and religious concepts, and on concepts of common parlance which have a characteristic significance for a special era or culture. The journal also embraces articles on revealing metaphors, on problems at translating concepts, as well as on theory and criticism of the method of conceptual history.\n\n\n"}
{"id": "26276222", "url": "https://en.wikipedia.org/wiki?curid=26276222", "title": "Bateman's principle", "text": "Bateman's principle\n\nBateman's principle, in evolutionary biology, is that in most species, variability in reproductive success (or reproductive variance) is greater in males than in females. It was first proposed by Angus John Bateman (1919–1996), an English geneticist. Bateman suggested that, since males are capable of producing millions of sperm cells with little effort, while females invest much higher levels of energy in order to nurture a relatively small number of eggs, the female plays a significantly larger role in their offspring's reproductive success. Bateman’s paradigm thus views females as the limiting factor of parental investment, over which males will compete in order to copulate successfully.\n\nAlthough Bateman's principle served as a cornerstone for the study of sexual selection for many decades, it has recently been subject to criticism. Attempts to reproduce Bateman's experiments in 2012 and 2013 were unable to support his conclusions. Some scientists have criticized Bateman's experimental and statistical methods, or pointed out conflicting evidence, while others have defended the veracity of the principle and cited evidence in support of it.\n\nTypically it is the females who have a relatively larger investment in producing each offspring. Bateman attributed the origin of the unequal investment to the differences in the production of gametes: sperm are cheaper than eggs. A single male can easily fertilize all females' eggs; she will not produce more offspring by mating with more than one male. A male is capable of fathering more offspring if he mates with several females. By and large, a male's potential reproductive success is limited by the number of females he mates with, whereas a female's potential reproductive success is limited by how many eggs she can produce. According to Bateman's principle, this results in sexual selection, in which males compete with each other, and females become choosy in which males to mate with. Thus, as a result of being anisogamous, males are fundamentally promiscuous, and females are fundamentally selective.\n\nBateman initially published his review in 1948. He was a botanist, contributing to the literature of sexual selection only once in his lifetime. Bateman initially saw his study on Drosophila to be a test of Charles Darwin’s doctrine of sexual selection. He saw Darwin’s theory of natural selection not as flawed, but as incomplete. He felt that if he were to provide a concrete demonstration of how sexual selection played a role in the reproductive success of certain species, he could explain the gap between Darwin’s ideas and sexual dimorphism.\n\nAlthough it is common to confuse Bateman's ideas with those of later scientists, his principle can be expressed in three simple statements. The first is that male reproductive success increases with the number of mates they attempt to copulate with, while female reproductive success does not. The second is that male reproductive success will show greater variance than female. The third is that sexual selection will have a greater effect on the sex with greater variance in reproductive success.\n\nThroughout his research, Bateman conducted experiments using fruit flies in order to observe their copulation and sexual behavior. A total of six series of experiments were conducted with the fruit fly \"Drosophila melanogaster\", using three to five individuals of each sex. Each trial ran for three or four days. Some ran to completion without the transfer of the Drosophila from one environment (bottle) to another. In the others, Bateman transferred the flies and their eggs to a new bottle every day. Bateman also varied the age of the flies depending on the experiment, with an age gap between one and six days total. He never watched the flies' copulations. The flies used were from several inbred strains, which meant they could be identified by their specific inbred strain. Therefore, he inferred the number of involved mates based on the number of offspring that were later found to have mutations from both a male and a female. The difficulty that arose was that if a female Drosophila had copulated with five males and only one larva survived, Bateman would not be able to account for the other four copulations.\n\nAnalysis of the data collected in sets one through four showed that the males' reproductive success, estimated as the number of sired offspring, increased at a steady rate until a total of three mates were reached. It is important to note that Bateman kept the sex ratio of males to females completely even throughout his trials. But after surpassing three mates, male reproductive success began to fall. Female reproductive success also increased with number of mates, but much more gradually than that of the males. The second series of data collected in sets five and six illustrated a dramatically different outcome. Male reproductive success increased at a steady and steep rate, never dropping. Female reproductive success, on the other hand, plateaued after a single mate. Bateman focused mainly on the second series of data when discussing his results. His main conclusion was that the reproductive success of females does not increase with an influx of mates, as one fit mate was enough to successfully complete fertilization. This is often referred to as Bateman’s Gradient.\n\nThroughout 2012 and 2013, Gowaty, Kim, and Anderson took it upon themselves to repeat Bateman's experiment in its entirety. While reproducing his tests, the same fly strains and mutations were used in order to maintain the same methodology. However, one of the 11 strains that Bateman used had gone extinct, and was thus replaced. (Tang-Martinez 2012)\n\nGowaty, Kim, and Anderson found that upon combining certain strains with one another, the offspring were unable to survive to adulthood. (Gowaty, Kim, & Anderson 2012) Thus, Bateman’s numbers regarding the number of individuals not having mated was higher than the actual number. Likewise, his estimate of those that mated with one or more mates was too low. This was valid for both the males and females of this species.\n\nGowaty desired to further explore the reasoning behind the premature death of the Drosophila. She began doing so by running monogamy trials between different strains of flies and found that 25% of the offspring died due to becoming double mutants. (Gowaty 2013) Bateman thought his work fit within the lines of Mendel’s laws of genetics, while Gowaty proved otherwise. The 1948 experiments inferred reproductive success based on the number of adults living by the end of the trial. In reality, many factors were left out of the equation when calculating reproductive success as a function of the number of mates, which had the ability to completely dislodge the accuracy behind Bateman's results. Gowaty was not able to confirm Bateman's conclusions and found no evidence for sexual selection in the experiment. (Gowaty 2013; Tang-Martinez 2012)\n\nNevertheless, some modern experiments between the relationship of number of mates and the reproductive success of males and females support Bateman's principle. Julie Collet conducted an experiment with a population of red jungle fowl. A total of thirteen replicate groups of three males and four females were monitored for ten days. In this experiment, the sex ratio was biased toward females. A male's reproductive success was calculated using the proportion of embryos fathered to the total number of embryos produced by all the females he mated with. The total sexual selection opportunity was calculated using the following formula.\n\nThe \"σ\" represents the variance in RS, while the is the square mean of reproductive success of members of one sex in a group.\n\nIn 2013, Fritzsche and Arnqvist tested Bateman's principle by estimating sexual selection between males and females in four seed beetles. They used a unique experimental design that showed sexual selection to be greater in males than in females. In contrast, sexual selection was also shown to be stronger for females in role-reversed species. They suggested that the Bateman gradient is typically the most accurate and informative measure of sexual selection between different sexes and species (Fritzsche, K. & Arnqvist, G).\n\nMore than 60 years later, Bateman's principle has received considerable attention. Sutherland argued that males' higher variance in reproductive success may result from random mating and coincidence. Hubbell and Johnson suggested that variance in reproductive success can be greatly influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies are subject to change depending on certain strategies. They argued that there are cases in which males can be more selective than females, whereas Bateman suggested that his paradigm would be “almost universal” among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.\n\nExperimental and statistical criticisms followed. Until approximately a decade ago, critics of Bateman’s model focused on his experimental design. In recent years, they have shifted attention to the actual experimental and statistical calculations Bateman published throughout his trials. Birkhead wrote a 2000 review arguing that since Bateman’s experiments lasted only three to four days, the female fruit fly, \"Drosophila melanogaster\", may not have needed to mate repeatedly, as it can store sperm for up to four days; if Bateman had used a species in which females had to copulate more often to fertilize their eggs, the results might have been different. Snyder and Gowaty conducted the first in-depth analysis of the data in Bateman’s 1948 paper. They found sampling biases, mathematical errors, and selective presentation of data.\n\nA 2012 review by Zuleyma Tang-Martínez concluded that various empirical and theoretical studies, especially Gowaty's reproduction of Bateman's original experiment, pose a major challenge to Bateman's conclusions, and that Bateman's principle should be considered an unproven hypothesis in need of further reexamination. According to Tang-Martínez, \"modern data simply don't support most of Bateman's and Trivers's predictions and assumptions.\"\n\nA 2016 review confirmed Darwinian sex roles across the animal kingdom, concluding that \"sexual selection, as captured by standard Bateman metrics, is indeed stronger in males than in females and that it is evolutionarily tied to sex biases in parental care and sexual dimorphism.\"\n\nOne error source that have been shown to give an illusion of greater differential in reproductive success in males than in females genetically is that chromosome effects cause a greater percentage of mutations to be lethal before even reaching sexual maturity in males than in females.\n\nThe assumption that any differential in reproductive success between males and females among the individuals that do reach sexual maturity must be due to sexual selection in the current population is also subject to criticism, such as the possibility of remnants of sexually selected traits in a previous species from which a new species have evolved can be negatively selected due to costs in nutrients and weakened immune systems and that such negative selection would cause a higher difference in reproductive success in males than in females even without any still ongoing sexual selection. Since lower degrees of selection during times of stable environment allows genetic variation to build up by random mutations and allow some individuals in a population to survive environmental change while strong constant selection offsets the effect and increases the risk of the entire population dying out during catastrophic environmental change due to less genetic variation, constant loss of genetic variation caused by sexual selection have been suggested as a factor contributing to higher extinction rates in more sexually dimorphic species besides the nutrient, immunity and other costs of the ornaments themselves. While the ornament cost risk would only be removed when the ornaments have been eliminated by selection, the genetic variation model predicts that the species ability to survive would improve significantly even at an early stage of reduction of sexual dimorphism due to other adaptive mutations arising and surviving due to minimal selection during times of stable environment while the genes causing sexually dimorphic anatomy have only in small part been affected by the mutations. Applied to human evolution, this model can explain why early Homo sapiens display a significantly increased adaptability to environmental change already at its early divergence from Homo erectus that had a high muscular sexual dimorphism, as well as why human anatomy through the history of Homo sapiens show a diversification during times of stable climate and a selective loss of the more robust male forms during environmental change that does not recover during later stability, continuing through the loss of many robust characteristics in regional bottlenecks as recent as the end of the Ice Age and the time around the agricultural revolution. It also explains genetic evidence of human genetic diversity increasing during stable environmental periods and being reduced during bottlenecks related to changes in the environment.\n\nRecently, DNA testing has permitted more detailed investigation of mating behavior in numerous species. The results, in many cases, have been cited as evidence against Bateman's principle.\n\nUntil recently, most bird species were believed to be sexually monogamous. DNA paternity testing, however, has shown that in nearly 90% of bird species, females copulate with multiple males during each breeding season. The superb fairy wren is socially monogamous, but 95% of its clutches contain young fathered by extra-pair males. Up to 87% of tree swallow clutches, 75% of coal tit clutches, and 70% of reed bunting clutches contain young fathered by extra-pair males. Even female waved albatrosses, which typically mate for life, are sexually promiscuous, with 17% of young fathered by extra-pair males.\n\nIn many primate species, females solicit sex from males and may mate with more than one male in quick succession. Female lions may mate 100 times per day with different males while they are in estrus. Females of the pseudoscorpion species, \"Cordylochernes scorpioides\", have been shown to have higher reproductive success when mated with more than one male.\n\nThere are a number of claims about sex differences in humans said by evolutionary psychologists to be well supported that are not logically predicted by Bateman's principle. One example that have been pointed out is language ability that is said to be more developed in women than in men. It is argued that language, being a derived human trait, is not predicted by a model that assumes males to be more selected than females to be present to a higher degree in women than in men but the opposite. The objection about a trade-off between spatial ability and language is found lacking due to the existence of animals that outperform humans on spatial tasks such as birds of prey and bats while having much smaller and more rapidly maturing brains than humans, while great ape language is limited and ungrammatical at best despite great apes being much closer to humans in brain capacity than bats or birds of prey are, showing that the system requirements for spatial ability are much lower than those for language and therefore not a serious trade-off for them. The generalization of Bateman's principle to societies lacking modern birth control is also argued to be incompatible with such generalization of the \"nuns more successful at celibacy than monks\" clause of the female erotic plasticity model, as absence of modern contraceptives would mean that absence of reproduction for many men required that many men had no sexual relationships with women while male sexual exclusivity to a particular gender, if present, would have precluded same-sex relationships as a form of non-celibacy for men who could not find a female mate as well. It is also mentioned that the evolutionary psychologists themselves claim that women are sexually picky due to an evolutionary history without modern contraception, while it is pointed out that the notion of celibacy being impossible for men in general with only a small number of individual exceptions is incompatible with the notion of evolution making celibacy inevitable for a significant percentage of men.\n\nThe most well-known exceptions to Bateman's principle are the existence of sex-role reversed species such as pipefish (seahorses), phalaropes and jacanas in which the males perform the majority of the parental care, and are cryptic while the females are highly ornamented and territorially aggressive (; ; ). \n\nIn these species, however, the typical fundamental sex differences are reversed: females have a faster reproductive rate than males (and thus greater reproductive variance), and males have greater assurance of genetic parentage than do females . Consequently, reversals in sex roles and reproductive variance are consistent with Bateman's principle, and with Robert Trivers's parental investment theory.\n\n\n\n"}
{"id": "3280462", "url": "https://en.wikipedia.org/wiki?curid=3280462", "title": "Belief–desire–intention model", "text": "Belief–desire–intention model\n\nThe belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\n\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\n\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. It has been proposed that autistic children do not recognise other people as folk-psychological agents (i.e., agents with their own beliefs, etc.). BDI has been used to develop a rehabilitation strategy to teach autistic children to reason about other people.\n\n"}
{"id": "6672748", "url": "https://en.wikipedia.org/wiki?curid=6672748", "title": "Causal model", "text": "Causal model\n\nA causal model is a conceptual model that describes the causal mechanisms of a system. Causal models can improve study designs by providing clear rules for deciding which independent variables need to be included/controlled for. \n\nThey can allow some questions to be answered from existing observational data without the need for an interventional study such as a randomized controlled trial. Some interventional studies are inappropriate for ethical or practical reasons, meaning that without a causal model, some questions cannot be answered. \n\nCasual models can help with the question of external validity(whether results from one study apply to unstudied populations). Causal models can allow data from multiple studies to be merged (in certain circumstances) to answer questions that cannot be answered by any individual data set.\n\nCausal models are falsifiable, in that if they do not match data, they must be rejected as invalid.\n\nCausal models have found applications in signal processing, epidemiology and machine learning.\n\n Pearl defines a causal model as an ordered triple formula_1, where U is a set of exogenous variables whose values are determined by factors outside the model; V is a set of endogenous variables whose values are determined by factors within the model; and E is a set of structural equations that express the value of each endogenous variable as a function of the values of the other variables in U and V.\n\nAristotle defined a taxonomy of causality, including \"material\", \"formal\", \"efficient\" and \"final\" causes. Hume rejected Aristotle's taxonomy in favor of counterfactuals. At one point, he denied that objects have \"powers\" that make one a cause and another an effect. Later he adopted \"if the first object had not been, the second had never existed\" (\"but-for\" causation).\n\nIn the late 19th century, the discipline of statistics began to form. After a years-long effort to identify causal rules for domains such as biological inheritance, Galton introduced the concept of mean regression (epitomized by the sophomore slump in sports) which later led him to the non-causal concept of correlation. \n\nAs a positivist, Pearson expunged the notion of causality from much of science as an unprovable special case of association and introduced the correlation coefficient as the metric of association. He wrote, \"Force as a cause of motion is exactly the same as a tree god as a cause of growth\" and that causation was only a \"fetish among the inscrutable arcana of modern science\". Pearson founded \"Biometrika\" and the Biometrics Lab at University College London, which became the world leader in statistics.\n\nIn 1908 Hardy and Weinberg solved the problem of trait stability that had led Galton to abandon causality, by invoking Mendelian inheritance.\n\nIn 1921 Wright's path analysis became the theoretical ancestor of causal modeling and causal graphs. He developed this approach while attempting to untangle the relative impacts of heredity, development and environment on guinea pig coat patterns. He backed up his heretical claims by showing how such analyses could explain the relationship between guinea pig birth weight, in utero time and litter size. Opposition to these ideas by prominent statisticians led them to be ignored for the following 40 years (except among animal breeders). Instead scientists relied on correlations, partly at the behest of Wright's critic (and leading statistician), Fisher. One exception was Burks, a student who in 1926 was the first to apply path diagrams to represent a mediator and to assert that holding a mediator constant induces errors. She may have invented path diagrams independently. \n\nIn 1923, Neyman introduced the concept of a potential outcome, but his paper was not translated from Polish to English until 1990. \n\nIn 1958 Cox wrote warned that controlling for a variable Z is valid only if it is highly unlikely to be affected independent variables. \n\nIn the 1960s, Duncan, Blalock, Goldberger and others rediscovered path analysis. While reading Blalock's work on path diagrams, Duncan remembered a lecture by Ogburn twenty years earlier that mentioned a paper by Wright that mentioned Burk. \n\nSociologists called causal models structural equation modeling, but once it became a rote method, it lost its utility, leading some practitioners to reject any relationship to causality. Economists adopted the algebraic part of path analysis, calling it simultaneous equation modeling. However, economists still avoided attributing causal meaning to their equations.\n\nSixty years after his first paper, Wright published a piece that recapitulated it, following Karlin et al.'s critique, which objected that it handled only linear relationships and that robust, model-free presentations of data are more revealing.\n\nIn 1973 Lewis advocated replacing correlation with but-for causality (counterfactuals). He referred to humans' ability to envision alternative worlds in which a cause did or not occur and in which effect an appeared only following its cause. In 1974 Rubin introduced the notion of \"potential outcomes\" as a language for asking causal questions. \n\nIn 1983 Cartwright proposed that any factor that is \"causally relevant\" to an effect be conditioned on, moving beyond simple probability as the only guide. \n\nIn 1986 Baron and Kenny introduced principles for detecting and evaluating mediation in a system of linear equations. As of 2014 their paper was the 33rd most-cited of all time. That year Greenland and Robins introduced the \"exchangeability\" approach to handling confounding by considering a counterfactual. They proposed assessing what would have happened to the treatment group if they had not received the treatment and comparing that outcome to that of the control group. If they matched, confounding was said to be absent. \n\nPearl's causal metamodel involves a three-level abstraction he calls the ladder of causation. The lowest level, Association (seeing/observing), entails the sensing of regularities or patterns in the input data, expressed as correlations. The middle level, Intervention (doing), predicts the effects of deliberate actions, expressed as causal relationships. The highest level, Counterfactuals (imagining), involves constructing a theory of (part of) the world that explains why specific actions have specific effects and what happens in he absence of such actions.\n\nOne object is associated with another if observing one changes the probability of observing the other. Example: shoppers who buy toothpaste are more likely to also buy dental floss. Mathematically: \n\nor the probability of (purchasing) floss given (the purchase of) toothpaste. Associations can also be measured via computing the correlation of the two events. Associations have no causal implications. One event could cause the other, the reverse could be true, or both events could be caused by some third event (unhappy hygenist shames shopper into treating their mouth better ).\n\nThis level asserts specific causal relationships between events. Causality is assessed by experimentally performing some action that affects one of the events. Example: doubling the price of toothpaste (then what happens). Causality cannot be established by examining history (of price changes) because the price change may have been for some other reason that could itself affect the second event (a tariff that increases the price of both goods). Mathematically: \n\nwhere \"do\" is an operator that signals the experimental intervention (doubling the price).\n\nThe highest, counterfactual, level involves consideration of an alternate version of a past event. Example: What is the probability that If a store had doubled the price of floss, the toothpaste-purchasing shopper would still have bought it? Answering yes asserts the existence of a causal relationship. Models that can answer counterfactuals allow precise interventions whose consequences can be predicted. At the extreme, such models are accepted as physical laws (as in the laws of physics, e.g., inertia, which says that if force is not applied to a stationary object, it will not move).\n\nStatistics revolves around the analysis of relationships among multiple variables. Traditionally, these relationships are described as correlations, associations without any implied causal relationships. Causal models attempt to extend this framework by adding the notion of causal relationships, in which changes in one variable cause changes in others.\n\nTwentieth century definitions of causality relied purely on probabilities/associations. One event (X) was said to cause another if it raises the probability of the other (Y). Mathematically this is expressed as: \n\nA later definition attempted to address this ambiguity by conditioning on background factors. Mathematically: \n\nOther attempts to define causality include Granger causality, a statistical hypothesis test that causality (in economics) can be assessed by measuring the ability to predict the future values of one time series using prior values of another time series.\n\nA cause can be necessary, sufficient, contributory or some combination.\n\nFor \"x\" to be a necessary cause of \"y\", the presence of \"y\" must imply the prior occurrence of \"x\". The presence of \"x\", however, does not imply that \"y\" will occur. Necessary causes are also known as \"but-for\" causes, as in \"y\" would not have occurred but for the occurrence of \"x\". \n\nFor \"x\" to be a sufficient cause of \"y\", the presence of \"x\" must imply the subsequent occurrence of \"y\". However, another cause \"z\" may independently cause \"y\". Thus the presence of \"y\" does not require the prior occurrence of \"x\".\n\nFor \"x\" to be a contributory cause of \"y\", the presence of \"x\" must increase the likelihood of \"y\". If the likelihood is 100%, then \"x\" is instead called sufficient. A contributory cause may also be necessary.\n\nA causal diagram is a directed graph that displays causal relationships between variables in a causal model. A causal diagram includes a set of variables (or nodes). Each node is connected by an arrow to one or more other nodes upon which it has a causal influence. An arrowhead delineates the direction of causality, e.g., an arrow connecting variables A and B with the arrowhead at B indicates that a change in A causes a change in B (with an associated probability).\n\nCausal diagrams include causal loop diagrams, directed acyclic graphs, and Ishikawa diagrams.\n\nCasual diagrams are independent of the quantitative probabilities that inform them. Changes to those probabilities (e.g., due to technological improvements) do not require changes to the model.\n\nCausal models have formal structures with elements with specific properties.\n\nThe three types of connections of three nodes are linear chains, branching forks and merging colliders.\n\nChains are straight line connections with arrows pointing from cause to effect. In this model, B is a mediator in that it mediates the change that A would otherwise have on C. \n\nIn forks, one cause has multiple effects. The two effects have a common cause. Conditioning on B (for a specific value of B) reveals a positive correlation between A and C that is not causal. \n\nAn elaboration of a fork is the confounder:\n\nIn such models, B is a common cause of A and C (which also causes A), making B the confounder. \n\nIn colliders, multiple causes affect one outcome. Conditioning on B (for a specific value of B) often reveals a non-causal negative correlation between A and C. This negative correlation has been called collider bias and the \"explain-away\" effect as in, B explains away the correlation between A and C. The correlation can be positive in the case where contributions from both A and C are necessary to affect B. \n\nA mediator node modifies the effect of other causes on an outcome (as opposed to simply affecting the outcome). \n\nA confounder node affects multiple outcomes, creating a positive correlation among them.\n\nAn instrumental variable is one that: \n\n\nRegression coefficients can serve as estimates of the causal effect of an instrumental variable on an outcome as long as that effect is not confounded. In this way, instrumental variables allow causal factors to be quantified without data on confounders. \n\nFor example, given the model:\n\nZ is an independent variable, because it has a path to the outcome Y and is unconfounded, e.g., by U.\n\nDefinition: In the above example, if Z and X take binary values, then the assumption that Z = 0, X = 1 does not occur is called monotonicity. \n\nRefinements to the technique include creating an instrument by conditioning on other variable to block the paths between the instrument and the confounder and combining multiple variables to form a single instrument. \n\nDefinition: Mendelian randomization uses measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in observational studies.\n\nBecause genes vary randomly across populations, presence of a gene typically qualifies as an instrumental variable, implying that in many cases, causality can be quantified using regression on an observational study.\n\nIndependence conditions are rules for deciding whether two variables are independent of each other. Variables are independent if the values of one do not directly affect the values of the other. Multiple causal models can share independence conditions. For example, the models\n\nand \n\nhave the same independence conditions, because conditioning on B leaves A and C independent. However, the two models do not have the same meaning and can be falsified based on data. (If observations show an association between A and C after conditioning on B, then both models are incorrect). Conversely, data cannot show which of these two models are correct, because they have the same independence conditions. Conditioning on a variable is a mechanism for conducting hypothetical experiments. Conditioning on a variable involves analyzing the values of other variables for a given value of the conditioned variable. In the first example, conditioning on B implies that observations for a given value of B should show no correlation between A and C. If such a correlation exists, then the model is incorrect. Non-causal models cannot make such distinctions, because they do not make causal assertions. \n\nAn essential element of correlational study design is to identify potentially confounding influences on the variable under study, such as demographics. These variables are controlled for to eliminate those influences. However, the correct list of confounding variables cannot be determined \"a priori\". It is thus possible that a study may control for irrelevant variables or even (indirectly) the variable under study. \n\nCausal models offer a robust technique for identifying appropriate confounding variables. Formally, Z is a confounder if \"Y is associated with Z via paths not going through X\". These can often be determined using data collected for other studies. Mathematically, if \n\nthen X is a confounder for Y. \n\nEarlier, allegedly incorrect definitions include: \n\n\nThe latter is flawed in that given that in the model: \n\nZ matches the definition, but is a mediator, not a confounder, and is an example of controlling for the outcome.\n\nIn the model \n\nTraditionally, B was considered to be a confounder, because it is associated with X and with Y but is not on a causal path nor is it a descendant of anything on a causal path. Controlling for B causes it to become a confounder. This is known as M-bias. \n\nIn a causal model, the method for identifying all appropriate counfounders (deconfounding) is to block every noncausal path between X and Y without disrupting any causal paths. \n\nDefinition: a backdoor path between two variables X and Y is any path from X to Y that starts with an arrow pointing to X. \n\nX and Y are deconfounded if every backdoor path is blocked and no controlled-for variable Z is descended from X. It is not necessary to control for any variables other than the deconfounders. \n\nDefinition: the backdoor criterion is satisfied when all backdoor paths in a model are blocked.\n\nWhen the causal model is a plausible representation of reality and the backdoor criterion is satisfied, then partial regression coefficients can be used as (causal) path coefficients (for linear relationships). \n\nDefinition: a frontdoor path is a direct causal path for which data is available for all variables. \n\nThe following converts a do expression into a do-free expression by conditioning on the variables along the front-door path. \n\nPresuming data for these observable probabilities is available, the ultimate probability can be computed without an experiment, regardless of the existence of other confounding paths and without backdoor adjustment. \n\nQueries are questions asked based on a specific model. They are generally answered via performing experiments (interventions). Interventions take the form of fixing the value of one variable in a model and observing the result. Mathematically, such queries take the form (from the example): \n\nwhere the \"do\" operator indicates that the experiment explicitly modified the price of toothpaste. Graphically, this blocks any causal factors that would otherwise affect that variable. Diagramatically, this erases all causal arrows pointing at the experimental variable. \n\nMore complex queries are possible, in which the do operator is applied (the value is fixed) to multiple variables.\n\nThe do calculus is the set of manipulations that are available to transform one expression into another, with the general goal of transforming expressions that contain the do operator into expressions that do not. Expressions that do not include the do operator can be estimated from observational data alone, without the need for an experimental intervention, which might be expensive, lengthy or even unethical (e.g., asking subjects to take up smoking). The set of rules is complete (it can be used to derive every true statement in this system). An algorithm can determine whether, for a given model, a solution is computable in polynomial time. \n\nThe calculus includes three rules for the transformation of conditional probability expressions involving the do operator. \n\nRule 1 permits the addition or deletion of observations.:\n\nin the case that the variable set Z blocks all paths from W to Y and all arrows leading into X have been deleted. \n\nRule 2 permits the replacement of an intervention with an observation or vice versa.:\n\nin the case that Z satisfies the back-door criterion. \n\nRule 3 permits the deletion or addition of interventions.:\n\nin the case where no causal paths connect X and Y. \n\nThe rules do not imply that any query can have its do operators removed. In those cases, it may be possible to substitute a variable that is subject to manipulation (e.g., diet) in place of one that is not (e.g., blood cholesterol), which can then be transformed to remove the do. Example: \n\nCounterfactuals consider possibilities that are not found in data, such as whether a nonsmoker would have developed cancer had they instead been a heavy smoker. They are the highest step on Pearl's causality ladder. \n\nDefinition: A potential outcome for a variable Y is \"the value Y would have taken for individual \"u\", had X been assigned the value x\". Mathematically: \n\nThe potential outcome is defined at the level of the individual \"u.\" \n\nThe conventional approach to potential outcomes is data-, not model-driven, limiting its ability to untangle causal relationships. It treats causal questions as problems of missing data and gives incorrect answers to even standard scenarios. \n\nIn the context of causal models, potential outcomes are interpreted causally, rather than statistically.\n\nThe first law of causal inference states that the potential outcome \n\ncan be computed by modifying causal model M (by deleting arrows into X) and computing the outcome for some \"x\". Formally: \n\nExamining a counterfactual using a causal model involves three steps. The approach is valid regardless of the form of the model relationships (linear or otherwise) When the model relationships are fully specified, point values can be computed. In other cases, (e.g., when only probabilities are available) a probability-interval statement (non-smoker x would have a 10-20% chance of cancer) can be computed. \n\nGiven the model:\n\nthe equations for calculating the values of A and C derived from regression analysis or another technique can be applied, substituting known values from an observation and fixing the value of other variables (the counterfactual). \n\nApply abductive reasoning (logical inference that uses observation to find the simplest/most likely explanation) to estimate \"u\", the proxy for the unobserved variables on the specific observation that supports the counterfactual. \n\nFor a specific observation, use the do operator to establish the counterfactual (e.g., \"m\"=0), modifying the equations accordingly. \n\nCalculate the values of the output (\"y\") using the modified equations. \n\nDirect and indirect (mediated) causes can only be distinguished via conducting counterfactuals. Understanding mediation requires holding the mediator constant while intervening on the direct cause. In the model\n\nformula_28\n\nM mediates X's influence on Y, while X also has an unmediated effect on Y. Thus M is held constant, while do(X) is computed.\n\nThe Mediation Fallacy instead involves conditioning on the mediator if the mediator and the outcome are confounded, as they are in the above model.\n\nFor linear models, the indirect effect can be computed by taking the product of all the path coefficients along a mediated pathway. The total indirect effect is computed by the sum of the individual indirect effects. For linear models mediation is indicated when the coefficients of an equation fitted without including the mediator vary significantly from an equation that includes it. \n\nIn experiments on such a model, the controlled direct effect (CDE) is computed by forcing the value of the mediator M (do(M = 0)) and randomly assigning some subjects to each of the values of X (do(X=0), do(X=1), ...) and observing the resulting values of Y. \n\nEach value of the mediator has a corresponding CDE.\n\nHowever, a better experiment is to compute the natural direct effect. (NDE) This is the effect determined by leaving the relationship between X and M untouched while intervening on the relationship between X and Y. \n\nFor example, consider the direct effect of increasing dental hygenist visits (X) from every other year to every year, which encourages flossing (M). Gums (Y) get healthier, either because of the hygenist (direct) or the flossing (mediator/indirect). The experiment is to continue flossing while skipping the hygenist visit.\n\nThe indirect effect of X on Y is the \"increase we would see in Y while holding X constant and increasing M to whatever value M would attain under a unit increase in X\". \n\nIndirect effects cannot be \"controlled\" because the direct path cannot be disabled by holding another variable constant. The natural indirect effect (NIE) is the effect on gum health (Y) from flossing (M). The NIE is calculated as the sum of (floss and no-floss cases) of the difference between the probability of flossing given the hygenist and without the hygenist, or: \n\nThe above NDE calculation includes counterfactual subscripts (formula_32). For nonlinear models, the seemingly obvious equivalence \n\ndoes not apply because of anomalies such as threshold effects and binary values. However, \n\nworks for all model relationships (linear and nonlinear). It allows NDE to then be calculated directly from observational data, without interventions or use of counterfactual subscripts. \n\nCausal models provide a vehicle for integrating data across datasets, known as transport, even though the causal models (and the associated data) differ. E.g., survey data can be merged with randomized, controlled trial data. Transport offers a solution to the question of external validity, whether a study can be applied in a different context.\n\nWhere two models match on all relevant variables and data from one model is known to be unbiased, data from one population can be used to draw conclusions about the other. In other cases, where data is known to be biased, reweighting can allow the dataset to be transported. In a third case, conclusions can be drawn from an incomplete dataset. In some cases, data from studies of multiple populations can be combined (via transportation) to allow conclusions about an unmeasured population. In some cases, combining estimates (e.g., P(W|X)) from multiple studies can increase the precision of a conclusion. \n\nDo-calculus provides a general criterion for transport: A target variable can be transformed into another expression via a series of do-operations that does not involve any \"difference-producing\" variables (those that distinguish the two populations). An analogous rule applies to studies that have relevantly different participants. \n\nAny causal model can be implemented as a Bayesian network. Bayesian networks can be used to provide the inverse probability of an event (given an outcome, what are the probabilities of a specific cause). This requires preparation of a conditional probability table, showing all possible inputs and outcomes with their associated probabilities. \n\nFor example, given a two variable model of Disease and Test (for the disease) the conditional probability table takes the form: \nAccording to this table, when a patient does not have the disease, the probability of a positive test is 12%.\n\nWhile this is tractable for small problems, as the number of variables and their associated states increase, the probability table (and associated computation time) increases exponentially. \n\nBayesian networks are used commercially in applications such as wireless data error correction and DNA analysis. \n\n\n"}
{"id": "31799202", "url": "https://en.wikipedia.org/wiki?curid=31799202", "title": "Comstock–Needham system", "text": "Comstock–Needham system\n\nThe Comstock–Needham system is a naming system for insect wing veins, devised by John Comstock and George Needham in 1898. It was an important step in showing the homology of all insect wings. This system was based on Needham's \"pretracheation theory\" that was later discredited by Frederic Charles Fraser in 1938.\n\nThe Comstock and Needham system attributes different names to the veins on an insect's wing. From the anterior (leading) edge of the wing towards the posterior (rear), the major longitudinal veins are named:\n\nApart from the costal and the anal veins, each vein can be branched, in which case the branches are numbered from anterior to posterior. For example, the two branches of the subcostal vein will be called Sc and Sc.\n\nThe radius typically branches once near the base, producing anteriorly the R and posteriorly the \"radial sector\" Rs. The radial sector may fork twice.\n\nThe media may also fork twice, therefore having four branches reaching the wing margin.\n\nAccording to the Comstock–Needham system, the cubitus forks once, producing the cubital veins Cu and Cu. \nAccording to some other authorities, Cu may fork again, producing the Cu and Cu.\n\nAs there are several anal veins, they are called A1, A2, and so on. They are usually unforked.\n\nCrossveins link the longitudinal veins, and are named accordingly (for example, the medio-cubital crossvein is termed m-cu). Some crossveins have their own name, like the humeral crossvein h and the sectoral crossvein s.\n\nThe cells are named after the vein on the anterior side; for instance, the cell between Sc and R is called Sc.\n\nIn the case where two cells are separated by a crossvein but have the same anterior longitudinal vein, they should have the same name. To avoid this, they are attributed a number. For example, the R cell is divided in two by the radial cross vein: the basal cell is termed \"first R\", and the distal cell \"second R\".\n\nIf a cell is bordered anteriorly by a forking vein, such as R and R, the cell is named after the distal vein, in this case R.\n\n"}
{"id": "178942", "url": "https://en.wikipedia.org/wiki?curid=178942", "title": "Conceptual art", "text": "Conceptual art\n\nConceptual art, sometimes simply called conceptualism, is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic, technical, and material concerns. Some works of conceptual art, sometimes called installations, may be constructed by anyone simply by following a set of written instructions. This method was fundamental to American artist Sol LeWitt's definition of Conceptual art, one of the first to appear in print:\n\nTony Godfrey, author of \"Conceptual Art (Art & Ideas)\" (1998), asserts that conceptual art questions the nature of art, a notion that Joseph Kosuth elevated to a definition of art itself in his seminal, early manifesto of conceptual art, \"Art after Philosophy\" (1969). The notion that art should examine its own nature was already a potent aspect of the influential art critic Clement Greenberg's vision of Modern art during the 1950s. With the emergence of an exclusively language-based art in the 1960s, however, conceptual artists such as Art & Language, Joseph Kosuth (who became the american editor of Art-Language), and Lawrence Weiner began a far more radical interrogation of art than was previously possible (see below). One of the first and most important things they questioned was the common assumption that the role of the artist was to create special kinds of material objects.\nThrough its association with the Young British Artists and the Turner Prize during the 1990s, in popular usage, particularly in the UK, \"conceptual art\" came to denote all contemporary art that does not practice the traditional skills of painting and sculpture. It could be said that one of the reasons why the term \"conceptual art\" has come to be associated with various contemporary practices far removed from its original aims and forms lies in the problem of defining the term itself. As the artist Mel Bochner suggested as early as 1970, in explaining why he does not like the epithet \"conceptual\", it is not always entirely clear what \"concept\" refers to, and it runs the risk of being confused with \"intention\". Thus, in describing or defining a work of art as conceptual it is important not to confuse what is referred to as \"conceptual\" with an artist's \"intention\".\n\nThe French artist Marcel Duchamp paved the way for the conceptualists, providing them with examples of prototypically conceptual works — the readymades, for instance. The most famous of Duchamp's readymades was \"Fountain\" (1917), a standard urinal-basin signed by the artist with the pseudonym \"R.Mutt\", and submitted for inclusion in the annual, un-juried exhibition of the Society of Independent Artists in New York (which rejected it). The artistic tradition does not see a commonplace object (such as a urinal) as art because it is not made by an artist or with any intention of being art, nor is it unique or hand-crafted. Duchamp's relevance and theoretical importance for future \"conceptualists\" was later acknowledged by US artist Joseph Kosuth in his 1969 essay, \"Art after Philosophy\", when he wrote: \"All art (after Duchamp) is conceptual (in nature) because art only exists conceptually\".\n\nIn 1956 the founder of Lettrism, Isidore Isou, developed the notion of a work of art which, by its very nature, could never be created in reality, but which could nevertheless provide aesthetic rewards by being contemplated intellectually. This concept, also called \"Art esthapériste\" (or \"infinite-aesthetics\"), derived from the infinitesimals of Gottfried Wilhelm Leibniz – quantities which could not actually exist except conceptually. The current incarnation () of the Isouian movement, Excoördism, self-defines as the art of the infinitely large and the infinitely small.\n\nIn 1961 the term \"concept art\", coined by the artist Henry Flynt in his article bearing the term as its title, appeared in a proto-Fluxus publication \"An Anthology of Chance Operations\". \nHowever, it assumed a different meaning when employed by Joseph Kosuth and by the English Art and Language group, who discarded the conventional art object in favour of a documented critical inquiry, that began in Art-Language The Journal of conceptual art in 1969, into the artist's social , philosophical and psychological status. By the mid-1970s they had produced publications, indices, performances, texts and paintings to this end. In 1970 \"Conceptual Art and Conceptual Aspects\", the first dedicated conceptual-art exhibition, took place at the New York Cultural Center.\n\nConceptual art emerged as a movement during the 1960s – in part as a reaction against formalism as then articulated by the influential New York art critic Clement Greenberg. According to Greenberg Modern art followed a process of progressive reduction and refinement toward the goal of defining the essential, formal nature of each medium. Those elements that ran counter to this nature were to be reduced. The task of painting, for example, was to define precisely what kind of object a painting truly is: what makes it a painting and nothing else. As it is of the nature of paintings to be flat objects with canvas surfaces onto which colored pigment is applied, such things as figuration, 3-D perspective illusion and references to external subject matter were all found to be extraneous to the essence of painting, and ought to be removed.\n\nSome have argued that conceptual art continued this \"dematerialization\" of art by removing the need for objects altogether,\nwhile others, including many of the artists themselves, saw conceptual art as a radical break with Greenberg's kind of formalist Modernism. Later artists continued to share a preference for art to be self-critical, as well as a distaste for illusion. However, by the end of the 1960s it was certainly clear that Greenberg's stipulations for art to continue within the confines of each medium and to exclude external subject matter no longer held traction.\nConceptual art also reacted against the commodification of art; it attempted a subversion of the gallery or museum as the location and determiner of art, and the art market as the owner and distributor of art. Lawrence Weiner said: \"Once you know about a work of mine you own it. There's no way I can climb inside somebody's head and remove it.\" Many conceptual artists' work can therefore only be known about through documentation which is manifested by it, e.g. photographs, written texts or displayed objects, which some might argue are not in themselves the art. It is sometimes (as in the work of Robert Barry, Yoko Ono, and Weiner himself) reduced to a set of written instructions describing a work, but stopping short of actually making it—emphasising the idea as more important than the artifact. This reveals an explicit preference for the \"art\" side of the ostensible dichotomy between art and craft, where art, unlike craft, takes place within and engages historical discourse: for example, Ono's \"written instructions\" make more sense alongside other conceptual art of the time.\n\nLanguage was a central concern for the first wave of conceptual artists of the 1960s and early 1970s. Although the utilisation of text in art was in no way novel, only in the 1960s did the artists Lawrence Weiner, Edward Ruscha, Joseph Kosuth, Robert Barry, and Art & Language begin to produce art by exclusively linguistic means. Where previously language was presented as one kind of visual element alongside others, and subordinate to an overarching composition (e.g. Synthetic Cubism), the conceptual artists used language in place of brush and canvas, and allowed it to signify in its own right. Of Lawrence Weiner's works Anne Rorimer writes, \"The thematic content of individual works derives solely from the import of the language employed, while presentational means and contextual placement play crucial, yet separate, roles.\"\n\nThe British philosopher and theorist of conceptual art Peter Osborne suggests that among the many factors that influenced the gravitation toward language-based art, a central role for conceptualism came from the turn to linguistic theories of meaning in both Anglo-American analytic philosophy, and structuralist and post structuralist Continental philosophy during the middle of the twentieth century. This linguistic turn \"reinforced and legitimized\" the direction the conceptual artists took. Osborne also notes that the early conceptualists were the first generation of artists to complete degree-based university training in art. Osborne later made the observation that contemporary art is \"post-conceptual\" in a public lecture delivered at the Fondazione Antonio Ratti, Villa Sucota in Como on July 9, 2010. It is a claim made at the level of the ontology of the work of art (rather than say at the descriptive level of style or movement).\n\nThe American art historian Edward A. Shanken points to the example of Roy Ascott who \"powerfully demonstrates the significant intersections between conceptual art and art-and-technology, exploding the conventional autonomy of these art-historical categories.\" Ascott, the British artist most closely associated with cybernetic art in England, was not included in Cybernetic Serendipity because his use of cybernetics was primarily conceptual and did not explicitly utilize technology. Conversely, although his essay on the application of cybernetics to art and art pedagogy, \"The Construction of Change\" (1964), was quoted on the dedication page (to Sol Lewitt) of Lucy R. Lippard's seminal \"Six Years: The Dematerialization of the Art Object from 1966 to 1972\", Ascott's anticipation of and contribution to the formation of conceptual art in Britain has received scant recognition, perhaps (and ironically) because his work was too closely allied with art-and-technology. Another vital intersection was explored in Ascott's use of the thesaurus in 1963 , which drew an explicit parallel between the taxonomic qualities of verbal and visual languages – a concept would be taken up in Joseph Kosuth's \"Second Investigation, Proposition 1\" (1968) and Mel Ramsden's \"Elements of an Incomplete Map\" (1968).\n\n\"By adopting language as their exclusive medium, Weiner, Barry, Wilson, Kosuth and Art & Language were able to sweep aside the vestiges of authorial presence manifested by formal invention and the handling of materials.\"\nAn important difference between conceptual art and more \"traditional\" forms of art-making goes to the question of artistic skill. Although skill in the handling of traditional media often plays little role in conceptual art, it is difficult to argue that no skill is required to make conceptual works, or that skill is always absent from them. John Baldessari, for instance, has presented realist pictures that he commissioned professional sign-writers to paint; and many conceptual performance artists (e.g. Stelarc, Marina Abramović) are technically accomplished performers and skilled manipulators of their own bodies. It is thus not so much an absence of skill or hostility toward tradition that defines conceptual art as an evident disregard for conventional, modern notions of authorial presence and of individual artistic expression.\n\nThe first wave of the \"conceptual art\" movement extended from approximately 1967 to 1978. Early \"concept\" artists like Henry Flynt, Robert Morris, and Ray Johnson influenced the later, widely accepted movement of conceptual art. Conceptual artists like Dan Graham, Hans Haacke, and Lawrence Weiner have proven very influential on subsequent artists, and well known contemporary artists such as Mike Kelley or Tracey Emin are sometimes labeled \"second- or third-generation\" conceptualists, or \"post-conceptual\" artists.\n\nMany of the concerns of the conceptual art movement have been taken up by contemporary artists. While they may or may not term themselves \"conceptual artists\", ideas such as anti-commodification, social and/or political critique, and ideas/information as medium continue to be aspects of contemporary art, especially among artists working with installation art, performance art, net.art and electronic/digital art.\n\n\n\nBooks\n\n\nEssays\n\n\nExhibition catalogues\n\n\n"}
{"id": "25707018", "url": "https://en.wikipedia.org/wiki?curid=25707018", "title": "Concision", "text": "Concision\n\nConcision (alternatively brevity, laconicism, terseness, or conciseness) is the cutting out of unnecessary words while conveying an idea. It aims to enhance communication by eliminating redundancy without omitting important information. Concision has been described as one of the elementary principles of writing. The related concept of succinctness is the opposite of verbosity.\n\nConcision means to be economical with words, expressing what's needed using the fewest words necessary. That may involve removing redundant or unnecessary phrases or replacing them with shorter ones. It is described in \"The Elements of Style\" by Strunk and White as follows:\n\nConcision has also been described as \"eliminat[ing] words that take up space without saying much.\" Simple examples include replacing \"\" with \"because\" or \"at this point in time\" with \"now\" or \"currently.\"\n\nAn example sentence, with explanation:\n\nThe following example is taken from:\n\nThe source suggests this replacement:\n\nIn the second quote, the same information is communicated in less than half the length. However, it could be more concisely rewritten and communicate the same information:\n\nConcise expression, particularly in writing, is considered one of the basic goals of teaching the English language. Techniques to achieve concise writing are taught for students at all levels, from the introduction to writing to the preparation of PhD dissertations, and legal writing for law students.\n\nIt has been argued that although \"in expository prose English places a high value on conciseness... [t]he value placed on conciseness... is not shared by all cultures\", with, for example, the Thai culture as one where redundancy is prized as an opportunity to use additional words to demonstrate the writer's command of the language. This may lead to a tendency for people from those cultures to use repetitive or redundant phrasing when learning English.\n\nThe related concept of succinctness is a characteristic of speech, writing, data structure, algorithmic games, and thought in general, exhibiting both clarity and brevity. It is the opposite of verbosity, in which there is an excess of words.\n\nBrevity in succinctness is not achieved by shortening original material by coding or compressing it, but rather by omitting redundant material from it.\n\n\n"}
{"id": "47792266", "url": "https://en.wikipedia.org/wiki?curid=47792266", "title": "Construction of Concept Map", "text": "Construction of Concept Map\n\nConcept is usually perceived as a regularity in events or objects or in their records. While constructing a concept map, it is essential to keep in mind that the concept be built with reference to a focus question. Hence, initially, the focus question to which we seek to answer is carefully chosen because learners usually tend to deviate from this question relating only to domains and thus, fails to answer the question.\n\nWith the selected domain and the focus question, the next step is to identify the key concepts which apply to this domain.About 15 to 25 concepts is sufficient which is usually ordered in a rank ordered list. Such list should be established from the most general and inclusive concept for the particular chosen problem. This list will assist in at least in the beginning of the construction of the concept map. The list is referred to as Parking Lot since the list is constantly shuffled to build up the required network of the concept. Two or more concepts are connected to each other using linking words or phrases which can only then complete a meaningful sentence.Another important characteristics of a concept map is the cross-links. This cross link acts as the relationship between two different domains used in the concept map.This helps in a clear representation of the knowledge contained in the concept and also gives a clear background with specific examples. \n"}
{"id": "173937", "url": "https://en.wikipedia.org/wiki?curid=173937", "title": "Cosmological principle", "text": "Cosmological principle\n\nIn modern physical cosmology, the cosmological principle is the notion that the spatial distribution of matter in the universe is homogeneous and isotropic when viewed on a large enough scale, since the forces are expected to act uniformly throughout the universe, and should, therefore, produce no observable irregularities in the large-scale structuring over the course of evolution of the matter field that was initially laid down by the Big Bang.\n\nAstronomer William Keel explains:\n\nThe cosmological principle is usually stated formally as 'Viewed on a sufficiently large scale, the properties of the universe are the same for all observers.' This amounts to the strongly philosophical statement that the part of the universe which we can see is a fair sample, and that the same physical laws apply throughout. In essence, this in a sense says that the universe is knowable and is playing fair with scientists.\n\nThe cosmological principle depends on a definition of \"observer,\" and contains an implicit qualification and two testable consequences.\n\n\"Observers\" means any observer at any location in the universe, not simply any human observer at any location on Earth: as Andrew Liddle puts it, \"the cosmological principle [means that] the universe looks the same whoever and wherever you are.\"\n\nThe qualification is that variation in physical structures can be overlooked, provided this does not imperil the uniformity of conclusions drawn from observation: the Sun is different from the Earth, our galaxy is different from a black hole, some galaxies advance toward rather than recede from us, and the universe has a \"foamy\" texture of galaxy clusters and voids, but none of these different structures appears to violate the basic laws of physics.\n\nThe two testable structural consequences of the cosmological principle are homogeneity and isotropy. Homogeneity means that the same observational evidence is available to observers at different locations in the universe (\"the part of the universe which we can see is a fair sample\"). Isotropy means that the same observational evidence is available by looking in any direction in the universe (\"the same physical laws apply throughout\" ). The principles are distinct but closely related, because a universe that appears isotropic from any two (for a spherical geometry, three) locations must also be homogeneous.\n\nThe cosmological principle is first clearly asserted in the \"Philosophiæ Naturalis Principia Mathematica\" (1687) of Isaac Newton. In contrast to earlier classical or medieval cosmologies, in which Earth rested at the center of universe, Newton conceptualized the Earth as a sphere in orbital motion around the Sun within an empty space that extended uniformly in all directions to immeasurably large distances. He then showed, through a series of mathematical proofs on detailed observational data of the motions of planets and comets, that their motions could be explained by a single principle of \"universal gravitation\" that applied as well to the orbits of the Galilean moons around Jupiter, the Moon around the Earth, the Earth around the Sun, and to falling bodies on Earth. That is, he asserted the equivalent material nature of all bodies within the Solar System, the identical nature of the Sun and distant stars and thus the uniform extension of the physical laws of motion to a great distance beyond the observational location of Earth itself.\n\nObservations show that more distant galaxies are closer together and have lower content of chemical elements heavier than lithium. Applying the cosmological principle, this suggests that heavier elements were not created in the Big Bang but were produced by nucleosynthesis in giant stars and expelled across a series of supernovae explosions and new star formation from the supernovae remnants, which means heavier elements would accumulate over time. Another observation is that the furthest galaxies (earlier time) are often more fragmentary, interacting and unusually shaped than local galaxies (recent time), suggesting evolution in galaxy structure as well.\n\nA related implication of the cosmological principle is that the largest discrete structures in the universe are in mechanical equilibrium. Homogeneity and isotropy of matter at the largest scales would suggest that the largest discrete structures are parts of a single indiscrete form, like the crumbs which make up the interior of a cake. At extreme cosmological distances, the property of mechanical equilibrium in surfaces lateral to the line of sight can be empirically tested; however, under the assumption of the cosmological principle, it cannot be detected parallel to the line of sight (see timeline of the universe).\n\nCosmologists agree that in accordance with observations of distant galaxies, a universe must be non-static if it follows the cosmological principle. In 1923, Alexander Friedmann set out a variant of Einstein's equations of general relativity that describe the dynamics of a homogeneous isotropic universe. Independently, Georges Lemaître derived in 1927 the equations of an expanding universe from the General Relativity equations. Thus, a non-static universe is also implied, independent of observations of distant galaxies, as the result of applying the cosmological principle to general relativity.\n\nKarl Popper criticized the cosmological principle on the grounds that it makes \"our \"lack\" of knowledge a principle of \"knowing something\"\". He summarized his position as:\n\nAlthough the universe is inhomogeneous at smaller scales, it \"is\" statistically homogeneous on scales larger than 250 million light years. The cosmic microwave background is isotropic, that is to say that its intensity is about the same whichever direction we look at.\n\nHowever, recent findings have called this view into question. Data from the Planck Mission shows hemispheric bias in 2 respects: one with respect to average temperature (i.e. temperature fluctuations), the second with respect to larger variations in the degree of perturbations (i.e. densities). Therefore, the European Space Agency (the governing body of the Planck Mission) has concluded that these anisotropies are, in fact, statistically significant and can no longer be ignored.\n\nThe \"cosmological principle\" implies that at a sufficiently large scale, the universe is homogeneous. This means that different places will appear similar to one another, so sufficiently large structures cannot exist. Yadav and his colleagues have suggested a maximum scale of 260/h Mpc for structures within the universe according to this heuristic. Other authors have suggested values as low as 60/h Mpc. Yadav's calculation suggests that the maximum size of a structure can be about 370 Mpc.\n\nA number of observations conflict with predictions of maximal structure sizes:\n\n\nIn September 2016, however, studies of the expansion of the Universe that have used data taken by the \"Planck\" mission show it to be highly isotropical, reinforcing the cosmological principle\n\nThe perfect cosmological principle is an extension of the cosmological principle, and states that the universe is homogeneous and isotropic in space \"and\" time. In this view the universe looks the same everywhere (on the large scale), the same as it always has and always will. The perfect cosmological principle underpins Steady State theory and emerges from chaotic inflation theory.\n\n"}
{"id": "152902", "url": "https://en.wikipedia.org/wiki?curid=152902", "title": "Dormant Commerce Clause", "text": "Dormant Commerce Clause\n\nThe Dormant Commerce Clause, or Negative Commerce Clause, in American constitutional law, is a legal doctrine that courts in the United States have inferred from the Commerce Clause in Article I of the US Constitution. The Dormant Commerce Clause is used to prohibit state legislation that discriminates against interstate or international commerce.\n\nFor example, it is lawful for Michigan to require food labels that specifically identify certain animal parts, if they are present in the product, because the state law applies to food produced in Michigan as well as food imported from other states and foreign countries; the state law would violate the Commerce Clause if it applied only to imported food or if it was otherwise found to favor domestic over imported products. Likewise, California law requires milk sold to contain a certain percentage of milk solids that federal law does not require, which is allowed under the Dormant Commerce Clause doctrine because California's stricter requirements apply equally to California-produced milk and imported milk and so does not discriminate against or inappropriately burden interstate commerce.\n\nThe idea that regulation of interstate commerce may to some extent be an exclusive Federal power was discussed even before adoption of the Constitution, but the framers did not use the word \"dormant\". On September 15, 1787, the Framers of the Constitution debated in Philadelphia whether to guarantee states the ability to lay duties of tonnage without Congressional interference so that the states could finance the clearing of harbors and the building of lighthouses. James Madison believed that the mere existence of the Commerce Clause would bar states from imposing any duty of tonnage: \"He was more and more convinced that the regulation of Commerce was in its nature indivisible and ought to be wholly under one authority.\"\n\nRoger Sherman disagreed: \"The power of the United States to regulate trade being supreme can control interferences of the State regulations when such interferences happen; so that there is no danger to be apprehended from a concurrent jurisdiction.\" Sherman saw the commerce power as similar to the tax power, the latter being one of the concurrent powers shared by the federal and state governments. Ultimately, the Philadelphia Convention decided upon the present language about duties of tonnage in , which says: \"No state shall, without the consent of Congress, lay any duty of tonnage ...\"\n\nThe word \"dormant,\" in connection with the Commerce Clause, originated in dicta of Chief Justice John Marshall. For example, in the case of \"Gibbons v. Ogden\", , he wrote that the power to regulate interstate commerce \"can never be exercised by the people themselves, but must be placed in the hands of agents, or lie dormant.\" Concurring Justice William Johnson was even more emphatic that the Constitution is \"altogether in favor of the exclusive grants to Congress of power over commerce.\"\n\nLater, in the case of \"Willson v. Black-Bird Creek Marsh Co.\", , Marshall wrote: \"We do not think that the [state] act empowering the Black Bird Creek Marsh Company to place a dam across the creek, can, under all the circumstances of the case, be considered as repugnant to the power to regulate commerce in its dormant state, or as being in conflict with any law passed on the subject.\"\n\nIf Marshall was suggesting that the power over interstate commerce is an exclusive federal power, the Dormant Commerce Clause doctrine eventually developed very differently: it treats regulation that does not discriminate against or unduly burden interstate commerce as a concurrent power, rather than an exclusive federal power, and it treats regulation that does so as an exclusive federal power. Thus, the modern doctrine says that congressional power over interstate commerce is somewhat exclusive but \"not absolutely exclusive\". The approach began in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court: \"Either absolutely to affirm, or deny that the nature of this [commerce] power requires exclusive legislation by Congress, is to lose sight of the nature of the subjects of this power, and to assert concerning all of them, what is really applicable but to a part.\" The first clear holding of the Supreme Court striking down a state law under the Dormant Commerce Clause came in 1873.\n\nJustice Anthony Kennedy has written that: \"The central rationale for the rule against discrimination is to prohibit state or municipal laws whose object is local economic protectionism, laws that would excite those jealousies and retaliatory measures the Constitution was designed to prevent.\" In order to determine whether a law violates a so-called \"dormant\" aspect of the Commerce Clause, the court first asks whether it discriminates on its face against interstate commerce. In this context, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter.\n\nThus, in a dormant Commerce Clause case, a court is initially concerned with whether the law facially discriminates against out-of-state actors or has the effect of favoring in-state economic interests over out-of-state interests. Discriminatory laws motivated by \"simple economic protectionism\" are subject to a \"virtually per se rule of invalidity\", \"City of Philadelphia v. New Jersey\" 437 U.S. 617 (1978), \"Dean Milk Co. v. City of Madison, Wisconsin\", 340 U.S. 349 (1951), \"Hunt v. Washington State Apple Advertising Comm.\", 432 U.S. 333 (1977) which can only be overcome by a showing that the State has no other means to advance a legitimate local purpose, \"Maine v. Taylor\", 477 U.S. 131(1986). See also \"Brown-Forman Distillers v. New York State Liquor Authority\", .\n\nOn the other hand, when a law is \"directed to legitimate local concerns, with effects upon interstate commerce that are only incidental\" (United Haulers Association, Inc.), that is, where other legislative objectives are credibly advanced and there is no patent discrimination against interstate trade, the Court has adopted a much more flexible approach, the general contours of which were outlined in \"Pike v. Bruce Church, Inc.\", 397 U.S. 137, 142 (1970) and \"City of Philadelphia v. New Jersey\", 437 U.S. at 624. If the law is not outright or intentionally discriminatory or protectionist, but still has some impact on interstate commerce, the court will evaluate the law using a balancing test. The Court determines whether the interstate burden imposed by a law outweighs the local benefits. If such is the case, the law is usually deemed unconstitutional. See \"Pike v. Bruce Church, Inc.\", . In the Pike case, the Court explained that a state regulation having only \"incidental\" effects on interstate commerce \"will be upheld unless the burden imposed on such commerce is clearly excessive in relation to the putative local benefits\". 397 U.S. at 142, 90 S.Ct. at 847. When weighing burdens against benefits, a court should consider both \"the nature of the local interest involved, and ... whether it could be promoted as well with a lesser impact on interstate activities\". Id. Thus regulation designed to implement public health and safety, or serve other legitimate state interests, but impact interstate commerce as an incident to that purpose, are subject to a test akin to the rational basis test, a minimum level of scrutiny. See \"Bibb v. Navajo Freight Lines, Inc.\" In USA Recycling, Inc. v. Town of Babylon, 66 F.3d 1272, 1281 (C.A.2 (N.Y.), 1995), the court explained:\n\nIf the state activity constitutes \"regulation\" of interstate commerce, then the court must proceed to a second inquiry: whether the activity regulates evenhandedly with only \"incidental\" effects on interstate commerce, or discriminates against interstate commerce. As we use the term here, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter. The party challenging the validity of a state statute or municipal ordinance bears the burden of showing that it discriminates against, or places some burden on, interstate commerce. \"Hughes v. Oklahoma\", 441 U.S. 322, 336, 99 S.Ct. 1727, 1736, 60 L.Ed.2d 250 (1979). If discrimination is established, the burden shifts to the state or local government to show that the local benefits of the statute outweigh its discriminatory effects, and that the state or municipality lacked a nondiscriminatory alternative that could have adequately protected the relevant local interests. If the challenging party cannot show that the statute is discriminatory, then it must demonstrate that the statute places a burden on interstate commerce that \"is clearly excessive in relation to the putative local benefits.\" \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456, 471(1981) (quoting Pike, 397 U.S. at 142, 90 S.Ct. at 847).\n\nOver the years, the Supreme Court has consistently held that the language of the Commerce Clause contains a further, negative command prohibiting certain state taxation even when Congress has failed to legislate on the subject. Examples of such cases are \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992); \"Northwestern States Portland Cement Co. v. Minnesota\", 358 U.S. 450, 458 (1959) and \"H.P. Hood & Sons, Inc. v. Du Mond\", 336 U.S. 525 (1949).\n\nMore recently, in the 2015 case of \"Comptroller of Treasury of MD. v. Wynne\", the Court addressed Maryland's unusual practice of taxing personal income earned in Maryland, and taxing personal income of its citizens earned outside Maryland, \"without\" any tax credit for income tax paid to other states. The Court held this sort of double-taxation to be a violation of the dormant Commerce Clause. The Court faulted Justice Antonin Scalia's criticism of the dormant Commerce Clause doctrine by saying that he failed to \"explain why, under his interpretation of the Constitution, the Import-Export Clause \nwould not lead to the same result that we reach under the dormant Commerce Clause\".\n\nApplication of the dormant commerce clause to state taxation is another manifestation of the Court's holdings that the Commerce Clause prevents a State from retreating into economic isolation or jeopardizing the welfare of the Nation as a whole, as it would do if it were free to place burdens on the flow of commerce across its borders that commerce wholly within those borders would not bear. The Court's taxation decisions thus \"reflected a central concern of the Framers that was an immediate reason for calling the Constitutional Convention: the conviction that in order to succeed, the new Union would have to avoid the tendencies toward economic Balkanization that had plagued relations among the Colonies and later among the States under the Articles of Confederation.\" \"Wardair Canada, Inc. v. Florida Dept. of Revenue\", 477 U.S. 1 (1986); \"Hughes v. Oklahoma\", 441 U.S. 322 (1979); \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995).\n\nAs with the Court's application of the dormant commerce clause to discriminatory regulation, the pre-New Deal Court attempted to apply a formalistic approach to state taxation alleged to interfere with interstate commerce. The history is described in \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995):\n\nThe command has been stated more easily than its object has been attained, however, and the Court's understanding of the dormant Commerce Clause has taken some turns. In its early stages, the Court held the view that interstate commerce was wholly immune from state taxation \"in any form\", \"even though the same amount of tax should be laid on (intrastate) commerce\". This position gave way in time to a less uncompromising but formal approach, according to which, for example, the Court would invalidate a state tax levied on gross receipts from interstate commerce, or upon the \"freight carried\" in interstate commerce, but would allow a tax merely measured by gross receipts from interstate commerce as long as the tax was formally imposed upon franchises, or \"'in lieu of all taxes upon (the taxpayer's) property,'\" Dissenting from this formal approach in 1927, Justice Stone remarked that it was \"too mechanical, too uncertain in its application, and too remote from actualities, to be of value.\" \n\nAccompanying the revolution in approach in the Court's Congressional powers jurisprudence, the New Deal Court began to change its approach to state taxation as well. The Jefferson Lines decision continues:\n\nIn 1938, the old formalism began to give way with Justice Stone's opinion in \"Western Live Stock v. Bureau of Revenue\", 303 U.S. 250, which examined New Mexico's franchise tax, measured by gross receipts, as applied to receipts from out-of-state advertisers in a journal produced by taxpayers in New Mexico but circulated both inside and outside the State. Although the assessment could have been sustained solely on prior precedent, Justice Stone added a dash of the pragmatism that, with a brief interlude, has since become our aspiration in this quarter of the law. ... The Court explained that \"[i]t was not the purpose of the commerce clause to relieve those engaged in interstate commerce from their just share of state tax burden even though it increases the cost of doing the business.\" \n\nDuring the transition period, some taxes were upheld based on a careful review of the actual economic impact of the tax, and other taxes were reviewed based on the kind of tax involved, whether the tax had a nefarious impact on commerce or not. Under this formalistic approach, a tax might be struck down, and then re-passed with exactly the same economic incidence, but under another name, and then withstand review.\n\nThe absurdity of this approach was made manifest in the two Railway Express cases. In the first, a tax imposed by the state of Virginia on American business concerns operating within the state was struck down because it was a business privilege tax imposed on the privilege of doing business in interstate commerce. But then, in the second, Virginia revised the wording of its statute to impose a \"franchise tax\" on \"intangible property\" in the form of \"going concern\" value as measured by gross receipts.\n\nThe Court upheld the reworded statute as not violative of the prohibition on privilege taxes, even though the impact of the old tax and new were essentially identical. There was no real economic difference between the statutes in Railway Express I and Railway Express II. The Court long since had recognized that interstate commerce may be made to pay its way. Yet under the Spector rule, the economic realities in Railway Express I became irrelevant. The Spector rule (against privilege taxes) had come to operate only as a rule of draftsmanship, and served only to distract the courts and parties from their inquiry into whether the challenged tax produced results forbidden by the Commerce Clause.\n\nThe death knell of formalism occurred in \"Complete Auto Transit, Inc v. Brady\", 430 U.S. 274 (1977), which approved a Mississippi privilege tax upon a Michigan company engaged in the business of shipping automobiles to Mississippi dealers. The Court there explained:\n\nAppellant's attack is based solely on decisions of this Court holding that a tax on the \"privilege\" of engaging in an activity in the State may not be applied to an activity that is part of interstate commerce. See, e. g., \"Spector Motor Service v. O'Connor\", 340 U.S. 602 (1951); \"Freeman v. Hewit\", 329 U.S. 249 (1946). This rule looks only to the fact that the incidence of the tax is the \"privilege of doing business\"; it deems irrelevant any consideration of the practical effect of the tax. The rule reflects an underlying philosophy that interstate commerce should enjoy a sort of \"free trade\" immunity from state taxation. \n\nComplete Auto Transit is the last in a line of cases that gradually rejected a per se approach to state taxation challenges under the commerce clause. In overruling prior decisions which struck down privilege taxes per se, the Court noted the following, in what has become a central component of commerce clause state taxation jurisprudence:\n\nWe note again that no claim is made that the activity is not sufficiently connected to the State to justify a tax, or that the tax is not fairly related to benefits provided the taxpayer, or that the tax discriminates against interstate commerce, or that the tax is not fairly apportioned.\n\nThese four factors, nexus, relationship to benefits, discrimination, and apportionment, have come to be regarded as the four Complete Auto Transit factors applied repeatedly in subsequent cases. Complete Auto Transit must be recognized as the culmination of the Court's emerging commerce clause approach, not just in taxation, but in all of its aspects. Application of Complete Auto Transit to State taxation remains a highly technical and specialized venture, requiring the application of commerce clause principles to an understanding of specialized tax law.\n\nIn addition to satisfying the four-prong test in \"Complete Auto Transit\", the Supreme Court has held state taxes which burden international commerce cannot create a substantial risk of multiple taxations and must not prevent the federal government from \"speaking with one voice when regulating commercial relations with foreign governments\". \"Japan Lines, Ltd. v. County of Los Angeles\", 441 U.S. 434 (1979).\n\nIn \"Kraft Gen. Foods, Inc. v. Iowa Dept. of Revenue and Finance\", 505 U.S. 71 (1992), the Supreme Court considered a case in which Iowa taxed dividends from foreign subsidiaries, without allowing a credit for taxes paid to foreign governments, but not dividends from domestic subsidiaries operating outside Iowa. This differential treatment arose from Iowa's adoption of the definition of \"net income\" used by the Internal Revenue Service. For federal income tax purposes, dividends from domestic subsidiaries are allowed to be exempted from the parent corporations income to avoid double taxation. The Iowa Supreme Court rejected a Commerce Clause claim because Kraft failed to show \"that Iowa businesses receive a commercial advantage over foreign commerce due to Iowa's taxing scheme.\" Considering an Equal Protection Clause challenge, the Iowa Supreme Court held that the use of the federal government's definitions of income were convenient for the state and was \"rationally related to the goal of administrative efficiency\". The Supreme Court rejected the notion that administrative convenience was a sufficient defense for subjecting foreign commerce to a higher tax burden than interstate commerce. The Supreme Court held that \"a State's preference for domestic commerce over foreign commerce is inconsistent with the Commerce Clause even if the State's own economy is not a direct beneficiary of the discrimination.\"\n\nDiscrimination in the flow of interstate commerce has arisen in a variety of contexts. A line of important cases has dealt with local processing requirements. Under the local processing requirement, a municipality seeks to force the local processing of raw materials before they are shipped in interstate commerce.\n\nThe basic idea of the local processing ordinance was to provide favored access to local processors of locally produced raw materials. Examples of Supreme Court decisions in this vein are set out in its Carbone decision. They include \"Minnesota v. Barber\", 136 U.S. 313, (1890) (striking down a Minnesota statute that required any meat sold within the State, whether originating within or without the State, to be examined by an inspector within the State); \"Foster-Fountain Packing Co. v. Haydel\", 278 U.S. 1 (1928) (striking down a Louisiana statute that forbade shrimp to be exported unless the heads and hulls had first been removed within the State); \"Johnson v. Haydel\", 278 U.S. 16 (1928) (striking down analogous Louisiana statute for oysters); \"Toomer v. Witsell\", 334 U.S. 385 (1948) (striking down South Carolina statute that required shrimp fishermen to unload, pack, and stamp their catch before shipping it to another State); \"Pike v. Bruce Church, Inc.\", supra (striking down Arizona statute that required all Arizona-grown cantaloupes to be packaged within the State prior to export); \"South-Central Timber Development, Inc. v. Wunnicke\", 467 U.S. 82 (1984) (striking down an Alaska regulation that required all Alaska timber to be processed within the State prior to export). The Court has defined \"protectionist\" state legislation as \"regulatory measures designed to benefit in-state economic interests by burdening out-of-state competitors\". \"New Energy Co. of Indiana v. Limbach\", 486 U.S. 269, 273–74 (1988).\n\nIn the 1980s, spurred by RCRA's emphasis on comprehensive local planning, many states and municipalities sought to promote investment in more costly disposal technologies, such as waste-to-energy incinerators, state-of-the-art landfills, composting and recycling. Some states and localities sought to promote private investment in these costly technologies by guaranteeing a longterm supply of customers. See Phillip Weinberg, Congress, the Courts, and Solid Waste Transport: Good Fences Don't Always Make Good Neighbors, 25 Envtl. L. 57 (1995); Atlantic Coast Demolition & Recycling, Inc., 112 F.3d 652, 657 (3d Cir. 1997). For about a decade, the use of regulation to channel private commerce to designated private disposal sites was greatly restricted as the result of the Carbone decision discussed below.\n\nFlow control laws typically came in various designs. One common theme was the decision to fund local infrastructure by guaranteeing a minimum volume of business for privately constructed landfills, incinerators, composters or other costly disposal sites. In some locales, choice of the flow control device was driven by state bonding laws, or municipal finance concerns. If a county or other municipality issued general obligation bonds for construction of a costly incinerator, for example, state laws might require a special approval process. If approval could be obtained, the bonds themselves would be counted against governmental credit limitations, or might impact the governmental body's credit rating: in either instance the ability to bond for other purposes might be impaired. But by guaranteeing customers for a privately constructed and financed facility, a private entity could issue its own bonds, privately, on the strength of the public's waste assurance.\n\nThe private character of flow control regimens can thus be explained in part by the desire to utilize particular kinds of public financing devices. It can also be explained by significant encouragement at the national level, in national legislation as well as in federal executive policy to achieve environmental objectives utilizing private resources. Ironically, these public-private efforts often took the form of local processing requirements which ultimately ran afoul of the commerce clause.\n\nThe Town of Clarkstown had decided that it wanted to promote waste assurance through a local private transfer station. The transfer station would process waste and then forward the waste to the disposal site designated by the Town. The ordinance had the following features:\n\nWaste hauling in the Town of Clarkstown was accomplished by private haulers, subject to local regulation. The scheme had the following aspects: (A) The Town promoted the financing of a privately owned transfer station through a waste assurance agreement with the private company. Thus the designated facility was a private company. (B) The Town of Clarkstown forced private haulers to bring their solid waste for local processing at the designated transfer station, even if the ultimate destination of solid waste was an out-of-state disposal site. (C) The primary rationale for forcing in-state waste into the designated private transfer station was financial; it was seen as a device to raise revenue to finance the transfer station.\n\nThe Town of Clarkstown's ordinance was designed and written right in the teeth of the long line of Supreme Court cases which had historically struck down local processing requirements. In short, it was as if the authors of the ordinance had gone to a treatise on the commerce clause and intentionally chosen a device which had been traditionally prohibited. A long line of Supreme Court case law had struck down local processing requirements when applied to goods or services in interstate commerce. As the Court in Carbone wrote:\n\nWe consider a so-called flow control ordinance, which requires all solid waste to be processed at a designated transfer station before leaving the municipality. The avowed purpose of the ordinance is to retain the processing fees charged at the transfer station to amortize the cost of the facility. Because it attains this goal by depriving competitors, including out-of-state firms, of access to a local market, we hold that the flow control ordinance violates the Commerce Clause.\n\nThe Court plainly regarded the decision as a relatively unremarkable decision, not a bold stroke. As the Court wrote: \"The case decided today, while perhaps a small new chapter in that course of decisions, rests nevertheless upon well-settled principles of our Commerce Clause jurisprudence.\" And, the Court made it plain, that the problem with Clarkstown's ordinance was that it created a local processing requirement protective of a local private processing company:\n\nIn this light, the flow control ordinance is just one more instance of local processing requirements that we long have held invalid ... The essential vice in laws of this sort is that they bar the import of the processing service. Out-of-state meat inspectors, or shrimp hullers, or milk pasteurizers, are deprived of access to local demand for their services. Put another way, the offending local laws hoard a local resource—be it meat, shrimp, or milk—for the benefit of local businesses that treat it. 511 U.S. at 392–393.\n\nThe Court's 2007 decision in \"United Haulers Association v. Oneida-Herkimer Solid Waste Management Authority\" starkly illustrates the difference in result when the Court finds that local regulation is not discriminatory. The Court dealt with a flow control regimen quite similar to that considered in Carbone. The \"only salient difference is that the laws at issue here require haulers to bring waste to facilities owned and operated by a state-created public benefit corporation.\" The Court decided that the balancing test should apply, because the regulatory scheme favored the government owned facility, but treated all private facilities equally.\n\nCompelling reasons justify treating these laws differently from laws favoring particular private businesses over their competitors. \"Conceptually, of course, any notion of discrimination assumes a comparison of substantially similar entities.\" \"General Motors Corp. v. Tracy\", 519 U.S. 278 (1997). But States and municipalities are not private businesses—far from it. Unlike private enterprise, government is vested with the responsibility of protecting the health, safety, and welfare of its citizens. See \"Metropolitan Life Ins. Co. v. Massachusetts\", 471 U.S. 724 (1985) ... These important responsibilities set state and local government apart from a typical private business.\n\nThe Court's United Haulers decision demonstrates an understanding of the regulatory justifications for flow control starkly missing in the Carbone decision:\n\nBy the 1980s, the Counties confronted what they could credibly call a solid waste \" 'crisis.' \"... Many local landfills were operating without permits and in violation of state regulations. Sixteen were ordered to close and remediate the surrounding environment, costing the public tens of millions of dollars. These environmental problems culminated in a federal clean-up action against a landfill in Oneida County; the defendants in that case named over local businesses and several municipalities and school districts as third-party defendants The \"crisis\" extended beyond health and safety concerns. The Counties had an uneasy relationship with local waste management companies, enduring price fixing, pervasive overcharging, and the influence of organized crime. Dramatic price hikes were not uncommon: In 1986, for example, a county contractor doubled its waste disposal rate on six weeks' notice\n\nThe Court would not interfere with local government's efforts to solve an important public and safety problem.\n\nThe contrary approach of treating public and private entities the same under the dormant Commerce Clause would lead to unprecedented and unbounded interference by the courts with state and local government. The dormant Commerce Clause is not a roving license for federal courts to decide what activities are appropriate for state and local government to undertake, and what activities must be the province of private market competition. In this case, the citizens of Oneida and Herkimer Counties have chosen the government to provide waste management services, with a limited role for the private sector in arranging for transport of waste from the curb to the public facilities. The citizens could have left the entire matter for the private sector, in which case any regulation they undertook could not discriminate against interstate commerce. But it was also open to them to vest responsibility for the matter with their government, and to adopt flow control ordinances to support the government effort. It is not the office of the Commerce Clause to control the decision of the voters on whether government or the private sector should provide waste management services. \"The Commerce Clause significantly limits the ability of States and localities to regulate or otherwise burden the flow of interstate commerce, but it does not elevate free trade above all other values.\"\n\nThe history of commerce clause jurisprudence evidences a distinct difference in approach where the state is seeking to exercise its public health and safety powers, on the one hand, as opposed to attempting to regulate the flow of commerce. The exact dividing line between the two interests, the right of states to exercise regulatory control over their public health and safety, and the interest of the national government in unfettered interstate commerce is not always easy to discern. One Court has written as follows:\n\nNot surprisingly, the Court's effort to preserve a national market has, on numerous occasions, come into conflict with the states' traditional power to \"legislat[e] on all subjects relating to the health, life, and safety of their citizens.\" \"Huron Portland Cement Co. v. City of Detroit\", 362 U.S. 440, 443 (1960). On these occasions, the Supreme Court has \"struggled (to put it nicely) to develop a set of rules by which we may preserve a national market without needlessly intruding upon the States' police powers, each exercise of which no doubt has some effect on the commerce of the Nation.\" \"Camps Newfound/Owatonna v. Town of Harrison\", 520 U.S. 564, 596 (1997) (Scalia, J., dissenting) (citing \"Okla. Tax Comm'n v. Jefferson Lines\", 514 U.S. 175, 180–83 (1995)); see generally Boris I. Bittker, Regulation of Interstate and Foreign Commerce § 6.01[A], at 6–5 (\"[T]he boundaries of the [State's] off-limits area are, and always have been, enveloped in a haze.\"). Those rules are \"simply stated, if not simply applied.\" Camps Newfound/Owatonna, 520 U.S. at 596 (Scalia, J., dissenting).\n\nA frequently cited example of the deference afforded to the powers of state and local government may be found in \"Exxon Corp. v. Maryland\", 437 U.S. 117 (1978), where the State of Maryland barred producers of petroleum products from operating retail service stations in the state. It is difficult to imagine a regimen which might have greater impact on the way in which markets are organized. Yet, the Court found the legislation constitutionally permitted: \"The fact that the burden of a state regulation falls on some interstate companies does not, by itself establish a claim of discrimination against interstate commerce,\" the Court wrote. The \"Clause protects interstate market, not particular interstate firms, from prohibitive or burdensome regulations.\"\n\nSimilarly, in \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456 (1981) the Court upheld a state law that banned nonreturnable milk containers made of plastic but permitted other nonreturnable milk containers. The Court found that the existence of a burden on out-of-state plastic industry was not 'clearly excessive' in comparison to the state's interest in promoting conservation. And the court continued:\n\nIn Exxon, the Court stressed that the Commerce Clause protects the interstate market, not particular interstate firms, from prohibitive or burdensome regulations. A nondiscriminatory regulation serving substantial state purpose is not invalid simply because it causes some business to shift from a predominantly out-of-state industry to a predominantly in-state industry. Only if the burden on interstate commerce clearly outweighs the State's legitimate purpose does such a regulation violate the commerce clause. When a state statute regarding safety matters applies equally to interstate and intrastate commerce, the courts are generally reluctant to invalidate it even if it may have some impact on interstate commerce. In \"Bibb v. Navajo Freight Lines\" 359 U.S. 520, 524 (1959), the United States Supreme Court stated: 'These safety measures carry a strong presumption of validity when challenged in court. If there are alternative ways of solving a problem, we do not sit to determine which of them is best suited to achieve a valid state objective. Policy decisions are for the state legislature, absent federal entry into the field. Unless we can conclude on the whole record that \"the total effect of the law as a safety measure in reducing accidents and casualties is so slight or problematical as not to outweigh the national interest in keeping interstate commerce free from interferences which seriously impede it\" we must uphold the statute.\n\nThere are two notable exceptions to the dormant Commerce Clause doctrine that can permit state laws or actions that otherwise violate the Dormant Commerce Clause to survive court challenges.\n\nThe first exception occurs when Congress has legislated on the matter. See \"Western & Southern Life Ins. v. State Board of California\", . In this case the Dormant Commerce Clause is no longer \"dormant\" and the issue is a Commerce Clause issue, requiring a determination of whether Congress has approved, preempted, or left untouched the state law at issue.\n\nThe second exception is \"market participation exception\". This occurs when the state is acting \"in the market\", like a business or customer, rather than as a \"market regulator\". For example, when a state is contracting for the construction of a building or selling maps to state parks, rather than passing laws governing construction or dictating the price of state park maps, it is acting \"in the market\". Like any other business in such cases, a state may favor or shun certain customers or suppliers.\n\nThe Supreme Court introduced the market participant doctrine in \"Hughes v. Alexandria Scrap Corp.\", 426 U.S. 794 (1976), which upheld a Maryland program that offered bounties to scrap processors to destroy abandoned automobile hulks. See also \"Wisconsin Dep't of Indus., Labor & Human Relations v. Gould Inc.\", 475 U.S. 282, 289 (1986); \"Reeves, Inc. v. Stake\", 447 U.S. 429, 437 (1980). Because Maryland required out-of-state processors, but not in-state processors, to submit burdensome documentation to claim their bounties, the state effectively favored in-state processors over out-of-state processors. The Court held that because the state was merely attaching conditions to its expenditure of state funds, the Maryland program affected the market no differently than if Maryland were a private company bidding up the price of auto hulks. Because the state was not \"regulating\" the market, its economic activity was not subject to the anti-discrimination principles underlying the dormant Commerce Clause—and the state could impose different paperwork burdens on out-of-state processors. \"Nothing in the purposes animating the Commerce Clause prohibits a State, in the absence of congressional action, from participating in the market and exercising the right to favor its own citizens over others.\"\n\nAnother important case is \"White v. Massachusetts Council of Constr. Employers, Inc.\", in which the Supreme Court held that the City of Boston could require its building contractors to hire at least fifty percent of their workforce from among Boston residents. 460 U.S. at 214–15. Because all of the employees covered by that mandate were \"in a substantial if informal sense, 'working for the city,' \" Boston was considered to be simply favoring its own residents through the expenditures of municipal funds. The Supreme Court stated, \"when a state or local government enters the market as a participant it is not subject to the restraints of the Commerce Clause.\" Id. at 208. Nothing in the Constitution precludes a local government from hiring a local company precisely because it is local.\n\nOther important cases enunciating the market participation exception principle are \"Reeves, Inc. v. Stake\", and \"South-Central Timber Development, Inc. v. Wunnicke\", . The \"Reeves\" case outlines the market participation exception test. In this case state-run cement co-ops were allowed to make restrictive rules (e.g. rules not to sell out-of-state). Here, this government-sponsored business was acting restrictively like an individually owned business and this action was held to be constitutional. \"South-Central Timber\" is important because it limits the market exception. \"South-Central Timber\" holds that the market-participant doctrine is limited in allowing a State to impose burdens on commerce within the market in which it is a participant, but allows it to go no further. The State may not impose conditions that have a substantial regulatory effect outside of that particular market.\n\nThe \"market participation exception\" to the dormant Commerce Clause does not give states unlimited authority to favor local interests, because limits from other laws and Constitutional limits still apply. In \"United Building & Construction Trades Council v. Camden\", , the city of Camden, New Jersey had passed an ordinance requiring that at least forty percent of the employees of contractors and subcontractors on city projects be Camden residents. The Supreme Court found that while the law was not infirm because of the Dormant Commerce Clause, it violated the Privileges and Immunities Clause of Article IV of the Constitution. Justice Rehnquist's opinion distinguishes the market-participant doctrine from the privileges and immunities doctrine. Similarly, Congress has the power itself under the Commerce Clause to regulate and sanction states acting as \"market participants\", but it lacks power to legislate in ways that violate Article IV.\n\nIn the 21st century, the dormant Commerce Clause has been a frequent legal issue in cases arising under state laws regulating some aspects of Internet activity. Because of the interstate, and often international, nature of Internet communications, state laws addressing internet-related subjects such as spam, online sales or online pornography can often trigger Dormant Commerce Clause issues.\n\nA \"negative\" or \"dormant\" component to the Commerce Clause has been the subject of scholarly discussion for many decades. Both Supreme Court Justices Antonin Scalia and Clarence Thomas have rejected the notion of a Dormant Commerce Clause. They believe that such a doctrine is inconsistent with an originalist interpretation of the Constitution—so much so that they believe the doctrine is a \"judicial fraud\".\n\nA number of earlier Supreme Court justices also expressed dissatisfaction with the dormant Commerce Clause doctrine. For example, Chief Justice Taney said this in 1847:\n\nIf it was intended to forbid the States from making any regulations of commerce, it is difficult to account for the omission to prohibit it, when that prohibition has been so carefully and distinctly inserted in relation to other powers ... [T]he legislation of Congress and the States has conformed to this construction from the foundation of the government ... The decisions of this court will also, in my opinion, when carefully examined, be found to sanction the construction I am maintaining.\n\nHowever, that statement by Taney in 1847 was before the doctrine morphed in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court that the Commerce Clause does not always require \"exclusive legislation by Congress\".\n\nIn \"Trailer Marine Transport Corp. v. Rivera Vázquez\", 977 F.2d 1, 7-8 (1st Cir. 1992), the First Circuit held that the dormant Commerce Clause applies to Puerto Rico.\n\n\n"}
{"id": "39228396", "url": "https://en.wikipedia.org/wiki?curid=39228396", "title": "Equivalence principle (geometric)", "text": "Equivalence principle (geometric)\n\nThe equivalence principle is one of the corner-stones of gravitation theory. Different formulations of the equivalence principle are labeled \"weakest\", \"weak\", \"middle-strong\" and \"strong.\" All of these formulations are based on the empirical equality of inertial mass, gravitational active and passive charges.\n\nThe \"weakest\" equivalence principle is restricted to the motion law of a probe point mass in a uniform gravitational field. Its localization is the \"weak\" equivalence principle that states the existence of a desired local inertial frame at a given world point. This is the case of equations depending on a gravitational field and its first order derivatives, e. g., the equations of mechanics of probe point masses, and the equations of electromagnetic and Dirac fermion fields. The \"middle-strong\" equivalence principle is concerned with any matter, except a gravitational field, while the \"strong\" one is applied to all physical laws.\n\nThe above-mentioned variants of the equivalence principle aim to guarantee the transition of General Relativity to Special Relativity in a certain reference frame. However, only the particular \"weakest\" and \"weak\" equivalence principles are true. \nTo overcome this difficulty, the equivalence principle can be formulated in geometric terms as follows.\n\nIn the spirit of Felix Klein's Erlanger program, Special Relativity can be characterized as the Klein geometry of Lorentz group invariants. Then the geometric equivalence principle is formulated to require the existence of Lorentz invariants on a world manifold formula_1. This requirement holds if the tangent bundle formula_2 of formula_1 admits an atlas with Lorentz transition functions, i.e., a structure group of the associated frame bundle formula_4 of linear tangent frames in formula_2 is reduced to the Lorentz group formula_6. By virtue of the well known theorem on structure group reduction, this reduction takes place if and only if the quotient bundle formula_7 possesses a global section, which is a pseudo-Riemannian metric on formula_1.\n\nThus the geometric equivalence principle provides the necessary and sufficient conditions of the existence of a pseudo-Riemannian metric, i.e., a gravitational field on a world manifold.\n\nBased on the geometric equivalence principle, gravitation theory is formulated as gauge theory where a gravitational field is described as a classical Higgs field responsible for spontaneous breakdown of space-time symmetries.\n\n\n"}
{"id": "730906", "url": "https://en.wikipedia.org/wiki?curid=730906", "title": "Exclusion principle (philosophy)", "text": "Exclusion principle (philosophy)\n\nThe exclusion principle is a philosophical principle that states:\n\nThe exclusion principle is most commonly applied when one poses this scenario; One usually considers that the desire to lift one’s arm as a mental event, and the lifting on one's arm, a physical event. According to the exclusion principle, there must be no event that does not supervene on \"e\" while causing \"e*\". To show this better, substitute \"\"the desire to lift one's arm\" for \"e\", and \"one to lift their arm\" for \"e*\"\".\n\nThis is interpreted as meaning that mental events supervene upon the physical. However, some philosophers do not accept this principle, and accept epiphenomenalism, which states that mental events are caused by physical events, but physical events are not caused by mental events (called \"causal impotence\"). However, If \"e#\" does not cause \"e\", then there is no way to verify that \"e*\" exists. Yet, this debate has not been settled in the philosophical community.\n\n"}
{"id": "36188092", "url": "https://en.wikipedia.org/wiki?curid=36188092", "title": "Fa (concept)", "text": "Fa (concept)\n\nFa (;) is a concept in Chinese philosophy that covers ethics, logic, and law. It can be translated as \"law\" in some contexts, but more often as \"model\" or \"standard.\" First gaining importance in the Mohist school of thought, the concept was principally elaborated in Legalism. In Han Fei's philosophy, the king is the sole source of \"fa\" (law), taught to the common people so that there would be a harmonious society free of chance occurrences, disorder, and \"appeal to privilege\". High officials were not to be held above \"fa\" (law or protocol), nor were they to be allowed to independently create their own \"fa\", uniting both executive fiat and rule of law.\n\nXunzi, a philosopher that would end up being foundational in Han dynasty Confucianism, also took up \"fa\", suggesting that it could only be properly assessed by the Confucian sage (ruler), and that the most important \"fa\" were the very rituals that Mozi had ridiculed for their ostentatious waste and lack of benefit for the people at large.\n\nThe concept of \"fa\" first gained importance in the Mohist school of thought. To Mozi, a standard must stand \"three tests\" in order to determine its efficacy and morality. The first of these tests was its origin; if the standard had precedence in the actions or thought of the semi-mythological sage kings of the Xia dynasty whose examples are frequently cited in classical Chinese philosophy. The second test was one of validity; does the model stand up to evidence in the estimation of the people? The third and final test was one of applicability; this final one is a utilitarian estimation of the net good that, if implemented, the standard would have on both the people and the state.\n\nThe third test speaks to the fact that to the Mohists, a \"fa\" was not simply an abstract model, but an active tool. The real-world use and practical application of \"fa\" were vital. Yet \"fa\" as models were also used in later Mohist logic as principles used in deductive reasoning. As classical Chinese philosophical logic was based on analogy rather than syllogism, \"fa\" were used as benchmarks to determine the validity of logical claims through comparison. There were three \"fa\" in particular that were used by these later Mohists (or \"Logicians\") to assess such claims, which were mentioned earlier. The first was considered a \"root\" standard, a concern for precedence and origin. The second, a \"source\", a concern for empiricism. The third, a \"use\", a concern for the consequence and pragmatic utility of a standard. These three \"fa\" were used by the Mohists to both promote social welfare and denounce ostentation or wasteful spending.\n\n"}
{"id": "18562589", "url": "https://en.wikipedia.org/wiki?curid=18562589", "title": "Gossen's second law", "text": "Gossen's second law\n\nGossen's Second “Law”, named for Hermann Heinrich Gossen (1810–1858), is the assertion that an economic agent will allocate his or her expenditures such that the ratio of the marginal utility of each good or service to its price (the marginal expenditure necessary for its acquisition) is equal to that for every other good or service. Formally,\nwhere\n\nImagine that an agent has spent money on various sorts of goods or services. If the last unit of currency spent on goods or services of one sort bought a quantity with \"less\" marginal utility than that which would have been associated with the quantity of another sort that could have been bought with the money, then the agent would have been \"better off\" instead buying more of that other good or service. Assuming that goods and services are continuously divisible, the only way that it is possible that the marginal expenditure on one good or service should not yield more utility than the marginal expenditure on the other (or \"vice versa\") is if the marginal expenditures yield \"equal\" utility.\n\nAssume that utility, goods, and services have the requisite properties so that formula_7 is well defined for each good or service. An agent then optimizes\nsubject to a budget constraint\nwhere\nUsing the method of Lagrange multipliers, one constructs the function\nand finds the first-order conditions for optimization as\n(which simply implies that all of formula_10 will be spent) and\nso that\nwhich is algebraically equivalent to\nSince every such ratio is equal to formula_17, the ratios are all equal one to another:\n\n\n"}
{"id": "20744016", "url": "https://en.wikipedia.org/wiki?curid=20744016", "title": "Health management system", "text": "Health management system\n\nThe health management system (HMS) is an evolutionary medicine regulative process proposed by Nicholas Humphrey in which actuarial assessment of fitness and economic-type cost–benefit analysis determines the body’s regulation of its physiology and health. This incorporation of cost–benefit calculations into body regulation provides a science grounded approach to mind–body phenomena such as placebos that are otherwise not explainable by low level, noneconomic, and purely feedback based homeostatic or allostatic theories.\n\n\nPlacebos are explained as the result of false information about the availability of external treatment and support that mislead the health management system into not deploying evolved self-treatments. This results in the placebo suppression of medical symptoms.\n\nSince Hippocrates, it has been recognized that the body has self-healing powers (vis medicatrix naturae). Modern evolutionary medicine identifies them with physiologically based self-treatments that provide the body with prophylactic, healing, or restorative capabilities against injuries, infections and physiological disruption. Examples include:\n\n\nThese evolved self-treatments deployed by the body are experienced by humans as unpleasant and unwanted illness symptoms.\n\nSuch self-treatments according to evolutionary medicine are deployed to increase an individual’s biological fitness.\n\nTwo factors affect their deployment.\n\nFirst, it is usually advantageous to deploy them on a precautionary basis. As a result, it will often turn out that they have been deployed apparently unnecessarily, though this has in fact been advantageous since in probabilistic terms they have provided an insurance against a potentially costly outcome. As Nesse notes: \"Vomiting, for example, may cost only a few hundred calories and a few minutes, whereas not vomiting may result in a 5% chance of death\" page 77.\n\nSecond, self-treatments are costly both in using energy, and also in their risk of damaging the body.\n\n\nOne factor in deployment is low level physiological control by proinflammatory cytokines such as IL-1 triggered by bacterial lipopolysaccharides (LPS).\n\nAnother is higher level control in which the brain takes into account what it learns about circumstances and how that makes it well and ill. Conditioning shows the existence of such learnt control: give saccharin paired in a drink with a drug that creates immunosuppression, and later on, giving saccharin alone will produce immunosuppression. Such conditioning happens both in experimental rodents and humans.\n\nEvolution, according to Nicholas Humphrey, has selected an internal health management system that uses cost benefit analysis upon whether the deployment of a self-treatment aids biological fitness, and so should be activated. \na specially designed procedure for “economic resource management” that is, I believe, one of the key features of the “natural health-care service” which has evolved in ourselves and other animals to help us deal throughout our lives with repeated bouts of sickness, injury, and other threats to our well-being.\n\nAn analogy is explicitly made with the health economics consideration used in management decisions involving external medical treatment.\n\nNow, if you wonder about this choice of managerial terminology for talking about biological healing systems, I should say that it is quite deliberate (and so is the pun on NHS.) With the phrase “natural health-care service” I do intend to evoke, at a biological level, all the economic connotations that are so much a part of modern health-care in society.\n\nExternal medications will affect the cost benefits advantages of deploying an evolved self-treatment. Some animals use external ones. Wild animals, including apes, do so in the form of ingested detoxifying clays, rough leaves that clear gut parasites, and pharmacologically active plants Complementary to this, research finds that animals have the ability to select and prefer substances that aid their recuperation from illness.\n\nThe welfare of social animals (including humans) depends upon other individuals (social buffering). The actuarial assessments of the costs and benefits of deploying a self-treatment therefore will depend upon the presence, or not, of other individuals. The presence of helpful others will affect, for example, the risk of predators when incapacitated, and—in those case in which animals do this (such as humans)—the provision of food, and care during sickness.\n\nThe health management system factors in the presence of such external treatment and social support as one aspect of the circumstances needed to determine whether it is advantageous to deploy or not an evolved self-treatment.\n\nAll humans societies use external medications, and some individuals exist that are considered to have special healing knowledge about illnesses and their treatments. Humans are also usually supportive to those in their group. The availability of these things will affect the cost benefits of the body deploying its own biological ones. This could, in turn, lead to the health management system (given its beliefs (information) about treatments and support) to deploy or not, or doing so differently, the body’s own treatments.\n\nNicholas Humphrey describes how the health management system explains placebos – an external treatment without direct physiological effects – as follows:\nSuppose, for example, a doctor gives someone who is suffering an infection a pill that she rightly believes to contain an antibiotic: because her hopes will be raised she will no doubt make appropriate adjustments to her health-management strategy – lowering her precautionary defences in anticipation of the sickness not lasting long.\n\nThe health management system, in other words, when faced with an infection is tricked into making a mistaken cost benefit analysis using false information. The effect of that false information is that the benefits of the self-treatment cease to outweigh its costs. As a result, it is not deployed, and an individual does not experience unwanted medical symptoms.\n\nFailure to deploy an evolved self-treatment need not put an individual at risk since evolution has advantaged their deployment on a precautionary basis. As Nicholas Humphrey notes:\n\nTherefore, not deploying an evolved self-treatment, and so not having a medical symptom due to placebo false information might be without consequence.\n\nThe health management system’s idea of a top down neural control of the body is also found in the idea that a central governor regulates muscle fatigue to protect the body from the harmful effects (such as anoxia and hyperglycemia) of over prolonged exercise.\n\nThe idea of a fatigue governor was first proposed in 1924 by the 1922 Nobel Prize winner Archibald Hill, and more recently, on the basis of modern research, by Tim Noakes.\n\nLike with the health management system, the central governor shares the idea that much of what is attributed to low level feedback homeostatic regulation is, in fact, due to top down control by the brain. The advantage of this top down management is that the brain can enhance such regulation by allowing it to be modified by information. For example, in endurance running, a cost benefit trade exists off between the advantages of continuing to run, and the risk if this is too prolonged that it might harm the body. Being able to regulate fatigue in terms of information about the benefits and costs of continued exercise would enhance biological fitness.\n\nLow level theories exist that suggest that fatigue is due mechanical failure of the exercising muscles (\"peripheral fatigue\"). However, such low level theories do not explain why running muscle fatigue is affected by information relevant to cost benefit trade offs. For example, marathon runners can carry on running longer if told they are near the finishing line, than far away. The existence of a central governor can explain this effect.\n\n\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "939578", "url": "https://en.wikipedia.org/wiki?curid=939578", "title": "List of eponymous laws", "text": "List of eponymous laws\n\nThis list of eponymous laws provides links to articles on laws, principles, adages, and other succinct observations or predictions named after a person. In some cases the person named has coined the law – such as Parkinson's law. In others, the work or publications of the individual have led to the law being so named – as is the case with Moore's law. There are also laws ascribed to individuals by others, such as Murphy's law; or given eponymous names despite the absence of the named person.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1280458", "url": "https://en.wikipedia.org/wiki?curid=1280458", "title": "Marginal revenue", "text": "Marginal revenue\n\nIn microeconomics, marginal revenue (R') is the additional revenue that will be generated by increasing product sales by one unit. It can also be described as the unit revenue the last item sold has generated for the firm. In a perfectly competitive market, the additional revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good. This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price. However, a monopoly determines the entire industry's sales. As a result, it will have to lower the price of all units sold to increase sales by 1 unit. Therefore, the marginal revenue generated is always lower than the price the firm is able to charge for the unit sold, since each reduction in price causes unit revenue to decline on every good the firm sells. The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. \n\nA firms profits will be maximized when marginal revenue (MR) equals marginal cost (MC). If formula_1 then a firm should increase output for more profits, if formula_2 then a firm should decrease output for additional profits. A firm should choose the output level which is profit maximizing under perfect competition theory formula_3.\n\nMarginal revenue is equal to the ratio of the change in revenue for some change in quantity sold to that change in quantity sold. This can also be represented as a derivative when the change in quantity sold becomes arbitrarily small. More formally, define the revenue function to be the following\n\nBy the product rule, marginal revenue is then given by\n\nFor a firm facing perfect competition, price does not change with quantity sold (formula_6), so marginal revenue is equal to price. For a monopoly, the price decreases with quantity sold (formula_7), so marginal revenue is less than price (for positive formula_8).\n\nThe marginal revenue curve is affected by the same factors as the demand curve - changes in income, change in the prices of complements and substitutes, change in populations. These factors can cause the R curve to shift and rotate.\n\nThe relationship between marginal revenue and the elasticity of demand by the firm's customers can be derived as follows:\n\nwhere e is the price elasticity of demand. If demand is inelastic (e < 1) then R' will be negative, because to sell a marginal (infinitesimal) unit the firm would have to lower the selling price so much that it would lose more revenue on the pre-existing units than it would gain on the incremental unit. If demand is elastic (e > 1) R' will be positive, because the additional unit would not drive down the price by so much. If the firm is a perfect competitor, so that it is so small in the market that its quantity produced and sold has no effect on the price, then the price elasticity of demand is negative infinity, and marginal revenue simply equals the (market-determined) price.\n\nProfit maximization requires that a firm produces where marginal revenue equals marginal costs. Firm managers are unlikely to have complete information concerning their marginal revenue function or their marginal costs. Fortunately, the profit maximization conditions can be expressed in a “more easily applicable form” or rule of thumb.\n\nMarkup is the difference between price and marginal cost. The formula states that markup as a percentage of price equals the negative of the inverse of elasticity of demand. Alternatively, the relationship can be expressed as:\n\nThus if e is - 2 and mc is $5.00 then price is $10.00.\n\n(<R> - C')/ <R> = - 1/e is called the Lerner index after economist Abba Lerner. The Lerner index is a measure of market power - the ability of a firm to charge a price that exceeds marginal cost. The index varies from zero to 1. The greater the difference between price and marginal cost the closer the index value is to 1. The Lerner index increases as demand becomes less elastic.\n\nExample\nIf a company can sell 10 units at $20 each or 11 units at $19 each, then the marginal revenue from the eleventh unit is (11 × 19) - (10 × 20) = $9.\n\n\n"}
{"id": "4675536", "url": "https://en.wikipedia.org/wiki?curid=4675536", "title": "Master–slave morality", "text": "Master–slave morality\n\nMaster–slave morality is a central theme of Friedrich Nietzsche's works, in particular the first essay of \"On the Genealogy of Morality\". Nietzsche argued that there were two fundamental types of morality: \"master morality\" and \"slave morality\". Master morality values pride and power, while slave morality values things like kindness, empathy, and sympathy. Master morality weighs actions on good or bad consequences (i. e., classical virtues and vices, consequentialism), unlike slave morality, which weighs actions on a scale of good or evil intentions (e. g., Christian virtues and vices, Kantian deontology).\n\nFor Nietzsche, a particular morality is inseparable from the formation of a particular culture, meaning that a culture's language, codes and practices, narratives, and institutions are informed by the struggle between these two moral structures (see valuation).\n\nNietzsche defined master morality as the morality of the strong-willed. Nietzsche criticizes the view (which he identifies with contemporary British ideology) that good is everything that is helpful, and bad is everything that is harmful. He argues proponents of this view have forgotten the origins of its values and it is based merely on a non-critical acceptance of habit: what is useful has always been defined as good, therefore usefulness is goodness as a value. He continues explaining that in the prehistoric state \"the value or non-value of an action was derived from its consequences\" but ultimately \"[t]here are no moral phenomena at all, only moral interpretations of phenomena.\" For strong-willed men, the \"good\" is the noble, strong, and powerful, while the \"bad\" is the weak, cowardly, timid, and petty. \n\nThe essence of master morality is \"nobility\". Other qualities that are often valued in master morality are open-mindedness, courage, truthfulness, trust, and an accurate sense of one's self-worth. Master morality begins in the \"noble man\", with a spontaneous idea of the good; then the idea of bad develops as what is not good. \"The noble type of man experiences \"itself\" as determining values; it does not need approval; it judges, \"what is harmful to me is harmful in itself\"; it knows itself to be that which first accords honour to things; it is \"value-creating\".\" In this sense, the master morality is the full recognition that \"oneself\" is the measure of all moral truths. Insofar as something is helpful to the strong-willed man, it is like what he values in himself; therefore, the strong-willed man values such things as good because they aid him in a life-long process of self-actualization through the will to power.\n\nAccording to Nietzsche, masters are creators of morality; slaves respond to master morality with their slave morality. Unlike master morality, which is sentiment, slave morality is based on re-sentiment—devaluing that which the master values and the slave does not have. As master morality originates in the strong, slave morality originates in the weak. Because slave morality is a reaction to oppression, it vilifies its oppressors. Slave morality is the inverse of master morality. As such, it is characterized by pessimism and cynicism. Slave morality is created in opposition to what master morality values as \"good\". \n\nSlave morality does not aim at exerting one's will by strength, but by careful subversion. It does not seek to transcend the masters, but to make them slaves as well. The essence of slave morality is \"utility\": The good is what is most useful for the whole community, not just the strong. Nietzsche saw this as a contradiction. Since the powerful are few in number, compared to the masses of the weak, the weak gain power by corrupting the strong into believing that the causes of slavery (viz., the will to power) are \"evil\", as are the qualities the weak originally could not choose because of their weakness. By saying humility is voluntary, slave morality avoids admitting that their humility was in the beginning forced upon them by a master. Biblical principles of humility, charity, and pity are the result of universalizing the plight of the slave onto all humankind, and thus enslaving the masters as well. \"The \"democratic\" movement is the heir to Christianity\"—the political manifestation of slave morality because of its obsession with freedom and equality.\n\nThis struggle between master and slave moralities recurs historically. According to Nietzsche, ancient Greek and Roman societies were grounded in master morality. The Homeric hero is the strong-willed man, and the classical roots of the \"Iliad\" and \"Odyssey\" exemplified Nietzsche's master morality. He calls the heroes \"men of a noble culture\", giving a substantive example of master morality. Historically, master morality was defeated, as the slave morality of Judaism and Christianity spread throughout the Roman Empire. \n\nThe essential struggle between cultures has always been between the Roman (master, strong) and the Judean (slave, weak). Nietzsche condemns the triumph of slave morality in the West, saying that the democratic movement is the \"collective degeneration of man\". He claimed that the nascent democratic movement of his time was essentially slavish and weak. Weakness conquered strength, slave conquered master, re-sentiment conquered sentiment. This ressentiment Nietzsche calls \"priestly vindictiveness\", which is based on the jealous weak seeking to enslave the strong, and thus erode the basis for power itself by pulling the powerful down. Such movements were, according to Nietzsche, inspired by \"the most intelligent revenge\" of the weak. Nietzsche saw democracy and Christianity as the same emasculating impulse which sought to make everyone equal by making everyone a slave.\n\nNietzsche did not necessarily believe that everyone should adopt master morality as the \"be-all, end-all\" behavior. He thought that the revaluation of morals would correct the inconsistencies in both master and slave moralities. But he asserted that for the individual, master morality was preferable to slave morality. Walter Kaufmann disagrees that Nietzsche actually preferred master morality to slave morality. He certainly gives slave morality a more thorough critique, but this is partly because he thought of slave morality as society's more imminent danger.\n\n\n"}
{"id": "315426", "url": "https://en.wikipedia.org/wiki?curid=315426", "title": "Mediocrity principle", "text": "Mediocrity principle\n\nThe mediocrity principle is the philosophical notion that \"if an item is drawn at random from one of several sets or categories, it's likelier to come from the most numerous category than from any one of the less numerous categories\". The principle has been taken to suggest that there is nothing very unusual about the evolution of the Solar System, Earth's history, the evolution of biological complexity, human evolution, or any one nation. It is a heuristic in the vein of the Copernican principle, and is sometimes used as a philosophical statement about the place of humanity. The idea is to assume mediocrity, rather than starting with the assumption that a phenomenon is special, privileged, exceptional, or even superior.\n\nThe mediocrity principle suggests, given the existence of life on Earth, that life typically exists on Earth-like planets throughout the universe.\n\nThe mediocrity principle is in contrast with the anthropic principle, which asserts that the presence of an intelligent observer (humans) limits the circumstances to bounds under which intelligent life can be observed to exist, no matter how improbable. Both stand in contrast to the fine-tuning hypothesis, which asserts that the natural conditions for intelligent life are implausibly rare.\n\nThe mediocrity principle implies that Earth-like environments are necessarily common, based in part on the evidence of any happening at all, whereas the anthropic principle suggests that no assertion can be made about the probability of intelligent life based on a sample set of one (self-described) example, who are necessarily capable of making such an assertion about themselves.\n\nIt is also possible to handle the Mediocrity Principle as a statistical problem, a case of a single Data point statistics, also present in the German tank problem.\n\nDavid Deutsch argues that the mediocrity principle is incorrect from a physical point of view, in reference to either humanity's part of the universe or to its species. Deutsch refers to Stephen Hawking's quote: \"The human race is just a chemical scum on a moderate-sized planet, orbiting around a very average star in the outer suburb of one among a hundred billion galaxies\". Deutsch wrote that Earth's neighborhood in the universe is not typical (80% of the universe's matter is dark matter) and that a concentration of mass such as the Solar System is an \"isolated, uncommon phenomenon\". He also disagrees with Richard Dawkins, who considers that humans, because of natural evolution, are limited to the capabilities of their species. Deutsch responds that even though evolution did not give humans the ability to detect neutrinos, scientists can currently detect them, which significantly expands their capabilities beyond what is available as a result of evolution.\n\n"}
{"id": "599917", "url": "https://en.wikipedia.org/wiki?curid=599917", "title": "Mental image", "text": "Mental image\n\nA mental image or mental picture is the representation in a person's mind of the physical world outside that person. It is an experience that, on most occasions, significantly resembles the experience of perceiving some object, event, or scene, but occurs when the relevant object, event, or scene is not actually present to the senses. There are sometimes episodes, particularly on falling asleep (hypnagogic imagery) and waking up (hypnopompic), when the mental imagery, being of a rapid, phantasmagoric and involuntary character, defies perception, presenting a kaleidoscopic field, in which no distinct object can be discerned. Mental imagery can sometimes produce the same effects as would be produced by the behavior or experience imagined.\n\nThe nature of these experiences, what makes them possible, and their function (if any) have long been subjects of research and controversy in philosophy, psychology, cognitive science, and, more recently, neuroscience. As contemporary researchers use the expression, mental images or imagery can comprise information from any source of sensory input; one may experience auditory images, olfactory images, and so forth. However, the majority of philosophical and scientific investigations of the topic focus upon \"visual\" mental imagery. It has sometimes been assumed that, like humans, some types of animals are capable of experiencing mental images. Due to the fundamentally introspective nature of the phenomenon, there is little to no evidence either for or against this view.\n\nPhilosophers such as George Berkeley and David Hume, and early experimental psychologists such as Wilhelm Wundt and William James, understood ideas in general to be mental images. Today it is very widely believed that much imagery functions as mental representations (or mental models), playing an important role in memory and thinking. William Brant (2013, p. 12) traces the scientific use of the phrase \"mental images\" back to John Tyndall's 1870 speech called the \"Scientific Use of the Imagination\". Some have gone so far as to suggest that images are best understood to be, by definition, a form of inner, mental or neural representation; in the case of hypnagogic and hypnapompic imagery, it is not representational at all. Others reject the view that the image experience may be identical with (or directly caused by) any such representation in the mind or the brain, but do not take account of the non-representational forms of imagery.\n\nIn 2010, IBM applied for a patent on a method to extract mental images of human faces from the human brain. It uses a feedback loop based on brain measurements of the fusiform face area in the brain that activates proportionate with degree of facial recognition. It was issued in 2015.\n\nThe notion of a \"mind's eye\" goes back at least to Cicero's reference to mentis oculi during his discussion of the orator's appropriate use of simile.\n\nIn this discussion, Cicero observed that allusions to \"the Syrtis of his patrimony\" and \"the Charybdis of his possessions\" involved similes that were \"too far-fetched\"; and he advised the orator to, instead, just speak of \"the rock\" and \"the gulf\" (respectively)—on the grounds that \"the eyes of the mind are more easily directed to those objects which we have seen, than to those which we have only heard\".\n\nThe concept of \"the mind's eye\" first appeared in English in Chaucer's (c. 1387) Man of Law's Tale in his \"Canterbury Tales\", where he tells us that one of the three men dwelling in a castle was blind, and could only see with \"the eyes of his mind\"; namely, those eyes \"with which all men see after they have become blind\".\n\nThe biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes:\nThe visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye – to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex - all higher cognitive processing centers of the brain.\n\nThe rudiments of a biological basis for the mind's eye is found in the deeper portions of the brain below the neocortex, or where the center of perception exists. The thalamus has been found to be discrete to other components in that it processes all forms of perceptional data relayed from both lower and higher components of the brain. Damage to this component can produce permanent perceptual damage, however when damage is inflicted upon the cerebral cortex, the brain adapts to neuroplasticity to amend any occlusions for perception. It can be thought that the neocortex is a sophisticated memory storage warehouse in which data received as an input from sensory systems are compartmentalized via the cerebral cortex. This would essentially allow for shapes to be identified, although given the lack of filtering input produced internally, one may as a consequence, hallucinate - essentially seeing something that isn't received as an input externally but rather internal (i.e. an error in the filtering of segmented sensory data from the cerebral cortex may result in one seeing, feeling, hearing or experiencing something that is inconsistent with reality).\n\nNot all people have the same internal perceptual ability. For many, when the eyes are closed, the perception of darkness prevails. However, some people are able to perceive colorful, dynamic imagery. The use of hallucinogenic drugs increases the subject's ability to consciously access visual (and auditory, and other sense) percepts.\n\nFurthermore, the pineal gland is a hypothetical candidate for producing a mind's eye; Rick Strassman and others have postulated that during near-death experiences (NDEs) and dreaming, the gland might secrete a hallucinogenic chemical \"N\",\"N\"-Dimethyltryptamine (DMT) to produce internal visuals when external sensory data is occluded. However, this hypothesis has yet to be fully supported with neurochemical evidence and plausible mechanism for DMT production.\n\nThe hypothesized condition where a person lacks a mind's eye is called aphantasia. The term was first suggested in a 2015 study.\n\nCommon examples of mental images include daydreaming and the mental visualization that occurs while reading a book. Another is of the pictures summoned by athletes during training or before a competition, outlining each step they will take to accomplish their goal. When a musician hears a song, he or she can sometimes \"see\" the song notes in their head, as well as hear them with all their tonal qualities. This is considered different from an after-effect, such as an after-image. Calling up an image in our minds can be a voluntary act, so it can be characterized as being under various degrees of conscious control.\n\nAccording to psychologist and cognitive scientist Steven Pinker, our experiences of the world are represented in our minds as mental images. These mental images can then be associated and compared with others, and can be used to synthesize completely new images. In this view, mental images allow us to form useful theories of how the world works by formulating likely sequences of mental images in our heads without having to directly experience that outcome. Whether other creatures have this capability is debatable.\n\nThere are several theories as to how mental images are formed in the mind. These include the dual-code theory, the propositional theory, and the functional-equivalency hypothesis. The dual-code theory, created by Allan Paivio in 1971, is the theory that we use two separate codes to represent information in our brains: image codes and verbal codes. Image codes are things like thinking of a picture of a dog when you are thinking of a dog, whereas a verbal code would be to think of the word \"dog\". Another example is the difference between thinking of abstract words such as \"justice\" or \"love\" and thinking of concrete words like \"elephant\" or \"chair.\" When abstract words are thought of, it is easier to think of them in terms of verbal codes—finding words that define them or describe them. With concrete words, it is often easier to use image codes and bring up a picture of a \"human\" or \"chair\" in your mind rather than words associated or descriptive of them.\n\nThe propositional theory involves storing images in the form of a generic propositional code that stores the meaning of the concept not the image itself. The propositional codes can either be descriptive of the image or symbolic. They are then transferred back into verbal and visual code to form the mental image.\n\nThe functional-equivalency hypothesis is that mental images are \"internal representations\" that work in the same way as the actual perception of physical objects. In other words, the picture of a dog brought to mind when the word \"dog\" is read is interpreted in the same way as if the person looking at an actual dog before them.\n\nResearch has occurred to designate a specific neural correlate of imagery; however, studies show a multitude of results. Most studies published before 2001 suggest neural correlates of visual imagery occur in brodmann area 17. Auditory performance imagery have been observed in the premotor areas, precunes, and medial brodmann area 40. Auditory imagery in general occurs across participants in the temporal voice area (TVA), which allows top-down imaging manipulations, processing, and storage of audition functions. Olfactory imagery research shows activation in the anterior piriform cortex and the posterior piriform cortex; experts in olfactory imagery have larger gray matter associated to olfactory areas. Tactile imagery is found to occur in the dorsolateral prefrontal area, inferior frontal gyrus, frontal gyrus, insula, precentral gyrus, and the medial frontal gyrus with basil ganglia activation in the ventral posteriomedial nucleus and putamen (hemisphere activation corresponds to the location of the imagined tactile stimulus). Research in gustatory imagery reveals activation in the anterior insular cortex, frontal operculum, and prefrontal cortex. Novices of a specific form of mental imagery show less gray matter than experts of mental imagery congruent to that form. A meta-analysis of neuroimagery studies revealed significant activation of the bilateral dorsal parietal, interior insula, and left inferior frontal regions of the brain.\n\nImagery has been thought to cooccur with perception; however, participants with damaged sense-modality receptors can sometimes perform imagery of said modality receptors. Neuroscience with imagery has been used to communicate with seemingly unconscious individuals through fMRI activation of different neural correlates of imagery, demanding further study into low quality consciousness. A study on one patient with one occipital lobe removed found the horizontal area of their visual mental image was reduced.\n\nVisual imagery is the ability to create mental representations of things, people, and places that are absent from an individual’s visual field. This ability is crucial to problem-solving tasks, memory, and spatial reasoning. Neuroscientists have found that imagery and perception share many of the same neural substrates, or areas of the brain that function similarly during both imagery and perception, such as the visual cortex and higher visual areas. Kosslyn and colleagues (1999) showed that the early visual cortex, Area 17 and Area 18/19, is activated during visual imagery. They found that inhibition of these areas through repetitive transcranial magnetic stimulation (rTMS) resulted in impaired visual perception and imagery. Furthermore, research conducted with lesioned patients has revealed that visual imagery and visual perception have the same representational organization. This has been concluded from patients in which impaired perception also experience visual imagery deficits at the same level of the mental representation.\n\nBehrmann and colleagues (1992) describe a patient C.K., who provided evidence challenging the view that visual imagery and visual perception rely on the same representational system. C.K. was a 33-year old man with visual object agnosia acquired after a vehicular accident. This deficit prevented him from being able to recognize objects and copy objects fluidly. Surprisingly, his ability to draw accurate objects from memory indicated his visual imagery was intact and normal. Furthermore, C.K. successfully performed other tasks requiring visual imagery for judgment of size, shape, color, and composition. These findings conflict with previous research as they suggest there is a partial dissociation between visual imagery and visual perception. C.K. exhibited a perceptual deficit that was not associated with a corresponding deficit in visual imagery, indicating that these two processes have systems for mental representations that may not be mediated entirely by the same neural substrates. \n\nSchlegel and colleagues (2013) conducted a functional MRI analysis of regions activated during manipulation of visual imagery. They identified 11 bilateral cortical and subcortical regions that exhibited increased activation when manipulating a visual image compared to when the visual image was just maintained. These regions included the occipital lobe and ventral stream areas, two parietal lobe regions, the posterior parietal cortex and the precuneus lobule, and three frontal lobe regions, the frontal eye fields, dorsolateral prefrontal cortex, and the prefrontal cortex. Due to their suspected involvement in working memory and attention, the authors propose that these parietal and prefrontal regions, and occipital regions, are part of a network involved in mediating the manipulation of visual imagery. These results suggest a top-down activation of visual areas in visual imagery.\n\nUsing Dynamic Causal Modeling (DCM) to determine the connectivity of cortical networks, Ishai et al. (2010) demonstrated that activation of the network mediating visual imagery is initiated by prefrontal cortex and posterior parietal cortex activity. Generation of objects from memory resulted in initial activation of the prefrontal and the posterior parietal areas, which then activate earlier visual areas through backward connectivity. Activation of the prefrontal cortex and posterior parietal cortex has also been found to be involved in retrieval of object representations from long-term memory, their maintenance in working memory, and attention during visual imagery. Thus, Ishai et al. suggest that the network mediating visual imagery is composed of attentional mechanisms arising from the posterior parietal cortex and the prefrontal cortex.\n\nVividness of visual imagery is a crucial component of an individual’s ability to perform cognitive tasks requiring imagery. Vividness of visual imagery varies not only between individuals but also within individuals. Dijkstra and colleagues (2017) found that the variation in vividness of visual imagery is dependent on the degree to which the neural substrates of visual imagery overlap with those of visual perception. They found that overlap between imagery and perception in the entire visual cortex, the parietal precuneus lobule, the right parietal cortex, and the medial frontal cortex predicted the vividness of a mental representation. The activated regions beyond the visual areas are believed to drive the imagery-specific processes rather than the visual processes shared with perception. It has been suggested that the precuneus contributes to vividness by selecting important details for imagery. The medial frontal cortex is suspected to be involved in the retrieval and integration of information from the parietal and visual areas during working memory and visual imagery. The right parietal cortex appears to be important in attention, visual inspection, and stabilization of mental representations. Thus, the neural substrates of visual imagery and perception overlap in areas beyond the visual cortex and the degree of this overlap in these areas correlates with the vividness of mental representations during imagery.\n\nMental images are an important topic in classical and modern philosophy, as they are central to the study of knowledge. In the \"Republic\", Book VII, Plato has Socrates present the Allegory of the Cave: a prisoner, bound and unable to move, sits with his back to a fire watching the shadows cast on the cave wall in front of him by people carrying objects behind his back. These people and the objects they carry are representations of real things in the world. Unenlightened man is like the prisoner, explains Socrates, a human being making mental images from the sense data that he experiences.\n\nThe eighteenth-century philosopher Bishop George Berkeley proposed similar ideas in his theory of idealism. Berkeley stated that reality is equivalent to mental images—our mental images are not a copy of another material reality but that reality itself. Berkeley, however, sharply distinguished between the images that he considered to constitute the external world, and the images of individual imagination. According to Berkeley, only the latter are considered \"mental imagery\" in the contemporary sense of the term.\n\nThe eighteenth century British writer Dr. Samuel Johnson criticized idealism. When asked what he thought about idealism, he is alleged to have replied \"I refute it thus!\" as he kicked a large rock and his leg rebounded. His point was that the idea that the rock is just another mental image and has no material existence of its own is a poor explanation of the painful sense data he had just experienced.\n\nDavid Deutsch addresses Johnson's objection to idealism in \"The Fabric of Reality\" when he states that, if we judge the value of our mental images of the world by the quality and quantity of the sense data that they can explain, then the most valuable mental image—or theory—that we currently have is that the world has a real independent existence and that humans have successfully evolved by building up and adapting patterns of mental images to explain it. This is an important idea in scientific thought.\n\nCritics of scientific realism ask how the inner perception of mental images actually occurs. This is sometimes called the \"homunculus problem\" (see also the mind's eye). The problem is similar to asking how the images you see on a computer screen exist in the memory of the computer. To scientific materialism, mental images and the perception of them must be brain-states. According to critics, scientific realists cannot explain where the images and their perceiver exist in the brain. To use the analogy of the computer screen, these critics argue that cognitive science and psychology have been unsuccessful in identifying either the component in the brain (i.e., \"hardware\") or the mental processes that store these images (i.e. \"software\").\n\nCognitive psychologists and (later) cognitive neuroscientists have empirically tested some of the philosophical questions related to whether and how the human brain uses mental imagery in cognition.\n\nOne theory of the mind that was examined in these experiments was the \"brain as serial computer\" philosophical metaphor of the 1970s. Psychologist Zenon Pylyshyn theorized that the human mind processes mental images by decomposing them into an underlying mathematical proposition. Roger Shepard and Jacqueline Metzler challenged that view by presenting subjects with 2D line drawings of groups of 3D block \"objects\" and asking them to determine whether that \"object\" is the same as a second figure, some of which rotations of the first \"object\". Shepard and Metzler proposed that if we decomposed and then mentally re-imaged the objects into basic mathematical propositions, as the then-dominant view of cognition \"as a serial digital computer\" assumed, then it would be expected that the time it took to determine whether the object is the same or not would be independent of how much the object had been rotated. Shepard and Metzler found the opposite: a linear relationship between the degree of rotation in the mental imagery task and the time it took participants to reach their answer.\n\nThis mental rotation finding implied that the human mind—and the human brain—maintains and manipulates mental images as topographic and topological wholes, an implication that was quickly put to test by psychologists. Stephen Kosslyn and colleagues showed in a series of neuroimaging experiments that the mental image of objects like the letter \"F\" are mapped, maintained and rotated as an image-like whole in areas of the human visual cortex. Moreover, Kosslyn's work showed that there are considerable similarities between the neural mappings for imagined stimuli and perceived stimuli. The authors of these studies concluded that, while the neural processes they studied rely on mathematical and computational underpinnings, the brain also seems optimized to handle the sort of mathematics that constantly computes a series of topologically-based images rather than calculating a mathematical model of an object.\n\nRecent studies in neurology and neuropsychology on mental imagery have further questioned the \"mind as serial computer\" theory, arguing instead that human mental imagery manifests both visually and kinesthetically. For example, several studies have provided evidence that people are slower at rotating line drawings of objects such as hands in directions incompatible with the joints of the human body, and that patients with painful, injured arms are slower at mentally rotating line drawings of the hand from the side of the injured arm.\n\nSome psychologists, including Kosslyn, have argued that such results occur because of interference in the brain between distinct systems in the brain that process the visual and motor mental imagery. Subsequent neuroimaging studies showed that the interference between the motor and visual imagery system could be induced by having participants physically handle actual 3D blocks glued together to form objects similar to those depicted in the line-drawings. Amorim et al. have shown that, when a cylindrical \"head\" was added to Shepard and Metzler's line drawings of 3D block figures, participants were quicker and more accurate at solving mental rotation problems. They argue that motoric embodiment is not just \"interference\" that inhibits visual mental imagery but is capable of facilitating mental imagery.\n\nAs cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain’s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain’s visual areas while subjects imagined visual objects and scenes.\n\nThe previously mentioned and numerous related studies have led to a relative consensus within cognitive science, psychology, neuroscience, and philosophy on the neural status of mental images. In general, researchers agree that, while there is no homunculus inside the head viewing these mental images, our brains do form and maintain mental images as image-like wholes. The problem of exactly how these images are stored and manipulated within the human brain, in particular within language and communication, remains a fertile area of study.\n\nOne of the longest-running research topics on the mental image has basis on the fact that people report large individual differences in the vividness of their images. Special questionnaires have been developed to assess such differences, including the Vividness of Visual Imagery Questionnaire (VVIQ) developed by David Marks. Laboratory studies have suggested that the subjectively reported variations in imagery vividness are associated with different neural states within the brain and also different cognitive competences such as the ability to accurately recall information presented in pictures Rodway, Gillies and Schepman used a novel long-term change detection task to determine whether participants with low and high vividness scores on the VVIQ2 showed any performance differences. Rodway et al. found that high vividness participants were significantly more accurate at detecting salient changes to pictures compared to low-vividness participants. This replicated an earlier study.\n\nRecent studies have found that individual differences in VVIQ scores can be used to predict changes in a person's brain while visualizing different activities. Functional magnetic resonance imaging (fMRI) was used to study the association between early visual cortex activity relative to the whole brain while participants visualized themselves or another person bench pressing or stair climbing. Reported image vividness correlates significantly with the relative fMRI signal in the visual cortex. Thus, individual differences in the vividness of visual imagery can be measured objectively.\n\nLogie, Pernet, Buonocore and Della Sala (2011) used behavioural and fMRI data for mental rotation from individuals reporting vivid and poor imagery on the VVIQ. Groups differed in brain activation patterns suggesting that the groups performed the same tasks in different ways. These findings help to explain the lack of association previously reported between VVIQ scores and mental rotation performance.\n\nSome educational theorists have drawn from the idea of mental imagery in their studies of learning styles. Proponents of these theories state that people often have learning processes that emphasize visual, auditory, and kinesthetic systems of experience. According to these theorists, teaching in multiple overlapping sensory systems benefits learning, and they encourage teachers to use content and media that integrates well with the visual, auditory, and kinesthetic systems whenever possible.\n\nEducational researchers have examined whether the experience of mental imagery affects the degree of learning. For example, imagining playing a 5-finger piano exercise (mental practice) resulted in a significant improvement in performance over no mental practice—though not as significant as that produced by physical practice. The authors of the study stated that \"mental practice alone seems to be sufficient to promote the modulation of neural circuits involved in the early stages of motor skill learning\".\n\nIn general, Vajrayana Buddhism, Bön, and Tantra utilize sophisticated visualization or \"imaginal\" (in the language of Jean Houston of Transpersonal Psychology) processes in the thoughtform construction of the yidam sadhana, kye-rim, and dzog-rim modes of meditation and in the yantra, thangka, and mandala traditions, where holding the fully realized form in the mind is a prerequisite prior to creating an 'authentic' new art work that will provide a sacred support or foundation for deity.\n\nMental imagery can act as a substitute for the imagined experience: Imagining an experience can evoke similar cognitive, physiological, and/or behavioral consequences as having the corresponding experience in reality. At least four classes of such effects have been documented.\n\n"}
{"id": "994704", "url": "https://en.wikipedia.org/wiki?curid=994704", "title": "Mental model", "text": "Mental model\n\nA mental model is an explanation of someone's thought process about how something works in the real world. It is a representation of the surrounding world, the relationships between its various parts and a person's intuitive perception about his or her own acts and their consequences. Mental models can help shape behaviour and set an approach to solving problems (similar to a personal algorithm) and doing tasks.\n\nA mental model is a kind of internal symbol or representation of external reality, hypothesized to play a major role in cognition, reasoning and decision-making. Kenneth Craik suggested in 1943 that the mind constructs \"small-scale models\" of reality that it uses to anticipate events.\n\nJay Wright Forrester defined general mental models as:\nThe image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system (Forrester, 1971).\n\nIn psychology, the term \"mental models\" is sometimes used to refer to mental representations or mental simulation generally. At other times it is used to refer to and to the mental model theory of reasoning developed by Philip Johnson-Laird and Ruth M.J. Byrne.\n\nThe term \"mental model\" is believed to have originated with Kenneth Craik in his 1943 book \"The Nature of Explanation\". in \"Le dessin enfantin\" (Children's drawings), published in 1927 by Alcan, Paris, argued that children construct internal models, a view that influenced, among others, child psychologist Jean Piaget.\n\nPhilip Johnson-Laird published \"Mental Models: Towards a Cognitive Science of Language, Inference and Consciousness\" in 1983. In the same year, Dedre Gentner and Albert Stevens edited a collection of chapters in a book also titled \"Mental Models\". The first line of their book explains the idea further: \"One function of this chapter is to belabor the obvious; people's views of the world, of themselves, of their own capabilities, and of the tasks that they are asked to perform, or topics they are asked to learn, depend heavily on the conceptualizations that they bring to the task.\" (see the book: \"Mental Models\").\n\nSince then, there has been much discussion and use of the idea in human-computer interaction and usability by researchers including Donald Norman and Steve Krug (in his book \"Don't Make Me Think\"). Walter Kintsch and Teun A. van Dijk, using the term \"situation model\" (in their book \"Strategies of Discourse Comprehension\", 1983), showed the relevance of mental models for the production and comprehension of discourse.\n\nOne view of human reasoning is that it depends on mental models. In this view, mental models can be constructed from perception, imagination, or the comprehension of discourse (Johnson-Laird, 1983). Such mental models are similar to architects' models or to physicists' diagrams in that their structure is analogous to the structure of the situation that they represent, unlike, say, the structure of logical forms used in formal rule theories of reasoning. In this respect, they are a little like pictures in the picture theory of language described by philosopher Ludwig Wittgenstein in 1922. Philip Johnson-Laird and Ruth M.J. Byrne developed a theory of mental models which makes the assumption that reasoning depends, not on logical form, but on mental models (Johnson-Laird and Byrne, 1991).\n\nMental models are based on a small set of fundamental assumptions (axioms), which distinguish them from other proposed representations in the psychology of reasoning (Byrne and Johnson-Laird, 2009). Each mental model represents a possibility. A mental model represents one possibility, capturing what is common to all the different ways in which the possibility may occur (Johnson-Laird and Byrne, 2002). Mental models are iconic, i.e., each part of a model corresponds to each part of what it represents (Johnson-Laird, 2006). Mental models are based on a principle of truth: they typically represent only those situations that are possible, and each model of a possibility represents only what is true in that possibility according to the proposition. However, mental models can represent what is false, temporarily assumed to be true, for example, in the case of counterfactual conditionals and counterfactual thinking (Byrne, 2005).\n\nPeople infer that a conclusion is valid if it holds in all the possibilities. Procedures for reasoning with mental models rely on counter-examples to refute invalid inferences; they establish validity by ensuring that a conclusion holds over all the models of the premises. Reasoners focus on a subset of the possible models of multiple-model problems, often just a single model. The ease with which reasoners can make deductions is affected by many factors, including age and working memory (Barrouillet, et al., 2000). They reject a conclusion if they find a counterexample, i.e., a possibility in which the premises hold, but the conclusion does not (Schroyens, et al. 2003; Verschueren, et al., 2005).\n\nScientific debate continues about whether human reasoning is based on mental models, versus formal rules of inference (e.g., O'Brien, 2009), domain-specific rules of inference (e.g., Cheng & Holyoak, 2008; Cosmides, 2005), or probabilities (e.g., Oaksford and Chater, 2007). Many empirical comparisons of the different theories have been carried out (e.g., Oberauer, 2006).\n\nA mental model is generally:\n\nMental models are a fundamental way to understand organizational learning. Mental models, in popular science parlance, have been described as \"deeply held images of thinking and acting\". Mental models are so basic to understanding the world that people are hardly conscious of them.\n\nS.N. Groesser and M. Schaffernicht (2012) describe three basic methods which are typically used:\nThese methods allow showing a mental model of a dynamic system, as an explicit, written model about a certain system based on internal beliefs. Analyzing these graphical representations has been an increasing area of research across many social science fields. Additionally software tools that attempt to capture and analyze the structural and functional properties of individual mental models such as Mental Modeler, \"a participatory modeling tool based in fuzzy-logic cognitive mapping\", have recently been developed and used to collect/compare/combine mental model representations collected from individuals for use in social science research, collaborative decision-making, and natural resource planning.\n\nIn the simplification of reality, creating a model can find a sense of reality, seeking to overcome systemic thinking and system dynamics.\n\nThese two disciplines can help to construct a better coordination with the reality of mental models and simulate it accurately. They increase the probability that the consequences of how to decide and act in accordance with how to plan.\n\n\nAfter analyzing the basic characteristics, it is necessary to bring the process of changing the mental models, or the process of learning. Learning is a back-loop process, and feedback loops can be illustrated as: single-loop learning or double-loop learning.\n\nMental models affect the way that people work with information, and also how they determine the final decision. The decision itself changes, but the mental models remain the same. It is the predominant method of learning, because it is very convenient.\n\nDouble-loop learning (\"see diagram below\") is used when it is necessary to change the mental model on which a decision depends. Unlike single loops, this model includes a shift in understanding, from simple and static to broader and more dynamic, such as taking into account the changes in the surroundings and the need for expression changes in mental models.\n\n\n\n"}
{"id": "1684561", "url": "https://en.wikipedia.org/wiki?curid=1684561", "title": "Method of loci", "text": "Method of loci\n\nThe method of loci (\"loci\" being Latin for \"places\") is a method of memory enhancement which uses visualizations with the use of spatial memory, familiar information about one's environment, to quickly and efficiently recall information. The method of loci is also known as the memory journey, memory palace, or mind palace technique. This method is a mnemonic device adopted in ancient Roman and Greek rhetorical treatises (in the anonymous \"Rhetorica ad Herennium\", Cicero's \"De Oratore\", and Quintilian's \"Institutio Oratoria\"). Many memory contest champions claim to use this technique to recall faces, digits, and lists of words.\n\nThe term is most often found in specialised works on psychology, neurobiology, and memory, though it was used in the same general way at least as early as the first half of the nineteenth century in works on rhetoric, logic, and philosophy. John O'Keefe and Lynn Nadel refer to:'the method of loci', an imaginal technique known to the ancient Greeks and Romans and described by Yates (1966) in her book \"The Art of Memory\" as well as by Luria (1969). In this technique the subject memorizes the layout of some building, or the arrangement of shops on a street, or any geographical entity which is composed of a number of discrete loci. When desiring to remember a set of items the subject 'walks' through these loci in their imagination and commits an item to each one by forming an image between the item and any feature of that locus. Retrieval of items is achieved by 'walking' through the loci, allowing the latter to activate the desired items. The efficacy of this technique has been well established (Ross and Lawrence 1968, Crovitz 1969, 1971, Briggs, Hawkins and Crovitz 1970, Lea 1975), as is the minimal interference seen with its use.\n\nThe items to be remembered in this mnemonic system are mentally associated with specific physical locations. The method relies on memorized spatial relationships to establish order and recollect memorial content. It is also known as the \"Journey Method\", used for storing lists of related items, or the \"Roman Room\" technique, which is most effective for storing unrelated information.\n\nMany effective memorisers today use the \"method of loci\" to some degree. Contemporary memory competition, in particular the World Memory Championship, was initiated in 1991 and the first United States championship was held in 1997. Part of the competition requires committing to memory and recalling a sequence of digits, two-digit numbers, alphabetic letters, or playing cards. In a simple method of doing this, contestants, using various strategies well before competing, commit to long-term memory a unique vivid image associated with each item. They have also committed to long-term memory a familiar route with firmly established stop-points or loci. Then in the competition they need only deposit the image that they have associated with each item at the loci. To recall, they retrace the route, \"stop\" at each locus, and \"observe\" the image. They then translate this back to the associated item. For example, Ed Cooke, a World Memory Champion Competitor, describes to Josh Foer in his book \"Moonwalking with Einstein\" how he uses the method of loci. First, he describes a very familiar location where he can clearly remember many different smaller locations like his sink in his childhood home or his dog's bed. Cooke also advises that the more outlandish and vulgar the symbol used to memorize the material, the more likely it will stick.\n\nMemory champions elaborate on this by combining images. Eight-time World Memory Champion Dominic O'Brien uses this technique. The 2006 World Memory Champion, Clemens Mayer, used a 300-point-long journey through his house for his world record in \"number half marathon\", memorising 1040 random digits in a half-hour. Gary Shang has used the method of loci to memorise pi to over 65,536 (2) digits.\n\nUsing this technique a person with ordinary memorisation capabilities, after establishing the route stop-points and committing the associated images to long-term memory, with less than an hour of practice, can remember the sequence of a shuffled deck of cards. The world record for this is held by Simon Reinhard at 21.19 seconds.\n\nThe technique is taught as a metacognitive technique in learning-to-learn courses. It is generally applied to encoding the key ideas of a subject. Two approaches are:\n\nThe method of loci has also been shown to help sufferers of depression remember positive, self-affirming memories.\n\nA study at the University of Maryland evaluated participants ability to accurately recall two sets of familiar faces, using a traditional desktop, and with a head-mounted display. The study was designed to leverage the method of loci technique, with virtual environments resembling memory palaces. The study found an 8.8% recall improvement in favor of the head-mounted display, in part due to participants being able to leverage their vestibular and proprioceptive sensations.\n\nThe \"Rhetorica ad Herennium\" and most other sources recommend that the method of loci should be integrated with elaborative encoding (i.e., adding visual, auditory, or other details) to strengthen memory. However, due to the strength of spatial memory, simply mentally placing objects in real or imagined locations without further elaboration can be effective for simple associations.\n\nA variation of the \"method of loci\" involves creating imaginary locations (houses, palaces, roads, and cities) to which the same procedure is applied. It is accepted that there is a greater cost involved in the initial setup, but thereafter the performance is in line with the standard loci method. The purported advantage is to create towns and cities that each represent a topic or an area of study, thus offering an efficient filing of the information and an easy path for the regular review necessary for long term memory storage.\n\nSomething that is likely a reference to the \"method of loci\" techniques survives to this day in the common English phrases \"in the first place\", \"in the second place\", and so forth.\n\nThe technique is also used for second language vocabulary learning, as polyglot Timothy Doner described in his 2014 TED talk. The method is further described in Anthony Metiver's book \"How to learn and memorise German vocabulary\". What the author suggests is creating a memory palace for each letter of the German alphabet. Each memory palace then shall include a number of loci where an entry (a word or a phrase) can be stored and recalled whenever you need it.\n\nThe designation is not used with strict consistency. In some cases it refers broadly to what is otherwise known as the art of memory, the origins of which are related, according to tradition, in the story of Simonides of Ceos and the collapsing banquet hall. For example, after relating the story of how Simonides relied on remembered seating arrangements to call to mind the faces of recently deceased guests, Stephen M. Kosslyn remarks \"[t]his insight led to the development of a technique the Greeks called the method of loci, which is a systematic way of improving one's memory by using imagery.\" Skoyles and Sagan indicate that \"an ancient technique of memorization called Method of Loci, by which memories are referenced directly onto spatial maps\" originated with the story of Simonides. Referring to mnemonic methods, Verlee Williams mentions, \"One such strategy is the 'loci' method, which was developed by Simonides, a Greek poet of the fifth and sixth centuries BC.\" Loftus cites the foundation story of Simonides (more or less taken from Frances Yates) and describes some of the most basic aspects of the use of space in the art of memory. She states, \"This particular mnemonic technique has come to be called the \"method of loci\". While place or position certainly figured prominently in ancient mnemonic techniques, no designation equivalent to \"method of loci\" was used exclusively to refer to mnemonic schemes relying upon space for organization.\n\nIn other cases the designation is generally consistent, but more specific: \"The Method of Loci is a Mnemonic Device involving the creation of a Visual Map of one's house.\"\n\nThis term can be misleading: the ancient principles and techniques of the art of memory, hastily glossed in some of the works, cited above, depended equally upon images \"and\" places. The designator \"method of loci\" does not convey the equal weight placed on both elements. Training in the art or arts of memory as a whole, as attested in classical antiquity, was far more inclusive and comprehensive in the treatment of this subject.\n\nBrain scans of \"superior memorizers\", 90% of whom use the method of loci technique, have shown that it involves activation of regions of the brain involved in spatial awareness, such as the medial parietal cortex, retrosplenial cortex, and the right posterior hippocampus. The medial parietal cortex is most associated with encoding and retrieving of information. Patients who have medial parietal cortex damage have trouble linking landmarks with certain locations; many of these patients are unable to give or follow directions and often get lost. The retrosplenial cortex is also linked to memory and navigation. In one study on the effects of selective granular retrosplenial cortex lesions in rats, the researcher found that damage to the retrosplenial cortex led to impaired spatial learning abilities. Rats with damage to this area failed to recall which areas of the maze they had already visited, rarely explored different arms of the maze, almost never recalled the maze in future trials, and took longer to reach the end of the maze, as compared to rats with a fully working retrosplenial cortex.\n\nIn a classic study in cognitive neuroscience, O'Keefe and Nadel proposed \"that the hippocampus is the core of a neural memory system providing an objective spatial framework within which the items and events of an organism's experience are located and interrelated.\"\n\nIn a more recent study, memory champions during resting periods did not exhibit specific regional brain differences, but distributed functional brain network connectivity changes compared to control subjects. When volunteers trained use of the method of loci for six weeks, the training-induced changes in brain connectivity were similar to the brain network organization that distinguished memory champions from controls.\n\nFictional portrayals of the method of loci extend as far back as ancient Greek myths. The method of loci also features prominently in the BBC series \"Sherlock\", in which the titular main character uses a \"mind palace\" to store information. In the original Arthur Conan Doyle stories, Sherlock Holmes referred to his brain as an attic. In \"Hannibal Rising\" by Thomas Harris, a detailed description of Hannibal Lecter's memory palace is provided.\n"}
{"id": "154616", "url": "https://en.wikipedia.org/wiki?curid=154616", "title": "Negative number", "text": "Negative number\n\nIn mathematics, a negative number is a real number that is less than zero. Negative numbers represent opposites. If positive represents a movement to the right, negative represents a movement to the left. If positive represents above sea level, then negative represents below sea level. If positive represents a deposit, negative represents a withdrawal. They are often used to represent the magnitude of a loss or deficiency. A debt that is owed may be thought of as a negative asset, a decrease in some quantity may be thought of as a negative increase. If a quantity may have either of two opposite senses, then one may choose to distinguish between those senses—perhaps arbitrarily—as \"positive\" and \"negative\". In the medical context of fighting a tumor, an expansion could be thought of as a negative shrinkage. Negative numbers are used to describe values on a scale that goes below zero, such as the Celsius and Fahrenheit scales for temperature. The laws of arithmetic for negative numbers ensure that the common sense idea of an opposite is reflected in arithmetic. For example, −(−3) = 3 because the opposite of an opposite is the original value.\n\nNegative numbers are usually written with a minus sign in front. For example, −3 represents a negative quantity with a magnitude of three, and is pronounced \"minus three\" or \"negative three\". To help tell the difference between a subtraction operation and a negative number, occasionally the negative sign is placed slightly higher than the minus sign (as a superscript). Conversely, a number that is greater than zero is called \"positive\"; zero is usually (but not always) thought of as neither positive nor negative. The positivity of a number may be emphasized by placing a plus sign before it, e.g. . In general, the negativity or positivity of a number is referred to as its sign.\n\nEvery real number other than zero is either positive or negative. The positive whole numbers are referred to as natural numbers, while the positive and negative whole numbers (together with zero) are referred to as integers.\n\nIn bookkeeping, amounts owed are often represented by red numbers, or a number in parentheses, as an alternative notation to represent negative numbers.\n\nNegative numbers appeared for the first time in history in the \"Nine Chapters on the Mathematical Art\", which in its present form dates from the period of the Chinese Han Dynasty (202 BC – AD 220), but may well contain much older material. Liu Hui (c. 3rd century) established rules for adding and subtracting negative numbers. By the 7th century, Indian mathematicians such as Brahmagupta were describing the use of negative numbers. Islamic mathematicians further developed the rules of subtracting and multiplying negative numbers and solved problems with negative coefficients. Western mathematicians accepted the idea of negative numbers by the 17th century. Prior to the concept of negative numbers, mathematicians such as Diophantus considered negative solutions to problems \"false\" and equations requiring negative solutions were described as absurd.\n\nNegative numbers can be thought of as resulting from the subtraction of a larger number from a smaller. For example, negative three is the result of subtracting three from zero:\nIn general, the subtraction of a larger number from a smaller yields a negative result, with the magnitude of the result being the difference between the two numbers. For example,\nsince .\n\nThe relationship between negative numbers, positive numbers, and zero is often expressed in the form of a number line:\n\nNumbers appearing farther to the right on this line are greater, while numbers appearing farther to the left are less. Thus zero appears in the middle, with the positive numbers to the right and the negative numbers to the left.\n\nNote that a negative number with greater magnitude is considered less. For example, even though (positive) is greater than (positive) , written\nnegative is considered to be less than negative :\n(Because, for example, if you have £-8, a debt of £8, you would have less after adding, say £10, to it than if you have £-5.) \nIt follows that any negative number is less than any positive number, so\n\nIn the context of negative numbers, a number that is greater than zero is referred to as positive. Thus every real number other than zero is either positive or negative, while zero itself is not considered to have a sign. Positive numbers are sometimes written with a plus sign in front, e.g. denotes a positive three.\n\nBecause zero is neither positive nor negative, the term nonnegative is sometimes used to refer to a number that is either positive or zero, while nonpositive is used to refer to a number that is either negative or zero. Zero is a neutral number.\n\n\n\n\n\nThe minus sign \"−\" signifies the operator for both the binary (two-operand) operation of subtraction (as in ) and the unary (one-operand) operation of negation (as in , or twice in ). A special case of unary negation occurs when it operates on a positive number, in which case the result is a negative number (as in ).\n\nThe ambiguity of the \"−\" symbol does not generally lead to ambiguity in arithmetical expressions, because the order of operations makes only one interpretation or the other possible for each \"−\". However, it can lead to confusion and be difficult for a person to understand an expression when operator symbols appear adjacent to one another. A solution can be to parenthesize the unary \"−\" along with its operand.\n\nFor example, the expression may be clearer if written (even though they mean exactly the same thing formally). The subtraction expression is a different expression that doesn't represent the same operations, but it evaluates to the same result.\n\nSometimes in elementary schools a number may be prefixed by a superscript minus sign or plus sign to explicitly distinguish negative and positive numbers as in\n\nAddition of two negative numbers is very similar to addition of two positive numbers. For example,\nThe idea is that two debts can be combined into a single debt of greater magnitude.\n\nWhen adding together a mixture of positive and negative numbers, one can think of the negative numbers as positive quantities being subtracted. For example:\nIn the first example, a credit of is combined with a debt of , which yields a total credit of . If the negative number has greater magnitude, then the result is negative:\nHere the credit is less than the debt, so the net result is a debt.\n\nAs discussed above, it is possible for the subtraction of two non-negative numbers to yield a negative answer:\nIn general, subtraction of a positive number yields the same result as the addition of a negative number of equal magnitude. Thus\nand\n\nOn the other hand, subtracting a negative number yields the same result as the addition a positive number of equal magnitude. (The idea is that \"losing\" a debt is the same thing as \"gaining\" a credit.) Thus\nand\n\nWhen multiplying numbers, the magnitude of the product is always just the product of the two magnitudes. The sign of the product is determined by the following rules:\nThus\nand\nThe reason behind the first example is simple: adding three 's together yields :\nThe reasoning behind the second example is more complicated. The idea again is that losing a debt is the same thing as gaining a credit. In this case, losing two debts of three each is the same as gaining a credit of six:\nThe convention that a product of two negative numbers is positive is also necessary for multiplication to follow the distributive law. In this case, we know that\nSince , the product must equal .\n\nThese rules lead to another (equivalent) rule—the sign of any product \"a\" × \"b\" depends on the sign of \"a\" as follows:\nThe justification for why the product of two negative numbers is a positive number can be observed in the analysis of complex numbers.\n\nThe sign rules for division are the same as for multiplication. For example,\nand\nIf dividend and divisor have the same sign, the result is always positive. Another method of dividing negative numbers is that if one of the numbers being divided is a negative, the answer will be negative.\n\nThe negative version of a positive number is referred to as its negation. For example, is the negation of the positive number . The sum of a number and its negation is equal to zero:\nThat is, the negation of a positive number is the additive inverse of the number.\n\nUsing algebra, we may write this principle as an algebraic identity:\nThis identity holds for any positive number . It can be made to hold for all real numbers by extending the definition of negation to include zero and negative numbers. Specifically:\nFor example, the negation of is . In general,\n\nThe absolute value of a number is the non-negative number with the same magnitude. For example, the absolute value of and the absolute value of are both equal to , and the absolute value of is .\n\nIn a similar manner to rational numbers, we can extend the natural numbers N to the integers Z by defining integers as an ordered pair of natural numbers (\"a\", \"b\"). We can extend addition and multiplication to these pairs with the following rules:\n\nWe define an equivalence relation ~ upon these pairs with the following rule:\nThis equivalence relation is compatible with the addition and multiplication defined above, and we may define Z to be the quotient set N²/~, i.e. we identify two pairs (\"a\", \"b\") and (\"c\", \"d\") if they are equivalent in the above sense. Note that Z, equipped with these operations of addition and multiplication, is a ring, and is in fact, the prototypical example of a ring.\n\nWe can also define a total order on Z by writing\n\nThis will lead to an \"additive zero\" of the form (\"a\", \"a\"), an \"additive inverse\" of (\"a\", \"b\") of the form (\"b\", \"a\"), a multiplicative unit of the form (\"a\" + 1, \"a\"), and a definition of subtraction\nThis construction is a special case of the Grothendieck construction.\n\nThe negative of a number is unique, as is shown by the following proof.\n\nLet \"x\" be a number and let \"y\" be its negative.\nSuppose \"y′\" is another negative of \"x\". By an axiom of the real number system\n\nAnd so, \"x\" + \"y′\" = \"x\" + \"y\". Using the law of cancellation for addition, it is seen that\n\"y′\" = \"y\". Thus \"y\" is equal to any other negative of \"x\". That is, \"y\" is the unique negative of \"x\".\n\nFor a long time, negative solutions to problems were considered \"false\". In Hellenistic Egypt, the Greek mathematician Diophantus in the 3rd century AD referred to an equation that was equivalent to 4\"x\" + 20 = 4 (which has a negative solution) in \"Arithmetica\", saying that the equation was absurd.\n\nNegative numbers appear for the first time in history in the \"Nine Chapters on the Mathematical Art\" (\"Jiu zhang suan-shu\"), which in its present form dates from the period of the Han Dynasty (202 BC – AD 220), but may well contain much older material. The mathematician Liu Hui (c. 3rd century) established rules for the addition and subtraction of negative numbers. The historian Jean-Claude Martzloff theorized that the importance of duality in Chinese natural philosophy made it easier for the Chinese to accept the idea of negative numbers. The Chinese were able to solve simultaneous equations involving negative numbers. The \"Nine Chapters\" used red counting rods to denote positive coefficients and black rods for negative. This system is the exact opposite of contemporary printing of positive and negative numbers in the fields of banking, accounting, and commerce, wherein red numbers denote negative values and black numbers signify positive values. Liu Hui writes:\n\nThe ancient Indian \"Bakhshali Manuscript\" carried out calculations with negative numbers, using \"+\" as a negative sign. The date of the manuscript is uncertain. LV Gurjar dates it no later than the 4th century, Hoernle dates it between the third and fourth centuries, Ayyangar and Pingree dates it to the 8th or 9th centuries, and George Gheverghese Joseph dates it to about AD 400 and no later than the early 7th century,\n\nDuring the 7th century AD, negative numbers were used in India to represent debts. The Indian mathematician Brahmagupta, in \"Brahma-Sphuta-Siddhanta\" (written c. AD 630), discussed the use of negative numbers to produce the general form quadratic formula that remains in use today. He also found negative solutions of quadratic equations and gave rules regarding operations involving negative numbers and zero, such as \"A debt cut off from nothingness becomes a credit; a credit cut off from nothingness becomes a debt. \" He called positive numbers \"fortunes,\" zero \"a cipher,\" and negative numbers \"debts.\"\n\nIn the 9th century, Islamic mathematicians were familiar with negative numbers from the works of Indian mathematicians, but the recognition and use of negative numbers during this period remained timid. Al-Khwarizmi in his \"Al-jabr wa'l-muqabala\" (from which we get the word \"algebra\") did not use negative numbers or negative coefficients. But within fifty years, Abu Kamil illustrated the rules of signs for expanding the multiplication formula_3, and al-Karaji wrote in his \"al-Fakhrī\" that \"negative quantities must be counted as terms\". In the 10th century, Abū al-Wafā' al-Būzjānī considered debts as negative numbers in \"A Book on What Is Necessary from the Science of Arithmetic for Scribes and Businessmen\".\n\nBy the 12th century, al-Karaji's successors were to state the general rules of signs and use them to solve polynomial divisions. As al-Samaw'al writes:\nthe product of a negative number — \"al-nāqiṣ\" — by a positive number — \"al-zāʾid\" — is negative, and by a negative number is positive. If we subtract a negative number from a higher negative number, the remainder is their negative difference. The difference remains positive if we subtract a negative number from a lower negative number. If we subtract a negative number from a positive number, the remainder is their positive sum. If we subtract a positive number from an empty power (\"martaba khāliyya\"), the remainder is the same negative, and if we subtract a negative number from an empty power, the remainder is the same positive number.\n\nIn the 12th century in India, Bhāskara II gave negative roots for quadratic equations but rejected them because they were inappropriate in the context of the problem. He stated that a negative value is \"in this case not to be taken, for it is inadequate; people do not approve of negative roots.\"\n\nEuropean mathematicians, for the most part, resisted the concept of negative numbers until the 17th century, although Fibonacci allowed negative solutions in financial problems where they could be interpreted as debits (chapter 13 of \"Liber Abaci\", AD 1202) and later as losses (in \"Flos\").\n\nIn the 15th century, Nicolas Chuquet, a Frenchman, used negative numbers as exponents but referred to them as “absurd numbers.” In his 1544 \"Arithmetica Integra\" Michael Stifel also dealt with negative numbers, also calling them \"numeri absurdi\".\n\nIn 1545, Gerolamo Cardano, in his \"Ars Magna\", provided the first satisfactory treatment of negative numbers in Europe. He did not allow negative numbers in his consideration of cubic equations, so he had to treat, for example, \"x\" + \"ax\" = \"b\" separately from \"x\" = \"ax\" + \"b\" (with \"a\",\"b\" > 0 in both cases). In all, Cardano was driven to the study of thirteen different types of cubic equations, each expressed purely in terms of positive numbers.\n\nIn A.D. 1759, Francis Maseres, an English mathematician, wrote that negative numbers \"darken the very whole doctrines of the equations and make dark of the things which are in their nature excessively obvious and simple\". He came to the conclusion that negative numbers were nonsensical.\n\nIn the 18th century it was common practice to ignore any negative results derived from equations, on the assumption that they were meaningless.\n\nGottfried Wilhelm Leibniz was the first mathematician to systematically employ negative numbers as part of a coherent mathematical system, the infinitesimal calculus. Calculus made negative numbers necessary and their dismissal as \"absurd numbers\" quickly faded.\n\n"}
{"id": "8499571", "url": "https://en.wikipedia.org/wiki?curid=8499571", "title": "Negative probability", "text": "Negative probability\n\nThe probability of the outcome of an experiment is never negative, although a quasiprobability distribution allows a negative probability, or quasiprobability for some events. These distributions may apply to unobservable events or conditional probabilities.\n\nIn 1942, Paul Dirac wrote a paper \"The Physical Interpretation of Quantum Mechanics\" where he introduced the concept of negative energies and negative probabilities:\n\nThe idea of negative probabilities later received increased attention in physics and particularly in quantum mechanics. Richard Feynman argued that no one objects to using negative numbers in calculations: although \"minus three apples\" is not a valid concept in real life, negative money is valid. Similarly he argued how negative probabilities as well as probabilities above unity possibly could be useful in probability calculations.\n\nMark Burgin gives another example:\nNegative probabilities have later been suggested to solve several problems and paradoxes. \"Half-coins\" provide simple examples for negative probabilities. These strange coins were introduced in 2005 by Gábor J. Székely. Half-coins have infinitely many sides numbered with 0,1,2... and the positive even numbers are taken with negative probabilities. Two half-coins make a complete coin in the sense that if we flip two half-coins then the sum of the outcomes is 0 or 1 with probability 1/2 as if we simply flipped a fair coin.\n\nIn \"Convolution quotients of nonnegative definite functions\" and \"Algebraic Probability Theory\" Imre Z. Ruzsa and Gábor J. Székely proved that if a random variable X has a signed or quasi distribution where some of the probabilities are negative then one can always find two random variables, Y and Z, with ordinary (not signed / not quasi) distributions such that X, Y are independent and X + Y = Z in distribution. Thus X can always be interpreted as the \"difference\" of two ordinary random variables, Z and Y. If Y is interpreted as a measurement error of X and the observed value is Z then the negative regions of the distribution of X are masked / shielded by the error Y.\n\nAnother example known as the Wigner distribution in phase space, introduced by Eugene Wigner in 1932 to study quantum corrections, often leads to negative probabilities. For this reason, it has later been better known as the Wigner quasiprobability distribution. In 1945, M. S. Bartlett worked out the mathematical and logical consistency of such negative valuedness. The Wigner distribution function is routinely used in physics nowadays, and provides the cornerstone of phase-space quantization. Its negative features are an asset to the formalism, and often indicate quantum interference. The negative regions of the distribution are shielded from direct observation by the quantum uncertainty principle: typically, the moments of such a non-positive-semidefinite quasiprobability distribution are highly constrained, and prevent \"direct measurability\" of the negative regions of the distribution. But these regions contribute negatively and crucially to the expected values of observable quantities computed through such distributions, nevertheless.\n\nConsider a double slit experiment with photons. The two waves exiting each slit can be written as:\n\nformula_1\n\nand\n\nformula_2\n\nwhere \"d\" is the distance to the detection screen, \"a\" is the separation between the two slits, \"x\" the distance to the center of the screen, \"λ\" the wavelength and \"dN/dt\" is the number of photons emitted per unit time at the source. The amplitude of measuring a photon at distance \"x\" from the center of the screen is the sum of these two amplitudes coming out of each hole, and therefore the probability that a photon is detected at position \"x\" will be given by the square of this sum:\n\nformula_3,\n\nThis should strike you as the well-known probability rule:\n\nformula_4\n\nwhatever the last term means. Indeed, if one closes either one of the holes forcing the photon to go through the other slit, the two corresponding intensities are\n\nformula_5 and formula_6.\n\nBut now, if one does interpret each of these terms in this way, the joint probability takes negative values roughly every formula_7 !formula_8\n\nHowever, these negative probabilities are never observed as one can't isolate the cases in which the photon \"goes through both slits\", but can hint at the existence of anti-particles.\n\nNegative probabilities have more recently been applied to mathematical finance. In quantitative finance most probabilities are not real probabilities but pseudo probabilities, often what is known as risk neutral probabilities. These are not real probabilities, but theoretical \"probabilities\" under a series of assumptions that helps simplify calculations by allowing such pseudo probabilities to be negative in certain cases as first pointed out by Espen Gaarder Haug in 2004.\n\nA rigorous mathematical definition of negative probabilities and their properties was recently derived by Mark Burgin and Gunter Meissner (2011). The authors also show how negative probabilities can be applied to financial option pricing.\n\nThe concept of negative probabilities have also been proposed for reliable facility location models where facilities are subject to negatively correlated disruption risks when facility locations, customer allocation, and backup service plans are determined simultaneously. Li et al. proposed a virtual station structure that transforms a facility network with positively correlated disruptions into an equivalent one with added virtual supporting stations, and these virtual stations were subject to independent disruptions. This approach reduces a problem from one with correlated disruptions to one without. Xie et al. later showed how negatively correlated disruptions can also be addressed by the same modeling framework, except that a supporting station now may be disrupted with a “failure propensity” which\n\n“... inherits all mathematical characteristics and properties of a failure probability except that we allow it to be larger than 1...”\n\nThis finding paves ways for using compact mixed-integer mathematical programs to optimally design reliable location of service facilities under site-dependent and positive/negative/mixed disruption correlations.\n\nThe proposed “propensity” concept in Xie et al. turns out to be what Feynman and others referred to as “quasi-probability.” Note that when a quasi-probability is larger than 1, then 1 minus this value gives a negative probability. The truly physically verifiable observation is the facility disruption states, and there is no direct information on the station states or their corresponding probabilities. Hence the failure probability of the stations, interpreted as “probabilities of imagined intermediary states,” could exceed unity.\n\n"}
{"id": "1969927", "url": "https://en.wikipedia.org/wiki?curid=1969927", "title": "Negative space", "text": "Negative space\n\nNegative space, in art, is the space around and between the subject(s) of an image. Negative space may be most evident when the space around a subject, not the subject itself, forms an interesting or artistically relevant shape, and such space occasionally is used to artistic effect as the \"real\" subject of an image.\n\nThe use of negative space is a key element of artistic composition. The Japanese word \"ma\" is sometimes used for this concept, for example in garden design.\n\nIn a two-tone, black-and-white image, a subject is normally depicted in black and the space around it is left blank (white), thereby forming a silhouette of the subject. Reversing the tones so that the space around the subject is printed black and the subject itself is left blank, however, causes the negative space to be apparent as it forms shapes around the subject. This is called figure-ground reversal.\n\nIn graphic design of printed or displayed materials, where effective communication is the objective, the use of negative space may be crucial. Not only within the typography, but in its placement in relation to the whole. It is the basis of why upper and lower case typography always is more legible than the use of all capital letters. Negative space varies around lower case letters, allowing the human eye to distinguish each word rapidly as one distinctive item, rather than having to parse out what the words are in a string of letters that all present the same overall profile as in all caps. The same judicious use of negative space drives the effectiveness of the entire design. Because of the long history of the use of black ink on white paper, \"white space\" is the term often used in graphics to identify the same separation.\n\nElements of an image that distract from the intended subject, or in the case of photography, objects in the same focal plane, are not considered negative space. Negative space may be used to depict a subject in a chosen medium by showing everything around the subject, but not the subject itself. Use of negative space will produce a silhouette of the subject. Most often, negative space is used as a neutral or contrasting background to draw attention to the main subject, which then is referred to as the positive space.\n\nConsidering and improving the balance between negative space and positive space in a composition is considered by many to enhance the design. This basic, but often overlooked, principle of design gives the eye a \"place to rest,\" increasing the appeal of a composition through subtle means.\n\nThe use of negative space in art may be analogous to silence in music, but only when it is juxtaposed with adjacent musical ideas. As such, there is a difference between inert and active silences in music, where the latter is more closely analogous to negative space in art.\n\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibits zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "4003593", "url": "https://en.wikipedia.org/wiki?curid=4003593", "title": "Negative volume index", "text": "Negative volume index\n\nNearly 78 years have passed since Paul L. Dysart, Jr. invented the Negative Volume Index and Positive Volume Index indicators. The indicators remain useful to identify primary market trends and reversals.\n\nIn 1936, Paul L. Dysart, Jr. began accumulating two series of advances and declines distinguished by whether volume was greater or lesser than the prior day's volume. He called the cumulative series for the days when volume had been greater than the prior day's volume the Positive Volume Index (PVI), and the series for the days when volume had been lesser the Negative Volume Index (NVI).\n\nA native of Iowa, Dysart worked in Chicago's LaSalle Street during the 1920s. After giving up his Chicago Board of Trade membership, he published an advisory letter geared to short-term trading using advance-decline data. In 1933, he launched the \"Trendway\" weekly stock market letter and published it until 1969 when he died. Dysart also developed the 25-day Plurality Index, the 25-day total of the absolute difference between the number of advancing issues and the number of declining issues, and was a pioneer in using several types of volume of trading studies. Richard Russell, editor of Dow Theory Letters, in his January 7, 1976 letter called Dysart \"one of the most brilliant of the pioneer market technicians.\"\n\nThe daily volume of the New York Stock Exchange and the NYSE Composite Index's advances and declines drove Dysart's indicators. Dysart believed that “volume is the driving force in the market.” He began studying market breadth numbers in 1931, and was familiar with the work of Leonard P. Ayres and James F. Hughes, who pioneered the tabulation of advances and declines to interpret stock market movements.\n\nDysart calculated NVI as follows: 1) if today's volume is less than yesterday's volume, subtract declines from advances, 2) add the difference to the cumulative NVI beginning at zero, and 3) retain the current NVI reading for the days when volume is greater than the prior day's volume. He calculated PVI in the same manner but for the days when volume was greater than the prior day's volume. NVI and PVI can be calculated daily or weekly.\n\nInitially, Dysart believed that PVI would be the more useful series, but in 1967, he wrote that NVI had “proved to be the most valuable of all the breadth indexes.” He relied most on NVI, naming it AMOMET, the acronym of “A Measure Of Major Economic Trend.”\n\nDysart's theory, expressed in his 1967 Barron's article, was that “if volume advances and prices move up or down in accordance [with volume], the move is assumed to be a good movement - if it is sustained when the volume subsides.” In other words, after prices have moved up on positive volume days, \"if prices stay up when the volume subsides for a number of days, we can say that such a move is 'good'.\" If the market “holds its own on negative volume days after advancing on positive volume, the market is in a strong position.”\n\nHe called PVI the “majority” curve. Dysart distinguished between the actions of the “majority” and those of the “minority.” The majority tends to emulate the minority, but its timing is not as sharp as that of the minority. When the majority showed an appetite for stocks, the PVI was usually “into new high ground” as happened in 1961.\n\nIt is said that the two indicators assume that \"smart\" money is traded on quiet days (low volume) and that the crowd trades on very active days. Therefore, the negative volume index picks out days when the volume is lower than on the previous day, and the positive index picks out days with a higher volume.\n\nBesides an article he wrote for Barron's in 1967, not many of Dysart's writings are available. What can be interpreted about Dysart's NVI is that whenever it rises above a prior high, and the DJIA is trending up, a “Bull Market Signal” is given. When the NVI falls below a prior low, and the DJIA is trending down, a “Bear Market Signal” is given. The PVI is interpreted in reverse.\nHowever, not all movements above or below a prior NVI or PVI level generate signals, as Dysart also designated “bullish” and “bearish penetrations.” These penetrations could occur before or after a Bull or Bear Market Signal, and at times were called “reaffirmations” of a signal. In 1969, he articulated one rule: “signals are most authentic when the NVI has moved sideways for a number of months in a relatively narrow range.” Dysart cautioned that “there is no mathematical system devoid of judgment which will continuously work without error in the stock market.”\n\nAccording to Dysart, between 1946 and 1967, the NVI “rendered 17 significant signals,” of which 14 proved to be right (an average of 4.32% from the final high or low) and 3 wrong (average loss of 6.33%). However, NVI “seriously erred” in 1963-1964 and in 1968, which concerned him. In 1969, Dysart reduced the weight he had previously given to the NVI in his analyses because NVI was no longer a “decisive” indicator of the primary trend, although it retained an “excellent ability to give us ‘leading’ indications of short-term trend reversals.”\n\nA probable reason for the NVI losing its efficacy during the mid-1960s may have been the steadily higher NYSE daily volume due to the dramatic increase in the number of issues traded so that prices rose on declining volume. Dysart’s NVI topped out in 1955 and trended down until at least 1968, although the DJIA moved higher during that period.\nNorman G. Fosback has attributed the “long term increase in the number of issues traded” as a reason for a downward bias in a cumulative advance-decline line. Fosback was the next influential technician in the story of NVI and PVI.\n\nFosback studied NVI and PVI and in 1976 reported his findings in his classic Stock Market Logic. He did not elucidate on the indicators’ background or mentioned Dysart except for saying that “in the past Negative Volume Indexes have always [his emphasis] been constructed using advance-decline data….” He posited, “There is no good reason for this fixation on the A/D Line. In truth, a Negative Volume Index can be calculated with any market index - the Dow Jones Industrial Average, the S&P 500, or even ‘unweighted’ market measures... Somehow this point has escaped the attention of technicians to date.”\n\nThe point had not been lost on Dysart, who wrote in Barron’s, “we prefer to use the issues-traded data [advances and declines] rather than the price data of any average because it is more all-encompassing, and more truly represents what’s happening in the entire market.” Dysart was a staunch proponent of using advances and declines.\n\nFosback made three variations to NVI and PVI:\n\n1. He cumulated the daily percent change in the market index rather than the difference between advances and declines. On negative volume days, he calculated the price change in the index from the prior day and added it to the most recent NVI. His calculations are as follows:\n\nIf C and C denote the closing prices of today and yesterday, respectively, the NVI for today is calculated by\n\n\nand the PVI is calculated by:\n\n\n2. He suggested starting the cumulative count at a base index level such as 100.\n\n3. He derived buy or sell signals by whether the NVI or PVI was above or below its one-year moving average.\n\nFosback's versions of NVI and PVI are what are popularly described in books and posted on Internet financial sites. Often reported are his findings that whenever NVI is above its one-year moving average there is a 96% (PVI - 79%) probability that a bull market is in progress, and when it is below its one-year moving average, there is a 53% (PVI - 67%) probability that a bear market is in place. These results were derived using a 1941-1975 test period. Modern tests might reveal different probabilities.\n\nToday, NVI and PVI are commonly associated with Fosback's versions, and Dysart, their inventor, is forgotten. It cannot be said that one version is better than the other. While Fosback provided a more objective interpretation of these indicators, Dysart's versions offer value to identify primary trends and short-term trend reversals.\n\nAlthough some traders use Fosback's NVI and PVI to analyze individual stocks, the indicators were created to track, and have been tested, on major market indexes. NVI was Dysart's most invaluable breadth index, and Fosback found that his version of “the Negative Volume Index is an excellent indicator of the primary market trend.” Traders can benefit from both innovations.\n\n"}
{"id": "577248", "url": "https://en.wikipedia.org/wiki?curid=577248", "title": "New riddle of induction", "text": "New riddle of induction\n\nGrue and bleen are examples of logical predicates coined by Nelson Goodman in \"Fact, Fiction, and Forecast\" to illustrate the \"new riddle of induction\". These predicates are unusual because their application is time-dependent; many have tried to solve the new riddle on those terms, but Hilary Putnam and others have argued such time-dependency depends on the language adopted, and in some languages it is equally true for natural-sounding predicates such as \"green.\" For Goodman they illustrate the problem of projectible predicates and ultimately, which empirical generalizations are law-like and which are not.\nGoodman's construction and use of \"grue\" and \"bleen\" illustrates how philosophers use simple examples in conceptual analysis.\n\nGoodman defined grue relative to an arbitrary but fixed time \"t\" as follows: An object is grue if and only if it is observed before \"t\" and is green, or else is not so observed and is blue. An object is bleen if and only if it is observed before \"t\" and is blue, or else is not so observed and is green.\n\nTo understand the problem Goodman posed, it is helpful to imagine some arbitrary future time \"t\", say January 1, 10. For all green things we observe up to time \"t\", such as emeralds and well-watered grass, both the predicates \"green\" and \"grue\" apply. Likewise for all blue things we observe up to time \"t\", such as bluebirds or blue flowers, both the predicates \"blue\" and \"bleen\" apply. On January 2, 10, however, emeralds and well-watered grass are \"bleen\" and bluebirds or blue flowers are \"grue\". Clearly, the predicates \"grue\" and \"bleen\" are not the kinds of predicates we use in everyday life or in science, but the problem is that they apply in just the same way as the predicates \"green\" and \"blue\" up until some future time \"t\". From our current perspective (i.e., before time \"t\"), how can we say which predicates are more projectible into the future: \"green\" and \"blue\" or \"grue\" and \"bleen\"?\n\nIn this section, Goodman's new riddle of induction is outlined in order to set the context for his introduction of the predicates \"grue\" and \"bleen\" and thereby illustrate their philosophical importance.\n\nGoodman poses Hume's problem of induction as a problem of the validity of the predictions we make. Since predictions are about what has yet to be observed and because there is no necessary connection between what has been observed and what will be observed, what is the justification for the predictions we make? We cannot use deductive logic to infer predictions about future observations based on past observations because there are no valid rules of deductive logic for such inferences. Hume's answer was that our observations of one kind of event following another kind of event result in our minds forming habits of regularity (i.e., associating one kind of event with another kind). The predictions we make are then based on these regularities or habits of mind we have formed.\n\nGoodman takes Hume's answer to be a serious one. He rejects other philosophers' objection that Hume is merely explaining the origin of our predictions and not their justification. His view is that Hume has identified something deeper. To illustrate this, Goodman turns to the problem of justifying a system of rules of deduction. For Goodman, the validity of a deductive system is justified by its conformity to good deductive practice. The justification of rules of a deductive system depends on our judgements about whether to reject or accept specific deductive inferences. Thus, for Goodman, the problem of induction dissolves into the same problem as justifying a deductive system and while, according to Goodman, Hume was on the right track with habits of mind, the problem is more complex than Hume realized.\n\nIn the context of justifying rules of induction, this becomes the problem of confirmation of generalizations for Goodman. However, the confirmation is not a problem of justification but instead it is a problem of precisely defining how evidence confirms generalizations. It is with this turn that \"grue\" and \"bleen\" have their philosophical role in Goodman's view of induction.\n\nThe new riddle of induction, for Goodman, rests on our ability to distinguish \"lawlike\" from \"non-lawlike\" generalizations. \"Lawlike\" generalizations are capable of confirmation while \"non-lawlike\" generalizations are not. \"Lawlike\" generalizations are required for making predictions. Using examples from Goodman, the generalization that all copper conducts electricity is capable of confirmation by a particular piece of copper whereas the generalization that all men in a given room are third sons is not \"lawlike\" but accidental. The generalization that all copper conducts electricity is a basis for predicting that this piece of copper will conduct electricity. The generalization that all men in a given room are third sons, however, is not a basis for predicting that a given man in that room is a third son.\n\nWhat then makes some generalizations \"lawlike\" and others accidental? This, for Goodman, becomes a problem of determining which predicates are projectible (i.e., can be used in \"lawlike\" generalizations that serve as predictions) and which are not. Goodman argues that this is where the fundamental problem lies. This problem, known as \"Goodman's paradox\", is as follows. Consider the evidence that all emeralds examined thus far have been green. This leads us to conclude (by induction) that all future emeralds will be green. However, whether this prediction is \"lawlike\" or not depends on the predicates used in this prediction. Goodman observed that (assuming \"t\" has yet to pass) it is equally true that every emerald that has been observed is \"grue\". Thus, by the same evidence we can conclude that all future emeralds will be \"grue\". The new problem of induction becomes one of distinguishing projectible predicates such as \"green\" and \"blue\" from non-projectible predicates such as \"grue\" and \"bleen\".\n\nHume, Goodman argues, missed this problem. We do not, by habit, form generalizations from all associations of events we have observed but only some of them. All past observed emeralds were green, and we formed a habit of thinking the next emerald will be green, but they were equally grue, and we do not form habits concerning grueness. \"Lawlike\" predictions (or projections) ultimately are distinguishable by the predicates we use. Goodman's solution is to argue that \"lawlike\" predictions are based on projectible predicates such as \"green\" and \"blue\" and not on non-projectible predicates such as \"grue\" and \"bleen\" and what makes predicates projectible is their \"entrenchment\", which depends on their successful past projections. Thus, \"grue\" and \"bleen\" function in Goodman's arguments to both illustrate the new riddle of induction and to illustrate the distinction between projectible and non-projectible predicates via their relative entrenchment.\n\nThe most obvious response is to point to the artificially disjunctive definition of grue. The notion of predicate \"entrenchment\" is not required. Goodman, however, noted that this move will not work. If we take \"grue\" and \"bleen\" as primitive predicates, we can define green as \"\"grue\" if first observed before \"t\" and \"bleen\" otherwise\", and likewise for blue. To deny the acceptability of this disjunctive definition of green would be to beg the question.\n\nAnother proposed resolution of the paradox (which Goodman addresses and rejects) that does not require predicate \"entrenchment\" is that \"\"x\" is grue\" is not solely a predicate of \"x\", but of \"x\" and a time \"t\"—we can know that an object is green without knowing the time \"t\", but we cannot know that it is grue. If this is the case, we should not expect \"\"x\" is grue\" to remain true when the time changes. However, one might ask why \"\"x\" is green\" is \"not\" considered a predicate of a particular time \"t\"—the more common definition of \"green\" does not require any mention of a time \"t\", but the definition \"grue\" does. As we have just seen, this response also begs the question because \"blue\" can be defined in terms of \"grue\" and \"bleen\", which explicitly refer to time.\n\nRichard Swinburne gets past the objection that green may be redefined in terms of \"grue\" and \"bleen\" by making a distinction based on how we test for the applicability of a predicate in a particular case. He distinguishes between qualitative and locational predicates. Qualitative predicates, like green, \"can\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event. Locational predicates, like \"grue\", \"cannot\" be assessed without knowing the spatial or temporal relation of \"x\" to a particular time, place or event, in this case whether \"x\" is being observed before or after time \"t\". Although green can be given a definition in terms of the locational predicates \"grue\" and \"bleen\", this is irrelevant to the fact that green meets the criterion for being a qualitative predicate whereas \"grue\" is merely locational. He concludes that if some \"x\"'s under examination—like emeralds—satisfy both a qualitative and a locational predicate, but projecting these two predicates yields conflicting predictions, namely, whether emeralds examined after time \"t\" shall appear blue or green, we should project the qualitative predicate, in this case green.\n\nRudolf Carnap responded to Goodman's 1946 article. Carnap's approach to inductive logic is based on the notion of \"degree of confirmation\" \"c\"(\"h\",\"e\") of a given hypothesis \"h\" by a given evidence \"e\". Both \"h\" and \"e\" are logical formulas expressed in a simple language \"L\" which allows for\nThe universe of discourse consists of denumerably many individuals, each of which is designated by its own constant symbol; such individuals are meant to be regarded as positions (\"like space-time points in our actual world\") rather than extended physical bodies. A state description is a (usually infinite) conjunction containing every possible ground atomic sentence, either negated or unnegated; such a conjunction describes a possible state of the whole universe. Carnap requires the following semantic properties:\nCarnap distinguishes three kinds of properties:\nTo illuminate this taxonomy, let \"x\" be a variable and \"a\" a constant symbol; then an example of 1. could be \"\"x\" is blue or \"x\" is non-warm\", an example of 2. \"\"x\" = \"a\", and an example of 3. \"x\" is red and not \"x\" = \"a\"\".\n\nBased on his theory of inductive logic sketched above, Carnap formalizes Goodman's notion of projectibility of a property \"W\" as follows: the higher the relative frequency of \"W\" in an observed sample, the higher is the probability that a non-observed individual has the property \"W\". Carnap suggests \"as a tentative answer\" to Goodman, that all purely qualitative properties are projectible, all purely positional properties are non-projectible, and mixed properties require further investigation.\n\nWillard Van Orman Quine discusses an approach to consider only \"natural kinds\" as projectible predicates.\nHe first relates Goodman's grue paradox to Hempel's raven paradox by defining two predicates \"F\" and \"G\" to be (simultaneously) projectible if all their shared instances count toward confirmation of the claim \"each \"F\" is a \"G\"\". Then Hempel's paradox just shows that the complements of projectible predicates (such as \"is a raven\", and \"is black\") need not be projectible, while Goodman's paradox shows that \"is green\" is projectible, but \"is grue\" is not.\n\nNext, Quine reduces projectibility to the subjective notion of \"similarity\". Two green emeralds are usually considered more similar than two grue ones if only one of them is green. Observing a green emerald makes us expect a similar observation (i.e., a green emerald) next time. Green emeralds are a \"natural kind\", but grue emeralds are not. Quine investigates \"the dubious scientific standing of a general notion of similarity, or of kind\". Both are basic to thought and language, like the logical notions of e.g. identity, negation, disjunction. However, it remains unclear how to relate the logical notions to \"similarity\" or \"kind\"; Quine therefore tries to relate at least the latter two notions to each other.\n\nRelation between similarity and kind\n\nAssuming finitely many \"kinds\" only, the notion of \"similarity\" can be defined by that of \"kind\": an object \"A\" is more similar to \"B\" than to \"C\" if \"A\" and \"B\" belong jointly to more kinds than \"A\" and \"C\" do.\n\nVice versa, it remains again unclear how to define \"kind\" by \"similarity\". Defining e.g. the kind of red things as the set of all things that are more similar to a fixed \"paradigmatical\" red object than this is to another fixed \"foil\" non-red object (cf. left picture) isn't satisfactory, since the degree of overall similarity, including e.g. shape, weight, will afford little evidence of degree of redness. (In the picture, the yellow paprika might be considered more similar to the red one than the orange.)\n\nAn alternative approach inspired by Carnap defines a natural kind to be a set whose members are more similar to each other than each non-member is to at least one member. \nHowever, Goodman argued, that this definition would make the set of all red round things, red wooden things, and round wooden things (cf. right picture) meet the proposed definition of a natural kind, while \"surely it is not what anyone means by a kind\".\n\nWhile neither of the notions of similarity and kind can be defined by the other, they at least vary together: if \"A\" is reassessed to be more similar to \"C\" than to \"B\" rather than the other way around, the assignment of \"A\", \"B\", \"C\" to kinds will be permuted correspondingly; and conversely.\n\nBasic importance of similarity and kind\n\nIn language, every general term owes its generality to some resemblance of the things referred to. Learning to use a word depends on a double resemblance, viz. between the present and past circumstances in which the word was used, and between the present and past phonetic utterances of the word.\n\nEvery reasonable expectation depends on resemblance of circumstances, together with our tendency to expect similar causes to have similar effects. This includes any scientific experiment, since it can be reproduced only under similar, but not under completely identical, circumstances. Already Heraclitus' famous saying \"No man ever steps in the same river twice\" highlighted the distinction between similar and identical circumstances.\n\nGenesis of similarity and kind\n\nIn a behavioral sense, humans and other animals have an innate standard of similarity. It is part of our animal birthright, and characteristically animal in its lack of intellectual status, e.g. its alieness to mathematics and logic, cf. bird example.\n\nInduction itself is essentially animal expectation or habit formation.\nOstensive learning\nis a case of induction, and a curiously comfortable one, since each man's spacing of qualities and kind is enough like his neighbor's.\nIn contrast, the \"brute irrationality of our sense of similarity\" offers little reason to expect it being somehow in tune with the unanimated nature, which we never made.\nWhy inductively obtained theories about it should be trusted is the perennial philosophical problem of induction. Quine, following Watanabe,\nsuggests Darwin's theory as an explanation: if people's innate spacing of qualities is a gene-linked trait, then the spacing that has made for the most successful inductions will have tended to predominate through natural selection.\nHowever, this cannot account for the human ability to dynamically refine one's spacing of qualities in the course of getting acquainted with a new area.\n\nIn his book \"Wittgenstein on Rules and Private Language\", Saul Kripke proposed a related argument that leads to skepticism about meaning rather than skepticism about induction, as part of his personal interpretation (nicknamed \"Kripkenstein\" by some) of the private language argument. He proposed a new form of addition, which he called \"quus\", which is identical with \"+\" in all cases except those in which either of the numbers added are equal to or greater than 57; in which case the answer would be 5, i.e.:\n\nHe then asks how, given certain obvious circumstances, anyone could know that previously when I thought I had meant \"+\", I had not actually meant \"quus\". Kripke then argues for an interpretation of Wittgenstein as holding that the meanings of words are not individually contained mental entities.\n\n\n"}
{"id": "30226192", "url": "https://en.wikipedia.org/wiki?curid=30226192", "title": "Polar concept argument", "text": "Polar concept argument\n\nA polar concept argument is a type of argument that posits the understanding of one concept, from the mere understanding of its polar opposite. A well-known instance of a polar concept argument is Gilbert Ryle's argument against scepticism (1960). According to Anthony Grayling's characterisation, Ryle's argument can be stated as follows:\n\nAccording to Ryle's polar concept argument, counterfeit and genuine coins come in pairs, and one cannot conceive of counterfeit coins without also capturing the essence of the genuine coins at the same time. When one grasps the essence of one polar concept, one also grasps immediately the essence of its polar opposite. Ryle's original argument (1960) runs as follows:\n\nA polar concept argument bears on some more or less strong version of dialectical monism, a philosophical doctrine that views reality as a unified whole, due to the complementarity of polar concepts.\n\n"}
{"id": "447492", "url": "https://en.wikipedia.org/wiki?curid=447492", "title": "Policy debate", "text": "Policy debate\n\nPolicy debate is a form of debate competition in which teams of two advocate for and against a resolution that typically calls for policy change by the United States federal government. It is also referred to as cross-examination debate (sometimes shortened to Cross-X, CX, Cross-ex, or C-X) because of the 3-minute questioning period following each constructive speech. Affirmative teams generally present a \"plan\" as a proposal for implementation of the resolution.\n\nHigh school policy debate is sponsored by various organizations including the National Speech and Debate Association, National Association of Urban Debate Leagues, Catholic Forensic League, Stoa USA, and the National Christian Forensics and Communications Association, as well as many other regional speech organizations. Collegiate policy debates are generally competed under the guidelines of National Debate Tournament (NDT) and the Cross Examination Debate Association (CEDA), which have been joined at the collegiate level. A one-person policy format is sanctioned by the National Forensic Association (NFA) on the collegiate level as well.\n\nAcademic debate had its origins in intracollegiate debating societies, in which students would engage in (often public) debates against their classmates. Wake Forest University's debate program claims to have its origins in student literary societies founded on campus in the mid-1830s, which first presented joint \"orations\" in 1854. Many debating societies that were founded at least as early as the mid-nineteenth century are still active today, though they have generally shifted their focus to intercollegiate competitive debate. In addition to Wake Forest, the debate society at Northwestern University dates to 1855. Boston College's Fulton Debating Society, which was founded in 1868, continues to stage an annual public \"Fulton Prize Debate\" between teams of its own students after the intercollegiate debate season has ended. Other universities continue similar traditions.\n\nIntercollegiate debates have been held since at least as early as the 1890s. Historical records indicate that debates between teams from Wake Forest University and Trinity College (later Duke University) occurred beginning in 1897. Additionally, a debate between students from Boston College and Georgetown University occurred on May 1, 1895, in Boston. Whitman College debated Washington State University, Willamette University, and the University of Idaho in the late 1890s. Southwestern claims that the first debate held on its campus was between Southwestern and Fairmount College (which eventually became Wichita State University) but that debate could not have occurred prior to 1895, the year Fairmount College began classes.\n\nBy the mid-1970s, structured rules for lengths of speeches developed. Each side (affirmative and negative) was afforded two opening \"constructive\" speeches, and two closing \"rebuttal\" speeches, for a total of eight speeches per debate. Each speaker was cross-examined by an opponent for a period following his or her constructive speech. Traditionally rebuttals were half the length of constructives, but when a style of faster delivery speed became more standard in the late 1980s this time structure became problematic. Wake Forest University introduced reformed speech times in both its college (9‑6 instead of 10‑5) and high school (8‑5 instead of 8‑4) tournaments, which spread rapidly to become the new de facto standards.\n\nIn 2014, debaters Ameena Ruffin and Korey Johnson made history by being the first black women to win the Cross Examination Debate Association tournament, which is the largest college debate tournament.\n\nPolicy debaters' speed of delivery will vary from league to league and tournament to tournament. In many tournaments, debaters will speak very quickly in order to read as much evidence and make as many arguments as possible within the time-constrained speech. Speed reading or spreading is normal at the majority of national circuit policy debate tournaments.\n\nSome feel that the rapid-fire delivery makes debate harder to understand for the lay person. Rapid delivery is encouraged by those who believe that increased quantity and diversity of argumentation makes debates more educational. Others, citing scientific studies, claim that learning to speak faster also increases short- and long-term memory. A slower style is preferred by those who want debates to be understandable to lay people and those who claim that the pedagogical purpose of the activity is to train rhetorical skills. This is often a argument made by those who oppose in-round spreading - that the use of incomprehensible speeds makes debate less appealing to lay people. Many further claim that the increased speed encourages debaters to make several poor arguments, as opposed to a few high-quality ones. Most debaters will vary their rate of delivery depending upon the judge's preferences.\n\nDebaters utilize a specialized form of note taking, called \"flowing\", to keep track of the arguments presented during a debate. Conventionally, debater's flowing is divided into separate \"flows\" for each different argument in the debate round (kritiks, disads, topicalities, case, etc.). There are multiple methods of flowing but the most common style incorporates columns of arguments made in a given speech which allows the debater to match the next speaker's responses up with the original arguments. Certain shorthands for commonly used words are used to keep up with the rapid rate of delivery. The abbreviations or stand-in symbols vary between debaters.\n\nFlowing on a laptop has become more and more popular among high school and college debaters, despite the reservations of certain schools, tournaments, and judges. Some debaters use a basic computer spreadsheet; others use specialized flowing templates, which includes embedded shortcut keys for the most common formatting needs.\n\nAlthough there are many accepted standards in policy debate, there is no written formulation of rules. Sometimes debaters will in fact debate about how policy debate should work. These arguments are known as \"theory arguments\", and they are most often brought up when one team believes the actions of the other team are unfair and therefore warrant a loss. In more recent years, however, theory arguments have not been nearly as present in the Novice level as before.\n\nWhen the Affirmative team presents a plan, they take upon themselves the Burden of Proof to prove that their plan should be adopted. They must prove that their plan is an example of the resolution, and they must prove that the plan is a good idea. The Affirmative traditionally must uphold this burden using evidence from published sources, to avoid ridiculous cases.\n\nOne traditional way to judge policy debate states that the affirmative team must win certain issues, called the stock issues. They are generally interpreted to be as follows:\nHow many negative disadvantages will the plan have? Will its disadvantages outweigh its advantages?\nWill the plan solve the harms and can it even happen in the real world? How much of an impact will the plan have?\nWhat is the problem in the status quo to justify implementation of the plan? Is the plan important enough to even warrant consideration or make a difference?\nIs the affirmative's plan happening already, and if not, why?\nIs the plan an example of the resolution? Does the affirmative team's proposed policy comply with the wording of the resolution?\n\nThe \"stock issues\" are also commonly described as Significance, Harms, Inherency, Topicality, and Solvency. The \"stock issues\" are then taught collectively as \"The S.H.I.T.S.\" . Stock issues are taught extensively to novice debaters, but typically become less relevant as debaters move into more complex and less traditional arguments. Stock issues are strongly associated with traditional policy debate, and are typically stressed in advanced debates only if the judge is known to have traditionalist preferences. In some places (notably the West Coast), there is the additional stock issue of disadvantages; or a 'bad thing' that may occur as a result of passing the plan.\n\nMost affirmative teams today generally frame their case around advantages, which are good effects of their plan. The negative team will often present disadvantages which contend that the affirmative plan causes undesirable consequences. In an attempt to make sure that their advantages/disadvantages outweigh those of the other team, debaters often present extreme scenarios such as the extinction of the human race or a global nuclear war.\n\nNegation Theory contends that the negative need only negate the affirmative instead of having to negate the resolution. The acceptance of negation theory allows negative teams to run arguments such as topical counter plans that may affirm the resolution but still negate the affirmative's specific plan.\n\nAfter the affirmative presents its case, the negative can attack the case with many different arguments, which include:\n\nEvidence in debates is organized into units called \"cards\" (because such evidence was originally printed on note cards, though the practice has long been out of favor). Cards are designed to condense an author's argument so that debaters have an easy way to access the information. A card is composed of three parts: the tag, the cite, and the body. The \"tag\" is the debater's summary of the argument presented in the body. A tag is usually only one or two sentences. The \"cite\" contains all relevant citation information (that is, the author, date of publication, journal, title, etc.). Although every card should contain a complete citation, only the author's name and date of publication are typically spoken aloud in a speech. Some teams will also read the author's qualifications if they wish to emphasize this information. The \"body\" is a fragment of the author's original text. The length of a body can vary greatly—cards can be as short as a few sentences and as long as two or more pages. Most cards are between one and five paragraphs in length. The body of a card is often underlined or highlighted in order to eliminate unnecessary or redundant sentences when the card is read in a round. In a round, the tag is read first, followed by the cite and the body.\n\nAs pieces of evidence accumulate use, multiple colors of highlighting and different thicknesses of underlining often occur, sometimes making it difficult to determine which portion of the evidence was read. If debaters stop before finishing the underlined or highlighted portion of a card, it is considered good form to \"mark\" the card to show where one stopped reading. To otherwise misrepresent how much of a card was read—either by stopping early or by skipping underlined or highlighted sections—is known as \"cross-reading\" or \"clipping\" which is generally considered cheating. Although many judges overtly condemn the practice on their paradigms, it is hard to enforce, especially if judges permit debaters to be excessively unclear. Opponents will generally stand behind a debater whom they believe to be cross-reading or clipping, as if waiting to take a card (see below), and silently read along with them in an attempt to get their opponent to stop or the judge to notice.\n\nAs cards are read in round, it is common for an opponent to collect and examine even while a speech is still going on. This practice originated in part because cards are read at a rate faster than conversational speed but also because the un-underlined portions of cards are not read in round. Taking the cards during the speech allows the opponent to question the author's qualifications, the original context of the evidence, etc. in cross-examination. It is generally accepted whichever team is using preparation time has priority to read evidence read previously during a round by both teams. As a result, large amounts of evidence may change hands after the use of preparation time but before a speech. Most judges will not deduct from a team's preparation time for time spent finding evidence which the other team has misplaced.\n\nAfter a round, judges often \"call for cards\" to examine evidence whose merit was contested during the round or whose weight was emphasized during rebuttals so that they can read the evidence for themselves. Although widespread, this practice is explicitly banned at some tournaments, most notably National Catholic Forensic League nationals, and some judges refuse to call for cards because they believe the practice constitutes \"doing work for debaters that should have been done during round\". Judges may also call for evidence for the purpose of obtaining its citation information so that they can produce the evidence for their own school. Opponents and spectators are also generally allowed to collect citations in this manner, and some tournaments send scouts to rounds to facilitate the collection of cites for every team at the tournament, information which is sometimes published online.\n\nA judge refers to the individual responsible for determining the winner and loser of a policy debate round as well as assessing the relative merit of the participant speakers. Judges must resolve the complex issues presented in short time while, ideally, avoiding inserting their own personal beliefs that might cloud impartiality. \n\nSome circuits see \"lay\" or inexperienced judges recruited from the community as an important \"part of the game.\" Debaters in these circuits must be able to adapt from presentations to individuals with no debate experience at all, to judges who have themselves been debaters. This use of lay judges significantly impacts delivery and argumentation as the rapid-fire style and complex debate-theory arguments are frequently incomprehensible to lay judges. For this reason, other circuits restrict policy debate judging to qualified judges, generally ex-debaters. The use of lay judges, and its impact in speed, presentation and argumentation is a source of great controversy in the US high school debate community.\n\nThe judge is charged not only with selecting a winner, but also must allot points to each competitor. Known as \"speaker points\" or simply \"speaks\", its goal is to provide a numerical evaluation of the debaters' speaking skills. Speaker point schemes vary throughout local state and regional organizations particularly at the high school level. However, the method accepted by most national organizations such as the National Forensic League, Tournament of Champions, National Catholic Forensic League, Cross-Examination Debate Association, and National Debate Tournament, use values ranging from 1 to 30. In practice, within these organizations the standard variation is 26‑29, where 26's are given to extremely poor speakers, where a perfect score is considered incredibly rare and warranted only by an outstanding performance. Most tournaments accept halfpoint gradiations, for example 28.5s. Generally, speaker points are seen as secondary in importance to wins and losses, yet often correlate with a team's win/loss rate. In other words, the judge usually awards the winning team cumulatively higher speaker points than the losing team. If the judge does not, the decision is considered a \"low-point win\". Low-point wins simply mean that the team with better argumentation spoke worse, but are often rare, because judges will vote for teams that speak better, and award better speaker points to victorious teams.\n\nIn some smaller jurisdictions, the judge ranks the speakers 1‑4 instead of awarding them speaker points. Either speaker-point calculation may be used to break ties among teams with like records. Some areas also use speaker rankings in addition to speaker points in order to differentiate between speakers awarded the same number of points.\n\nAt a majority of tournaments, debaters also receive \"speaker awards\", which are awarded to the debaters who received the greatest number of speaker points. Many tournaments also drop the highest and lowest score received by each debater, in order to ensure that the speaker award calculations are fair and consistent, despite the preferences of different judges. The amount of speaker awards given out varies based on the number of debaters competing at any given tournament. For instance, a small local tournament might only award trophies or plaques to the top three debaters, whereas a widely attended \"national circuit\" tournament might give out awards to the top ten or fifteen speakers.\n\nExperienced debate judges (who were generally debaters in high school and/or college) generally carry a mindset that favors certain arguments and styles over others. Depending on what mindset, or paradigm, the judge uses, the debate can be drastically different. Because there is no one view of debate agreed upon by everyone, many debaters question a judge about their paradigm and/or their feelings on specific arguments before the round.\n\nNot every judge fits perfectly into one paradigm or another. A judge may say that they are \"tabula rasa\" or tab for short, or willing to listen to anything, but draw the line at arguments they consider to be offensive (such as arguments in favor of racism). Or, a judge might be a \"policymaker\", but still look at the debate in an offense/defense framework like a games-playing judge.\n\nExamples of paradigms include:\n\nMost high school debaters debate in local tournaments in their city, state or nearby states. Thousands of tournaments are held each year at high schools throughout the US.\n\nA small subset of high school debaters, mostly from elite public and private schools, travel around the country to tournaments in what is called the 'national circuit.' The championship of the national circuit is usually considered to be the Tournament of Champions, also called the T.O.C, at the University of Kentucky, which requires formal qualification in the form of two or more bids to the tournament. Bids are achieved by reaching a certain level of elimination rounds (for example, quarter-finals) at select, highly competitive, and carefully chosen tournaments across the country based upon the quality of debaters they attract and the diversity of locations from across the United States they represent.\n\nUrban debate leagues give students in urban school districts an opportunity to participate in policy debate. There are currently urban debate leagues in 24 of the largest cities in the United States. In total, more than 500 high schools participate in the league and more than 40,000 students have competed in urban debate.\n\nThere is some dispute over what constitutes the \"national championship\" in the United States per se, but two tournaments generally compete for the title:\nThe Tournament of Champions held at the University of Kentucky, and the National Speech and Debate tournament sponsored by the National Forensic League (now known as the National Speech & Debate Association). For the highest level of competition, the Tournament of Champions is generally considered to be the more prestigious title to hold.\n\nIn Texas, some debate occurs on the Texas Forensic Association (TFA) level. This organization includes mostly the progressive judging paradigms and favors many off topic arguments. TFA is geared towards the larger schools/programs who tend to be in the suburban areas in the major cities in the eastern part of the state. The other type of debate is UIL. UIL is open to all public schools throughout Texas.\n\nTFA State policy debate tends to favor a multitude of off-case arguments and teams that favor a progressive style; while UIL State tends to be more policy focused.\n\nThere is no single unified national championship in college debate; though the National Debate Tournament (NDT), the Cross Examination Debate Association (CEDA) and the American Debate Association (ADA) all host national tournaments. The NDT committee issues a ranking report of the top 16 teams in the country (\"first round bids\") for automatic advancement to the NDT in early February. The report roughly determines a regular season champion called the 'Copeland Award' for the team rated the highest over the course of the year through early February.\n\nWhile once attended by only highly competitive policy debaters, many high school students now attend debate institutes, which are typically held at colleges in the summer. Most institutes range from about two to seven weeks.\n\nMany institutes divide students into work groups, or \"labs\", based on skill level and experience. Many even offer specialized \"advanced\" or \"scholars\" workshops, to which acceptance is highly limited.\n\nA resolution or topic is a statement which the affirmative team affirms and the negative team negates. Resolutions are selected annually by affiliated schools. Most resolutions from the 1920s to 2005 have begun \"Resolved: that The United States federal government should\" although some variations from this structure have been apparent both before the NDT-CEDA merger and with the 2006–2007 college policy debate topic, which limited the affirmative agent to the United States Supreme Court.\n\nAt the college level, a number of topics are proposed and interested parties write \"topic papers\" discussing the pros and cons of that individual topic. Each school then gets one vote on the topic. The single topic area voted on then has a number of proposed topic wordings, one is chosen, and it is debated by affiliated students nationally for the entire season (standard academic school year).\n\nAt the high-school level, \"topic papers\" are also prepared but the voting procedure is different. These papers are then presented to a topic selection committee which rewords each topic and eventually narrows down the number of topics to five topics. Then the five resolutions are put to a two-tiered voting system. State forensic associations, the National Forensic League, and the National Catholic Forensic League all vote on the five topics, narrowing it down to two. Then the two topics are again put to a vote, and one topic is selected.\n\nResolved: The United States federal government should substantially reduce its military and/or police presence in one or more of the following: South Korea, Japan, Afghanistan, Kuwait, Iraq, Turkey.\n\nResolved: The United States federal government should substantially increase its exploration and/or development of space beyond the Mesosphere.\n\nResolved: The United States federal government should substantially increase its transportation infrastructure investment in the United States.\n\nResolved: The United States federal government should substantially increase its economic engagement toward Cuba, Mexico or Venezuela.\n\nResolved: The United States federal government should substantially increase its non-military exploration and/or development of the Earth's oceans.\n\nResolved: The United States federal government should substantially curtail its domestic surveillance.\nResolved: The United States federal government should substantially increase its economic and/or diplomatic engagement with the People's Republic of China.\nResolved: The United States federal government should substantially increase its funding and/or regulation of primary and/or secondary education in the United States.\nResolved: The United States federal government should substantially reduce its restrictions on legal immigration to the United States.\n\nThe times and speech order are generally as follows:\n\nIn addition to speeches, policy debates may allow for a certain amount of preparation time, or \"prep time,\" during a debate round. NFL rules call for 5 minutes of total prep time that can be used, although in practice high school debate tournaments usually give 8 minutes of prep time. College debates typically have 10 minutes of preparation time. The preparation time is used at each team's preference; they can use different amounts of preparation time before any of their speeches, or even none at all. Prep time can be allocated strategically to intimidate or inconvenience the other team: for instance, normally a 1AR requires substantial prep time, so a well-executed \"stand up 1AR,\" delivered after no prep time intimidates the negative team and takes away from time that the 2NR may have used to prepare the parts of his/her speech which do not rely on what the 1AR says.\n\n\n\n\n\n\n"}
{"id": "780566", "url": "https://en.wikipedia.org/wiki?curid=780566", "title": "Possible world", "text": "Possible world\n\nIn philosophy and logic, the concept of a possible world is used to express modal claims. The concept of possible worlds is common in contemporary philosophical discourse but has been disputed.\n\nThose theorists who use the concept of possible worlds consider the \"actual\" world to be one of the many possible worlds. For each distinct way the world could have been, there is said to be a distinct possible world; the actual world is the one we in fact live in. Among such theorists there is disagreement about the nature of possible worlds; their precise ontological status is disputed, and especially the difference, if any, in ontological status between the actual world and all the other possible worlds. One position on these matters is set forth in David Lewis's modal realism (see below). There is a close relation between propositions and possible worlds. We note that every proposition is either true or false at any given possible world; then the \"modal status\" of a proposition is understood in terms of the \"worlds in which it is true\" and \"worlds in which it is false\". The following are among the assertions we may now usefully make:\n\n\nThe idea of possible worlds is most commonly attributed to Gottfried Leibniz, who spoke of possible worlds as ideas in the mind of God and used the notion to argue that our actually created world must be \"the best of all possible worlds\". Arthur Schopenhauer argued that on the contrary our world must be the worst of all possible worlds, because if it were only a little worse it could not continue to exist.\n\nScholars have found implicit earlier traces of the idea of possible worlds in the works of René Descartes, a major influence on Leibniz, Al-Ghazali (\"The Incoherence of the Philosophers\"), Averroes (\"The Incoherence of the Incoherence\"), Fakhr al-Din al-Razi (\"Matalib al-'Aliya\") and John Duns Scotus. The modern philosophical use of the notion was pioneered by David Lewis and Saul Kripke.\n\nA semantics for modal logic was first introduced in the late-1950s work of Saul Kripke and his colleagues. A statement in modal logic that is \"possible\" is said to be true in at least one possible world; a statement that is \"necessary\" is said to be true in all possible worlds.\n\nFrom this groundwork, the theory of possible worlds became a central part of many philosophical developments, from the 1960s onwards – including, most famously, the analysis of counterfactual conditionals in terms of \"nearby possible worlds\" developed by David Lewis and Robert Stalnaker. On this analysis, when we discuss what \"would\" happen \"if\" some set of conditions \"were\" the case, the truth of our claims is determined by what is true at the nearest possible world (or the \"set\" of nearest possible worlds) where the conditions obtain. (A possible world W is said to be near to another possible world W in respect of R to the degree that the same things happen in W and W in respect of R; the more different something happens in two possible worlds in a certain respect, the \"further\" they are from one another in that respect.) Consider this conditional sentence: \"If George W. Bush hadn't become president of the U.S. in 2001, Al Gore would have.\" The sentence would be taken to express a claim that could be reformulated as follows: \"In all nearest worlds to our actual world (nearest in relevant respects) where George W. Bush didn't become president of the U.S. in 2001, Al Gore became president of the U.S. then instead.\" And on this interpretation of the sentence, if there is or are some nearest worlds to the actual world (nearest in relevant respects) where George W. Bush didn't become president but Al Gore didn't either, then the claim expressed by this counterfactual would be false.\n\nToday, possible worlds play a central role in many debates in philosophy, including especially debates over the Zombie Argument, and physicalism and supervenience in the philosophy of mind. Many debates in the philosophy of religion have been reawakened by the use of possible worlds. Intense debate has also emerged over the ontological status of possible worlds, provoked especially by David Lewis's defense of modal realism, the doctrine that talk about \"possible worlds\" is best explained in terms of innumerable, \"really existing\" worlds beyond the one we live in. The fundamental question here is: \"given\" that modal logic works, and that some possible-worlds semantics for modal logic is correct, \"what has to be true\" of the world, and just what \"are\" these possible worlds that we range over in our interpretation of modal statements? Lewis argued that what we range over are real, concrete \"worlds\" that exist just as unequivocally as our actual world exists, but that are distinguished from the actual world simply by standing in no spatial, temporal, or causal relations with the actual world. (On Lewis's account, the only \"special\" property that the \"actual\" world has is a relational one: that \"we\" are in it. This doctrine is called \"the indexicality of actuality\": \"actual\" is a merely indexical term, like \"now\" and \"here\".) Others, such as Robert Adams and William Lycan, reject Lewis's picture as metaphysically extravagant, and suggest in its place an interpretation of possible worlds as consistent, maximally complete sets of descriptions of or propositions about the world, so that a \"possible world\" is conceived of as a complete \"description\" of \"a way the world could be\" – rather than a \"world that is that way\". (Lewis describes their position, and similar positions such as those advocated by Alvin Plantinga and Peter Forrest, as \"\"ersatz\" modal realism\", arguing that such theories try to get the benefits of possible worlds semantics for modal logic \"on the cheap\", but that they ultimately fail to provide an adequate explanation.) Saul Kripke, in \"Naming and Necessity\", took explicit issue with Lewis's use of possible worlds semantics, and defended a \"stipulative\" account of possible worlds as purely \"formal\" (logical) entities rather than either really existent worlds or as some set of propositions or descriptions.\n\nPossible worlds theory in literary studies uses concepts from possible-world logic and applies them to worlds that are created by fictional texts, fictional universe. In particular, possible-world theory provides a useful vocabulary and conceptual framework with which to describe such worlds. However, a literary world is a specific type of possible world, quite distinct from the possible worlds in logic. This is because a literary text houses its own system of modality, consisting of actual worlds (actual events) and possible worlds (possible events). In fiction, the principle of simultaneity, it extends to cover the dimensional aspect, when it is contemplated that two or more physical objects, realities, perceptions and objects non-physical, can coexist in the same space-time. Thus, a literary universe is granted autonomy in much the same way as the actual universe.\n\nLiterary critics, such as Marie-Laure Ryan, Lubomír Doležel, and Thomas Pavel, have used possible-worlds theory to address notions of literary truth, the nature of fictionality, and the relationship between fictional worlds and reality. Taxonomies of fictional possibilities have also been proposed where the likelihood of a fictional world is assessed. Possible-world theory is also used within narratology to divide a specific text into its constituent worlds, possible and actual. In this approach, the modal structure of the fictional text is analysed in relation to its narrative and thematic concerns. Rein Raud has extended this approach onto \"cultural\" worlds, comparing possible worlds to the particular constructions of reality of different cultures. However, the metaphor of the \"cultural possible worlds\" relates to the framework of cultural relativism and, depending on the ontological status ascribed to possible worlds, warrants different, often controversial claims ranging from ethnocentrism to cultural imperialism.\n\n\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
{"id": "49535", "url": "https://en.wikipedia.org/wiki?curid=49535", "title": "Thought experiment", "text": "Thought experiment\n\nA thought experiment (, \"Gedanken-Experiment\", or \"Gedankenerfahrung\",) considers some hypothesis, theory, or principle for the purpose of thinking through its consequences. Given the structure of the experiment, it may not be possible to perform it, and even if it could be performed, there need not be an intention to perform it.\n\nThe common goal of a thought experiment is to explore the potential consequences of the principle in question:\n\nExamples of thought experiments include Schrödinger's cat, illustrating quantum indeterminacy through the manipulation of a perfectly sealed environment and a tiny bit of radioactive substance, and Maxwell's demon, which attempts to demonstrate the ability of a hypothetical finite being to violate the 2nd law of thermodynamics.\n\nThe ancient Greek δείκνυμι \"(transl.: deiknymi)\", or thought experiment, \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought-experiment. Perhaps the key experiment in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the 'thought experiment' technique. The 'experiment' is described by Galileo in \"Discorsi e dimostrazioni matematiche\" (1638) (literally, 'Discourses and Mathematical Demonstrations') thus:\n\nAlthough the extract does not convey the elegance and power of the 'demonstration' terribly well, it is clear that it is a 'thought' experiment, rather than a practical one. Strange then, as Cohen says, that philosophers and scientists alike refuse to acknowledge either Galileo in particular, or the thought experiment technique in general for its pivotal role in both science and philosophy. (The exception proves the rule — the iconoclastic philosopher of science, Paul Feyerabend, has also observed this methodological prejudice.)\n\nInstead, many philosophers prefer to consider 'Thought Experiments' to be merely the use of a hypothetical scenario to help understand the way things are.\n\nThought experiments have been used in a variety of fields, including philosophy, law, physics, and mathematics. In philosophy, they have been used at least since classical antiquity, some pre-dating Socrates. In law, they were well-known to Roman lawyers quoted in the Digest. In physics and other sciences, notable thought experiments date from the 19th and especially the 20th century, but examples can be found at least as early as Galileo.\n\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the Latin-German mixed term \"Gedankenexperiment\" (lit. thought experiment) circa 1812. Ørsted was also the first to use its entirely German equivalent, \"Gedankenversuch\", in 1820.\n\nMuch later, Ernst Mach used the term \"Gedankenexperiment\" in a different way, to denote exclusively the \"imaginary\" conduct of a \"real\" experiment that would be subsequently performed as a \"real physical experiment\" by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\n\nThe English term \"thought experiment\" was coined (as a calque) from Mach's \"Gedankenexperiment\", and it first appeared in the 1897 English translation of one of Mach’s papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time (for both scientists and philosophers). However, people had no way of categorizing it or speaking about it. This helps to explain the extremely wide and diverse range of the application of the term \"thought experiment\" once it had been introduced into English.\n\nThought experiments, which are well-structured, well-defined hypothetical questions that employ subjunctive reasoning (irrealis moods) – \"What might happen (or, what might have happened) if . . . \" – have been used to pose questions in philosophy at least since Greek antiquity, some pre-dating Socrates. In physics and other sciences many thought experiments date from the 19th and especially the 20th Century, but examples can be found at least as early as Galileo.\n\nIn thought experiments we gain new information by rearranging or reorganizing already known empirical data in a new way and drawing new (a priori) inferences from them or by looking at these data from a different and unusual perspective. In Galileo’s thought experiment, for example, the rearrangement of empirical experience consists in the original idea of combining bodies of different weight.\n\nThought experiments have been used in philosophy (especially ethics), physics, and other fields (such as cognitive psychology, history, political science, economics, social psychology, law, organizational studies, marketing, and epidemiology). In law, the synonym \"hypothetical\" is frequently used for such experiments.\n\nRegardless of their intended goal, all thought experiments display a patterned way of thinking that is designed to allow us to explain, predict and control events in a better and more productive way.\n\nIn terms of their theoretical consequences, thought experiments generally:\n\nThought experiments can produce some very important and different outlooks on previously unknown or unaccepted theories. However, they may make those theories themselves irrelevant, and could possibly create new problems that are just as difficult, or possibly more difficult to resolve.\n\nIn terms of their practical application, thought experiments are generally created to:\n\nScientists tend to use thought experiments as imaginary, \"proxy\" experiments prior to a real, \"physical\" experiment (Ernst Mach always argued that these gedankenexperiments were \"a necessary precondition for physical experiment\"). In these cases, the result of the \"proxy\" experiment will often be so clear that there will be no need to conduct a physical experiment at all.\n\nScientists also use thought experiments when particular physical experiments are impossible to conduct (Carl Gustav Hempel labeled these sorts of experiment \"theoretical experiments-in-imagination\"), such as Einstein's thought experiment of chasing a light beam, leading to special relativity. This is a unique use of a scientific thought experiment, in that it was never carried out, but led to a successful theory, proven by other empirical means.\n\nThe relation to real experiments can be quite complex, as can be seen again from an example going back to Albert Einstein. In 1935, with two coworkers, he published a paper on a newly created subject called later the EPR effect (EPR paradox). In this paper, starting from certain philosophical assumptions, on the basis of a rigorous analysis of a certain, complicated, but in the meantime assertedly realizable model, he came to the conclusion that \"quantum mechanics should be described as \"incomplete\"\". Niels Bohr asserted a refutation of Einstein's analysis immediately, and his view prevailed. After some decades, it was asserted that feasible experiments could prove the error of the EPR paper. These experiments tested the Bell inequalities published in 1964 in a purely theoretical paper. The above-mentioned EPR philosophical starting assumptions were considered to be falsified by empirical fact (e.g. by the optical \"real experiments\" of Alain Aspect).\n\nThus \"thought experiments\" belong to a theoretical discipline, usually to theoretical physics, but often to theoretical philosophy. In any case, it must be distinguished from a real experiment, which belongs naturally to the experimental discipline and has \"the final decision on \"true\" or \"not true\"\", at least in physics.\n\nThe first characteristic pattern that thought experiments display is their orientation\nin time. They are either:\n\nThe second characteristic pattern is their movement in time in relation to “the present\nmoment standpoint” of the individual performing the experiment; namely, in terms of:\n\nGenerally speaking, there are seven types of thought experiments in which one reasons from causes to effects, or effects to causes:\n\n\"Prefactual (before the fact) thought experiments\" — the term prefactual was coined by Lawrence J. Sanna in 1998 — speculate on possible future outcomes, given the present, and ask \"What will be the outcome if event E occurs?\"\n\n\"Counterfactual (contrary to established fact) thought experiments\" — the term \"counterfactual\" was coined by Nelson Goodman in 1947, extending Roderick Chisholm's (1946) notion of a \"contrary-to-fact conditional\" — speculate on the possible outcomes of a different past; and ask \"What might have happened if A had happened instead of B?\" (e.g., \"If Isaac Newton and Gottfried Leibniz had cooperated with each other, what would mathematics look like today?\").\n\nThe study of counterfactual speculation has increasingly engaged the interest of scholars in a wide range of domains such as philosophy, psychology, cognitive psychology, history, political science, economics, social psychology, law, organizational theory, marketing, and epidemiology.\n\n\"Semifactual thought experiments\" — the term \"semifactual\" was coined by Nelson Goodman in 1947 — speculate on the extent to which things might have remained the same, despite there being a different past; and asks the question Even though X happened instead of E, would Y have still occurred? (e.g., Even if the goalie had moved left, rather than right, could he have intercepted a ball that was traveling at such a speed?).\n\nSemifactual speculations are an important part of clinical medicine.\n\nThe activity of prediction attempts to project the circumstances of the present into the future. According to David Sarewitz and Roger Pielke (1999, p123), scientific prediction takes two forms:\n\nAlthough they perform different social and scientific functions, the only difference between the qualitatively identical activities of \"predicting\", \"forecasting,\" and \"nowcasting\" is the distance of the speculated future from the present moment occupied by the user. Whilst the activity of nowcasting, defined as “a detailed description of the current weather along with forecasts obtained by extrapolation up to 2 hours ahead”, is essentially concerned with describing the current state of affairs, it is common practice to extend the term “to cover very-short-range forecasting up to 12 hours ahead” (Browning, 1982, p.ix).\n\nThe activity of hindcasting involves running a forecast model after an event has happened in order to test whether the model's simulation is valid.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n\nThe activity of \"retrodiction\" (or \"postdiction\") involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (e.g., reverse engineering and forensics).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) the produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\", the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient.\n\nThe activity of \"backcasting\" — the term \"backcasting\" was coined by John Robinson in 1982 — involves establishing the description of a very definite and very specific future situation. It then involves an imaginary moving backwards in time, step-by-step, in as many stages as are considered necessary, from the future to the present to reveal the mechanism through which that particular specified future could be attained from the present.\n\nBackcasting is not concerned with predicting the future:\n\nAccording to Jansen (1994, p. 503:\n\nIn philosophy, a thought experiment typically presents an imagined scenario with the intention of eliciting an intuitive or reasoned response about the way things are in the thought experiment. (Philosophers might also supplement their thought experiments with theoretical reasoning designed to support the desired intuitive response.) The scenario will typically be designed to target a particular philosophical notion, such as morality, or the nature of the mind or linguistic reference. The response to the imagined scenario is supposed to tell us about the nature of that notion in any scenario, real or imagined.\n\nFor example, a thought experiment might present a situation in which an agent intentionally kills an innocent for the benefit of others. Here, the relevant question is not whether the action is moral or not, but more broadly whether a moral theory is correct that says morality is determined solely by an action's consequences (See Consequentialism). John Searle imagines a man in a locked room who receives written sentences in Chinese, and returns written sentences in Chinese, according to a sophisticated instruction manual. Here, the relevant question is not whether or not the man understands Chinese, but more broadly, whether a functionalist theory of mind is correct.\n\nIt is generally hoped that there is universal agreement about the intuitions that a thought experiment elicits. (Hence, in assessing their own thought experiments, philosophers may appeal to \"what we should say,\" or some such locution.) A successful thought experiment will be one in which intuitions about it are widely shared. But often, philosophers differ in their intuitions about the scenario.\n\nOther philosophical uses of imagined scenarios arguably are thought experiments also. In one use of scenarios, philosophers might imagine persons in a particular situation (maybe ourselves), and ask what they would do.\n\nFor example, in the veil of ignorance, John Rawls asks us to imagine a group of persons in a situation where they know nothing about themselves, and are charged with devising a social or political organization. The use of the state of nature to imagine the origins of government, as by Thomas Hobbes and John Locke, may also be considered a thought experiment. Søren Kierkegaard explored the possible ethical and religious implications of Abraham's binding of Isaac in \"Fear and Trembling\" Similarly, Friedrich Nietzsche, in \"On the Genealogy of Morals\", speculated about the historical development of Judeo-Christian morality, with the intent of questioning its legitimacy.\n\nAn early written thought experiment was Plato's allegory of the cave. Another historic thought experiment was Avicenna's \"Floating Man\" thought experiment in the 11th century. He asked his readers to imagine themselves suspended in the air isolated from all in order to demonstrate human self-awareness and self-consciousness, and the substantiality of the soul.\n\nIn many thought experiments, the scenario would be nomologically possible, or possible according to the laws of nature. John Searle's Chinese room is nomologically possible.\n\nSome thought experiments present scenarios that are not nomologically possible. In his Twin Earth thought experiment, Hilary Putnam asks us to imagine a scenario in which there is a substance with all of the observable properties of water (e.g., taste, color, boiling point), but is chemically different from water. It has been argued that this thought experiment is not nomologically possible, although it may be possible in some other sense, such as metaphysical possibility. It is debatable whether the nomological impossibility of a thought experiment renders intuitions about it moot.\n\nIn some cases, the hypothetical scenario might be considered metaphysically impossible, or impossible in any sense at all. David Chalmers says that we can imagine that there are zombies, or persons who are physically identical to us in every way but who lack consciousness. This is supposed to show that physicalism is false. However, some argue that zombies are inconceivable: we can no more imagine a zombie than we can imagine that 1+1=3. Others have claimed that the conceivability of a scenario may not entail its possibility.\n\nThe philosophical work of Stefano Gualeni focuses on the use of virtual worlds to materialize thought experiments and to playfully negotiate philosophical ideas. His arguments were originally presented in his 2015 book \"Virtual Worlds as Philosophical Tools\".\n\nGualeni's argument is that the history of philosophy has, until recently, merely been the history of written thought, and digital media can complement and enrich the limited and almost exclusively linguistic approach to philosophical thought. He considers virtual worlds to be philosophically viable and advantageous in contexts like those of thought experiments, when the recipients of a certain philosophical notion or perspective are expected to objectively test and evaluate different possible courses of action, or in cases where they are confronted with interrogatives concerning non-actual or non-human phenomenologies .\n\nAmong the most visible thought experiments designed by Stefano Gualeni:\n\nOther examples of playful, interactive thought experiments:\n\n\n\n\n\n\n\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
{"id": "31883", "url": "https://en.wikipedia.org/wiki?curid=31883", "title": "Uncertainty principle", "text": "Uncertainty principle\n\nIn quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position \"x\" and momentum \"p\", can be known.\n\nIntroduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. The formal inequality relating the standard deviation of position \"σ\" and the standard deviation of momentum \"σ\" was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928:\n\nwhere is the reduced Planck constant, ).\n\nHistorically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical \"explanation\" of quantum uncertainty. It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, \"the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology\". It must be emphasized that \"measurement\" does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.\n\nSince the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.\n\nThe uncertainty principle is not readily apparent on the macroscopic scales of everyday experience. So it is helpful to demonstrate how it applies to more easily understood physical situations. Two alternative frameworks for quantum physics offer different explanations for the uncertainty principle. The wave mechanics picture of the uncertainty principle is more visually intuitive, but the more abstract matrix mechanics picture formulates it in a way that generalizes more easily.\n\nMathematically, in wave mechanics, the uncertainty relation between position and momentum arises because the expressions of the wavefunction in the two corresponding orthonormal bases in Hilbert space are Fourier transforms of one another (i.e., position and momentum are conjugate variables). A nonzero function and its Fourier transform cannot both be sharply localized. A similar tradeoff between the variances of Fourier conjugates arises in all systems underlain by Fourier analysis, for example in sound waves: A pure tone is a sharp spike at a single frequency, while its Fourier transform gives the shape of the sound wave in the time domain, which is a completely delocalized sine wave. In quantum mechanics, the two key points are that the position of the particle takes the form of a matter wave, and momentum is its Fourier conjugate, assured by the de Broglie relation , where is the wavenumber.\n\nIn matrix mechanics, the mathematical formulation of quantum mechanics, any pair of non-commuting self-adjoint operators representing observables are subject to similar uncertainty limits. An eigenstate of an observable represents the state of the wavefunction for a certain measurement value (the eigenvalue). For example, if a measurement of an observable is performed, then the system is in a particular eigenstate of that observable. However, the particular eigenstate of the observable need not be an eigenstate of another observable : If so, then it does not have a unique associated measurement for it, as the system is not in an eigenstate of that observable.\n\nAccording to the de Broglie hypothesis, every object in the universe is a wave, i.e., a situation which gives rise to this phenomenon. The position of the particle is described by a wave function formula_1. The time-independent wave function of a single-moded plane wave of wavenumber \"k\" or momentum \"p\" is\n\nThe Born rule states that this should be interpreted as a probability density amplitude function in the sense that the probability of finding the particle between \"a\" and \"b\" is\n\nIn the case of the single-moded plane wave, formula_4 is a uniform distribution. In other words, the particle position is extremely uncertain in the sense that it could be essentially anywhere along the wave packet. \n\nOn the other hand, consider a wave function that is a sum of many waves, which we may write this as\n\nwhere \"A\" represents the relative contribution of the mode \"p\" to the overall total. The figures to the right show how with the addition of many plane waves, the wave packet can become more localized. We may take this a step further to the continuum limit, where the wave function is an integral over all possible modes\n\nwith formula_7 representing the amplitude of these modes and is called the wave function in momentum space. In mathematical terms, we say that formula_7 is the \"Fourier transform\" of formula_9 and that \"x\" and \"p\" are conjugate variables. Adding together all of these plane waves comes at a cost, namely the momentum has become less precise, having become a mixture of waves of many different momenta.\n\nOne way to quantify the precision of the position and momentum is the standard deviation \"σ\". Since formula_4 is a probability density function for position, we calculate its standard deviation.\n\nThe precision of the position is improved, i.e. reduced σ, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σ. Another way of stating this is that σ and σ have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the \"show\" button below to see a semi-formal derivation of the Kennard inequality using wave mechanics.\n(Ref ) \n\nIn matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the \"commutator\". For a pair of operators and , one defines their commutator as\nIn the case of position and momentum, the commutator is the canonical commutation relation\n\nThe physical meaning of the non-commutativity can be understood by considering the effect of the commutator on position and momentum eigenstates. Let formula_13 be a right eigenstate of position with a constant eigenvalue . By definition, this means that formula_14 Applying the commutator to formula_13 yields\nwhere is the identity operator.\n\nSuppose, for the sake of proof by contradiction, that formula_13 is also a right eigenstate of momentum, with constant eigenvalue . If this were true, then one could write\nOn the other hand, the above canonical commutation relation requires that\nThis implies that no quantum state can simultaneously be both a position and a momentum eigenstate.\n\nWhen a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is \"not\" a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, \n\nAs in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.\n\nThe most common general form of the uncertainty principle is the \"Robertson uncertainty relation\".\n\nFor an arbitrary Hermitian operator formula_22 we can associate a standard deviation\n\nwhere the brackets formula_24 indicate an expectation value. For a pair of operators formula_25 and formula_26, we may define their \"commutator\" as\n\nIn this notation, the Robertson uncertainty relation is given by\n\nThe Robertson uncertainty relation immediately follows from a slightly stronger inequality, the \"Schrödinger uncertainty relation\",\n\nwhere we have introduced the \"anticommutator\",\n\nSince the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below.\nSuppose we consider a quantum particle on a ring, where the wave function depends on an angular variable formula_41, which we may take to lie in the interval formula_42. Define \"position\" and \"momentum\" operators formula_25 and formula_26 by\n\nand\n\nwhere we impose periodic boundary conditions on formula_26. Note that the definition of formula_25 depends on our choice to have formula_41 range from 0 to formula_50. These operators satisfy the usual commutation relations for position and momentum operators, formula_51.\n\nNow let formula_52 be any of the eigenstates of formula_26, which are given by formula_54. Note that these states are normalizable, unlike the eigenstates of the momentum operator on the line. Note also that the operator formula_25 is bounded, since formula_41 ranges over a bounded interval. Thus, in the state formula_52, the uncertainty of formula_58 is zero and the uncertainty of formula_59 is finite, so that \nAlthough this result appears to violate the Robertson uncertainty principle, the paradox is resolved when we note that formula_52 is not in the domain of the operator formula_62, since multiplication by formula_41 disrupts the periodic boundary conditions imposed on formula_26. Thus, the derivation of the Robertson relation, which requires formula_65 and formula_66 to be defined, does not apply. (These also furnish an example of operators satisfying the canonical commutation relations but not the Weyl relations.)\n\nFor the usual position and momentum operators formula_67 and formula_68 on the real line, no such counterexamples can occur. As long as formula_69 and formula_70 are defined in the state formula_52, the Heisenberg uncertainty principle holds, even if formula_52 fails to be in the domain of formula_73 or of formula_74.\n\nConsider a one-dimensional quantum harmonic oscillator (QHO). It is possible to express the position and momentum operators in terms of the creation and annihilation operators:\n\nUsing the standard rules for creation and annihilation operators on the eigenstates of the QHO,\nthe variances may be computed directly,\nThe product of these standard deviations is then\n\nIn particular, the above Kennard bound is saturated for the ground state , for which the probability density is just the normal distribution.\n\nIn a quantum harmonic oscillator of characteristic angular frequency ω, place a state that is offset from the bottom of the potential by some displacement \"x\" as\nwhere Ω describes the width of the initial state but need not be the same as ω. Through integration over the , we can solve for the -dependent solution. After many cancelations, the probability densities reduce to\nwhere we have used the notation formula_85 to denote a normal distribution of mean μ and variance σ. Copying the variances above and applying trigonometric identities, we can write the product of the standard deviations as\n\nFrom the relations\n\nwe can conclude the following: (the right most equality holds only when Ω = \"ω\") .\n\nA coherent state is a right eigenstate of the annihilation operator,\nwhich may be represented in terms of Fock states as\n\nOne expects that the factor may be replaced by , \nwhich is only known if either or is convex.\n\nThe mathematician G. H. Hardy formulated the following uncertainty principle: it is not possible for and to both be \"very rapidly decreasing\". Specifically, if in formula_91 is such that\nand\n\nthen, if , while if , then there is a polynomial of degree such that\n\nThis was later improved as follows: if formula_96 is such that\n\nthen\nwhere is a polynomial of degree and is a real positive definite matrix.\n\nThis result was stated in Beurling's complete works without proof and proved in Hörmander (the case formula_99) and Bonami, Demange, and Jaming for the general case. Note that Hörmander–Beurling's version implies the case in Hardy's Theorem while the version by Bonami–Demange–Jaming covers the full strength of Hardy's Theorem. A different proof of Beurling's theorem based on Liouville's theorem appeared in\nref.\n\nA full description of the case as well as the following extension to Schwartz class distributions appears in ref.\n\nTheorem. If a tempered distribution formula_100 is such that\n\nand\nthen\nfor some convenient polynomial and real positive definite matrix of type .\n\nWerner Heisenberg formulated the uncertainty principle at Niels Bohr's institute in Copenhagen, while working on the mathematical foundations of quantum mechanics.\n\nIn 1925, following pioneering work with Hendrik Kramers, Heisenberg developed matrix mechanics, which replaced the ad hoc old quantum theory with modern quantum mechanics. The central premise was that the classical concept of motion does not fit at the quantum level, as electrons in an atom do not travel on sharply defined orbits. Rather, their motion is smeared out in a strange way: the Fourier transform of its time dependence only involves those frequencies that could be observed in the quantum jumps of their radiation.\n\nHeisenberg's paper did not admit any unobservable quantities like the exact position of the electron in an orbit at any time; he only allowed the theorist to talk about the Fourier components of the motion. Since the Fourier components were not defined at the classical frequencies, they could not be used to construct an exact trajectory, so that the formalism could not answer certain overly precise questions about where the electron was or how fast it was going.\n\nIn March 1926, working in Bohr's institute, Heisenberg realized that the non-commutativity implies the uncertainty principle. This implication provided a clear physical interpretation for the non-commutativity, and it laid the foundation for what became known as the Copenhagen interpretation of quantum mechanics. Heisenberg showed that the commutation relation implies an uncertainty, or in Bohr's language a complementarity. Any two variables that do not commute cannot be measured simultaneously—the more precisely one is known, the less precisely the other can be known. Heisenberg wrote:It can be expressed in its simplest form as follows: One can never know with perfect accuracy both of those two important factors which determine the movement of one of the smallest particles—its position and its velocity. It is impossible to determine accurately \"both\" the position and the direction and speed of a particle \"at the same instant\".\n\nIn his celebrated 1927 paper, \"Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik\" (\"On the Perceptual Content of Quantum Theoretical Kinematics and Mechanics\"), Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties Δx and Δp. Instead, he gave some plausible estimates in each case separately. In his Chicago lecture he refined his principle:\n\nKennard in 1927 first proved the modern inequality:\n\nwhere , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states.\n\nThroughout the main body of his original 1927 paper, written in German, Heisenberg used the word, \"Ungenauigkeit\" (\"indeterminacy\"),\nto describe the basic theoretical principle. Only in the endnote did he switch to the word, \"Unsicherheit\" (\"uncertainty\"). When the English-language version of Heisenberg's textbook, \"The Physical Principles of the Quantum Theory\", was published in 1930, however, the translation \"uncertainty\" was used, and it became the more commonly used term in the English language thereafter.\n\nThe principle is quite counter-intuitive, so the early students of quantum theory had to be reassured that naive measurements to violate it were bound always to be unworkable. One way in which Heisenberg originally illustrated the intrinsic impossibility of violating the uncertainty principle is by utilizing the observer effect of an imaginary microscope as a measuring device.\n\nHe imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it.\n\nThe combination of these trade-offs implies that no matter what photon wavelength and aperture size are used, the product of the uncertainty in measured position and measured momentum is greater than or equal to a lower limit, which is (up to a small numerical factor) equal to Planck's constant. Heisenberg did not care to formulate the uncertainty principle as an exact limit (which is elaborated below), and preferred to use it instead, as a heuristic quantitative statement, correct up to small numerical factors, which makes the radically new noncommutativity of quantum mechanics inevitable.\n\nThe Copenhagen interpretation of quantum mechanics and Heisenberg's Uncertainty Principle were, in fact, seen as twin targets by detractors who believed in an underlying determinism and realism. According to the Copenhagen interpretation of quantum mechanics, there is no fundamental reality that the quantum state describes, just a prescription for calculating experimental results. There is no way to say what the state of a system fundamentally is, only what the result of observations might be.\n\nAlbert Einstein believed that randomness is a reflection of our ignorance of some fundamental property of reality, while Niels Bohr believed that the probability distributions are fundamental and irreducible, and depend on which measurements we choose to perform. Einstein and Bohr debated the uncertainty principle for many years.\n\nWolfgang Pauli called Einstein's fundamental objection to the uncertainty principle \"the ideal of the detached observer\" (phrase translated from the German):\n\nThe first of Einstein's thought experiments challenging the uncertainty principle went as follows:\n\nBohr's response was that the wall is quantum mechanical as well, and that to measure the recoil to accuracy , the momentum of the wall must be known to this accuracy before the particle passes through. This introduces an uncertainty in the position of the wall and therefore the position of the slit equal to , and if the wall's momentum is known precisely enough to measure the recoil, the slit's position is uncertain enough to disallow a position measurement.\n\nA similar analysis with particles diffracting through multiple slits is given by Richard Feynman.\n\nBohr was present when Einstein proposed the thought experiment which has become known as Einstein's box. Einstein argued that \"Heisenberg's uncertainty equation implied that the uncertainty in time was related to the uncertainty in energy, the product of the two being related to Planck's constant.\" Consider, he said, an ideal box, lined with mirrors so that it can contain light indefinitely. The box could be weighed before a clockwork mechanism opened an ideal shutter at a chosen instant to allow one single photon to escape. \"We now know, explained Einstein, precisely the time at which the photon left the box.\" \"Now, weigh the box again. The change of mass tells the energy of the emitted light. In this manner, said Einstein, one could measure the energy emitted and the time it was released with any desired precision, in contradiction to the uncertainty principle.\"\n\nBohr spent a sleepless night considering this argument, and eventually realized that it was flawed. He pointed out that if the box were to be weighed, say by a spring and a pointer on a scale, \"since the box must move vertically with a change in its weight, there will be uncertainty in its vertical velocity and therefore an uncertainty in its height above the table. ... Furthermore, the uncertainty about the elevation above the earth's surface will result in an uncertainty in the rate of the clock,\" because of Einstein's own theory of gravity's effect on time.\n\"Through this chain of uncertainties, Bohr showed that Einstein's light box experiment could not simultaneously measure exactly both the energy of the photon and the time of its escape.\"\n\nBohr was compelled to modify his understanding of the uncertainty principle after another thought experiment by Einstein. In 1935, Einstein, Podolsky and Rosen (see EPR paradox) published an analysis of widely separated entangled particles. Measuring one particle, Einstein realized, would alter the probability distribution of the other, yet here the other particle could not possibly be disturbed. This example led Bohr to revise his understanding of the principle, concluding that the uncertainty was not caused by a direct interaction.\n\nBut Einstein came to much more far-reaching conclusions from the same thought experiment. He believed the \"natural basic assumption\" that a complete description of reality would have to predict the results of experiments from \"locally changing deterministic quantities\" and therefore would have to include more information than the maximum possible allowed by the uncertainty principle.\n\nIn 1964, John Bell showed that this assumption can be falsified, since it would imply a certain inequality between the probabilities of different experiments. Experimental results confirm the predictions of quantum mechanics, ruling out Einstein's basic assumption that led him to the suggestion of his \"hidden variables\". These hidden variables may be \"hidden\" because of an illusion that occurs during observations of objects that are too large or too small. This illusion can be likened to rotating fan blades that seem to pop in and out of existence at different locations and sometimes seem to be in the same place at the same time when observed. This same illusion manifests itself in the observation of subatomic particles. Both the fan blades and the subatomic particles are moving so fast that the illusion is seen by the observer. Therefore, it is possible that there would be predictability of the subatomic particles behavior and characteristics to a recording device capable of very high speed tracking...Ironically this fact is one of the best pieces of evidence supporting Karl Popper's philosophy of invalidation of a theory by falsification-experiments. That is to say, here Einstein's \"basic assumption\" became falsified by experiments based on Bell's inequalities. For the objections of Karl Popper to the Heisenberg inequality itself, see below.\n\nWhile it is possible to assume that quantum mechanical predictions are due to nonlocal, hidden variables, and in fact David Bohm invented such a formulation, this resolution is not satisfactory to the vast majority of physicists. The question of whether a random outcome is predetermined by a nonlocal theory can be philosophical, and it can be potentially intractable. If the hidden variables are not constrained, they could just be a list of random digits that are used to produce the measurement outcomes. To make it sensible, the assumption of nonlocal hidden variables is sometimes augmented by a second assumption—that the size of the observable universe puts a limit on the computations that these variables can do. A nonlocal theory of this sort predicts that a quantum computer would encounter fundamental obstacles when attempting to factor numbers of approximately 10,000 digits or more; a potentially achievable task in quantum mechanics.\n\nKarl Popper approached the problem of indeterminacy as a logician and metaphysical realist. He disagreed with the application of the uncertainty relations to individual particles rather than to ensembles of identically prepared particles, referring to them as \"statistical scatter relations\". In this statistical interpretation, a \"particular\" measurement may be made to arbitrary precision without invalidating the quantum theory. This directly contrasts with the Copenhagen interpretation of quantum mechanics, which is non-deterministic but lacks local hidden variables.\n\nIn 1934, Popper published \"Zur Kritik der Ungenauigkeitsrelationen\" (\"Critique of the Uncertainty Relations\") in \"Naturwissenschaften\", and in the same year \"Logik der Forschung\" (translated and updated by the author as \"The Logic of Scientific Discovery\" in 1959), outlining his arguments for the statistical interpretation. In 1982, he further developed his theory in \"Quantum theory and the schism in Physics\", writing:\n[Heisenberg's] formulae are, beyond all doubt, derivable \"statistical formulae\" of the quantum theory. But they have been \"habitually misinterpreted\" by those quantum theorists who said that these formulae can be interpreted as determining some upper limit to the \"precision of our measurements\". [original emphasis]\n\nPopper proposed an experiment to falsify the uncertainty relations, although he later withdrew his initial version after discussions with Weizsäcker, Heisenberg, and Einstein; this experiment may have influenced the formulation of the EPR experiment.\n\nThe many-worlds interpretation originally outlined by Hugh Everett III in 1957 is partly meant to reconcile the differences between Einstein's and Bohr's views by replacing Bohr's wave function collapse with an ensemble of deterministic and independent universes whose \"distribution\" is governed by wave functions and the Schrödinger equation. Thus, uncertainty in the many-worlds interpretation follows from each observer within any universe having no knowledge of what goes on in the other universes.\n\nSome scientists including Arthur Compton and Martin Heisenberg have suggested that the uncertainty principle, or at least the general probabilistic nature of quantum mechanics, could be evidence for the two-stage model of free will. One critique, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature. The standard view, however, is that this decoherence is overcome by both screening and decoherence-free subspaces found in biological cells.\n\nThere is reason to believe that violating the uncertainty principle also strongly implies the violation of the second law of thermodynamics.\n\n"}
{"id": "97517", "url": "https://en.wikipedia.org/wiki?curid=97517", "title": "Unintended consequences", "text": "Unintended consequences\n\nIn the social sciences, unintended consequences (sometimes unanticipated consequences or unforeseen consequences) are outcomes that are not the ones foreseen and intended by a purposeful action. The term was popularised in the twentieth century by American sociologist Robert K. Merton.\n\nUnintended consequences can be grouped into three types:\n\nThe idea of \"unintended consequences\" dates back at least to John Locke who discussed the unintended consequences of interest rate regulation in his letter to Sir John Somers, Member of Parliament.\n\nThe idea was also discussed by Adam Smith, the Scottish Enlightenment, and consequentialism (judging by results).\n\nSociologist Robert K. Merton popularised this concept in the twentieth century.\n\nIn \"The Unanticipated Consequences of Purposive Social Action\" (1936), Merton tried to apply a systematic analysis to the problem of unintended consequences of deliberate acts intended to cause social change. He emphasized that his term \"purposive action\", \"[was exclusively] concerned with 'conduct' as distinct from 'behavior.' That is, with action that involves motives and consequently a choice between various alternatives\". Merton's usage included deviations from what Max Weber defined as rational social action: instrumentally rational and value rational. Merton also stated that \"no blanket statement categorically affirming or denying the practical feasibility of \"all\" social planning is warranted.\"\n\nMore recently, the \"law of unintended consequences\" has come to be used as an adage or idiomatic warning that an intervention in a complex system tends to create unanticipated and often undesirable outcomes.\n\nAkin to Murphy's law, it is commonly used as a wry or humorous warning against the hubristic belief that humans can fully control the world around them.\n\nPossible causes of unintended consequences include the world's inherent complexity (parts of a system responding to changes in the environment), perverse incentives, human stupidity, self-deception, failure to account for human nature, or other cognitive or emotional biases. As a sub-component of complexity (in the scientific sense), the chaotic nature of the universe—and especially its quality of having small, apparently insignificant changes with far-reaching effects (e.g., the butterfly effect)—applies.\n\nRobert K. Merton listed five possible causes of unanticipated consequences in 1936:\n\n\nIn addition to Merton's causes, psychologist Stuart Vyse has noted that groupthink, described by Irving Janis, has been blamed for some decisions that result in unintended consequences.\n\nThe creation of \"no-man's lands\" during the Cold War, in places such as the border between Eastern and Western Europe, and the Korean Demilitarized Zone, has led to large natural habitats.\n\nThe sinking of ships in shallow waters during wartime has created many artificial coral reefs, which can be scientifically valuable and have become an attraction for recreational divers. Retired ships have been purposely sunk in recent years, in an effort to replace coral reefs lost to global warming and other factors.\n\nIn medicine, most drugs have unintended consequences ('side effects') associated with their use. However, some are beneficial. For instance, aspirin, a pain reliever, is also an anticoagulant that can help prevent heart attacks and reduce the severity and damage from thrombotic strokes. The existence of beneficial side effects also leads to off-label use—prescription or use of a drug for an unlicensed purpose. Famously, the drug Viagra was developed to lower blood pressure, with its use for treating erectile dysfunction being discovered as a side effect in clinical trials.\n\nThe implementation of a profanity filter by AOL in 1996 had the unintended consequence of blocking residents of Scunthorpe, North Lincolnshire, England from creating accounts due to a false positive. The accidental censorship of innocent language, known as the Scunthorpe problem, has been repeated and widely documented.\n\nThe objective of microfinance initiatives is to foster micro-entrepreneurs but an unintended consequence can be informal intermediation: That is, some entrepreneurial borrowers become informal intermediaries between microfinance initiatives and poorer micro-entrepreneurs. Those who more easily qualify for microfinance split loans into smaller credit to poorer borrowers. Informal intermediation ranges from casual intermediaries at the good or benign end of the spectrum to 'loan sharks' at the professional and sometimes criminal end of the spectrum.\n\nIn 1990, the Australian state of Victoria made safety helmets mandatory for all bicycle riders. While there was a reduction in the number of head injuries, there was also an unintended reduction in the number of juvenile cyclists—fewer cyclists obviously leads to fewer injuries, assuming all else being equal. The risk of death and serious injury per cyclist seems to have increased, possibly due to risk compensation. Research by Vulcan, \"et al.\" found that the reduction in juvenile cyclists was because the youths considered wearing a bicycle helmet unfashionable. A health-benefit model developed at Macquarie University in Sydney suggests that, while helmet use reduces \"the risk of head or brain injury by approximately two-thirds or more\", the decrease in exercise caused by reduced cycling as a result of helmet laws is counterproductive in terms of net health.\n\nProhibition in the 1920s United States, originally enacted to suppress the alcohol trade, drove many small-time alcohol suppliers out of business and consolidated the hold of large-scale organized crime over the illegal alcohol industry. Since alcohol was still popular, criminal organisations producing alcohol were well-funded and hence also increased their other activities. Similarly, the War on Drugs, intended to suppress the illegal drug trade, instead increased the power and profitability of drug cartels who became the primary source of the products.\n\nIn CIA jargon, \"blowback\" describes the unintended, undesirable consequences of covert operations, such as the funding of the Afghan Mujahideen and the destabilization of Afghanistan contributing to the rise of the Taliban and Al-Qaeda.\n\nThe introduction of exotic animals and plants for food, for decorative purposes, or to control unwanted species often leads to more harm than good done by the introduced species.\n\nThe protection of the steel industry in the United States reduced production of steel in the United States, increased costs to users, and increased unemployment in associated industries.\n\nIn 2003, Barbra Streisand unsuccessfully sued Kenneth Adelman and Pictopia.com for posting a photograph of her home online. Before the lawsuit had been filed, only 6 people had downloaded the file, two of them Streisand's attorneys. The lawsuit drew attention to the image, resulting in 420,000 people visiting the site. The Streisand effect was named after this incident, describing when an attempt to censor or remove a certain piece of information instead draws attention to the material being suppressed, resulting in the material instead becoming widely known, reported on, and distributed.\n\nPassenger-side airbags in motorcars were intended as a safety feature, but led to an increase in child fatalities in the mid-1990s as small children were being hit by deploying airbags during collisions. The supposed solution to this problem, moving the child seat to the back of the vehicle, led to an increase in the number of children forgotten in unattended vehicles, some of whom died under extreme temperature conditions.\n\nRisk compensation, or the Peltzman effect, occurs after implementation of safety measures intended to reduce injury or death (e.g. bike helmets, seatbelts, etc.). People may feel safer than they really are and take additional risks which they would not have taken without the safety measures in place. This may result in no change, or even an increase, in morbidity or mortality, rather than a decrease as intended.\n\nThe British government, concerned about the number of venomous cobra snakes in Delhi, offered a bounty for every dead cobra. This was a successful strategy as large numbers of snakes were killed for the reward. Eventually, enterprising people began breeding cobras for the income. When the government became aware of this, they scrapped the reward program, causing the cobra breeders to set the now-worthless snakes free. As a result, the wild cobra population further increased. The apparent solution for the problem made the situation even worse, becoming known as the Cobra effect.\n\nTheobald Mathew's temperance campaign in 19th-century Ireland resulted in thousands of people vowing never to drink alcohol again. This led to the consumption of diethyl ether, a much more dangerous intoxicant — due to its high flammability — by those seeking to become intoxicated without breaking the letter of their pledge.\n\nIt was thought that adding south-facing conservatories to British houses would reduce energy consumption by providing extra insulation and warmth from the sun. However, people tended to use the conservatories as living areas, installing heating and ultimately increasing overall energy consumption.\n\nA reward for lost nets found along the Normandy coast was offered by the French government between 1980 and 1981. This resulted in people vandalizing nets to collect the reward.\n\nBeginning in the 1940s and continuing into the 1960s, the Canadian federal government gave the Catholic Church in Quebec $2.25 per day per psychiatric patient for their cost of care, but only $0.75 a day per orphan. The perverse result is that the orphan children were diagnosed as mentally ill so the church could receive the larger amount of money. This psychiatric misdiagnosis affected up to 20,000 people, and the children are known as the Duplessis Orphans.\n\nThere have been attempts to curb the consumption of sugary beverages by imposing a tax on them. However, a study found that the reduced consumption was only temporary. Also, there was an increase in the consumption of beer among households.\n\nThe New Jersey Childproof Handgun Law, which was intended to protect children from accidental discharge of firearms by forcing all future firearms sold in New Jersey to contain \"smart\" safety features, has delayed, if not stopped entirely, the introduction of such firearms to New Jersey markets. The wording of the law caused significant public backlash, fuelled by gun rights lobbyists, and several shop owners offering such guns received death threats and stopped stocking them In 2014, 12 years after the law was passed, it was suggested the law be repealed if gun rights lobbyists agree not to resist the introduction of \"smart\" firearms.\n\nDrug prohibition can lead drug traffickers to prefer stronger, more dangerous substances, that can be more easily smuggled and distributed than other, less concentrated substances.\n\nTelevised drug prevention advertisements may lead to increased drug use.\n\nAbstinence-only sex education has been shown to increase teenage pregnancy rates, rather than reduce them, when compared to either comprehensive sex education or no sex education at all.\n\nIncreasing usage of search engines, also including recent image search features, has contributed in the ease of which media is consumed. Some abnormalities in usage may have shifted preferences for pornographic film actors, as the producers began using common search queries or tags to label the actors in new roles.\n\nThe passage of the Stop Enabling Sex Traffickers Act has led to a reported increase in risky behaviors by sex workers as a result of quashing their ability to seek and screen clients online, forcing them back onto the streets or into the dark web. The ads posted were previously an avenue for advocates to reach out to those wanting to escape the trade.\n\nMost modern technologies have negative consequences that are both unavoidable and unpredictable. For example, almost all environmental problems, from chemical pollution to global warming, are the unexpected consequences of the application of modern technologies. Traffic congestion, deaths and injuries from car accidents, air pollution, and global warming are unintended consequences of the invention and large scale adoption of the automobile. Hospital infections are the unexpected side-effect of antibiotic resistance, and even human overpopulation is the side effect of various technological (i.e., agricultural and industrial) revolutions.\n\nBecause of the complexity of ecosystems, deliberate changes to an ecosystem or other environmental interventions will often have (usually negative) unintended consequences. Sometimes, these effects cause permanent irreversible changes. Examples include:\n\n"}
