{"id": "38746191", "url": "https://en.wikipedia.org/wiki?curid=38746191", "title": "Bote-Darai language", "text": "Bote-Darai language\n\nBote (Bote-Majhi) and Darai are mutually intelligible tribal dialects of Nepal that are close to Danwar Rai but otherwise unclassified. Speakers are rapidly shifting to Nepali.\n"}
{"id": "3691505", "url": "https://en.wikipedia.org/wiki?curid=3691505", "title": "Brava Creole", "text": "Brava Creole\n\nBrava Creole is the name given to the variant of Cape Verdean Creole spoken mainly in the Brava Island of Cape Verde. It belongs to the Sotavento Creoles branch. The speakers of this form of Capeverdean Creole are 8,000 (1.36% of the national population). One of the least spoken being seventh place and one of the firsts to have written literature, in which Eugénio Tavares wrote some of his poems.\n\nBesides the main characteristics of Sotavento Creoles the Brava Creole has also the following ones:\n\n\n"}
{"id": "7946278", "url": "https://en.wikipedia.org/wiki?curid=7946278", "title": "Co-premise", "text": "Co-premise\n\nA co-premise is a premise in reasoning and informal logic which is not the main supporting reason for a contention or a lemma, but is logically necessary to ensure the validity of an argument. One premise by itself, or a group of co-premises can form a reason. \n\nEvery significant term or phrase appearing in a premise of a simple argument, should also appear in the contention/conclusion or in a co-premise. But this by itself does not guarantee a valid argument, see the fallacy of the undistributed middle for an example of this.\n\nSometimes a co-premise will not be explicitly stated. This type of argument is known as an 'enthymematic' argument, and the co-premise may be referred to as a 'hidden' or an 'unstated' co-premise and will often be subject to an inference objection. In this argument map of a simple argument the two reasons for the main contention are co-premises and not separate reasons for believing the contention to be true. They are both necessary to ensure that the argument as a whole retains logical validity.\nIn this example, \"What the Bible says is true\" is a hidden co-premise.\n"}
{"id": "10249042", "url": "https://en.wikipedia.org/wiki?curid=10249042", "title": "Coaxial power connector", "text": "Coaxial power connector\n\nA coaxial power connector is an electrical power connector used for attaching extra-low voltage devices such as consumer electronics to external electricity. Also known as barrel connectors, concentric barrel connectors or tip connectors, these small cylindrical connectors come in an enormous variety of sizes.\n\nBarrel plug connectors are commonly used to interface the secondary side of a power supply with the device. Some of these jacks contain a normally closed switch; the switch can disconnect internal batteries whenever the external power supply is connected.\n\nThe connector pairs for barrel connectors are defined in terms of 'plugs' and 'receptacles'; receptacles are more commonly called 'sockets' or 'jacks' (US). Receptacles may be panel-mounted or circuit board-mounted. Plugs are on cables. Some 'in-line' receptacles are also cable-mounted.\n\nThere is a long history in electrical engineering of referring to such power plugs\"—that is to say, plugs with holes instead of prongs—\"as female, particularly regarding coaxial transmission of electricity. Type N connectors, and all EC 60320#Appliance couplers IEC 60320 \"appliance coupler\" plugs are examples of this. That said, while IEC 60320 provides gender-based standards for higher-voltage plugs (such as the cable plugged into a standard computer power supply), they have not yet defined gender-based standards for low-voltage coaxial power connectors such as those discussed herein; I.E., which component is \"male\" and which \"female.\" As a result, there are varying opinions in this regard. Many industrial suppliers avoid gender terminology but many do not. Similarly, some people view the corded plug as female and some perceive it as male. Some, after consideration and surveys, found that user perception of which was male and which female was evenly split.\n\nPower is generally supplied \"by\" a plug \"to\" a receptacle. Cables are available with one in-line receptacle fanning out to a number of plugs, so that many devices may be supplied by one supply. As the use of a plug implies a cable, even a short stub, some power supplies carry panel-mounted receptacles instead to avoid this cable, i.e. the normal convention of power from plug to receptacle is reversed. Cables for such cases are available with a plug at each end, although cables or adapters with two \"receptacles\" are not widely available.\n\nOn the female plug, the outer body is metallic and cylindrical in shape, and comprises one of the two contacts. The second, inside contact is a hollow metallic cylinder constructed to accept insertion of the pin in the corresponding male connector. The inner and outer barrels are separated by an insulating layer. The outer contact is generally called the barrel, sleeve or ring, and the inner contact is called the tip.\n\nThere is typically a single spring-loaded contact at the side of the male connector and a pin in the center corresponding to the intended female plug.\n\nThere are many different sizes of coaxial power connectors (see table at end of this article).\n\nContact ratings commonly vary from unspecified up to 5 amperes (11 amperes for special high-power versions). Voltage is often unspecified, but may be up to 48V with 12V typical. The smaller types usually have lower current and voltage ratings.\n\nIt is quite possible that new sizes will continue to appear and disappear. One possible reason that a particular manufacturer may use a new size is to discourage use of third-party power supplies, either for technical reasons or to force use of their own accessories, or both.\n\nThe sizes and shapes of connectors do not consistently correspond to the same power specifications across manufacturers and models. Two connectors from different manufacturers with different sizes could potentially be attached to power supplies with the same voltage and current. Alternatively, connectors of the same size can be part of power supplies with different voltages and currents. Use of the wrong power supply may cause severe equipment damage, or even fire.\n\nGeneric plugs are often described by their inside diameter, such as \"2.1mm DC plugs\" and \"2.5mm DC (direct current) plugs\".\n\nAfter the two common 5.5mm OD (outer diameter) plugs, the next-most common size is 3.5mm OD with a 1.3mm ID (inner diameter), usually about 9.5mm in length, although both longer and shorter versions also exist. These 3.5mm OD plugs are normally used for lower voltages and currents.\n\nA ring-shaped 'locking detent' or 'high-retention feature', present on the barrel of some DC coaxial connectors, is a feature intended to prevent accidental disconnection. Typically, this feature is a conical cut-back section of the tip, just behind the insulator that separates the inner from outer contact surfaces.\n\nA 'lock-ring DC coaxial connector' uses a captive threaded ring or collar to secure the connection between the plug and jack. This design offers strong resistance to unplugging when used properly.\nA 'lock-tab DC coaxial connector' (also called 'bayonet lock') offers a compromise that resists unplugging, but which will disengage when pulled hard enough. This connector uses small metal tab protrusions on the connector barrel to lock the plug in place, requiring a special push-and-rotate motion to engage the locks.\n\nThere are several standards in existence, such as IEC, EIAJ in Japan and DIN in Germany. More recently, some manufacturers appear to have implemented their own system correlating voltage and plug size. In addition, there appears to be a trend to standardize DC connector to negative barrel (or sleeve) of a coaxial power connector.\n\nIEC 60130-10:1971 defines five DC power connectors.\n\n\nFive plug and matching socket or jack designs are defined by the EIAJ standard RC-5320A (also called JEITA RC-5320A). Each of these plugs is used with a specified voltage range. Most manufacturers use a yellow insulating material to distinguish these plugs from other similar-looking DC plugs.\n\n\nEIAJ-04 and 05 have an internal male pin in the plug. The 01 through 03 sizes do not and are similar to the generic plugs in structure. These five EIAJ plugs are 9.5 mm in length and have a current rating of 2A.\n\nThere are two other, less common, connectors defined by EIAJ; RC-5321 and RC-5322. The latter is designed for both 12 V and 24 V automotive applications.\n\nThe German national standards organization DIN (Deutsches Institut für Normung—German Institute for Standardization) issued DIN 45323, which defines two DC power plug and jack (respectively) sizes. At least one of these sizes has a maximum rating of 34 V and 3 A. The information here is inferred from catalog references, as the German standard has not been translated into English yet.\n\n\nThis list attempts to show all known sizes, and is annotated with some manufacturers producing selected types, since each manufacturer makes its own unique subset of the known types. Note that the example part numbers given may have different connector barrel (sleeve) lengths, and are not necessarily exact equivalents. There are many more design variants than can be listed in this table, so only a small sampling of part numbers is given.\n\nConnector size is often listed in the format OD (outer diameter) x ID (inner diameter) x L (length of barrel) and expressed in millimeters. Designations may vary between manufacturers.\n\nCoaxial plugs that have a male center pin will have another measurement, Center Pin Diameter (CPD). These plugs are often used for higher power applications such as portable computers.\n\nThere are a number of sizes listed below that appear to be quite similar, and while the tolerances of these connectors are typically indicated as ±0.05 or ±0.03 mm by the manufacturers, there is still ambiguity as to whether two sizes differing by only 0.05 mm (or where the specification is only given to the nearest 0.10 mm) warrants listing them separately here.\nRadioShack sold a line of adapter plugs for universal AC adapters. Each \"Adaptaplug\" had a single-letter code, but did not provide any other official designation, nor did RadioShack publish the complete specifications and tolerances on barrel and pin dimensions. RadioShack's web site listed the diameters to the nearest 0.1 mm, and sometimes differs slightly from the official EIAJ RC-5320A standard dimensions. This list may include some parts RadioShack has discontinued but are retained here for completeness.\n\n\n"}
{"id": "1168184", "url": "https://en.wikipedia.org/wiki?curid=1168184", "title": "Confessional writing", "text": "Confessional writing\n\nIn literature, confessional writing is a first-person style that is often presented as an ongoing diary or letters, distinguished by revelations of a person's deeper or darker motivations.\n\nOriginally, the term derived from confession: The writer is not only autobiographically recounting his life, but confessing to his sins. Among the earlier examples is St. Augustine's \"Confessions\", perhaps the first autobiography of Western Europe. In it, he not only recounted the events of his life, he wrestled with their meaning and significance, as in a passage where he tried to fathom why he had stolen pears with friends, not to eat but to throw away.\n\nJean-Jacques Rousseau turned it to a secular purpose in his \"Confessions\".\n\nFrom this meaning evolved the meaning of writing that reveals more of the writer's motivations, particularly the darker reactions, and the events that are normally kept secret.\n\nFictionally, the confessional story is a story written, in the first person, about emotionally fraught and morally charged situations in which a fictional character is caught. These stories may be anything from thinly veiled recounting of the writer's life to completely fictional works.\n\nWith the advent of the magazine \"True Story\" in 1919 and the imitations of it, the confessional (or romance) magazine was created, containing such stories. Such confessions magazines were chiefly aimed at an audience of working-class women. Their formula has been characterized as \"sin-suffer-repent\": The heroine violates standards of behavior, suffers as a consequence, learns her lesson and resolves to live in light of it, not embittered by her pain.\n\n"}
{"id": "23789867", "url": "https://en.wikipedia.org/wiki?curid=23789867", "title": "Critical language awareness", "text": "Critical language awareness\n\nIn linguistics, critical language awareness (CLA) refers to an understanding of social, political, and ideological aspects of language, linguistic variation, and discourse. It functions as a pedagogical application of a critical discourse analysis (CDA), which is a research approach that regards language as a social practice. Critical language awareness as a part of language education teaches students how to analyze the language that they and others use. More specifically, critical language awareness is a consideration of how features of language such as words, grammar, and discourse choices reproduce, reinforce, or challenge certain ideologies and struggles for power and dominance.\n\nRegarding linguistic variation, Fairclough argued that it is insufficient to teach students to use \"appropriate\" language without considering why that language is preferred and who makes that decision (as well as the implications for speakers who do not use \"appropriate language\").\n\nCLA generally includes consideration of how a person may be marginalized by speaking a particular way, especially if that way of speaking serves as an index of their race, ethnicity, religion, social status, etc.\n\nBecause power is reproduced through language, CLA is \"a prerequisite for effective democratic citizenship, and should therefore be seen as an entitlement for citizens, especially children developing towards citizenship in the educational system\".\n\nCritical language awareness has been applied to educating students in South Africa how language was used to maintain and perpetuate the apartheid state.\n\n\n"}
{"id": "613505", "url": "https://en.wikipedia.org/wiki?curid=613505", "title": "Distributive writing", "text": "Distributive writing\n\nDistributive writing is the collective authorship of texts.\n\nThis further requires both a definition of \"collective\" and \"texts\", where collective means a connected group of individuals and texts are inscribed symbols chained together to achieve a larger meaning than isolated symbols. This places emphasis on texts being represented as writings. This could be written words, iconic symbology (e.g., graffiti), computer programming languages (C/C++, Java, Perl, etc.), meta-level mark-up (HTML, XML, SVG, PostScript), and their derivative works. Also, not to be excluded are all the above in various languages. Further, to define texts, we must also have an interpreter for the texts. For computer programming languages, we have a compiler, for writings we have written words interpreted by our mental faculties, and for meta-level mark-up there are web browsers, printers to interpret postscript, and various software applications which turn textual representations into another format. (Patrick Deegan and Jon Phillips, 2004)\n\nSocial Software enables people to connect, communicate, and collaborate. It is explicitly the social which is of importance and is what is operated on. It is the commodity in the system. This is different from Distributive Writing because social software is based upon software, whereas DW is not, and is not just about collaborative writing. It is also about other forms of socializing.\n\nGroupWare is about software and more importantly, in common use to describe combining many pieces of software together into a group for so-called easy access for an individual. The original definition had to do with a group of people operating on something collaboratively through software, but this has changed meaning due to corporate appropriation to describe software suites like Microsoft Office and OpenOffice.org.\n\nDistributive writing is not just bound to computing like CSCT.\n\nSynchronous – System of authorship where both author's make changes in real time (at the same time).\n\nAsynchronous – System of authorship where both author's make changes in non-real time (render time or not at the time).\n\n"}
{"id": "2964506", "url": "https://en.wikipedia.org/wiki?curid=2964506", "title": "Double articulation", "text": "Double articulation\n\nDouble articulation, or duality of patterning is a concept used in linguistics and semiotics. It refers to the two-level structure inherent to a sign system, insofar as it is composed by two kinds of elements: 1) significant or meaningful, and 2) distinctive or meaningless.\n\n\"Double articulation\" refers to the two fold structure of the stream of speech, which can be primarily divided into \"meaningful\" signs (like words or morphemes), and then secondarily into \"distinctive\" elements (like letters or phonemes). For example, the meaningful English word \"cat\" is composed of the sounds [k], [æ], and [t], which are meaningless as separate individual sounds (and which can also be combined to form the separate words \"tack\" and \"act\", with distinct meanings). These sounds, called phonemes, represent the secondary and lowest level of articulation in the hierarchy of the organization of speech. Higher, primary, levels of organization (including morphology, syntax, and semantics) govern the combination of these individually meaningless phonemes into meaningful elements.\n\nThe French concept of \"double articulation\" was first introduced by the André Martinet, in 1949. The English calque \"double articulation\" has been criticised as inaccurate and is often replaced by \"duality of patterning\".\n\nAccording to Charles F. Hockett and other linguists, this duality is an important property of human languages, since it allows for the expression of a potentially infinite number of meaningful language sequences. Strictly speaking, however, such expressiveness follows from generativity or productivity (a finite number of components combining via rules to produce a potentially infinite arrangement of novel utterances), not of duality per se (one could have a system with 2 levels of the kind referred to as duality, and yet have only finite productivity. For further discussion, see figurae, as well as Hockett's_design_features, which treats productivity and duality as distinct essential properties of language.\n\nSign languages may have less double articulation because more gestures are possible than sound and able to convey more meaning without double articulation.\n\n"}
{"id": "25032938", "url": "https://en.wikipedia.org/wiki?curid=25032938", "title": "Dyslalia", "text": "Dyslalia\n\nDyslalia means difficulties in talking due to structural defects in speech organs. for example; sigmatism: which is defective pronunciation of sibilant sounds for example \"S\" pronounced as \"TH\" and Rhotacism: in which the letter \"R\" pronounced as \"I or Y\". It does not include speech impairment due to neurological or other factors.\n\n\n"}
{"id": "2545484", "url": "https://en.wikipedia.org/wiki?curid=2545484", "title": "English in computing", "text": "English in computing\n\nThe English language is sometimes described as the \"lingua franca\" of computing. In comparison to other sciences, where Latin and Greek are the principal sources of vocabulary, computer science borrows more extensively from English. Due to the technical limitations of early computers, and the lack of international standards on the Internet, computer users were limited to using English and the Latin alphabet. However, this historical limitation is less present today. Most software products are localized in numerous languages and the use of the Unicode character encoding has resolved problems with non-Latin alphabets. Some limitations have only been changed recently, such as with domain names, which previously allowed only ASCII characters.\n\nThe computing terminology of many languages borrows from English. Some language communities resist actively to that trend, and in other cases English is used extensively and more directly. This section gives some examples for the use of English terminology in other languages, and also mentions any notable differences.\n\nBoth English and Russian have influence over Bulgarian computing vocabulary. However, in many cases the borrowed terminology is translated, and not transcribed phonetically. Combined with the use of Cyrillic this can make it difficult to recognize loanwords. For example, the Bulgarian term for motherboard is 'дънна платка' (IPA /danna platka/ or literally \"bottom board\").\n\n\nThe Faroese language has a sparse scientific vocabulary \"based on the language itself\". Many Faroese scientific words are borrowed and/or modified versions of especially Nordic and English equivalents. The vocabulary is constantly evolving and thus new words often die out, and only a few survive and become widely used. Examples of successful words include e.g. \"telda\" (computer), \"kurla\" (at sign) and \"ambætari\" (server).\n\nIn French, there are some generally accepted English loan-words, but there is also a distinct effort to avoid them. In France, the Académie française is responsible for the standardisation of the language and often coins new technological terms. Some of them are accepted in practice, in other cases the English loanwords remain predominant. In Quebec, the Office québécois de la langue française has a similar function.\n\n\nIn German, English words are very often used as well:\n\nThe Icelandic language has its own vocabulary of scientific terms, still English borrowings exist. English or Icelandicised words are mostly used in casual conversations, whereas the Icelandic words might be longer or not widespread.\n\nIt's quite common to use English words in regards to computing in all Scandinavian languages.\n\nnouns: keyboard, webside, mail, software, blogg, spam\n\nverbs: å boote, å spamme, å blogge\n\nPolish language words derived from English:\n\n\nThe English influence on the software industry and the internet in Latin America has borrowed significantly from the Castilian lexicon.\n\n\n\nMany computing terms in Spanish share a common root with their English counterpart. In these cases, both terms are understood, but the Spanish is preferred for formal use:\n\nThe early computer software and hardware had very little support for alphabets other than the Latin. As a result of this it was difficult or impossible to represent languages based on other scripts. The ASCII character encoding, created in the 1960s, only supported 128 different characters. With the use of additional software it was possible to provide support for some languages, for instance those based on the Cyrillic alphabet. However, complex-script languages like Chinese or Japanese need more characters than the 256 limit imposed by 8-bit character encodings. Some computers created in the former USSR had native support for the Cyrillic alphabet.\n\nThe wide adoption of Unicode, and UTF-8 on the web, resolved most of these historical limitations. ASCII remains the de facto standard for command interpreters, programming languages and text-based communication protocols.\n\n\nThe syntax of most programming languages uses English keywords, and therefore it could be argued some knowledge of English is required in order to use them. However, it is important to recognize all programming languages are in the class of formal languages. They are very different from any natural language, including English.\n\nSome examples of non-English programming languages:\n\n\nMany application protocols use text strings for requests and parameters, rather than the binary values commonly used in lower layer protocols. The request strings are generally based on English words, although in some cases the strings are contractions or acronyms of English expressions, which renders them somewhat cryptic to anyone not familiar with the protocol, whatever their proficiency in English. Nevertheless, the use of word-like strings is a convenient mnemonic device that allows a person skilled in the art (and with sufficient knowledge of English) to execute the protocol manually from a keyboard, usually for the purpose of finding a problem with the service.\n\nExamples:\n\nIt is notable that response codes, that is, the strings sent back by the recipient of a request, are typically numeric: for instance, in HTTP (and some borrowed by other protocols)\n\nThis is because response codes also need to convey unambiguous information, but can have various nuances that the requester may optionally use to vary its subsequent actions. To convey all such \"sub-codes\" with alphabetic words would be unwieldy, and negate the advantage of using pseudo-English words. Since responses are usually generated by software they do not need to be mnemonic. Numeric codes are also more easily analysed and categorised when they are processed by software, instead of a human testing the protocol by manual input.\n\nMany personal computers have a BIOS chip, displaying text in English during boot time.\n\nKeyboard shortcuts are usually defined in terms of English keywords such as CTRL+F for find.\n\nEnglish is the largest language on the World Wide Web, with 27% of internet users.\n\nWeb user percentages usually focus on raw comparisons of the first language of those who access the web. Just as important is a consideration of second- and foreign-language users; i.e., the first language of a user does not necessarily reflect which language he or she regularly employs when using the web.\n\nEnglish-language users appear to be a plurality of web users, consistently cited as around one-third of the overall (near one billion). This reflects the relative affluence of English-speaking countries and high Internet penetration rates in them. This lead may be eroding due mainly to a rapid increase of Chinese users.\n\nFirst-language users among other relatively affluent countries appear generally stable, the two largest being German and Japanese, which each have between 5% and 10% of the overall share.\n\nOne widely quoted figure for the amount of web content in English is 80%. Other sources show figures five to fifteen points lower, though still well over 50%. There are two notable facts about these percentages:\n\nThe English web content is greater than the number of first-language English users by as much as 2 to 1.\n\nGiven the enormous lead it already enjoys and its increasing use as a \"lingua franca\" in other spheres, English web content may continue to dominate even as English first-language Internet users decline. This is a classic positive feedback loop: new Internet users find it helpful to learn English and employ it online, thus reinforcing the language's prestige and forcing subsequent new users to learn English as well.\n\nCertain other factors (some predating the medium's appearance) have propelled English into a majority web-content position. Most notable in this regard is the tendency for researchers and professionals to publish in English to ensure maximum exposure. The largest database of medical bibliographical information, for example, shows English was the majority language choice for the past forty years and its share has continually increased over the same period.\n\nThe fact that non-Anglophones regularly publish in English only reinforces the language's dominance. English has a rich technical vocabulary (largely because native and non-native speakers alike use it to communicate technical ideas) and many IT and technical professionals use English regardless of country of origin (Linus Torvalds, for instance, comments his code in English, despite being from Finland and having Swedish as his first language).\n"}
{"id": "10374", "url": "https://en.wikipedia.org/wiki?curid=10374", "title": "Essay", "text": "Essay\n\nAn essay is, generally, a piece of writing that gives the author's own argument — but the definition is vague, overlapping with those of a paper, an article, a pamphlet, and a short story. Essays have traditionally been sub-classified as formal and informal. Formal essays are characterized by \"serious purpose, dignity, logical organization, length,\" whereas the informal essay is characterized by \"the personal element (self-revelation, individual tastes and experiences, confidential manner), humor, graceful style, rambling structure, unconventionality or novelty of theme,\" etc.\n\nEssays are commonly used as literary criticism, political manifestos, learned arguments, observations of daily life, recollections, and reflections of the author. Almost all modern essays are written in prose, but works in verse have been dubbed essays (e.g., Alexander Pope's \"An Essay on Criticism\" and \"An Essay on Man\"). While brevity usually defines an essay, voluminous works like John Locke's \"An Essay Concerning Human Understanding\" and Thomas Malthus's \"An Essay on the Principle of Population\" are counterexamples.\n\nIn some countries (e.g., the United States and Canada), essays have become a major part of formal education. Secondary students are taught structured essay formats to improve their writing skills; admission essays are often used by universities in selecting applicants, and in the humanities and social sciences essays are often used as a way of assessing the performance of students during final exams.\n\nThe concept of an \"essay\" has been extended to other media beyond writing. A film essay is a movie that often incorporates documentary filmmaking styles and focuses more on the evolution of a theme or idea. A photographic essay covers a topic with a linked series of photographs that may have accompanying text or captions.\n\nAn essay has been defined in a variety of ways. One definition is a \"prose composition with a focused subject of discussion\" or a \"long, systematic discourse\".\nIt is difficult to define the genre into which essays fall. Aldous Huxley, a leading essayist, gives guidance on the subject. He notes that \"the essay is a literary device for saying almost everything about almost anything\", and adds that \"by tradition, almost by definition, the essay is a short piece\". Furthermore, Huxley argues that \"essays belong to a literary species whose extreme variability can be studied most effectively within a three-poled frame of reference\". \nThese three poles (or worlds in which the essay may exist) are:\nHuxley adds that the most satisfying essays \"...make the best not of one, not of two, but of all the three worlds in which it is possible for the essay to exist.\"\n\nThe word \"essay\" derives from the French infinitive \"essayer\", \"to try\" or \"to attempt\". In English \"essay\" first meant \"a trial\" or \"an attempt\", and this is still an alternative meaning. The Frenchman Michel de Montaigne (1533–1592) was the first author to describe his work as essays; he used the term to characterize these as \"attempts\" to put his thoughts into writing, and his essays grew out of his commonplacing. Inspired in particular by the works of Plutarch, a translation of whose \"Œuvres Morales\" (\"Moral works\") into French had just been published by Jacques Amyot, Montaigne began to compose his essays in 1572; the first edition, entitled \"Essais\", was published in two volumes in 1580. For the rest of his life, he continued revising previously published essays and composing new ones. Francis Bacon's essays, published in book form in 1597, 1612, and 1625, were the first works in English that described themselves as \"essays\". Ben Jonson first used the word \"essayist\" in English in 1609, according to the \"Oxford English Dictionary\".\n\nEnglish essayists included Robert Burton (1577–1641) and Sir Thomas Browne (1605–1682). In France, Michel de Montaigne's three volume \"Essais\" in the mid 1500s contain over 100 examples widely regarded as the predecessor of the modern essay. In Italy, Baldassare Castiglione wrote about courtly manners in his essay \"Il Cortigiano\". In the 17th century, the Jesuit Baltasar Gracián wrote about the theme of wisdom. During the Age of Enlightenment, essays were a favored tool of polemicists who aimed at convincing readers of their position; they also featured heavily in the rise of periodical literature, as seen in the works of Joseph Addison, Richard Steele and Samuel Johnson. In the 18th and 19th centuries, Edmund Burke and Samuel Taylor Coleridge wrote essays for the general public. The early 19th century, in particular, saw a proliferation of great essayists in English – William Hazlitt, Charles Lamb, Leigh Hunt and Thomas de Quincey all penned numerous essays on diverse subjects. In the 20th century, a number of essayists tried to explain the new movements in art and culture by using essays (e.g., T.S. Eliot). Whereas some essayists used essays for strident political themes, Robert Louis Stevenson and Willa Cather wrote lighter essays. Virginia Woolf, Edmund Wilson, and Charles du Bos wrote literary criticism essays.\n\nAs with the novel, essays existed in Japan several centuries before they developed in Europe with a genre of essays known as \"zuihitsu\" — loosely connected essays and fragmented ideas. Zuihitsu have existed since almost the beginnings of Japanese literature. Many of the most noted early works of Japanese literature are in this genre. Notable examples include \"The Pillow Book\" (c. 1000), by court lady Sei Shōnagon, and \"Tsurezuregusa\" (1330), by particularly renowned Japanese Buddhist monk Yoshida Kenkō. Kenkō described his short writings similarly to Montaigne, referring to them as \"nonsensical thoughts\" written in \"idle hours\". Another noteworthy difference from Europe is that women have traditionally written in Japan, though the more formal, Chinese-influenced writings of male writers were more prized at the time.\n\nThis section describes the different forms and styles of essay writing. These forms and styles are used by an array of authors, including university students and professional essayists.\n\nThe defining features of a \"cause and effect\" essay are causal chains that connect from a cause to an effect, careful language, and chronological or emphatic order. A writer using this rhetorical method must consider the subject, determine the purpose, consider the audience, think critically about different causes or consequences, consider a thesis statement, arrange the parts, consider the language, and decide on a conclusion.\n\nClassification is the categorization of objects into a larger whole while division is the breaking of a larger whole into smaller parts.\n\nCompare and contrast essays are characterized by a basis for comparison, points of comparison, and analogies. It is grouped by the object (chunking) or by point (sequential). The comparison highlights the similarities between two or more similar objects while contrasting highlights the differences between two or more objects. When writing a compare/contrast essay, writers need to determine their purpose, consider their audience, consider the basis and points of comparison, consider their thesis statement, arrange and develop the comparison, and reach a conclusion. Compare and contrast is arranged emphatically.\n\nExpository essay is used to inform, describe or explain a topic, using important facts and teaching reader about the topic. Mostly written in third-person, using \"it\", \"he\", \"she\", \"they\". Expository essay uses formal language to discuss someone or something. Examples of expository essays are: a medical or biological condition, social or technological process, life or character of a famous person. Writing of expository essay often consists of following next steps: organizing thoughts (brainstorming), researching a topic, developing a thesis statement, writing the introduction, writing the body of essay, writing the conclusion. Expository essays are often assigned as a part of SAT and other standardized testings or as a homework for high school and college students.\n\nDescriptive writing is characterized by sensory details, which appeal to the physical senses, and details that appeal to a reader's emotional, physical, or intellectual sensibilities. Determining the purpose, considering the audience, creating a dominant impression, using descriptive language, and organizing the description are the rhetorical choices to consider when using a description. A description is usually arranged spatially but can also be chronological or emphatic. The focus of a description is the scene. Description uses tools such as denotative language, connotative language, figurative language, metaphor, and simile to arrive at a dominant impression. One university essay guide states that \"descriptive writing says what happened or what another author has discussed; it provides an account of the topic\".\nLyric essays are an important form of descriptive essays.\n\nIn the dialectic form of the essay, which is commonly used in philosophy, the writer makes a thesis and argument, then objects to their own argument (with a counterargument), but then counters the counterargument with a final and novel argument. This form benefits from presenting a broader perspective while countering a possible flaw that some may present. This type is sometimes called an ethics paper.\n\nAn exemplification essay is characterized by a generalization and relevant, representative, and believable examples including anecdotes. Writers need to consider their subject, determine their purpose, consider their audience, decide on specific examples, and arrange all the parts together when writing an exemplification essay.\nAn essayist writes a \"familiar essay\" if speaking to a single reader, writing about both themselves, and about particular subjects. Anne Fadiman notes that \"the genre's heyday was the early nineteenth century,\" and that its greatest exponent was Charles Lamb. She also suggests that while critical essays have more brain than the heart, and personal essays have more heart than brain, familiar essays have equal measures of both.\n\nA history essay sometimes referred to as a thesis essay describes an argument or claim about one or more historical events and supports that claim with evidence, arguments, and references. The text makes it clear to the reader why the argument or claim is as such.\n\nA narrative uses tools such as flashbacks, flash-forwards, and transitions that often build to a climax. The focus of a narrative is the plot. When creating a narrative, authors must determine their purpose, consider their audience, establish their point of view, use dialogue, and organize the narrative. A narrative is usually arranged chronologically.\n\nAn argumentative essay is a critical piece of writing, aimed at presenting objective analysis of the subject matter, narrowed down to a single topic. The main idea of all the criticism is to provide an opinion either of positive or negative implication. As such, a critical essay requires research and analysis, strong internal logic and sharp structure. Its structure normally builds around introduction with a topic's relevance and a thesis statement, body paragraphs with arguments linking back to the main thesis, and conclusion. In addition, an argumentative essay may include a refutation section where conflicting ideas are acknowledged, described, and criticized. Each argument of argumentative essay should be supported with sufficient evidence, relevant to the point.\n\nA process essay is used for an explanation of making or breaking something. Often, it is written in chronological order or numerical order to show step-by-step processes. It has all the qualities of a technical document with the only difference is that it is often written in descriptive mood, while a technical document is mostly in imperative mood.\n\nAn economic essay can start with a thesis, or it can start with a theme. It can take a narrative course and a descriptive course. It can even become an argumentative essay if the author feels the need. After the introduction, the author has to do his/her best to expose the economic matter at hand, to analyze it, evaluate it, and draw a conclusion. If the essay takes more of a narrative form then the author has to expose each aspect of the economic puzzle in a way that makes it clear and understandable for the reader\n\nA \"reflective essay\" is an analytical piece of writing in which the writer describes a real or imaginary scene, event, interaction, passing thought, memory, or form — adding a personal reflection on the meaning of the topic in the author's life. Thus, the focus is not merely descriptive. The writer doesn’t just describe the situation, but revisits the scene with more detail and emotion to examine what went well, or reveal a need for additional learning — and may relate what transpired to the rest of the author's life.\n\nThe logical progression and organizational structure of an essay can take many forms. Understanding how the movement of thought is managed through an essay has a profound impact on its overall cogency and ability to impress. A number of alternative logical structures for essays have been visualized as diagrams, making them easy to implement or adapt in the construction of an argument.\n\nIn countries like the United States and the United Kingdom, essays have become a major part of a formal education in the form of free response questions. Secondary students in these countries are taught structured essay formats to improve their writing skills, and essays are often used by universities in these countries in selecting applicants (\"see\" admissions essay). In both secondary and tertiary education, essays are used to judge the mastery and comprehension of the material. Students are asked to explain, comment on, or assess a topic of study in the form of an essay. In some courses, university students must complete one or more essays over several weeks or months. In addition, in fields such as the humanities and social sciences, mid-term and end of term examinations often require students to write a short essay in two or three hours.\n\nIn these countries, so-called academic essays also called \"papers\", are usually more formal than literary ones. They may still allow the presentation of the writer's own views, but this is done in a logical and factual manner, with the use of the first person often discouraged. Longer academic essays (often with a word limit of between 2,000 and 5,000 words) are often more discursive. They sometimes begin with a short summary analysis of what has previously been written on a topic, which is often called a literature review.\n\nLonger essays may also contain an introductory page that defines words and phrases of the essay's topic. Most academic institutions require that all substantial facts, quotations, and other supporting material in an essay be referenced in a bibliography or works cited page at the end of the text. This scholarly convention helps others (whether teachers or fellow scholars) to understand the basis of facts and quotations the author uses to support the essay's argument and helps readers evaluate to what extent the argument is supported by evidence, and to evaluate the quality of that evidence. The academic essay tests the student's ability to present their thoughts in an organized way and is designed to test their intellectual capabilities.\n\nOne of the challenges facing universities is that in some cases, students may submit essays purchased from an essay mill (or \"paper mill\") as their own work. An \"essay mill\" is a ghostwriting service that sells pre-written essays to university and college students. Since plagiarism is a form of academic dishonesty or academic fraud, universities and colleges may investigate papers they suspect are from an essay mill by using plagiarism detection software, which compares essays against a database of known mill essays and by orally testing students on the contents of their papers.\n\nEssays often appear in magazines, especially magazines with an intellectual bent, such as \"The Atlantic\" and \"Harpers\". Magazine and newspaper essays use many of the essay types described in the section on forms and styles (e.g., descriptive essays, narrative essays, etc.). Some newspapers also print essays in the op-ed section.\nEmployment essays detailing experience in a certain occupational field are required when applying for some jobs, especially government jobs in the United States. Essays known as Knowledge Skills and Executive Core Qualifications are required when applying to certain US federal government positions.\n\nA KSA, or \"Knowledge, Skills, and Abilities,\" is a series of narrative statements that are required when applying to Federal government job openings in the United States. KSAs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The knowledge, skills, and abilities necessary for the successful performance of a position are contained on each job vacancy announcement. KSAs are brief and focused essays about one's career and educational background that presumably qualify one to perform the duties of the position being applied for.\n\nAn Executive Core Qualification, or ECQ, is a narrative statement that is required when applying to Senior Executive Service positions within the US Federal government. Like the KSAs, ECQs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The Office of Personnel Management has established five executive core qualifications that all applicants seeking to enter the Senior Executive Service must demonstrate.\n\nA film essay (or \"cinematic essay\") consists of the evolution of a theme or an idea rather than a plot per se, or the film literally being a cinematic accompaniment to a narrator reading an essay. From another perspective, an essay film could be defined as a documentary film visual basis combined with a form of commentary that contains elements of self-portrait (rather than autobiography), where the signature (rather than the life story) of the filmmaker is apparent. The cinematic essay often blends documentary, fiction, and experimental film making using tones and editing styles.\n\nThe genre is not well-defined but might include propaganda works of early Soviet parliamentarians like Dziga Vertov, present-day filmmakers including Chris Marker, Michael Moore (\"Roger & Me\" (1989), \"Bowling for Columbine\" (2002) and \"Fahrenheit 9/11\" (2004)), Errol Morris (\"The Thin Blue Line\" (1988)), Morgan Spurlock (\"Supersize Me: A Film of Epic Portions\") and Agnès Varda. Jean-Luc Godard describes his recent work as \"film-essays\". Two filmmakers whose work was the antecedent to the cinematic essay include Georges Méliès and Bertolt Brecht. Méliès made a short film (\"The Coronation of Edward VII\" (1902)) about the 1902 coronation of King Edward VII, which mixes actual footage with shots of a recreation of the event. Brecht was a playwright who experimented with film and incorporated film projections into some of his plays. Orson Welles made an essay film in his own pioneering style, released in 1974, called \"F for Fake\", which dealt specifically with art forger Elmyr de Hory and with the themes of deception, \"fakery,\" and authenticity in general. These are often published online on video hosting services.\n\nDavid Winks Gray's article \"The essay film in action\" states that the \"essay film became an identifiable form of filmmaking in the 1950s and '60s\". He states that since that time, essay films have tended to be \"on the margins\" of the filmmaking the world. Essay films have a \"peculiar searching, questioning tone ... between documentary and fiction\" but without \"fitting comfortably\" into either genre. Gray notes that just like written essays, essay films \"tend to marry the personal voice of a guiding narrator (often the director) with a wide swath of other voices\". The University of Wisconsin Cinematheque website echoes some of Gray's comments; it calls a film essay an \"intimate and allusive\" genre that \"catches filmmakers in a pensive mood, ruminating on the margins between fiction and documentary\" in a manner that is \"refreshingly inventive, playful, and idiosyncratic\".\n\nIn the realm of music, composer Samuel Barber wrote a set of \"Essays for Orchestra,\" relying on the form and content of the music to guide the listener's ear, rather than any extra-musical plot or story.\n\nA photographic essay strives to cover a topic with a linked series of photographs. Photo essays range from purely photographic works to photographs with captions or small notes to full-text essays with a few or many accompanying photographs. Photo essays can be sequential in nature, intended to be viewed in a particular order — or they may consist of non-ordered photographs viewed all at once or in an order that the viewer chooses. All photo essays are collections of photographs, but not all collections of photographs are photo essays. Photo essays often address a certain issue or attempt to capture the character of places and events.\nIn the visual arts, an essay is a preliminary drawing or sketch that forms a basis for a final painting or sculpture, made as a test of the work's composition (this meaning of the term, like several of those following, comes from the word \"essay\"'s meaning of \"attempt\" or \"trial\").\n\n\n\n"}
{"id": "2651813", "url": "https://en.wikipedia.org/wiki?curid=2651813", "title": "Horizontal and vertical writing in East Asian scripts", "text": "Horizontal and vertical writing in East Asian scripts\n\nMany East Asian scripts can be written horizontally or vertically. Chinese, Japanese and Korean scripts can be oriented in either direction, as they consist mainly of disconnected logographic or syllabic units, each occupying a square block of space, thus allowing for flexibility for which direction texts can be written, be it horizontally from left-to-right, horizontally from right-to-left, vertically from top-to-bottom, and even vertically from bottom-to-top. \n\nHorizontal writing is known in Chinese as hengpai (), in Japanese as yokogaki (, \"horizontal writing\", also \"yokogumi\", ), and in Korean as garosseugi () or \"hoengseo\" (; ).\n\nVertical writing is known respectively as zongpai (), tategaki (, \"vertical writing\", also \"tategumi\", ), or serosseugi () or \"jongseo\" (; ).\n\nTraditionally, Chinese, Japanese, and Korean are written vertically in columns going from top to bottom and ordered from right to left, with each new column starting to the left of the preceding one. The stroke order and stroke direction of Chinese characters (\"hanzi\" in Chinese, \"kanji\" in Japanese, \"hanja\" in Korean), Japanese kana, and Korean Hangul all facilitate writing in this manner. In addition, writing in vertical columns from right to left facilitated writing with a brush in the right hand while continually unrolling the sheet of paper or scroll with the left. Since the nineteenth century, it has become increasingly common for these languages to be written horizontally, from left to right, with successive rows going from top to bottom, under the influence of European languages such as English, although vertical writing is still frequently used in Hong Kong, Japan, Macau, Korea, and Taiwan.\n\nChinese characters, Japanese kana, and Korean hangul can be written horizontally or vertically. There are some small differences in orthography. In horizontal writing it is more common to use Arabic numerals, whereas Chinese numerals are more common in vertical text.\n\nIn these scripts, the positions of punctuation marks, for example the relative position of commas and full stops (periods), differ between horizontal and vertical writing. Punctuation such as the parentheses, quotation marks, book title marks (Chinese), ellipsis mark, dash, wavy dash (Japanese), proper noun mark (Chinese), wavy book title mark (Chinese), emphasis mark, and \"chōon\" mark (Japanese) are all rotated 90 degrees when switching between horizontal and vertical text.\n\nWhere a text is written in horizontal format, pages are read in the same order as English books, with the binding at the left and pages progressing to the right. Vertical books are printed the other way round, with the binding at the right, and pages progressing to the left.\n\nRuby characters, like \"furigana\" in Japanese which provides a phonetic guide for unusual or difficult-to-read characters, follow the direction of the main text. Example in Japanese, with furigana in green:\n\nHowever, \"zhuyin\" in Taiwanese Chinese is usually written vertically regardless of the direction of the main text.\n\nInserted text in the Roman alphabet is usually written horizontally, or turned sideways when it appears in vertical text, with the base of the characters on the left.\n\nHistorically, vertical writing was the standard system, and horizontal writing was only used where a sign had to fit in a constrained space, such as over the gate of a temple or the signboard of a shop. This horizontal writing is in fact a special case of vertical writing in which each column contains just one character.\n\nTherefore, before the end of World War II in Japan, those signs were read right to left.\n\nToday, the left-to-right direction is dominant in all three languages for horizontal writing: this is due partly to the influence of English and other Western languages to make it easier to read when the two languages are found together (for example, on signs at an airport or train station), and partly to the increased use of computerized typesetting and word processing software, most of which does not directly support right-to-left layout of East Asian languages. However, right-to-left horizontal writing is still seen in these scripts, in such places as signs, on the right-hand side of vehicles, and on the right-hand side of stands selling food at festivals. It is also used to simulate archaic writing, for example in reconstructions of old Japan for tourists, and it is still found in the captions and titles of some newspapers.\n\nThe first printed Chinese text in horizontal alignment was Robert Morrison's \"Dictionary of the Chinese language\", published in 1815–1823 in Macau.\n\nThe earliest widely known Chinese publication using horizontal alignment was the magazine \"Science\" (). Its first issue in January 1915 explained the (then) unusual format:\n\nIn the subsequent decades, the occurrence of words in a Western script (predominantly English) became increasingly frequent, and readers began to appreciate the unwieldiness of rotating the paper at each occurrence for vertically set texts. This accelerated acceptance of horizontal writing.\nWith the proliferation of horizontal text, both horizontal and vertical came to be used concurrently. Proponents of horizontal text argued that vertical text in right-to-left columns was smudged easily when written, and moreover demanded greater movement from the eyes when read. Vertical text proponents considered horizontal text to be a break from established tradition.\n\nAfter the success of the communist revolution in 1949, the People's Republic of China decided to use horizontal writing. All newspapers in China changed from vertical to horizontal alignment on January 1, 1956. In publications, text is run horizontally although book titles on spines and some newspaper headlines remain vertical for convenience. Inscriptions of signs on most state organs are still vertical.\n\nIn Singapore, vertical writing has also become rare. In Taiwan, Hong Kong, Macau, and among older overseas Chinese communities, horizontal writing has been gradually adopted since the 1990s. By the early 2000s, most newspapers in these areas had switched to left-to-right horizontal writing, either entirely or in a combination of vertical text with horizontal left-to-right headings.\n\nHorizontal text came into Japanese in the Meiji era, when the Japanese began to print Western language dictionaries. Initially they printed the dictionaries in a mixture of horizontal Western and vertical Japanese text, which meant readers had to rotate the book ninety degrees to read the Western text. Because this was unwieldy, the idea of \"yokogaki\" came to be accepted. One of the first publications to partially use \"yokogaki\" was a German to Japanese dictionary (, \"Shūchinsōzu Dokuwa Jisho\", \"pocket illustrated German to Japanese dictionary\") published in 1885 (Meiji 18).\n\nAt the beginning of the change to horizontal alignment in Meiji era Japan, there was a short-lived form called \"migi yokogaki\" (, literally \"right horizontal writing\"), in contrast to \"hidari yokogaki\", (, literally \"left horizontal writing\"), the current standard. This resembled the right-to-left horizontal writing style of languages such as Arabic with line breaks on the left. It was probably based on the traditional single-row right-to-left writing. This form was widely used for pre-WWII advertisements and official documents (like banknotes), but has not survived outside of old-fashioned signboards.\n\nVertical writing is still commonly used in Japan in novels, newspapers and magazines including the prolific manga comic book genre, while horizontal writing is used more often in other media, especially those containing English language references.\n\nTraditionally, Korean writing has been vertical, with columns running right to left.\n\nIn 1988, The Hankyoreh became the first Korean newspaper to use horizontal writing, and after 1990, all other major newspapers followed suit. Today, major Korean newspapers hardly ever run text vertically.\n\nIn East Asian calligraphy, vertical writing remains the dominant direction.\n\nJapanese comics, also known as manga, tend to use vertical direction for text. Manga frames tend to flow in right-to-left horizontal direction. Frames in yonkoma manga tend to flow in a vertical direction. Page ordering is the same as books that use vertical direction: from right to left. Frames that are chronologically before or after each other use less spacing in between as a visual cue.\n\nMost manga text (dialog and narration) is written vertically, which dictates the vertical shapes of the speech bubbles. Some, however, such as \"Levius\", are aimed at the international market and strive to optimize for translation and localization, therefore make use of horizontal text and speech bubbles. Some authors, such as Kenshi Hirokane, use Japanese text arranged horizontally to imply that a character is actually speaking in a foreign language, like English for example.\n\nIn some cases, horizontal writing in text bubbles may be used to indicate that a translation convention is in use – for example, that a character is actually speaking in English instead of Japanese.\n\nSome publishers that translate manga into European languages may choose to keep the original page order (a notable example is \"Shonen Jump\" magazine), while other publishers may reverse the page flow with use of mirrored pages.\n\nBoth horizontal and vertical writing are used in Japan, Hong Kong, Macau and Taiwan. Traditional Chinese is also used in Mainland China in a few limited contexts, such as some books on ancient literature, or as an aesthetic choice for some signs on shops, temples, etc. In those contexts, both horizontal and vertical writing are used as well.\nVertical writing is commonly used for novels, newspapers, comics, and many other forms of writing. Because it goes downwards, vertical writing is invariably used on the spines of books. Some newspapers combine the two forms, using the vertical format for most articles but including some written horizontally, especially for headlines. Musical notation for some Japanese instruments such as the shakuhachi is written vertically.\n\nHorizontal writing is easier for some purposes; academic texts are sometimes written this way since they often include words and phrases in other languages, which are more easily incorporated horizontally. Scientific and mathematical texts are nearly always written horizontally, since in vertical writing equations must be turned sideways, making them more difficult to read.\n\nSimilarly, English language textbooks, which contain many English words, are usually printed in horizontal writing. This is not a fixed rule, however, and it is also common to see English words printed sideways in vertical writing texts.\n\nComputer text is usually presented in horizontal format; see Japanese language and computers.\n\nBusiness cards in Japan (meishi) are often printed vertically in Japanese on one side, and horizontally in English on the other. Postcards and handwritten letters may be arranged horizontally or vertically, but the more formal the letter the more likely it is to be written vertically. Envelope addresses are usually vertical, with the recipient's address on the right and the recipient's name in the exact centre of the envelope. See also Japanese etiquette.\n\nIn mainland China, where the Simplified Chinese orthographical reform has been adopted, vertical writing is now comparatively rare, more so in print than in writing and signage. Most publications are now printed in horizontal alignment, like English. Horizontal writing is written left to right in the vast majority of cases, with a few exceptions such as bilingual dictionaries of Chinese and right-to-left scripts like Arabic, in which case Chinese may follow the right-to-left alignment. Right-to-left writing direction can also often be seen on the right side of tourist buses, as it is customary to have the text run (on both sides of the vehicle) from the front of the bus to its rear.\n\nVertical alignment is generally used for artistic or aesthetic purposes (e.g. on logos and book covers), in scholarly works on Classical Chinese literature, or when space constraints demand it (e.g. on the spines of books and in labelling diagrams). Naturally, vertical text is also used on signs that are longer than they are wide; such signs are the norm at the entrances of schools, government offices and police stations. Calligraphy – in Simplified or Traditional Chinese – is invariably written vertically. Additionally, vertical text may still be encountered on some business cards and personal letters in China.\n\nSince 2012, street markings are written vertically, but unorthodoxically from bottom to top. This is so that the characters are read from nearest to furthest from the drivers' perspective.\n\nIn modern Korea, vertical writing is uncommon. Modern Korean is usually written in left-to-right horizontally. Vertical writing is used when the writing space is long vertically and narrow horizontally. For example, titles on the spines of books are usually written vertically. When a foreign language film is subtitled into Korean, the subtitles are sometimes written vertically at the right side of the screen.\n\nIn the Standard language (; ) of South Korea, punctuation marks are used differently in horizontal and vertical writing. Western punctuation marks are used in horizontal writing and the Japanese/Eastern-style punctuation marks are used in vertical writing. However, vertical writing using Western punctuation marks is sometimes found.\n\nEarly computer installations were designed only to support left-to-right horizontal writing based on the Latin alphabet. Today, most computer programs do not fully support the vertical writing system; however, most advanced word processing and publication software which target the East Asian region support the vertical writing system either fully or to a limited extent.\n\nEven though vertical text display is generally not well supported, composing vertical text for print has been made possible. For example, on Asian editions of Windows, Asian fonts are also available in a vertical version, with font names prefixed by \"@\". Users can compose and edit the document as normal horizontal text. When complete, changing the text font to a vertical font converts the document to vertical orientation for printing purposes. Additionally, OpenType also has codice_1, codice_2, codice_3, codice_4, codice_5, codice_6, codice_7, codice_8 \"feature tags\" to define glyphs that can be transformed or adjusted within vertical text, they can be enabled or disabled in CSS3 using codice_9 property.\n\nSince the late 1990s, W3C (World Wide Web Consortium) has been drafting Cascading Style Sheets properties to enable display on the Web of the various languages of the world according to their heritage text directions. Their latest efforts in 2011 show some revisions to the previous format for the Writing Mode property which provides for vertical layout and text display. The format \"writing-mode:tb-rl\" has been revised as \"writing-mode: vertical-rl\" in CSS, but the former syntax was preserved as a part of SVG 1.1 specification.\n\nAmong Web browsers, Internet Explorer is the first one that has been supporting vertical text and layout coded in HTML. Starting with IE 5.5 in 2000, Microsoft has enabled the writing mode property as a \"Microsoft extension to Cascading Style Sheets (CSS)\". Google Chrome (since 8.0), Safari (since 5.1), Opera (since 15.0) has support the codice_10 property. Mozilla Firefox got support for unprefixed codice_11 property in version 38.0, later enabled by default in version 41.0.\n\n\n\n"}
{"id": "3303998", "url": "https://en.wikipedia.org/wiki?curid=3303998", "title": "How Much Wood Would a Woodchuck Chuck (film)", "text": "How Much Wood Would a Woodchuck Chuck (film)\n\nHow Much Wood Would a Woodchuck Chuck (German: Beobachtungen zu einer neuen Sprache, literally \"Observations of a New Language\") is a 1976 documentary film by German director Werner Herzog, produced by Werner Herzog Filmproduktion. It is a 44-minute film documenting the World Livestock Auctioneer Championship held in New Holland, Pennsylvania. The film also contains a section about the Amish and shows Amish speaking Pennsylvania German.\n\nHerzog has said that he believes auctioneering to be \"the last poetry possible, the poetry of capitalism.\" Herzog describes the auctioneering as an \"extreme language ... frightening but quite beautiful at the same time.\"\n\nHerzog used two of the featured auctioneers as actors in his later film \"Stroszek\".\n\nCinematographer Edward Lachman got his start working with Herzog on this film; he would work on \"La Soufrière\" (1977) shortly after.\n\n"}
{"id": "15078792", "url": "https://en.wikipedia.org/wiki?curid=15078792", "title": "Human Speechome Project", "text": "Human Speechome Project\n\nThe Human Speechome Project, (speechome to rhyme with \"genome\"), is an effort to closely observe and model the language acquisition of a child over the first three years of life.\n\nThe project was conducted at the Massachusetts Institute of Technology's Media Laboratory by the Associate Professor Deb Roy with an array of technology that is used to comprehensively but unobtrusively observe a single child – Roy's own son – with the resulting data being used to create computational models to yield further insight into language acquisition.\nMost studies of human speech acquisition in children have been done in laboratory settings and with sampling rates of only a couple of hours per week. The need for studies in the more natural setting of the child's home, and at a much higher sampling rate approaching the child's total experience, led to the development of this project concept.\n\nA digital network consisting of eleven video cameras, fourteen microphones, and an array of data capture hardware was installed in the home of the subject. A cluster of ten computers and audio samplers is located in the basement of the house to capture the data. Data from the cluster is moved manually to the MIT campus as necessary for storage in a one-million-gigabyte (one-petabyte) storage facility.\n\nTo provide control of the observation system to the occupants of the house, eight touch-activated displays were wall-mounted throughout the house to allow for stopping and starting video and or audio recording, and also erase any number of minutes permanently from the system. Audio recording was turned off throughout the house at night after the child was asleep.\n\nData was gathered at an average rate of 200 gigabytes per day, necessitating the development of sophisticated data-mining tools to reduce analysis efforts to a manageable level, and transcribing significant speech added a labor-intensive dimension.\n\n"}
{"id": "59114506", "url": "https://en.wikipedia.org/wiki?curid=59114506", "title": "Influence of French on English", "text": "Influence of French on English\n\nThere has been a long-standing influence of French on English, in terms of syntax, grammar, lexicon, spelling and pronunciation.\n\nMost French vocabulary that entered English occurred after the Norman conquest of England in 1066 and the establishment of a French-speaking administration. French became the language of the court, the administration and the elites for several centuries, until after the Hundred Years War. English has been constantly influenced by French from that time till present day.\nAccording to Laura K. Lawless, more than a third of the current English vocabulary is of French origin. According to linguist Henriette Walter, words of French origin represent more than two thirds of the English vocabulary. It is estimated by linguist Anthony Lacoudre that over 40 000 English words are directly French and may be understood without orthographical change by French speakers.\n\nAt the beginning of XIth century, the English language did not have a well-defined status. Indeed, the inhabitants of what would become Great Britain did not have a language that allowed them to communicate with each other. There were many different dialectal forms. Great Britain, in which various Celtic idioms had coexisted since the IVth century, had experienced partial Roman occupation since the 1st century A.D., and this for four centuries.\n\nFrom 450 onwards, the Saxons, the Angles and the Jutes, who came from the continent, settled in the south and east. Germanic dialects would prevail in these regions, supplanting Celtic dialects, which would remain in the west and north of the island (Wales, Cornwall, Scotland) and Ireland. In the VIIIth century, Vikings from Scandinavia settled on the island. Their languages, also Germanic, in turn influence the languages already present on the island. Thus, at the dawn of XIth century, the country was made up of a series of peoples with significantly different speeches, most of them Germanic, with multiple influences.\n\nIt is therefore a linguistically disunited people that the Normans will get massively in contact with, from 1066. William II of Normandy, supported by his King, Philip I of France, and his blood legitimacy to the throne of England, landed at Hastings, in Sussex, on 29 September 1066. His men are deployed around the city waiting for the king Harold II's troops. On October 14, exhausted by the long journey to Hastings, Harold II's troops lost the battle after a day. Following the defeat of the English, Duke William II of Normandy became King of England on December 25, 1066, crowned under the name of William I of England, also known as William the Conqueror. This date marks the beginning of a long period of ties between the peoples and languages.\n\nIn fact, these links already existed before the Battle of Hastings. Indeed, the geographical location of Normandy, facing the English Channel, favoured commercial contacts with England. These ties will be further strengthened at the beginning of the XIth century when the daughter of the Duke Richard II of Normandy, Emma, marries King Æthelred II of England. But it is really from the 1066 conquest that proto-English becomes massively impregnated with Old French, then modern French. It should be noted, however, that only French will influence English in the centuries following the conquest. The reverse contribution of English to French will only be real from the XVIIIth century.\n\nThe arrival of William the Conqueror and his barons significantly changed the linguistic situation in England. Norman is essentially imposed in the upper layers of society. The Anglo-Saxon dialects were supplanted by Norman in the circles of the court and aristocracy, justice and the Church. The influential circles, who came from Normandy and settled in England, kept their Norman mother tongue, while the more modest rural and urban strata continued to speak English.\n\nNorman is a particular variety of the Gallo-Roman language, spoken in Normandy. It is one of the Oil languages alongside, among others, the Picard and the Walloon. The Norman language is modified in contact with the Anglo-Saxon language. It then integrates words and phrases from English and will give birth to a dialect, Anglo-Norman, still spoken on the Anglo-Norman isles. Anglo-Norman can be described as a vernacular language, on English soil in the XIth century, in the field of literature, culture, court and among the clergy. French was therefore, at first, spoken in England under the form of this Anglo-Norman dialect.\n\nDuring the XIIth century, continental French has a greater influence on Old English. It acquires great prestige in England, especially within the aristocracy and the clergy. It becomes the language of law and justice nationwide. Rich and noble families, most of them of Norman origin, teach their children French or send them to study in France. The expansion of the French language in England was also encouraged by royal marriages. From Henry II Plantagenet and Eleanor of Aquitaine at the beginning of the century, to Henri VI and Marguerite in the XVth century, all kings of England married French princesses. These marriages made French the language of the English court for several centuries and were decisive in strengthening the use of French in England. This period (XIIth-XVth centuries) is characterized by a massive influx of French words into Old English vocabulary.\n\nIn 1204, Philippe Auguste Normandy is officially annexed to the kingdom of France, politically isolating England from the continent. Normans who choose to stay in England move further away from France and, therefore, from the French language. Keeping its status as the language of justice and the language of power, England saw the first teaching manuals for teaching French to the English. These manuals were intended for English nobles who wish to perfect their knowledge of French and teach it to their children. Two types of French spoken in the higher spheres of English society can be distinguished during the XIIIth century : the Anglo-Norman dialect, which was the aristocrats' mother tongue, and a more prestigious type of French as a second language. Knowing \"parisian\" French was a mark of social distinction. As a language of culture, French supplanted Latin from the XIIth century onward as the language of diplomacy and worldly relations throughout Europe. The mass and influence of French literature reinforced its reputation and appeal.\n\nThe XVIth century, that of the Renaissance, is a decisive century for French since king François I of France, through the Ordonnance de Villers-Cotterêts (1539), makes French the official language of administration in the whole kingdom.\nAlthough troubled by the European wars of religion, the Italian Wars, the language is marked by intellectual, technical and scientific effervescence. It ushered in an era of prosperity that would also spread to England through French.\n\nThe XVIIth century announces the apogee of the Kingdom of France. This period was characterized by the political, literary and artistic prestige of France and the French language. Peace restored and unity ensured in the country, economy grew considerably. Personalities such as the King Henri IV, the Cardinal of Richelieu or the Sun King contribute to fixing and enhancing the French language in Europe, the Americas, India and Oceania.\n\nThe creation of the Académie française by Richelieu in 1635, under Louis XIII, was a step that led to the standardization of French in continental Europe and abroad, including England. French is then the second language of all the elites in Europe, from Turkey to Ireland and from Moscow to Lisbon. The greatest scholars and intellectuals, writers and scientists, express themselves and correspond in this new standardised French. French is considered a perfect language, whose beauty and elegance are determined by the development of scientific logic, aided by dictionaries and grammars.\n\nThe geographical use of French has continuously and greatly diversified in the last five hundred years, with countries and states like New-Brunswick, Quebec, Ivory Coast, Benin, Togo, Guinea, Cameroon, Congo, Democratic Republic of the Congo, Madagascar, Mauritius, Tchad, Djibouti, Senegal, Morocco, Algeria, Tunisia, Lebanon, France, Belgium, Switzerland, Luxembourg, Monaco, Aosta Valley, French Polynesia, New Caledonia, and Vanuatu adopting it as their official language. This geographical diversity has led to many different contacts with vernacular dialects, regional and international languages, from which French has often been enriched locally.\nIn a number of countries and regions where French shares co-officiality with English (Cameroon, Canada, Jersey, Mauritius, Rwanda, Vanuatu), particular lexical regionalisms are observed where French and English terms are used interchangeably.\n\nSeveral elements must be observed.\n\n\nThe following French glossary in English is in no way exhaustive. These words come as examples to illustrate the countless French words that are part of the English language.\n\nIn this section, examples of French-to-English lexical contributions are classified by field and in chronological order. The periods during which these words were used in the English language are specified as much as possible. It is not always possible to state with certainty the precise period from which a word was borrowed or integrated.\n\nThe English word is on the left, with its current French equivalent in brackets, then comes its Old French origin in bold and the century of its introduction on the right.\n\n\n\n\n\n\n\n\n\n"}
{"id": "10956960", "url": "https://en.wikipedia.org/wiki?curid=10956960", "title": "Information and media literacy", "text": "Information and media literacy\n\nInformation and media literacy (IML) enables people to interpret and make informed judgments as users of information and media, as well as to become skillful creators and producers of information and media messages in their own right.\n\nPrior to the 1990s, the primary focus of information literacy was research skills. Media literacy, a study that emerged around the 1970s, traditionally focuses on the analysis and the delivery of information through various forms of media. Nowadays, the study of information literacy has been extended to include the study of media literacy in many countries like the UK, Australia and New Zealand. The term \"information and media literacy\" is used by UNESCO to differentiate the combined study from the existing study of information literacy. It is also referred to as information and communication technologies (ICT) in the United States. Educators such as Gregory Ulmer have also defined the field as electracy.\n\nIML is a combination of information literacy and media literacy. The purpose of being information and media literate is to engage in a digital society; one needs to be able to understand, inquire, create, communicate and think critically. It is important to effectively access, organize, analyze, evaluate, and create messages in a variety of forms. The transformative nature of IML includes creative works and creating new knowledge; to publish and collaborate responsibly requires ethical, cultural and social understanding.\n\nThe IML learning capacities prepare students to be 21st century literate. According to Jeff Wilhelm (2000), \"technology has everything to do with literacy. And being able to use the latest electronic technologies has everything to do with being literate.\" He supports his argument with J. David Bolter's statement that \"if our students are not reading and composing with various electronic technologies, then they are illiterate. They are not just unprepared for the future; they are illiterate right now, in our current time and context\".\n\nWilhelm's statement is supported by the 2005 Wired World Phase II (YCWW II) survey conducted by the Media Awareness Network of Canada on 5000 Grade 4 – 11 students. The key findings of the survey were:\n\nMarc Prensky (2001) uses the term \"digital native\" to describe people who have been brought up in a digital world. The Internet has been a pervasive element of young people's home lives. 94% of kids reported that they had Internet access at home, and a significant majority (61%) had a high-speed connection.\n\nBy the time kids reach Grade 11, half of them (51 percent) have their own Internet-connected computer, separate and apart from the family computer. The survey also showed that young Canadians are now among the most wired in the world. Contrary to the earlier stereotype of the isolated and awkward computer nerd, today's wired kid is a social kid.\n\nIn general, many students are better networked through the use of technology than most teachers and parents, who may not understand the abilities of technology. Students are no longer limited to desktop computera. They may use mobile technologies to graph mathematical problems, research a question for social studies, text message an expert for information, or send homework to a drop box. Students are accessing information by using MSN, personal Web pages, Weblogs and social networking sites.\n\nMany teachers continue the tradition of teaching of the past 50 years. Traditionally teachers have been the experts sharing their knowledge with children. Technology, and the learning tools it provides access to, forces us to change to being facilitators of learning. We have to change the stereotype of teacher as the expert who delivers information, and students as consumers of information, in order to meet the needs of digital students. Teachers not only need to learn to speak digital, but also to embrace the language of digital natives.\n\nLanguage is generally defined as a system used to communicate in which symbols convey information. Digital natives can communicate fluently with digital devices and convey information in a way that was impossible without digital devices. People born prior to 1988 are sometimes referred to as \"digital immigrants.\" They experience difficulty programming simple devices like a VCR. Digital immigrants do not start pushing buttons to make things work.\n\nLearning a language is best done early in a child's development.\n\nIn acquiring a second language, Hyltenstam (1992) found that around the age of 6 and 7 seemed to be a cut-off point for bilinguals to achieve native-like proficiency. After that age, second language learners could get near-native-like-ness but their language would, while consisting of very few actual errors, have enough errors that would set them apart from the first language group.\n\nKindergarten and grades 1 and 2 are critical to student success as digital natives because not all students have a \"digital\"-rich childhood. Students learning technological skills before Grade 3 can become equivalently bilingual. \"Language-minority students who cannot read and write proficiently in English cannot participate fully in American schools, workplaces, or society. They face limited job opportunities and earning power.\" Speaking \"digital\" is as important as being literate in order to participate fully in North American society and opportunities.\n\nMany students are considered illiterate in media and information for various reasons. They may not see the value of media and information literacy in the 21st-century classroom. Others are not aware of the emergence of the new form of information. Educators need to introduce IML to these students to help them become media and information literate. Very little changes will be made if the educators are not supporting information and media literacy in their own classrooms.\n\nPerformance standards, the foundation to support them, and tools to implement them are readily available. Success will come when there is full implementation and equitable access are established. Shared vision and goals that focus on strategic actions with measurable results are also necessary.\n\nWhen the staff and community, working together, identify and clarify their values, beliefs, assumptions, and perceptions about what they want children to know and be able to do, an important next step will be to discover which of these values and expectations will be achieved. Using the capacity tools to assess IML will allow students, staff and the community to reflect on how well students are meeting learning needs as related to technology.\n\nThe IML Performance standards allow data collection and analysis to evidence that student-learning needs are being met. After assessing student IML, three questions can be asked:\n\n\nTeachers can use classroom assessment for learning to identify areas that might need increased focus and support. Students can use classroom assessment to set learning goals for themselves.\n\nThis integration of technology across the curriculum is a positive shift from computers being viewed as boxes to be learned to computers being used as technical communication tools. In addition, recent learning pedagogy recognizes the inclusion for students to be creators of knowledge through technology. International Society for Technology in Education (ISTE) has been developing a standard IML curriculum for the US and other countries by implementing the National Educational Technology Standards.\n\nIn the UK, IML has been promoted among educators through an information literacy website developed by several organizations that have been involved in the field.\n\nIML is included in the Partnership for the 21st Century program sponsored by the US Department of Education. Special mandates have been provided to Arizona, Iowa, Kansas, Maine, New Jersey, Massachusetts, North Carolina, South Dakota, West Virginia and Wisconsin. Individual school districts, such as the Clarkstown Central School District, have also developed their own information literacy curriculum. ISTE has also produced the National Educational Technology Standards for Students, Teachers and Administrators.\n\nIn British Columbia, Canada, the Ministry of Education has de-listed the Information Technology K to 7 IRP as a stand-alone course. It is still expected that all the prescribed learning outcomes continue to be integrated.\n\nThis integration of technology across the curriculum is a positive shift from computers being viewed as boxes to be learned to computers being used as technical communication tools. In addition, recent learning pedagogy recognizes the inclusion for students to be creators of knowledge through technology. Unfortunately, there has been no clear direction to implement IML.\n\nThe BC Ministry of Education published the Information and Communications Technology Integration Performance Standards, Grades 5 to 10 ICTI in 2005. These standards provide performance standards expectations for Grade 5 to 10; however, they do not provide guidance for other grades, and the expectation for a Grade 5 and Grade 10 student are the same.\n\nIn the Arab region, media and information literacy was largely ignored up until 2011, when the Media Studies Program at the American University of Beirut, the Open Society Foundations and the Arab-US Association for Communication Educators (AUSACE) launched a regional conference themed \"New Directions: Digital and Media Literacy\". The conference attracted significant attention from Arab universities and scholars, who discussed obstacles and needs to advance media literacy in the Arab region, including developing curricula in Arabic, training faculty and promoting the field. \n\nFollowing up on that recommendation, the Media Studies Program at AUB and the Open Society Foundations in collaboration with the Salzburg Academy on Media and Global Change launched in 2013 the first regional initiative to develop, vitalize, and advance media literacy education in the Arab region. The Media and Digital Literacy Academy of Beirut (MDLAB) offered an annual two-week summer training program in addition to working year-round to develop media literacy curricula and programs. The academy is conducted in Arabic and English and brings pioneering international instructors and professionals to teach advanced digital and media literacy concepts to young Arab academics and graduate students from various fields. MDLAB hopes that the participating Arab academics will carry what they learned to their countries and institutions and offers free curricular material in Arabic and English, including media literacy syllabi, lectures, exercises, lesson plans, and multi-media material, to assist and encourage the integration of digital and media literacy into Arab university and school curricula. \n\nIn recognition of MDLAB's accomplishments in advancing media literacy education in the Arab region, the founder of MDLAB received the 2015 UNESCO-UNAOC International Media and Information Literacy Award. \n\nPrior to 2013, only two Arab universities offered media literacy courses: the American University of Beirut (AUB) and the American University of Sharjah (AUS). Three years after the launch of MDLAB, over two dozen Arab universities incorporated media literacy education into their curricula, both as stand-alone courses or as modules injected into their existing media courses. Among the universities who have full-fledged media literacy courses (as of 2015) are Lebanese American University (Lebanon), Birzeit University (Palestine), University of Balamand (Lebanon), Damascus University (Syria), Rafik Hariri University (Lebanon), Notre Dame University (Lebanon), Ahram Canadian University (Egypt), American University of Beirut (Lebanon), American University of Sharjah (UAE), and Al Azm University (Lebanon). The first Arab school to adopt media literacy as part of its strategic plan is the International College (IC) in Lebanon. Efforts to introduce media literacy to the region's other universities and schools continues with the help of other international organizations, such as UNESCO, UNAOC, AREACORE, DAAD, and OSF.\n\nIn Singapore and Hong Kong, information literacy or information technology was listed as a formal curriculum.\n\nOne barrier to learning to read is the lack of books, while a barrier to learning IML is the lack of technology access. Highlighting the value of IML helps to identify existing barriers within school infrastructure, staff development, and support systems. While there is a continued need to work on the foundations to provide a sustainable and equitable access, the biggest obstacle is school climate.\n\nMarc Prensky identifies one barrier as teachers viewing digital devices as distractions: \"Let's admit the real reason that we ban cell phones is that, given the opportunity to use them, students would vote with their attention, just as adults would 'vote with their feet' by leaving the room when a presentation is not compelling.\"\n\nThe mindset of banning new technology, and fearing the bad things that can happen, can affect educational decisions. The decision to ban digital devices impacts students for the rest of their lives.\n\nAny tool that is used poorly or incorrectly can be unsafe. Safety lessons are mandatory in industrial technology and science. Yet safety or ethical lessons are not mandatory to use technology.\n\nNot all decisions in schools are measured by common ground beliefs. One school district in Ontario banned digital devices from their schools. Local schools have been looking at doing the same. These kinds of reactions are often about immediate actions and not about teaching, learning or creating solutions. Many barriers to IML exist.\n\n"}
{"id": "22404187", "url": "https://en.wikipedia.org/wiki?curid=22404187", "title": "Information structure", "text": "Information structure\n\nIn linguistics, information structure, also called information packaging, describes the way in which information is formally packaged within a sentence. This generally includes only those aspects of information that “respond to the temporary state of the addressee’s mind”, and excludes other aspects of linguistic information such as references to background (encyclopedic/common) knowledge, choice of style, politeness, and so forth. For example, the difference between an active clause (e.g., \"the police want him\") and a corresponding passive (e.g., \"he is wanted by police\") is a syntactic difference, but one motivated by information structuring considerations. Other structures motivated by information structure include preposing (e.g., \"that one I don't like\") and inversion (e.g., \"\"the end\", said the man\").\n\nThe basic notions of information structure are focus, givenness, and topic, as well as their complementary notions of background, newness, and comment respectively. Focus “indicates the presence of alternatives that are relevant for the interpretation of linguistic expressions”, givenness indicates that “the denotation of an expression is present” in the immediate context of the utterance, and topic is “the entity that a speaker identifies, about which then information, the comment, is given”. Additional notions in information structure may include contrast and exhaustivity, but there is no general agreement in the linguistic literature about extensions of the basic three notions. There are many different approaches, such as generative or functional architectures, to information structure.\n\nInformation structure can be realized through a wide variety of linguistic mechanisms. In the spoken form of English Language, one of the primary methods of indicating information structure is through intonation, whereby pitch is modified from some default pattern. Other languages use syntactic mechanisms like dislocation, anaphora, and gapping; morphological mechanisms like specialized focus or topic-marking affixes; and specialized discourse particles. English in fact uses more than intonation for expressing information structure, so that clefts are used for exhaustive focus, and grammatical particles like \"only\" also induce contrastive focus readings.\n\nCross-linguistically, there are clear tendencies that relate notions of information structure to particular linguistic phenomena. For instance, focus tends to be prosodically prominent, and there do not seem to be any languages that express focus by deaccenting or destressing.\n\nThe following German sentences exhibit three different kinds of syntactic ‘fronting’ that correlate with topic.\n\nb. _Diesen Mann_, den habe ich noch nie gesehen.\n\nIt is often assumed that answers to questions are focused elements. Question and answer pairs are often used as diagnostics for focus, as in the following English examples.\n\n\"Focus\" is a grammatical category or attribute that determines indicating that part of an utterance contributes new, non-derivable, or contrastive information. Some theories (in line with work by Mats Rooth) link focus to the presence of \"alternatives\" (see Focus: Alternative Semantics). An alternatives theory of focus would account for the stress pattern in the example from the previous section (When did Jane sell the book? She sold the book YESTERDAY), saying that YESTERDAY receives focus because it could be substituted with alternative time periods (TODAY or LAST WEEK) and still serve to answer the question the first speaker asked.\n\n\"Background\" is a more difficult concept to define; it's not simply the complement of focus. Daniel P. Hole gives the following framework: \"‘Focus’ is a relational notion, and the entity a focus relates to is called its background, or presupposition.\"\n\nThe \"topic\" (or theme) of a sentence is what is being talked about, and the \"comment\" (or rheme, or sometimes focus) is what is being said about the topic. That the information structure of a clause is divided in this way is generally agreed on, but the boundary between topic/theme depends on grammatical theory. Topic is grammaticized in languages like Japanese and Korean, which have a designated topic-marker morpheme affixed to the topic.\n\nSome diagnostics have been proposed for languages that lack grammatical topic-markers, like English; they attempt to distinguish between different kinds of topics (such as \"aboutness\" topics and \"contrastive\" topics). The diagnostics consist of judging how felicitous it is to follow a discourse with either questions (\"What about x?\") or sentences beginning with certain phrases (\"About x, ... Speaking of x,\" ... \"As for x\", ...) to determine how \"topical\" \"x\" is in that context.\n\nIntuitively, \"givenness\" classifies words and information in a discourse that are already known (or given) by virtue of being common knowledge, or by having been discussed previously in the same discourse (\"anaphorically recoverable\"). Certain theories (such as Roger Schwarzschild's GIVENness Constraint) require all non-focus-marked constituents to be given.\n\nWords/information that are not given, or are \"textually and situationally non-derivable\" are by definition \"new\".\n"}
{"id": "31869354", "url": "https://en.wikipedia.org/wiki?curid=31869354", "title": "Interaction hypothesis", "text": "Interaction hypothesis\n\nThe Interaction hypothesis is a theory of second-language acquisition which states that the development of language proficiency is promoted by face-to-face interaction and communication. The idea existed in the 1980s, but is usually credited to Michael Long for his 1996 paper \"The role of the linguistic environment in second language acquisition\". There are two forms of the Interaction Hypothesis: the \"strong\" form and the \"weak\" form. The \"strong\" form is the position that the interaction itself contributes to language development. The \"weak\" form is the position that interaction is simply the way that learners find learning opportunities, whether or not they make productive use of them.\n\nSimilar to Krashen's input hypothesis, the interaction hypothesis claims that comprehensible input is important for language learning. In addition, it claims that the effectiveness of comprehensible input is greatly increased when learners have to negotiate for meaning. This occurs when there is a breakdown in communication which interlocutors attempt to overcome. One of the participants in a conversation will say something that the other does not understand; the participants will then use various communicative strategies to help the interaction progress. The strategies used when negotiating meaning may include slowing down speech, speaking more deliberately, requests for clarification or repair of speech, or paraphrases.\n\nInteractions often result in learners receiving negative evidence. That is, if learners say something that their interlocutors do not understand, after negotiation the interlocutors may model the correct language form. In doing this, learners can receive feedback on their production and on grammar that they have not yet mastered. The process of interaction may also result in learners receiving more input from their interlocutors than they would otherwise. Furthermore, if learners stop to clarify things that they do not understand, they may have more time to process the input they receive. This can lead to better understanding and possibly the acquisition of new language forms. Finally, interactions may serve as a way of focusing learners' attention on a difference between their knowledge of the target language and the reality of what they are hearing; it may also focus their attention on a part of the target language of which they are not yet aware.\n\nAlthough there are several studies that link interaction with language acquisition, not all researchers subscribe to the idea that interaction is the primary means by which language proficiency develops. In a survey of the literature on the subject, Larsen-Freeman and Long say that interaction is not necessary for language acquisition; they do say, however, that it helps in certain circumstances. Gass and Selinker claim that as well as interaction facilitating learning, it may also function as a priming device, \"setting the stage\" for learning rather than being the means by which learning takes place. In addition, Ellis notes that interaction is not always positive. He says that sometimes it can make the input more complicated, or produce amounts of input which overwhelm learners. According to Ellis, this can happen if interlocutors use lengthy paraphrases or give complex definitions of a word that was not understood, and he comes to the conclusion that the role of interaction in language acquisition is a complex one.\n\n"}
{"id": "47639769", "url": "https://en.wikipedia.org/wiki?curid=47639769", "title": "Kanglish", "text": "Kanglish\n\nKanglish (Kannada: ಕಂಗ್ಲಿಷ್) is a macaronic language of Kannada and English; the name is a portmanteau of the names of the two languages.\n\nEn sir samachara? : whats the matter sir?\n\nHello, tiffin aytha? : Hello, have you had your tiffin?\n\nSwalpa move madi : Just move a bit.\n\nSwalpa break haki : Please break\n\nYav movie hakiddare? : Which movie is playing?\n"}
{"id": "22499764", "url": "https://en.wikipedia.org/wiki?curid=22499764", "title": "LGBT linguistics", "text": "LGBT linguistics\n\nLGBT linguistics is the study of language revolving around people identifying as gay, lesbian, bisexual, transgender, and queer (LGBTQ). Related or synonymous terms include lavender linguistics, advanced by William Leap in the 1990s, which \"encompass[es] a wide range of everyday language practices\" in LGBTQ communities, and queer linguistics, which more specifically refers to linguistics overtly concerned with exposing heteronormativity. The former term derives from the longtime association of the color lavender with LGBTQ communities. \"Language\", in this context, may refer to any aspect of spoken or written linguistic practices, including speech patterns and pronunciation, use of certain vocabulary, and, in a few cases, an elaborate alternative lexicon such as Polari.\nEarly studies in the field of LGBT linguistics were dominated by the concept of distinct \"lavender lexicons\" such as that recorded by Gershon Legman in 1941. In 1995 William Leap, whose work incorporates LGBTQ culture studies, cultural theory, and linguistics, called for scholarship to move toward a fuller and more nuanced study of LGBTQ language use. Anna Livia and Kira Hall have noted that while research in the 1960s and 1970s on the difference between men's and women's speech made the implicit assumption that gender was the relevant way to divide the social space, there is still considerable room for linguistic research based on sexual orientation, rather than gender.\n\nLinguistics research, particularly within North American English, has revealed a number of phonetically salient features used by many gay men, some of which adhere to stereotypes. Studies have repeatedly confirmed that male American English speakers are recognized as gay by their speech at rates above chance. Relevant features include what is popularly known as a gay lisp: in fact, the articulation of and with a higher frequency and longer duration than average speakers. Also, gay men may tend to lower the and front vowels, especially in \"fun\" or casual social situations. Many gay speech characteristics match those that other speakers use when trying to speak especially clearly or carefully, including (over-)enunciating and widening the vowel spaces in the mouth. The notion that some gay male speech entirely imitates women's speech is inaccurate, though certain vocal qualities are certainly shared between the two speech styles. Research has also shown unique speech of gay men in other languages, such as Puerto Rican Spanish and Flemish Dutch.\n\nSpeech scientist Benjamin Munson confirmed such features among lesbians as the use of lower pitch and more direct communication styles found in previous studies, plus more backed variants of back vowels, but he noted too that differences between lesbians and straight women are \"even more subtle\" than differences between gay and straight men. In one English-language experiment, listeners were unable to identify female speakers as either lesbian or straight based solely on voice. At the same time, lesbian speech studies have long been neglected, making introductory research difficult. Another study showed that speakers' self-assessed \"familiarity with queer culture\" had a statistically significant correlation with phonetic variation like lower median pitch and faster rate of speech, though mostly for straight women, somewhat for bisexual women, and not at all for lesbians. The study's author theorized that the straight women, aware of the study's purpose, may have been attempting to express their affinity with lesbians by adopting their stereotype of a lower pitch. Another experiment found that listeners indeed were able to accurately judge female speakers on a scale from \"least\" to \"most likely to be a lesbian\" (the slight lesbian and bisexual differences approaching statistical significance), perceiving the straight women as significantly more feminine, bisexual speakers as only slightly more, and lesbians as correlated with lower median pitch, wider pitch range, lower second formant, and more use of creaky voice. However, no direct correlations between these phonetic variables and sexual orientation were found, perhaps with listeners identifying other features that were not tested.\n\nLinguist Robin Queen argues that analyses have been too simplistic and that a uniquely lesbian language is constructed through the combination of sometimes-conflicting stylistic tropes: stereotypical women's language (e.g. hypercorrect grammar), stereotypical nonstandard forms associated with the (male) working class (e.g. contractions), stereotypical gay male lexical items, and stereotypical lesbian language (e.g. flat intonation, cursing). Sometimes lesbians deliberately avoid stereotypical female speech, according to Queen, in order to distance themselves from \"normative\" heterosexual female speech patterns. Because femininity is a marked style, adopting it is more noticeable than avoiding it, which may add to the lack of socially salient styles for lesbians in contrast with socially identifiable stereotypically gay male speech. However, lesbians may have more slang than gay males, with one article listing nearly eighty common lesbian slang words for sexual acts and organs.\n\nSpeech features of transgender people include dissociating specific physical or gender-specific characteristics of genital terms, including using certain words for specific genitalia in broader ways or as all-purpose terms. The physical voices of trans men (and transmasculine) and trans women (and transfeminine) individuals is often but not always affected by social and medical transition, including through voice training, tracheal shaves, feminizing hormones, masculinizing hormones, or other drugs, all of which can alter sociolingustic characteristics. A 2006 study noted that, after undergoing five oral resonance sessions targeted at lip spreading and forward tongue carriage, ten transfeminine individuals demonstrated a general increase in the formant frequency values F1, F2, and F3 as well as the fundamental frequency value F0, thus more closely approximating the desired vocal frequency of cisgender women. A 2012 doctorate dissertation followed fifteen transmasculine individuals from the San Francisco Bay Area in a long-term study focused on formant and fundamental frequency, for one to two years after the start of masculinizing hormone replacement therapy (HRT), concluding that all ten underwent a drop in fundamental frequency in the early stages of HRT but that social factors also probably affected many of the changes in voice and mannerisms.\n\nNonbinary individuals (including genderqueer, agender, bigender, genderfluid, etc.) may perform gender in a unique way through speech. Neopronouns, pronouns which avoid indexing gender and/or index a nonbinary gender identity, originated in the late 1800s as \"thon\" and \"e\" to refer to people without defining gender. Newer pronouns, include \"ee,\" \"em,\" \"xe,\" and \"ve\", as well as new words for traditionally gender-expressing relationships.\n\nSpecialized dictionaries that record gay and lesbian slang tend to revolve heavily around sexual matters, which may reflect the publications' methodological assumptions about the hyper-sexuality of conversations among LGBT people.\n\nOne study showed gay pornographic imagery to men and asked them to discuss the imagery, finding that conversations between gay men used more slang and fewer commonly-known terms about sexual behavior than conversations where both participants were heterosexual males or where the pair consisted of one heterosexual and one homosexual male. Methodological issues of this study may include that the findings reflect homophobia among the heterosexual participants.\n\nStudies have also been done into whether words used within the gay community are understood by heterosexuals. A study of Deaf sign language users showed that all the gay male participants understood the sign for a bathhouse and that 83% of lesbians knew the sign. This compared to zero heterosexual men and only one out of eleven heterosexual women knowing the sign.\n\nOne prominent example of LGBT slang is the rising reappropriation among lesbians of the word \"dyke\". Though still in many contexts considered pejorative, dyke has become a symbol for increasing acceptance of the lesbian movement and identity. Lesbians themselves use it to further solidarity and unity among their community. Examples include dyke marches (female-exclusive pride parades), \"dykes with tykes\" (describing lesbian motherhood), and Dykes on Bikes (a motorcycle group that traditionally leads the San Francisco Pride parade). Like other minorities, female homosexuals are slowly reclaiming a word that was once used to hurt them in the past. This even had legal repercussions, in that the \"Dykes on Bikes\" group was formally known as the \"Women's Motorcycle Contingent\" since they were refused the right to register under their preferred name by the United States Patent and Trademark Office, until 2006 when they finally were able to trademark the name, having persuaded the Office that \"dyke\" was not an offensive word.\n\nTraditionally it was believed that one's way of speaking is a result of one's identity, but the postmodernist approach reversed this theory to suggest that the way we talk is a part of identity formation, specifically suggesting that gender identity is variable and not fixed. In the early 20th century sexuality-related theories about language were common (for example, Freud and his theories of psychoanalysis), using a quite different basis from that used by modern studies on this topic. One of these early views was that homosexuality was a pathology. In the 1980s, however the LGBTQ community was increasingly viewed as an oppressed minority group, and scholars began to investigate the possibility of characterizing gay language use in a different way, influenced in part by studies of African-American Vernacular English. There was a shift in beliefs from language being a result of identity to language being employed to \"reflect\" a shared social identity and even to \"create\" sexual or gender identities.\n\nShared ways of speaking can be used to create a single, cohesive identity that in turn help organize political struggle. Sexuality is one form of social identity, discursively constructed and represented. This shared identity can in some cases be strengthened through shared forms of language use and used for political organizing. Language can be used to negotiate relations and contradictions of gender and sexual identities, and can index identity in various ways, even if there is no specific gay or lesbian code of speaking.\n\nGay men and lesbians may, through the use of language, form speech communities. A speech community is a community that shares linguistic traits and tends to have community boundaries that coincide with social units. Membership in speech communities is often assumed based on stereotypes about the community as defined by non-linguistic factors. Speakers may resist culturally dominant language and oppose cultural authority by maintaining their own varieties of speech.\n\nGender performativity relates to speech in that people may consciously or unconsciously modify their speech styles to conform with their gender role, which men often pick speech styles that reflect the culturally defined standards of masculinity. Gay men may be associated with \"femininity\" in their speech styles because others perceive that their speech performance doesn't conform with their gender.\n\nFor example, in the west, parodies of gay styles employ resources that are heard as hyper-feminine, supporting that gay speech is feminine. However, because many speech varieties associated with 'masculinity' are learned and not biological, certain gay men may be using a wider variety of speech than a stereotypical 'masculine' male.\n\nThese stylistic innovations are made possible by the iterability of speech, and are used to index elements of identity that often do not conform with the gender binary. Conversely, lesbian women already have a wider variety of speech available, yet refrain from using a distinctive style of speech. Masculinity, and speech associated with a heterosexual male, is constrained by cultural expectations for men to avoid 'abjection' (as further elaborated in \"Gender Trouble\"); power differences amongst the genders may lead to speakers adopting different speech styles that conform with their identities, or expected gender performances (e.g. adolescent males often use the term 'fag' to police one another, which challenges their sexual orientation through gender performance, and reinforces the avoidance of the 'abject' or femininity). 'Masculine' speech is associated with non-feminine sounding speech and because some gay men may not wish to identify with straight masculine speech in some contexts, they may access other speech styles to convey their identity (because the possibilities have two options, 'masculine' or 'feminine,' to be not-'masculine' is often associated with 'feminine'). The boundary between 'masculine' and 'feminine' is maintained by cultural norms and societal orders, that do not permit masculinity to include femininity, the abject.\n\nLanguage use can also mimic culturally dominant forms or stereotypes. Performing identity can only work as long as the indexes used are conventional and socially recognized, which is why stereotypes are sometimes adopted. Community members can establish their affiliation with the group through shared ways of speaking, acting, and thinking. Such discourses may in turn reproduce or modify social relationships. Sometimes, however, such a code may fall out of use when it becomes widely known and therefore no longer exclusive, as occurred with Polari after it was used on the BBC.\n\nIn a particular example of how this process of language community formation happens in a specific LGBTQ community, transsexuals and transvestites may use vocabulary that includes members and excludes non-members to establish social identity and solidarity and to exclude outsiders. As these social groups are particularly likely to be viewed negatively by outsiders, the use of a private language can serve to keep membership in the group a secret to outsiders while allowing group members to recognize their own.\n\nSome members of a community may use stylistic and pragmatic devices to index and exaggerate orientations and identities, but others may deliberately avoid stereotypical speech. Gender is frequently indexed indirectly, through traits that are associated with certain gender identities. In this way, for example, speaking forcefully is associated with masculinity but also with confidence and authority. Similarly, LGBTQ speech has a relationship with the speaker's community of practice. Speakers may have a shared interest, and respond to a mutual situation, and through communicating regularly they may develop certain speech norms. The innovative speech norms, that LGBTQ folks may use within their communities of practice, can be spread through institutions like schools where person of many classes, races, and genders, come together. These particular speech traits may be spread through the adoption of use by people with association to LGBTQ identities.\n\nPeople often are members of multiple communities, and which community they want to be most closely associated with may vary. For some gay men, the primary self-categorization is their identity as gay men. To achieve recognition as such, gay men may recognize and imitate forms of language that reflect the social identity of gay men, or which are stereotypically considered to be characteristic to gay men. For example, the use of female pronouns dissociates gay men from heterosexual norms and designates them in opposition to heterosexual masculinity. The reason for using female pronouns and the frequency of use may vary, however. For example, they may be used only in jest, or may be used more seriously to stabilize a group of gay men and bond its members together.\n\nThe development of gay identity may differ for men and women. For many women, regardless of orientation, female identity is more important than sexual identity. Where gay men feel a need to assert themselves against male heterosexual norms, lesbians may be more concerned about sexism than about lesbian identity.\n\nMost studies of lesbian speech patterns focus on conversational patterns, as in Coates and Jordan (1997) and Morrish and Saunton (2007). Women draw on a variety of discourses, particularly feminist discourses, to establish themselves as not submissive to heteropatriarchy by using cooperative all-female talk, which is marked by less distinct turns and a more collaborative conversational environment. Often the conversational bond between women overrides their sexual identities. However, the content of lesbian discourse can separate those who use it from heteronormativity and the values of dominant cultures. Collaborative discourse involves resisting dominant gender norms through more subtle creation of solidarity, and not necessarily resisting “gender-typical” linguistic behavior.\n\nAn example of a distinctive way of speaking for a female community is that of female bikers. Dykes on Bikes, a mostly lesbian group, and Ladies of Harley, a mostly heterosexual group, have demonstrated shared experiences. Though the two cultures differ, both have a focus on female bonding and motorcycles and have a shared female biker language. Their shared language helps to establish their shared identity in a largely male-dominated domain and to mark boundaries between them and traditional femininity.\n\nChanging speech styles, or code-switching, can indicate which identity individuals want to put forward as primary at a given time. Choices of language use among gay men depend on the audience and context, and shift depending on situational needs such as the need to demonstrate or conceal gay identity in a particular environment. Likewise, lesbians may foreground lesbian identity in some contexts but not in others. Podesva discusses an example of code-switching where a gay lawyer is being interviewed about anti-gay discrimination on the radio, so he balances the need to sound recognizably gay and the need to sound recognizably educated, since \"gay speech\" tends to be associated with frivolity and lack of education.\n\n“Exploratory switching” can be used to determine whether an interlocutor shares the speaker's identity. For example, a gay man might use certain key words and mannerisms generally known by the community as a test to see whether they are recognized by the interlocutor. This allows the gay man to establish solidarity with a community member previously unknown to him without having to disclose his orientation to a heterosexual and potentially hostile person. However, inconsistency of language use between different sub-groups of the LGBTQ community, along with the existence of non-members who may be familiar with a gay mode of speech, can make such exploratory switching unreliable.\n\nPeople may also use code-switching to comment on society or for entertainment. Black drag performers often use stereotypical “female white English” to disrupt societal assumptions about gender and ethnicity and to express criticisms of these assumptions. Imitations do not necessarily represent actual language use of a group, but rather the generally recognized stereotypical speech of that group. In the language of drag performers, language play is also marked by juxtaposition of contradictory aspects such as very proper language mixed with obscenities, adding to the queens' and kings' deliberate disruption of cultural and linguistic norms.\n\nDon Kulick argues that the search for a link between sexual identity categories and language is misplaced, since studies have failed to show that the language gay men and lesbians use is unique. Kulick argues that though some researchers may be politically motivated to imagine a LGBTQ community that is a unified whole and identifiable through linguistic means, this speech community does not necessarily exist as such. Kulick points out that the LGBTQ community is not homogeneous, nor is its language use. Features of “gay speech” are not used consistently by gay individuals, nor are they consistently absent from the speech of all heterosexual individuals. Further, Kulick takes issue with frequently circular definitions of queer speech. He argues that speech patterns cannot be labeled LGBTQ language simply because they are used by LGBTQ people.\n\nStudies of a speech community that presuppose the existence of that community may reproduce stereotypes that fail to accurately depict the social reality of variance among subgroups within a community and overlapping identities for individuals. Furthermore, studies of gay male language use often look at middle class European Americans who are out as gay to the exclusion of other subgroups of the LGBTQ community, and hence may draw misleading conclusions about the community as a whole.\n\nRusty Barrett suggests that the idea of the homogeneous speech community could perhaps be more accurately replaced by one of a queer community based on community spirit or a queer cultural system, since language use varies so greatly. Kulick proposes, instead of studying speech communities that he concludes \"do not and cannot exist\" because of methodological problems, researchers should study \"language and desire\" through examining repression in the context of linguistics, considering both what is said and what is not or cannot be said. Kulick addresses the need for consideration of the role of sexuality in sexual identity, unlike some lavender linguists who neglect sexuality in favor of linguistic features that might be more likely than sexuality to legitimize gay identity.\n\nThis section explores how traditional approach to the study of language and gender may be flawed and why.\n\nGeorge Lakoff explained the inaccuracy of metonymic models, through which people jump to conclusions without sufficient elaboration, giving rise to prototype effects, in his book \"Women, Fire, and Dangerous Things\". First of all, we commonly consider typical examples as the better examples of a category. For instance, in the category of fruits, apples and oranges are typical examples. It is common practice that we engage in reasoning by making inferences from typical to non-typical examples. As a matter of fact, an enormous amount of our knowledge about categories of things is organized in terms of typical cases. We constantly draw inferences on the basis of that kind of knowledge. Second, salient examples, which are familiar and memorable, are unconsciously used in our understanding of things. For instance, if one's best friend is a vegetarian and they don't know any others well, they will tend to generalize from their best friend to other vegetarians. This is what Tversky and Kahneman referred to as the \"conjunction fallacy\". To understand this notion via probability theory, think of two mutually unrelated events. The theory assumes that the likelihood of the co-occurrence of the two events is lower than that of the occurrence of either, ignoring the fact that the two events are actually unrelated to one another. To understand this with regards to lavender linguistics, just because two individuals are both self-identified bisexual males does not necessarily mean that they must engage in the same linguistic patterns and social styles. The failure to capture this asymmetry between prototypical and non-prototypical cases results in ineffective study of lavender linguistics. Typical and salient examples are just two kinds of metonymic models. Others include social stereotypes, ideal cases, paragons, generators, and submodels.\n\nA significant multitude of scholastic studies have shown that the linguistic styles of GLB and straight people are not mutually exclusive. Munson et al. (2006), for instance, examined the gradient nature of perceived sexual orientation by having 40 listeners rate 44 talkers' sexual orientation on a five-point equally appearing interval scale. The 44 talkers included equal number of GLB and straight people. When averaged across the 40 listeners, ratings for individual talkers showed some overlap between GLB and straight people. For example, the two men who were tied with the most-gay average ratings included one self-identified straight man, and one self-identified gay man. While there are group level differences between GLB and straight people in the gay soundness of their voices, overlap does exist, providing a serious challenge to a simple model in which speech differences were the inevitable consequence of sexual orientation. The fact that there is no clean cut between the linguistic patterns of GLB and straight people suggests that too many generations in the study of language and gender can be dangerous.\n\nContemporary sociolinguistic studies suggest that styles are learned, rather than assigned at the time of birth. With that said, identities emerge in a time series of social practice, through the combined effects of structure and agency. Because social identities are not static, the speech community model, which was traditionally employed as a sociolinguistic framework in the study of language and gender, is not as reliable as the community of practice model, the new framework emerged from practice theory. Also, because social identities are not static, speech styles are actively subject to change, such that one's speech styles have different social meanings across time. Similarly, it is possible for an individual to engage in multiple identity practices simultaneously, and move from one identity to another unconsciously and automatically, and thus the term polyphonous identity. Podesva (2004) is a paper that studies recordings of a gay medical student, whom he called \"Heath\", as he moved through different situations in the course of his everyday life. The fact that Heath's pronunciation of the voiceless alveolar stop, /t/, varies when he deals with different groups of people suggest not only some of gay people's speech features, but also the multiplicity of a person's social identity. Furthermore, Podesva also examined the relationship between the California Vowel Shift (CVS) and the gay identity, again by investigating intra-speaker patterns in a single individual, Regan, as opposed to inter-speaker variation, and found that Regan, who is a self-identified gay Asian American, realized CVS differently depending on the context, whether it be a \"boys' night out\", \"dinner with friend\", or \"meeting with supervisor\". This cross-situational patterns are critical in the sense that an individual's speech styles can change not only across time, but also across space, depending on which social identity the individual is attempting to engage in under a given situation. Over-generalizations of social identity, however, overlook this intra-speaker variability.\n\nAccording to many language scholars, it is misleading to assume that all sex and gender roles are the same as those that are salient within Western society or that the linguistic styles associated with given groups will be like the styles associated with similarly identified Western groups.\n\nBaklas are homosexual Filipino men, but the concept of bakla identity does not map cleanly to Western male homosexuality. With baklas, as with other non-Western sexual minority groups, sexual identity is very closely related to gender identity. Baklas often assume female attributes and dress like women. They also use female terms for themselves and occasionally for their body parts, and are sometimes referred to and refer to themselves as not being \"real men\".\n\nAlthough they have contact with other gay cultures through technology, bakla culture remains fairly distinct. They have their own rapidly shifting linguistic code called Swardspeak, which is influenced by Spanish and English loan words. This code mostly consists of lexical items, but also includes sound changes such as [p] to [f]. Some baklas who move to the United States continue to use this code, but others abandon it, regarding it as a Filipino custom that is out of place abroad and replacing it with aspects of American gay culture.\n\nHijras are Indians who refer to themselves as neither man nor woman. Some describe hijras as a \"third sex\". Their identity is distinct from a Western gay or transgender identity, though many hijras have male sexual partners. There is a distinctive mode of speech often attributed to hijras, but it is stereotypical frequently derogatory. It is often the standard for Hijras to adopt feminine mannerisms, feminine gender agreement when addressing the self or other Hijaras, and pronouns, depending on context and their interlocutors, to create solidarity or distance. They also use stereotypically male elements of speech, such as vulgarity. Hijras often refer to themselves as masculine in the past tense and females in the present. Their combined use of masculine and feminine speech styles can be seen as reflecting their ambiguous sexual identities and challenging dominant sexuality and gender ideologies. Thus, hijras use grammar as a form of resistance against gender roles.\n\n\n"}
{"id": "16851393", "url": "https://en.wikipedia.org/wiki?curid=16851393", "title": "Language bank", "text": "Language bank\n\nA language bank is an organization that helps people who need translation or interpretation services fulfill those needs through the assistance of qualified translators or interpreters. Such organizations usually, but not always, provide such services free of charge, often as a service of local government. Language banks often service immigrant or refugee communities, often in collaboration with health service providers such as the American Red Cross.\n\nVarious sorts of businesses also manage internal language service needs by setting up language banks. Often, employees with language skills may donate a part of their work time to participating in language banks.\n"}
{"id": "10261226", "url": "https://en.wikipedia.org/wiki?curid=10261226", "title": "Language bioprogram theory", "text": "Language bioprogram theory\n\nThe language bioprogram theory or language bioprogram hypothesis (LBH) is a theory arguing that the structural similarities between different creole languages cannot be solely attributed to their superstrate and substrate languages. As articulated mostly by Derek Bickerton, creolization occurs when the linguistic exposure of children in a community consists solely of a highly unstructured pidgin; these children use their innate language capacity to transform the pidgin, which characteristically has high syntactic variability, into a language with a highly structured grammar. As this capacity is universal, the grammars of these new languages have many similarities.\n\nBy comparing Hawaiian Creole, Haitian Creole and Sranan, Bickerton identified twelve features which he believed to be integral to any creole: \n\nHaving analyzed these features, he believed that he was able to characterize, at least partly, the properties of innate grammar. Bickerton in his LBH, defined very precisely what he considers to be a creole: a language that has arisen out of a prior pidgin that had not existed for more than a generation and among a population where, at most, 20% were speakers of the dominant language and where the remaining 80% were linguistically diverse. Such a definition excludes many languages that might be called creoles. Moreover, lack of historical data makes it often impossible to evaluate such claims. In addition, many of the creole languages that fit this definition do not display all the twelve features, while, according to , the left-out creoles often display more of them. Another problem, raised by , is that if the same bioprogram was the starting point of all creoles, one must explain the differences between them, and language diversity in general, as the bioprogram is universal.\n\nOn the other hand, Bickerton, puts emphasis on children's contribution to the development of a creole and the abrupt character of this process. For example, in , he exhibits ungrammatical utterances made by English-speaking children between the ages of two and four, and argues that they are very similar to perfectly grammatical sentences of English-based creole languages:\n\nNormally, the grammar behind such utterances made by children is eventually altered as parents continue to model a grammar different from this innate one. Presumably, if such children were removed from exposure to English parents, their grammars would continue to be that of creole languages.\n\nHowever, according to , the differences between the speech of children and adults in Tok Pisin are so big that communication is drastically hindered.\n\nThe verb conjugation is typically close to an ideal tense–modality–aspect pattern. In this system, the absence or presence of auxiliary verbs indicate tense (concurrent or anterior), modality (realis or irrealis) and aspect (punctual or progressive), and when present these auxiliaries occur in that order, and typically are based on similar meaning words in the pidgin or superstrate language. Thus anterior tense may be marked by words such as \"bin\" in English-based creoles (from \"been\"), or \"té\" in French-based creoles (from \"été\"), a future or subjunctive tense may be marked by \"go\" (from English \"go\") or \"al\" (from French \"aller\"), and a non-punctual (non-stative) aspect by a word such as \"stei\" (from English \"stay\").\n\nThe above table demonstrates syntactic similarities of creole languages. Stative verbs are those that cannot form the nonpunctual aspect. According to Bickerton, all observed creole languages strictly follow a structure that has the anterior particle precede the irreal particle, and the irreal particle precede the nonpunctual particle, although in certain languages some compounded forms may be replaced by other constructions.\n\nMcWhorter contributed to the LBH with his Creole Prototype Theory, which argues that creoles exhibit some features that may be used to distinguish them from other languages without referring to the socio-historical dimension. According to , creoles are much less likely than other languages:\n\nThese features do not appear in creoles because creoles are relatively young languages, but they may appear later on in their grammars as the languages change. He does not claim that all creoles are ideal examples of the prototype, rather they exhibit varying degrees of conformity with the prototype.\n\nBickerton proposed in 1976 an empirical test of his theory, which involved putting families speaking mutually unintelligible languages on a previously uninhabited island for three years. Federal funding for the test was obtained, but the experiment was cancelled over concerns that informed consent could not be obtained, given the breadth of unknown possible hazards of participation.\n\n\n"}
{"id": "2383086", "url": "https://en.wikipedia.org/wiki?curid=2383086", "title": "Language development", "text": "Language development\n\nLanguage development is a process starting early in human life. Infants start without knowing a language, yet by 10 months, babies can distinguish speech sounds and engage in babbling. Some research has shown that the earliest learning begins in utero when the fetus starts to recognize the sounds and speech patterns of its mother's voice and differentiate them from other sounds after birth.\n\nTypically, children develop receptive language abilities before their verbal or expressive language develops. Receptive language is the internal processing and understanding of language. As receptive language continues to increase, expressive language begins to slowly develop.\n\nUsually, productive language is considered to begin with a stage of pre-verbal communication in which infants use gestures and vocalizations to make their intents known to others. According to a general principle of development, new forms then take over old functions, so that children learn words to express the same communicative functions they had already expressed by proverbial means.\n\nLanguage development is thought to proceed by ordinary processes of learning in which children acquire the forms, meanings, and uses of words and utterances from the linguistic input. Children often begin reproducing the words that they are repetitively exposed to. The method in which we develop language skills is universal; however, the major debate is how the rules of syntax are acquired. There are two major approaches to syntactic development, an empiricist account by which children learn all syntactic rules from the linguistic input, and a nativist approach by which some principles of syntax are innate and are transmitted through the human genome.\n\nThe nativist theory, proposed by Noam Chomsky, argues that language is a unique human accomplishment, and can be attributed to either \"millions of years of evolution\" or to \"principles of neural organization that may be even more deeply grounded in physical law\". Chomsky says that all children have what is called an innate language acquisition device (LAD). Theoretically, the LAD is an area of the brain that has a set of universal syntactic rules for all languages. This device provides children with the ability to make sense of knowledge and construct novel sentences with minimal external input and little experience. Chomsky's claim is based upon the view that what children hear—their linguistic input—is insufficient to explain how they come to learn language. He argues that linguistic input from the environment is limited and full of errors. Therefore, nativists assume that it is impossible for children to learn linguistic information solely from their environment. However, because children possess this LAD, they are in fact, able to learn language despite incomplete information from their environment. Their capacity to learn language is also attributed to the theory of universal grammar (UG), which posits that a certain set of structural rules are innate to humans, independent of sensory experience. This view has dominated linguistic theory for over fifty years and remains highly influential, as witnessed by the number of articles in journals and books.\n\nThe empiricist theory suggests, contra Chomsky, that there is enough information in the linguistic input children receive and therefore, there is no need to assume an innate language acquisition device exists (see above). Rather than a LAD evolved specifically for language, empiricists believe that general brain processes are sufficient enough for language acquisition. During this process, it is necessary for the child to actively engage with their environment. For a child to learn language, the parent or caregiver adopts a particular way of appropriately communicating with the child; this is known as child-directed speech (CDS). CDS is used so that children are given the necessary linguistic information needed for their language. Empiricism is a general approach and sometimes goes along with the interactionist approach. Statistical language acquisition, which falls under empiricist theory, suggests that infants acquire language by means of pattern perception.\n\nOther researchers embrace an interactionist perspective, consisting of social-interactionist theories of language development. In such approaches, children learn language in the interactive and communicative context, learning language forms for meaningful moves of communication. These theories focus mainly on the caregiver's attitudes and attentiveness to their children in order to promote productive language habits.\n\nAn older empiricist theory, the behaviorist theory proposed by B. F. Skinner suggested that language is learned through operant conditioning, namely, by imitation of stimuli and by reinforcement of correct responses. This perspective has not been widely accepted at any time, but by some accounts, is experiencing a resurgence. New studies use this theory now to treat individuals diagnosed with autism spectrum disorders. Additionally, Relational Frame Theory is growing from the behaviorist theory, which is important for Acceptance and Commitment Therapy. Some empiricist theory accounts today use behaviorist models.\n\nOther relevant theories about language development include Piaget's theory of cognitive development, which considers the development of language as a continuation of general cognitive development and Vygotsky's social theories that attribute the development of language to an individual's social interactions and growth.\n\nEvolutionary biologists are skeptical of the claim that syntactic knowledge is transmitted in the human genome. However, many researchers claim that the ability to acquire such a complicated system is unique to the human species. Non-biologists also tend to believe that our ability to learn spoken language may have been developed through the evolutionary process and that the foundation for language may be passed down genetically. The ability to speak and understand human language requires speech production skills and abilities as well as multisensory integration of sensory processing abilities.\n\nOne hotly debated issue is whether the biological contribution includes capacities specific to language acquisition, often referred to as universal grammar. For fifty years, linguist Noam Chomsky has argued for the hypothesis that children have innate, language-specific abilities that facilitate and constrain language learning. In particular, he has proposed that humans are biologically prewired to learn language at a certain time and in a certain way, arguing that children are born with a language acquisition device (LAD). However, since he developed the minimalist program, his latest version of theory of syntactic structure, Chomsky has reduced the elements of universal grammar, which he believes are prewired in humans to just the principle of recursion, thus voiding most of the nativist endeavor.\n\nResearchers who believe that grammar is learned rather than innate, have hypothesized that language learning results from general cognitive abilities and the interaction between learners and their human interactants. It has also recently been suggested that the relatively slow development of the prefrontal cortex in humans may be one reason that humans are able to learn language, whereas other species are not. Further research has indicated the influence of the FOXP2 gene.\n\nLanguage development and processing begins before birth. Evidence has shown that there is language development occurring antepartum. DeCasper and Spence performed a study in 1986 by having mothers read aloud during the last few weeks of pregnancy. When the infants were born, they were then tested. They were read aloud a story while sucking on a pacifier; the story was either the story read by the mother when the infant was in utero or a new story. The pacifier used was able to determine the rate of sucking that the infant was performing. When the story that the mother had read before was heard, the sucking of the pacifier was modified. This did not occur during the story that the infant had not heard before. The results for this experiment had shown that the infants were able to recognize what they had heard in utero, providing insight that language development had been occurring in the last six weeks of pregnancy.\n\nThroughout the first year of life, infants are unable to communicate with language. Instead, infants communicate with gestures. This phenomenon is known as prelinguistic gestures, which are nonverbal ways that infants communicate that also had a plan backed with the gesture. Examples of these could be pointing at an object, tugging on the shirt of a parent to get the parent's attention, etc. Harding, 1983, devised the major criteria that come along with the behavior of prelinguistic gestures and their intent to communicate. There are three major criteria that go along with a prelinguistic gesture: waiting, persistence, and ultimately, development of alternative plans. This process usually occurs around 8 months of age, where an appropriate scenario may be of a child tugging on the shirt of a parent to wait for the attention of the parent who would then notice the infant, which causes the infant to point to something they desire. This would describe the first two criteria. The development of alternative plans may arise if the parent does not acknowledge what the infant wants, the infant may entertain itself to satisfy the previous desire.\n\nWhen children reach about 15–18 months of age, language acquisition flourishes. There is a surge in word production resulting from the growth of the cortex. Infants begin to learn the words that form a sentence and within the sentence, the word endings can be interpreted. Elissa Newport and colleagues (1999) found that humans learn first about the sounds of a language, and then move on to how to speak the language. This shows how infants learn the end of a word and know that a new word is being spoken. From this step, infants are then able to determine the structure of a language and word.\n\nIt appears that during the early years of language development females exhibit an advantage over males of the same age. When infants between the age of 16 to 22 months were observed interacting with their mothers, a female advantage was obvious. The females in this age range showed more spontaneous speech production than the males and this finding was not due to mothers speaking more with daughters than sons. In addition, boys between 2 and 6 years as a group did not show higher performance in language development over their girl counterparts on experimental assessments. In studies using adult populations, 18 and over, it seems that the female advantage may be task dependent. Depending on the task provided, a female advantage may or may not be present. Similarly, one study found that out of the 5.5% of American children with language impairments, 7.2% are male, and 3.8% are female. There are many different suggested explanations for this gender gap in language impairment prevalence.\n\nIt is currently believed that in regards to brain lateralization males are left-lateralized, while females are bilateralized. Studies on patients with unilateral lesions have provided evidence that females are in fact more bilateralized with their verbal abilities. It seems that when a female has experienced a lesion to the left hemisphere, she is better able to compensate for this damage than a male can. If a male has a lesion in the left hemisphere, his verbal abilities are greatly impaired in comparison to a control male of the same age without that damage. However, these results may also be task-dependent as well as time-dependent.\n\nShriberg, Tomblin, and McSweeny (1999) suggest that the fine motor skills necessary for correct speech may develop more slowly in males. This could explain why some of the language impairments in young males seems to spontaneously improve over time.\n\nIt is also suggested that the gender gap in language impairment prevalence could also be explained by the clinical over diagnosis of males. Males tend to be clinically over diagnosed with a variety of disorders.\n\nThe study by Shriber et al. (1999) further explains that this gap in the prevalence of language impairment could be because males tend to be more visible. These researchers reveal that male children tend to act out behaviorally when they have any sort of disorder, while female children tend to turn inward and develop emotional disorders as well. Thus, the high ratio of males with language impairments may be connected with the fact that males are more visible, and thus more often diagnosed.\n\nResearch in writing development has been limited in psychology. In the research that has been conducted, focus has generally centred on the development of written and spoken language and their connection. Spoken and written skills could be considered linked. Researchers believe that children's spoken language influences their written language. When a child learns to write they need to master letter formation, spelling, punctuation and they also have to gain an understanding of the structure and the organisational patterns involved in written language.\n\nKroll's theory is one of the most significant on children's writing development. He proposed that children's writing development is split into 4 phases. Kroll explicitly states that these phases are 'artificial' in the sense that the boundaries between the phases are imprecise and he recognises that each child is different, thus their development is unique. The phases of writing development have been highlighted to give the reader a broad outline of what phases a child goes through during writing development; however when studying an individual's development in depth, the phases may be disregarded to an extent.\n\nThe first of Kroll's phases is the preparation for writing phase. In this phase the child is believed to grasp the technical skills needed for writing, allowing them to create the letters needed to write the words the children say. In this initial phase children experience many opportunities to extend their spoken language skills. Speaking and writing are considered fairly separate processes here, as children's writing is less well developed at this stage, whereas their spoken language is becoming more skilled.\n\nKroll considers the second phase in writing development to be consolidation. Here, children begin to consolidate spoken and written language. In this phase children's writing skills rely heavily on their spoken language skills, and their written and spoken language becoming integrated. Children's written language skills become stronger as they use their spoken language skills to improve their writing. Then in turn, when a development in children's written language skills is seen, their spoken language skills have also improved. A child's written language in this phase mirrors their spoken language.\n\nIn the third phase, differentiation, children begin to learn that written language regularly differs in structure and style from spoken language. The growth from consolidation to differentiation can be challenging for some children to grasp. Children can 'struggle with the transformation from the basically overt language of speech to the essentially covert activity of writing'. In this phase, the child learns that writing is generally considered more formal than spoken language, which is thought to be casual and conversational. Here, it is believed that children begin to understand that writing serves a purpose.\n\nKroll considers the last phase to be the systematic integration phase. A differentiation and integration between the child's speaking and writing can be seen in this phase. This means that speaking and writing have 'well-articulated forms and functions'; however, they are also integrated in the sense that they use the same system. As a result of the individual being aware of the audience, context and reason they are communicating, both written and spoken language are able to overlap and take several forms at this stage.\n\nKroll used the four phases to give an explanation and generalise about the development of these two aspects of language. The highest significance is placed on the second and third phase, consolidation and differentiation respectively. It could be concluded that children's written and spoken language, in certain respects, become more similar to age, maturation, and experience; however, they are also increasingly different in other respects. The content of the skills are more similar, but the approach used for both writing and speaking are different. When writing and speaking development is looked at more closely it can be seen that certain elements of written and spoken language are differentiating and other elements are integrating, all in the same phase.\n\nPerera conducted a survey and her view mirrors that of Kroll to the extent that she used Kroll's four phases. When a child undergoes initial learning of the written language, they have not yet fully mastered the oral language. It is clear that their written language development is aided by their spoken language; it can also be said that their spoken language development is aided by the development of their written language skills. Kantor and Rubin believe that not all individuals successfully move into the final stage of integration. Perera is also aware that it is hard to assign chronological ages to each phase of writing development, because each child is an individual, and also the phases are 'artificial'.\n\nOther than Kroll's theory, there are four principles on early patterns in writing development discussed by Marie Clay in her book \"What Did I Write?\". The four principles are recurring principle, the generative principle, the sign principle, and the inventory principle. The recurring principle involves patterns and shapes in English writing that develop throughout writing development. The generative principle incorporates the idea that a writer can create new meanings by organizing units of writing and letters of the alphabet. The sign principle is understanding that the word print also involves paper arrangement and word boundaries. And lastly, the inventory principle is the fact that children have the urge to list and name items that they are familiar with, and because of this they can practice their own writing skills.\n\nMore recent research has also explored writing development. Myhill concentrated on the development of written language skills in adolescents aged 13 to 15. Myhill discovered that the more mature writer was aware of the shaping of text, and used non-finite clauses, which mirrored Perera's results (1984). Other researchers focused on writing development up until late adolescence, as there has been a limited research in this area. Chrisite and Derewianke recognize that the survey conducted by Perera (1984) is still one of the most significant research studies in the writing development field and believe Perera's study is similar to theirs. Chrisite and Derewianke (2010) again propose four phases of writing development. The researchers believe that the process of writing development does not stop when an individual leaves formal education, and again, the researchers highlight that these phases are flexible in their onset. The first phase focuses on spoken language as the main aid for writing development, and the development then takes its course reaching the fourth phase, which continues beyond formal education.\n\nThe environment a child develops in has influences on language development. The environment provides language input for the child to process. Speech by adults to children help provide the child with correct language usage repetitively. Environmental influences on language development are explored in the tradition of social interactionist theory by such researchers as Jerome Bruner, Alison Gopnik, Andrew Meltzoff, Anat Ninio, Roy Pea, Catherine Snow, Ernest Moerk and Michael Tomasello. Jerome Bruner who laid the foundations of this approach in the 1970s, emphasized that adult \"scaffolding\" of the child's attempts to master linguistic communication is an important factor in the developmental process.\n\nOne component of the young child's linguistic environment is child-directed speech (also known as baby talk or motherese), which is language spoken in a higher pitch than normal with simple words and sentences. Although the importance of its role in developing language has been debated, many linguists think that it may aid in capturing the infant's attention and maintaining communication. When children begin to communicate with adults, this motherese speech allows the child the ability to discern the patterns in language and to experiment with language.\n\nThroughout existing research, it is concluded that children exposed to extensive vocabulary and complex grammatical structures more quickly develop language and also have a more accurate syntax than children raised in environments without complex grammar exposed to them. With motherese, the mother talks to the child and responds back to the child, whether it be a babble the child made or a short sentence. While doing this, the adult prompts the child to continue communicating, which may help a child develop language sooner than children raised in environments where communication is not fostered.\n\nChild-directed speech concentrates on small core vocabulary, here and now topics, exaggerated facial expressions and gestures, frequent questioning, para-linguistic changes, and verbal rituals. An infant is least likely to produce vocalizations when changed, fed, or rocked. The infant is more likely to produce vocalizations in response to a nonverbal behavior such as touching or smiling.\n\nChild-directed speech also catches the child's attention, and in situations where words for new objects are being expressed to the child, this form of speech may help the child recognize the speech cues and the new information provided. Data shows that children raised in highly verbal families had higher language scores than those children raised in low verbal families. Continuously hearing complicated sentences throughout language development increases the child's ability to understand these sentences and then to use complicated sentences as they develop. Studies have shown that students enrolled in high language classrooms have two times the growth in complex sentences usage than students in classrooms where teachers do not frequently use complex sentences.\n\nAdults use strategies other than child-directed speech like recasting, expanding, and labeling:\nSome language development experts have characterized child directed speech in stages. Primarily, the parents use repetition and also variation to maintain the infant's attention. Secondly, the parent simplifies speech to help in language learning. Third, any speech modifications maintain the responsiveness of the child. These modifications develop into a conversation that provides context for the development.\n\nWhile most children throughout the world develop language at similar rates and without difficulty, cultural and socioeconomic differences have been shown to influence development. An example of cultural differences in language development can be seen when comparing the interactions of mothers in the United States with their infants with mothers in Japan. Mothers in the United States use more questions, are more information-oriented, and use more grammatically correct utterances with their 3-month-olds. Mothers in Japan, on the other hand, use more physical contact with their infants, and more emotion-oriented, nonsense, and environmental sounds, as well as baby talk, with their infants. These differences in interaction techniques reflect differences in \"each society's assumptions about infants and adult-to-adult cultural styles of talking.\"\n\nSpecifically in North American culture, maternal race, education, and socioeconomic class influence parent-child interactions in the early linguistic environment. When speaking to their infants, mothers from middle class \"incorporate language goals more frequently in their play with their infants,\" and in turn, their infants produce twice as many vocalizations as lower class infants. Mothers from higher social classes who are better educated also tend to be more verbal, and have more time to spend engaging with their infants in language. Additionally, lower class infants may receive more language input from their siblings and peers than from their mothers.\n\nIt is crucial that children are allowed to socially interact with other people who can vocalize and respond to questions. For language acquisition to develop successfully, children must be in an environment that allows them to communicate socially in that language. Children who have learnt sound, meaning and grammatical system of language that can produce clear sentence may still not have the ability to use language effectively in various social circumstance. Social interaction is the footing stone of language.\n\nThere are a few different theories as to why and how children develop language. The most popular—and yet heavily debated—explanation is that language is acquired through imitation. This theory has been challenged by Lester Butler, who argues that children do not use the grammar that an adult would use. Furthermore, \"children's language is highly resistant to alteration by adult intervention\", meaning that children do not use the corrections given to them by an adult. The two most accepted theories in language development are psychological and functional. Psychological explanations focus on the mental processes involved in childhood language learning. Functional explanations look at the social processes involved in learning the first language.\n\n\nEach component has its own appropriate developmental periods.\n\nBabies can recognize their mother's voice from as early as few weeks old. It seems like they have a unique system that is designed to recognize speech sound. Furthermore, they can differentiate between certain speech sounds. A significant first milestone in phonetic development is the babbling stage (around the age of six months). This is the baby's way of practicing his control over that apparatus. Babbling is independent from the language. Deaf children for instance, babble the same way as hearing ones.\nAs the baby grows older, the babbling increases in frequency and starts to sound more like words (around the age of twelve months). Although every child is an individual with different pace of mastering speech, there is a tendency to an order of which speech sounds are mastered:\n\nAs the children's ability to produce sound develops, their ability to perceive the phonetic contrast of their language develops. The better they get in mastering the sound, the more sensitive they become to the changes in those sounds in their language once they get exposed to it. They learn to isolate individual phenomes while speaking which also serves as the basis of reading.\n\nSome processes that occur in early age:\n\n\nFrom shortly after birth to around one year, the baby starts to make speech sounds. \nAt around two months, the baby engages in cooing, which mostly consists of vowel sounds. At around four to six months, cooing turns into babbling, which is the repetitive consonant-vowel combinations. Babies understand more than they are able to say. In this 0–8 months range, the child is engaged in vocal play of vegetative sounds, laughing, and cooing.\n\nOnce the child hits the 8–12 month, range the child engages in canonical babbling, i.e. dada as well as variegated babbling. This jargon babbling with intonational contours the language being learned.\n\nFrom 12–24 months, babies can recognize the correct pronunciation of familiar words. Babies also use phonological strategies to simplify word pronunciation. Some strategies include repeating the first consonant-vowel in a multisyllable word ('TV' → 'didi') or deleting unstressed syllables in a multisyllable word ('banana' → 'nana'). Within this first year, two word utterances and two syllable words emerge. This period is often called the holophrastic stage of development, because one word conveys as much meaning as an entire phrase. For instance, the simple word \"milk\" can imply that the child is requesting milk, noting spilled milk, sees a cat drinking milk, etc. One study found that children at this age were capable of comprehending 2-word sentences, producing 2–3 word sentences, and naming basic colors.\n\nBy 24–30 months awareness of rhyme emerges as well as rising intonation. One study concludes that children between the ages of 24–30 months typically can produce 3–4 word sentence, create a story when prompted by pictures, and at least 50% of their speech is intelligible.\n\nBy 36–60 months, phonological awareness continues to improve as well as pronunciation. At this age, children have a considerable experience with language and are able to form simple sentences that are 3 words in length. They use basic prepositions, pronouns, and plurals. They become immensely creative in their language use and learn to categorize items such as recognizing that a shoe is not a fruit. At this age, children also learn to ask questions and negate sentences to develop these questions. Over time, their syntax gets more and more unique and complex. A study reveals that at this age, a child's speech should be at least 75% intelligible.\n\nBy 6–10 years, children can master syllable stress patterns, which helps distinguish slight differences between similar words.\n\nThe average child masters about fifty words by the age of eighteen months. These might include words such as, milk, water, juice and apple (noun-like words). Afterwards they acquire 12 to 16 words a day. By the age of six, they master about 13 to 14 thousand words.\n\nThe most frequent words include adjective-like expressions for displeasure and rejection such as 'no'. They also include social interaction words, such as \"please\" and \"bye\".\n\nThere are three stages for learning the meaning of new words:\n\n\nIn other words, when the child hears the word \"sheep\" he overgeneralizes it to other animals that look like sheep by the external appearance, such as white, wooly and four-legged animal.\n\nContextual clues are a major factor in the child's vocabulary development.\n\nThe child uses contextual clues to draw inferences about the category and meaning of new words. By doing so, the child distinguishes between names and ordinary nouns.\n\nFor example, when an object is presented to the child with the determiner \"a\" (a cat, a dog, a bottle) he perceives it as an ordinary noun.\n\nHowever, when the child hears a noun without the determiner, he perceives it as a name, for instance \"this is Mary\".\n\nChildren usually make correct meaning associations with the words that the adults say. However, sometimes they make semantic errors.\n\nThere are a few types of semantic errors:\n\nOverextension: When a child says or hears a word, they might associate what they see or hear as more generalized concept than the real meaning of the word. For example, if they say \"cat\", they might overextend it to other animals with same features.\n\nUnderextension: It involves the use of lexical items in an overly restrictive fashion. In other words, the child focuses on core members of a certain category. For example: 'cat' may only refer to the family cat and no other cat, or 'dog' may refer to certain kinds of dogs that the child is exposed to.\n\nVerb meaning: when a pre-school child hears the verb 'fill', he understands it as the action 'pour' rather than the result, which is 'make full'.\n\nDimensional terms: the first dimensional adjectives acquired are big and small because they belong to the size category. The size category is the most general one. Later children acquire the single dimension adjectives, such as, tall-short, long-short, high-low. Eventually they acquire the adjectives that describe the secondary dimension, such as thick-thin, wide-narrow and deep-shallow.\n\nFrom birth to one year, comprehension (the language we understand) develops before production (the language we use). There is about a 5-month lag in between the two. Babies have an innate preference to listen to their mother's voice. Babies can recognize familiar words and use preverbal gestures.\n\nWithin the first 12–18 months semantic roles are expressed in one word speech including agent, object, location, possession, nonexistence and denial. Words are understood outside of routine games but the child still needs contextual support for lexical comprehension.\n\n18–24 months Prevalent relations are expressed such as agent-action, agent-object, action-location.\nAlso, there is a vocabulary spurt between 18–24 months, which includes fast mapping. Fast mapping is the babies' ability to learn a lot of new things quickly. The majority of the babies' new vocabulary consists of object words (nouns) and action words (verbs).\n\n30–36 months The child is able to use and understand why question and basic spatial terms such as in, on or under.\n\n36–42 months There is an understanding of basic color words and kinship terms. Also, the child has an understanding of the semantic relationship between adjacent and conjoined sentences, including casual and contrastive.\n\n42–48 months When and how questions are comprehended as well as basic shape words such as circle, square and triangle.\n\n48–60 months Knowledge of letter names and sounds emerges, as well as numbers.\n\nBy 3–5 years, children usually have difficulty using words correctly. Children experience many problems such as underextensions, taking a general word and applying it specifically (for example, 'cartoons' specifically for 'Mickey Mouse') and overextensions, taking a specific word and applying it too generally (example, 'ant' for any insect). However, children coin words to fill in for words not yet learned (for example, someone is a cooker rather than a chef because a child may not know what a chef is). Children can also understand metaphors.\n\nFrom 6–10 years, children can understand meanings of words based on their definitions. They also are able to appreciate the multiple meanings of words and use words precisely through metaphors and puns. Fast mapping continues.\nWithin these years, children are now able to acquire new information from written texts and can explain relationships between multiple meaning words. Common idioms are also understood.\n\nThe development of syntactic structures follows a particular pattern and reveals much on the nature of language acquisition, which has several stages. According to O'Grady and Cho (2011), the first stage, occurring between the ages of 12–18 months, is called \"one-word stage.\" In this stage, children cannot form syntactic sentences and therefore use one-word utterances called \"holophrases\" that express an entire sentence. In addition, children's comprehension is more advanced than their production abilities. For example, a child who wants candy may say \"candy\" instead of expressing a full sentence.\n\nThe following stage is the \"two-word stage\" in which children begin to produce \"mini-sentences\" that are composed of two words, such as \"doggy bark\" and \"Ken water\" (O'Grady & Cho, 2011, p. 346). At this stage, it is unclear whether children have an understanding of underlying rules of the language such as syntactic categories, since their \"mini-sentences\" often lack distinction between the categories. However, children do exhibit sensitivity to sentence structures and they frequently use appropriate word order.\n\nAfter several months of speech that is restricted to short utterances, children enter the \"telegraphic stage\" and begin to produce longer and more complex grammatical structures (O'Grady & Cho, 2011, p. 347). This stage is characterized by production of complex structures as children begin to form phrases consisting of a subject and a complement in addition to use of modifiers and composition of full sentences. Children use mostly content words and their sentences lack function words. For example, a child may say \"fill cup water,\" instead of saying, \"Fill my cup with water.\" Subsequently, language acquisition continues to develop rapidly and children begin to acquire complex grammar that shows understanding of intricate linguistic features, such as the ability to switch the position of words in sentences.\n\nThroughout the process of syntactic development, children acquire and develop question structures. According to O'Grady and Cho (2011), at the early stages of language acquisition, children ask yes-no questions by rising intonation alone as they develop awareness to auxiliary verbs only at a later stage. When auxiliary verbs make their appearance, it takes children a few months before they are able to use inversion in yes-no questions. The development of WH- questions occurs between the ages of two and four, when children acquire auxiliary verbs that then leads to the ability to use inversion in questions. However, some children find inversion easier in yes-no questions than in WH- questions, since the position of the WH- word and the auxiliary verb both must changed (e.g., \"You are going where?\" instead of \"Where are you going?\").\n\nMorphological structures development occurs over a period of several years. Before language is acquired, children lack any use of morphological structures.\n\nThe morphological structures that children acquire during their childhood, and even up to the early school years, are: determiners (a, the), -ing inflection, plural –s, auxiliary be, possessive –s, third person singular –s, past tense –ed).\n\nWhen children start using them they tend to overgeneralize the rules and project them upon all the words in the language, including irregulars. For example: if a child knows the –ed (past tense) there is a possibility that they'll say \"I eated\"( Man-mans cat-cats). These errors result from overgeneralization of rules.\n\nThis overgeneralization is very noticeable, in fact, children do that in less than 25 percent of the time at any point before reducing that percentage. Then they improve their mastery, which can be tested in various ways, such as the \"wug test\" (Berko, 1958).\n\nChildren often figure out quickly frequent irregular verbs, such as go and buy rather than less common ones, such as win. This suggests that children must hear the word several hundred times before they are able to use it correctly.\n\nThis development of bound morphemes is similar in order among children, for example: -\"ing\" is acquired before the article \"the\". The interesting part though is that parents tend to use a different order while speaking to their kids, for example, parents use the article 'the' more frequently than -ing. Meaning, other factors determine the order of acquisition, such as:\n\n\nAs it comes to word formation processes such as derivation and compounding, the earliest derivational suffixes to show up in children's speech are those they hear adults use most frequently. (-er as the *'doer' of the action such as walker.) When it comes to compounds, children first make up names for agents and instruments that they don't know by a pattern (N-N), though some of them do not follow the pattern (*cutter grass for grass cutter). Then, they might have the right structure but the words are inappropriate since English already has words with the intended meaning such as car-smoke = exhaust. This process points to a preference for building words from other words, thus place less demand on memory than learning an entirely new word for each concept.\n\nGrammar describes the way sentences of a language are constructed, which includes both syntax and morphology.\n\nSyntactic development involves the ways that various morphemes are ordered or combined in sentences.Morphemes, which are basic units of meaning in language, get added as children learn to produce simple sentences and become more precise speakers. Morphemes can be whole words (like \"happy\") or parts of words that change meaning of words (\"un\"happy). Brown proposed a stage model that describes the various types of morphological structures that are developed in English and the age range within which they are normally acquired.\n\nStage I: From 15–30 months, children start using telegraphic speech, which are two word combinations, for example 'wet diaper'. Brown (1973) observed that 75% of children's two-word utterances could be summarized in the existence of 11 semantic relations:\nStage II: At around 28–36 months, children begin to engage in the production of simple sentences, usually 3 word sentences. These simple sentences follow syntactic rules and are refined gradually as development continues. The morphological developments seen in this age range include use of present progressive (-ing endings), the prepositions \"in\" and \"on\", and regular plurals (-s endings).\n\nStage III: Around 36–42 months, children continue to add morphemes and gradually produce complex grammatical structures. The morphemes that are added at this age include irregular past tense, possessive ('s), and use of the verb 'to be' (It is, I am, etc.).\n\nStage IV:Around 40–46 months children continue to add to their morphological knowledge. This range is associated with use of articles (a or the), regular past tense (-ed endings), and regular third person speech (He likes it).\n\nStage V: Around 42-52+ months children refine the complex grammatical structures and increase their use of morphemes to convey more complex ideas. Children in this stage use irregular third-person speech, the verb 'to be' as an auxiliary verb (She was not laughing), and in its contraction forms (It's, She's, etc.).\n\nFrom birth to one year, babies can engage in joint attention (sharing the attention of something with someone else). Infants also can engage in turn taking activities on the basis of their sensitivity to reactive contingency, which can elicit social responses in the babies from very early on.\n\n\nThere is a large debate regarding whether or not bilingualism is truly beneficial to children. Parents of children often view learning a second language throughout elementary and high school education beneficial to the child. Another perspective dictates that the second language just confuses the child and prevents them from mastering their primary language. Studies have shown that American bilingual children have greater cognitive flexibility, better perceptual skills and tend to be divergent thinkers than monolingual children between the ages of five to ten. Better executive functioning skills are likely because bilingual children have to choose one language to speak while actively suppressing the other. This builds stronger selective attention and cognitive flexibility because these skills are being exercised more. In addition, bilingual children have a better understanding of universal language concepts, such as grammar, because these concepts are applied in multiple languages. However, studies comparing Swedish-Finnish bilingual children and Swedish monolingual children between the ages of five to seven have also shown that the bilingual children have a smaller vocabulary than monolingual children. In another study throughout America, elementary school English-monolingual children performed better in mathematics and reading activities than their non-English-dominant bilingual and non-English monolingual peers from kindergarten to grade five. Learning two languages simultaneously can be beneficial or a hindrance to a child's language and intellectual development. Further research is necessary to continue to shed light on this debate.\n\nIn addition to the study of bilingualism in children, similar research is being conducted in adults. Research findings show that although bilingual benefits are muted in middle adulthood, they are more profound in older age when those who develop dementia experience onset about 4.5 years later in bilingual subjects. The increased attentional control, inhibition, and conflict resolution developed from bilingualism may be accountable for the later onset of dementia.\n\nThere is some research that demonstrates that bilingualism may often be misdiagnosed as a language impairment. A subtopic of bilingualism in the literature is nonstandard varieties of English. While bilingualism and nonstandard varieties of English cannot be considered a true language impairment, they are misrepresented in the population of those receiving language interventions.\n\nA language disorder is the impaired comprehension and or use of a spoken, written, and/or other symbol system. A disorder may involve problems in the following areas: \n\nOlswang and colleagues have identified a series of behaviors in children in the 18–36 month range that are predictors for the need of language intervention.\n\nThese predictors include: \n\nSome of the many conditions that cause language development problems include:\n\n\n\n\n"}
{"id": "2264137", "url": "https://en.wikipedia.org/wiki?curid=2264137", "title": "Language of the birds", "text": "Language of the birds\n\nIn mythology, medieval literature and occultism, the language of the birds is postulated as a mystical, perfect divine language, green language, adamic language, Enochian, angelic language or a mythical or magical language used by birds to communicate with the initiated.\n\nIn Indo-European religion, the behavior of birds has long been used for the purposes of divination by augurs. According to a suggestion by Walter Burkert, these customs may have their roots in the Paleolithic when, during the Ice Age, early humans looked for carrion by observing scavenging birds.\n\nThere are also examples of contemporary bird-human communication and symbiosis. In North America, ravens have been known to lead wolves (and native hunters) to prey they otherwise would be unable to consume. In Africa, the greater honeyguide is known to guide humans to beehives in the hope that the hive will be incapacitated and opened for them.\n\nDating to the Renaissance, birdsong was the inspiration for some magical engineered languages, in particular musical languages. Whistled languages based on spoken natural languages are also sometimes referred to as the language of the birds. Some language games are also referred to as the language of birds, such as in Oromo and Amharic of Ethiopia.\n\nUkrainian language is known as \"nightingale speech\" amongst its speakers.\n\nIn Norse mythology, the power to understand the language of the birds was a sign of great wisdom. The god Odin had two ravens, called Hugin and Munin, who flew around the world and told Odin what happened among mortal men.\n\nThe legendary king of Sweden Dag the Wise was so wise that he could understand what birds said. He had a tame house sparrow which flew around and brought back news to him. Once, a farmer in Reidgotaland killed Dag's sparrow, which brought on a terrible retribution from the Swedes.\n\nIn the \"Rígsþula\", Konr was able to understand the speech of birds. When Konr was riding through the forest hunting and snaring birds, a crow spoke to him and suggested he would win more if he stopped hunting mere birds and rode to battle against foemen.\n\nThe ability could also be acquired by tasting dragon blood. According to the \"Poetic Edda\" and the \"Völsunga saga\", Sigurd accidentally tasted dragon blood while roasting the heart of Fafnir. This gave him the ability to understand the language of birds, and his life was saved as the birds were discussing Regin's plans to kill Sigurd. Through the same ability Áslaug, Sigurd's daughter, found out the betrothment of her husband Ragnar Lodbrok, to another woman.\nThe 11th century Ramsund carving in Sweden depicts how Sigurd learnt the language of birds, in the \"Poetic Edda\" and the \"Völsunga saga\".\n\nIn an eddic poem loosely connected with the Sigurd tradition which is named \"Helgakviða Hjörvarðssonar\", the reason why a man named Atli once had the ability is not explained. Atli's lord's son Helgi would marry what was presumably Sigurd's aunt, the Valkyrie Sváfa.\n\nAccording to Apollonius Rhodius, the figurehead of Jason's ship, the \"Argo\", was built of oak from the sacred grove at Dodona and could speak the language of birds. Tiresias was also said to have been given the ability to understand the language of the birds by Athena. The language of birds in Greek mythology may be attained by magical means. Democritus, Anaximander, Apollonius of Tyana, Melampus and Aesopus were all said to have understood the birds.\n\nThe 'birds' are also mentioned in Homer's Odyssey : \"“[...] although I am no prophet really, and I do not know much about the meaning of birds. I tell you he will not long be absent from his dear native land, not if chains of iron hold him fast. He will find a way to get back, for he is never at a loss.\"\n\nIn the Quran, Suleiman (Solomon) and David are said to have been taught the language of the birds. Within Sufism, the language of birds is a mystical divine language. \"The Conference of the Birds\" is a mystical poem of 4647 verses by the 12th century Persian poet Attar of Nishapur.\n\nIn the Jerusalem Talmud, Solomon's proverbial wisdom was due to his being granted understanding of the language of birds by God.\n\nIn Egyptian Arabic, hieroglyphic writing is called \"the alphabet of the birds\".\n\nThe concept is also known from many folk tales (including Welsh, Russian, German, Estonian, Greek, Romany), where usually the protagonist is granted the gift of understanding the language of the birds either by some magical transformation, or as a boon by the king of birds. The birds then inform or warn the hero about some danger or hidden treasure. One example is the Russian story The Language of the Birds.\n\nIn Kabbalah, Renaissance magic, and alchemy, the language of the birds was considered a secret and perfect language and the key to perfect knowledge, sometimes also called the \"langue verte\", or green language (Jean Julien Fulcanelli, Heinrich Cornelius Agrippa \"de occulta philosophia\", (Emmanuel-Yves Monin, \"Hieroglyphes Français Et Langue Des Oiseaux)\", \n\nCompare also the rather comical and satirical \"Birds\" of Aristophanes and \"Parliament of Fowls\" by Chaucer.\n\nIn medieval France, the language of the birds (\"la langue des oiseaux\") was a secret language of the Troubadours, connected with the Tarot, allegedly based on puns and symbolism drawn from homophony, e. g. an inn called \"au lion d'or\" (\"the Golden Lion\") is allegedly \"code\" for \"au lit on dort\" \"in the bed one sleeps\".\n\nRené Guénon has written an article about the symbolism of the language of the birds.\n\n\"Hiéroglyphes Français Et La Langue Des Oiseaux, Editions du Point d'Eau\" by Emmanuel Yves-Monin is a systematic study on the subject but is only available in French.\n\nThe artificial language zaum of Russian Futurism was described as \"language of the birds\" by Velimir Khlebnikov.\n\nThe children's book author Rafe Martin has written \"The Language of Birds\" as an adaptation of a Russian folk tale; it was made into a children's opera by composer John Kennedy.\n\n\n\n"}
{"id": "5496967", "url": "https://en.wikipedia.org/wiki?curid=5496967", "title": "Languages of Senegal", "text": "Languages of Senegal\n\nSenegal is a multilingual country: \"Ethnologue\" lists 36 languages, Wolof being the most widely spoken language.\n\nFrench, which was inherited from the colonial era, is the official language of Senegal. It is used by the administration and understood by about 15–20% of all males and about 1–2% of all women. Senegal is a member State of the Organisation internationale de la Francophonie. A Senegalese, Abdou Diouf, held the position of its Executive Secretary between 2003 and 2014.\n\nSeveral of the Senegalese languages have the status of \"national languages\": Balanta-Ganja, Hassaniya Arabic, Jola-Fonyi, Mandinka, Mandjak, Mankanya, Noon (Serer-Noon), Pulaar, Serer, Soninke, and Wolof.\n\nIn terms of usage, Wolof is the lingua franca and the most widely spoken language in Senegal, as a first or second language (80%).\n\nMande languages spoken include Soninke, and Mandinka. Jola (Diola) is a main language in the Casamance region. The Guinea Creole dialect, based on Portuguese is also spoken in that region. In 2008 Senegal, due to its historical connections to Portuguese colonisation in Casamance, was admitted as Associate Observer in the CPLP (Community of Portuguese Language Countries).\n\nEducation for the deaf in Senegal uses American Sign Language, introduced by the deaf American missionary Andrew Foster. A local language is Mbour Sign Language.\n\nA report for the High Council of Francophonie in Paris stated in 1986 that in Senegal, 60,000 people spoke French as a first language and 700,000 spoke French as a second language. The total population of Senegal at the time was 6.5 million.\n\n\n\n"}
{"id": "1723892", "url": "https://en.wikipedia.org/wiki?curid=1723892", "title": "Linguistic philosophy", "text": "Linguistic philosophy\n\nLinguistic philosophy is the view that philosophical problems are problems which may be solved (or dissolved) either by reforming language, or by understanding more about the language we presently use. The former position is that of ideal language philosophy, the latter the position of ordinary language philosophy.\n\n\n"}
{"id": "59787", "url": "https://en.wikipedia.org/wiki?curid=59787", "title": "Loanword", "text": "Loanword\n\nA loanword (also loan word or loan-word) is a word adopted from one language (the donor language) and incorporated into another language without translation. This is in contrast to cognates, which are words in two or more languages that are similar because they share an etymological origin, and calques, which involve translation.\n\nA loanword is distinguished from a calque (loan translation), which is a word or phrase whose meaning or idiom is adopted from another language by translation into existing words or word-forming roots of the recipient language.\n\nExamples of loanwords in the English language include \"café\" (from French \"café\", which literally means \"coffee\"), bazaar (from Persian \"bāzār\", which means \"market\"), and kindergarten (from German \"Kindergarten\", which literally means \"children's garden\").\n\nIn a bit of heterological irony, the word \"calque\" is a loanword from the French noun, derived from the verb \"calquer\" (to trace, to copy); the word \"loanword\" is a calque of the German word \"Lehnwort\"; and the phrase \"loan translation\" is a calque of the German \"Lehnübersetzung\".\n\nLoans of multi-word phrases, such as the English use of the French term \"déjà vu\", are known as adoptions, adaptations, or lexical borrowings.\n\nStrictly speaking, the term \"loanword\" conflicts with the ordinary meaning of \"loan\" in that something is taken from the donor language without it being something that is possible to return.\n\nMost of the technical vocabulary of classical music (such as concerto, allegro, tempo, aria, opera, and soprano) is borrowed from Italian, and that of ballet from French.\n\nThe studies by Werner Betz (1949, 1939), Einar Haugen (1950, also 1956), and Uriel Weinreich (1953) are regarded as the classical theoretical works on loan influence. The basic theoretical statements all take Betz’s nomenclature as their starting point. Duckworth (1977) enlarges Betz’s scheme by the type “partial substitution” and supplements the system with English terms. A schematic illustration of these classifications is given below.\n\nThe expression \"foreign word\" used in the illustration below is, however, an incorrect translation of the German term \"Fremdwort\", which refers to loanwords whose pronunciation, spelling, and possible inflection or gender have not yet been so much adapted to the new language that they cease to feel foreign. Such a separation of loanwords into two distinct categories is not used by linguists in English in talking about any language. In addition, basing such a separation mainly on spelling as described in the illustration is (or, in fact, was) not usually done except by German linguists and only when talking about German and sometimes other languages that tend to adapt foreign spellings, which is rare in English unless the word has been in wide use for a very long time.\n\nAccording to the linguist Suzanne Kemmer, the expression \"foreign word\" can be defined as follows in English: \"[W]hen most speakers do not know the word and if they hear it think it is from another language, the word can be called a foreign word. There are many foreign words and phrases used in English such as bon vivant (French), mutatis mutandis (Latin), and Schadenfreude (German).\" This is however not how the term is (incorrectly) used in this illustration:\n\nOn the basis of an importation-substitution distinction, Haugen (1950: 214f.) distinguishes three basic groups of borrowings: “(1) \"Loanwords\" show morphemic importation without substitution... (2) \"Loanblends\" show morphemic substitution as well as importation... (3) \"Loanshifts\" show morphemic substitution without importation”. Haugen later refined (1956) his model in a review of Gneuss’s (1955) book on Old English loan coinages, whose classification, in turn, is the one by Betz (1949) again.\n\nWeinreich (1953: 47ff.) differentiates between two mechanisms of lexical interference, namely those initiated by simple words and those initiated by compound words and phrases. Weinreich (1953: 47) defines \"simple words\" “from the point of view of the bilinguals who perform the transfer, rather than that of the descriptive linguist. Accordingly, the category ‘simple’ words also includes compounds that are transferred in unanalysed form”. After this general classification, Weinreich then resorts to Betz’s (1949) terminology.\n\nThere is a distinction between \"popular\" and \"learned\" loanwords. Popular loanwords are transmitted orally. Learned loanwords are first used in written language, often for scholarly, scientific, or literary purposes.\n\nThe English language has often borrowed words from other cultures or languages:\n\nSome English loanwords remain relatively faithful to the donor language's phonology even though a particular phoneme might not exist or have contrastive status in English. For example, the Hawaiian word \"aā\" is used by geologists to specify lava that is relatively thick, chunky, and rough. The Hawaiian spelling indicates the two glottal stops in the word, but the English pronunciation, , contains at most one. In addition, the English spelling usually removes the ʻokina and macron diacritics.\n\nThe majority of English affixes, such as \"un-\", \"-ing\", and \"-ly\", were present in older forms in Old English. However, a few English affixes are borrowed. For example, the English verbal suffix \"-ize\" (American English) or \"ise\" (British English) comes from Greek -ιζειν (\"-izein\") via Latin \"-izare\".\n\nDuring more than 600 years of the Ottoman Empire, the literary and administrative language of the empire was Turkish, with many Persian, and Arabic loanwords, called Ottoman Turkish, considerably differing from the everyday spoken Turkish of the time. Many such words were exported to other languages of the empire, such as Albanian, Bosnian, Bulgarian, Croatian, Greek, Hungarian, Ladino, Macedonian, Montenegrin and Serbian. After the empire fell after World War I and the Republic of Turkey was founded, the Turkish language underwent an extensive language reform led by the newly founded Turkish Language Association, during which many adopted words were replaced with new formations derived from Turkic roots. That was part of the ongoing cultural reform of the time, in turn a part in the broader framework of Atatürk's Reforms, which also included the introduction of the new Turkish alphabet.\n\nTurkish also has taken many words from French, such as \"pantolon\" for \"trousers\" (from French \"pantalon\") and \"komik\" for \"funny\" (from French \"comique\"), most of them pronounced very similarly. Word usage in modern Turkey has acquired a political tinge: right-wing publications tend to use more Arabic or Persian originated words, left-wing ones use more adopted from European languages, while centrist ones use more native Turkish root words.\n\nAlmost 350 years of Dutch presence in what is now Indonesia have left significant linguistic traces. Though very few Indonesians have a fluent knowledge of Dutch, the Indonesian language inherited many words from Dutch, both in words for everyday life and as well in scientific or technological terminology. One scholar argues that 20% of Indonesian words can be traced back to Dutch words.\n\nA large percentage of the lexicon of Romance languages, themselves descended from Vulgar Latin, consists of loanwords (later learned or scholarly borrowings) from Latin. These words can be distinguished by lack of typical sound changes and other transformations found in descended words, or by meanings taken directly from Classical or Ecclesiastical Latin that did not evolve or change over time as expected; in addition, there are also semi-learned terms which were adapted partially to the Romance language's character. Latin borrowings can be known by several names in Romance languages: in Spanish, for example, they are usually referred to as \"cultismos\", and in Italian as \"latinismi\". \n\nLatin is usually the most common source of loanwords in these languages, such as in Italian, Spanish, French, etc., and in some cases the total number of loans may even outnumber inherited terms (although the learned borrowings are less often used in common speech, with the most common vocabulary being of inherited, orally transmitted origin from Vulgar Latin). This has led to many cases of etymological doublets in these languages.\n\nFor most Romance languages, these loans were initiated by scholars, clergy, or other learned people and occurred in Medieval times, peaking in the late Middle Ages and early Renaissance era- in Italian, the 14th century had the highest number of loans. In the case of Romanian, the language underwent a \"re-Latinization\" process later than the others (see Romanian lexis, ), in the 18th and 19th centuries, partially using French and Italian words (many of these themselves being earlier borrowings from Latin) as intermediaries, in an effort to modernize the language, often adding concepts that did not exist until then, or replacing words of other origins. These common borrowings and features also essentially serve to raise mutual intelligibility of the Romance languages, particularly in academic/scholarly, literary, technical, and scientific domains. Many of these same words are also found in English (through its numerous borrowings from Latin and French) and other European languages.\n\nIn addition to Latin loanwords, many words of Ancient Greek origin were also borrowed into Romance languages, often in part through scholarly Latin intermediates, and these also often pertained to academic, scientific, literary, and technical topics. Furthermore, to a lesser extent, Romance languages borrowed from a variety of other languages; in particular English has become an important source in more recent times. Study of the origin of these words and their function and context within the language can illuminate some important aspects and characteristics of the language, and can reveal insights on the general phenomenon of lexical borrowing in linguistics as a method of enriching a language.\n\nAccording to Hans Henrich Hock and Brian Joseph, \"languages and dialects ... do not exist in a vacuum\": there is always linguistic contact between groups. The contact influences what loanwords are integrated into the lexicon and which certain words are chosen over others.\n\nIn some cases, the original meaning shifts considerably through unexpected logical leaps. The English word \"Viking\" became Japanese バイキング \"baikingu\" meaning 'buffet', because Imperial Viking was the first restaurant in Japan to offer buffet-style meals.\n\n\n\n"}
{"id": "45231832", "url": "https://en.wikipedia.org/wiki?curid=45231832", "title": "Modern Greek studies", "text": "Modern Greek studies\n\nModern Greek studies () refers to an academic discipline of the humanities whose object is the linguistic, literary, cultural as well as geographical and folkloristic exploration and teaching of Greek in the world (Greece, Cyprus and the Greek diaspora) in the Modern Age and present time.\n"}
{"id": "6014023", "url": "https://en.wikipedia.org/wiki?curid=6014023", "title": "Modern Language Aptitude Test", "text": "Modern Language Aptitude Test\n\nThe Modern Language Aptitude Test (MLAT) was designed to predict a student's likelihood of success and ease in learning a foreign language. It is published by the Language Learning and Testing Foundation.\n\nThe Modern Language Aptitude Test was developed to measure foreign language learning aptitude. Language learning aptitude does not refer to whether or not an individual can or cannot learn a foreign language (it is assumed that virtually everyone can learn a foreign language given adequate opportunity). According to John Carroll and Stanley Sapon, the authors of the MLAT, language learning aptitude refers to the \"prediction of how well, relative to other individuals, an individual can learn a foreign language in a given amount of time and under given conditions\". The MLAT has primarily been used for adults in government language programs and missionaries, but it is also appropriate for students in grades 9 to 12 as well as college/university students so it is also used by private schools and school and clinical psychologists. Similar tests have been created for younger age groups. For example, the Pimsleur Language Aptitude Battery was designed for junior high and high school students while the MLAT-E is for children in grades 3 through 6.\n\nJohn B. Carroll and Stanley Sapon are responsible for the development of the MLAT. They designed the test as part of a five-year research study at Harvard University between 1953 and 1958. One initial purpose of developing the Modern Language Aptitude Test was to help the US Government find and train people who would be successful learners of a foreign language in an intensive program of instruction. \n\nAfter field testing many different kinds of verbal tasks, Carroll chose five tasks that he felt worked well as a combination in predicting foreign language learning success in a variety of contexts. These tasks were minimally correlated with one another, but used together they had demonstrated high predictive validity with respect to such criteria as language proficiency ratings and grades in foreign language classes.\n\nThe design of the MLAT also reflects a major conclusion of Carroll's research, which was that language learning aptitude was not a \"general\" unitary ability, but rather a composite of at least four relatively independent \"specialized\" abilities. The four aspects, or \"components\", of language learning aptitude that Carroll identified were phonetic coding ability, grammatical sensitivity, rote learning ability and inductive language learning ability. In the article \"The prediction of success in intensive foreign language training\", Carroll defined these components as follows:\n\nThe data used to calculate the statistical norms for the MLAT were collected in 1958. The MLAT was administered to approximately nineteen hundred students in grades nine to twelve and thirteen hundred students from ten colleges and universities. For adult norms, the MLAT was administered to about one thousand military and civilian employees of the government. The test was given to the subjects before starting a language course in a school or university or and intensive training program of the US Government. Their performance in the language program was later compared to their score on the MLAT to determine the predictive validity of the test.\n\nThe MLAT consists of five sections, each one testing separate abilities.\n\n\n\n\n\n\nThe uses for the Modern Language Aptitude Test include selection, placement and diagnosis of learning abilities.\n\n\n\n\n\nIn 1967, Carroll and Sapon authored the Modern Language Aptitude Test – Elementary (EMLAT; more recently, MLAT-E). This was an adaptation of the adult version of the MLAT intended for younger students (grades 3 through 6). The MLAT-E is broken down into four parts, three of which are modified versions of the MLAT's \"Part 3 - Hidden Words\", \"Part 4 - Words in Sentences\" and \"Part 1 - Number Learning\". It also includes a new section called \"Finding Rhymes\", which tests the subject's ability to hear speech sounds.\n\nCarroll and Sapon suggest using the MLAT-E in ways similar to the MLAT. It can be used to select students who have the capability to excel in foreign language learning (and may be ready to start instruction earlier), provide a profile of strengths and weaknesses, place students with similar learning rates in the appropriate class, and start to build a history of language learning difficulty, which could be used in conjunction with other evidence to diagnose a foreign language learning disability.\n\nOne issue taken with the MLAT is that it does not include any measure of motivation. Motivation can be a powerful factor; low motivation may cause poor performance in a language course or training program despite a high score on an aptitude test like the MLAT. Alternatively, a relatively low score on an aptitude test combined with high motivation to learn a language may result in average or even above average performance because of a student putting more time and effort into the language program. Accordingly, proper use of the MLAT would be to use it as one part of a more comprehensive assessment of the learner, or to use the test in a setting where motivation is known to be uniformly high. In response to role of motivation in successful learning, Paul Pimsleur developed the Pimsleur Language Aptitude Battery (PLAB), which includes a section that assesses motivation in examinees. \n\nAnother issue taken with using language aptitude tests like the MLAT is that they are not directly helpful to individuals who are required to learn a language regardless of their language learning abilities. According to John Carroll, language learning aptitude is relatively stable over an individual's lifetime, so if an individual scores poorly on the MLAT, there is no proven method to increase their language learning aptitude if they must learn a language. One way the MLAT could be helpful in this situation is as an indicator that more language learning time will be needed relative to someone who received a higher score on the MLAT. It can also assist them by showing which learning strategies that they use best. Similarly, level of difficulty of the language can be considered when selecting those wish to learn a foreign language, i.e. the more difficult the language the greater the need for higher language learning aptitude.\n\nThe age of the test along with its norms is another area of concern. The test was developed in 1953-58 and the norms were calculated with data collected in 1958. \n\nThe validity of the MLAT has also been challenged due to changes in teaching methods since the 1950s. The grammar translation method was likely used with norming subjects in high schools and universities, while government employees and soldiers in intensive language programs focused on oral language skills. Thus, learners included in the validation represented two quite different methods of instruction. The grammar translation method used in high schools and universities has been replaced by more communicative teaching methods. In 1998, research conducted by Madeline Ehrman, Director of Research and Evaluation at the U.S. Foreign Service Institute, where adult government employees are enrolled in a communication oriented intensive language program, produced validity coefficients at approximately the same levels as the original validity coefficients from 1958. This supports the validity of the test as a predictor of success under communicative language teaching. Also, research by Leila Ranta (Associate Professor of Educational Psychology at University of Alberta) as well as Harley and Hart (with the Ontario Institute for Studies in Education of the University of Toronto) has shown an association between good language analytic ability and good language learners in a communicative learning environment (2002).\n\n\n"}
{"id": "21173", "url": "https://en.wikipedia.org/wiki?curid=21173", "title": "Natural language", "text": "Natural language\n\nIn neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.\n\nThough the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Such examples include bees' waggle dance and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax. \n\nAll language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.\n\nControlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.\n\nConstructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L.L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally \"standardized\" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in \"The Language Instinct\"), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino Sine Flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.\n\n\n"}
{"id": "25948712", "url": "https://en.wikipedia.org/wiki?curid=25948712", "title": "Official languages of the United Nations", "text": "Official languages of the United Nations\n\nThe official languages of the United Nations are the six languages that are used in UN meetings, and in which all official UN documents are written. In alphabetical order, they are:\n\n\nThese languages are used at meetings of various UN organs, particularly the General Assembly (Article 51 of its Rules of Procedure), the Economic and Social Council, and the Security Council (Article 41 of its Rules of Procedure). Each representative of a country may speak in any one of these six languages, or may speak in any language and provide interpretation into one of the six official languages. The UN provides simultaneous interpretation from the official language into the other five official languages, via the United Nations Interpretation Service.\n\nThe six official languages are also used for the dissemination of official documents. Generally, the texts in each of the six languages are equally authoritative.\n\nThe United Nations has drawn criticism for relying too heavily on English, and not enough on the other five official languages. Spanish-speaking member nations formally brought this to the attention of the Secretary-General in 2001. Secretary-General Kofi Annan then responded that full parity of the six official languages was unachievable within current budgetary restraints, but he nevertheless attached great importance to improving the linguistic balance. In 2008 and 2009, resolutions of the General Assembly have urged the Secretariat to respect the parity of the six official languages, especially in the dissemination of public information.\n\nOn 8 June 2007, resolutions concerning human resources management at the UN, the General Assembly had emphasized \"the paramount importance of the equality of the six official languages of the United Nations\" and requested that the Secretary-General \"ensure that vacancy announcements specified the need for either of the working languages of the Secretariat, unless the functions of the post required a specific working language\".\n\nThe Secretary-General's most recent report on multilingualism was issued on 4 October 2010. In response, on 19 July 2011, the General Assembly adopted Resolution No. A/RES/65/311 on multilingualism, calling on the Secretary-General, once again, to ensure that all six official languages are given equally favourable working conditions and resources. The resolution noted with concern that the multilingual development of the UN website had improved at a much slower rate than expected.\n\nThe six official languages spoken at the UN are the first or second language of 2.8 billion people on the planet, less than half of the world population. The six languages are official languages in more than half the countries in the world (about one hundred).\n\nThe Charter of the United Nations, its 1945 constituent document, did not expressly provide for official languages of the UN. The Charter was enacted in five languages (Chinese, French, Russian, English, and Spanish) and provided (in Article 111) that the five texts are equally authoritative.\n\nIn 1946, the first session of the United Nations General Assembly adopted rules of procedure concerning languages that purported to apply to \"all the organs of the United Nations, other than the International Court of Justice\", setting out five official languages and two working languages (English and French).\n\nThe following year, the second session of the General Assembly adopted permanent rules of procedure, Resolution 173 (II). The part of those rules relating to language closely followed the 1946 rules, except that the 1947 rules did not purport to apply to other UN organs, just the General Assembly.\n\nMeanwhile, a proposal had been in the works to add Spanish as a third working language in addition to English and French. This was adopted in Resolution 262 (III), passed on 11 December 1948.\n\nIn 1968, Russian was added as a working language of the General Assembly so that, of the GA's five official languages, four (all but Chinese) were working.\n\nIn 1973, the General Assembly made Chinese a working language and added Arabic as both an official language and working language of the GA. Thus all six official languages were also working languages. Arabic was made an official and working language of \"the General Assembly and its Main Committees\", whereas the other five languages had status in all GA committees and subcommittees (not just the main committees). The Arab members of the UN had agreed to pay the costs of implementing the resolution, for three years.\n\nIn 1980, the General Assembly got rid of this final distinction, making Arabic an official and working language of all its committees and subcommittees, as of 1 January 1982. At the same time, the GA requested the Security Council to include Arabic among its official and working languages, and the Economic and Social Council to include Arabic among its official languages, by 1 January 1983.\n\nAs of 1983, the Security Council (like the General Assembly) recognized six official and working languages: Arabic, Chinese, English, French, Russian, and Spanish.\n\nIn the Economic and Social Council, as of 1992, there were six official languages (Arabic, Chinese, English, French, Russian and Spanish) of which three were working languages (English, French, and Spanish). Later, Arabic, Chinese, and Russian were added as working languages in the Economic and Social Council.\n\nIn 1996, the Universal Declaration of Linguistic Rights was presented to UNESCO but has yet to be officially ratified. Many international linguistics still expect that the UN will eventually add new official languages such as Hindi, Swahili, and Malay to represent Asian and African populations and put more resources into assisting endangered languages. \n\nIn 2002, it was reportedly proposed to the then current UN Secretary General Kofi Annan that the UN develops a 'Semi-Official' or 'Regional' language status for commonly spoken languages that do not have Official status so that a higher percentage of the worlds population could be familiar with UN actions. As of 2006, the six official languages are the first or second language of 2.8 billion people on the planet, which is less than half of the world population. The list of potential 'Semi-Official' or 'Regional' languages, many of which can be considered lingua francas in their areas, could include: Amharic, Bengali, Burmese, Cantonese, Chichewa, Fula, German, Gujarati, Hausa, Hindi, Igbo, Indonesian, Italian, Japanese, Javanese, Kannada, Korean, Lingala, Malay, Marathi, Oromo, Pashto, Persian, Portuguese, Punjabi, Shanghaiese, Shona, Swahili, Tagalog, Tamil, Telugu, Thai, Turkish, Vietnamese, Urdu, Yoruba, Zulu, International Sign Language, International Braille, and possibly an international auxiliary language such as Esperanto, Ido, or Interlingua. As of 2017, the UN has taken no public action to approve a \"Semi-Official\" or \"Regional\" status mainly due to expected translation costs.\n\nSince the late 1990's and increasingly in the 2000's, there has been much discussion of adding Hindi, Portuguese, Swahili, Malay and to a lesser extent, Turkish, Bengali, or other languages as additional official languages. The most recent example being when the Deputy Secretary General was asked in January 2018 about the process of adding Hindi as an official language. Most proposed languages are spoken in more than one nation and/or have a large number of speakers. \n\nAs of June 2018, the media branch of the United Nations, UN News (https://news.un.org/), includes website translations into Portuguese and Swahili in addition to the 6 official languages. Other UN documents and websites are already translated into Bengali (referred to as Bangla), Hindi, Urdu, Malay, French Creole, Portuguese, and Swahili but not on an official or consistent basis.\n\nBengali is one of the most widely spoken languages in the world, ranking seventh, with over 240 million speakers. In April 2009, Prime Minister of Bangladesh Sheikh Hasina argued in front of the United Nations General Assembly that the Bengali language should be made one of the official languages of the UN. This was backed by a resolution adopted unanimously by the assembly of the Indian state of West Bengal in December, and support was also given by the states of Assam and Tripura.\n\nHindi is one of the official languages of India and Fiji and is also spoken in Suriname, Mauritius, Trinidad and Tobago and Guyana. It is mutually intelligible to a high degree with Urdu which is spoken in Pakistan and together they are often considered the same language, referred to as Hindustani or Hindi-Urdu. Although very similar verbally, they do have different written scripts; Hindi is written in the Devanagari script and Urdu is written in the Nastaʿlīq script. Hindi has more than 550 million speakers in India alone, of whom 422 million are native, 98.2 million are second language speakers, and 31.2 million are third language speakers. Hindi is the lingua franca of the majority of Indians in the subcontinent, along with Pakistan (as Urdu), Sri Lanka, Bangladesh and Nepal, with its importance as a global language increasing day by day. Hindi is the fourth most-spoken first language in the world, after Mandarin, Spanish and English.\n\nIn 2007, it was reported that the government would \"make immediate diplomatic moves to seek the status of an official language for Hindi at the United Nations\". According to a 2009 press release from its Ministry of External Affairs, the Government of India has been \"working actively\" to have Hindi recognized as an official language of the UN. In 2015, Nepal's Vice President Parmananda Jha stated his firm support for the inclusion of Hindi as an official language of the UN.\n\nBecause of the linguistic diversity in India, the expansion of Hindi has become political and has led to a rise of English in the post-British colonial period. This is why English is included along with Hindi on Indian passports. If the Indian government is able to grant more federal-level protection for other languages such as Bengali, Gujarati, Marathi, Tamil, Telugu and Urdu, as a compromise then the UN may be more likely to grant a vote on adding Hindi as an official language. But as of now, the international community does not seem to see enough benefit to overcome the potential political issues that adding Hindi as official language of the UN could cause. \n\nMalay is an Austronesian language spoken throughout the Malay Peninsula and large swaths of Southeast Asia. Malay is an official language of Malaysia, Brunei, Indonesia (which is the 4th most populous nation in the world), Singapore, East Timor, Christmas Island, Cocos Islands and is a recognized minority language in Thailand where is it spoken in the Narathiwat, Yala, and Pattani regions, and the Philippines where it is spoken in the Zamboanga Peninsula, the Sulu Archipelago, and Bataraza and Balabac in Palawan. \n\nSpoken by more than 280 million people, Malay is considered the 7th most commonly spoken language by Ethnologue 2017. Malay has several names depending on the local vernacular designation, for example it is referred to as Bahasa Melayu in Malaysia, Bahasa Indonesia in Indonesia and Kelantan-Pattani Malay in Thailand. The level of mutual intelligibility between these standards is debated. Having Malay as an official language of the UN would expand its presence into Asia which is growing rapidly. Malay is also prominent on the internet as the Malay language has the sixth most Internet users of all languages.\n\nMany Lusophones have advocated for greater recognition of their language, being the sixth most spoken language in the world and spread across four continents: Portugal in Europe; Brazil (the largest lusophone nation) in South America; Angola, Mozambique, Cape Verde, Guinea-Bissau, São Tomé and Príncipe in Africa; and in Asia, Timor-Leste and Macau. Portuguese is still spoken by a few thousand people in the former Portuguese colonies of Goa and Daman and Diu in India, and is an official language in nine countries.\n\nIn 2008, the President of Portugal announced that the then eight leaders of the Community of Portuguese Language Countries (CPLP) had agreed to take the necessary steps to make Portuguese an official language. This followed a decision by Portugal's legislators to adopt a standardization of Portuguese spelling. The media branch of the UN, UN News, already includes translations into Portuguese.\n\nSwahili is a lingua franca throughout eastern Africa and is especially prevalent in the African Great Lakes region. Swahili, known as Kiswahili by its speakers, is an official language of Kenya, Tanzania, Uganda, Rwanda, and the Democratic Republic of Congo, is an official language of the African Union and is officially recognised as a lingua franca of the East African Community. It is one of the most commonly spoken languages in Africa, is a compulsory subject in all Kenyan schools and is increasingly being used in eastern Burundi and northern Mozambique.\n\nWith between 50 – 100 million speakers, Swahili is lexiconically similar to other eastern Bantu languages such as Comorian which have differing levels of mutual intelligibility. Swahili is already used unofficially in many UN organizations as the United Nations Office at Nairobi, one of the four major UN global offices (in addition to offices in New York City, Vienna, and Geneva), is located in Nairobi, Kenya. The media branch of the UN, UN News (https://news.un.org/), already includes translations into Swahili.\n\nIn September 2011, during a meeting with UN Secretary-General Ban Ki-moon, Turkish Prime Minister Recep Tayyip Erdoğan expressed a desire to see Turkish become an official UN language. Turkic family languages, among which there is high mutual intelligibility, are spoken in Azerbaijan, Turkmenistan, Uzbekistan, Kazakhstan, Kyrgyzstan, Northern Cyprus, Iran, China, Russia, and Afghanistan.\n\nIn a 1999 resolution, the General Assembly requested the Secretary-General to \"appoint a senior Secretariat official as coordinator of questions relating to multilingualism throughout the Secretariat\".\n\nThe first such coordinator was Federico Riesco of Chile, appointed on 6 September 2000.\n\nFollowing Riesco's retirement, Miles Stoby of Guyana was appointed Coordinator for Multilingualism, effective 6 September 2001.\n\nIn 2003, Secretary-General Kofi Annan appointed Shashi Tharoor of India as Coordinator for Multilingualism. This responsibility was in addition to Tharoor's role as Under-Secretary-General for Communications and Public Information, head of the Department of Public Information.\n\nThe current coordinator for multilingualism is Catherine Pollard of Guyana. She replaces Kiyo Akasaka of Japan, who was also Under-Secretary-General for Communications and Public Information.\n\nIn 2010, the UN's Department of Public Information announced an initiative of six \"language days\" to be observed throughout the year, one for each official language, with the goal of celebrating linguistic diversity and learning about the importance of cross-cultural communication. The days and their historical significance are:\n\nUN independent agencies have their own sets of official languages that sometimes are different from that of the principal UN organs. For example, the General Conference of UNESCO has nine official languages including Hindi, Italian, and Portuguese. The Universal Postal Union has just one official language, French. IFAD has four official languages: Arabic, English, French, and Spanish.\nThe next largest international grouping after the UN is the Commonwealth of Nations which is exclusively English speaking. All other international bodies in commerce, transport and sport have tended to the adoption of one or a few languages as the means of communication. This is usually English, closely followed by French (see: list of international organisations which have French as an official language). Regional groups have adopted what is common to other elements of their ethnic or religious background. Standard Arabic is usually adopted across Muslim nation groups. Most of non-Arab Africa is either Francophone or Anglophone because of their imperial past, but there is also a lusophone grouping of countries for the same reason.\n\n\n"}
{"id": "25231955", "url": "https://en.wikipedia.org/wiki?curid=25231955", "title": "Opening sentence", "text": "Opening sentence\n\nAt the beginning of a written work stands the opening sentence. The opening line is part or all of the opening sentence that may start the lead paragraph. For older texts the Latin term \"incipit\" (it begins) is in use for the very first words of the opening sentence.\n\nAs in speech, a personal document such as a letter normally starts with a salutation; this, however, tends not to be the case in documents, articles, essays, poetry, lyrics, and general works of fiction and nonfiction. In nonfiction, the opening sentence generally points the reader to the subject under discussion directly in a matter-of-fact style. In journalism, the opening line typically sets out the scope of the article.\n\nIn fiction, authors have much liberty in the way they can cast the beginning.\nTechniques to hold the reader's attention include keeping the opening sentence to the point, showing attitude, shocking, and being controversial. One of the most famous opening lines, \"It was the best of times, it was the worst of times ...\", starts a sentence of 118 words that draws the reader in by its contradiction; the first sentence of \"Yes\" even contains 477 words. \"Call me Ishmael\" is an example of a short opening sentence. Formulaic openings are generally eschewed, but expected in certain genres, such as fairy tales beginning \"Once upon a time...\".\n\nInspired by the opening, \"It was a dark and stormy night...\", the annual tongue-in-cheek Bulwer-Lytton Fiction Contest invites entrants to compose \"the opening sentence of the worst of all possible novels\", and its derivative, the Lyttle Lytton Contest, for its equivalent in brevity.\n\nThe opening sentence may sometimes be also used as the title for the work, e.g. \"Everything I Possess I Carry With Me\"; papal encyclicals and bulls are titled according to their incipits.\n\n"}
{"id": "39577891", "url": "https://en.wikipedia.org/wiki?curid=39577891", "title": "Phememe", "text": "Phememe\n\nA phememe (from Ancient Greek wikt:φημί ‘I speak, say’) is a hypothesized speech sound with an abstract gestural meaning, proposed by the linguist Mary LeCron Foster. (The term \"phememe\" appeared earlier in the works of Leonard Bloomfield, who defined it as a \"smallest and meaningless unit of linguistic signaling\" – either a phoneme or a taxeme.)\n\nAccording to Foster, phememes were the fundamental building block of the earliest human spoken language (Primordial Language), as opposed to the phonemes of attested languages, contemporary or historical. The phememe concept is thus important in the debate regarding the gestural origin of speech. By way of an example, the sound [m], as a meaningful oral gesture, is understood as pointing to “two opposed surfaces in tapering, pressing together, holding together, crushing, or resting against”. Reflexes of this primordial [m], and its original meaning, are found in ostensibly unrelated present-day languages in words referring to the mouth, the female genitals, and semantic extensions of these, e.g. Latin \"mugio\" ‘to moo, bugle’, \"mutus\" ‘mute’, \"mucus\" ‘mucous’, Japanese \"mugon\" ‘silence, muteness’, Dravidian \"muka-\" ‘face, mouth’, Piro (Arawak) \"musi-\" ‘to be pregnant, impregnate’. According to her most recent published work, Foster proposes that the phememic system of primordial spoken language consisted of the following phememes.\n\nThe philosopher Maxine Sheets-Johnstone makes extensive use of Foster’s concept of the phememe in arguing iconicity to be the foundation of the symbolic process. Earl R. Anderson points out that the phememe is thus an elaboration of earlier theories postulating a role for oral gesture in the origin of spoken language, such as those of the naturalist Alfred Russel Wallace and the anthropologist Edward Burnett Tylor, who called his method \"generative philology\". An implication of the phememe hypothesis is that Primordial Language lacked double articulation, which the American linguist Charles F. Hockett proposed as a central design feature of human language. According to this principle, linguistic signs are made up of building blocks, such as phonemes, which are themselves meaningless, e.g. \"cat\" is composed of the phonemes /k/, /æ/ and /t/. Phememes came to be desemanticized and replaced by meaningless phonemes in later evolution. The anthropologist Gordon Hewes proposes that this shift was motivated by the advantages phonemes entail in rapid word retrieval.\n\n"}
{"id": "16415709", "url": "https://en.wikipedia.org/wiki?curid=16415709", "title": "Phonological development", "text": "Phonological development\n\nSound is at the beginning of language learning. Children have to learn to distinguish different sounds and to segment the speech stream they are exposed to into units – eventually meaningful units – in order to acquire words and sentences. Here is one reason that speech segmentation is challenging: When you read, there are spaces between the words. No such spaces occur between spoken words. So, if an infant hears the sound sequence “thisisacup,” it has to learn to segment this stream into the distinct units “this”, “is”, “a”, and “cup.” Once the child is able to extract the sequence “cup” from the speech stream it has to assign a meaning to this word. Furthermore, the child has to be able to distinguish the sequence “cup” from “cub” in order to learn that these are two distinct words with different meanings. Finally, the child has to learn to produce these words. \nThe acquisition of native language phonology begins in the womb and isn’t completely adult-like until the teenage years. Perceptual abilities (such as being able to segment “thisisacup” into four individual word units) usually precede production and thus aid the development of speech production.\n\nChildren don’t utter their first words until they are about 1 year old, but already at birth they can tell some utterances in their native language from utterances in languages with different prosodic features.\n\nInfants as young as 1 month perceive some speech sounds as speech categories (they display categorical perception of speech). For example, the sounds /b/ and /p/ differ in the amount of breathiness that follows the opening of the lips. Using a computer generated continuum in breathiness between /b/ and /p/, Eimas et al. (1971) showed that English-learning infants paid more attention to differences near the boundary between /b/ and /p/ than to equal-sized differences within the /b/-category or within the /p/-category. Their measure, monitoring infant sucking-rate, became a major experimental method for studying infant speech perception.\n\nInfants up to 10–12 months can distinguish not only native sounds but also nonnative contrasts. Older children and adults lose the ability to discriminate some nonnative contrasts. Thus, it seems that exposure to one’s native language causes the perceptual system to be restructured. The restructuring reflects the system of contrasts in the native language.\n\nAt four months infants still prefer infant-directed speech to adult-directed speech. Whereas 1-month-olds only exhibit this preference if the full speech signal is played to them, 4-month-old infants prefer infant-directed speech even when just the pitch contours are played. This shows that between 1 and 4 months of age, infants improve in tracking the suprasegmental information in the speech directed at them. By 4 months, finally, infants have learned which features they have to pay attention to at the suprasegmental level.\n\nBabies prefer to hear their own name to similar-sounding words. It is possible that they have associated the meaning “me” with their name, although it is also possible that they simply recognize the form because of its high frequency.\n\nWith increasing exposure to the ambient language, infants learn not to pay attention to sound distinctions that are not meaningful in their native language, e.g., two acoustically different versions of the vowel /i/ that simply differ because of inter-speaker variability. By 6 months of age infants have learned to treat acoustically different sounds that are representations of the same sound category, such as an /i/ spoken by a male versus a female speaker, as members of the same phonological category /i/.\n\nInfants are able to extract meaningful distinctions in the language they are exposed to from statistical properties of that language. For example, if English-learning infants are exposed to a prevoiced /d/ to voiceless unaspirated /t/ continuum (similar to the /d/ - /t/ distinction in Spanish) with the majority of the tokens occurring near the endpoints of the continuum, i.e., showing extreme prevoicing versus long voice onset times (bimodal distribution) they are better at discriminating these sounds than infants who are exposed primarily to tokens from the center of the continuum (unimodal distribution).\n\nThese results show that at the age of 6 months infants are sensitive to how often certain sounds occur in the language they are exposed to and they can learn which cues are important to pay attention to from these differences in frequency of occurrence. In natural language exposure this means typical sounds in a language (such as prevoiced /d/ in Spanish) occur often and infants can learn them from mere exposure to them in the speech they hear. All of this occurs before infants are aware of the meaning of any of the words they are exposed to, and therefore the phenomenon of statistical learning has been used to argue for the fact that infants can learn sound contrasts without meaning being attached to them.\n\nAt 6 months, infants are also able to make use of prosodic features of the ambient language to break the speech stream they are exposed to into meaningful units, e.g., they are better able to distinguish sounds that occur in stressed vs. unstressed syllables. This means that at 6 months infants have some knowledge of the stress patterns in the speech they are exposed and they have learned that these patterns are meaningful.\n\nAt 7.5 months English-learning infants have been shown to be able to segment words from speech that show a strong-weak (i.e., trochaic) stress pattern, which is the most common stress pattern in the English language, but they were not able to segment out words that follow a weak-strong pattern. In the sequence ‘guitar is’ these infants thus heard ‘taris’ as the word-unit because it follows a strong-weak pattern.\nThe process that allows infants to use prosodic cues in speech input to learn about language structure has been termed “prosodic bootstrapping”.\n\nWhile children generally don’t understand the meaning of most single words yet, they understand the meaning of certain phrases they hear a lot, such as “Stop it,” or “Come here.”\n\nInfants can distinguish native from nonnative language input using phonetic and phonotactic patterns alone, i.e., without the help of prosodic cues. They seem to have learned their native language’s phonotactics, i.e., which combinations of sounds are possible in the language.\n\nInfants now can no longer discriminate most nonnative sound contrasts that fall within the same sound category in their native language. Their perceptual system has been tuned to the contrasts relevant in their native language. \nAs for word comprehension, Fenson et al. (1994) tested 10-11-month-old children’s comprehension vocabulary size and found a range from 11 words to 154 words. At this age, children normally have not yet begun to speak and thus have no production vocabulary. So clearly, comprehension vocabulary develops before production vocabulary.\n\nEven though children do not produce their first words until they are approximately 12 months old, the ability to produce speech sounds starts to develop at a much younger age. Stark (1980) distinguishes five stages of early speech development:\n\nThese earliest vocalizations include crying and vegetative sounds such as breathing, sucking or sneezing. For these vegetative sounds, infants’ vocal cords vibrate and air passes through their vocal apparatus, thus familiarizing infants with processes involved in later speech production.\n\nInfants produce cooing sounds when they are content. Cooing is often triggered by social interaction with caregivers and resembles the production of vowels.\n\nInfants produce a variety of vowel- and consonant-like sounds that they combine into increasingly longer sequences. The production of vowel sounds (already in the first 2 months) precedes the production of consonants, with the first back consonants (e.g., [g], [k]) being produced around 2–3 months, and front consonants (e.g., [m], [n], [p]) starting to appear around 6 months of age. \nAs for pitch contours in early infant utterances, infants between 3 and 9 months of age produce primarily flat, falling and rising-falling contours. Rising pitch contours would require the infants to raise subglottal pressure during the vocalization or to increase vocal fold length or tension at the end of the vocalization, or both. At 3 to 9 months infants don’t seem to be able to control these movements yet.\n\nReduplicated babbling contains consonant-vowel (CV) syllables that are repeated in reduplicated series of the same consonant and vowel (e.g., [bababa]). At this stage, infants’ productions resemble speech much more closely in timing and vocal behaviors than at earlier stages. \nStarting around 6 months babies also show an influence of the ambient language in their babbling, i.e., babies’ babbling sounds different depending on which languages they hear. For example, French learning 9-10 month-olds have been found to produce a bigger proportion of prevoiced stops (which exist in French but not English) in their babbling than English learning infants of the same age. This phenomenon of babbling being influenced by the language being acquired has been called babbling drift.\n\nInfants now combine different vowels and consonants into syllable strings. At this stage, infants also produce various stress and intonation patterns. During this transitional period from babbling to the first word children also produce “protowords”, i.e., invented words that are used consistently to express specific meanings, but that are not real words in the children’s target language. Around 12–14 months of age children produce their first word.\nInfants close to one year of age are able to produce rising pitch contours in addition to flat, falling, and rising-falling pitch contours.\n\nAt the age of 1, children only just begin to speak, and their utterances are not adult-like yet at all. Children’s perceptual abilities are still developing, too. In fact, both production and perception abilities continue to develop well into the school years, with the perception of some prosodic features not being fully developed until about 12 years of age.\n\nChildren are able to distinguish newly learned ‘words’ associated with objects if they are not similar-sounding, such as ‘lif’ and ‘neem’. They cannot distinguish similar-sounding newly learned words such as ‘bih’ and ‘dih’, however. So, while children at this age are able to distinguish monosyllabic minimal pairs at a purely phonological level, if the discrimination task is paired with word meaning, the additional cognitive load required by learning the word meanings leaves them unable to spend the extra effort on distinguishing the similar phonology.\n\nChildren’s comprehension vocabulary size ranges from about 92 to 321 words. The production vocabulary size at this age is typically around 50 words. This shows that comprehension vocabulary grows faster than production vocabulary.\n\nAt 18–20 months infants can distinguish newly learned ‘words’, even if they are phonologically similar, e.g. ‘bih’ and ‘dih’. While infants are able to distinguish syllables like these already soon after birth, only now are they able to distinguish them if they are presented to them as meaningful words rather than just a sequence of sounds. \nChildren are also able to detect mispronunciations such as ‘vaby’ for ‘baby’. Recognition has been found to be poorer for mispronounced than for correctly pronounced words. This suggests that infants’ representations of familiar words are phonetically very precise. This result has also been taken to suggest that infants move from a word-based to a segment-based phonological system around 18 months of age.\n\nOf course, the reason why children need to learn the sound distinctions of their language is because then they also have to learn the meaning associated with those different sounds. Young children have a remarkable ability to learn meanings for the words they extract from the speech they are exposed to, i.e., to map meaning onto the sounds. Often children already associate a meaning with a new word after only one exposure. This is referred to as “fast mapping”. \nAt 20 months of age, when presented with three familiar objects (e.g., a ball, a bottle and a cup) and one unfamiliar object (e.g., an egg piercer), children are able to conclude that in the request “Can I have the zib,” zib must refer to the unfamiliar object, i.e., the egg piercer, even if they have never heard that pseudoword before. Children as young as 15 months can complete this task successfully if the experiment is conducted with fewer objects. This task shows that children aged 15 to 20 months can assign meaning to a new word after only a single exposure. Fast mapping is a necessary ability for children to acquire the number of words they have to learn during the first few years of life: Children acquire an average of nine words per day between 18 months and 6 years of age.\n\nAt 2 years, infants show first signs of phonological awareness, i.e., they are interested in word play, rhyming, and alliterations. Phonological awareness does continue to develop until the first years of school. For example, only about half of the 4- and 5-year olds tested by Liberman et al. (1974) were able to tap out the number of syllables in multisyllabic words, but 90% of the 6-year-olds were able to do so.\nMost 3-4-year olds are able to break simple consonant-vowel-consonant (CVC) syllables up into their constituents (onset and rime). The onset of a syllable consists of all the consonants preceding the syllable’s vowel, and the rime is made up of the vowel and all following consonants. For example, the onset in the word ‘dog’ is /d/ and the rime is /og/. Children at 3–4 years of age were able to tell that the nonwords /fol/ and /fir/ would be liked by a puppet whose favorite sound is /f/. 4-year olds are less successful at this task if the onset of the syllable contains a consonant cluster, such as /fr/ or /fl/. \nLiberman et al. found that no 4-year-olds and only 17% of 5-year-olds were able to tap out the number of phonemes (individual sounds) in a word. 70% of 6-year-olds were able to do so. This might mean that children are aware of syllables as units of speech early on, while they don’t show awareness of individual phonemes until school age. Another explanation is that individual sounds do not easily translate into beats, which makes clapping individual phonemes a much more difficult task than clapping syllables. One reason why phoneme awareness gets much better once children start school is because learning to read provides a visual aid as how to break up words into their smaller constituents.\n\nAlthough children perceive rhythmic patterns in their native language at 7–8 months, they are not able to reliably distinguish compound words and phrases that differ only in stress placement, such as ‘HOT dog’ vs. ‘hot DOG’ until around 12 years of age. Children in a study by Vogel and Raimy (2002) were asked to show which of two pictures (i.e., a dog or a sausage) was being named. Children younger than 12 years generally preferred the compound reading (i.e., the sausage) to the phrasal reading (the dog). The authors concluded from this that children start out with a lexical bias, i.e., they prefer to interpret phrases like these as single words, and the ability to override this bias develops until late in childhood.\n\nInfants usually produce their first word around 12 –14 months of age. First words are simple in structure and contain the same sounds that were used in late babbling. The lexical items they produce are probably stored as whole words rather than as individual segments that get put together online when uttering them. This is suggested by the fact that infants at this age may produce the same sounds differently in different words.\n\nChildren’s production vocabulary size at this age is typically around 50 words, although there is great variation in vocabulary size among children in the same age group, with a range between 0 and 160 words for the majority of children.\n\nChildren’s productions become more consistent around the age of 18 months. When their words differ from adult forms, these differences are more systematic than before. These systematic transformations are referred to as “phonological processes”, and often resemble processes that are typically common in the adult phonologies of the world’s languages (cf. reduplication in adult Jamaican Creole: “yellow yellow” = “very yellow” ). Some common phonological processes are listed below.\n\n- \"Weak syllable deletion\": omission of an unstressed syllable in the target word, e.g., for ‘banana’\n\n- \"Final consonant deletion\": omission of the final consonant in the target word, e.g., for ‘because’\n\n- \"Reduplication\": production of two identical syllables based on one of the target word syllables, e.g., for ‘bottle’\n\n- \"Consonant harmony\": a target word consonant takes on features of another target word consonant, e.g., for ‘duck’\n\n- \"Consonant cluster reduction\": omission of a consonant in a target word cluster, e.g., for ‘cracker’\n\n- \"Velar fronting\": a velar is replaced by a coronal sound, e.g., for ‘key’\n\n- \"Stopping\": a fricative is replaced by a stop, e.g., for ‘sea’\n\n- \"Gliding\": a liquid is replaced by a glide, e.g., for ‘rabbit’\n\nThe size of the production vocabulary ranges from about 50 to 550 words at the age of 2 years. Influences on the rate of word learning, and thus on the wide range of vocabulary sizes of children of the same age, include the amount of speech children are exposed to by their caregivers as well as differences in how rich the vocabulary in the speech a child hears is. Children also seem to build up their vocabulary faster if the speech they hear is related to their focus of attention more often. This would be the case if a caregiver talks about a ball the child is currently looking at.\n\nA study by Gathercole and Baddeley (1989) showed the importance of sound for early word meaning. They tested the phonological memory of 4- and 5-year-old children, i.e., how well these children were able to remember a sequence of unfamiliar sounds. They found that children with better phonological memory also had larger vocabularies at both ages. Moreover, phonological memory at age 4 predicted the children’s vocabulary at age 5, even with earlier vocabulary and nonverbal intelligence factored out.\n\nChildren produce mostly adult-like segments. Their ability to produce complex sound sequences and multisyllabic words continues to improve throughout middle childhood.\n\nThe developmental changes in infants’ vocalizations over the first year of life are influenced by physical developments during that time. Physical growth of the vocal tract, brain development, and development of neurological structures responsible for vocalization are factors for the development of infants’ vocal productions.\n\nInfants vocal tracts are smaller, and initially also shaped differently from adults’ vocal tracts. The infant’s tongue fills the entire mouth, thus reducing the range of movement. As the facial skeleton grows, the range for movement increases, which probably contributes to the increased variety of sounds infants start to produce. Development of muscles and sensory receptors also gives infants more control over sound production.\nThe limited movement possible by the infant jaw and mouth might be responsible for the typical consonant-vowel (CV) alternation in babbling and it has even been suggested that the predominance of CV syllables in the languages of the world might evolutionarily have been caused by this limited range of movements of the human vocal organs.\n\nThe differences between the vocal tract of infants and adults can be seen in figure 3 (infants) and figure 4 (adults) below.\n\nCrying and vegetative sounds are controlled by the brain stem, which matures earlier than the cortex. Neurological development of higher brain structures coincides with certain developments in infants’ vocalizations. For example, the onset of cooing at 6 to 8 weeks happens as some areas of the limbic system begin to function. The limbic system is known to be involved in the expression of emotion, and cooing in infants is associated with a feeling of contentedness. Further development of the limbic system might be responsible for the onset of laughter around 16 weeks of age. The motor cortex, finally, which develops later than the abovementioned structures may be necessary for canonical babbling, which start around 6 to 9 months of age.\n"}
{"id": "54540850", "url": "https://en.wikipedia.org/wiki?curid=54540850", "title": "Ronde script (calligraphy)", "text": "Ronde script (calligraphy)\n\nRonde (\"round\" in French) is a kind of script in which the heavy strokes are nearly upright, giving the characters when taken together a round look.\n\nRonde script appeared in France at the end of the 16th century, and was popularized by writing masters such as Louis Barbedor in the 17th century. In its original form, it borrowed some of its letterforms from the round Gothic style. This style of writing was still in use (with some modifications) until the 20th century because it was used in French school manuals to teach the bases of cursive writing. It was also commonly used by the scribes of the French Ministry of Finance until right after World War II, which gave this style the name of \"écriture ronde finnancière\" (\"round financial writing\", not to be confused with the \"financière\" writing style).\n\nThe classic French rondes where also very present in the work of 18th century type founder and calligrapher Nicholas Gando, which has been revived for the digital medium by way of the French 111 font.\n"}
{"id": "2917649", "url": "https://en.wikipedia.org/wiki?curid=2917649", "title": "Speech", "text": "Speech\n\nSpeech is human vocal communication using language. Each language uses phonetic combinations of a limited set of perfectly articulated and individualized vowel and consonant sounds that form the sound of its words (that is, all English words sound different from all French words, even if they are the same word, e.g., \"role\" or \"hotel\"), and using those words in their semantic character as words in the lexicon of a language according to the syntactic constraints that govern lexical words' function in a sentence. In speaking, speakers perform many different intentional speech acts, e.g., informing, declaring, asking, persuading, directing, and can use enunciation, intonation, degrees of loudness, tempo, and other non-representational or paralinguistic aspects of vocalization to convey meaning. In their speech speakers also unintentionally communicate many aspects of their social position such as sex, age, place of origin (through accent), physical states (alertness and sleepiness, vigor or weakness, health or illness), psychic states (emotions or moods), physico-psychic states (sobriety or drunkenness, normal consciousness and trance states), education or experience, and the like. \n\nAlthough people ordinarily use speech in dealing with other persons (or animals), when people swear they do not always mean to communicate anything to anyone, and sometimes in expressing urgent emotions or desires they use speech as a quasi-magical cause, as when they encourage a player in a game to do or warn them not to do something. There are also many situations in which people engage in solitary speech. People talk to themselves sometimes in acts that are a development of what some psychologists (e.g., Lev Vygotsky) have maintained is the use in thinking of silent speech in an interior monologue to vivify and organize cognition, sometimes in the momentary adoption of a dual persona as self addressing self as though addressing another person. Solo speech can be used to memorize or to test one's memorization of things, and in prayer or in meditation (e.g., the use of a mantra).\n\nResearchers study many different aspects of speech: speech production and speech perception of the sounds used in a language, speech repetition, speech errors, the ability to map heard spoken words onto the vocalizations needed to recreate them, which plays a key role in children's enlargement of their vocabulary, and what different areas of the human brain, such as Broca's area and Wernicke's area, underlie speech. Speech is the subject of study for linguistics, cognitive science, communication studies, psychology, computer science, speech pathology, otolaryngology, and acoustics. \nSpeech compares with written language\n, which may differ in its vocabulary, syntax, and phonetics from the spoken language, a situation called diglossia. \n\nThe evolutionary origins of speech are unknown and subject to much debate and speculation. While animals also communicate using vocalizations, and trained apes such as Washoe and Kanzi can use simple sign language, no animals' vocalizations are articulated phonemically and syntactically, and do not constitute speech.\n\nSpeech production is a multi-step process by which thoughts are generated into spoken utterances. Production involves the selection of appropriate words and the appropriate form of those words from the lexicon and morphology, and the organization of those words through the syntax. Then, the phonetic properties of the words are retrieved and the sentence is uttered through the articulations associated with those phonetic properties.\n\nIn linguistics (articulatory phonetics), articulation refers to how the tongue, lips, jaw, vocal cords, and other speech organs used to produce sounds are used to make sounds. Speech sounds are categorized by manner of articulation and place of articulation. Place of articulation refers to where the airstream in the mouth is constricted. Manner of articulation refers to the manner in which the speech organs interact, such as how closely the air is restricted, what form of airstream is used (e.g. pulmonic, implosive, ejectives, and clicks), whether or not the vocal cords are vibrating, and whether the nasal cavity is opened to the airstream. The concept is primarily used for the production of consonants, but can be used for vowels in qualities such as voicing and nasalization. For any place of articulation, there may be several manners of articulation, and therefore several homorganic consonants.\n\nNormal human speech is pulmonic, produced with pressure from the lungs, which creates phonation in the glottis in the larynx, which is then modified by the vocal tract and mouth into different vowels and consonants. However humans can pronounce words without the use of the lungs and glottis in alaryngeal speech, of which there are three types: esophageal speech, pharyngeal speech and buccal speech (better known as Donald Duck talk).\n\nSpeech production is a complex activity, and as a consequence errors are common, especially in children. Speech errors come in many forms and are often used to provide evidence to support hypotheses about the nature of speech. As a result, speech errors are often used in the construction of models for language production and child language acquisition. For example, the fact that children often make the error of over-regularizing the -ed past tense suffix in English (e.g. saying 'singed' instead of 'sang') shows that the regular forms are acquired earlier. Speech errors associated with certain kinds of aphasia have been used to map certain components of speech onto the brain and see the relation between different aspects of production: for example, the difficulty of expressive aphasia patients in producing regular past-tense verbs, but not irregulars like 'sing-sang' has been used to demonstrate that regular inflected forms of a word are not individually stored in the lexicon, but produced from affixation of the base form.\n\nSpeech perception refers to the processes by which humans can interpret and understand the sounds used in language. The study of speech perception is closely linked to the fields of phonetics and phonology in linguistics and cognitive psychology and perception in psychology. Research in speech perception seeks to understand how listeners recognize speech sounds and use this information to understand spoken language. Research into speech perception also has applications in building computer systems that can recognize speech, as well as improving speech recognition for hearing- and language-impaired listeners.\n\nSpeech perception is categorical, in that people put the sounds they hear into categories rather than perceiving them as a spectrum. People are more likely to be able to hear differences in sounds across categorical boundaries than within them. A good example of this is voice onset time (VOT). For example, Hebrew speakers, who distinguish voiced /b/ from voiceless /p/, will more easily detect a change in VOT from -10 ( perceived as /b/ ) to 0 ( perceived as /p/ ) than a change in VOT from +10 to +20, or -10 to -20, despite this being an equally large change on the VOT spectrum.\n\nIn speech repetition, speech being heard is quickly turned from sensory input into motor instructions needed for its immediate or delayed vocal imitation (in phonological memory). This type of mapping plays a key role in enabling children to expand their spoken vocabulary. Masur (1995) found that how often children repeat novel words versus those they already have in their lexicon is related to the size of their lexicon later on, with young children who repeat more novel words having a larger lexicon later in development. Speech repetition could help facilitate the acquisition of this larger lexicon.\n\nThere are several organic and psychological factors that can affect speech. Among these are:\n\n\nThe classical or Wernicke-Geschwind model of the language system in the brain focuses on Broca's area in the inferior prefrontal cortex, and Wernicke's area in the posterior superior temporal gyrus on the dominant hemisphere of the brain (typically the left hemisphere for language). In this model, a linguistic auditory signal is first sent from the auditory cortex to Wernicke's area. The lexicon is accessed in Wernicke's area, and these words are sent via the arcuate fasciculus to Broca's area, where morphology, syntax, and instructions for articulation are generated. This is then sent from Broca's area to the motor cortex for articulation.\n\nPaul Broca identified an approximate region of the brain in 1861 which, when damaged in two of his patients, caused severe deficits in speech production, where his patients were unable to speak beyond a few monosyllabic words. This deficit, known as Broca's or expressive aphasia, is characterized by difficulty in speech production where speech is slow and labored, function words are absent, and syntax is severely impaired, as in telegraphic speech. In expressive aphasia, speech comprehension is generally less affected except in the comprehension of grammatically complex sentences. Wernicke's area is named after Carl Wernicke, who in 1874 proposed a connection between damage to the posterior area of the left superior temporal gyrus and aphasia, as he noted that not all aphasic patients had suffered damage to the prefrontal cortex. Damage to Wernicke's area produces Wernicke's or receptive aphasia, which is characterized by relatively normal syntax and prosody but severe impairment in lexical access, resulting in poor comprehension and nonsensical or jargon speech.\n\nModern models of the neurological systems behind linguistic comprehension and production recognize the importance of Broca's and Wernicke's areas, but are not limited to them nor solely to the left hemisphere. Instead, multiple streams are involved in speech production and comprehension. Damage to the left lateral sulcus has been connected with difficulty in processing and producing morphology and syntax, while lexical access and comprehension of irregular forms (e.g. eat-ate) remain unaffected.\nMoreover, the circuits involved in human speech comprehension dynamically adapt with learning, for example, by becoming more efficient in terms of processing time when listening to familiar messages such as learned verses.\n\n\n\n"}
{"id": "148951", "url": "https://en.wikipedia.org/wiki?curid=148951", "title": "Speech disorder", "text": "Speech disorder\n\nSpeech disorders or speech impediments are a type of communication disorder where 'normal' speech is disrupted. This can mean stuttering, lisps, etc. Someone who is unable to speak due to a speech disorder is considered mute.\n\nClassifying speech into normal and disordered is more problematic than it first seems. By a strict classification, only 5% to 10% of the population has a completely normal manner of speaking (with respect to all parameters) and healthy voice; all others suffer from one disorder or another.\n\nThere are three different levels of classification when determining the magnitude and type of a speech disorders and the proper treatment or therapy:\n\n\nIn most cases the cause is unknown. However, there are various known causes of speech impediments, such as \"hearing loss, neurological disorders, brain injury, intellectual disability, drug abuse, physical impairments such as cleft lip and palate, and vocal abuse or misuse.\"\n\nMany of these types of disorders can be treated by speech therapy, but others require medical attention by a doctor in phoniatrics. Other treatments include correction of organic conditions and psychotherapy.\n\nIn the United States, school-age children with a speech disorder are often placed in special education programs. Children who struggle to learn to talk often experience persistent communication difficulties in addition to academic struggles. More than 700,000 of the students served in the public schools’ special education programs in the 2000-2001 school year were categorized as having a speech or language impediment. This estimate does not include children who have speech and language impairments secondary to other conditions such as deafness\". Many school districts provide the students with speech therapy during school hours, although extended day and summer services may be appropriate under certain circumstances.\n\nPatients will be treated in teams, depending on the type of disorder they have. A team can include SLPs, specialists, family doctors, teachers, and family members.\n\nSuffering from a speech disorder can have negative social effects, especially among young children. Those with a speech disorder can be targets of bullying because of their disorder. The bullying can result in decreased self-esteem.\n\nLanguage disorders are usually considered distinct from speech disorders, even though they are often used synonymously.\n\nSpeech disorders refer to problems in producing the sounds of speech or with the quality of voice, where language disorders are usually an impairment of either understanding words or being able to use words and do not have to do with speech production.\n\n"}
{"id": "14405771", "url": "https://en.wikipedia.org/wiki?curid=14405771", "title": "Speech science", "text": "Speech science\n\nSpeech science refers to the study of production, transmission and perception of speech. Speech science involves anatomy, in particular the anatomy of the oro-facial region and neuroanatomy, physiology, and acoustics.\n\nThe production of speech is a highly complex motor task that involves approximately 100 orofacial, laryngeal, pharyngeal, and respiratory muscles. Precise and expeditious timing of these muscles is essential for the production of temporally complex speech sounds, which are characterized by transitions as short as 10 ms between frequency bands and an average speaking rate of approximately 15 sounds per second. Speech production requires airflow from the lungs (respiration) to be phonated through the vocal folds of the larynx (phonation) and resonated in the vocal cavities shaped by the jaw, soft palate, lips, tongue and other articulators (articulation).\n\nRespiration is the physical process of gas exchange between an organism and its environment involving four steps (ventilation, distribution, perfusion and diffusion) and two processes (inspiration and expiration). Respiration can be described as the mechanical process of air flowing into and out of the lungs on the principle of Boyle's law, stating that, as the volume of a container increases, the air pressure will decrease. This relatively negative pressure will cause air to enter the container until the pressure is equalized. During inspiration of air, the diaphragm contracts and the lungs expand drawn by pleurae through surface tension and negative pressure. When the lungs expand, air pressure becomes negative compared to atmospheric pressure and air will flow from the area of higher pressure to fill the lungs. Forced inspiration for speech uses accessory muscles to elevate the rib cage and enlarge the thoracic cavity in the vertical and lateral dimensions. During forced expiration for speech, muscles of the trunk and abdomen reduce the size of the thoracic cavity by compressing the abdomen or pulling the rib cage down forcing air out of the lungs.\n\nPhonation is the production of a periodic sound wave by vibration of the vocal folds. Airflow from the lungs, as well as laryngeal muscle contraction, causes movement of the vocal folds. It is the properties of tension and elasticity that allow the vocal folds to be stretched, bunched, brought together and separated. During prephonation, the vocal folds move from the abducted to adducted position. Subglottal pressure builds and air flow forces the folds apart, inferiorly to superiorly. If the volume of airflow is constant, the velocity of the flow will increase at the area of constriction and cause a decrease in pressure below once distributed. This negative pressure will pull the initially blow open folds back together again. The cycle repeats until the vocal folds are abducted to inhibit phonation or to take a breath.\n\nIn a third process of speech production, articulation, mobile and immobile structures of the face (articulators) adjust the shape of the mouth, pharynx and nasal cavities (vocal tract) as the vocal fold vibration sound passes through producing varying resonant frequencies.\n\nThe analysis of brain lesions and the correlation between lesion locations and behavioral deficits were the most important sources of knowledge about the cerebral mechanisms underlying speech production for many years. The seminal lesion studies of Paul Broca indicated that the production of speech relies on the functional integrity of the left inferior frontal gyrus.\n\nMore recently, the results of noninvasive neuroimaging techniques, such as functional magnetic resonance imaging (fMRI), provide growing evidence that complex human skills are not primarily located in highly specialized brain areas (e.g., Broca's area) but are organized in networks connecting several different areas of both hemispheres instead. Functional neuroimaging identified a complex neural network underlying speech production including cortical and subcortical areas, such as the supplementary motor area, cingulate motor areas, primary motor cortex, basal ganglia, and cerebellum.\n\nSpeech perception refers to the understanding of speech. The beginning of the process towards understanding speech is first hearing the message that is spoken. The auditory system receives sound signals starting at the outer ear. They enter the pinna and continue into the external auditory canal (ear canal) and then to the eardrum. Once in the middle ear, which consists of the malleus, the incus, and the stapes; the sounds are changed into mechanical energy. After being converted into mechanical energy, the message reaches the oval window, which is the beginning of the inner ear. Once inside the inner ear, the message is transferred into hydraulic energy by going through the cochlea, which is filled with fluid, and on to the Organ of Corti. This organ again helps the sound to be transferred into a neural impulse that stimulates the auditory pathway and reaches the brain. Sound is then processed in Heschl's gyrus and associated with meaning in Wernicke's area. As for theories of speech perception, there are a motor and an auditory theory. The motor theory is based upon the premise that speech sounds are encoded in the acoustic signal rather than enciphered in it. The auditory theory puts greater emphasis on the sensory and filtering mechanisms of the listener and suggests that speech knowledge is a minor role that’s only used in hard perceptual conditions.\n\nSpeech is transmitted through sound waves, which follow the basic principles of acoustics. The source of all sound is vibration. For sound to exist, a source (something put into vibration) and a medium (something to transmit the vibrations) are necessary.\n\nSince sound waves are produced by a vibrating body, the vibrating object moves in one direction and compresses the air directly in front of it. As the vibrating object moves in the opposite direction, the pressure on the air is lessened so that an expansion, or rarefaction, of air molecules occurs. One compression and one rarefaction make up one longitudinal wave. The vibrating air molecules move back and forth parallel to the direction of motion of the wave, receiving energy from adjacent molecules nearer the source and passing the energy to adjacent molecules farther from the source.\nSound waves have two general characteristics: A disturbance is in some identifiable medium in which energy is transmitted from place to place, but the medium does not travel between two places.\n\nImportant basic characteristics of waves are wavelength, amplitude, period, and frequency. Wavelength is the length of the repeating wave shape. Amplitude is the maximum displacement of the particles of the medium, which is determined by the energy of the wave. A period (measured in seconds) is the time for one wave to pass a given point. Frequency of the wave is the number of waves passing a given point in a unit of time. Frequency is measured in hertz (hz); (Hz cycles per second) and is perceived as pitch. Each complete vibration of a sound wave is called a cycle. Two other physical properties of sound are intensity and duration. Intensity is measured in decibels (dB) and is perceived as loudness.\n\nThere are two types of tones: pure tones and complex tones. The musical note produced by a tuning fork is called a pure tone because it consists of one tone sounding at just one frequency. Instruments get their specific sounds — their timbre — because their sound comes from many different tones all sounding together at different frequencies. A single note played on a piano, for example, actually consists of several tones all sounding together at slightly different frequencies.\n\n\n"}
{"id": "11171507", "url": "https://en.wikipedia.org/wiki?curid=11171507", "title": "Sphoṭa", "text": "Sphoṭa\n\nThe theory of \"\" is associated with ( 5th century), an early figure in Indic linguistic theory, mentioned in the 670s by Chinese traveller Yi-Jing.\n\nHe theorized the act of speech as being made up of three stages:\n\nAccording to George Cardona, \"Vākyapadīya is considered to be the major Indian work of its time on grammar, semantics and philosophy.\"\n\nWhile the ' theory proper (') originates with , the term has a longer history of use in the technical vocabulary of Sanskrit grammarians, and Bhartṛhari may have been building on the ideas of his predecessors, whose works are partly lost.\n\nSanskrit ' is etymologically derived from the root ' 'to burst'.\nIt is used in its technical linguistic sense by Patañjali (2nd century BCE), in reference to the \"bursting forth\" of meaning or idea on the mind as language is uttered. Patañjali's \"sphoṭa\" is the invariant quality of speech. The acoustic element (\"dhvani\") can be long or short, loud or soft, but the \"sphoṭa\" remains unaffected by individual speaker differences. Thus, a single phoneme (\"varṇa\") such as /k/, /p/ or /a/ is an abstraction, distinct from variants produced in actual enunciation. \nEternal qualities in language are already postulated by Yāska, in his \"Nirukta\" (1.1), where reference is made to another ancient grammarian, \"\", about whose work nothing is known, but who has been suggested as the original source of the concept. \nThe grammarian Vyāḍi, author of the lost text \"Saṃgraha\", may have developed some ideas in \"sphoṭa\" theory; in particular some distinctions relevant to \"dhvani\" are referred to by Bhartṛhari.\n\nThere is no use of ' as a technical term prior to Patañjali, but Pāṇini (6.1.123) refers to a grammarian named ' as one of his predecessors. This has induced Pāṇini's medieval commentators (such as Haradatta) to ascribe the first development of the ' to '.\n\nThe account of the Chinese traveller Yi-Jing places a firm \"terminus ante quem\" of AD 670 on Bhartrhari. Scholarly opinion had formerly tended to place him in the 6th or 7th century; current consensus places him in the 5th century. By some traditional accounts, he is the same as the poet Bhartṛhari who wrote the Śatakatraya.\n\nIn the , the term \"sphoṭa\" takes on a finer nuance, but there is some dissension among scholars as to what Bhartṛhari intended to say. \"Sphoṭa\" retains its invariant attribute, but sometimes its indivisibility is emphasized and at other times it is said to operate at several levels.\nIn verse I.93, Bhartṛhari states that the \"sphota\" is the universal or linguistic type — sentence-type or word-type, as opposed to their tokens (sounds).\n\nBhartṛhari develops this doctrine in a metaphysical setting, where he views \"sphoṭa\" as the language capability of man, revealing his consciousness. \nIndeed, the ultimate reality is also expressible in language, the \"śabda-brahman\", or \"Eternal Verbum\".\nEarly Indologists such as A. B. Keith felt that Bhartṛhari's \"sphoṭa\" was a mystical notion, owing to the metaphysical underpinning of Bhartṛhari's text, \"Vākyapādiya\" where it is discussed. Also, the notion of \"flash or insight\" or \"revelation\" central to the concept also lent itself to this viewpoint. However, the modern view is that it is perhaps a more psychological distinction.\n\nBhartṛhari expands on the notion of \"sphoṭa\" in Patañjali, and discusses three levels:\n\nHe makes a distinction between \"sphoṭa\", which is whole and indivisible, and \"nāda\", the sound, which is sequenced and therefore divisible. The \"sphoṭa\" is the causal root, the intention, behind an utterance, in which sense is similar to the notion of lemma in most psycholinguistic theories of speech production. However, \"sphoṭa\" arises also in the listener, which is different from the lemma position. Uttering the \"nāda\" induces the same mental state or \"sphoṭa\" in the listener - it comes as a whole, in a flash of recognition or intuition (\"pratibhā\", 'shining forth'). This is particularly true for \"vakya-sphoṭa\", where the entire sentence is thought of (by the speaker), and grasped (by the listener) as a whole.\n\nBimal K. Matilal (1990) has tried to unify these views - he feels that for Bhartṛhari the very process of thinking involves vibrations, so that thought has some sound-like properties. Thought operates by \"śabdana\"or 'speaking', - so that the mechanisms of thought are the same as that of language. Indeed, Bhartṛhari seems to be saying that thought is not possible without language. This leads to a somewhat whorfian position on the relationship between language and thought. The \"sphoṭa\" then is the carrier of this thought, as a primordial vibration.\n\nSometimes the \"nāda-sphoṭa\" distinction is posited in terms of the signifier-signified mapping, but this is a misconception. In traditional Sanskrit linguistic discourse (e.g. in Katyāyana), \"vācaka\" refers to the signifier, and 'vācya' the signified. The 'vācaka-vācya' relation is eternal for Katyāyana and the Mīmāṃsakas, but is conventional among the Nyāya. However, in Bhartṛhari, this duality is given up in favour of a more holistic view - for him, there is no independent meaning or signified; the meaning is inherent in the word or the sphoṭa itself.\n\n\"Sphoṭa\" theory remained widely influential in Indian philosophy of language and was the focus of much debate over several centuries. It was adopted by most scholars of Vyākaraṇa (grammar), but both the Mīmāṃsā and Nyāya schools rejected it, primarily on the grounds of compositionality. Adherents of the 'sphota' doctrine were holistic or non-compositional (\"a-khanḍa-pakṣa\"), suggesting that many larger units of language are understood as a whole, whereas the Mīmāṃsakas in particular proposed compositionality (\"khanḍa-pakṣa\"). According to the former, word meanings, if any, are arrived at after analyzing the sentences in which they occur. This debate had many of the features animating present day debates in language over semantic holism, for example.\n\nThe \"Mīmāṃsakas\" felt that the sound-units or the letters alone make up the word. The sound-units are uttered in sequence, but each leaves behind an impression, and the meaning is grasped only when the last unit is uttered. The position was most ably stated by Kumarila Bhatta (7th century) who argued that the 'sphoṭas' at the word and sentence level are after all composed of the smaller units, and cannot be different from their combination. However, in the end it is cognized as a whole, and this leads to the misperception of the \"sphoṭa\" as a single indivisible unit. Each sound unit in the utterance is an eternal, and the actual sounds differ owing to differences in manifestation.\n\nThe \"Nyāya\" view is enunciated among others by Jayanta (9th century), who argues against the \"Mīmāṃsā\" position by saying that the sound units as uttered are different; e.g. for the sound [g], we infer its 'g-hood' based on its similarity to other such sounds, and not because of any underlying eternal. Also, the \"vācaka-vācya\" linkage is viewed as arbitrary and conventional, and not eternal. However, he agrees with Kumarila in terms of the compositionality of an utterance.\n\nThroughout the second millennium, a number of treatises discussed the \"sphoṭa\" doctrine. Particularly notable is Nageśabhaṭṭa's \"Sphotavāda\" (18th century). Nageśa clearly defines \"sphoṭa\" as a carrier of meaning, and identifies eight levels, some of which are divisible.\n\nIn modern times, scholars of Bhartṛhari have included Ferdinand de Saussure, who did his doctoral work on the genitive in Sanskrit, and lectured on Sanskrit and Indo-European languages at the Paris and at the University of Geneva for nearly three decades. It is thought that he might have been influenced by some ideas of Bhartṛhari, particularly the \"sphoṭa\" debate. In particular, his description of the sign, as composed of the signifier and the signified, where these entities are not separable - the whole mapping from sound to denotation constitutes the sign, seems to have some colourings of \"sphoṭa\" in it. Many other prominent European scholars around 1900, including linguists such as Leonard Bloomfield and Roman Jakobson may have been influenced by Bhartṛhari.\n\n\n\n\n"}
{"id": "35676775", "url": "https://en.wikipedia.org/wiki?curid=35676775", "title": "Statistical language acquisition", "text": "Statistical language acquisition\n\nStatistical language acquisition, is a branch of developmental psycholinguistics, that studies the process by which humans develop the ability to perceive, produce, comprehend, and communicate with natural language in all of its aspects (phonological, syntactic, lexical, morphological, semantic) through the use of general learning mechanisms operating on statistical patterns in the linguistic input. Statistical learning acquisition claims that infants language learning is based on pattern perception rather than an innate biological grammar. Several statistical elements such as frequency of words, frequent frames, phonotactic patterns and other regularities provide information on language structure and meaning for facilitation of language acquisition.\n\nFundamental to the study of statistical language acquisition is the centuries-old debate between rationalism (or its modern manifestation in the psycholinguistic community, nativism) and empiricism, with researchers in this field falling strongly in support of the latter category. Nativism is the position that humans are born with innate domain-specific knowledge, especially inborn capacities for language learning. Ranging from seventeenth century rationalist philosophers such as Descartes, Spinoza, and Leibniz to contemporary philosophers such as Richard Montague and linguists such as Noam Chomsky, nativists posit an innate learning mechanism with the specific function of language acquisition.\n\nIn modern times, this debate has largely surrounded Chomsky's support of a universal grammar, properties that all natural languages must have, through the controversial postulation of a language acquisition device (LAD), an instinctive mental 'organ' responsible for language learning which searches all possible language alternatives and chooses the parameters that best match the learner's environmental linguistic input. Much of Chomsky's theory is founded on the poverty of the stimulus (POTS) argument, the assertion that a child's linguistic data is so limited and corrupted that learning language from this data alone is impossible. As an example, many proponents of POTS claim that because children are never exposed to negative evidence, that is, information about what phrases are ungrammatical, the language structure they learn would not resemble that of correct speech without a language-specific learning mechanism. Chomsky's argument for an internal system responsible for language, biolinguistics, poses a three factor model. \"Genetic endowment\" allows the infant to extract linguistic info, detect rules, and have universal grammar. \"External environment\" illuminates the need to interact with others and the benefits of language exposure at an early age. The last factor encompasses the brain properties, learning principles, and computational efficiencies that enable children to pick up on language rapidly using patterns and strategies.\n\nStanding in stark contrast to this position is empiricism, the epistemological theory that all knowledge comes from sensory experience. This school of thought often characterizes the nascent mind as a tabula rasa, or blank slate, and can in many ways be associated with the nurture perspective of the \"nature vs. nurture debate\". This viewpoint has a long historical tradition that parallels that of rationalism, beginning with seventeenth century empiricist philosophers such as Locke, Bacon, Hobbes, and, in the following century, Hume. The basic tenet of empiricism is that information in the environment is structured enough that its patterns are both detectable and extractable by domain-general learning mechanisms. In terms of language acquisition, these patterns can be either linguistic or social in nature.\n\nChomsky is very critical of this empirical theory of language acquisition. He has said, \"It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures.\" He claims the idea of using statistical methods to acquire language is simply a mimicry of the process, rather than a true understanding of how language is acquired.\n\nOne of the most used experimental paradigms in investigations of infants' capacities for statistical language acquisition is the Headturn Preference Procedure (HPP), developed by Stanford psychologist Anne Fernald in 1985 to study infants' preferences for prototypical child-directed speech over normal adult speech. In the classic HPP paradigm, infants are allowed to freely turn their heads and are seated between two speakers with mounted lights. The light of either the right or left speaker then flashes as that speaker provides some type of audial or linguistic input stimulus to the infant. Reliable orientation to a given side is taken to be an indication of a preference for the input associated with that side's speaker. This paradigm has since become increasingly important in the study of infant speech perception, especially for input at levels higher than syllable chunks, though with some modifications, including using the listening times instead of the side preference as the relevant dependent measure.\n\nSimilar to HPP, the Conditioned Headturn Procedure also makes use of an infant's differential preference for a given side as an indication of a preference for, or more often a familiarity with, the input or speech associated with that side. Used in studies of prosodic boundary markers by Gout et al. (2004) and later by Werker in her classic studies of categorical perception of native-language phonemes, infants are conditioned by some attractive image or display to look in one of two directions every time a certain input is heard, a whole word in Gout's case and a single phonemic syllable in Werker's. After the conditioning, new or more complex input is then presented to the infant, and their ability to detect the earlier target word or distinguish the input of the two trials is observed by whether they turn their head in expectation of the conditioned display or not.\n\nWhile HPP and the Conditioned Headturn Procedure allow for observations of behavioral responses to stimuli and after the fact inferences about what the subject's expectations must have been to motivate this behavior, the Anticipatory Eye Movement paradigm allows researchers to directly observe a subject's expectations before the event occurs. By tracking subjects' eye movements researchers have been able to investigate infant decision-making and the ways in which infants encode and act on probabilistic knowledge to make predictions about their environments. This paradigm also offers the advantage of comparing differences in eye movement behavior across a wider range of ages than others.\n\nArtificial languages, that is, small-scale languages that typically have an extremely limited vocabulary and simplified grammar rules, are a commonly used paradigm for psycholinguistic researchers. Artificial languages allow researchers to isolate variables of interest and wield a greater degree of control over the input the subject will receive. Unfortunately, the overly simplified nature of these languages and the absence of a number of phenomena common to all human natural languages such as rhythm, pitch changes, and sequential regularities raise questions of external validity for any findings obtained using this paradigm, even after attempts have been made to increase the complexity and richness of the languages used. The artificial language's lack of complexity or decreased complexity fails to account for a child's need to recognize a given syllable in natural language regardless of the sound variability inherent to natural language, though \"it is possible that the complexity of natural language actually facilitates learning.\"\n\nAs such, artificial language experiments are typically conducted to explore what the relevant linguistic variables are, what sources of information infants are able to use and when, and how researchers can go about modeling the learning and acquisition process. Aslin and Newport, for example, have used artificial languages to explore what features of linguistic input make certain patterns salient and easily detectable by infants, allowing them to easily contrast the detection of syllable repetition with that of word-final syllables and make conclusions about the conditions under which either feature is recognized as important.\n\nStatistical learning has been shown to play a large role in language acquisition, but social interaction appears to be a necessary component of learning as well. In one study, infants presented with audio or audiovisual recordings of Mandarin speakers failed to distinguish the phonemes of the language. This implies that simply hearing the sounds is not sufficient for language learning; social interaction cues the infant to take statistics. Particular interactions geared towards infants is known as \"child-directed\" language because it is more repetitive and associative, which makes it easier to learn. These \"child directed\" interactions could also be the reason why it is easier to learn a language as a child rather than an adult.\n\nStudies of bilingual infants, such as a study Bijeljac-Babic, et al., on French-learning infants, have offered insight to the role of prosody in language acquisition. The Bijeljac-Babic study found that language dominance influences \"sensitivity to prosodic contrasts.\" Although this was not a study on statistical learning, its findings on prosodic pattern recognition might have implications for statistical learning.\n\nIt is possible that the kinds of language experience and knowledge gained through the statistical learning of the first language influences one's acquisition of a second language. Some research points to the possibility that the difficulty of learning a second language may be derived from the structural patterns and language cues that one has already picked up from his or her acquisition of first language. In that sense, the knowledge of and skills to process the first language from statistical acquisition may act as a complicating factor when one tries to learn a new language with different sentence structures, grammatical rules, and speech patterns.\n\nThe first step in developing knowledge of a system as complex as natural language is learning to distinguish the important language-specific classes of sounds, called phonemes, that distinguish meaning between words. UBC psychologist Janet Werker, since her influential series of experiments in the 1980s, has been one of the most prominent figures in the effort to understand the process by which human babies develop these phonological distinctions. While adults who speak different languages are unable to distinguish meaningful sound differences in other languages that do not delineate different meanings in their own, babies are born with the ability to universally distinguish all speech sounds. Werker's work has shown that while infants at six to eight months are still able to perceive the difference between certain Hindi and English consonants, they have completely lost this ability by 11 to 13 months.\n\nIt is now commonly accepted that children use some form of perceptual distributional learning, by which categories are discovered by clumping similar instances of an input stimulus, to form phonetic categories early in life. Developing children have been found to be effective judges of linguistic authority, screening the input they model their language on by shifting their attention less to speakers who mispronounce words. Infants also use statistical tracking to calculate the likelihood that particular phonemes will follow each other.<ref name = \"Romberg/saffron\"> Romberg, Alexa R and Sarron, Jenny R. (2010). \"Statistical Learning and Language Acquisition.\" WIREs Cogn Sci 10.1002/wbs.78</ref>\n\nParsing is the process by which a continuous speech stream is segmented into its discrete meaningful units, e.g. sentences, words, and syllables. Saffran (1996) represents a singularly seminal study in this line of research. Infants were presented with two minutes of continuous speech of an artificial language from a computerized voice to remove any interference from extraneous variables such as prosody or intonation. After this presentation, infants were able to distinguish words from nonwords, as measured by longer looking times in the second case.\n\nAn important concept in understanding these results is that of transitional probability, the likelihood of an element, in this case a syllable, following or preceding another element. In this experiment, syllables that went together in words had a much higher transitional probability than did syllables at word boundaries that just happened to be adjacent. Incredibly, infants, after a short two-minute presentation, were able to keep track of these statistics and recognize high probability words. Further research has since replicated these results with natural languages unfamiliar to infants, indicating that learning infants also keep track of the direction (forward or backward) of the transitional probabilities. Though the neural processes behind this phenomenon remain largely unknown, recent research reports increased activity in the left inferior frontal gyrus and the middle frontal gyrus during the detection of word boundaries.\n\nThe development of syllable-ordering biases is an important step along the way to full language development. The ability to categorize syllables and group together frequently co-occurring sequences may be critical in the development of a \"protolexicon\", a set of common language-specific word templates based on characteristic patterns in the words an infant hears. The development of this protolexicon may in turn allow for the recognition of new types of patterns, e.g. the high frequency of word-initially stressed consonants in English, which would allow infants to further parse words by recognizing common prosodic phrasings as autonomous linguistic units, restarting the dynamic cycle of word and language learning.\n\nThe question of how novice language-users are capable of associating learned labels with the appropriate referent, the person or object in the environment which the label names, has been at the heart of philosophical considerations of language and meaning from Plato to Quine to Hofstadter. This problem, that of finding some solid relationship between word and object, of finding a word's meaning without succumbing to an infinite recursion of dictionary look-up, is known as the symbol grounding problem.\n\nResearchers have shown that this problem is intimately linked with the ability to parse language, and that those words that are easy to segment due to their high transitional probabilities are also easier to map to an appropriate referent. This serves as further evidence of the developmental progression of language acquisition, with children requiring an understanding of the sound distributions of natural languages to form phonetic categories, parse words based on these categories, and then use these parses to map them to objects as labels.\n\nThe developmentally earliest understanding of word to referent associations have been reported at six months old, with infants comprehending the words ' mommy' and 'daddy' or their familial or cultural equivalents. Further studies have shown that infants quickly develop in this capacity and by seven months are capable of learning associations between moving images and nonsense words and syllables.\n\nIt is important to note that there is a distinction, often confounded in acquisition research, between mapping a label to a specific instance or individual and mapping a label to an entire class of objects. This latter process is sometimes referred to as generalization or rule learning. Research has shown that if input is encoded in terms of perceptually salient dimensions rather than specific details and if patterns in the input indicate that a number of objects are named interchangeably in the same context, a language learner will be much more likely to generalize that name to every instance with the relevant features. This tendency is heavily dependent on the consistency of context clues and the degree to which word contexts overlap in the input. These differences are furthermore linked to the well-known patterns of under and overgeneralization in infant word learning. Research has also shown that the frequency in co-occurrence of referents is tracked as well, which helps create associations and dispel ambiguities in object-referent models.\n\nThe ability to appropriately generalize to whole classes of yet unseen words, coupled with the abilities to parse continuous speech and keep track of word-ordering regularities, may be the critical skills necessary to develop proficiency with and knowledge of syntax and grammar.\n\nAccording to recent research, there is no neural evidence of statistical language learning in children with autism spectrum disorders. When exposed to a continuous stream of artificial speech, neurotypical children displayed less cortical activity in the dorsolateral frontal cortices (specifically the middle frontal gyrus) as cues for word boundaries increased. However activity in these networks remained unchanged in autistic children, regardless of the verbal cues provided. This evidence, highlighting the importance of proper Frontal Lobe brain function is in support of the \"Executive Functions\" Theory, used to explain some of the biologically related causes of Autistic language deficits. With impaired working memory, decision making, planning, and goal setting, which are vital functions of the Frontal Lobe, Autistic children are at loss when it comes to socializing and communication (Ozonoff, et al., 2004). Additionally, researchers have found that the level of communicative impairment in autistic children was inversely correlated with signal increases in these same regions during exposure to artificial languages. Based on this evidence, researchers have concluded that children with autism spectrum disorders don't have the neural architecture to identify word boundaries in continuous speech. Early word segmentation skills have been shown to predict later language development, which could explain why language delay is a hallmark feature of autism spectrum disorders.\n\nLanguage learning takes place in different contexts, with both the infant and the caregiver engaging in social interactions. Recent research have investigated how infants and adults use cross-situational statistics in order to learn about not only the meanings of words but also the constraints within a context. For example, Smith and his colleagues proposed that infants learn language by acquiring a bias to label objects to similar objects that come from categories that are well-defined. Important to this view is the idea that the constraints that assist learning of words are not independent of the input itself or the infant's experience. Rather, constraints come about as infants learn about the ways that the words are used and begin to pay attention to certain characteristics of objects that have been used in the past to represent the words.\n\nInductive learning problem can occur as words are oftentimes used in ambiguous situations in which there are more than one possible referents available. This can lead to confusion for the infants as they may not be able to distinguish which words should be extended to label objects being referenced to. Smith and Yu proposed that a way to make a distinction in such ambiguous situations is to track the word-referent pairings over multiple scenes. For instance, an infant who hears a word in the presence of object A and object B will be unsure of whether the word is the referent of object A or object B. However, if the infant then hears the label again in the presence of object B and object C, the infant can conclude that object B is the referent of the label because object B consistently pairs with the label across different situations.\n\nComputational models have long been used to explore the mechanisms by which language learners process and manipulate linguistic information. Models of this type allow researchers to systematically control important learning variables that are oftentimes difficult to manipulate at all in human participants.\n\nAssociative neural network models of language acquisition are one of the oldest types of cognitive model, using distributed representations and changes in the weights of the connections between the nodes that make up these representations to simulate learning in a manner reminiscent of the plasticity-based neuronal reorganization that forms the basis of human learning and memory. Associative models represent a break with classical cognitive models, characterized by discrete and context-free symbols, in favor of a dynamical systems approach to language better capable of handling temporal considerations.\n\nA precursor to this approach, and one of the first model types to account for the dimension of time in linguistic comprehension and production was Elman's simple recurrent network (SRN). By making use of a feedback network to represent the system's past states, SRNs were able in a word-prediction task to cluster input into self-organized grammatical categories based solely on statistical co-occurrence patterns.\n\nEarly successes such as these paved the way for dynamical systems research into linguistic acquisition, answering many questions about early linguistic development but leaving many others unanswered, such as how these statistically acquired lexemes are represented. Of particular importance in recent research has been the effort to understand the dynamic interaction of learning (e.g. language-based) and learner (e.g. speaker-based) variables in lexical organization and competition in bilinguals. In the ceaseless effort to move toward more psychologically realistic models, many researchers have turned to a subset of associative models, self-organizing maps (SOMs), as established, cognitively plausible models of language development.\n\nSOMs have been helpful to researchers in identifying and investigating the constraints and variables of interest in a number of acquisition processes, and in exploring the consequences of these findings on linguistic and cognitive theories. By identifying working memory as an important constraint both for language learners and for current computational models, researchers have been able to show that manipulation of this variable allows for syntactic bootstrapping, drawing not just categorical but actual content meaning from words' positional co-occurrence in sentences.\n\nSome recent models of language acquisition have centered around methods of Bayesian Inference to account for infants' abilities to appropriately parse streams of speech and acquire word meanings. Models of this type rely heavily on the notion of conditional probability (the probability of A given B), in line with findings concerning infants' use of transitional probabilities of words and syllables to learn words.\n\nModels that make use of these probabilistic methods have been able to merge the previously dichotomous language acquisition perspectives of social theories that emphasize the importance of learning speaker intentions and statistical and associative theories that rely on cross-situational contexts into a single joint-inference problem. This approach has led to important results in explaining acquisition phenomena such as mutual exclusivity, one-trial learning or fast mapping, and the use of social intentions.\n\nWhile these results seem to be robust, studies concerning these models' abilities to handle more complex situations such as multiple referent to single label mapping, multiple label to single referent mapping, and bilingual language acquisition in comparison to associative models' successes in these areas have yet to be explored. Hope remains, though, that these model types may be merged to provide a comprehensive account of language acquisition.\n\nAlong the lines of probabilistic frequencies, the C/V hypothesis basically states all language hearers use consonantal frequencies to distinguish between words (lexical distinctions) in continuous speech strings, in comparison to vowels. Vowels are more pertinent to rhythmic identification. Several follow-up studies revealed this finding, as they showed that vowels are processed independently of their local statistical distribution.\nOther research has shown that the consonant-vowel ratio doesn't influence the sizes of lexicons when comparing distinct languages. In the case of languages with a higher consonant ratio, children may depend more on consonant neighbors than rhyme or vowel frequency.\n\nSome models of language acquisition have been based on adaptive parsing and grammar induction algorithms.\n"}
{"id": "15720083", "url": "https://en.wikipedia.org/wiki?curid=15720083", "title": "Synchrony and diachrony", "text": "Synchrony and diachrony\n\nSynchrony and diachrony are two different and complementary viewpoints in linguistic analysis. A synchronic approach (from Greek συν- \"together\" and χρόνος \"time\") considers a language at a moment in time without taking its history into account. Synchronic linguistics aims at describing a language at a specific point of time, usually the present. By contrast, a diachronic approach (from δια- \"through\" and χρόνος \"time\") considers the development and evolution of a language through history. Historical linguistics is typically a diachronic study.\n\nThe concepts were theorized by the Swiss linguist Ferdinand de Saussure, professor of general linguistics in Geneva from 1896 to 1911, and appeared in writing in his posthumous \"Course in General Linguistics\" published in 1916. In contrast with most of his predecessors, who focused on historical evolution of languages, Saussure emphasized the primacy of synchronic analysis of languages to understand their inner functioning, though never forgetting the importance of complementary diachrony. This dualistic opposition has been carried over into philosophy and sociology, for instance by Roland Barthes and Jean-Paul Sartre. Jacques Lacan also used it for psychoanalysis. Prior to de Saussure, many similar concepts were also developed independently by Polish linguists Jan Baudouin de Courtenay and Mikołaj Kruszewski of the Kazan school, who used the terms statics and dynamics of language.\n"}
{"id": "39590024", "url": "https://en.wikipedia.org/wiki?curid=39590024", "title": "Tautology (language)", "text": "Tautology (language)\n\nIn literary criticism and rhetoric, a tautology is a statement which repeats the same idea, using near-synonymous morphemes, words, or phrases, that is, \"saying the same thing twice\". Tautology and pleonasm are not consistently differentiated in the literature.\n\nLike pleonasm, it is often considered a fault of style when unintentional. On the other hand, an intentional repetition may be an effective way to emphasize a thought, or help the listener or reader understand a point. \n\nSometimes logical tautologies like \"Boys will be boys\" are conflated with language tautologies, but in general, a rhetorical tautology is not inherently true.\n\nThe word was coined in Hellenistic Greek from ταὐτός (\"the same\") plus λόγος (\"word/idea\"), and transmitted through 3rd-century Latin \"tautologia\" and French \"tautologie\". It first appears in English in the 16th century. The use in the term logical tautology was introduced in English by Wittgenstein in 1919, perhaps following Auguste Comte's usage in 1835.\n\n\nIntentional repetition of meaning intends to amplify or emphasize a particular, usually significant, fact about what is being discussed. For example, a gift is, by definition, free of charge; using the phrase \"free gift\" might emphasize that there are no hidden conditions or fine print, be it the expectation of money or reciprocation, or that the gift is being given by volition.\n\nThis is related to the rhetorical device of hendiadys, where one concept is expressed through the use of two descriptive words or phrases. For example, \"goblets and gold\" meaning wealth, or \"this day and age\" meaning the present time. Superficially these expressions may seem tautological, but they are stylistically sound because the repeated meaning is just a way to emphasize the same idea.\n\nThe use of tautologies is, however, usually unintentional. For example, the phrases \"mental telepathy\", \"planned conspiracies\", and \"small dwarfs\" imply that there are such things as \"physical telepathy, spontaneous conspiracies, and giant dwarfs.\")\n\nParallelism is not tautology, but rather a particular stylistic device. Much Old Testament poetry is based on parallelism: the same thing said twice, but in slightly different ways (Fowler puts it as pleonasm). However, modern biblical study emphasizes that there are subtle distinctions and developments between the two lines, such that they are usually not truly the \"same thing.\" Parallelism can be found wherever there is poetry in the Bible: Psalms, the Books of the Prophets, and in other areas as well.\n\n"}
{"id": "34989090", "url": "https://en.wikipedia.org/wiki?curid=34989090", "title": "Termbase", "text": "Termbase\n\nA termbase, or term base (a contraction of terminology and database), is a database consisting of concept-oriented terminological entries (or ‘concepts’) and related information, usually in multilingual format. Entries may include any of the following additional information:\n\nA termbase allows for the systematic management of approved or verified terms and is a powerful tool for promoting consistency in terminology.\n\n\n\n"}
{"id": "2332663", "url": "https://en.wikipedia.org/wiki?curid=2332663", "title": "Tongue thrust", "text": "Tongue thrust\n\nTongue thrust (also called reverse swallow or immature swallow) is the common name of orofacial muscular imbalance, a human behavioral pattern in which the tongue protrudes through the anterior incisors during swallowing, during speech, and while the tongue is at rest. Nearly all infants exhibit a swallowing pattern involving tongue protrusion, but by six months of age most lose this reflex allowing for the ingestion of solid foods.\n\nSince 1958, the term \"tongue thrust\" has been described and discussed in speech and dental publications by many writers. Many school-age children have tongue thrust. For example, according to recent literature, as many as 67–95 percent of children 5–8 years old exhibit tongue thrust, which may be associated with or contributing to an orthodontic or speech problem. Up to the age of four, there is a possibility that the child will outgrow tongue thrust. However, if the tongue thrust swallowing pattern is retained beyond that age, it may be strengthened.\n\nFactors that can contribute to tongue thrusting include macroglossia (enlarged tongue), thumb sucking, large tonsils, hereditary factors, ankyloglossia (tongue tie), and certain types of artificial nipples used in feeding infants, also allergies or nasal congestion can cause the tongue to lie low in the mouth because of breathing obstruction and finally contributing to tongue thrusting. In addition, it is also seen after prolonged therapy by levodopa in Parkinsonism, also it occurs as extra pyramidal side effect (Acute muscular dystonia) after use of Neuroleptics (Anti-Psychotics). \n\nTongue thrust is normal in infants until approximately the age of six months. The loss of this normal infant behavior is one of the signs that the baby is ready to begin eating baby food.\n\nTongue thrusting can adversely affect the teeth and mouth. A person swallows from 1,200 to 2,000 times every 24 hours with about four pounds (1.8 kg) of pressure each time. If a person suffers from tongue thrusting, this continuous pressure tends to force the teeth out of alignment. Many people who tongue thrust have open bites; the force of the tongue against the teeth is an important factor in contributing to \"bad bite\" (malocclusion). Many orthodontists have completed dental treatment with what appeared to be good results, only to discover that the case relapsed because of the patient's tongue thrust. If the tongue is allowed to continue its pushing action against the teeth, it will continue to push the teeth forward and reverse the orthodontic work.\n\nSpeech is not frequently affected by the tongue thrust swallowing pattern. The \"S\" sound (lisping) is the one most affected. The lateral lisp (air forced on the side of the tongue rather than forward) shows dramatic improvement when the tongue thrust is corrected. However, lisping and tongue thrust are not always associated.\n\nTreatment for uncontrolled tongue thrust requires habit retraining in conjunction with a dental appliance.\n\n\n"}
{"id": "40758290", "url": "https://en.wikipedia.org/wiki?curid=40758290", "title": "Truth-apt", "text": "Truth-apt\n\nIn philosophy, to say that a statement is truth-apt is to say that it could be uttered in some context (without its meaning being altered) and would then express a true or false proposition. \n\nTruth-apt sentences are capable of being true or false, unlike questions or commands. Whether paradoxical sentences, prescriptions (especially moral claims), or attitudes are truth-apt is sometimes controversial.\n\n"}
{"id": "10593425", "url": "https://en.wikipedia.org/wiki?curid=10593425", "title": "Truth claim", "text": "Truth claim\n\nA truth claim is a proposition or statement that a particular person or belief system holds to be true. The term is commonly used in philosophy in discussions of logic, metaphysics, and epistemology, particularly when discussing the doctrinal statements of religions; however, it is also used when discussing non-religious ideologies.\n\nA major division of truth claims is that between positive and negative truth claims. Positive truth claims proclaim the existence of an object or entity. Negative truth claims, which are the opposite of truth claims, proclaim the non-existence of an object or entity.\n\nSome religions make strong absolutist truth claims about the laws of nature and human nature while others make relativist or universalist truth claims.\n\nIn the post-modern period Hinduism is generally known to be Universalist and accepts\nall other religions to be true and valid. Mahatma Gandhi is credited to be the original proponent. However, this seems to be true from a socio-political perspective, since Hinduism itself is a congregation of large numbers of individual sects and peoples that have long lived in harmony among themselves without persecuting each other. But if one goes into what is in the scriptures of\nmore organised Hindu sects like Vedanta; we do find exclusive truth claims.\n\nBhagavad Gita says -\n\n\"ye me matamidaM nityamanutishhThanti maanavaaH .\"\n\"shraddhaavanto.anasuuyanto muchyante te.api karmabhiH .. 3.31\"\n\nThose who continuously practice what I preach they will be freed from Karma.\n\n\"ye tvetadabhyasuuyanto naanutishhThanti me matam.h .\"\n\"sarvaGYaanavimuuDhaa.nstaanviddhi nashhTaanachetasaH .. 3.32\"\n\nBut those who, out of envy, disregard these teachings and do not practice them regularly, are to be considered bereft of all knowledge, befooled, and doomed to ignorance and bondage.\n\nA Hindu is expected to examine a truth claim based on his intellect. He can add or improve upon the vast ocean of Hindu philosophy. This is unlike religions such as Christianity and Islam, where, because of the truth claim is endowed as infallible, a contradiction of single verse is a contradiction of the entire claim.\n\nTraditionally Hinduism (more specifically Vedanta) considers itself to be eternal religion (Sanatana Dharma). Fundamental belief is that all beings are divine, that the human condition is one of ignorance to not recognize this divinity inside, and that direct experience of God is achievable for all human beings.\n\nIt has been debated as to whether science makes any truth claims of its own, or if it is a set of methods for evaluating or falsifying other truth claims.\n\nAgnosticism makes the claim that the existence or non-existence of any deity is unknown and possibly unknowable. This is a claim about what we can or do know, not a metaphysical claim as to the nature of the world.\n\nThe truth claims of atheism are divided between negative atheism, which does not accept the positive truth claims of religions, and positive atheism, which specifically claims the non-existence of deities (and other spiritual phenomena).\n\n"}
{"id": "40313", "url": "https://en.wikipedia.org/wiki?curid=40313", "title": "Universal grammar", "text": "Universal grammar\n\nUniversal grammar (UG) in linguistics, is the theory of the genetic component of the language faculty, usually credited to Noam Chomsky. The basic postulate of UG is that a certain set of structural rules are innate to humans, independent of sensory experience. With more linguistic stimuli received in the course of psychological development, children then adopt specific syntactic rules that conform to UG. It is sometimes known as \"mental grammar\", and stands contrasted with other \"grammars\", e.g. prescriptive, descriptive and pedagogical. The advocates of this theory emphasize and partially rely on the poverty of the stimulus (POS) argument and the existence of some universal properties of natural human languages. However, the latter has not been firmly established, as some linguists have argued languages are so diverse that such universality is rare. It is a matter of empirical investigation to determine precisely what properties are universal and what linguistic capacities are innate.\n\nThe theory of universal grammar proposes that if human beings are brought up under normal conditions (not those of extreme sensory deprivation), then they will always develop language with certain properties (e.g., distinguishing nouns from verbs, or distinguishing function words from content words). The theory proposes that there is an innate, genetically determined language faculty that knows these rules, making it easier and faster for children to learn to speak than it otherwise would be. This faculty does not know the vocabulary of any particular language (so words and their meanings must be learned), and there remain several parameters which can vary freely among languages (such as whether adjectives come before or after nouns) which must also be learned.\n\nAs Chomsky puts it, \"Evidently, development of language in the individual must involve three factors: (1) genetic endowment, which sets limits on the attainable languages, thereby making language acquisition possible; (2) external data, converted to the experience that selects one or another language within a narrow range; (3) principles not specific to the Faculty of Language.\"\n\nOccasionally, aspects of universal grammar seem describable in terms of general details regarding cognition. For example, if a predisposition to categorize events and objects as different classes of things is part of human cognition, and directly results in nouns and verbs showing up in all languages, then it could be assumed that rather than this aspect of universal grammar being specific to language, it is more generally a part of human cognition. To distinguish properties of languages that can be traced to other facts regarding cognition from properties of languages that cannot, the abbreviation UG* can be used. UG is the term often used by Chomsky for those aspects of the human brain which cause language to be the way that it is (i.e. are universal grammar in the sense used here) but here for discussion, it is used for those aspects which are furthermore specific to language (thus UG, as Chomsky uses it, is just an abbreviation for universal grammar, but UG* as used here is a subset of universal grammar).\n\nIn the same article, Chomsky casts the theme of a larger research program in terms of the following question: \"How little can be attributed to UG while still accounting for the variety of 'I-languages' attained, relying on third factor principles?\" (I-languages meaning internal languages, the brain states that correspond to knowing how to speak and understand a particular language, and third factor principles meaning (3) in the previous quote).\n\nChomsky has speculated that UG might be extremely simple and abstract, for example only a mechanism for combining symbols in a particular way, which he calls \"merge\". The following quote shows that Chomsky does not use the term \"UG\" in the narrow sense UG* suggested above:\n\n\"The conclusion that merge falls within UG holds whether such recursive generation is unique to FL (faculty of language) or is\nappropriated from other systems.\"\n\nIn other words, merge is seen as part of UG because it causes language to be the way it is, universal, and is not part of the environment or general properties independent of genetics and environment. Merge is part of universal grammar whether it is specific to language, or whether, as Chomsky suggests, it is also used for an example in mathematical thinking.\n\nThe distinction is important because there is a long history of argument about UG*, whereas most people working on language agree that there is universal grammar. Many people assume that Chomsky means UG* when he writes UG (and in some cases he might actually mean UG* [though not in the passage quoted above]).\n\nSome students of universal grammar study a variety of grammars to extract generalizations called linguistic universals, often in the form of \"If X holds true, then Y occurs.\" These have been extended to a variety of traits, such as the phonemes found in languages, the word orders which languages choose, and the reasons why children exhibit certain linguistic behaviors.\n\nLater linguists who have influenced this theory include Chomsky and Richard Montague, developing their version of this theory as they considered issues of the argument from poverty of the stimulus to arise from the constructivist approach to linguistic theory. The application of the idea of universal grammar to the study of second language acquisition (SLA) is represented mainly in the work of McGill linguist Lydia White.\n\nSyntacticians generally hold that there are parametric points of variation between languages, although heated debate occurs over whether UG constraints are essentially universal due to being \"hard-wired\" (Chomsky's principles and parameters approach), a logical consequence of a specific syntactic architecture (the generalized phrase structure approach) or the result of functional constraints on communication (the functionalist approach).\n\nIn an article titled, \"The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?\" Hauser, Chomsky, and Fitch present the three leading hypotheses for how language evolved and brought humans to the point where they have a universal grammar.\n\nThe first hypothesis states that the faculty of language in the broad sense (FLb) is strictly homologous to animal communication.\nThis means that homologous aspects of the faculty of language exist in non-human animals.\n\nThe second hypothesis states that the FLb is a derived, uniquely human, adaptation for language. This hypothesis holds that individual traits were subject to natural selection and came to be specialized for humans.\n\nThe third hypothesis states that only the faculty of language in the narrow sense (FLn) is unique to humans. It holds that while mechanisms of the FLb are present in both human and non-human animals, the computational mechanism of recursion is recently evolved solely in humans. This is the hypothesis which most closely aligns to the typical theory of universal grammar championed by Chomsky.\n\nThe idea of a universal grammar can be traced back to Roger Bacon's observations in his \"Overview of Grammar\" and \"Greek Grammar\" that all languages are built upon a common grammar, even though it may undergo incidental variations; and the 13th century speculative grammarians who, following Bacon, postulated universal rules underlying all grammars. The concept of a universal grammar or language was at the core of the 17th century projects for philosophical languages. There is a Scottish school of universal grammarians from the 18th century, as distinguished from the philosophical language project, which included authors such as James Beattie, Hugh Blair, James Burnett, James Harris, and Adam Smith. The article on grammar in the first edition of the \"Encyclopædia Britannica\" (1771) contains an extensive section titled \"Of Universal Grammar\".\n\nThe idea rose to prominence and influence, in modern linguistics with theories from Chomsky and Montague in the 1950s–1970s, as part of the \"linguistics wars\".\n\nDuring the early 20th century, in contrast, language was usually understood from a behaviourist perspective, suggesting that language acquisition, like any other kind of learning, could be explained by a succession of trials, errors, and rewards for success. In other words, children learned their mother tongue by simple imitation, through listening and repeating what adults said. For example, when a child says \"milk\" and the mother will smile and give her some as a result, the child will find this outcome rewarding, thus enhancing the child's language development.\n\nChomsky argued that the human brain contains a limited set of constraints for organizing language. This implies in turn that all languages have a common structural basis: the set of rules known as \"universal grammar\".\n\nSpeakers proficient in a language know which expressions are acceptable in their language and which are unacceptable. The key puzzle is how speakers come to know these restrictions of their language, since expressions that violate those restrictions are not present in the input, indicated as such. Chomsky argued that this poverty of stimulus means that Skinner's behaviourist perspective cannot explain language acquisition. The absence of negative evidence—evidence that an expression is part of a class of ungrammatical sentences in a given language—is the core of his argument. For example, in English, an interrogative pronoun like \"what\" cannot be related to a predicate within a relative clause:\n\nSuch expressions are not available to language learners: they are, by hypothesis, ungrammatical. Speakers of the local language do not use them, or note them as unacceptable to language learners. Universal grammar offers an explanation for the presence of the poverty of the stimulus, by making certain restrictions into universal characteristics of human languages. Language learners are consequently never tempted to generalize in an illicit fashion.\n\nThe presence of creole languages is sometimes cited as further support for this theory, especially by Bickerton's controversial language bioprogram theory. Creoles are languages that develop and form when disparate societies come together and are forced to devise a new system of communication. The system used by the original speakers is typically an inconsistent mix of vocabulary items, known as a pidgin. As these speakers' children begin to acquire their first language, they use the pidgin input to effectively create their own original language, known as a creole. Unlike pidgins, creoles have native speakers (those with acquisition from early childhood) and make use of a full, systematic grammar.\n\nAccording to Bickerton, the idea of universal grammar is supported by creole languages because certain features are shared by virtually all in the category. For example, their default point of reference in time (expressed by bare verb stems) is not the present moment, but the past. Using pre-verbal auxiliaries, they uniformly express tense, aspect, and mood. Negative concord occurs, but it affects the verbal subject (as opposed to the object, as it does in languages like Spanish). Another similarity among creoles can be seen in the fact that questions are created simply by changing the intonation of a declarative sentence, not its word order or content.\n\nHowever, extensive work by Carla Hudson-Kam and Elissa Newport suggests that creole languages may not support a universal grammar at all. In a series of experiments, Hudson-Kam and Newport looked at how children and adults learn artificial grammars. They found that children tend to ignore minor variations in the input when those variations are infrequent, and reproduce only the most frequent forms. In doing so, they tend to standardize the language that they hear around them. Hudson-Kam and Newport hypothesize that in a pidgin-development situation (and in the real-life situation of a deaf child whose parents are or were disfluent signers), children systematize the language they hear, based on the probability and frequency of forms, and not that which has been suggested on the basis of a universal grammar. Further, it seems to follow that creoles would share features with the languages from which they are derived, and thus look similar in terms of grammar.\n\nMany researchers of universal grammar argue against a concept of relexification, which says that a language replaces its lexicon almost entirely with that of another. This goes against universalist ideas of a universal grammar, which has an innate grammar.\n\nGeoffrey Sampson maintains that universal grammar theories are not falsifiable and are therefore pseudoscientific. He argues that the grammatical \"rules\" linguists posit are simply post-hoc observations about existing languages, rather than predictions about what is possible in a language. Similarly, Jeffrey Elman argues that the unlearnability of languages assumed by universal grammar is based on a too-strict, \"worst-case\" model of grammar, that is not in keeping with any actual grammar. In keeping with these points, James Hurford argues that the postulate of a language acquisition device (LAD) essentially amounts to the trivial claim that languages are learnt by humans, and thus, that the LAD is less a theory than an explanandum looking for theories.\n\nMorten H. Christiansen and Nick Chater have argued that the relatively fast-changing nature of language would prevent the slower-changing genetic structures from ever catching up, undermining the possibility of a genetically hard-wired universal grammar. Instead of an innate universal grammar, they claim, \"apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics\".\n\nHinzen summarizes the most common criticisms of universal grammar:\n\nIn addition, it has been suggested that people learn about probabilistic patterns of word distributions in their language, rather than hard and fast rules (see Distributional hypothesis). For example, children overgeneralize the past tense marker \"ed\" and conjugate irregular verbs incorrectly, producing forms like \"goed\" and \"eated\" and correct these errors over time. It has also been proposed that the poverty of the stimulus problem can be largely avoided, if it is assumed that children employ \"similarity-based generalization\" strategies in language learning, generalizing about the usage of new words from similar words that they already know how to use.\n\nLanguage acquisition researcher Michael Ramscar has suggested that when children erroneously expect an ungrammatical form that then never occurs, the repeated failure of expectation serves as a form of implicit negative feedback that allows them to correct their errors over time such as how children correct grammar generalizations like \"goed\" to \"went\" through repetitive failure. This implies that word learning is a probabilistic, error-driven process, rather than a process of fast mapping, as many nativists assume.\n\nIn the domain of field research, the Pirahã language is claimed to be a counterexample to the basic tenets of universal grammar. This research has been led by Daniel Everett. Among other things, this language is alleged to lack all evidence for recursion, including embedded clauses, as well as quantifiers and colour terms. According to the writings of Everett, the Pirahã showed these linguistic shortcomings not because they were simple-minded, but because their culture—which emphasized concrete matters in the present and also lacked creation myths and traditions of art making—did not necessitate it. Some other linguists have argued, however, that some of these properties have been misanalyzed, and that others are actually expected under current theories of universal grammar. Other linguists have attempted to reassess Pirahã to see if it did indeed use recursion. In a corpus analysis of the Pirahã language, linguists failed to disprove Everett's arguments against universal grammar and the lack of recursion in Pirahã. However, they also stated that there was \"no strong evidence for the lack of recursion either\" and they provided \"suggestive evidence that Pirahã may have sentences with recursive\nstructures\".\n\nDaniel Everett has gone as far as claiming that universal grammar does not exist. In his words, \"universal grammar doesn't seem to work, there doesn't seem to be much evidence for [it]. And what can we put in its place? A complex interplay of factors, of which culture, the values human beings share, plays a major role in structuring the way that we talk and the things that we talk about.\" Michael Tomasello, a developmental psychologist, also supports this claim, arguing that \"although many aspects of human linguistic competence have indeed evolved biologically, specific grammatical principles and constructions have not. And universals in the grammatical structure of different languages have come from more general processes and constraints of human cognition, communication, and vocal-auditory processing, operating during the conventionalization and transmission of the particular grammatical constructions of particular linguistic communities.\"\n\n"}
{"id": "34760855", "url": "https://en.wikipedia.org/wiki?curid=34760855", "title": "Word learning biases", "text": "Word learning biases\n\nWord learning biases are certain biases or assumptions that allow children to quickly rule out unlikely alternatives in order to effectively process and learn word meanings. They begin to manifest themselves around 18 months, when children begin to rapidly expand their vocabulary. These biases are important for children with limited processing abilities if they are to be successful in word learning. The guiding lexical principles have been defined as implicit and explicit strategies towards language acquisition. When a child learns a new word they must decide whether the word refers to the whole object, part of the object, its substance, color, or texture, solving an indeterminacy problem.\n\nOne way in which children constrain the meaning of novel words is through the whole object assumption. When an adult points to an object and says a word, a child assumes this word labels the entire object, not parts or characteristics of the object. For example, if a child is shown an object and given the label \"truck\", the child will assume \"truck\" refers to the entire object instead of the tires, doors, color or other parts. Ellen Markman pioneered work in this field. Her studies suggest that even in cases where color or a dynamic activity are made salient to children, they will still interpret the new word as a label for whole objects.\n\nAccording to cognitive psychologist Elizabeth Spelke, infants' perception of the physical world is guided by three constraints on the behavior of physical objects: objects must move as wholes, objects move independently of each other, and objects move on connected paths. These three constraints help guide children's interpretations of scenes, and, in turn, explains how the whole object bias reflects the non-linguistic status of objects.\n\nIt is often questioned if the word-learning constraints are specific to the domain of language, or if they apply to other cognitive domains. As for the whole object assumption, evidence suggests that the idea of objects is more advantaged than characteristics or relations. Children assume a label refers to a whole object because the object is more salient than its properties. The whole object assumption may reflect non-linguistic levels of an object and exploits the cognitive tendency to analyze the world through a whole object lens, meaning the whole object assumption can be applied to cognitive domains outside of language.\n\nOne popular criticism is that evidence is only provided for children 18 months and older. A more recent study strengthened the breadth of ages and stimuli conditions under which this bias occurs. As early as 12 months, infants can associate words with whole objects when the objects can be viewed as two separate objects and even when one of the parts is made salient. Another criticism is the claim that a restricted set of stimuli has been used that possibly favors a \"whole\" interpretation. To counter this claim, the whole object assumption has been tested with adults as well. Even when participants, 18–36 years of age, were instructed that they would be tested more frequently for parts, they were better able to recognize the whole objects rather than parts. These findings support the hypothesis that there is an assumption to encode the overall shape of the stimuli in working memory rather than individual details.\n\nAfter a child constrains a novel word to label a whole object, the child must learn how to apply the label to similar objects. Ordinarily, children focus on thematic relations between objects when categorizing. For example, if given soup, children will group it together with a bowl and a spoon. Those items would be thematically related. However, when children are given a new label they shift their attention to taxonomic relationships. What this means for the previous example is instead of soup being related to a bowl or spoon, children relate it to ice cream or pudding. The new label is assumed to refer to other objects within the same taxonomic category.\n\nEllen Markman's early studies showed this constraint at work. When two- and three-year-olds were presented with two basic-level objects, two different kinds of dogs, and a third thematically related object, dog food, they showed a tendency to select a dog and dog food; however, if one of the dogs was labeled with an unfamiliar word, the children were more likely to select the two dogs. Another study conducted by Backscheider & Markman attempted to clarify whether this assumption was powerful enough to overcome the preference for thematic relations when objects are engaged in dynamic thematic relations at the time of labeling. A doll was repeatedly seated in a chair when the child either heard \"see the bif\" or \"see this\". The label, \"bif\", caused children to pick objects of the same kind, whereas, the absence of the label caused them to organize objects to the thematic event they had witnessed. Children use this assumption as early as 18 months of age.\n\nSimilar to the taxonomic constraint researchers have looked into the principle of categorical scope, which also follows the assumption that children will believe new object labels refer to objects within taxonomic categories. An example of categorical scope and perceptual similarity can be illustrated when children learn animal names. Studies show that children think the identity of an animal only changes if its internal properties change. Children extended labels to two perceptually similar animals more often than when they were dissimilar.\n\nThe taxonomic assumption is very clearly applicable to cognitive domains outside of language. One obvious domain is children's inductive reasoning. An example of this assumption at work in this domain would be for a child to know that Edgar is a grandfather, and Edgar is bald, so they assume all grandfathers are bald. While there are domains that taxonomic assumptions are seen, there are also clear cognitive domains where these assumptions are avoided, such as identifying causality or classical conditioning. Identifying causality obviously does not utilize the taxonomic assumption. For example, learning that you are allergic to dogs means you realize being around dogs causes your allergies to flare up. However, you do not relate this causality to taxonomic associations and claim that you are also allergic to cats. Another domain where taxonomic associations are not made is in classical conditioning. The popular example is Pavlov's dog. The dog learns to associate the bell with salivating after the dog has associated the bell with food. While these things are thematically related, they are not considered to be associated in a taxonomic fashion.\n\nThere aren't as many criticisms on the taxonomic assumption as there are for other word learning assumptions. However, Nelson (1988) argues against the taxonomic assumption because children aren't responding to tests 100% accurately 100% of the time. The concept of have perfect accuracy every time with every participant isn't something found in most research, but Nelson claims this assumption might not be biological.\n\nAnother critique of the taxonomic assumption is that it extends past words thus should not be considered a word learning bias. In 1990, Premack conducted a taxonomic assumption experiment with chimpanzees who were being taught words and those who were not. Premack found similar results of what studies using children found—chimpanzees learning language used the taxonomic assumption. Premack claimed these chimps did not have an idea of real words since they were in the beginning of the word learning process thus making the assumption a nonlinguistic assumption. Others criticize Premack by saying this assumption can fit language but doesn't stop at language which is where the domain specificity comes in.\n\nThe whole object assumption leads children to constraining labels to an entire object, but children must also learn labels for characteristics or parts of an object. To override the whole object assumption, children also utilize the mutual exclusivity assumption. Simply put, the mutual exclusivity assumption suggests that every object only has one name. Children resist assigning a label to an object for which they already have a name or at least will not learn the new name as easily. Children are then able to start considering other possibilities for the new label, for instance, a part of the object. For example, an adult presents a child with two objects, a truck and a novel object. The adult asks the child to pick up the blicket. If the child already knows \"truck\" but has not heard \"blicket\" as a label for an object, the child will assume this label maps onto the novel object.\n\nMarkman and Wachtel's 1988 studies demonstrated the learning process through the whole-object and mutual exclusivity assumption. The experimenter told three-year-old children a word and then showed them a picture. She asked whether the label referred to the whole object or a part and outlined each option with her finger. When the whole object was unfamiliar they pointed to the part in only 20% of the trials, but pointed to the part in 57% of the trials when the object was familiar. A recent study attempted to replicate and extend these results. Hansen and Markman taught children a new word for a part of a real object by saying the word and tracing the object's contours. (These gestures were meant to remain as naturalistic as possible). They then asked children to point to the new part in order to identify if they have linked the new name to the intended part. The main manipulation was whether the object was familiar or not. Upholding the mutual exclusivity assumption children pointed to the intended part more often in the familiar object condition. Furthermore, the gesture of pointing/outlining the part itself was insufficient for children to learn the part name. Mutual exclusivity and a gesture were necessary for children to select the novel part.\n\nOther researchers have come up with similar principles. Clark's contrast theory holds that \"every two forms contrast in meaning\". When a new word is presented the child assumes it refers to something that does not yet have a label, but contrast does not take into account the overlap words may have in meaning. Golinkoff's novel name-nameless category (N3C) also states that a child will map a new name to the unnamed object when a named object is present. Unlike contrast, N3C does not require children to understand synonymy, and unlike mutual exclusivity it does not hold that objects have only one name. Furthermore, this principle is not one of the first assumptions child learn mainly because, at this point, children are not dependent on an explicit link between the new word and the object (i.e. by pointing). By acquiring this principle the indirect link of seeing an unnamed object while hearing a new word is enough for the child to map the new label to the unnamed object.\n\nUnlike the other two constraints, mutual exclusivity is easily identified as domain-general. The mutual exclusivity assumption is the one of the three constraints that is easily generalized to other cognitive domains. Within the domain of language but outside of word-learning, mutual exclusivity is applied to the one-to-one mapping principle of language acquisition as well as the acquisition of syntax. While it is commonly applied throughout the domain of language, mutual exclusivity seems to be a domain-general principle used in analyzing many new domains of knowledge.\n\nThe mutual exclusivity assumption is disputed when children learn objects can have more than one name. For example, a dog can be a \"dog\" and an \"animal\" and named \"Spot.\" According to the mutual exclusivity assumption, one would assume children would not accept that the dog could be three different names. However, children tend to accept the differences. Merriman (1986) found that children who were introduced to both or all names initially accepted one object having multiple names. Markman and Wachtel (1988) hypothesize that children preference the taxonomic constraint when it interferes with the mutual exclusivity assumption. Merriman and Bowman (1989) found that when children have a specific name for an object, they'd use that name if the object was atypical. The example they use is if a unique car was a \"bave,\" children would not call it a \"car.\" Just like any of the assumptions, it's hard if not impossible to tell where one assumption starts and another stops.\n\nThe noun-category bias suggests that children learn nouns more quickly than any other syntactic category. It has been found to appear in young children as early as the age of two and is used to help \nchildren differentiate between syntactic categories such as nouns and adjectives. Preschool-age children have been found to be inclined to interpret words from just one linguistic category- nouns. Gentner proposes that this might be due to the fact that nouns represent a more concrete object.\n\nThe noun-category bias places regulations on the possible interpretations that a child might attach to a newly encountered noun. Experiments from Waxman and Gelman as well as Markman and Hutchinson provide results which support the claim that children show preference for categorical relations over random hypothesizing when learning new nouns. This suggests a correlation between language and thought and provides evidence for the theory that syntax and semantics are related. Kauschke and Hofmeister divide the noun-category bias into four separate components: (1) nouns are acquired\nearlier than verbs and other word classes; (2) nouns form the majority of children’s early\nvocabularies; (3) nouns in children’s early vocabulary are predominantly object labels; (4) a preference\nfor nouns promotes further language development.\n\nResearch has found that a noun bias exists in at least English, French, Dutch, German, Spanish, Hebrew, and Japanese. However, conflicting data from Korean, Mandarin, and Turkish leads researchers\nto believe that the noun-category bias may be language dependent. Dhillon claims that whether or not a language displays a noun-category bias depends on a language's null subject parameter \n\nThe shape bias proposes that children apply names to same-shaped objects. This stems from the idea that children are associative learners that have abstract category knowledge at many different \nlevels. They should be able to identify specifics of each category (e.g. pickles are round, long, green, and bumpy). This knowledge aids children in categorizing newly encountered objects. The shape bias is a widely contested area of study in psycholinguistics. As of now, identically performed experiments have provided evidence that can be used to argue either for or against the shape bias. The argument is, essentially, whether or not there is a shift in language learning from perceptual to conceptual.\n\nPerception plays a part in child development, however, it is a matter of to what extent. According to the shape bias, children would choose same-shaped object no matter which category they\nbelonged to. For example, the word cat would mean all things that are cat-shaped. The juxtaposition to this is that children refer to kinds of objects which share unforeseen properties and perceptual features. For example, the word cat would refer to the idea of cats which share a same basic, though not exact, shape and often behave similarly.\n\nCimpian and Markman argue for this view. Their \nresearch found that children were less likely to extend a shape bias when other alternative methods of categorization were offered. However, Smith and Samuelson argue that Cimpian and Markman tested only already known lexical categories which negates the effects of simulating word learning. In addition. they argue that the shape bias is not to be considered as the exclusive tool used in word learning, only that it aids the process.\n"}
