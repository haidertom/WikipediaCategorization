{"id": "51313643", "url": "https://en.wikipedia.org/wiki?curid=51313643", "title": "Agbirigba", "text": "Agbirigba\n\nAgbirigba is a cant (or argot) based on the Ogbakiri dialect of the Nigerian language Ikwerre of Port Harcourt. There are about thirty speakers, from a persecuted section of the community.\n\nAgbirigba is unintelligible to other speakers of Ikwerre, but the rule for its derivation is simple: the consonant \"t\" is added before every CV syllable (or, more accurately, every CV mora). Some speakers add an epenthetic vowel to break up the resulting consonant cluster.\n\nThe addition of the \"t\" results in consonant clusters that do not occur in Ikwerre or other local languages. Some speakers pronounce Agbirigba with the resulting clusters. For speakers to break them up with vowels, the vowels are all high (one of the four vowels ), and match the subsequent vowel in ATR, backness, nasality and tone. \n\nAn NCV sequence becomes NtCV. For example, \"m̀fù\" 'horn' becomes \"ǹtfù\" or \"ǹtùfù\". \n\nThere are some complications to this: if the following vowel is /a/, with no ATR quality for the epenthetic vowel to match, then the epenthetic vowel will be /i/ or /u/ depending on, apparently, whether the following consonant is coronal or velar, and if the tone of the following syllable (whether CV or CVV) is complex (rising or falling), then the first element of that tone will move to the epenthetic vowel.\n\n\nbecomes:\nor \n\nHere all the vowels are ATR, so the epenthetic vowels are /i/ or /u/ depending on whether the following vowel is front (/e/ or /i/) or back (/o/ or /u/). There is no t before the word-initial syllabic nasal in \"Nkechi\" or word-initial vowel in \"iji\", as neither is a CV syllable.\n\nbecomes:\nor \n\nHere we have some RTR and nasal vowels. With the high–low tone on \"lêm\", the high element shifts to the epenthetic vowel for \"tílèm\".\n"}
{"id": "30307086", "url": "https://en.wikipedia.org/wiki?curid=30307086", "title": "Analogical change", "text": "Analogical change\n\nIn language, an analogical change is the process of inventing a new element in conformity with some part of the language system that you already know. For instance, child learns pairs like dog/dogs, cat/cats and is then able to form other plurals. The way in which analogy can lead to a change is seen when the child learns words like man and mouse, and forms the analogical plurals mans and mouses (instead of men and mice).\n"}
{"id": "52715832", "url": "https://en.wikipedia.org/wiki?curid=52715832", "title": "Anglophone problem (Cameroon)", "text": "Anglophone problem (Cameroon)\n\nThe Anglophone Problem, as it is commonly referred to in Cameroon, is a socio-political issue rooted in Cameroon's colonial legacies from the Germans, British, and the French.\n\nThe issue classically and principally opposes many Cameroonians from the Northwest and Southwest regions, many of whom consider themselves anglophones, to the Cameroon government. This is based on the fact that these two regions (formally British Southern Cameroons) were controlled by Britain as a mandated and trust territory of the League of Nations and the United Nations respectively.\" \n\nWhile many Northwesterners and Southwesterners believe there is an anglophone problem, some do not. In fact, the term \"anglophone\" today creates a lot of controversy, as many former French-speaking Cameroonians who are either bilingual or speak only English (most of whom have gone through the English sub-system of education) consider themselves as anglophones. The root of the Anglophone problem in Cameroon can be traced back to the Foumban Conference of 1961 that united the two territories, with different colonial legacies, into one state. The Anglophone Problem is increasingly dominating the political agenda of Cameroon. This problem has led to arguments and actions (protests, strikes, etc.) that argue for federalism or separation from the union by the Anglophones. Failure to address the Anglophone Problem threatens Cameroon's ability to create national unity between the two groups of people.\n\nThe roots of the Anglophone problem can be traced back to World War I, when Cameroon was known as German Kamerun. The German Empire first gained influence in Cameroon in 1845 when Alfred Saker of the Baptist Missionary Society introduced a mission station. In 1860, German merchants established a factory: the Woermann Company. On July 5, 1884, local tribes provided the Woermann Company with rights to control the Kamerun River, consequently setting the foundation for the later German colonization of Kamerun. In 1916, during World War I, France and Britain joined forces to attack and seize German Kamerun. Later, the Treaty of Versailles would award France and Britain mandates over Cameroon as punishment of the Germans who lost the war. Most of German Kamerun was given to the French, over 167,000 square miles of territory. The British were given Northern Cameroons, about 17,500 square miles of territory and Southern Cameroons, 16,580 square miles. Each colonizer would later influence the colonies with their European languages and cultures, thus rendering them as Anglophones and Francophones. The large difference in awarded territory has resulted in present-day Cameroon having a huge majority Francophone population and a very small minority Anglophone population.\n\nFollowing World War II, a wave of independence flowed rapidly throughout Africa. The United Nations obliged that Britain and France relinquish their colonies and guide them towards independence. There were three political options for British Southern Cameroons. They could become independent by uniting with Nigeria or with French Cameroun. No option of self-determination by becoming independent was given. The most desired option was independence with the least popular being unification with French Cameroun. However, during the British Plebiscite of 1961, the British argued that Southern Cameroons was not economically viable enough to sustain itself as an independent nation and could only survive by joining with Nigeria or La République du Cameroun (the Republic of Cameroon). Though documents on the United Nations' \"Non-Self-Governing Territories\" state, \"integration should be the result of the freely expressed wishes of the territory's peoples\", the United Nations would later reject Southern Cameroons' appeal to have independence as a sovereign nation placed on the ballot. The plebiscite questions were: \nThe United Nations documents defined the basis of integration as: \"Integration with an independent State should be on the basis of complete equality between the peoples of the erstwhile Non-Self-Governing Territory and those of the independent country with which it is integrated. The peoples of both territories should have equal status and rights of citizenship... at all levels in the executive, legislative and judicial organs of government.\" With this promise in mind, on February 1961, British Northern Cameroons voted to join Nigeria, while British Southern Cameroons voted to join La République du Cameroun.\n\nThe purpose of the Foumban Constitutional Conference was to create a constitution for the new Federal state of British Southern Cameroon and La République du Cameroun. The conference brought together representatives from La République du Cameroun, including Amadou Ahidjo, their president, with representatives from Southern Cameroons. Two weeks before the Foumban Conference, there were reports that more than one hundred people were killed by terrorists in Loum, Bafang, Ndom, and Douala. The reports worried unification advocates who wanted British Cameroon to unify with French Cameroun. For the conference, the location of Foumban had been carefully chosen to make Ahidjo, appear as if he had everything under control. Mr. Mbile, a Southern Cameroonian representative at the conference noted, \"Free from all the unrest that had scared Southern Cameroonians, the Francophone authorities had picked the place deliberately for the occasion. The entire town had been exquisitely cleaned up and houses splashed with whitewash. Food was good and receptions lavish. The climate in Foumban real or artificial went far to convince us that despite the stories of 'murder and fire,' there could be at least this island of peace, east of the Mungo.\"\n\nBefore the Foumban Conference, all the parties in Southern Cameroons, the Native Authority Councils and the traditional leaders attended the Bamenda Conference. This conference decided on a common proposal to present when negotiations with La République du Cameroun arrived. Among many things, the Bamenda Conference agreed on a non-centralized federation to ensure there was a distinction between the powers of the states and the powers of the federation. Most of the proposals from the Bamenda Conference were ignored by Ahidjo. Some of these proposals included having a bicameral legislature and decentralizing power, but instead a unicameral system was established with a centralized system of power.\n\nAt the Foumban conference, Ahidjo presented delegates with a draft constitution. By the end of the conference, instead of creating an entirely new constitution, the contributions of the Southern Cameroons delegates were reflected in suggestions made to the draft initially presented to them. John Ngu Foncha and Ahidjo intended for the Foumban Constitutional Conference to be brief, however delegates left the three day conference with the impression that there would be sequential conferences to continue the drafting of the constitution. Mbile later noted, \"We may have done more if we had spent five months instead of five days in writing our constitution at Foumban.\" The Constitution for the new Federal Republic was agreed in Yaoundé in August 1961, between Ahidjo and Foncha, pending approval by the House of Assembly of the two states. In the end, the West Cameroon House of Assembly never ratified the Constitution. However, on October 1, 1961, the Federal Republic of Cameroon nevertheless came to fruition.\n\nOn May 6, 1972, Ahidjo announces his decision to convert the Federal Republic into a unitary state, on the provision that the idea was supported via referendum. This suggestion violated the articles in the Foumban document that read: 'any proposal for the revision of the present constitution, which impairs the unity and integrity of the Federation shall be inadmissible,' and 'proposals for revision shall be adopted by simple majority vote of the members of the Federal Assembly, provided that such majority includes a majority of the representatives ... of each of the Federated States,'... not through referendum. Such violations easily allowed for the passing of the referendum that turned the Federal Republic into the United Republic of Cameroon. Taking into account these actions, the evidence shows that the Francophone's intentions may have not been to form a federal state, but rather to annex Southern Cameroons and not treat them as equals. In 1984, Ahidjo's successor, Paul Biya, replaced the name \"United Republic of Cameroon\" with \"La République du Cameroun,\" the same name the francophone Cameroon had before federation talks. With changes in the Constitution of 1996, reference to the existence of a territory called the British Southern Cameroons that had a \"functioning self-government and recognized international boundaries\" was essentially erased.\n\nDespite the non-acknowledgement/denial of the Anglophone problem from Francophone government leaders, there exists a discontent by Anglophones, both young and old, as to how Anglophones are treated. This discontent presents itself in calls for federation or separation with movements that are garnering strength. At the core of Anglophone grievances is the loss of the former West Cameroons as a \"distinct community defined by differences in official language and inherited colonial traditions of education, law, and public administration.\" On 22 December 2016, in a letter to Paul Biya, the Anglophone Archbishops of Southern Cameroons define the Anglophone problem as follows:\n\nMovements which advocate the separation of English-speaking Cameroon from French-speaking Cameroun exist, led by the Cameroon Action Group, the Southern Cameroons Youth League, the Southern Cameroons National Council, the Southern Cameroon Peoples Organization and the Ambazonia Movement.\n\nAdvocates of Federation want a return to the constitution agreed upon in the 1961 Foumban Conference that acknowledges the history and culture of the two regions while giving equal power to the two. This federation had been dismantled on 20 May 1972 by the larger French-speaking Cameroon and extended the latter's executive power throughout West Cameroon. Federation advocates include the instrumental Consortium of the leaders of three Cameroon-based trade unions: Lawyers, Teachers, and Transporters. It also includes some Cameroonians in the diaspora led by a well organized US-based Anglophone Action Group, Inc. (AAG). AAG was one of the first groups in the diaspora to endorse the Cameroon-based Consortium as a peaceful alternative to achieving a return to the pre-1972 federated system. Opponents of federation include the ruling Cameroon Peoples Democratic Movement.\n\nUnitarianism do not want Federation or Separation, but rather a decentralized unitary government; whereas, now the government is highly centralized in power. This violates the tenets of the 1996 Constitution as decentralization has yet to be implemented.\n\nIn March 1990, the Social Democratic Front (SDF) led by John Fru Ndi, was founded on the perception of widespread Anglophone alienation. The SDF was the first major opposition party to the People's Democratic Movement, led by Paul Biya.\n\nBelow are various reasons that Anglophones feel marginalized, systemically, by the government. \n\n\n, the Anglophone problem is still on-going. It has spiraled into violence with police officers and gendarmes shooting dead several civilians. Official sources have put the number at 17 dead, but local individuals and groups have talked of 50 or more. Radical members of some secessionist groups have killed several police officers and gendarmes. 15,000 refugees have fled Southern Cameroons into neighboring Nigeria, with the UNHCR expecting that number to grow to 40,000 if the situation continues.\n\nWithout clearly acknowledging the existence of the Anglophone problem, the President of Cameroon has attempted to appease tensions by making a number of announcements:\nSeveral separatist or secessionist groups have emerged or become more prominent as a result of the harsh response by the government to the Anglophone problem. These groups desire to see Southern Cameroons completely separate from \"La République du Cameroun\" and form its own state, sometimes referred to as \"Ambazonia\". Some groups such as the \"Southern Cameroon Ambazonia United Front\" (SCACUF) are using diplomatic means in an attempt to gain independence for the Anglophone regions, whereas other groups have begun to employ armed confrontation with artisan weapons against the deployed gendarmes and soldiers in those regions.\n\n"}
{"id": "44875149", "url": "https://en.wikipedia.org/wiki?curid=44875149", "title": "Association of Translation Companies", "text": "Association of Translation Companies\n\nThe Association of Translation Companies (ATC) is a professional membership association promoting language services in the United Kingdom and beyond. The ATC represents the interests of translation companies operating in the UK’s expanding language services industry which is home to over 1,500 translation companies, is worth more than £1 billion and employs more than 12, 000 people.\n\nThe ATC’s stated vision is to create recognition and trust for stakeholders by promoting and regulating quality-driven language industry standards and best practice, and to support and nurture its members for sustainability and growth.\n\nThe Association of Translation Companies Ltd is governed by a volunteer Council elected from ATC member companies. The association’s direction is set by the Council, led by its elected Officers: Chair, Vice Chair and Treasurer. Responsible for implementing the Council’s vision and strategy is the ATC’s Chief Executive Officer who heads the association’s activities supported by its Secretariat.\n\nIn addition to convening a minimum of four times a year, Council members actively participate in the association’s committees and lead its projects.\n\nThe ATC’s promotes quality-driven professional language services for the benefit of its member companies. The association’s activities serve the needs of the people, businesses and public-sector organisations commissioning translation, interpreting and other language services.\n\nThe Association provides specific information to translation buyers about members’ specialities; speaks with authority to Government on matters concerning the profession; provides information and assistance to business, industry and members of the public who wish to use the services of a language service provider.\n\nIts objectives are to:\n\n\nThe Association of Translation Companies was formed in 1976 by leading British translation companies keen to promote the use of professionally produced, high quality, translation work by professional translation companies, to support the UK’s exporters.\n\nSince its inception, the ATC has played a pivotal role in the development of European language services. It was the driving force behind the establishment of the European Union of Associations of Translation Companies, and one of the original developers of the European translation services standard EN 15038, a precursor to the international translation services standard ISO 17100.\n\nMembership is available to established companies able to provide full accounts of the previous three trading years. A minimum of 65% of the company’s core business must be the provision of translation services.\n\nMembership of the ATC as a Supported Start-up is available to start-up Language Service Provider businesses that have been trading for over six months who are able to provide the supporting documents in the ATC’s Eligibility and Criteria section.\n\nMembership is available to translation departments{dn|date=November 2018}} within companies whose core business may be in any sector.\n\nAccredited Partner status is available to those companies who are stakeholders in the translation industry, for example office furniture suppliers.\n\nTechnology Partner status is available to those companies supplying hardware or software specifically tailored to the translation business.\n\nThe ATC organises regular networking and training events and webinars, and hosts an annual conference, the Language Industry Summit. The association publishes an annual Language Industry Survey, charting the state of the language services industry in the UK.\n\nThe ATC speaks with authority on the economic case of languages for the future of the UK’s international trade and exporting and supports several research activities and government lobbying initiatives.\n\nFull ATC member companies have the use of the ATC Certification Stamp, which maybe be used to certify official translations such as certificates. The stamp carries the member’s name, membership number, the current year, and the stamp line “The mark of quality-managed translation service”.\n\nThe ATC’s commercial ISO Certification Service provides auditing and certification services and training on language service and quality management ISO standards to ATC members and translation companies worldwide.\n\n\n"}
{"id": "12328438", "url": "https://en.wikipedia.org/wiki?curid=12328438", "title": "Auditory processing disorder", "text": "Auditory processing disorder\n\nAuditory processing disorder (APD), also known as central auditory processing disorder (CAPD), is an umbrella term for a variety of disorders that affect the way the brain processes auditory information. Individuals with APD usually have normal structure and function of the outer, middle, and inner ear (peripheral hearing). However, they cannot process the information they hear in the same way as others do, which leads to difficulties in recognizing and interpreting sounds, especially the sounds composing speech. It is thought that these difficulties arise from dysfunction in the central nervous system.\n\nThe American Academy of Audiology notes that APD is diagnosed by difficulties in one or more auditory processes known to reflect the function of the central auditory nervous system.\n\nAPD can affect both children and adults, although the actual prevalence is currently unknown. It has been suggested that males are twice as likely to be affected by the disorder as females, but there are no good epidemiological studies.\n\nCAPD can continue into adulthood. Cooper and Gates (1991) estimated the prevalence of adult APD to be 10 to 20%. Many people experience problems with learning and day-to-day tasks with difficulties over time. Adults with this disorder\n\nIt has been discovered that APD and ADHD present overlapping symptoms. Below is a ranked order of behavioral symptoms that are most frequently observed in each disorder. Professionals evaluated the overlap of symptoms between the two disorders. The order below is of symptoms that are almost always observed. This chart proves that although the symptoms listed are different, it is easy to get confused between many of them.\n\nThere is a high rate of co-occurrence between AD/HD and CAPD. An article published in 1994 showed that 84% of children with APD have confirmed or suspected ADHD. Co-occurrence between ADHD and APD is 41% for children with confirmed diagnosis of ADHD, and 43% for children suspected of having ADHD. \"more recently published data is needed to support or refute this statement.\" \n\nThere has been considerable debate over the relationship between APD and Specific language impairment (SLI).\n\nSLI is diagnosed when a child has difficulties with understanding or producing spoken language for no obvious cause. The problems cannot be explained in terms of peripheral hearing loss. The child is typically late in starting to talk, and may have problems in producing speech sounds clearly, and in producing or understanding complex sentences. Some theoretical accounts of SLI regard it as the result of auditory processing problems. However, this view of SLI is not universally accepted, and others regard the main difficulties in SLI as stemming from problems with higher-level aspects of language processing. Where a child has both auditory and language problems, it can be hard to sort out cause-and-effect.\n\nSimilarly with developmental dyslexia, there has been considerable interest in the idea that for some children reading problems are downstream consequences of difficulties in rapid auditory processing. Again, cause and effect can be hard to unravel. This is one reason why experts such as Moore have recommended using non-verbal auditory tests to diagnose APD. Specifically regarding the neurological factors of dyslexia, the disorder has been linked to polymicrogyria which causes cell migrational problems. This relates to APD because children that have polymicrogyri almost always present deficits on APD testing.\n\nIt has also been suggested that APD may be related to cluttering, a fluency disorder marked by word and phrase repetitions.\n\nIt has been found that a higher than expected proportion of individuals diagnosed with SLI and dyslexia on the basis of language and reading tests also perform poorly on tests in which APD is tested. APD can be assessed using tests that involve identifying, repeating or discriminating speech, and a child may do poorly because of primary language problems. In a study comparing children with a diagnosis of dyslexia and those with a diagnosis of APD, they found the two groups could not be distinguished. obtained similar findings in studies comparing children diagnosed with SLI or APD. The two groups had very similar profiles. This raises the worrying possibility that the diagnosis that a child receives may be largely a function of the specialist they see: the same child who would be diagnosed with APD by an audiologist may be diagnosed with SLI by a speech-language therapist or with dyslexia by a psychologist.\n\nAcquired APD can be caused by any damage to or dysfunction of the central auditory nervous system and can cause auditory processing problems. For an overview of neurological aspects of APD, see Griffiths.\n\nThe ability to listen to and comprehend multiple messages at the same time is a trait that is heavily influenced by our genes say federal researchers. These \"short circuits in the wiring\" sometimes run in families or result from a difficult birth, just like any learning disability. Auditory processing disorder can be associated with conditions affected by genetic traits, such as various developmental disorders. Inheritance of Auditory Processing Disorder refers to whether the condition is inherited from your parents or \"runs\" in families. Central auditory processing disorder may be hereditary neurological traits from the mother or the father.\n\nIn the majority of cases of developmental APD, the cause is unknown. An exception is acquired epileptic aphasia or Landau-Kleffner syndrome, where a child's development regresses, with language comprehension severely affected. The child is often thought to be deaf, but normal peripheral hearing is found. In other cases, suspected or known causes of APD in children include delay in myelin maturation, ectopic (misplaced) cells in the auditory cortical areas, or genetic predisposition. In a family with autosomal dominant epilepsy, seizures which affected the left temporal lobe seemed to cause problems with auditory processing. In another extended family with a high rate of APD, genetic analysis showed a haplotype in chromosome 12 that fully co-segregated with language impairment.\n\nHearing begins in utero, but the central auditory system continues to develop for at least the first decade. There is considerable interest in the idea that disruption to hearing during a sensitive period may have long-term consequences for auditory development. One study showed thalamocortical connectivity in vitro was associated with a time sensitive developmental window and required a specific cell adhesion molecule (lcam5) for proper brain plasticity to occur. This points to connectivity between the thalamus and cortex shortly after being able to hear (in vitro) as at least one critical period for auditory processing. Another study showed that rats reared in a single tone environment during critical periods of development had permanently impaired auditory processing. ‘Bad’ auditory experiences, such as temporary deafness by cochlear removal in rats leads to neuron shrinkage. In a study looking at attention in APD patients, children with one ear blocked developed a strong right-ear advantage but were not able to modulate that advantage during directed-attention tasks.\n\nIn the 1980s and 1990s, there was considerable interest in the role of chronic Otitis media (middle ear disease or 'glue ear') in causing APD and related language and literacy problems. Otitis media with effusion is a very common childhood disease that causes a fluctuating conductive hearing loss, and there was concern this may disrupt auditory development if it occurred during a sensitive period. Consistent with this, in a sample of young children with chronic ear infections recruited from a hospital otolargyngology department, increased rates of auditory difficulties were found later in childhood. However, this kind of study will suffer from sampling bias because children with otitis media will be more likely to be referred to hospital departments if they are experiencing developmental difficulties. Compared with hospital studies, epidemiological studies, which assesses a whole population for otitis media and then evaluate outcomes, have found much weaker evidence for long-term impacts of otitis media on language outcomes.\n\nQuestionnaires can be used for the identification persons with possible auditory processing disorders, as these address common problems of listening. They can help in the decision for pursuing clinical evaluation. One of the most common listening problems is speech recognition in the presence of background noise. According to the respondents who participated in a study by Neijenhuis, de Wit, and Luinge (2017), the following symptoms are characteristic in children with listening difficulties, and they are typically problematic with adolescents and adults. They include:\n\n\nAccording to the New Zealand Guidelines on Auditory Processing Disorders (2017) a checklist of key symptoms of APD or comorbidities that can be used to identify individuals who should be referred for audiological and APD assessment includes, among others:\n\n\nFinally, the New Zealand guidelines state that behavioral checklists and questionnaires should only be used to provide guidance for referrals, for information gathering (for example, prior to assessment or as outcome measures for interventions), and as measures to describe the functional impact of auditory processing disorder.  They are not designed for the purpose of diagnosing auditory processing disorders. The New Zealand guidelines indicate that a number of questionnaires have been developed to identify children who might benefit from evaluation of their problems in listening.  Examples of available questionnaires include the Fisher’s Auditory Problems Checklist (1976), the Children’s Auditory Performance Scale (1998), the Screening Instrument for Targeting Educational Risk (1989), and the Auditory Processing Domains Questionnaire (O’Haraa and Mealings, 2017) among others. All of the previous questionnaires were designed for children and none are useful for adolescents and adults.   \n\nThe University of Cincinnati Auditory Processing Inventory (UCAPI) (Keith, Tektas and Ramsey, 2018) was designed for use with adolescents and adults seeking testing for evaluation of problems with listening and/or to be used following diagnosis of an auditory processing disorder to determine the subject’s status.  Following a model described by Zoppo et al. (2015) a 34-item questionnaire was developed that investigates auditory processing abilities in each of the six common areas of complaint in APD (listening and concentration, understanding speech, following spoken instructions, attention, and other.)  The final questionnaire was standardized on normally achieving young adults ranging from 18 to 27 years of age.  Validation data was acquired from subjects with language-learning or auditory processing disorders who were either self-reported or confirmed by diagnostic testing.  A UCAPI  total score is calculated by combining the totals from the six listening conditions and provides an overall value to categorize listening abilities. Additionally, analysis of the scores from the six listening conditions provides an auditory profile for the subject. Each listening condition can then be utilized by the professional in making recommendation for diagnosing problem of learning through listening and treatment decisions.  The UCAPI provides information on listening problems in various populations that can aid examiners in making recommendations for assessment and management.\n\nAPD is a difficult disorder to detect and diagnose. The subjective symptoms that lead to an evaluation for APD include an intermittent inability to process verbal information, leading the person to guess to fill in the processing gaps. There may also be disproportionate problems with decoding speech in noisy environments.\n\nAPD has been defined anatomically in terms of the integrity of the auditory areas of the nervous system. However, children with symptoms of APD typically have no evidence of neurological disease and the diagnosis is made on the basis of performance on behavioral auditory tests. Auditory processing is \"what we do with what we hear\", and in APD there is a mismatch between peripheral hearing ability (which is typically normal) and ability to interpret or discriminate sounds. Thus in those with no signs of neurological impairment, APD is diagnosed on the basis of auditory tests. There is, however, no consensus as to which tests should be used for diagnosis, as evidenced by the succession of task force reports that have appeared in recent years. The first of these occurred in 1996. This was followed by a conference organized by the American Academy of Audiology. Experts attempting to define diagnostic criteria have to grapple with the problem that a child may do poorly on an auditory test for reasons other than poor auditory perception: for instance, failure could be due to inattention, difficulty in coping with task demands, or limited language ability. In an attempt to rule out at least some of these factors, the American Academy of Audiology conference explicitly advocated that for APD to be diagnosed, the child must have a modality-specific problem, i.e. affecting auditory but not visual processing. However, an ASHA committee subsequently rejected modality-specificity as a defining characteristic of auditory processing disorders.\nThe American Speech-Language-Hearing Association (ASHA) published \"(Central) Auditory Processing Disorders\" in January 2005 as an update to the \"Central Auditory Processing: Current Status of Research and Implications for Clinical Practice (ASHA, 1996)\". The American Academy of Audiology has released more current practice guidelines related to the disorder. ASHA formally defines APA as \"a difficulty in the efficiency and effectiveness by which the central nervous system (CNS) utilizes auditory information.\"\n\nIn 2011, the British Society of Audiology published 'best practice guidelines'.\n\nAuditory processing disorder can be developmental or acquired. It may result from ear infections, head injuries or neurodevelopmental delays that affect processing of auditory information. This can include problems with: \"...sound localization and lateralization (see also binaural fusion); auditory discrimination; auditory pattern recognition; temporal aspects of audition, including temporal integration, temporal discrimination (e.g., temporal gap detection), temporal ordering, and temporal masking; auditory performance in competing acoustic signals (including dichotic listening); and auditory performance with degraded acoustic signals\".\n\nThe Committee of UK Medical Professionals Steering the UK Auditory Processing Disorder Research Program have developed the following working definition of Auditory Processing Disorder: \"APD results from impaired neural function and is characterized by poor recognition, discrimination, separation, grouping, localization, or ordering of speech sounds. It does not solely result from a deficit in general attention, language or other cognitive processes.\"\n\n1. The SCAN-C for children and SCAN-A for adolescents and adults are the most common tool for screening and diagnosing APD in the USA. Both tests are standardized on a large number of subjects and include validation data on subjects with auditory processing disorders. The test batteries include screening tests: norm-based criterion-referenced scores; diagnostic tests: scaled scores, percentile ranks and ear advantage scores for all tests except the Gap Detection test. The four tests include four subsets on which the subject scores are derived include: discrimination of monaurally presented single words against background noise (speech in noise), acoustically degraded single words (filtered words), dichotically presented single words and sentences. \n\n2. Random Gap Detection Test (RGDT) is also a standardized test. It assesses an individual’s gap detection threshold of tones and white noise. The exam includes stimuli at four different frequencies (500, 1000, 2000, and 4000 Hz) and white noise clicks of 50 ms duration. It is a useful test because it provides an index of auditory temporal resolution. In children, an overall gap detection threshold greater than 20 ms means they have failed and may have an auditory processing disorder based on abnormal perception of sound in the time domain.\n\n3. Gaps in Noise Test (GIN) also measures temporal resolution by testing the patient's gap detection threshold in white noise.\n\n4. Pitch Patterns Sequence Test (PPT) and Duration Patterns Sequence Test (DPT) measure auditory pattern identification. The PPS has s series of three tones presented at either of two pitches (high or low). Meanwhile, the DPS has a series of three tones that vary in duration rather than pitch (long or short). Patients are then asked to describe the pattern of pitches presented.\n\nThe issue of modality-specificity has led to considerable debate among experts in this field. Cacace and McFarland have argued that APD should be defined as a \"modality-specific\" perceptual dysfunction that is not due to peripheral hearing loss. They criticise more inclusive conceptualizations of APD as lacking diagnostic specificity. A requirement for modality-specificity could potentially avoid including children whose poor auditory performance is due to general factors such as poor attention or memory. Others, however, have argued that a modality-specific approach is too narrow, and that it would miss children who had genuine perceptual problems affecting both visual and auditory processing. It is also impractical, as audiologists do not have access to standardized tests that are visual analogs of auditory tests. The debate over this issue remains unresolved. It is clear, however, that a modality-specific approach will diagnose fewer children with APD than a modality-general one, and that the latter approach runs a risk of including children who fail auditory tests for reasons other than poor auditory processing. Although modality-specific testing has been advocated for well over a decade, to date no tests have been published which would allow audiologists to perform a modality-specific evaluation (i.e., no clinical versions of visual analogs to auditory processing tests exist).\n\nAnother controversy concerns the fact that most traditional tests of APD use verbal materials. The British Society of Audiology has embraced Moore's (2006) recommendation that tests for APD should assess processing of \"non-speech sounds\". The concern is that if verbal materials are used to test for APD, then children may fail because of limited language ability. An analogy may be drawn with trying to listen to sounds in a foreign language. It is much harder to distinguish between sounds or to remember a sequence of words in a language you do not know well: the problem is not an auditory one, but rather due to lack of expertise in the language.\n\nIn recent years there have been additional criticisms of some popular tests for diagnosis of APD. Tests that use tape-recorded American English have been shown to over-identify APD in speakers of other forms of English. Performance on a battery of non-verbal auditory tests devised by the Medical Research Council's Institute of Hearing Research was found to be heavily influenced by non-sensory task demands, and indices of APD had low reliability when this was controlled for. This research undermines the validity of APD as a distinct entity in its own right and suggests that the use of the term \"disorder\" itself is unwarranted. In a recent review of such diagnostic issues, it was recommended that children with suspected auditory processing impairments receive a holistic psychometric assessment including general intellectual ability, auditory memory, and attention, phonological processing, language, and literacy. The authors state that \"a clearer understanding of the relative contributions of perceptual and non-sensory, unimodal and supramodal factors to performance on psychoacoustic tests may well be the key to unravelling the clinical presentation of these individuals.\"\n\nDepending on how it is defined, APD may share common symptoms with ADD/ADHD, specific language impairment, and autism spectrum disorders. A review showed substantial evidence for atypical processing of auditory information in children with autism. Dawes and Bishop noted how specialists in audiology and speech-language pathology often adopted different approaches to child assessment, and they concluded their review as follows: \"We regard it as crucial that these different professional groups work together in carrying out assessment, treatment and management of children and undertaking cross-disciplinary research.\" In practice, this seems rare.\n\nTo ensure that APD is correctly diagnosed, the examiners must differentiate APD from other disorders with similar symptoms. Factors that should be taken into account during the diagnosis are: attention, auditory neuropathy, fatigue, hearing and sensitivity, intellectual and developmental age, medications, motivation, motor skills, native language and language experience, response strategies and decision-making style, and visual acuity.\n\nIt should also be noted that children under the age of seven cannot be evaluated correctly because their language and auditory processes are still developing. In addition, the presence of APD cannot be evaluated when a child's primary language is not English.\n\nThe National Institute on Deafness and Other Communication Disorders state that children with Auditory Processing Disorder often:\n\nAPD can manifest as problems determining the direction of sounds, difficulty perceiving differences between speech sounds and the sequencing of these sounds into meaningful words, confusing similar sounds such as \"hat\" with \"bat\", \"there\" with \"where\", etc. Fewer words may be perceived than were actually said, as there can be problems detecting the gaps between words, creating the sense that someone is speaking unfamiliar or nonsense words. In addition, it is common for APD to cause speech errors involving the distortion and substitution of consonant sounds. Those suffering from APD may have problems relating what has been said with its meaning, despite obvious recognition that a word has been said, as well as repetition of the word. Background noise, such as the sound of a radio, television or a noisy bar can make it difficult to impossible to understand speech, since spoken words may sound distorted either into irrelevant words or words that don't exist, depending on the severity of the auditory processing disorder. Using a telephone can be problematic for someone with auditory processing disorder, in comparison with someone with normal auditory processing, due to low quality audio, poor signal, intermittent sounds and the chopping of words. Many who have auditory processing disorder subconsciously develop visual coping strategies, such as lip reading, reading body language, and eye contact, to compensate for their auditory deficit, and these coping strategies are not available when using a telephone.\n\nAs noted above, the status of APD as a distinct disorder has been queried, especially by speech-language pathologists and psychologists, who note the overlap between clinical profiles of children diagnosed with APD and those with other forms of specific learning disability. Many audiologists, however, would dispute that APD is just an alternative label for dyslexia, SLI, or ADHD, noting that although it often co-occurs with these conditions, it can be found in isolation.\n\nTreatment of APD typically focuses on three primary areas: changing learning environment, developing higher-order skills to compensate for the disorder, and remediation of the auditory deficit itself. However, there is a lack of well-conducted evaluations of intervention using randomized controlled trial methodology. Most evidence for effectiveness adopts weaker standards of evidence, such as showing that performance improves after training. This does not control for possible influences of practice, maturation, or placebo effects. Recent research has shown that practice with basic auditory processing tasks (i.e. auditory training) may improve performance on auditory processing measures and phonemic awareness measures. Changes after auditory training have also been recorded at the physiological level. Many of these tasks are incorporated into computer-based auditory training programs such as Earobics and Fast ForWord, an adaptive software available at home and in clinics worldwide, but overall, evidence for effectiveness of these computerised interventions in improving language and literacy is not impressive. One small-scale uncontrolled study reported successful outcomes for children with APD using auditory training software.\n\nTreating additional issues related to APD can result in success. For example, treatment for phonological disorders (difficulty in speech) can result in success in terms of both the phonological disorder as well as APD. In one study, speech therapy improved auditory evoked potentials (a measure of brain activity in the auditory portions of the brain).\n\nWhile there is evidence that language training is effective for improving APD, there is no current research supporting the following APD treatments:\n\nHowever, use of a FM transmitter has been shown to produce significant improvements over time with children.\n\nThe first research into APD began in 1954 with Helmer Myklebust's study, \"Auditory Disorders in Children\". Myklebust's work suggested auditory processing disorder was separate from language learning difficulties. His work sparked interest in auditory deficits after acquired brain lesions affecting the temporal lobes and led to additional work looking at the physiological basis of auditory processing, but it was not until the late seventies and early eighties that research began on APD in depth.\nIn 1977, the first conference on the topic of APD was organized by Robert W. Keith, Ph.D. at the University of Cincinnati. The proceedings of that conference was published by Grune and Stratton under the title \"Central Auditory Dysfunction\" (Keith RW Ed.) That conference started a new series of studies focusing on APD in children. Virtually all tests currently used to diagnose APD originate from this work. These early researchers also invented many of the auditory training approaches, including interhemispheric transfer training and interaural intensity difference training. This period gave us a rough understanding of the causes and possible treatment options for APD.\nMuch of the work in the late nineties and 2000s has been looking to refining testing, developing more sophisticated treatment options, and looking for genetic risk factors for APD. Scientists have worked on improving behavioral tests of auditory function, neuroimaging, electroacoustic, and electrophysiologic testing. Working with new technology has led to a number of software programs for auditory training. With global awareness of mental disorders and increasing understanding of neuroscience, auditory processing is more in the public and academic consciousness than ever before.\n\n\n"}
{"id": "50451161", "url": "https://en.wikipedia.org/wiki?curid=50451161", "title": "Boeraans", "text": "Boeraans\n\nBoeraans is a dialect of Afrikaans which is one of the 11 official languages in South Africa, and is one of the youngest Germanic languages with official status.\n\nBoeraans was also known as \"Die Taal\" (meaning \"The Language). The Border Boers moved from the Western Cape to the Eastern Cape, and stayed there.\n\nIn 1820, the Boers moved to the region known as the Free State Province in South Africa and Transvaal (now the provinces of Limpopo, Mpumalanga, Gauteng and a part of North West) in South Africa.\n\n"}
{"id": "26756364", "url": "https://en.wikipedia.org/wiki?curid=26756364", "title": "Business writing process prewriting", "text": "Business writing process prewriting\n\nIn terms of the 3 × 3 writing process, prewriting belongs to phase one of the writing process. Prewriting focuses on how to properly convey the information in a message by analyzing its purpose, anticipating the audience's reaction and adapting the content of the message to that audience. Examples of methods that may assist you in your pre-writing phase include: free-writing, planning, research, outlining, storyboarding or clustering. \n\nBusiness writing is different from other types of writing because it needs to be purposeful, economical and reader-oriented. It is important for business writers to focus on expressing their ideas rather than impressing the intended audience. The goal is to get the message across in a clear and simple manner. \n\nThe 3 × 3 writing process is divided into three different phases. This chapter focuses on Phase 1 which involves analyzing, anticipating and adapting. Phase 2 requires research, organization and composition. Finally, Phase 3 includes revising, proofreading and evaluating. In the case of short messages, writing can be done relatively quickly by spending a small amount of time on each phase. However, for longer documents, it is better to spend a good amount of time working through each phase of the writing process. It is also possible to rearrange the steps in the process and even repeat some steps if necessary. The writing process is recursive rather than linear so the writer is free to revise the text at any point. \n\nBusiness writing often involves collaborating with others, such as when working in teams. Generally, team members get together in the beginning, during Phase 1, to exchange ideas. During Phase 2, each member can work separately to do some writing. Once that's done, members can meet again for Phase 3 to revise the document as a team.\n\nAnalyzing, which is the first step of the prewriting process, focuses on both detecting the main goals of the business message and choosing the most effective way to express the information to the audience. Finding out the main goals of business writing requires considering and summarizing the needs of the audience, would all be conveyed by: email, instant message, business letters, memos, reports, etc. Only when the information presented through the writing matches the demands of the audience will it attract the audience's attention. Generally, most business writing can be devoted to informing and persuading the audience. However, it is just as necessary to establish a good relationship with the audience.\nChoosing the most appropriate way of conveying information is fairly significant for improving efficiency of expression. Delivering a message in the proper way depends on a few factors. It is helpful to note the importance of the message, the feedback needed and the cost of the method of delivery.\n\nAnticipating includes profiling the audience and learning to adjust the message according to its recipients. Profiling the audience helps the writer establish the proper tone and language of the message. It also helps with choosing the right method to deliver the message. Another benefit of profiling the audience is identifying the possibility of a secondary audience. More information might be needed if the message is also intended for a secondary audience.\n\nAdapting involves using certain techniques to tailor the message to the intended audience. When writing a message, it is essential to pay attention to the tone because it is a good indicator of how the reader will feel while reading the message. Words that are chosen improperly can contribute to an overall negative tone and can make the message sound unpleasant. Therefore, it is a good idea to choose words that will have a positive impact on the tone of the message. One technique involves putting the focus of the message on the receiver. This can be achieved by using second-person pronouns throughout the text and it shows that the writer has empathy towards the reader. Another technique involves using bias-free language which means the message should be free from gender, race, age and disability bias so as to not offend anyone. It is also recommended to use a professional yet friendly tone to make the writer sound professional and approachable at the same time. It is a good idea to use positive words and avoid words that have negative connotations. Finally, it is in the writer's best interest to be polite, use simple language and words that are precise.\n\nIt is the responsibility of the writer to use language that is ethical for the purpose of avoiding litigation. When writing messages about stocks or financial services, it is important to follow the laws that protect investors. Also, regarding safety information, it is essential to write warnings on dangerous products as clearly and succinctly as possible. Messages that are used in sales and marketing should not have any false or misleading information. The messages should not be written in a way that will deceive customers. The use of proper language is also helpful regarding employee evaluation. In letters of recommendation, it is best to use positive language and stick to information that is related to the job.\n\n"}
{"id": "6867", "url": "https://en.wikipedia.org/wiki?curid=6867", "title": "Context-free language", "text": "Context-free language\n\nIn formal language theory, a context-free language (CFL) is a language generated by a context-free grammar (CFG).\n\nContext-free languages have many applications in programming languages, in particular, most arithmetic expressions are generated by context-free grammars.\n\nDifferent context-free grammars can generate the same context-free language. Intrinsic properties of the language can be distinguished from extrinsic properties of a particular grammar by comparing multiple grammars that describe the language.\n\nThe set of all context-free languages is identical to the set of languages accepted by pushdown automata, which makes these languages amenable to parsing. Further, for a given CFG, there is a direct way to produce a pushdown automaton for the grammar (and thereby the corresponding language), though going the other way (producing a grammar given an automaton) is not as direct.\n\nA model context-free language is formula_1, the language of all non-empty even-length strings, the entire first halves of which are 's, and the entire second halves of which are 's. is generated by the grammar formula_2.\nThis language is not regular.\nIt is accepted by the pushdown automaton formula_3 where formula_4 is defined as follows:\n\nUnambiguous CFLs are a proper subset of all CFLs: there are inherently ambiguous CFLs. An example of an inherently ambiguous CFL is the union of formula_6 with formula_7. This set is context-free, since the union of two context-free languages is always context-free. But there is no way to unambiguously parse strings in the (non-context-free) subset formula_8 which is the intersection of these two languages.\n\nThe language of all properly matched parentheses is generated by the grammar formula_9.\n\nThe context-free nature of the language makes it simple to parse with a pushdown automaton.\n\nDetermining an instance of the membership problem; i.e. given a string formula_10, determine whether formula_11 where formula_12 is the language generated by a given grammar formula_13; is also known as \"recognition\". Context-free recognition for Chomsky normal form grammars was shown by Leslie G. Valiant to be reducible to boolean matrix multiplication, thus inheriting its complexity upper bound of \"O\"(\"n\").\nConversely, Lillian Lee has shown \"O\"(\"n\") boolean matrix multiplication to be reducible to \"O\"(\"n\") CFG parsing, thus establishing some kind of lower bound for the latter.\n\nPractical uses of context-free languages require also to produce a derivation tree that exhibits the structure that the grammar associates with the given string. The process of producing this tree is called \"parsing\". Known parsers have a time complexity that is cubic in the size of the string that is parsed.\n\nFormally, the set of all context-free languages is identical to the set of languages accepted by pushdown automata (PDA). Parser algorithms for context-free languages include the CYK algorithm and Earley's Algorithm.\n\nA special subclass of context-free languages are the deterministic context-free languages which are defined as the set of languages accepted by a deterministic pushdown automaton and can be parsed by a LR(k) parser.\n\nSee also parsing expression grammar as an alternative approach to grammar and parser.\n\nContext-free languages are closed under the following operations. That is, if \"L\" and \"P\" are context-free languages, the following languages are context-free as well:\n\nThe context-free languages are not closed under intersection. This can be seen by taking the languages formula_22 and formula_23, which are both context-free. Their intersection is formula_24, which can be shown to be non-context-free by the pumping lemma for context-free languages. As a consequence, context-free languages cannot be closed under complementation, as for any languages \"A\" and \"B\", their intersection can be expressed by union and complement: formula_25. In particular, context-free language cannot be closed under difference, since complement can be expressed by difference: formula_26. \n\nHowever, if \"L\" is a context-free language and \"D\" is a regular language then both their intersection formula_27 and their difference formula_28 are context-free languages.\n\nIn formal language theory, questions about regular languages are usually decidable, but ones about context-free languages are often not. It is decidable whether such a language is finite, but not whether it contains every possible string, is regular, is unambiguous, or is equivalent to a language with a different grammar.\n\nThe following problems are undecidable for arbitrarily given context-free grammars A and B:\n\nThe following problems are \"decidable\" for arbitrary context-free languages:\n\nAccording to Hopcroft, Motwani, Ullman (2003), \nmany of the fundamental closure and (un)decidability properties of context-free languages were shown in the 1961 paper of Bar-Hillel, Perles, and Shamir\n\nThe set formula_8 is a context-sensitive language, but there does not exist a context-free grammar generating this language. So there exist context-sensitive languages which are not context-free. To prove that a given language is not context-free, one may employ the pumping lemma for context-free languages or a number of other methods, such as Ogden's lemma or Parikh's theorem.\n\n"}
{"id": "31312600", "url": "https://en.wikipedia.org/wiki?curid=31312600", "title": "Crossed letter", "text": "Crossed letter\n\nA crossed letter is a manuscript letter which contains two separate sets of writing, one written over the other at right-angles. This was done during the early days of the postal system in the 19th century to save on expensive postage charges, as well as to save paper. This technique is also called cross-hatching or cross-writing.\n\nA cross letter is distinct from a palimpsest, as cross-hatched manuscripts were written this way at one sitting or for the same purpose (such as a diary), rather than being re-used later.\n"}
{"id": "514932", "url": "https://en.wikipedia.org/wiki?curid=514932", "title": "Cursive", "text": "Cursive\n\nCursive (also known as script or longhand, among other names) is any style of penmanship in which some characters are written joined together in a flowing manner, generally for the purpose of making writing faster. Formal cursive is generally joined, but casual cursive is a combination of joins and pen lifts. The writing style can be further divided as \"looped\", \"italic\" or \"connected\".\n\nThe cursive method is used with a number of alphabets due to its improved writing speed and infrequent pen lifting. In some alphabets, many or all letters in a word are connected, sometimes making a word one single complex stroke.\n\nCursive is a style of penmanship in which the symbols of the language are written in a conjoined and/or \"flowing\" manner, generally for the purpose of making writing faster. This writing style is distinct from \"printscript\" using block letters, in which the letters of a word are unconnected and in Roman/Gothic letterform rather than joined-up script. Not all cursive copybooks join all letters: formal cursive is generally joined, but casual cursive is a combination of joins and pen lifts. In the Arabic, Syriac, Latin, and Cyrillic alphabets, many or all letters in a word are connected, sometimes making a word one single complex stroke. In Hebrew cursive and Roman cursive, the letters are not connected. In Maharashtra there is a version of Cursive called 'Modi'\n\nLigature is writing the letters of words with lines connecting the letters so that one does not have to pick up the pen or pencil between letters. Commonly some of the letters are written in a looped manner to facilitate the connections. In common printed Greek texts, the modern small letter fonts are called \"cursive\" (as opposed to uncial) though the letters do not connect.\n\nIn \"looped cursive\" penmanship, some ascenders and descenders have loops which provide for joins. This is generally what people refer to when they say \"cursive\".\n\n\"Cursive italic\" penmanship—derived from chancery cursive—uses non-looped joins or no joins. In italic cursive, there are no joins from g, j, q or y, and a few other joins are discouraged. Italic penmanship became popular in the 15th-century Italian Renaissance. The term \"italic\" as it relates to handwriting is not to be confused with italic typed letters that slant forward. Many, but not all, letters in the handwriting of the Renaissance were joined, as most are today in cursive italic.\n\nThe origins of the cursive method are associated with practical advantages of writing speed and infrequent pen-lifting to accommodate the limitations of the quill. Quills are fragile, easily broken, and will spatter unless used properly. Steel dip pens followed quills; they were sturdier, but still had some limitations. The individuality of the provenance of a document (see Signature) was a factor also, as opposed to machine font.\nCursive was also favored because the writing tool was rarely taken off the paper.\nThe term \"cursive\" derives from the 18th century Italian \"corsivo\" from Medieval Latin \"cursivus\", which literally means \"running\". This term in turn derives from Latin \"currere\" (\"to run, hasten\").\n\nIn Bengali cursive script\n\n(also known in Bengali as \"professional writing\") the letters are more likely to be more curvy in appearance than in standard Bengali handwriting. Also, the horizontal supporting bar on each letter (\"matra\") runs continuously through the entire word, unlike in standard handwriting. This cursive handwriting often used by literature experts differs in appearance from the standard Bengali alphabet as it is free hand writing, where sometimes the alphabets are complex and appear different from the standard handwriting.\n\"Roman cursive\" is a form of handwriting (or a script) used in ancient Rome and to some extent into the Middle Ages. It is customarily divided into old (or ancient) cursive, and new cursive. Old Roman cursive, also called majuscule cursive and capitalis cursive, was the everyday form of handwriting used for writing letters, by merchants writing business accounts, by schoolchildren learning the Latin alphabet, and even by emperors issuing commands. New Roman, also called minuscule cursive or later cursive, developed from old cursive. It was used from approximately the 3rd century to the 7th century, and uses letter forms that are more recognizable to modern eyes; \"a\", \"b\", \"d\", and \"e\" have taken a more familiar shape, and the other letters are proportionate to each other rather than varying wildly in size and placement on a line.\n\nThe Greek alphabet has had several cursive forms in the course of its development. In antiquity, a cursive form of handwriting was used in writing on papyrus. It employed slanted and partly connected letter forms as well as many ligatures. Some features of this handwriting were later adopted into Greek minuscule, the dominant form of handwriting in the medieval and early modern era. In the 19th and 20th centuries, an entirely new form of cursive Greek, more similar to contemporary Western European cursive scripts, was developed.\n\nDuring the Middle Ages, the flowing, connected cursive script of the Arabic language inspired Western Christian scholars to develop similar cursive scripts for Latin. These scripts then became the basis for all of the Latin-based cursive scripts used in Europe.\n\nCursive writing was used in English before the Norman conquest. Anglo-Saxon Charters typically include a boundary clause written in Old English in a cursive script. A cursive handwriting style—secretary hand—was widely used for both personal correspondence and official documents in England from early in the 16th century.\n\nCursive handwriting developed into something approximating its current form from the 17th century, but its use was neither uniform, nor standardized either in England itself or elsewhere in the British Empire. In the English colonies of the early 17th century, most of the letters are clearly separated in the handwriting of William Bradford, though a few were joined as in a cursive hand. In England itself, Edward Cocker had begun to introduce a version of the French \"ronde\" style, which was then further developed and popularized throughout the British Empire in the 17th and 18th centuries as round hand by John Ayers and William Banson.\n\nIn the American colonies, on the eve of their independence from the Kingdom of Great Britain, it is notable that Thomas Jefferson joined most, but not all the letters when drafting the United States Declaration of Independence. However, a few days later, Timothy Matlack professionally re-wrote the presentation copy of the Declaration in a fully joined, cursive hand. Eighty-seven years later, in the middle of the 19th century, Abraham Lincoln drafted the Gettysburg Address in a cursive hand that would not look out of place today.\n\nNote that not all such cursive, then or now, joined all of the letters within a word.\nIn both the British Empire and the United States in the 18th and 19th centuries, before the typewriter, professionals used cursive for their correspondence. This was called a \"fair hand\", meaning it looked good, and firms trained their clerks to write in exactly the same script.\n\nIn the early days of the post office, letters were written in cursive – and to fit more text on a single sheet, the text was continued in lines crossing at 90 degrees from the original text. Block letters were not suitable for this.\n\nAlthough women's handwriting had noticeably different particulars from men's, the general forms were not prone to rapid change. In the mid-19th century, most children were taught the contemporary cursive; in the United States, this usually occurred in second or third grade (around ages seven to nine). Few simplifications appeared as the middle of the 20th century approached.\n\nAfter the 1960s, a movement originally begun by Paul Standard in the 1930s to replace looped cursive with cursive italic penmanship resurfaced. It was motivated by the claim that cursive instruction was more difficult than it needed to be: that conventional (looped) cursive was unnecessary, and it was easier to write in cursive italic. Because of this, a number of various new forms of cursive italic appeared, including Getty-Dubay, and Barchowsky Fluent Handwriting. In the 21st century, some of the surviving cursive writing styles are Spencerian, Palmer Method, D'Nealian, and Zaner-Bloser script.\n\nOne of the earliest forms of new technology that caused the decline of handwriting was the invention of the ballpoint pen, patented in 1888 by John Loud. Two brothers, László and György Bíró, further developed the pen by changing the design and using different ink that dried quickly. With their design, it was guaranteed that the ink would not smudge, as it would with the earlier design of pen, and it no longer required the careful penmanship one would use with the older design of pen. After World War II, the ballpoint pen was mass-produced and sold for a cheap price, changing the way people wrote. Over time the emphasis of using the style of cursive to write slowly declined, only to be later impacted by other technologies such as the phone, computer, and keyboard.\nCursive has been in decline throughout the 21st century due to its perceived lack of necessity. The Fairfax Education Association, the largest teachers' union in Fairfax County, Virginia, has called cursive a \"dying art\". Many consider cursive too tedious to learn and believe that it is not a useful skill.\n\nOn the 2006 SAT, a United States post-secondary education entrance exam, only 15 percent of the students wrote their essay answers in cursive. However, students might be discouraged from using cursive on standardized tests due to exams written in hard to read handwriting receive less marks, and some graders may have difficulties reading cursive.\n\nIn a 2007 survey of 200 teachers of first through third grades in all 50 American states, 90 percent of respondents said their schools required the teaching of cursive.\n\nA 2008 nationwide survey found elementary school teachers lacking formal training in teaching handwriting to students. Only 12 percent of teachers reported having taken a course in how to teach it.\n\nIn 2012, the American states of Indiana and Hawaii announced that their schools will no longer be required to teach cursive (but will still be permitted to), and instead will be required to teach \"keyboard proficiency\". Since the nationwide proposal of the Common Core State Standards in 2009, which do not include instruction in cursive, the standards have been adopted by 44 states as of July 2011, all of which have debated whether to augment them with cursive.\n\nMany historical documents, such as the United States Constitution, are written in cursive—the inability to read cursive therefore precludes one from being able to fully appreciate such documents in their original format. Despite the decline in the day-to-day use of cursive, it is being reintroduced to the curriculum of schools in the United States. States such as California, Idaho, Kansas, Massachusetts, North Carolina, South Carolina, New Jersey, and Tennessee have already mandated cursive in schools as a part of the Back to Basics program designed to maintain the integrity of cursive handwriting. Cursive instruction is required by grade 5 in Illinois, starting with the 2018-2019 school year. Some argue that cursive is not worth teaching in schools and \"in the 1960s cursive was implemented because of preference and not an educational basis; Hawaii and Indiana have replaced cursive instruction with 'keyboard proficiency' and 44 other states are currently weighing similar measures.\"\n\nWith the widespread use of computers, researchers set out to test the effectiveness of both mediums. In a study done by Pam Mueller which compared scores of students who took notes by hand and via laptop computer showed that students who took notes by hand showed advantages in both factual and conceptual learning. Another study done by Anne Mangen showed that children showed an acceleration in learning new words when they wrote them by hand rather than on a computer screen. Learning to write in cursive is alleged (by its practitioners) to be a stepping stone to developing neat handwriting, and, in a third study conducted by Florida International University, professor Laura Dinehart concluded that students with neater handwriting tend to develop better reading and writing skills, though it is difficult to conclude causation from such an association. Aside from these cognitive benefits, students with dyslexia, who have difficulty learning to read because their brains have difficulty associating sounds and letter combinations efficiently, have found that cursive can help them with the decoding process because it integrates hand-eye coordination, fine motor skills and other brain and memory functions.. However, students with dysgraphia may be badly served, even substantially hindered, by demands for cursive.\n\nUp to the 19th century, Kurrent (also known as \"German cursive\") was used in German language longhand. Kurrent was not used exclusively, but in parallel to modern cursive (which is the same as English cursive). Writers used both cursive styles: location, contents and context of the text determined which style to use. A successor of Kurrent, Sütterlin, was widely used in the period 1911-1941 until the Nazi Party banned it, and German speakers brought up with Sütterlin continued to use it well into the post-war period.\n\nToday, three different styles of cursive writing are taught in German schools, the (introduced in 1953), the (1968), and the (1969). The German National Primary Schoolteachers' Union has proposed replacing all three with Grundschrift, a simplified form of non-cursive handwriting adopted by Hamburg schools.\n\nThe \"Russian Cursive Cyrillic\" alphabet is used (instead of the block letters) when handwriting the modern Russian language. While several letters resemble Latin counterparts, many of them represent different sounds. Most handwritten Russian, especially personal letters and schoolwork, uses the cursive Russian (Cyrillic) alphabet. Most children in Russian schools are taught in the 1st grade how to write using this Russian script.\n\nCursive forms of Chinese characters are used in calligraphy; \"running script\" is the semi-cursive form and \"rough script\" (mistakenly called \"grass script\" due to misinterpretation) is the cursive. The running aspect of this script has more to do with the formation and connectedness of strokes \"within\" an individual character than with connections between characters as in Western connected cursive. The latter are rare in hanzi and in the derived Japanese kanji characters which are usually well separated by the writer.\n\n"}
{"id": "10374", "url": "https://en.wikipedia.org/wiki?curid=10374", "title": "Essay", "text": "Essay\n\nAn essay is, generally, a piece of writing that gives the author's own argument — but the definition is vague, overlapping with those of a paper, an article, a pamphlet, and a short story. Essays have traditionally been sub-classified as formal and informal. Formal essays are characterized by \"serious purpose, dignity, logical organization, length,\" whereas the informal essay is characterized by \"the personal element (self-revelation, individual tastes and experiences, confidential manner), humor, graceful style, rambling structure, unconventionality or novelty of theme,\" etc.\n\nEssays are commonly used as literary criticism, political manifestos, learned arguments, observations of daily life, recollections, and reflections of the author. Almost all modern essays are written in prose, but works in verse have been dubbed essays (e.g., Alexander Pope's \"An Essay on Criticism\" and \"An Essay on Man\"). While brevity usually defines an essay, voluminous works like John Locke's \"An Essay Concerning Human Understanding\" and Thomas Malthus's \"An Essay on the Principle of Population\" are counterexamples.\n\nIn some countries (e.g., the United States and Canada), essays have become a major part of formal education. Secondary students are taught structured essay formats to improve their writing skills; admission essays are often used by universities in selecting applicants, and in the humanities and social sciences essays are often used as a way of assessing the performance of students during final exams.\n\nThe concept of an \"essay\" has been extended to other media beyond writing. A film essay is a movie that often incorporates documentary filmmaking styles and focuses more on the evolution of a theme or idea. A photographic essay covers a topic with a linked series of photographs that may have accompanying text or captions.\n\nAn essay has been defined in a variety of ways. One definition is a \"prose composition with a focused subject of discussion\" or a \"long, systematic discourse\".\nIt is difficult to define the genre into which essays fall. Aldous Huxley, a leading essayist, gives guidance on the subject. He notes that \"the essay is a literary device for saying almost everything about almost anything\", and adds that \"by tradition, almost by definition, the essay is a short piece\". Furthermore, Huxley argues that \"essays belong to a literary species whose extreme variability can be studied most effectively within a three-poled frame of reference\". \nThese three poles (or worlds in which the essay may exist) are:\nHuxley adds that the most satisfying essays \"...make the best not of one, not of two, but of all the three worlds in which it is possible for the essay to exist.\"\n\nThe word \"essay\" derives from the French infinitive \"essayer\", \"to try\" or \"to attempt\". In English \"essay\" first meant \"a trial\" or \"an attempt\", and this is still an alternative meaning. The Frenchman Michel de Montaigne (1533–1592) was the first author to describe his work as essays; he used the term to characterize these as \"attempts\" to put his thoughts into writing, and his essays grew out of his commonplacing. Inspired in particular by the works of Plutarch, a translation of whose \"Œuvres Morales\" (\"Moral works\") into French had just been published by Jacques Amyot, Montaigne began to compose his essays in 1572; the first edition, entitled \"Essais\", was published in two volumes in 1580. For the rest of his life, he continued revising previously published essays and composing new ones. Francis Bacon's essays, published in book form in 1597, 1612, and 1625, were the first works in English that described themselves as \"essays\". Ben Jonson first used the word \"essayist\" in English in 1609, according to the \"Oxford English Dictionary\".\n\nEnglish essayists included Robert Burton (1577–1641) and Sir Thomas Browne (1605–1682). In France, Michel de Montaigne's three volume \"Essais\" in the mid 1500s contain over 100 examples widely regarded as the predecessor of the modern essay. In Italy, Baldassare Castiglione wrote about courtly manners in his essay \"Il Cortigiano\". In the 17th century, the Jesuit Baltasar Gracián wrote about the theme of wisdom. During the Age of Enlightenment, essays were a favored tool of polemicists who aimed at convincing readers of their position; they also featured heavily in the rise of periodical literature, as seen in the works of Joseph Addison, Richard Steele and Samuel Johnson. In the 18th and 19th centuries, Edmund Burke and Samuel Taylor Coleridge wrote essays for the general public. The early 19th century, in particular, saw a proliferation of great essayists in English – William Hazlitt, Charles Lamb, Leigh Hunt and Thomas de Quincey all penned numerous essays on diverse subjects. In the 20th century, a number of essayists tried to explain the new movements in art and culture by using essays (e.g., T.S. Eliot). Whereas some essayists used essays for strident political themes, Robert Louis Stevenson and Willa Cather wrote lighter essays. Virginia Woolf, Edmund Wilson, and Charles du Bos wrote literary criticism essays.\n\nAs with the novel, essays existed in Japan several centuries before they developed in Europe with a genre of essays known as \"zuihitsu\" — loosely connected essays and fragmented ideas. Zuihitsu have existed since almost the beginnings of Japanese literature. Many of the most noted early works of Japanese literature are in this genre. Notable examples include \"The Pillow Book\" (c. 1000), by court lady Sei Shōnagon, and \"Tsurezuregusa\" (1330), by particularly renowned Japanese Buddhist monk Yoshida Kenkō. Kenkō described his short writings similarly to Montaigne, referring to them as \"nonsensical thoughts\" written in \"idle hours\". Another noteworthy difference from Europe is that women have traditionally written in Japan, though the more formal, Chinese-influenced writings of male writers were more prized at the time.\n\nThis section describes the different forms and styles of essay writing. These forms and styles are used by an array of authors, including university students and professional essayists.\n\nThe defining features of a \"cause and effect\" essay are causal chains that connect from a cause to an effect, careful language, and chronological or emphatic order. A writer using this rhetorical method must consider the subject, determine the purpose, consider the audience, think critically about different causes or consequences, consider a thesis statement, arrange the parts, consider the language, and decide on a conclusion.\n\nClassification is the categorization of objects into a larger whole while division is the breaking of a larger whole into smaller parts.\n\nCompare and contrast essays are characterized by a basis for comparison, points of comparison, and analogies. It is grouped by the object (chunking) or by point (sequential). The comparison highlights the similarities between two or more similar objects while contrasting highlights the differences between two or more objects. When writing a compare/contrast essay, writers need to determine their purpose, consider their audience, consider the basis and points of comparison, consider their thesis statement, arrange and develop the comparison, and reach a conclusion. Compare and contrast is arranged emphatically.\n\nExpository essay is used to inform, describe or explain a topic, using important facts and teaching reader about the topic. Mostly written in third-person, using \"it\", \"he\", \"she\", \"they\". Expository essay uses formal language to discuss someone or something. Examples of expository essays are: a medical or biological condition, social or technological process, life or character of a famous person. Writing of expository essay often consists of following next steps: organizing thoughts (brainstorming), researching a topic, developing a thesis statement, writing the introduction, writing the body of essay, writing the conclusion. Expository essays are often assigned as a part of SAT and other standardized testings or as a homework for high school and college students.\n\nDescriptive writing is characterized by sensory details, which appeal to the physical senses, and details that appeal to a reader's emotional, physical, or intellectual sensibilities. Determining the purpose, considering the audience, creating a dominant impression, using descriptive language, and organizing the description are the rhetorical choices to consider when using a description. A description is usually arranged spatially but can also be chronological or emphatic. The focus of a description is the scene. Description uses tools such as denotative language, connotative language, figurative language, metaphor, and simile to arrive at a dominant impression. One university essay guide states that \"descriptive writing says what happened or what another author has discussed; it provides an account of the topic\".\nLyric essays are an important form of descriptive essays.\n\nIn the dialectic form of the essay, which is commonly used in philosophy, the writer makes a thesis and argument, then objects to their own argument (with a counterargument), but then counters the counterargument with a final and novel argument. This form benefits from presenting a broader perspective while countering a possible flaw that some may present. This type is sometimes called an ethics paper.\n\nAn exemplification essay is characterized by a generalization and relevant, representative, and believable examples including anecdotes. Writers need to consider their subject, determine their purpose, consider their audience, decide on specific examples, and arrange all the parts together when writing an exemplification essay.\nAn essayist writes a \"familiar essay\" if speaking to a single reader, writing about both themselves, and about particular subjects. Anne Fadiman notes that \"the genre's heyday was the early nineteenth century,\" and that its greatest exponent was Charles Lamb. She also suggests that while critical essays have more brain than the heart, and personal essays have more heart than brain, familiar essays have equal measures of both.\n\nA history essay sometimes referred to as a thesis essay describes an argument or claim about one or more historical events and supports that claim with evidence, arguments, and references. The text makes it clear to the reader why the argument or claim is as such.\n\nA narrative uses tools such as flashbacks, flash-forwards, and transitions that often build to a climax. The focus of a narrative is the plot. When creating a narrative, authors must determine their purpose, consider their audience, establish their point of view, use dialogue, and organize the narrative. A narrative is usually arranged chronologically.\n\nAn argumentative essay is a critical piece of writing, aimed at presenting objective analysis of the subject matter, narrowed down to a single topic. The main idea of all the criticism is to provide an opinion either of positive or negative implication. As such, a critical essay requires research and analysis, strong internal logic and sharp structure. Its structure normally builds around introduction with a topic's relevance and a thesis statement, body paragraphs with arguments linking back to the main thesis, and conclusion. In addition, an argumentative essay may include a refutation section where conflicting ideas are acknowledged, described, and criticized. Each argument of argumentative essay should be supported with sufficient evidence, relevant to the point.\n\nA process essay is used for an explanation of making or breaking something. Often, it is written in chronological order or numerical order to show step-by-step processes. It has all the qualities of a technical document with the only difference is that it is often written in descriptive mood, while a technical document is mostly in imperative mood.\n\nAn economic essay can start with a thesis, or it can start with a theme. It can take a narrative course and a descriptive course. It can even become an argumentative essay if the author feels the need. After the introduction, the author has to do his/her best to expose the economic matter at hand, to analyze it, evaluate it, and draw a conclusion. If the essay takes more of a narrative form then the author has to expose each aspect of the economic puzzle in a way that makes it clear and understandable for the reader\n\nA \"reflective essay\" is an analytical piece of writing in which the writer describes a real or imaginary scene, event, interaction, passing thought, memory, or form — adding a personal reflection on the meaning of the topic in the author's life. Thus, the focus is not merely descriptive. The writer doesn’t just describe the situation, but revisits the scene with more detail and emotion to examine what went well, or reveal a need for additional learning — and may relate what transpired to the rest of the author's life.\n\nThe logical progression and organizational structure of an essay can take many forms. Understanding how the movement of thought is managed through an essay has a profound impact on its overall cogency and ability to impress. A number of alternative logical structures for essays have been visualized as diagrams, making them easy to implement or adapt in the construction of an argument.\n\nIn countries like the United States and the United Kingdom, essays have become a major part of a formal education in the form of free response questions. Secondary students in these countries are taught structured essay formats to improve their writing skills, and essays are often used by universities in these countries in selecting applicants (\"see\" admissions essay). In both secondary and tertiary education, essays are used to judge the mastery and comprehension of the material. Students are asked to explain, comment on, or assess a topic of study in the form of an essay. In some courses, university students must complete one or more essays over several weeks or months. In addition, in fields such as the humanities and social sciences, mid-term and end of term examinations often require students to write a short essay in two or three hours.\n\nIn these countries, so-called academic essays also called \"papers\", are usually more formal than literary ones. They may still allow the presentation of the writer's own views, but this is done in a logical and factual manner, with the use of the first person often discouraged. Longer academic essays (often with a word limit of between 2,000 and 5,000 words) are often more discursive. They sometimes begin with a short summary analysis of what has previously been written on a topic, which is often called a literature review.\n\nLonger essays may also contain an introductory page that defines words and phrases of the essay's topic. Most academic institutions require that all substantial facts, quotations, and other supporting material in an essay be referenced in a bibliography or works cited page at the end of the text. This scholarly convention helps others (whether teachers or fellow scholars) to understand the basis of facts and quotations the author uses to support the essay's argument and helps readers evaluate to what extent the argument is supported by evidence, and to evaluate the quality of that evidence. The academic essay tests the student's ability to present their thoughts in an organized way and is designed to test their intellectual capabilities.\n\nOne of the challenges facing universities is that in some cases, students may submit essays purchased from an essay mill (or \"paper mill\") as their own work. An \"essay mill\" is a ghostwriting service that sells pre-written essays to university and college students. Since plagiarism is a form of academic dishonesty or academic fraud, universities and colleges may investigate papers they suspect are from an essay mill by using plagiarism detection software, which compares essays against a database of known mill essays and by orally testing students on the contents of their papers.\n\nEssays often appear in magazines, especially magazines with an intellectual bent, such as \"The Atlantic\" and \"Harpers\". Magazine and newspaper essays use many of the essay types described in the section on forms and styles (e.g., descriptive essays, narrative essays, etc.). Some newspapers also print essays in the op-ed section.\nEmployment essays detailing experience in a certain occupational field are required when applying for some jobs, especially government jobs in the United States. Essays known as Knowledge Skills and Executive Core Qualifications are required when applying to certain US federal government positions.\n\nA KSA, or \"Knowledge, Skills, and Abilities,\" is a series of narrative statements that are required when applying to Federal government job openings in the United States. KSAs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The knowledge, skills, and abilities necessary for the successful performance of a position are contained on each job vacancy announcement. KSAs are brief and focused essays about one's career and educational background that presumably qualify one to perform the duties of the position being applied for.\n\nAn Executive Core Qualification, or ECQ, is a narrative statement that is required when applying to Senior Executive Service positions within the US Federal government. Like the KSAs, ECQs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The Office of Personnel Management has established five executive core qualifications that all applicants seeking to enter the Senior Executive Service must demonstrate.\n\nA film essay (or \"cinematic essay\") consists of the evolution of a theme or an idea rather than a plot per se, or the film literally being a cinematic accompaniment to a narrator reading an essay. From another perspective, an essay film could be defined as a documentary film visual basis combined with a form of commentary that contains elements of self-portrait (rather than autobiography), where the signature (rather than the life story) of the filmmaker is apparent. The cinematic essay often blends documentary, fiction, and experimental film making using tones and editing styles.\n\nThe genre is not well-defined but might include propaganda works of early Soviet parliamentarians like Dziga Vertov, present-day filmmakers including Chris Marker, Michael Moore (\"Roger & Me\" (1989), \"Bowling for Columbine\" (2002) and \"Fahrenheit 9/11\" (2004)), Errol Morris (\"The Thin Blue Line\" (1988)), Morgan Spurlock (\"Supersize Me: A Film of Epic Portions\") and Agnès Varda. Jean-Luc Godard describes his recent work as \"film-essays\". Two filmmakers whose work was the antecedent to the cinematic essay include Georges Méliès and Bertolt Brecht. Méliès made a short film (\"The Coronation of Edward VII\" (1902)) about the 1902 coronation of King Edward VII, which mixes actual footage with shots of a recreation of the event. Brecht was a playwright who experimented with film and incorporated film projections into some of his plays. Orson Welles made an essay film in his own pioneering style, released in 1974, called \"F for Fake\", which dealt specifically with art forger Elmyr de Hory and with the themes of deception, \"fakery,\" and authenticity in general. These are often published online on video hosting services.\n\nDavid Winks Gray's article \"The essay film in action\" states that the \"essay film became an identifiable form of filmmaking in the 1950s and '60s\". He states that since that time, essay films have tended to be \"on the margins\" of the filmmaking the world. Essay films have a \"peculiar searching, questioning tone ... between documentary and fiction\" but without \"fitting comfortably\" into either genre. Gray notes that just like written essays, essay films \"tend to marry the personal voice of a guiding narrator (often the director) with a wide swath of other voices\". The University of Wisconsin Cinematheque website echoes some of Gray's comments; it calls a film essay an \"intimate and allusive\" genre that \"catches filmmakers in a pensive mood, ruminating on the margins between fiction and documentary\" in a manner that is \"refreshingly inventive, playful, and idiosyncratic\".\n\nIn the realm of music, composer Samuel Barber wrote a set of \"Essays for Orchestra,\" relying on the form and content of the music to guide the listener's ear, rather than any extra-musical plot or story.\n\nA photographic essay strives to cover a topic with a linked series of photographs. Photo essays range from purely photographic works to photographs with captions or small notes to full-text essays with a few or many accompanying photographs. Photo essays can be sequential in nature, intended to be viewed in a particular order — or they may consist of non-ordered photographs viewed all at once or in an order that the viewer chooses. All photo essays are collections of photographs, but not all collections of photographs are photo essays. Photo essays often address a certain issue or attempt to capture the character of places and events.\nIn the visual arts, an essay is a preliminary drawing or sketch that forms a basis for a final painting or sculpture, made as a test of the work's composition (this meaning of the term, like several of those following, comes from the word \"essay\"'s meaning of \"attempt\" or \"trial\").\n\n\n\n"}
{"id": "5977707", "url": "https://en.wikipedia.org/wiki?curid=5977707", "title": "Etymologicum Genuinum", "text": "Etymologicum Genuinum\n\nThe Etymologicum Genuinum (standard abbreviation E Gen) is the conventional modern title given to a lexical encyclopedia compiled at Constantinople in the mid ninth century. The anonymous compiler drew on the works of numerous earlier lexicographers and scholiasts, both ancient and recent, including Aelius Herodianus, Georgius Choeroboscus, Saint Methodius, Orion of Thebes, Oros of Alexandria and Theognostus the Grammarian. The \"Etymologicum Genuinum\" was possibly a product of the intellectual circle around Photius. It was an important source for the subsequent Byzantine lexicographical tradition, including the \"Etymologicum Magnum\", \"Etymologicum Gudianum\" and \"Etymologicum Symeonis\".\n\nModern scholarship discovered the \"Etymologicum Genuinum\" only in the nineteenth century. It is preserved in two tenth-century manuscripts, \"Codex Vaticanus Graecus\" 1818 (= A) and \"Codex Laurentianus Sancti Marci\" 304 (= B; AD 994). Neither contains the earliest recension nor the complete text, but rather two different abridgements. The manuscript evidence and citations in later works suggest that the original title was simply τὸ Ἐτυμολογικόν and later τὸ μέγα Ἐτυμολογικόν. Its modern name was coined in 1897 by Richard Reitzenstein, who was the first to edit a sample section. The \"Etymologicum Genuinum\" remains for the most part unpublished except for specimen glosses. Two editions are in long-term preparation, one begun by Ada Adler and continued by Klaus Alpers, the other by François Lasserre and Nikolaos Livadaras. The latter edition is published under the title \"Etymologicum Magnum Genuinum\", but this designation is not widely used and is a potential source of confusion with the twelfth-century lexical compendium conventionally titled the \"Etymologicum Magnum\".\n\n\n"}
{"id": "2625672", "url": "https://en.wikipedia.org/wiki?curid=2625672", "title": "Extensional context", "text": "Extensional context\n\nIn philosophy of language, a context in which a sub-sentential expression \"e\" appears is called extensional if and only if \"e\" can be replaced by an expression with the same extension and necessarily preserve truth-value. The extension of a term is the set of objects that that term denotes.\n\nTake the case of Clark Kent, who is secretly Superman. Suppose that Lois Lane fell out of a window and Superman caught her. Thus the statement, \"Clark Kent caught Lois Lane,\" is true because it has an extensional context. The names \"Superman\" and \"Clark Kent\" have the same extension, which is to say that they both refer to the same person, i.e., that superhero who is vulnerable to kryptonite. Anybody that Superman caught, Clark Kent caught.\n\nIn opposition to extensional contexts are intensional contexts, where synonymous terms cannot be substituted in without potentially compromising the truth-value. Suppose that Lois Lane believes that Clark Kent will investigate a news story with her. The statement, \"Lois Lane believes that Superman will investigate a news story with her,\" is false, even though Superman is Clark Kent. This is because 'believes' is typically an intensional context.\n\n"}
{"id": "912426", "url": "https://en.wikipedia.org/wiki?curid=912426", "title": "Folk etymology", "text": "Folk etymology\n\nFolk etymology or reanalysis – sometimes called pseudo-etymology, popular etymology, or analogical reformation – is a change in a word or phrase resulting from the replacement of an unfamiliar form by a more familiar one. The form or the meaning of an archaic, foreign, or otherwise unfamiliar word is reanalyzed as resembling more familiar words or morphemes. Rebracketing is a form of folk etymology in which a word is broken down or \"bracketed\" into a new set of supposed elements. Back-formation, creating a new word by removing or changing parts of an existing word, is often based on folk etymology.\n\nThe term \"folk etymology\" is a loan translation from German Volksetymologie, coined by Ernst Förstemann in 1852. Folk etymology is a productive process in historical linguistics, language change, and social interaction. Reanalysis of a word's history or original form can affect its spelling, pronunciation, or meaning. This is frequently seen in relation to loanwords or words that have become archaic or obsolete.\n\nExamples of words created or changed through folk etymology include the English dialectal form \"sparrowgrass\", originally from Greek (\"asparagus\") remade by analogy to the more familiar words \"sparrow\" and \"grass\", or the word \"burger\", originally from \"Hamburg\" + \"-er\" (\"thing connected with\"), but understood as \"ham\" + \"burger\".\n\nThe technical term \"folk etymology\" refers to a change in the form of a word caused by erroneous popular beliefs about its etymology. The English word is a translation of the German term \"Volksetymologie\", coined by Ernst Förstemann. Förstemann noted that in addition to scientific etymology based on careful study in philology, there exist scholarly but often unsystematic accounts, as well as popular accounts for the history of linguistic forms. Until academic linguists developed comparative philology and described the laws underlying sound changes, the derivation of words was a matter mostly of guess-work. Speculation about the original form of words in turn feeds back into the development of the word and thus becomes a part of a new etymology.\n\nBelieving a word to have a certain origin, people begin to pronounce, spell, or otherwise use the word in a manner appropriate to that perceived origin. This popular etymologizing has had a powerful influence on the forms which words take. Examples in English include \"crayfish\" or \"crawfish\", which are not historically related to \"fish\" but come from Middle English \"crevis\", cognate with French \"écrevisse\". Likewise \"chaise lounge\", from the original French \"chaise longue\" (\"long chair\"), has come to be associated with the word \"lounge\".\n\nRebracketing is a process of language change in which parts of a word that appear to be meaningful (such as *\"ham\" in \"hamburger\") are mistaken as elements of the word's etymology (in this case, the word \"ham\"). Rebracketing functions by reanalyzing the constituent parts of a word. For example, the Old French word \"orenge\" (\"orange tree\") comes from Arabic \"an nāranj\" (\"the orange tree\"), with the initial \"n\" of \"nāranj\" understood as part of the article.\n\nIn back-formation a new word is created, often by removing elements thought to be affixes. For example, Italian \"pronuncia\" (\"pronunciation; accent\") is derived from the verb \"pronunciare\" (\"to pronounce; to utter\") and English \"edit\" derives from \"editor\". Some cases of back-formation are based on folk etymology.\n\nIn linguistic change caused by folk etymology, the form of a word changes so that it better matches its popular rationalisation. Typically this happens either to unanalyzable foreign words or to compounds where the word underlying one part of the compound becomes obsolete.\n\nThere are many examples of words borrowed from foreign languages, and subsequently changed by folk etymology.\n\nThe spelling of many borrowed words reflects folk etymology. For example, \"\" borrowed from Old French was variously spelled \"aundyre\" or \"aundiren\" in Middle English, but was altered by association with \"iron\". Other Old French loans altered in a similar manner include \"belfry\" (from \"berfrei\") by association with \"bell\", \"female\" (from \"femelle\") by \"male\", and \"penthouse\" (from \"apentis\") by \"house\". The variant spelling of \"licorice\" as \"liquorice\" comes from the supposition that it has something to do with liquid. Anglo-Norman \"licoris\" (influenced by \"licor\" \"liquor\") and Late Latin \"liquirītia\" were respelled for similar reasons, though the ultimate origin of all three is Greek \"\" (glycyrrhiza) \"sweet root\".\n\nReanalysis of loan words can affect their spelling, pronunciation, or meaning. The word \"cockroach\", for example, was borrowed from Spanish \"cucaracha\" but was assimilated to the existing English words \"cock\" and \"roach\". \"Jerusalem artichoke\", from Italian \"girasole\", is a kind of sunflower; it is not related to artichokes and does not come from Jerusalem. The phrase \"forlorn hope\" originally meant \"storming party, body of skirmishers\" from Dutch \"verloren hoop\" \"lost troop\". But confusion with English \"hope\" has given the term an additional meaning of \"hopeless venture\".\n\nSometimes imaginative stories are created to account for the link between a borrowed word and its popularly assumed sources. The names of the \"serviceberry\", \"service tree\", and related plants, for instance, come from the Latin name \"sorbus\". The plants were called \"syrfe\" in Old English, which eventually became \"service\". Fanciful stories suggest that the name comes from the fact that the trees bloom in spring, a time when circuit-riding preachers resume church services or when funeral services are carried out for people who died during the winter.\n\nA seemingly plausible but no less speculative etymology accounts for the form of \"Welsh rarebit\", a dish made of cheese and toasted bread. The earliest known reference to the dish in 1725 called it \"Welsh rabbit\". The origin of that name is unknown, but presumably humorous, since the dish contains no rabbit. In 1785 Francis Grose suggested in \"A Classical Dictionary of the Vulgar Tongue\" that the dish is \"a Welch rare bit\", though the word \"rarebit\" was not common prior to Grose's dictionary. Both versions of the name are in current use; individuals sometimes express strong opinions concerning which version is correct.\n\nWhen a word or other form becomes obsolete, words or phrases containing the obsolete portion may be reanalyzed and changed.\n\nSome compound words from Old English were reanalyzed in Middle or Modern English when one of the constituent words fell out of use. Examples include \"bridegroom\" from Old English \"brydguma\" \"bride-man\". The word \"gome\" \"man\" from Old English \"guma\" fell out of use during the sixteenth century and the compound was eventually reanalyzed with the Modern English word \"groom\" \"male servant\". A similar reanalysis caused \"sandblind\", from Old English \"sāmblind\" \"half-blind\" with a once-common prefix \"sām-\" \"semi-\", to be respelled as though it is related to \"sand\". The word \"island\" derives from Old English \"igland\". The modern spelling with the letter \"s\" is the result of comparison with the synonym \"isle\" from Old French and ultimately Latin \"insula\", though the Old French and Old English words are not historically related. In a similar way, the spelling of \"wormwood\" was likely affected by comparison with \"wood\".\n\nThe phrase \"curry favour\", meaning to flatter, comes from Middle English \"curry favel\", \"groom a chestnut horse\". This was an allusion to a fourteenth century French morality poem, \"Roman de Fauvel\", about a chestnut-colored horse who corrupts men through duplicity. The phrase was reanalyzed in early Modern English by comparison to \"favour\" as early as 1510.\n\nWords need not completely disappear before their compounds are reanalyzed. The word \"shamefaced\" was originally \"shamefast\". The original meaning of \"fast\" \"fixed in place\" still exists but mainly in frozen expressions such as \"stuck fast\", \"hold fast\", and \"play fast and loose\". The songbird \"wheatear\" or \"white-ear\" is a back-formation from Middle English \"whit-ers\" \"white arse\", referring to the prominent white rump found in most species. Although both \"white\" and \"arse\" are common in Modern English, the folk etymology may be euphemism.\n\nReanalysis of archaic or obsolete forms can lead to changes in meaning as well. The original meaning of \"hangnail\" referred to a corn on the foot. The word comes from Old English \"ang-\" + \"nægel\" (\"anguished nail\" or \"compressed spike\"), but the spelling and pronunciation were affected by folk etymology in the seventeenth century or earlier. Thereafter, the word came to be used for a tag of skin or torn cuticle near a fingernail or toenail.\n\nSeveral words in Medieval Latin were subject to folk etymology. For example, the word \"widerdonum\" meaning \"reward\" was borrowed from Old High German \"widarlōn\" \"repayment of a loan\". The \"l→d\" alteration is due to confusion with Latin \"donum\" \"gift\". Similarly, the word \"baceler\" or \"bacheler\" (related to modern English \"bachelor\") referred to a junior knight. It is attested from the eleventh century, though its ultimate origin is uncertain. By the late Middle Ages its meaning was extended to the holder of a university degree inferior to master or doctor. This was later re-spelled \"baccalaureus\", probably reflecting a false derivation from \"bacca laurea\" \"laurel berry\", alluding to the possible laurel crown of a poet or conqueror.\n\nIn the fourteenth or fifteenth century French scholars began to spell the verb \"savoir\" (\"to know\") as \"sçavoir\" on the false belief it was derived from Latin \"scire\" \"to know\". In fact it comes from \"sapere\" \"to be wise\".\n\nThe Italian word \"liocorno\" \"unicorn\" derives from 13th century \"lunicorno\" (\"lo\" \"the\" + \"unicorno\" \"unicorn\"). Folk etymology based on \"lione\" \"lion\" altered the spelling and pronunciation. Dialectal \"liofante\" \"elephant\" was likewise altered from \"elefante\" by association with \"lione\".\n\nThe Dutch word for \"hammock\" is \"hangmat\". It was borrowed from Spanish \"hamaca\" (ultimately from Arawak \"amàca\") and altered by comparison with \"hangen\" and \"mat\", \"hanging mat\". German \"Hängematte\" shares this folk etymology.\n\nThe Finnish compound word for \"jealous\" \"mustasukkainen\" literally means \"black-socked\" (\"musta\" \"black\" and \"sukka\" \"sock\"). However, the word is a case of a misunderstood loan translation from Swedish \"svartsjuk\" \"black-sick\". The Finnish word \"sukka\" fit with a close phonological equivalent to the Swedish \"sjuk\"\n\"Islambol\", a folk etymology meaning \"full of Islam\", is one of the names of Istanbul used after the Ottoman conquest of 1453.\n\nAn example from Persian is the word shatranj (chess), which is derived from the Sanskrit chaturanga (2nd century BCE), and after losing the \"u\" to syncope, becomes \"chatrang\" in Middle Persian (6th century CE). Today it is sometimes factorized as \"shat\" (hundred) + \"ranj\" (worry / mood), or \"a hundred worries\".\n\n"}
{"id": "3284859", "url": "https://en.wikipedia.org/wiki?curid=3284859", "title": "Formulaic language", "text": "Formulaic language\n\nFormulaic language (previously known as automatic speech or embolalia) is a linguistic term for verbal expressions that are fixed in form, often non-literal in meaning with attitudinal nuances, and closely related to communicative-pragmatic context. Along with idioms, expletives and proverbs, formulaic language includes pause fillers (e.g., \"Like\", \"Er\" or \"Uhm\") and conversational speech formulas (e.g., \"You've got to be kidding,\" \"Excuse me?\" or \"Hang on a minute\").\n\nThe word embolalia comes from the Greek word \"embolos\" which means 'something thrown in', from the word \"emballo-\" meaning 'to throw in', and \"-lalia\" meaning 'speech, chattering and babbling; abnormal or disordered form of speech.\n\nModern linguists led by Leonard Bloomfield in 1933 call these \"hesitation forms\", the sounds of stammering (uh), stuttering (um, um), throat-clearing (ahem!), stalling (well, um, that is), interjected when the speaker is groping for words or at a loss for the next thought.\n\nFrench psychiatrist Jules Séglas, on the other hand, referred to the term embolalia, as \"the regular addition of prefixes or suffixes to words\" and mentioned that the behavior is sometimes used by normal individuals to demonstrate to their interlocutor that they are paying attention to the conversation.\n\nHarry Levin and Irene Silverman called formulaic language \"vocal segregates\" in their 1965 paper on hesitation phenomena and found out from their experiments on children that these segregates seem to be less voluntary hesitation phenomena and may be signs of uncontrolled emotionality under stress.\n\nThe Irish poet William Butler Yeats argued for formulaic language experiments with his wife, which provided him with symbols for his poetry as well as literary theories.\n\nAccording to \"The Canadian Modern Language Review\", formulaic sequences are \"fixed combinations of words that ... can facilitate fluency in speech by making pauses shorter and less frequent, and allowing longer runs of speech between pauses\".\n\nA formulaic sequence is \"a sequence, continuous or discontinuous, of words or other elements, which is, or appears to be, prefabricated: that is, stored and retrieved whole from memory at the time of use, rather than being subject to generation or analysis by the language grammar.\"\n\nThey can be found everywhere in language use and \"make up a large proportion of any discourse\". Formulaic sequences can be of any length and can be used to express messages, functions, social solidarity and process information very fast without communication misunderstanding.\n\n\"Filled pauses\"\n\nFilled pauses consist of repetitions of syllables and words, reformulation or false starts where speakers rephrase their speech to fit the representation they best perceive, grammatical repairs, and partial repeats that often involve searching for the right words in one's lexicon to carry across an intended meaning. There are basically three distinct forms for filled pauses: (i) an elongated central vowel only; (ii) a nasal murmur only; and (iii) a central vowel followed by a nasal murmur. Although a schwa-like quality [ə:], appears to be the most commonly used, some speakers consistently using the neutral vowel [ɨ:] instead, and others use both vowels in the same sentence, depending on the quality of the previous word last vowel. Filled pauses vocalizations may be built around central vowels and speakers may differ in their preferences, but that they do not appear to behave as other words in the language. The lengthening of words ending in a coronal fricative, for instance, could be obtained by prolonging the entire rhyme and/or the fricative only. Most of the time, however, the neutral vowel [ɨ:] is appended to achieve the desired effect.\n\n\"Prolonged pauses\"\n\nSimilarly to filled pauses, single occurrences of prolonged pauses occurring between stretches of fluent speech, may be preceded and followed by silent pauses, as they most often occur on function words with a CV or V structure. Even though they are not always central, the vowels of such syllables may be as long as the ones observed for filled pauses.\n\n\"Retraced and unretraced restarts\"\n\nRiggenbach's 1991 study of fluency development in Chinese learners of English had an analysis of repair phenomena, which included retraced restarts and unretraced restarts. Retraced restarts refer to the reformulations whereby a portion of the original utterance is duplicated. They can either involve repetition, that is, the precise adjacent duplication of a sound, syllable, word or phrase, or insertion, which refers to a retraced restart with the addition of new unretraced lexical items. Conversely, unretraced restarts refer to reformulations that reject the original utterance, similarly known as false starts.\n\nThe semantics of formulaic language have often been debated on, and to date, there lacks a consensus on whether or not filler words are intentional in speech and whether or not they should be considered as words or if they are simply side effects of difficulties in the planning process of speech by speakers. Bailey & Ferriera's (2007) paper found that there is little evidence to suggest that the use of filler words are intentional in speech and that they should not be considered as words in the conventional sense.\n\nFiller words consist of \"Non-lexical fillers\" and \"Lexical fillers\". \"Non-lexical fillers\" are recognized as fillers that are not words and \"Lexical fillers\" are recognized as fillers that are words and both types of fillers are thought to contain little or no semantic information. However, some filler words are used to express certain speech acts. \"Yeah\", a \"Lexical filler\", is used to give affirmation, introduce a new topic, shows speaker's perception and understanding, and occurs after a speech management problem when the speakers does not how to continue their speech. Fillers like \"Mmmm\", a \"Non-lexical filler\", and \"Well\", a \"Lexical filler\", are also said to signal listener's understanding of the information provided.\n\nResearch has shown that people were less likely to use formulaic language in general topics and domains they were more well-versed in, because they were more adept at selecting the appropriate terms. To date, there is insufficient research done to say if fillers are a part of integral meaning, or if they are aspect of performance, but we can say that they are useful in facilitating information for the listener.\n\nFormulaic language is more likely to occur at the beginning of utterance or phrase and the reason is because it is presumed that there is a greater demand on planning processes at these junctures. Features of formulaic language, like filled pauses or repetitions, are most likely to occur immediately prior to the onset of a complex syntactic constituent. Filled pauses are also likely after the initial word in a complex constituent, especially after function words. Therefore, listeners might be able to use the presence of a recent filled pause to predict that an ambiguous structure, and this trait is in favor of a more complex analysis .\n\nThere are several different types of formulaic language. One type is relatively universal, often transcending differences in language and to some degree culture. Simple fillers like \"Uhm\", \"Uh\", or \"Er\" are used by many different people in many different settings. For the most part, these types of fillers are considered innocuous, and are often overlooked by listeners, as long as they are not utilized so often that they overshadow the remainder of the conversation.\n\nOther forms of formulaic language are ingrained within specific cultures, and in fact are sometimes considered an identifying characteristic of people who share a particular religion, or live in a specific geographical region. Along with accents, formulaic language of this type is sometimes considered colorful and somewhat entertaining. Writers often make use of this type of speech to give the characters in their writings additional personality, helping to make them unique.\n\n\"Fluency\"\n\nThe study conducted by Dechert (1980) that investigated the speech performance of a German student of English revealed that there is a tendency for speech pauses to be situated at breaks that are consistent with \"episodic units\". Dechert (1980) found that the more fluent utterances exhibited more pauses at those junctures and lesser within the \"episodic units\", leading him to posit that the study subject was able to use the narrative structure to pace his own speech with natural breaks in order for him to scout for the words and phrases that are to follow subsequently.\n\nThrough the comparison of the story retelling utterances collated of second language learners, Lennon (1984) discovered notable disparities in the distribution of pauses between recounting in the research subjects' first and second languages respectively. The study found that all of the pauses were found to be located either at clause breaks or following nonintegral components of the clause, without pauses within the clauses. On the other hand, the narrators who spoke using their second language exhibited different patterns, with a higher frequency of pauses occurring within the clauses, leading to the conclusion posited by Lennon to be that the speakers seem to be \"planning within clauses as well as in suprasegmental units\", and hence, the occurrence of pauses within clauses and not at the intersection of clauses could well be an indicator distinguishing fluent and confluent speech.\n\nCognitive load is an important predictor of formulaic language. More disfluency is found in longer utterances and when the topic is unfamiliar. In Wood's book, he suggested that when a high degree of cognitive load occurs, such as during expository speech or impromptu descriptions of complex interrelated topics, even native speakers can suffer from disfluency.\n\nFormulaic phrases are uttered more quickly than comparable non-formulaic phrases. Speech rate is closely related to cognitive load. Depending on the cognitive load, the rates of a speaker's utterances are produced either faster or slower, in comparison to a fixed speaking rate which happens usually. For example, speech rate becomes slower when having to make choices that are not anticipated, and tend to accelerate when words are being repeated.\nIn fast conditions, cognitive processes that result in a phonetic plan, fail to keep up with articulation, and thus, the articulation of the existing plan is restarted, resulting in the repetition of words which is more likely to happen but no more likely than fillers.\n\nIn Beattie and Butterworth's (1979) study, low frequency content words and those rated as contextually improbable were preceded by hesitations such as fillers. Speakers, when choosing to use low frequency words in their speech, are aware, and are more likely to be disfluent. This is further supported by Schnadt and Corley where they found that prolongations and fillers increased in words just before multiple-named or low frequency items.\n\nHumans are found to be more disfluent overall when addressing other humans as compared to when addressing machines. More instances of formulaic language is found in dialogues than in monologues. The different roles the addresser played (such as a sister, a daughter or a mother) greatly influences the numbers of disfluencies, particularly, fillers produced, regardless of length or complexity.\n\nThere is a common agreement that disfluencies are accompanied by important modifications both at the segmental and prosodic levels and that speakers and listeners use such cues systematically and meaningfully. Thus they appear as linguistic universal devices that are similar to other devices and are controlled by the speaker and regulated by language specific constraints. In addition, speech disfluencies such as fillers can help listeners to identify upcoming words.\n\nWhile formulaic language can serve as a useful cue that more is to come, some people do develop an unconscious dependence on these filler words. When this is the case, it is necessary to correct the problem by making the speaker be aware of their over-reliance on formulaic language production and by training the person to make more efficient use of other verbal strategies. As the individual gains confidence and is less apt to have a need for filler words, the predilection toward formulaic language is then able to gradually diminish.\n\nA study done by Foxtree (2001) showed that both English and Dutch listeners were faster to identify words in a carrier sentence when it was preceded with an \"Uh\" instead of without an \"Uh\", which suggested that different fillers have different effects as they might be conveying different information.\n\nFischer and Brandt-Pook also found out that discourse particles mark thematic breaks, signal the relatedness between the preceding and following utterance, indicate if the speaker has understood the content communicated, and support the formulation process by signalling possible problems in speech management.\n\nWhile fillers might give listeners cues about the information being conveyed, Bailey & Ferreira's study made a distinction between \"Good Cues\" and \"Bad Cues\" in facilitating listener's comprehension. A \"Good Cue\" leads the listener to correctly predict the onset of a new constituent (Noun Phrase, Verb Phrase), whereas a \"Bad Cue\" leads the listener to incorrectly predict the onset of a new constituent. \"Good Cue\" make it easier for listeners to process the information they have been presented while \"Bad Cue\" make it harder for listeners process the relevant information.\n\nThere is strong empirical evidence that speakers use formulaic language in similar ways across languages and that formulaic language plays a fundamental role in the structuring of spontaneous speech, as they are used to achieve a better synchronization between interlocutors by announcing upcoming topic changes, delays related to planning load or preparedness problems, as well as speaker's intentions to take/give the floor or to revise/abandon an expression he/she had already presented.\n\nA study conducted by Clark and Foxtree (2002) mentioned that parts of formulaic language, such as fillers, serve a communicative function and are considered integral to the information the speaker tries to convey, although they do not add to the propositional content or the primary message. Instead, they are considered part of a collateral message where the speaker is commenting on her performance. Speakers produce filled pauses (e.g. \"Uh\" or \"Um\") for a variety of reasons, including the intention to discourage interruptions or to gain additional time to plan utterances.\n\nAnother communicative goal includes the attention-impelling function, which explores another purpose of hesitation forms as being to dissociate oneself slightly from the harsh reality of what is to follow. With the use of a beat of time filled with a meaningless interjection, uncommitted people who are \"into distancing\" make use of such formulaic language to create a little distance between themselves and their words, as if it might lessen the impact of their words.\n\nHowever, not all forms of formulaic language are considered appropriate or harmless.There are examples of formulaic language production that lean towards being offensive, for instance, the use of anything considered to be profanity within a given culture.\n\nIn this form, the speech is usually the insertion of swear words within the sentence structure used to convey various ideas. At times, this use of formulaic language comes about due to the individual being greatly distressed or angry. However, there are situations where swear words are inserted unconsciously even if the individual is extremely happy. When the use of swear words is called to the attention of the individual, he or she may not even have been aware of the usage of such formulaic language.\n\nMany patients who suffer from aphasia retain the ability to produce formulaic language, including conversational speech formulas and swear words—in some cases, patients are unable to create words or sentences, but they are able to swear. Also, the ability to pronounce other words can change and evolve during the process of recovery, while pronunciation and use of swear words remain unchanged.\n\nPatients who are affected by transcortical sensory aphasia, a rare form of aphasia, have been found to exhibit formulaic language that is characterised by \"lengthy chunks of memorized material\".\n\nApraxia of speech can also occur in conjunction with dysarthria (muscle weakness affecting speech production) or aphasia (language difficulties related to neurological damage).\n\nOne of the articulatory characteristics of apraxia of speech found in adults includes speech behavior that \"exhibits fewer errors with formulaic language than volitional speech\". Developmental verbal dyspraxia has also been found to have more effect on volitional speech than on formulaic language.\n\nThe characteristics of apraxia of speech include difficulties in imitating speech sounds, imitating no-speech movements, such as sticking out the tongue, groping for sounds, and in severe cases, the inability to produce any sounds, inconsistent errors and a slow rate of speech. However, patients who suffer from apraxia of speech may retain the ability to produce formulaic language, such as \"thank you\" or \"how are you?\". Apraxia of speech can also occur in conjunction with dysarthria, an illness which inflicts muscle weakness affecting speech production), or aphasia, which causes language difficulties related to neurological damage.\n\nDevelopmental coordination disorder is a chronic neurological disorder that affects the voluntary movements of speech.\nChildren with developmental coordination disorder are unable to formulate certain kinds of voluntary speech; however, they may speak set words or phrases spontaneously, constituting formulaic language—although they may not be able to repeat them on request.\n\n"}
{"id": "1930406", "url": "https://en.wikipedia.org/wiki?curid=1930406", "title": "Impredicativity", "text": "Impredicativity\n\nSomething that is impredicative, in mathematics, logic and philosophy of mathematics, is a self-referencing definition. Roughly speaking, a definition is impredicative if it invokes (mentions or quantifies over) the set being defined, or (more commonly) another set that contains the thing being defined. There is no generally accepted precise definition of what it means to be predicative or impredicative. Authors have given different but related definitions.\n\nThe opposite of impredicativity is predicativity, which essentially entails building stratified (or ramified) theories where quantification over lower levels results in variables of some new type, distinguished from the lower types that the variable ranges over. A prototypical example is intuitionistic type theory, which retains ramification so as to discard impredicativity.\n\nRussell's paradox is a famous example of an impredicative construction—namely the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be asked whether it contains itself or not — if it does then by definition it should not, and if it does not then by definition it should.\n\nThe greatest lower bound of a set , , also has an impredicative definition: if and only if for all elements of , is less than or equal to , and any less than or equal to all elements of is less than or equal to . This definition quantifies over the set (potentially infinite, depending on the order in question) whose members are the lower bounds of , one of which being the glb itself. Hence predicativism would reject this definition.\n\nThe terms \"predicative\" and \"impredicative\" were introduced by , though the meaning has changed a little since then. \n\nSolomon Feferman provides a historical review of predicativity, connecting it to current outstanding research problems.\n\nThe vicious circle principle was suggested by Henri Poincaré (1905-6, 1908) and Bertrand Russell in the wake of the paradoxes as a requirement on legitimate set specifications. Sets that do not meet the requirement are called \"impredicative\".\n\nThe first modern paradox appeared with Cesare Burali-Forti's 1897 \"A question on transfinite numbers\" and would become known as the Burali-Forti paradox. Cantor had apparently discovered the same paradox in his (Cantor's) \"naive\" set theory and this become known as Cantor's paradox. Russell's awareness of the problem originated in June 1901 with his reading of Frege's treatise of mathematical logic, his 1879 \"Begriffsschrift\"; the offending sentence in Frege is the following:\nIn other words, given the function is the variable and is the invariant part. So why not substitute the value for itself? Russell promptly wrote Frege a letter pointing out that:\nFrege promptly wrote back to Russell acknowledging the problem:\nWhile the problem had adverse personal consequences for both men (both had works at the printers that had to be emended), van Heijenoort observes that \"The paradox shook the logicians' world, and the rumbles are still felt today. ... Russell's paradox, which uses the bare notions of set and element, falls squarely in the field of logic. The paradox was first published by Russell in \"The principles of mathematics\" (1903) and is discussed there in great detail ...\". Russell, after six years of false starts, would eventually answer the matter with his 1908 theory of types by \"propounding his \"axiom of reducibility\". It says that any function is coextensive with what he calls a \"predicative\" function: a function in which the types of apparent variables run no higher than the types of the arguments\". But this \"axiom\" was met with resistance from all quarters.\n\nThe rejection of impredicatively defined mathematical objects (while accepting the natural numbers as classically understood) leads to the position in the philosophy of mathematics known as predicativism, advocated by Henri Poincaré and Hermann Weyl in his \"Das Kontinuum\". Poincaré and Weyl argued that impredicative definitions are problematic only when one or more underlying sets are infinite.\n\nErnst Zermelo in his 1908 \"A new proof of the possibility of a well-ordering\" presents an entire section \"b. \"Objection concerning nonpredicative definition\"\" where he argued against \"Poincaré (1906, p. 307) [who states that] a definition is 'predicative' and logically admissible only if it \"excludes\" all objects that are dependent upon the notion defined, that is, that can in any way be determined by it\". He gives two examples of impredicative definitions – (i) the notion of Dedekind chains and (ii) \"in analysis wherever the maximum or minimum of a previously defined \"completed\" set of numbers is used for further inferences. This happens, for example, in the well-known Cauchy proof of the fundamental theorem of algebra, and up to now it has not occurred to anyone to regard this as something illogical\". He ends his section with the following observation: \"A definition may very well rely upon notions that are equivalent to the one being defined; indeed, in every definition \"definiens\" and \"definiendum\" are equivalent notions, and the strict observance of Poincaré's demand would make every definition, hence all of science, impossible\".\n\nZermelo's example of minimum and maximum of a previously defined \"completed\" set of numbers reappears in Kleene 1952:42-42 where Kleene uses the example of Least upper bound in his discussion of impredicative definitions; Kleene does not resolve this problem. In the next paragraphs he discusses Weyl's attempt in his 1918 \"Das Kontinuum\" (\"The Continuum\") to eliminate impredicative definitions and his failure to retain the \"theorem that an arbitrary non-empty set of real numbers having an upper bound has a least upper bound (cf. also Weyl 1919)\".\n\nRamsey argued that \"impredicative\" definitions can be harmless: for instance, the definition of \"tallest person in the room\" is impredicative, since it depends on a set of things of which it is an element, namely the set of all persons in the room. Concerning mathematics, an example of an impredicative definition is the smallest number in a set, which is formally defined as: if and only if for all elements of , is less than or equal to , and is in .\n\nBurgess (2005) discusses predicative and impredicative theories at some length, in the context of Frege's logic, Peano arithmetic, second order arithmetic, and axiomatic set theory.\n\n\n"}
{"id": "2428804", "url": "https://en.wikipedia.org/wiki?curid=2428804", "title": "Invective", "text": "Invective\n\nInvective (from Middle English \"invectif\", or Old French and Late Latin \"invectus\") is abusive, reproachful, or venomous language used to express blame or censure; or, a form of rude expression or discourse intended to offend or hurt; vituperation, or deeply seated ill will, vitriol. The Latin adjective \"invectivus\" means 'scolding.'\n\nThe \"genre of invective\" or \"invective (genre)\" or \"vituperatio\" in Latin is a form of classical libel used in Greek and Roman polemical verse as well as in prose, but its primary context is rhetoric.\n\nThe rhetorical genre of \"vituperatio\" belongs to the \"genus demonstrativum\", which is composed by the praise and blame.\n\nDuring the Roman Republic, personal invectives and character assassination were widely used during the forensic speeches as well as orations. Cicero made a large use of the invectives against his political foes such as Clodius, Catilina (Catilinarian speeches) or Mark Antony (Philippics). The commonly charges are avarice, cupidity, cowardice, effeminacy, drunkenness, low writing and speaking skills, luxury, disapproved sexual habits, tyrannical behaviour, etc.\n\nBetween 44 BC and 30 BC, the invective became a tool during the propaganda war between Octavian and Mark Antony. Among many other slanders, Mark Antony was accused to have married a foreign queen Cleopatra, to be the submissive subject of her and to have lose his Roman identity. According to this propaganda, Cleopatra would plan to invade Italia. This propaganda before the battle of Actium in 31 BC permits to Octavian to present his campaign as a legitimate war for the safe of the Roman Republic.\n\nThe preferred literary term for invective of the Renaissance is libel.\n\n\n"}
{"id": "58113239", "url": "https://en.wikipedia.org/wiki?curid=58113239", "title": "Jay-driver", "text": "Jay-driver\n\nThe term jay driver originated to refer to carriage drivers who drove on the incorrect side of the road. While the term may generally refer to an individual operating a vehicle in a manner not consistent with traffic laws, it was used more specifically to individuals who drive a vehicle on the incorrect side of the road. Merriam-Webster indicates that the first known use of the term is by \"The Junction City Union\" newspaper on June 28, 1905.\n\nMost sources indicate that the term \"jay driver\" came before \"jaywalker\" there being numerous article headlines from 1905 through the next decade that include the term \"Jay Driver.\" Peter Norton's book \"Fighting Traffic\" on 78 indicates that in 1922 use of the term \"jay driver\" was an attempt by George A. Davies to use the known meaning of \"jay walker\" and apply it to drivers of vehicles. However, an opinion piece attributed to \"A MAN BOOSTER\" from the January 17, 1907 \"Salt Lake Telegram\" titled \"Oh yes, there's the 'Jay Driver'\" describes in it both jay walkers and jay drivers. However, another article from November 11, 1919 in \"The Pittsburgh Gazette Times\" cites from the \"Seattle Post-Intelligencer\" the text: \"There are so many jay walkers and so many jay drivers that it hardly behooves any driver to talk about jay walkers, or any walkjer to mention jay drivers,\" clearly demonstrating that the term existed prior to that time.\n\nA 1939 instructional video overdubbed by the voice of an official from a British agency indicates that the term was not isolated to the United States. Titled School For Jay-Drivers! the video provides an example of a motorcycle rider in Vienna failing to give the proper signal who is then required to attend a Sunday morning class in which an instructor explains what he did incorrectly and what the rules indicate he should have done.\n"}
{"id": "8167741", "url": "https://en.wikipedia.org/wiki?curid=8167741", "title": "KE family", "text": "KE family\n\nThe KE family is a medical name designated for a British family, about half of whom exhibit a severe speech disorder called developmental verbal dyspraxia. It is the first family with speech disorder to be investigated using genetic analyses, by which the speech impediment is discovered to be due to genetic mutation, and from which the gene \"FOXP2\", often dubbed the \"language gene\", was discovered. Their condition is also the first human speech and language disorder known to exhibit strict Mendelian inheritance.\n\nBrought to medical attention from their school children in the late 1980s, the case of KE family was taken up at the UCL Institute of Child Health in London in 1990. Initial report suggested that the family was affected by a genetic disorder. Canadian linguist Myrna Gopnik suggested that the disorder was characterized primarily by grammatical deficiency, supporting the controversial notion of a \"grammar gene\". Geneticists at the University of Oxford determined that the condition was indeed genetic, but with complex physical and physiological effects, and in 1998, they identified the actual gene, eventually named \"FOXP2\". This discovery directly led to a broader knowledge on human evolution as the gene is directly implicated with the origin of language.\n\nTwo family members, a boy and a girl, were featured in the National Geographic documentary film \"Human Ape\".\n\nKE family children attended Elizabeth Augur's special educational needs unit at Brentford primary school in west London. Towards the end of 1980s seven children of the family attended there. Augur began to learn that the family had a speech disorder for three generations. Of the 30 members, about half suffer from severe deficiency, some are affected mildly, and few are unaffected. Their faces show rigidity at the lower half, and most cannot complete pronouncing a word. Many of them have severe stuttering and with limited vocabulary. In particular, they have difficulty with consonants, and omit them, such as \"boon\" for \"spoon\", \"able\" for \"table\", and \"bu\" for \"blue\". Linguistic deficiency is also noted in written language both in reading and writing. They are characterized by lower nonverbal IQ.\n\nAugur convinced the family to undergo medical studies and approached geneticist Michael Baraitser, of the Institute of Child Health. With colleagues Marcus Prembey and Jane Hurst, they started taking blood samples for analyses in 1987. Their first report in 1990 shows that 16 family members were affected by severe abnormality, though their intelligence and hearing are normal, and that the condition was genetically inherited (autosomal dominant). Upon the news, BBC was preparing a documentary of the case in the scientific serial \"Antenna\". By this time, a Canadian linguist from McGill University, Myrna Gopnik, was visiting her son in Oxford, and delivered an invited lecture at the university, where she noticed the flyer for the BBC programme. She contacted the medical geneticists, interviewed KE family members, and returned to Montreal, Quebec. She was convinced that the genetic defect was largely centred on grammatical ability, and wrote letters to \"Nature\" in 1990. Her reports promulgated a notion of \"grammar gene\" and a controversial concept of grammar-specific disorder.\n\nNeuroscientist and language expert at the Institute of Child Health, Faraneh Vargha-Khadem, began to investigate teaming up with University of Oxford and University of Reading linguists. In 1995 they found, contrary to Gopnik's hypothesis, from comparison of 13 affected and 8 normal individuals that the genetic disorder was a complex impairment of not only linguistic ability, but also intellectual and anatomical features, thereby disproving the \"grammar gene\" notion. Using positron emission tomography (PET) and magnetic resonance imaging (MRI), they found that some brain regions were underactive (compared to baseline levels) in the KE family members and that some were overactive, when compared to normal people. The underactive regions included motor neurons that control face and mouth regions. The areas that were overactive includes Broca's area, the speech centre. With Oxford geneticists Simon Fisher and Anthony Monaco, they identified the exact location of the gene on the long arm of chromosome 7 (7q31) in 1998. The chromosomal region (locus) was named \"SPCH1\" (for speech-and-language-disorder-1), and it contains 70 genes. Using the known gene location of speech disorder from a boy, designated CS, of unrelated family, they discovered in 2001 that the main gene responsible for speech impediment in both KE family and CS was \"FOXP2\", and that this gene plays a major role in the origin and development of language. Mutations in the genes result in speech and language problems.\n\n"}
{"id": "36367419", "url": "https://en.wikipedia.org/wiki?curid=36367419", "title": "Ku Klux Klan titles and vocabulary", "text": "Ku Klux Klan titles and vocabulary\n\nKu Klux Klan nomenclature has evolved over the order's nearly 160 years of existence. The titles and designations were first laid out in the original Klan's prescripts of 1867 and 1868, then revamped with William J. Simmons' Kloran of 1916. Subsequent Klans have made various modifications.\n\nThe sources of the rituals, titles and even the name of the original KKK may be found in antebellum college fraternities and secret societies such as the Kuklos Adelphon. John Lester, one of the original members of the group, stated that the Klan rituals were \"modeled on and embraced the leading features of the rituals of an order which has long been popular in many colleges and universities under various names\" such as the Sons of Confucius or Guiasticutus but always styled Ancient and Honorable and Mirth-Provoking. Walter L. Fleming stated in a footnote to Lester's text that the contemporary (early twentieth century) Southern college fraternity that most nearly mirrored the early Klan was Alpha Sigma Sigma and the institution of snipe hunting.\nThe original prescript of the Ku Klux Klan was adopted by a convention in Nashville, Tennessee in April 1867. A slightly revised edition appeared the next year.\n\nIn both prescripts there were four levels or \"departments\" of organization, above the basic level:\n\n\nIn the first prescript each officer is given the power to appoint Deputies to organize Realms, Dominions, Provinces and Dens until the latter can elect their own officers. The Grand Wizard was to be elected by a majority of Grand Dragons, and each lower level was elected by a majority of the next lower level of officers (Dragons elected by Titans, Titans by Giants etc.), as soon as three units had been formed at each level (three Dominions within a Realm for Grand Dragon, three Provinces within a Dominion for Grand Titan etc.). In the second prescript each officer appoints the lower officer with the approval of his superior.\n\nThe Dens were the basic level of organization for the Reconstruction Klan. In the original prescript, its chief officer was the Grand Cyclops, who appointed two Nighthawks, a Grand Turk, a Grand Sentinel, Grand Magi and a Grand Ensign in addition to his Grand Scribe. The Grand Cyclops, Grand Exchequer, Grand Magi (second officer) and Grand Monk (third officer) were elected by the body politic of the dens, identified as Ghouls In the second prescript the Grand Ensign is dispensed with, while the Grand Exchequer was appointed by the Grand Cyclops, who was now appointed by the Grand Giant. Only the Grand Magi and Grand Monk were elected by the Ghouls.\n\nThe exact function of these officers and the meaning of the titles varied. The two Nighthawks have been identified as couriers. The Grand Sentinel was in charge of the \"Grand Guard\", an organization which is not otherwise elaborated upon in the prescripts, but apparently served as the Den's security detail. The Grand Turk was the den's \"executive officer\" and was charged with informing Klansmen of \"all informal or irregular meetings\" and helping the Grand Cyclops and the Grand Magi maintain the \"control and government\" of the Den. The Grand Ensigns job was to take care of the Klan's flag.\n\nThe organ for initiation into the Klan was called the Investigating Committee, composed of the Grand Cyclops, Grand Magi and Grand Monk. Upon the nomination of a new member by a current member, the committee would investigate the candidate's \"antecedents and his past and present standings and connections\" and would then pronounce the candidate \"competent and worthy\" to become a member. The Grand Turk would escort the candidate to an \"outpost\" where he would question him and administer a preliminary oath. After this the Grand Turk would conduct the candidate to the Den, where the Grand Cyclops would administer the final oath. The second prescript elaborates that the candidate must have his left hand on the Bible and his right hand toward heaven and includes a ten-point \"interrogation\" that the candidate must answer satisfactorily in order to proceed with the final oath. The questions asked included: are you now or have ever been a member of the Radical Republican Party, Loyal Leagues or Grand Army of the Republic? Did you fight with the Union Army during the Civil War? Do you believe in Negro equality? among other things.\n\nA system of \"judiciary\" organs was created in each prescript. In the first the judiciary was divided into a Grand Council of Yahoos, to try officers of the Klan, and a Grand Council of Centaurs to try regular Ghouls. In Art.IV Sec.4, the Grand Giant is charged with conducting the Council of Yahoo, but Art.VI Sec.2 states that the Council will be composed of officers of the equivalent rank as the accused and presided over by an officer of the next higher rank. A trial of the Grand Wizard would be held by a meeting of all the Grand Dragons, the most senior Grand Dragon presiding It is unclear if these tribunals were ever functional.\n\nThe second prescript presented an entirely different judiciary, with officers of the first three levels tried by three judges, appointed by the chief officer of the given jurisdiction, and the officers and attaches of the headquarters (presumably the Genii, Hydras, and Furies). Trials of Den officers would be conducted at the provincial headquarters and include five judges, and ordinary ghouls to be tried at their dens with seven judges. As before a trial of the Grand Wizard would be conducted by the most senior Grand Dragon, this time with a quorum of seven dragons. All defendants had the right of appeal to the next higher court, and the proceeding was to be governed as \"ordinary court martials\".\n\nIn addition to the structure outlined above, there are documented cases of organs of slightly different nomenclature from Reconstruction. For instance, a group of twenty men who were arrested on April 6, 1868, at their \"den\" at the corners of Beale Street and Hernando street in Memphis, was called the Supreme Cyclopean Council. The constitution that the police captured outlined an organization with a Grand Cyclops, Vice-Grand Cyclops, and Secretary and openly advocated assassination of the \"murders and robbers\" now ruling the South. Members were bound to participate in the activities of the order, even if it meant leaving the \"embraces\" of their wife. A Grand Klan, composed of delegates from Spartanburg, York, Union, and Chester counties in South Carolina and a few from North Carolina met in Spartanburg, South Carolina and declared that no more raids or whippings would be conducted by members of the Klan except by their order and that the penalty for violating this order would be 100 lashes for the first offense and death for the second.\n\nAn attempt was apparently made to make Ghoul titles for other officers in Maury County, Tennessee in 1867 and early 1868, but the result was only confusion. Additionally a \"Grand Tycoon\" notified Klansmen in Lebanon, Tennessee to cease night riding and the order was apparently effective in restoring calm to Wilson County, Tennessee. In some early reconstruction Klan units there was also a Lictor, whose role as a guard of the den was later subsumed by the Nighthawks.\n\nThe Knights of the Ku Klux Klan, Inc. which existed from 1915 to 1944, elaborated on the original prescript in its Kloran and in the constitution and by-laws adopted in 1922. Some titles and jurisdictional designations were carried over from the Reconstruction prescripts intact or slightly modified, and others were original with Imperial Wizard Simmons.\n\n\nThere was a nearly identical set of subordinate officers at each level:\n\nThe officers at each level above the local Klan unit were designated by a unique prefix: Imperial at the Empire level; Grand at the Realm level; and Great at the Province level. Thus, for instance, an Imperial Kludd would be the chaplain for the whole organization, a Grand Kludd for the Realm, Great Kludd for a Province and simply Kludd for a local Klan.\n\nNot all offices were reproduced at each level. The Kladd, Klarogo, Klexter, and Klokard were not present at the Province level, and the Kladd was not included on the Realm level. The Klazik, second vice-president, and Klonsul, attorney, positions only existed at the Imperial level. The Klazik's duties included being head of the department of Realms and organizing new Realms and Provinces At the Province level there were three Klaliffs that served as an advisory board. A Klokann of three members, each one named a Klokan, filled this role at the local levels and a Klokann of four members at the Imperial level, in which they served as a \"Supreme Board of Auditors and Special Advisers and was led by a Chief Imperial Klokan.\n\nThe Imperial Wizards Genii constituted his Imperial Kloncilium; this was the Imperial Wizards supreme advisory board, as well as the Klans highest administrative organ in between Klonvokations. It met regularly every July, but could also be called when the Imperial Wizard or five Geniis petitioned him to do so. The Kloncilium was also the Supreme Tribunal of Justice of the organization with appellate jurisdiction and the right to finally determine disputes between Realms, Provinces, Klans and members in unorganized states\n\nGiant was adopted as an emeritus title: Imperial Giant for an ex-Imperial Wizard, Grand Giant for an ex-Grand Dragon, Great Giant for ex-Great Titan and Klan Giant for an ex-Exalted Cyclops.\n\nA Kleagle was a recruiter and was responsible for organizing local Klans.\n\nSince the dissolution of the Knights of the Ku Klux Klan, Inc. in 1944, there have been many Klan groups. Details of the nomenclature have varied, somewhat, among the different groups, but some terms have had more currency than others. Over time, the term klavern replaced klan for local groups. Imperial Wizard and Grand Dragon have still been generally used for the leader of a Klan organization and for state (realm) leaders. Exceptions included cases of one state Klans, such as the Association of Georgia Klans, whose leader Dr. Samuel Green, kept the title Grand Dragon until shortly before his death. The White Knights of the Ku Klux Klan created some innovations in their organization including a bicameral Klongress with an upper house Klonvokation and lower Klanburgess.\n\nIn the late 1970s David Duke's Knights of the Ku Klux Klan dropped the Imperial Wizard title, and the leader was called national director. That organization, now known as the Knights Party, no longer uses most of the traditional Klan titles, and the only fraternal titles used are Page, Squire and Knight for levels of membership.\n\nAside from titles and geographical designations, a distinctive vocabulary has grown around the Ku Klux Klan organizations. These include names for rituals, code words, and practices of the various Klans.\n\n\nThere are terms related to membership and non-membership:\n\nGroups of Klansmen commissioned for \"special activities\"\n\n\n\nTwo KKK codes for dates and times have been developed, the Ku Klux Register in the original prescripts and the Kalender developed by William J. Simmons.\n\nIn the original prescripts the register contained twelve designations, thought to correspond to months:\n\nThe second prescript had a slightly different scheme.\n\nColors were used for the days of the week:\n\nTwelve designations were used for the hours of the day:\n\nThe Kalender developed by W. J. Simmons included codes for days of the week, weeks, months and years. For months:\n\nFor weeks and days:\n\nYears were reckoned according to reigns and cycles. The Reign of Incarnation and Incantation was all time up to the American Revolutionary War. The First Reign of our Incarnation and Incantation was the period between the Revolution and the establishment of the original Klan, which was reckoned to May 6, 1866 in this scheme. The Reign of our Second Incarnation and Incantation was reckoned between 1866 and Grand Wizard Nathan Bedford Forrest's dissolution of the Klan, which is reckoned to 1872. The Reign of our Third Incarnation and Incantation began in 1915. The Klan year, Anno Klanslar, began in March of each year, and the cycle was reckoned from December of each calendar year.\n\n\n"}
{"id": "1737517", "url": "https://en.wikipedia.org/wiki?curid=1737517", "title": "Language planning", "text": "Language planning\n\nLanguage planning (also known as language engineering) is a deliberate effort to influence the function, structure, or acquisition of languages or language variety within a speech community. It is often associated with government planning, but is also used by a variety of non-governmental organizations, such as grass-roots organizations, and individuals. Goals of such planning vary. Better communication through assimilation of a single dominant language can bring economic benefits to minorities but is also perceived to facilitate their political domination. It involves the establishment of language regulators, such as formal or informal agencies, committees, societies or academies, to design or develop new structures to meet contemporary needs.\n\nFour overarching language ideologies motivate decision making in language planning. The first, linguistic assimilation, is the belief that every member of a society, irrespective of their native language, should learn and use the dominant language of the society in which they live. An example is the English-only movement of some residents of the United States.\n\nIn contrast is the second ideology, linguistic pluralism - the recognition and support of multiple languages within one society. Examples include the coexistence of French, German, Italian, and Romansh in Switzerland; and the shared official status of English, Malay, Tamil, and Mandarin Chinese in Singapore. The coexistence of many languages may not necessarily arise from a conscious language ideology, but rather related to the relative efficiency in communication of a common language.\n\nThe third ideology, vernacularization, denotes the restoration and development of an indigenous language, along with its adoption by the state as an official language. Examples include Hebrew in the state of Israel and Quechua in Peru.\n\nThe final ideology, internationalization, is the adoption of a non-indigenous language as a means of wider communication, as an official language or in a particular domain, such as the use of English in India, Singapore, the Philippines, Papua New Guinea, and South Africa.\n\nEleven language planning goals have been recognized (Nahir 2003):\n\nLanguage planning has been divided into three types:\n\nStatus planning is the allocation or reallocation of a language or variety to functional domains within a society, thus affecting the status, or standing, of a language.\n\nLanguage status is a concept distinct from, though intertwined with, language prestige and language function. Strictly speaking, language status is the position or standing of a language vis-à-vis other languages. A language garners status according to the fulfillment of four attributes, described in 1968 by two different authors, Heinz Kloss and William Stewart. Both Kloss and Stewart stipulated four qualities of a language that determine its status. Their respective frameworks differ slightly, but they emphasize four common attributes:\n\nTogether, origin, degree of standardization, juridical status, and vitality express a language's status.\n\nWilliam Stewart outlines ten functional domains in language planning: \n\nRobert Cooper, in reviewing Stewart's list, makes several additions. First, he creates three sub-types of official functions: statutory, working, and symbolic. A statutory language is a language that a government has declared official by law. A working language is a language that a government uses as a medium for daily activities, and a symbolic language is a language that is a symbol of the state. Cooper also adds two functional domains to Stewart's list: mass media and work.\n\nCorpus planning refers to the prescriptive intervention in the forms of a language, whereby planning decisions are made to engineer changes in the structure of the language. Corpus planning activities often arise as the result of beliefs about the adequacy of the form of a language to serve desired functions. Unlike status planning, which is primarily undertaken by administrators and politicians, corpus planning generally is performed by individuals with greater linguistic expertise. There are three traditionally recognized types of corpus planning: graphization, standardization, and modernization.\n\nGraphization refers to development, selection and modification of scripts and orthographic conventions for a language. The use of writing in a speech community can have lasting sociocultural effects, which include easier transmission of material through generations, communication with larger numbers of people, and a standard against which varieties of spoken language are often compared. Linguist Charles A. Ferguson made two key observations about the results of adopting a writing system. First, the use of writing adds another variety of the language to the community's repertory. Although written language is often viewed as secondary to spoken language, the vocabulary, grammatical structures and phonological structures of a language often adopt characteristics in the written form that are distinct from the spoken variety. Second, the use of writing often leads to a folk belief that the written language is the 'real' language, and speech is a corruption of it. Written language is viewed as more conservative, while the spoken variety is more susceptible to language change. Isolated relic areas of the spoken language may be less innovative than the written form, or the written language may have been based on a divergent variety of the spoken language.\n\nIn establishing a writing system for a language, corpus planners have the option of using an existing system or creating a new one. The Ainu of Japan chose to adopt the Japanese language's katakana syllabary as the writing system for the Ainu language. Katakana is designed for a language with a basic CV syllable structure, but Ainu contains many CVC syllables that cannot easily be adapted to this syllabary. As a result, Ainu uses a modified katakana system, in which syllable-final codas are consonants by a subscript version of a katakana symbol that begins with the desired consonant.\n\nAn example on a created script includes the development of the Armenian script in 405 AD by St. Mesrop Mashtots. Though the script was modeled after the Greek alphabet, the original script distinguished Armenian from the Greek and Syriac alphabets of the neighboring peoples. Similarly, in the early 19th century, Sequoyah (Cherokee) created an orthography for written Cherokee in the Southeast of the present-day United States. It uses some Latin characters but also introduces new ones.\n\nThe process of Standardization often involves one variety of a language taking precedence over other social and regional dialects of a language. Another approach, where dialects are mutually intelligible, is to introduce a poly-phonemic written form that is intended to represent all dialects of a language adequately but with no standard spoken form. If one variety of a language is chosen, that variety comes to be understood as supra-dialectal and the 'best' form of the language.\n\nThe choice of which language takes precedence has important societal consequences, as it confers privilege upon speakers whose spoken and written dialect conforms closest to the chosen standard. The standard that is chosen as the norm is generally spoken by the most powerful social group within the society, and is imposed upon the less powerful groups as the form to emulate. This often reinforces the dominance of the powerful social group and makes the standard norm necessary for socioeconomic mobility. In practice, standardization generally entails increasing the uniformity of the norm, as well as the codification of the norm.\n\nThe history of English provides an example of standardization occurring over an extended time period, without formally recognized language planning. The standardization process began when William Caxton introduced the printing press in England in 1476. This was accompanied by the adoption of the south-east Midlands variety of English, spoken in London, as the print language. Because of the dialect's use for administrative, government, business, and literary purposes, this variety became entrenched as the prestigious variety of English. After the creation of grammars and dictionaries in the 18th century, the rise of print capitalism, industrialization, urbanization, and mass education led to the dissemination of this dialect as the standard norm for the English language.\n\nModernization is a form of language planning that occurs when a language needs to expand its resources to meet functions. Modernization often occurs when a language undergoes a shift in status, such as when a country gains independence from a colonial power or when there is a change in the language education policy. The most significant force in modernization is the expansion of the lexicon, which allows the language to discuss topics in modern semantic domains. Language planners generally focus on creating new lists and glossaries to describe new technical terms, but it is also necessary to ensure that the new terms are consistently used by the appropriate sectors within society. While some languages, such as Japanese and Hungarian, have experienced rapid lexical expansion to meet the demands of modernization, other languages, such as Hindi and Arabic, have failed to do so. Rapid lexical expansion is aided by the use of new terms in textbooks and professional publications, as well as frequent use among specialists. Issues of linguistic purism often play a significant role in lexical expansion, but technical vocabulary can be effective within a language, regardless of whether it comes from the language's own process of word formation or from extensive borrowing from another language. While Hungarian has almost exclusively used language-internal processes to create new lexical items, Japanese has borrowed extensively from English to derive new words as part of its modernization.\n\nAcquisition planning is a type of language planning in which a national, state or local government system aims to influence aspects of language, such as language status, distribution and literacy through education. Acquisition planning can also be used by non-governmental organizations, but it is more commonly associated with government planning.\n\nFrequently, acquisition planning is integrated into a larger language planning process in which the statuses of languages are evaluated, corpuses are revised and the changes are finally introduced to society on a national, state or local level through education systems, ranging from primary schools to universities. This process of change can entail a variety of modifications, such as an alteration in student textbook formatting, a change in methods of teaching an official language or the development of a bilingual language program, only to name a few. For example, if a government decides to raise the status level of a certain language or change its level of prestige, it can establish a law that requires teachers to teach only in this language or that textbooks are written using only this language's script. This, in turn, would support the elevation of the language's status or could increase its prestige. In this way, acquisition planning is often used to promote language revitalization, which can change a language's status or reverse a language shift, or to promote linguistic purism. In a case where a government revises a corpus, new dictionaries and educational materials will need to be revised in schools in order to maintain effective language acquisition.\n\nThe education ministry or education sector of government is typically in charge of making national language acquisition decisions based on state and local evaluation reports. The responsibilities of education sectors vary by country; Robert B. Kaplan and Richard B. Baldauf describe the sectors' six principal goals:\n\n\nAlthough acquisition planning can be useful to governments, there are several problems that must be considered. Even with a solid evaluation and assessment system, the effects of planning methods can never be certain; governments must consider the effects on other aspects of state planning, such as economic and political planning. Some proposed acquisition changes could also be too drastic or instituted too suddenly without proper planning and organization. Acquisition planning can also be financially draining, so adequate planning and awareness of financial resources is essential. It is important therefore that government goals, such as those described above, be organized and planned carefully.\n\nThere is also a growing concern over the treatment of multilingualism in education, especially in many countries that were once colonized. Deciding on which language of instruction would be most beneficial to effective communication on the local and state level is a task requiring thoughtful planning and is surrounded by debate. Some states prefer instruction only in the official language, but some aim to foster linguistic and thus social diversity by encouraging teaching in several (native) languages. One reason some states prefer a single language of instruction is that it supports national unity and homogeneity. Some states prefer incorporating different languages in order to help students learn better by giving them diverse perspectives.\n\nIn addition to the education sector, there are non-governmental sectors or organizations that have a significant effect on language acquisition, such as the Académie française of France or the Real Academia Española of Spain. These organizations often create their own dictionaries and grammar books, thus affecting the materials which students are exposed to in schools. Although these organizations do not hold official power, they influence government planning decisions, such as with educational materials, affecting acquisition.\n\nBefore the partition of Ireland, a movement began which aimed at the restoration of Irish, as the nation's primary language, based on a widespread sentiment for Irish nationalism and cultural identity. During and after colonisation, Irish had competed with English and Scots; the movement to restore the language gained momentum after the Irish War of Independence. The Gaelic League was created to promote the acquisition of Irish in schools, thus \"de-Anglicizing\" Ireland. Immediately after Ireland gained independence in 1922, the League declared that Irish must be the language of instruction for at least one hour in primary schools nationwide. Irish-speaking teachers were recruited, and preparatory colleges were established to train new ones.\n\nThe program implementation was mostly left to the individual schools, which did not consistently carry it out. Additionally, educating a generation is a long process, for which the League was not prepared. There was no consensus as to how the Irish language should be reinstituted; the League and schools did not develop a system assessment plan to monitor progress. As a result, the movement lost strength, and the number of native Irish speakers has been in steady decline.\n\nPeru's history of language planning begins in the 16th century with Spanish colonization. When the Spanish first arrived in Peru, Quechua served as a language of wider communication, a lingua franca, between Spaniards and Peruvian natives. As the years passed, Spaniards asserted the superiority of the Spanish language; as a result, Spanish gained prestige, taking over as a language of wider communication and the dominant language of Peru. In 1975, under the leadership of President Juan Velasco Alvarado, the revolutionary government of Peru declared Quechua an official language of the Peruvian state, \"coequal with Spanish.\" Four years later, the law was reversed. Peru's 1979 constitution declares Spanish the only official language of the state; Quechua and Aymara are relegated to \"official use zones,\" equivalent to Stewart's provincial function described above. Quechua has officially remained a provincial language since 1979. Today, Quechua also serves a limited international function throughout South America in Argentina, Bolivia, Brazil, Chile, Colombia, and Ecuador; communities of Quechua speakers outside Peru enable communication in Quechua across borders. Still, because of Quechua's low status, Spanish is almost always used as the lingua franca instead. Recently, Quechua has also gained ground in the academic world, both as a school subject and a topic of literary interest.\n\nThe three main types of corpus planning are all evident in the development of Quechua languages in Peru since the colonial era. Graphization has been in process since the arrival of the Spanish in the region, when the Spanish imperialists attempted to describe the exotic sounds of the language to Europeans.\n\nWhen Quechua was made an official language in Peru in 1975, the introduction of the language into the education and government domains made it essential to have a standard written language. The task of adopting a writing system proved to be a point of contention among Peruvian linguists. Although most agreed to use the Latin alphabet, linguists disagreed about how to represent the phonological system of Quechua, particularly in regards to the vowel system. Representatives from the Peruvian Academy of the Quechua language and the Summer Institute of Linguistics wanted to represent allophones of the vowels /i/ and /u/ with separate letters <e> and <o>, which creates an apparent five-vowel system. They argued that this makes the language easier to learn for people who are already familiar with written Spanish. However, other Peruvian linguists argued that a three-vowel system was more faithful to the phonology of Quechua. After several years of debate and disagreement, in 1985 Peruvian linguists proposed the Pan-Quechua alphabet as an accurate representation of the language, and this was adopted in intercultural bilingual education programs and textbooks. However, the Peruvian Academy and the SIL both refused to adopt it and continued to propose new alphabets, leaving the issue unsettled. For more information, see Quechua writing system and Quechuan and Aymaran spelling shift. Another of the primary issues disagreements was about how to reflect the phonological differences apparent in different dialects of Quechua. For example, some distinct dialects utilize aspirated and glottalized versions of the voiceless uvular stop /q/, while others do not and some language planners found it important to reflect these dialectal differences.\n\nThe search for a unified alphabet reflects the process of standardization. Unlike other cases of standardization, in Quechua this has been applied only to the written language, not to the spoken language, and no attempt was made to change the spoken language of native speakers, which varied by regions. Rather, standardization was required in order to create a uniform writing system to provide education to Quechua speakers in their native language.\n\nLanguage planners in Peru have proposed several varieties to serve as the supradialectal spoken norm. Some saw Qusqu-Qullaw as the natural choice for a standard norm, as it is recognized to be the variety closest to that spoken by the Incas. Others argued that Ayacucho Quechua is a better option, as the language is more conservative and similar to the proto-language, while Qusqu-Qullaw has been influenced by contact with the Aymara language.\n\nRodolfo Cerrón-Palomino proposed a standard literary norm, Southern Quechua that combines features of both these dialects. This norm has been accepted by many institutions in Peru.\n\nLexical modernization has also been critical to the development of Quechua. Language planners have attempted to create new Quechua words primarily through Quechua morphemes, which are combined in new ways to give new meanings. In general, loanwords from other languages are considered only when there are no possibilities to develop the word through existing Quechua structures. If loanwords are adopted into the language, linguists attempt to phonologically adapt the word to match typical Quechua pronunciation norms.\n\nSince Quechua is no longer an official language of Peru, Quechua literacy is not consistently encouraged in schools. Peru's education system is instead primarily based on Spanish, the nation's official language. Despite its low prestige, Quechua is still spoken by millions of indigenous Peruvians, a large portion of whom are bilingual in Quechua and Spanish. There is a desire to preserve the uniqueness of Quechua as a language with its own attributes and representations of culture. Some argue that promoting a diverse literacy program gives students diverse perspectives on life, which could only enhance their educational experience. Before 1975, Peru had bilingual education programs, but Quechua was not taught as a subject in primary and secondary schools. After the 1975 education reform, Quechua and Spanish both had standing in bilingual programs, but only in restricted speech communities. These experimental programs were then canceled due to a change in government planning, but again reinstated in 1996. Even with national intercultural bilingual education programs, teachers at local schools and members of the community often prefer using Spanish, destabilizing support for bilingual education. This underscores the importance of community support as a goal for the education sector as mentioned earlier. Some believe that due to Spanish's higher national prestige, it is more socially and economically beneficial to learn and speak Spanish. It is debatable whether these education programs will benefit education or raise the status of Quechua.\n\n\n\n\n"}
{"id": "19083749", "url": "https://en.wikipedia.org/wiki?curid=19083749", "title": "Level of detail (writing)", "text": "Level of detail (writing)\n\nLevel of detail in writing, sometimes known as level of abstraction, refers to three concepts: the precision in using the right words to form phrases, clauses and sentences; the generality of statements; and the organisational strategy in which authors arrange ideas according to a common topic in the hierarchy of detail. Placing different objects or ideas in categories is a type of classification in expert writing which allows more efficient cognitive retrieval of information by placing it in context. Maintaining appropriate level of detail in any body of text is a part of ensuring that the cognitive effort required by the reader is appropriate to the general subject of the written as a whole. Authors use level of detail to maintain continuity in syntactic hierarchy in texts, such as a screenplays. Continuity in text is achieved by using transitional expressions to move from one detail, or level of detail, to another.\n\nWithin the basic writing structure of \"introducing\", \"characterising\" and \"bringing to a close\" of any proper subject description, level of detail is used in theme development during \"elaboration\", \"evaluation\" and \"adding context\" as a repertoire of retrieval strategies. Van der Pool in 1995 had found that omission of detail in text structuring is an age-related effect that differentiates mature and young writers.\n\nAlthough the general rule that the level of detail must be both sufficient and appropriate\nfor the author's audience and their subject in literature intended for experts, it is also used in primary and secondary education to assess student understanding. In general the depth of detail is gradually developed to one appropriate for the subject.\n\nThe suggested list in identifying appropriate level of detail may include\n\nPresenting the reader with specific details without first introducing it with general statements can be dangerous because it omits a qualifier, and therefore introduces elements that invite questions and create confusion.\n\nLevel of detail is often important in technical writing due to the need to differentiate between different levels of audience need for information within the organisation.\n\nSimilar to the engineering design process, writing also takes place by the author usually adopting either a top-down or a bottom-up process by identifying components of the text that become the focus subjects in the overall theme.\n\nThe content of a text is often assessed for its level of detail as \"high\", \"intermediate\" or \"low\" based on the objective of the author in addressing the needs of the audience. The highly detailed text refers to the bottom-up structuring design where \nFacts that are generally available but not used frequently (and likely to be forgotten) should be included in this ideal level of writing.\n\n"}
{"id": "3331706", "url": "https://en.wikipedia.org/wiki?curid=3331706", "title": "Logical form", "text": "Logical form\n\nIn philosophy and mathematics, a logical form of a syntactic expression is a precisely-specified semantic version of that expression in a formal system. Informally, the logical form attempts to formalize a possibly ambiguous statement into a statement with a precise, unambiguous logical interpretation with respect to a formal system. In an ideal formal language, the meaning of a logical form can be determined unambiguously from syntax alone. Logical forms are semantic, not syntactic constructs; therefore, there may be more than one string that represents the same logical form in a given language.\n\nThe logical form of an argument is called the argument form or \"test form\" of the argument.\n\nThe importance of the concept of form to logic was already recognized in ancient times. Aristotle, in the Prior Analytics, was probably the first to employ variable letters to represent valid inferences. Therefore, Łukasiewicz claims that the introduction of variables was 'one of Aristotle's greatest inventions'.\n\nAccording to the followers of Aristotle like Ammonius, only the logical principles stated in schematic terms belong to logic, and not those given in concrete terms. The concrete terms \"man\", \"mortal\", etc., are analogous to the substitution values of the schematic placeholders 'A', 'B', 'C', which were called the 'matter' (Greek \"hyle\", Latin \"materia\") of the argument.\n\nThe term \"logical form\" itself was introduced by Bertrand Russell in 1914, in the context of his program to formalize natural language and reasoning, which he called philosophical logic. Russell wrote: \"Some kind of knowledge of logical forms, though with most people it is not explicit, is involved in all understanding of discourse. It is the business of philosophical logic to extract this knowledge from its concrete integuments, and to render it explicit and pure.\"\nTo demonstrate the important notion of the \"form\" of an argument, substitute letters for similar items throughout the sentences in the original argument.\n\nAll we have done in the \"Argument form\" is to put 'H' for 'human' and 'humans', 'M' for 'mortal', and 'S' for 'Socrates'; what results is the \"form\" of the original argument. Moreover, each individual sentence of the \"Argument form\" is the \"sentence form\" of its respective sentence in the original argument.\n\nAttention is given to argument and sentence form, because \"form is what makes an argument valid or cogent\". All logical form arguments are either inductive or deductive. Inductive logical forms include inductive generalization, statistical arguments, causal argument, and arguments from analogy. Common deductive argument forms are hypothetical syllogism, categorical syllogism, argument by definition, argument based on mathematics, argument from definition. The most reliable forms of logic are modus ponens, modus tollens, and chain arguments because if the premises of the argument are true, then the conclusion necessarily follows. Two invalid argument forms are affirming the consequent and denying the antecedent. \n\n\n\nA logical argument, seen as an ordered set of sentences, has a logical form that derives from the form of its constituent sentences; the logical form of an argument is sometimes called argument form. Some authors only define logical form with respect to whole arguments, as the schemata or inferential structure of the argument. In argumentation theory or informal logic, an argument form is sometimes seen as a broader notion than the logical form.\n\nIt consists of stripping out all spurious grammatical features from the sentence (such as gender, and passive forms), and replacing all the expressions specific to \"the subject matter\" of the argument by schematic variables. Thus, for example, the expression 'all A's are B's' shows the logical form which is common to the sentences 'all men are mortals', 'all cats are carnivores', 'all Greeks are philosophers' and so on.\n\nThe fundamental difference between modern formal logic and traditional, or Aristotelian logic, lies in their differing analysis of the logical form of the sentences they treat: \n\nThe more complex modern view comes with more power. On the modern view, the fundamental form of a simple sentence is given by a recursive schema, like natural language and involving logical connectives, which are joined by juxtaposition to other sentences, which in turn may have logical structure. Medieval logicians recognized the problem of multiple generality, where Aristotelian logic is unable to satisfactorily render such sentences as \"Some guys have all the luck\", because both quantities \"all\" and \"some\" may be relevant in an inference, but the fixed scheme that Aristotle used allows only one to govern the inference. Just as linguists recognize recursive structure in natural languages, it appears that logic needs recursive structure.\n\n\n\n"}
{"id": "1383618", "url": "https://en.wikipedia.org/wiki?curid=1383618", "title": "Mama and papa", "text": "Mama and papa\n\nIn linguistics, mama and papa are considered a special case of false cognates. In many languages of the world, sequences of sounds similar to and mean \"mother\" and \"father\", usually but not always in that order. This is thought to be a coincidence resulting from the process of early language acquisition.\n\nThese terms use speech sounds that are among the easiest to produce: bilabials like , , and , and the open vowel . They are, therefore, often among the first word-like sounds made by babbling babies (babble words), and parents tend to associate the first sound babies make with themselves and to employ them subsequently as part of their baby-talk lexicon. Thus, there is no need to ascribe to common ancestry the similarities of !Kung \"ba\", Aramaic \"abba\", Mandarin Chinese \"bàba\", and Persian \"baba\" (all \"father\"); or Navajo \"amá\", Mandarin Chinese \"māma\", Swahili \"mama\", Quechua \"mama\", and Polish \"mama\" (all \"mother\"). For the same reason, some scientists believe that 'mama' and 'papa' were among the first words that humans spoke. However, there is nothing of motherhood or fatherhood inherent in the sounds.\n\nVariants using other sounds do occur: for example, in Fijian, the word for \"mother\" is \"nana\", the Turkish word is \"ana\", and in Old Japanese, the word for \"mother\" was \"papa\". The modern Japanese word for \"father,\" \"chichi\", is from older \"titi\". Very few languages lack labial consonants (this mostly being attested on a family basis, in the Iroquoian and some of the Athabaskan languages), and only Arapaho is known to lack an open vowel /a/. The Tagalog \"-na-\" / \"-ta-\" (\"mom\" / \"dad\" words) parallel the more common \"ma\" / \"pa\" in nasality / orality of the consonants and identity of place of articulation. \n\nThe linguist Roman Jakobson hypothesized that the nasal sound in \"mama\" comes from the nasal murmur that babies produce when breastfeeding:\n\n'Mama' and 'papa' in different languages:\n\n\n\n\n\n\n\n\nIn the Proto-Indo-European language, \"*mā́tēr\" (modern reconstruction: \"*méh₂tēr\") meant \"mother\" and \"*pǝtḗr\" (modern reconstruction: \"*ph₂tḗr\") meant \"father\", and \"átta\" meant \"papa\", a nursery word for \"father\".\n\n\n\n\n\n\n\n"}
{"id": "1984214", "url": "https://en.wikipedia.org/wiki?curid=1984214", "title": "Martian scientist", "text": "Martian scientist\n\nA Martian scientist or Martian researcher is a hypothetical Martian frequently used in thought experiments as an outside observer of conditions on Earth. The most common variety is the Martian anthropologist, but Martians researching subjects such as philosophy, linguistics and biology have also been invoked.\n\nThe following extract from an essay by Richard Dawkins is more or less typical.\n\nIn American structuralist linguistics, the Martian approach is recommended for language description:\n\nThe hypothetical Martian anthropologist is described in the writings of Noam Chomsky as one who, upon studying the world's languages, would conclude that they are all dialects of a single language embodying a \"universal grammar\" reflecting a hardwired, genetically determined linguistic module inherent in the human brain.\n\nIn philosophy, especially philosophy of language and philosophy of mind, the Martian is often invoked as an example of an intelligent being with a cognitive apparatus that differs from that of humans, e.g. the following example given by Saul Kripke:\n\nIn a common rhetorical turn, invoking the Martian scientist forces the reader to observe an obvious state of affairs that is ordinarily overlooked:\n\nExtraordinary World War II era Hungarian scientists who emigrated to the United States in the early half of the 20th century. The most prominent Martians (scientists) included Theodore von Kármán, John von Neumann, Eugene Wigner, and Edward Teller. They were referred to as \"Martians\" due to their brilliant problem solving and invention talents that seemed \"out-of-this world \".\n"}
{"id": "4153402", "url": "https://en.wikipedia.org/wiki?curid=4153402", "title": "Meaning (semiotics)", "text": "Meaning (semiotics)\n\nIn semiotics, the meaning of a sign is its place in a sign relation, in other words, the set of roles that it occupies within a given sign relation. \n\nThis statement holds whether \"sign\" is taken to mean a \"sign type\" or a \"sign token\". Defined in these global terms, the meaning of a sign is not in general analyzable with full exactness into completely localized terms, but aspects of its meaning can be given approximate analyses, and special cases of sign relations frequently admit of more local analyses.\n\nTwo aspects of meaning that may be given approximate analyses are the \"connotative relation\" and the \"denotative relation\". The connotative relation is the relation between signs and their interpretant signs. The denotative relation is the relation between signs and objects. An arbitrary association exists between the \"signified\" and the \"signifier.\"\nFor example, a US salesperson doing business in Japan might interpret silence following an offer as rejection, while to Japanese negotiators silence means the offer is being considered. This difference in interpretations represents a difference in: semiotics\n\nThe triadic (three part) model of the sign separates the meaning of a sign into three distinct components:\n\n1.The representamen, which is the medium, or ‘sign vehicle’, through which the sign is represented. For example, this could be written/spoken words, a photograph, or a painting.\n\n2.The interpretant, or what is meant by the sign \n\n3. The object, or that to which the sign refers \n\nTogether, these three components generate semiosis. For example, an exclamation mark can be broken down into these components. The representamen is the exclamation mark itself, the interpretant is the idea of excitement or an elevated volume of speech, and the object is the actual excitement or elevated volume of speech to which it refers.\nWhile it might appear that the latter two are the same, the subtle difference lies in the fact that the interpretant refers to the idea of something, and the object is the thing itself.\n\nThe representamen component of the sign can be further broken down into three categories, which are icon, index, and symbol. These denote the degree of abstraction from the object to which they refer. A symbol, which is the most abstract, does not resemble or bear any physical relation to the thing that it represents in any way. For example, a peace sign has no relation to peace aside from its social construction as a symbol that represents it. An icon is slightly less abstract, and resembles to some degree the thing that it represents, and bears some physical likeness to it. A good example of this would be a painted portrait. An index is the least arbitrary category of representamen, and has a definite physical tie to that which it represents. This could be something like a weather vane blowing in the wind indicating that it is windy out, or smoke, which indicates a fire.\n\nThe triadic model of the sign was proposed by Charles Peirce. In contradistinction to Ferdinand de Saussure's dyadic model, which assumed no material referent, Peirce's model assumes that in order for a sign to be meaningful, it must refer to something external and cannot be self-contained, as it is for Saussure. Thus, Peirce's model includes the addition of an 'object'. The ‘representamen’ and ‘interpretant’ components of the triadic model are comparable to Saussure’s dyadic model of the sign, which breaks down into signifier and signified.\n\n\n"}
{"id": "7623580", "url": "https://en.wikipedia.org/wiki?curid=7623580", "title": "Mission statement", "text": "Mission statement\n\nA mission statement is a short statement of an organization's purpose, identifying the goal of its operations: what kind of product or service it provides, its primary customers or market, and its geographical region of operation. It may include a short statement of such fundamental matters as the organization's values or philosophies, a business's main competitive advantages, or a desired future state—the \"vision\".\n\nA mission is not simply a description of an organization by an external party, but an expression, made by its leaders, of their desires and intent for the organization. The purpose of a mission statement is to focus and direct the organization itself. It communicates primarily to the people who make up the organization—its members or employees—giving them a shared understanding of the organization's intended direction. Organizations normally do not change their mission statements over time, since they define their continuous, ongoing purpose and focus.\n\nAccording to Chris Bart, professor of strategy and governance at McMaster University, a commercial mission statement consists of three essential components:\n\n\nBart estimates that in practice, only about ten percent of mission statements say something meaningful. For this reason, they are widely regarded with contempt.\n\nThe sole purpose of a mission statement is to serve as a company's goal/agenda, it outlines clearly what the goal of the company is. Some generic examples of mission statements would be, \"To provide the best service possible within the banking sector for our customers.\" or \"To provide the best experience for all of our customers.\" The reason why businesses make use of mission statements is to make it clear what they look to achieve as an organization, not only to themselves and their employees but to the customers and other people who are a part of the business, such as shareholders. As a company evolves, so will their mission statement. This is to make sure that the company remains on track and to ensure that the mission statement does not lose its touch and become boring or stale.\n\nIt is important that a mission statement is not confused with a vision statement. As discussed earlier, the main purpose of a mission statement is to get across the ambitions of an organisation in a short and simple fashion, it is not necessary to go into detail for the mission statement which is evident in examples given. The reason why it is important that a mission statement and vision statement are not confused is because they both serve different purposes. Vision statements tend to be more related to strategic planning and lean more towards discussing where a company aims to be in the future.\n\nProvides direction: Mission statements are a way to direct a business into the right path. They play a part in helping the business make better decisions which can be beneficial to them. Without the mission statement providing direction, businesses may struggle when it comes to making decisions and planning for the future. This is why providing direction could be considered one of the most advantageous points of a mission statement.\n\nClear purpose: Having a clear purpose can remove any potential ambiguities that can surround the existence of a business. People who are interested in the progression of the business, such as stakeholders, will want to know that the business is making the right choices and progressing more towards achieving their goals, which will help to remove any doubt the stakeholders may have in the business.\n\nA mission statement can act as a motivational tool within an organisation, and it can allow employees to all work towards one common goal that benefits both the organisation and themselves. This can help with factors such as employee satisfaction and productivity. It is important that employees feel a sense of purpose. Giving them this sense of purpose will allow them to focus more on their daily tasks and help them to realise the goals of the organisation and their role.\n\nAlthough it is mostly beneficial for a business to craft a good mission statement, there are some situations where a mission statement can be considered pointless or not useful to a business.\n\nUnrealistic: In most cases, mission statements turn out to be unrealistic and far too optimistic. An unrealistic mission statement can also affect the performance and morale of the employees within the workplace. This is because an unrealistic mission statement would reduce the likelihood of employees being able to meet this standard which could demotivate employees in the long term. Unrealistic mission statements also serve no purpose and can be considered a waste of management's time. Another issue which could arise from an unrealistic mission statement is that poor decisions could be made in an attempt to achieve this goal which has the potential to harm the business and be seen as a waste of both time and resources.\n\nWaste of time and resources: Mission statements require planning. This takes time and effort for those who are responsible for creating the mission statement. If the mission statement is not achieved, then the process of creating the mission statement could be seen as a waste of time for all of the people involved. A lot of thought and time can be spent in designing a good mission statement, and to have all of that time wasted is not what businesses can afford. The wasted time could have been spent on much more important tasks within the organisation such as decision-making for the business.\n\nAccording to an independent contributor to Forbes, the following questions must be answered in the mission statement:\n\nWhen designing a mission statement, it should be very clear to the audience what the purpose of it is. It is ideal for a business to be able to communicate their mission, goals and objectives to the reader without including any unnecessary information through the mission statement.\n\nRichard Branson has commented on ways of crafting a good mission statement; he explains the importance of having a mission statement that is clear and straight to the point and does not contain unnecessary baffling. He went on to analyse a mission statement, using Yahoo's mission statement at the time (2013) as an example. In his evaluation of the mission statement, he seemed to suggest that while the statement sounded interesting, most people would not be able to understand the message it is putting across. In other words the message of the mission statement potentially meant nothing to the audience.\n"}
{"id": "5650454", "url": "https://en.wikipedia.org/wiki?curid=5650454", "title": "Montevideo Resolution", "text": "Montevideo Resolution\n\nResolution IV.4.422-4224, commonly referred to as the Montevideo Resolution, is a resolution passed in Montevideo, Uruguay on December 10, 1954 by the General Conference of UNESCO. The resolution officially supports the constructed language Esperanto as an international auxiliary language and recommends that the Director-General of UNESCO follow current developments in the use of the language. The Montevideo Resolution was the result of a long campaign by Ivo Lapenna.\n\nIn 1977, the Director-General visited the World Esperanto Congress in Reykjavík, Iceland, and in 1985, UNESCO passed a further resolution recommending that member countries encourage the teaching of Esperanto.\n\n\"General Conference of Unesco. Eight session. Montevideo (Uruguay), 1954.\nResolution adopted on December 10, 1954, in the 18th plenary meeting.\"\n\n\n\n"}
{"id": "871470", "url": "https://en.wikipedia.org/wiki?curid=871470", "title": "Mutual intelligibility", "text": "Mutual intelligibility\n\nIn linguistics, mutual intelligibility is a relationship between languages or dialects in which speakers of different but related varieties can readily understand each other without prior familiarity or special effort. It is sometimes used as an important criterion for distinguishing languages from dialects, although sociolinguistic factors are often also used.\n\nIntelligibility between languages can be asymmetric, with speakers of one understanding more of the other than speakers of the other understanding the first. When it is relatively symmetric, it is characterized as \"mutual\". It exists in differing degrees among many related or geographically proximate languages of the world, often in the context of a dialect continuum.\n\nLinguistic distance is the name for the concept of calculating a measurement for how different languages are from one another. The higher the linguistic distance, the lower the mutual intelligibility. One common metric used is the Levenshtein distance.\n\nFor individuals to achieve moderate proficiency or understanding in a language (called L2) other than their first language (L1) typically requires considerable time and effort through study and/or practical application. However, many groups of languages are partly mutually intelligible, i.e. most speakers of one language find it relatively easy to achieve some degree of understanding in the related language(s). Often the languages are genetically related, and they are likely to be similar to each other in grammar, vocabulary, pronunciation, or other features.\n\nIntelligibility among languages can vary between individuals or groups within a language population according to their knowledge of various registers and vocabulary in their own language, their exposure to additional related languages, their interest in or familiarity with other cultures, the domain of discussion, psycho-cognitive traits, the mode of language used (written vs. oral), and other factors.\n\nThere is no formal distinction between two distinct languages and two varieties of a single language, but some linguists use mutual intelligibility as one of the primary factors in deciding between the two cases.\n\nSome linguists claim that mutual intelligibility is, ideally at least, the primary criterion separating languages from dialects. On the other hand, speakers of closely related languages can often communicate with each other; thus there are varying degrees of mutual intelligibility, and often other criteria are also used. As an example, in the case of a linear dialect continuum that shades gradually between varieties, where speakers near the center can understand the varieties at both ends, but speakers at one end cannot understand the speakers at the other end, the entire chain is often considered a single language. If the central varieties then die out and only the varieties at both ends survive, they may then be reclassified as two languages, even though no actual language change has occurred.\n\nIn addition, political and social conventions often override considerations of mutual intelligibility. For example, the varieties of Chinese are often considered a single language even though there is usually no mutual intelligibility between geographically separated varieties. Another similar example would be varieties of Arabic. In contrast, there is often significant intelligibility between different Scandinavian languages, but as each of them has its own standard form, they are classified as separate languages. There is also significant intelligibility between Thai languages of different regions of Thailand.\n\nTo deal with the conflict in cases such as Arabic, Chinese and German, the term \"Dachsprache\" (a sociolinguistic \"umbrella language\") is sometimes seen: Chinese and German are languages in the sociolinguistic sense even though some speakers cannot understand each other without recourse to a standard or prestige form.\n\nAsymmetric intelligibility refers to two languages that are considered partially mutually intelligible, but where one group of speakers has more difficulty understanding the other language than the other way around. There can be various reasons for this. If, for example, one language is related to another but has simplified its grammar, the speakers of the original language may understand the simplified language, but less vice versa. For example, Dutch speakers tend to find it easier to understand Afrikaans than vice versa as a result of Afrikaans's simplified grammar.\n\nPerhaps the most common reason for apparent asymmetric intelligibility is that speakers of one variety have more exposure to the other than vice versa. For example, speakers of Scottish English have frequent exposure to standard American English through movies and TV programs, whereas speakers of American English have little exposure to Scottish English; hence, American English speakers often find it difficult to understand Scottish English or, especially, Scots (which differs significantly from standard Scottish English), whereas Scots tend to have few problems understanding standard American English.\n\nNorthern Germanic languages spoken in Scandinavia form a dialect continuum where two furthermost dialects have almost no mutual intelligibility. As such, spoken Danish and Swedish normally have low mutual intelligibility, but Swedes in the Öresund region (including Malmö and Helsingborg), across a strait from the Danish capital Copenhagen, understand Danish somewhat better, largely due to the proximity of the region to Danish-speaking areas (see \"Mutual intelligibility in North Germanic languages\"). While Norway was under Danish rule, the Bokmål written standard of Norwegian originates from Dano-Norwegian, a koiné that evolved among the urban elite in Norwegian cities during the later years of the union. Additionally, Norwegian assimilated a considerable amount of Danish vocabulary as well as traditional Danish expressions. As a consequence, spoken mutual intelligibility is not reciprocal.\n\nSimilarly, in Germany and Italy, standard German or Italian speakers may have great difficulty understanding the \"dialects\" from regions other than their own, but virtually all \"dialect\" speakers learn the standard languages in school and from the media.\n\nBelow is an \"incomplete\" list of fully and partially mutually intelligible varieties sometimes considered languages.\n\n\n\n\n\n\nBecause of the difficulty of imposing boundaries on a continuum, various counts of the Romance languages are given; in \"The Linguasphere register of the world’s languages and speech communities\" David Dalby lists 23 based on mutual intelligibility:\n\n\n\n"}
{"id": "4804623", "url": "https://en.wikipedia.org/wiki?curid=4804623", "title": "Mythical origins of language", "text": "Mythical origins of language\n\nThere have been many accounts of the origin of language in the world's mythologies and other stories pertaining to the origin of language, the development of language and the reasons behind the diversity in languages today.\n\nThese myths have similarities, recurring themes, and differences, having been passed down through oral tradition. Some myths go further than just storytelling and are religious, with some even having a literal interpretation even today. Recurring themes in the myths of language dispersal are floods and catastrophes. Many stories tell of a great deluge or flood which caused the peoples of the Earth to scatter over the face of the planet. Punishment by a god or gods for perceived wrongdoing on the part of man is another recurring theme.\n\nMyths regarding the origins of language and languages are generally subsumed or footnoted into larger creation myths, although there are differences. Some tales say a creator endowed language from the beginning, others count language among later gifts, or curses.\n\nThe Hebrew Bible attributes the origin of language per se to humans, with Adam being asked to name the creatures that God had created.\n\nOne of the most well known examples in the West is the Tower of Babel passage from Genesis. \nIt tells of God punishing humanity for arrogance and disobedience by means of the confusion of tongues.\n\nThis became the standard account in the European Middle Ages, reflected in medieval literature such as the tale of Fénius Farsaid.\n\nVāc is the Hindu goddess of speech, or \"speech personified\". As \"brahman\" \"sacred utterance\", she has a cosmological role as the \"Mother of the Vedas\".\nShe is presented as the consort of Prajapati, who is likewise presented as the origin of the Veda.\nShe became conflated with Sarasvati in later Hindu mythology.\n\nIn common with the mythology of many other civilizations and cultures which tell of a Great Flood, certain Native American tribes tell of a deluge which came over the Earth. After the water subsides, various explanations are given for the new diversity in speech.\n\nThe Aztecs' story maintains that only a man, Coxcox, and a woman, Xochiquetzal, survive, having floated on a piece of bark. They found themselves on land and begot many children who were at first born unable to speak, but subsequently, upon the arrival of a dove were endowed with language, although each one was given a different speech such that they could not understand one another.\n\nA similar flood is described by the Kaska people from North America, however, like with the story of Babel, the people were now \"widely scattered over the world\". The narrator of the story adds that this explains the many different centres of population, the many tribes and the many languages, \"Before the flood, there was but one centre; for all the people lived together in one country, and spoke one language.\"\n\nAn Iroquois story tells of the god Taryenyawagon (\"Holder of the Heavens\") guiding his people on a journey and directing them to settle in different places whence their languages changed.\n\nA Salishan myth tells how an argument led to the divergence of languages. Two people were arguing whether the high-pitched humming noise that accompanies ducks in flight is from air passing through the beak or from the flapping of wings. The argument is not settled by the chief, who then calls a council of all the leading people from nearby villages. This council breaks down in argument when nobody can agree, and eventually the dispute leads to a split where some people move far away. Over time they slowly began to speak differently, and eventually other languages were formed.\n\nIn the mythology of the Yuki, indigenous people of California, a creator, accompanied by Coyote creates language as he creates the tribes in various localities. He lays sticks which will transform into people upon daybreak.\n\nThe Ticuna people of the Upper Amazon tell that all the peoples were once a single tribe, speaking the same language until two hummingbird eggs were eaten, it is not told by whom. Subsequently the tribe split into groups and dispersed far and wide.\n\nIn Ancient Greece there was a myth which told that for ages men had lived without law under the rule of Zeus and speaking one language, gifted to them by the god and goddess of ingenuity, Philarios and Philarion. The god Hermes brought diversity in speech and along with it separation into nations and discord ensued. Zeus then resigned his position, yielding it to the first king of men, Phoroneus.\n\nIn Norse mythology, the faculty of speech is a gift from the third son of Borr, Vé, who gave also hearing and sight.\n\nThe Wa-Sania, a Bantu people of East African origin have a tale that in the beginning, the peoples of the earth knew only one language, but during a severe famine, a madness struck the people, causing them to wander in all directions, jabbering strange words, and this is how different languages came about.\n\nA god who speaks all languages is a theme in African mythology, two examples being Eshu of the Yoruba, a trickster who is a messenger of the gods. Eshu has a parallel in Legba from the Fon people of Benin. Another Yoruba god who speaks all the languages of the world is Orunmila, the god of divination.\n\nA group of people on the island of Hao in Polynesia tell a very similar story to the Tower of Babel, speaking of a God who, \"in anger chased the builders away, broke down the building, and changed their language, so that they spoke diverse tongues\".\n\nIn South Australia, a people of Encounter Bay tell a story of how diversity in language came about from cannibalism:\n\nAnother group of Australian Aboriginal people, the Gunwinggu, tell of a goddess in dreamtime giving each of her children a language of their own to play with.\n\nThe traditional beliefs of the indigenous inhabitants of the Andaman Islands in the Bay of Bengal describe language as being given by the god Pūluga to the first man and woman at their union following a great deluge. The language given was called \"bojig-yâb-\", which is the language spoken to this day, according to their belief, by the tribe inhabiting the south and south-eastern portion of middle Andaman. This language is described by the inhabitants as the \"mother tongue\" from which all other dialects have been made.\n\nTheir beliefs hold that even before the death of the first man,\n\nThus explaining the diversity of language.\n\n\n"}
{"id": "8830624", "url": "https://en.wikipedia.org/wiki?curid=8830624", "title": "National minimum dataset", "text": "National minimum dataset\n\nIn health informatics, a national minimum dataset is a database of health encounters held by a central repository.\n\n\"Minimum\" implies that the data fields will be only those required to aggregate information for the purposes of administering the health system in the particular country and for reporting information required as a member country of WHO.\n\n"}
{"id": "36253964", "url": "https://en.wikipedia.org/wiki?curid=36253964", "title": "Origin of speech", "text": "Origin of speech\n\nThe origin of speech refers to the more general problem of the origin of language in the context of the physiological\ndevelopment of the human speech organs such as the tongue, lips and vocal organs used to produce phonological units in all human languages.\n\nAlthough related to the more general problem of the origin of language, the evolution of distinctively human speech capacities has become a distinct and in many ways separate area of scientific research. The topic is a separate one because language is not necessarily spoken: it can equally be written or signed. Speech is in this sense optional, although it is the default modality for language.\n\nUncontroversially, monkeys, apes and humans, like many other animals, have evolved specialised mechanisms for producing \"sound\" for purposes of social communication. On the other hand, no monkey or ape uses its \"tongue\" for such purposes. Our species' unprecedented use of the tongue, lips and other moveable parts seems to place speech in a quite separate category, making its evolutionary emergence an intriguing theoretical challenge in the eyes of many scholars.\n\nThe term \"modality\" means the chosen representational format for encoding and transmitting information. A striking feature of language is that it is \"modality-independent.\" Should an impaired child be prevented from hearing or producing sound, its innate capacity to master a language may equally find expression in signing. Sign languages of the deaf are independently invented and have all the major properties of spoken language except for the modality of transmission. From this it appears that the language centres of the human brain must have evolved to function optimally irrespective of the selected modality.\n\nThis feature is extraordinary. Animal communication systems routinely combine visible with audible properties and effects, but not one is modality-independent. No vocally impaired whale, dolphin or songbird, for example, could express its song repertoire equally in visual display. Indeed, in the case of animal communication, message and modality are not capable of being disentangled. Whatever message is being conveyed stems from intrinsic properties of the signal.\n\nModality independence should not be confused with the ordinary phenomenon of multimodality. Monkeys and apes rely on a repertoire of species-specific \"gesture-calls\" — emotionally expressive vocalisations inseparable from the visual displays which accompany them. Humans also have species-specific gesture-calls — laughs, cries, sobs and so forth — together with involuntary gestures accompanying speech. Many animal displays are polymodal in that each appears designed to exploit multiple channels simultaneously.\n\nThe human linguistic property of \"modality independence\" is conceptually distinct from this. It allows the speaker to encode the informational content of a message in a single channel, while switching between channels as necessary. Modern city-dwellers switch effortlessly between the spoken word and writing in its various forms — handwriting, typing, e-mail and so forth. Whichever modality is chosen, it can reliably transmit the full message content without external assistance of any kind. When talking on the telephone, for example, any accompanying facial or manual gestures, however natural to the speaker, are not strictly necessary. When typing or manually signing, conversely, there's no need to add sounds. In many Australian Aboriginal cultures, a section of the population — perhaps women observing a ritual taboo — traditionally restrict themselves for extended periods to a silent (manually signed) version of their language. Then, when released from the taboo, these same individuals resume narrating stories by the fireside or in the dark, switching to pure sound without sacrifice of informational content.\n\nSpeaking is the default modality for language in all cultures. Humans' first recourse is to encode our thoughts in sound — a method which depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus.\n\nThe speech organs, everyone agrees, evolved in the first instance not for speech but for more basic bodily functions such as feeding and breathing. Nonhuman primates have broadly similar organs, but with different neural controls. Apes use their highly flexible, maneuverable tongues for eating but not for vocalizing. When an ape is not eating, fine motor control over its tongue is deactivated. \"Either\" it is performing gymnastics with its tongue \"or\" it is vocalising; it cannot perform both activities simultaneously. Since this applies to mammals in general, \"Homo sapiens\" is exceptional in harnessing mechanisms designed for respiration and ingestion to the radically different requirements of articulate speech.\n\nThe word \"language\" derives from the Latin \"lingua,\" \"tongue\". Phoneticians agree that the tongue is the most important speech articulator, followed by the lips. A natural language can be viewed as a particular way of using the tongue to express thought.\n\nThe human tongue has an unusual shape. In most mammals, it's a long, flat structure contained largely within the mouth. It is attached at the rear to the hyoid bone, situated below oral level in the pharynx. In humans, the tongue has an almost circular sagittal (midline) contour, much of it lying vertically down an extended pharynx, where it is attached to a hyoid bone in a lowered position. Partly as a result of this, the horizontal (inside-the-mouth) and vertical (down-the-throat) tubes forming the supralaryngeal vocal tract (SVT) are almost equal in length (whereas in other species, the vertical section is shorter). As we move our jaws up and down, the tongue can vary the cross-sectional area of each tube independently by about 10:1, altering formant frequencies accordingly. That the tubes are joined at a right angle permits pronunciation of the vowels [i], [u] and [a], which nonhuman primates cannot do. Even when not performed particularly accurately, in humans the articulatory gymnastics needed to distinguish these vowels yield consistent, distinctive acoustic results, illustrating the quantal nature of human speech sounds. It may not be coincidental that [i], [u] and [a] are the most common vowels in the world's languages.\n\nIn humans, the lips are important for the production of stops and fricatives, in addition to vowels. Nothing, however, suggests that the lips evolved for those reasons. During primate evolution, a shift from nocturnal to diurnal activity in tarsiers, monkeys and apes (the haplorhines) brought with it an increased reliance on vision at the expense of olfaction. As a result, the snout became reduced and the rhinarium or \"wet nose\" was lost. The muscles of the face and lips consequently became less constrained, enabling their co-option to serve purposes of facial expression. The lips also became thicker. \"Hence\", according to one major authority, \"the evolution of mobile, muscular lips, so important to human speech, was the exaptive result of the evolution of diurnality and visual communication in the common ancestor of haplorhines\". It is unclear whether our lips have undergone more recent adaptation to the specific requirements of speech.\n\nCompared with nonhuman primates, humans have significantly enhanced control of breathing, enabling exhalations to be extended and inhalations shortened as we speak. While we are speaking, intercostal and interior abdominal muscles are recruited to expand the thorax and draw air into the lungs, and subsequently to control the release of air as the lungs deflate. The muscles concerned are markedly more innervated in humans than in nonhuman primates. Evidence from fossil hominins suggests that the necessary enlargement of the vertebral canal, and therefore spinal cord dimensions, may not have occurred in \"Australopithecus\" or \"Homo erectus\" but was present in the Neanderthals and early modern humans.\n\nThe larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is \"descended\". Our species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls. Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays. Lions, jaguars, cheetahs and domestic cats also do this. However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.\nDespite all this, scholars remain divided as to how \"special\" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent. As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs — increasing the risk of choking while swallowing food — we are forced to ask what benefits might have outweighed those costs. The obvious benefit — so it is claimed — must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact \"not\" seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare. Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of \"Homo sapiens.\" A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was \"initially\" driven by selection pressures for speech.\n\nAt least one orangutan has demonstrated the ability to control the voice box.\n\nTo lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds \"deeper\" — giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself. Ohala points out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning. W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in our species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalising to protect their infants would also have benefited from this ability.\n\nMost specialists credit the Neanderthals with speech abilities not radically different from those of modern \"Homo sapiens\". An indirect line of argument is that their tool-making and hunting tactics would have been difficult to learn or execute without some kind of speech. A recent extraction of DNA from Neanderthal bones indicates that Neanderthals had the same version of the FOXP2 gene as modern humans. This gene, once mistakenly described as the \"grammar gene\", plays a role in controlling the orofacial movements which (in modern humans) are involved in speech.\n\nDuring the 1970s, it was widely believed that the Neanderthals lacked modern speech capacities. It was claimed that they possessed a hyoid bone so high up in the vocal tract as to preclude the possibility of producing certain vowel sounds.\n\nThe hyoid bone is present in many mammals. It allows a wide range of tongue, pharyngeal and laryngeal movements by bracing these structures alongside each other in order to produce variation. It is now realised that its lowered position is not unique to \"Homo sapiens\", while its relevance to vocal flexibility may have been overstated: although men have a lower larynx, they do not produce a wider range of sounds than women or two-year-old babies. There is no evidence that the larynx position of the Neanderthals impeded the range of vowel sounds they could produce. The discovery of a modern-looking hyoid bone of a Neanderthal man in the Kebara Cave in Israel led its discoverers to argue that the Neanderthals had a descended larynx, and thus human-like speech capabilities. However, other researchers have claimed that the morphology of the hyoid is not indicative of the larynx's position. It is necessary to take into consideration the skull base, the mandible and the cervical vertebrae and a cranial reference plane.\n\nThe morphology of the outer and middle ear of Middle Pleistocene hominins from Atapuerca SH in Spain, believed to be proto-Neanderthal, suggests they had an auditory sensitivity similar to modern humans and very different from chimpanzees. They were probably able to differentiate between many different speech sounds.\n\nThe hypoglossal nerve plays an important role in controlling movements of the tongue. In 1998, one research team used the size of the hypoglossal canal in the base of fossil skulls in an attempt to estimate the relative number of nerve fibres, claiming on this basis that Middle Pleistocene hominins and Neanderthals had more fine-tuned tongue control than either australopithecines or apes. Subsequently, however, it was demonstrated that hypoglossal canal size and nerve sizes are not correlated, and it is now accepted that such evidence is uninformative about the timing of human speech evolution.\n\nAccording to one influential school, the human vocal apparatus is intrinsically digital on the model of a keyboard or digital computer. If so, this is remarkable: nothing about a chimpanzee's vocal apparatus suggests a digital keyboard, notwithstanding the anatomical and physiological similarities. This poses the question as to when and how, during and the course of human evolution, the transition from analog to digital structure and function occurred.\n\nThe human supralaryngeal tract is said to be digital in the sense that it is an arrangement of moveable toggles or switches, each of which, at any one time, must be in one state or another. The vocal cords, for example, are either vibrating (producing a sound) or not vibrating (in silent mode). By virtue of simple physics, the corresponding distinctive feature — in this case, \"voicing\" — cannot be somewhere in between. The options are limited to \"off\" and \"on\". Equally digital is the feature known as \"nasalisation\". At any given moment the soft palate or velum either allows or doesn't allow sound to resonate in the nasal chamber. In the case of lip and tongue positions, more than two digital states may be allowed. (To experiment with this, click on Interactive Saggital Section).\n\nThe theory that speech sounds are composite entities constituted by complexes of binary phonetic features was first advanced in 1938 by the Russian linguist Roman Jakobson. A prominent early supporter of this approach was Noam Chomsky, who went on to extend it from phonology to language more generally, in particular to the study of syntax and semantics. In his 1965 book, \"Aspects of the Theory of Syntax,\" Chomsky treated semantic concepts as combinations of binary-digital atomic elements explicitly on the model of distinctive features theory. The lexical item \"bachelor\", on this basis, would be expressed as [+ Human], [+ Male], [- Married].\n\nSupporters of this approach view the vowels and consonants recognised by speakers of a particular language or dialect at a particular time as cultural entities of little scientific interest. From a natural science standpoint, the units which matter are those common to \"Homo sapiens\" by virtue of our biological nature. By combining the atomic elements or \"features\" with which all humans are innately equipped, anyone may in principle generate the entire range of vowels and consonants to be found in any of the world's languages, whether past, present or future. The distinctive features are in this sense atomic components of a universal language.\n\nIn recent years, the notion of an innate \"universal grammar\" underlying phonological variation has been called into question. The most comprehensive monograph ever written about speech sounds, \"Sounds of the World's Languages,\" by Peter Ladefoged and Ian Maddieson, found virtually no basis for the postulation of some small number of fixed, discrete, universal phonetic features. Examining 305 languages, for example, they encountered vowels that were positioned basically everywhere along the articulatory and acoustic continuum. Ladefoged concludes that phonological features are not determined by human nature: \"Phonological features are best regarded as artifacts that linguists have devised in order to describe linguistic systems.\" The controversy remains unresolved.\n\nSelf-organization characterizes systems where macroscopic structures are spontaneously formed out of local interactions between the many components of the system. In self-organized systems, global organizational properties are not to be found at the local level. In colloquial terms, self-organisation is roughly captured by the idea of \"bottom-up\" (as opposed to \"top-down\") organisation. Examples of self-organized systems range from ice crystals to galaxy spirals in the inorganic world, and from spots on the leopard skins to the architecture of termite nests or shape of a flock of starlings.\nAccording to many phoneticians, the sounds of language arrange and re-arrange themselves through self-organization Speech sounds have both perceptual (\"how you hear them\") and articulatory (\"how you produce them\") properties, all with continuous values. Speakers tend to minimise effort, favouring ease of articulation over clarity. Listeners do the opposite, favouring sounds which are easy to distinguish even if difficult to pronounce. Since speakers and listeners are constantly switching roles, the syllable systems actually found in the world's languages turn out to be a compromise between acoustic distinctiveness on the one hand, and articulatory ease on the other.\n\nHow, precisely, do systems of vowels, consonants and syllables arise? Agent-based computer models take the perspective of self-organisation at the level of the speech community or population. The two main paradigms here are (1) the iterated learning model and (2) the language game model. Iterated learning focuses on transmission from generation to generation, typically with just one agent in each generation.\nIn the language game model, a whole population of agents simultaneously produce, perceive and learn language, inventing novel forms when the need arises.\n\nSeveral models have shown how relatively simple peer-to-peer vocal interactions, such as imitation, can spontaneously self-organize a system of sounds shared by the whole population, and different in different populations. For example, models elaborated by Berrah et al., as well as de Boer, and recently reformulated using Bayesian theory, showed how a group of individuals playing imitation games can self-organize repertoires of vowel sounds which share substantial properties with human vowel systems. For example, in de Boer's model, initially vowels are generated randomly, but agents learn from each other as they interact repeatedly over time. Agent A chooses a vowel from its repertoire and produces it, inevitably with some noise. Agent B hears this vowel and chooses the closest equivalent from its own repertoire. To check whether this truly matches the original, B produces the vowel \"it thinks it has heard\", whereupon A refers once again to its own repertoire to find the closest equivalent. If this matches the one it initially selected, the game is successful, otherwise it has failed. \"Through repeated interactions,\" according to de Boer, \"vowel systems emerge that are very much like the ones found in human languages.\"\n\nIn a different model, the phonetician Björn Lindblom was able to predict, on self-organizational grounds, the favoured choices of vowel systems ranging from three to nine vowels on the basis of a principle of optimal perceptual differentiation.\n\nFurther models studied the role of self-organization in the origins of phonemic coding and combinatoriality, that is the existence of phonemes and their systematic reuse to build structured syllables. Pierre-Yves Oudeyer developed models which showed that basic neural equipment for adaptive holistic vocal imitation, coupling directly motor and perceptual representations in the brain, can generate spontaneously shared combinatorial systems of vocalizations, including phonotactic patterns, in a society of babbling individuals. These models also characterized how morphological and physiological innate constraints can interact with these self-organized mechanisms to account for both the formation of statistical regularities and diversity in vocalization systems.\n\nThe gestural theory states that speech was a relatively late development, evolving by degrees from a system that was originally gestural.\n\nTwo types of evidence support this theory:\n\nResearch has found strong support for the idea that spoken language and signing depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language. Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.\n\nHumans spontaneously use hand and facial gestures when formulating ideas to be conveyed in speech. There are also, of course, many sign languages in existence, commonly associated with deaf communities; as noted above, these are equal in complexity, sophistication, and expressive power, to any oral language. The main difference is that the \"phonemes\" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.\n\nCritics note that for mammals in general, sound turns out to be the best medium in which to encode information for transmission over distances at speed. Given the probability that this applied also to early humans, it's hard to see why they should have abandoned this efficient method in favour of more costly and cumbersome systems of visual gesturing — only to return to sound at a later stage.\n\nBy way of explanation, it has been proposed that at a relatively late stage in human evolution, our ancestors' hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language is said to have occurred only at that point. Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this puzzle — one setting out from considerations of signal reliability and trust — see \"from pantomime to speech\" below).\n\nLittle is known about the timing of language's emergence in the human species. Unlike writing, speech leaves no material trace, making it archaeologically invisible. Lacking direct linguistic evidence, specialists in human origins have resorted to the study of anatomical features and genes arguably associated with speech production. While such studies may provide information as to whether pre-modern \"Homo\" species had speech \"capacities\", it is still unknown whether they actually spoke. While they may have communicated vocally, the anatomical and genetic data lack the resolution necessary to differentiate proto-language from speech.\n\nUsing statistical methods to estimate the time required to achieve the current spread and diversity in modern languages today, Johanna Nichols — a linguist at the University of California, Berkeley — argued in 1998 that vocal languages must have begun diversifying in our species at least 100,000 years ago.\n\nMore recently — in 2012 — anthropologists Charles Perreault and Sarah Mathew used phonemic diversity to suggest a date consistent with this. \"Phonemic diversity\" denotes the number of perceptually distinct units of sound — consonants, vowels and tones — in a language. The current worldwide pattern of phonemic diversity potentially contains the statistical signal of the expansion of modern \"Homo sapiens\" out of Africa, beginning around 60-70 thousand years ago. Some scholars argue that phonemic diversity evolves slowly and can be used as a clock to calculate how long the oldest African languages would have to have been around in order to accumulate the number of phonemes they possess today. As human populations left Africa and expanded into the rest of the world, they underwent a series of bottlenecks — points at which only a very small population survived to colonise a new continent or region. Allegedly such population crash led to a corresponding reduction in genetic, phenotypic and phonemic diversity. African languages today have some of the largest phonemic inventories in the world, while the smallest inventories are found in South America and Oceania, some of the last regions of the globe to be colonized. For example, Rotokas, a language of New Guinea, and Pirahã, spoken in South America, both have just 11 phonemes, while !Xun, a language spoken in Southern Africa has 141 phonemes.\nThe authors use a natural experiment — the colonization of mainland Southeast Asia on the one hand, the long-isolated Andaman Islands on the other — to estimate the rate at which phonemic diversity increases through time. Using this rate, they estimate that the world's languages date back to the Middle Stone Age in Africa, sometime between 350 thousand and 150 thousand years ago. This corresponds to the speciation event which gave rise to \"Homo sapiens\".\n\nThese and similar studies have however been criticized by linguists who argue that they are based on a flawed analogy between genes and phonemes, since phonemes are frequently transferred laterally between languages unlike genes, and on a flawed sampling of the world's languages, since both Oceania and the Americas also contain languages with very high numbers of phonemes, and Africa contains languages with very few. They argue that the actual distribution of phonemic diversity in the world reflects recent language contact and not deep language history - since it is well demonstrated that languages can lose or gain many phonemes over very short periods. In other words, there is no valid linguistic reason to expect genetic founder effects to influence phonemic diversity.\n\nIn 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language:\n\nMost scholars today consider all such theories not so much wrong — they occasionally offer peripheral insights — as comically naïve and irrelevant. The problem with these theories is that they are so narrowly mechanistic. They assume that once our ancestors had stumbled upon the appropriate ingenious \"mechanism\" for linking sounds with meanings, language automatically evolved and changed.\n\nFrom the perspective of modern science, the main obstacle to the evolution of speech-like communication in nature is not a mechanistic one. Rather, it is that symbols — arbitrary associations of sounds with corresponding meanings — are unreliable and may well be false. As the saying goes, \"words are cheap\". The problem of reliability was not recognised at all by Darwin, Müller or the other early evolutionist theorists.\n\nAnimal vocal signals are for the most part intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. One can \"trust\" the signal not because the cat is inclined to be honest, but because it just can't fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason — because they are hard to fake. Primate social intelligence is \"Machiavellian\" — self-serving and unconstrained by moral scruples. Monkeys and apes often attempt to deceive one another, while at the same time constantly on guard against falling victim to deception themselves. Paradoxically, it is precisely primates' resistance to deception that blocks the evolution of their vocal communication systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.\n\nWords are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favour of hard-to-fake indices or cues. For language to work, then, listeners must be confident that those with whom they are on speaking terms are generally likely to be honest. A peculiar feature of language is \"displaced reference\", which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate \"here\" and \"now\". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot (see signalling theory).\n\nThe \"mother tongues\" hypothesis was proposed in 2004 as a possible solution to this problem. W. Tecumseh Fitch suggested that the Darwinian principle of \"kin selection\" — the convergence of genetic interests between relatives — might be part of the answer. Fitch suggests that spoken languages were originally \"mother tongues\". If speech evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable vocal signals — spoken words — to become accepted as trustworthy and so begin evolving for the first time.\n\nCritics of this theory point out that kin selection is not unique to humans. Ape mothers also share genes with their offspring, as do all animals, so why is it only humans who speak? Furthermore, it is difficult to believe that early humans restricted linguistic communication to genetic kin: the incest taboo must have forced men and women to interact and communicate with non-kin. So even if we accept Fitch's initial premises, the extension of the posited \"mother tongue\" networks from relatives to non-relatives remains unexplained.\n\nIb Ulbæk invokes another standard Darwinian principle — \"reciprocal altruism\" — to explain the unusually high levels of intentional honesty necessary for language to evolve. 'Reciprocal altruism' can be expressed as the principle that \"if you scratch my back, I'll scratch yours.\" In linguistic terms, it would mean that \"if you speak truthfully to me, I'll speak truthfully to you.\" Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, early society as a whole must have been subject to moral regulation.\n\nCritics point out that this theory fails to explain when, how, why or by whom \"obligatory reciprocal altruism\" could possibly have been enforced. Various proposals have been offered to remedy this defect. A further criticism is that language doesn't work on the basis of reciprocal altruism anyway. Humans in conversational groups don't withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting it to anyone who will listen without thought of return.\n\nGossip, according to Robin Dunbar, does for group-living humans what manual grooming does for other primates — it allows individuals to service their relationships and so maintain their alliances. As humans began living in larger and larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable. In response to this problem, humans invented \"a cheap and ultra-efficient form of grooming\" — \"vocal grooming\". To keep your allies happy, you now needed only to \"groom\" them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming (the production of pleasing sounds lacking syntax or combinatorial semantics) then evolved somehow into syntactical speech.\n\nCritics of this theory point out that the very efficiency of \"vocal grooming\" — that words are so cheap — would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming. A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming — the production of pleasing but meaningless sounds — to the cognitive complexities of syntactical speech.\n\nAccording to another school of thought, language evolved from mimesis — the \"acting out\" of scenarios using vocal and gestural pantomime. For as long as utterances needed to be emotionally expressive and convincing, it was not possible to complete the transition to purely conventional signs. On this assumption, pre-linguistic gestures and vocalisations would have been required not just to disambiguate intended meanings, but also to inspire confidence in their intrinsic reliability. If contractual commitments were necessary in order to inspire community-wide trust in communicative intentions, it would follow that these had to be in place before humans could shift at last to an ultra-efficient, high-speed — digital as opposed to analog — signalling format. Vocal distinctive features (sound contrasts) are ideal for this purpose. It is therefore suggested that the establishment of contractual understandings enabled the decisive transition from mimetic gesture to fully conventionalised, digitally encoded speech.\n\nThe ritual/speech coevolution theory was originally proposed by the distinguished social anthropologist Roy Rappaport before being elaborated by anthropologists such as Chris Knight, Jerome Lewis, Nick Enfield, Camilla Power and Ian Watts. Cognitive scientist and robotics engineer Luc Steels is another prominent supporter of this general approach, as is biological anthropologist/neuroscientist Terrence Deacon.\n\nThese scholars argue that there can be no such thing as a \"theory of the origins of language\". This is because language is not a separate adaptation but an internal aspect of something much wider — namely, human symbolic culture as a whole. Attempts to explain language independently of this wider context have spectacularly failed, say these scientists, because they are addressing a problem with no solution. Can we imagine a historian attempting to explain the emergence of credit cards independently of the wider system of which they are a part? Using a credit card makes sense only if you have a bank account institutionally recognised within a certain kind of advanced capitalist society — one where communications technology has already been invented and fraud can be detected and prevented. In much the same way, language would not work outside a specific array of social mechanisms and institutions. For example, it would not work for an ape communicating with other apes in the wild. Not even the cleverest ape could make language work under such conditions.\n\nSpeech consists of digital contrasts whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world — they are a theoretical impossibility. Being intrinsically unreliable, language works only if you can build up a reputation for trustworthiness within a certain kind of society — namely, one where symbolic cultural facts (sometimes called \"institutional facts\") can be established and maintained through collective social endorsement. In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective \"ritual\". Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human symbolic culture as a whole, with language an important but subsidiary component.\n\nCritics of the theory include Noam Chomsky, who terms it the \"non-existence\" hypothesis — a denial of the very existence of language as an object of study for natural science. Chomsky's own theory is that language emerged in an instant and in perfect form, prompting his critics in turn to retort that only something that doesn't exist — a theoretical construct or convenient scientific fiction — could possibly emerge in such a miraculous way. The controversy remains unresolved.\n\nThe essay \"The festal origin of human speech\", though published in the late nineteenth century, made little impact until the American philosopher Susanne Langer re-discovered and publicised it in 1941. \nThe theory sets out from the observation that primate vocal sounds are above all \"emotionally\" expressive. The emotions aroused are socially contagious. Because of this, an extended bout of screams, hoots or barks will tend to express not just the feelings of this or that individual but the mutually contagious ups and downs of everyone within earshot.\n\nTurning to the ancestors of \"Homo sapiens\", the \"festal origin\" theory suggests that in the \"play-excitement\" preceding or following a communal hunt or other group activity, everyone might have combined their voices in a comparable way, emphasising their mood of togetherness with such noises as rhythmic drumming and hand-clapping. Variably pitched voices would have formed conventional patterns, such that choral singing became an integral part of communal celebration.\n\nAlthough this was not yet speech, according to Langer, it developed the vocal capacities from which speech would later derive. There would be conventional modes of ululating, clapping or dancing appropriate to different festive occasions, each so intimately associated with \"that kind of occasion\" that it would tend to collectively uphold and embody the concept of it. Anyone hearing a snatch of sound from such a song would recall the associated occasion and mood. A melodic, rhythmic sequence of syllables conventionally associated with a certain type of celebration would become, in effect, its vocal mark. On that basis, certain familiar sound sequences would become \"symbolic\".\n\nIn support of all this, Langer cites ethnographic reports of tribal songs consisting entirely of \"rhythmic nonsense syllables\". She concedes that an English equivalent such as \"hey-nonny-nonny\", although perhaps suggestive of certain feelings or ideas, is neither noun, verb, adjective, nor any other syntactical part of speech. So long as articulate sound served only in the capacity of \"hey nonny-nonny\", \"hallelujah\" or \"alack-a-day\", it cannot yet have been speech. For that to arise, according to Langer, it was necessary for such sequences to be emitted increasingly \"out of context\" — outside the total situation that gave rise to them. Extending a set of associations from one cognitive context to another, completely different one, is the secret of \"metaphor\". Langer invokes an early version of what is nowadays termed \"grammaticalisation\" theory to show how, from, such a point of departure, syntactically complex speech might progressively have arisen.\n\nLanger acknowledges Emile Durkheim as having proposed a strikingly similar theory back in 1912. For recent thinking along broadly similar lines, see Steven Brown on \"musilanguage\", Chris Knight on \"ritual\" and \"play\", Jerome Lewis on \"mimicry\", Steven Mithen on \"Hmmmmm\" Bruce Richman on \"nonsense syllables\" and Alison Wray on \"holistic protolanguage\".\n\nThe term \"musilanguage\" (or \"hmmmmm\") refers to a pre-linguistic system of vocal communication from which (according to some scholars) \"both\" music \"and\" language later derived. The idea is that rhythmic, melodic, emotionally expressive vocal ritual helped bond coalitions and, over time, set up selection pressures for enhanced volitional control over the speech articulators. Patterns of synchronised choral chanting are imagined to have varied according to the occasion. For example, \"we're setting off to find honey\" might sound qualitatively different from \"we're setting off to hunt\" or \"we're grieving over our relative's death\". If social standing depended on maintaining a regular beat and harmonising one's own voice with that of everyone else, group members would have come under pressure to demonstrate their choral skills.\n\nArchaeologist Steven Mithen speculates that the Neanderthals possessed some such system, expressing themselves in a \"language\" known as \"Hmmmmm\", standing for Holistic, manipulative, multi-modal, musical and mimetic. In Bruce Richman's earlier version of essentially the same idea, frequent repetition of the same few songs by many voices made it easy for people to remember those sequences as whole units. Activities that a group of people were doing while they were vocalising together — activities that were important or striking or richly emotional — came to be associated with particular sound sequences, so that each time a fragment was heard, it evoked highly specific memories. The idea is that the earliest lexical items (words) started out as abbreviated fragments of what were originally communal songs.\n\nAs group members accumulated an expanding repertoire of songs for different occasions, interpersonal call-and-response patterns evolved along one trajectory to assume linguistic form. Meanwhile, along a divergent trajectory, polyphonic singing and other kinds of music became increasingly specialised and sophisticated.\n\nTo explain the establishment of syntactical speech, Richman cites English \"I wanna go home\". He imagines this to have been learned in the first instance not as a combinatorial sequence of free-standing words, but as a single stuck-together combination — the melodic sound people make to express \"feeling homesick\". Someone might sing \"I wanna go home\", prompting other voices to chime in with \"I need to go home\", \"I'd love to go home\", \"Let's go home\" and so forth. Note that one part of the song remains constant, while another is permitted to vary. If this theory is accepted, syntactically complex speech began evolving as each chanted mantra allowed for variation at a certain point, allowing for the insertion of an element from some other song. For example, while mourning during a funeral rite, someone might want to recall a memory of collecting honey with the deceased, signalling this at an appropriate moment with a fragment of the \"we're collecting honey\" song. Imagine that such practices became common. Meaning-laden utterances would now have become subject to a distinctively linguistic creative principle — that of recursive embedding.\n\nMany scholars associate the evolutionary emergence of speech with profound social, sexual, political and cultural developments. One view is that primate-style dominance needed to give way to a more cooperative and egalitarian lifestyle of the kind characteristic of modern hunter-gatherers.\n\nAccording to Michael Tomasello, the key cognitive capacity distinguishing \"Homo sapiens\" from our ape cousins is \"intersubjectivity\". This entails turn-taking and role-reversal: your partner strives to read your mind, you simultaneously strive to read theirs, and each of you makes a conscious effort to assist the other in the process. The outcome is that each partner forms a representation of the other's mind in which their own can be discerned by reflection.\n\nTomasello argues that this kind of bi-directional cognition is central to the very possibility of linguistic communication. Drawing on his research with both children and chimpanzees, he reports that human infants, from one year old onwards, begin viewing their own mind as if from the standpoint of others. He describes this as a cognitive revolution. Chimpanzees, as they grow up, never undergo such a revolution. The explanation, according to Tomasello, is that their evolved psychology is adapted to a deeply competitive way of life. Wild-living chimpanzees form despotic social hierarchies, most interactions involving calculations of dominance and submission. An adult chimp will strive to outwit its rivals by guessing at their intentions while blocking them from reciprocating. Since bi-directional intersubjective communication is impossible under such conditions, the cognitive capacities necessary for language don't evolve.\n\nIn the scenario favoured by David Erdal and Andrew Whiten, primate-style dominance provoked equal and opposite coalitionary resistance — \"counter-dominance.\" During the course of human evolution, increasingly effective strategies of rebellion against dominant individuals led to a compromise. While abandoning any attempt to dominate others, group members vigorously asserted their personal autonomy, maintaining their alliances to make potentially dominant individuals think twice. Within increasingly stable coalitions, according to this perspective, status began to be earned in novel ways, social rewards accruing to those perceived by their peers as especially cooperative and self-aware.\n\nWhile counter-dominance, according to this evolutionary narrative, culminates in a stalemate, anthropologist Christopher Boehm extends the logic a step further. Counter-dominance tips over at last into full-scale \"reverse dominance\". The rebellious coalition decisively overthrows the figure of the primate alpha-male. No dominance is allowed except that of the self-organised community as a whole.\n\nAs a result of this social and political change, hunter-gatherer egalitarianism is established. As children grow up, they are motivated by those around them to reverse perspective, engaging with other minds on the model of their own. Selection pressures favour such psychological innovations as imaginative empathy, joint attention, moral judgment, project-oriented collaboration and the ability to evaluate one's own behaviour from the standpoint of others. Underpinning enhanced probabilities of cultural transmission and cumulative cultural evolution, these developments culminate in the establishment of hunter-gatherer-style egalitarianism in association with intersubjective communication and cognition. It is in this social and political context that language evolves.\n\nAccording to Dean Falk's \"putting the baby down\" theory, vocal interactions between early hominin mothers and infants sparked a sequence of events that led, eventually, to our ancestors' earliest words. The basic idea is that evolving human mothers, unlike their monkey and ape counterparts, couldn't move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed reassurance that they were not being abandoned. Mothers responded by developing \"motherese\" — an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling and emotionally expressive contact calls. The argument is that language somehow developed out of all this.\n\nWhile this theory may explain a certain kind of infant-directed \"protolanguage\" — known today as \"motherese\" — it does little to solve the really difficult problem, which is the emergence among adults of syntactical speech. \n\nEvolutionary anthropologist Sarah Hrdy observes that only human mothers among great apes are willing to let another individual take hold of their own babies; further, we are routinely willing to let others babysit. She identifies lack of trust as the major factor preventing chimp, bonobo or gorilla mothers from doing the same: \"If ape mothers insist on carrying their babies everywhere ... it is because the available alternatives are not safe enough.\" The fundamental problem is that ape mothers (unlike monkey mothers who may often babysit) do not have female relatives nearby. The strong implication is that, in the course of \"Homo\" evolution, allocare could develop because \"Homo\" mothers did have female kin close by — in the first place, most reliably, their own mothers. Extending the Grandmother hypothesis, Hrdy argues that evolving \"Homo erectus\" females necessarily relied on female kin initially; this novel situation in ape evolution of mother, infant and mother's mother as allocarer provided the evolutionary ground for the emergence of intersubjectivity. She relates this onset of \"cooperative breeding in an ape\" to shifts in life history and slower child development, linked to the change in brain and body size from the 2 million year mark.\n\nPrimatologist Klaus Zuberbühler uses these ideas to help explain the emergence of vocal flexibility in the human species. Co-operative breeding would have compelled infants to struggle actively to gain the attention of caregivers, not all of whom would have been directly related. A basic primate repertoire of vocal signals may have been insufficient for this social challenge. Natural selection, according to this view, would have favoured babies with advanced vocal skills, beginning with babbling (which triggers positive responses in care-givers) and paving the way for the elaborate and unique speech abilities of modern humans.\n\nThese ideas might be linked to those of the renowned structural linguist Roman Jakobson, who claimed that \"the sucking activities of the child are accompanied by a slight nasal murmur, the only phonation to be produced when the lips are pressed to the mother's breast ... and the mouth is full\". He proposed that later in the infant's development, \"this phonatory reaction to nursing is reproduced as an anticipatory signal at the mere sight of food and finally as a manifestation of a desire to eat, or more generally, as an expression of discontent and impatient longing for missing food or absent nurser, and any ungranted wish.\" So, the action of opening and shutting the mouth, combined with the production of a nasal sound when the lips are closed, yielded the sound sequence \"Mama\", which may therefore count as the very first word. Peter MacNeilage sympathetically discusses this theory in his major book, The \"Origin of Speech\", linking it with Dean Falk's \"putting the baby down\" theory (see above). Needless to say, other scholars have suggested completely different candidates for \"Homo sapiens\"' very first word.\n\nWhile the biological language faculty is genetically inherited, actual languages or dialects are culturally transmitted, as are social norms, technological traditions and so forth. Biologists expect a robust co-evolutionary trajectory linking human genetic evolution with the evolution of culture. Individuals capable of rudimentary forms of protolanguage would have enjoyed enhanced access to cultural understandings, while these, conveyed in ways that young brains could readily learn, would in turn have become transmitted with increasing efficiency.\n\nIn some ways like beavers as they construct their dams, humans have always engaged in niche construction, creating novel environments to which they subsequently become adapted. Selection pressures associated with prior niches tend to become relaxed as humans depend increasingly on novel environments created continuously by their own productive activities. According to Steven Pinker, language is an adaptation to \"the cognitive niche\". Variations on the theme of ritual/speech co-evolution — according to which speech evolved for purposes of internal communication within a ritually constructed domain — have attempted to specify more precisely when, why and how this special niche was created by human collaborative activity.\n\n The Swiss scholar Ferdinand de Saussure founded linguistics as a twentieth-century professional discipline. Saussure regarded a language as a rule-governed system, much like a board game such as chess. In order to understand chess, he insisted, we must ignore such external factors as the weather prevailing during a particular session or the material composition of this or that piece. The game is autonomous with respect to its material embodiments. In the same way, when studying language, it's essential to focus on its internal structure as a social institution. External matters (\"e.g.\", the shape of the human tongue) are irrelevant from this standpoint. Saussure regarded 'speaking' \"(parole)\" as individual, ancillary and more or less accidental by comparison with \"language\" \"(langue)\", which he viewed as collective, systematic and essential.\n\nSaussure showed little interest in Darwin's theory of evolution by natural selection. Nor did he consider it worthwhile to speculate about how language might originally have evolved. Saussure's assumptions in fact cast doubt on the validity of narrowly conceived origins scenarios. His structuralist paradigm, when accepted in its original form, turns scholarly attention to a wider problem: how our species acquired the capacity to establish social institutions in general.\n\n In the United States, prior to and immediately following World War II, the dominant psychological paradigm was behaviourism. Within this conceptual framework, language was seen as a certain kind of behaviour — namely, verbal behaviour, to be studied much like any other kind of behaviour in the animal world. Rather as a laboratory rat learns how to find its way through an artificial maze, so a human child learns the verbal behaviour of the society into which it is born. The phonological, grammatical and other complexities of speech are in this sense \"external\" phenomena, inscribed into an initially unstructured brain. Language's emergence in \"Homo sapiens,\" from this perspective, presents no special theoretical challenge. Human behaviour, whether verbal or otherwise, illustrates the malleable nature of the mammalian — and especially the human — brain.\n\nNativism is the theory that humans are born with certain specialised cognitive modules enabling us to acquire highly complex bodies of knowledge such as the grammar of a language.\n\nFrom the mid-1950s onwards, Noam Chomsky, Jerry Fodor and others mounted what they conceptualised as a 'revolution' against behaviourism. Retrospectively, this became labelled 'the cognitive revolution'. Whereas behaviourism had denied the scientific validity of the concept of \"mind\", Chomsky replied that, in fact, the concept of \"body\" is more problematic. Behaviourists tended to view the child's brain as a \"tabula rasa\", initially lacking structure or cognitive content. According to B. F. Skinner, for example, richness of behavioural detail (whether verbal or non-verbal) emanated from the environment. Chomsky turned this idea on its head. The linguistic environment encountered by a young child, according to Chomsky's version of psychological nativism, is in fact hopelessly inadequate. No child could possibly acquire the complexities of grammar from such an impoverished source. Far from viewing language as wholly external, Chomsky re-conceptualised it as wholly internal. To explain how a child so rapidly and effortlessly acquires its natal language, he insisted, we must conclude that it comes into the world with the essentials of grammar already pre-installed. No other species, according to Chomsky, is genetically equipped with a language faculty — or indeed with anything remotely like one. The emergence of such a faculty in \"Homo sapiens\", from this standpoint, presents biological science with a major theoretical challenge.\n\nOne way to explain biological complexity is by reference to its inferred function. According to the influential philosopher John Austin, speech's primary function is action in the social world.\n\nSpeech acts, according to this body of theory, can be analysed on three different levels: locutionary, illocutionary and perlocutionary. An act is \"locutionary\" when viewed as the production of certain linguistic sounds — for example, practicing correct pronunciation in a foreign language. An act is \"illocutionary\" insofar as it constitutes an intervention in the world as jointly perceived or understood. Promising, marrying, divorcing, declaring, stating, authorising, announcing and so forth are all speech acts in this \"illocutionary\" sense. An act is \"perlocutionary\" when viewed in terms of its direct psychological effect on an audience. Frightening a baby by saying 'Boo!' would be an example of a \"perlocutionary\" act.\n\nFor Austin, \"doing things\" with words means, first and foremost, deploying \"illocutionary\" force. The secret of this is community participation or collusion. There must be a 'correct' (conventionally agreed) procedure, and all those concerned must accept that it has been properly followed. In the case of a priest declaring a couple to be man and wife, his words will have illocutionary force only if he is properly authorised and only if the ceremony is properly conducted, using words deemed appropriate to the occasion. Austin points out that should anyone attempt to baptize a penguin, the act would be null and void. For reasons which have nothing to do with physics, chemistry or biology, baptism is inappropriate to be applied to penguins, irrespective of the verbal formulation used.\n\nThis body of theory may have implications for speculative scenarios concerning the origins of speech. \"Doing things with words\" presupposes shared understandings and agreements pertaining not just to language but to social conduct more generally. Apes might produce sequences of structured sound, influencing one another in that way. To deploy \"illocutionary\" force, however, they would need to have entered a non-physical and non-biological realm — one of shared contractual and other intangibles. This novel cognitive domain consists of what philosophers term \"institutional facts\" — objective facts whose existence, paradoxically, depends on communal faith or belief. Few primatologists, evolutionary psychologists or anthropologists consider that nonhuman primates are capable of the necessary levels of joint attention, sustained commitment or collaboration in pursuit of future goals.\n\nBiosemiotics is a relatively new discipline, inspired in large part by the discovery of the genetic code in the early 1960s. Its basic assumption is that \"Homo sapiens\" is not alone in its reliance on codes and signs. Language and symbolic culture must have biological roots, hence semiotic principles must apply also in the animal world.\n\nThe discovery of the molecular structure of DNA apparently contradicted the idea that life could be explained, ultimately, in terms of the fundamental laws of physics. The letters of the genetic alphabet seemed to have \"meaning\", yet meaning is not a concept which has any place in physics. The natural science community initially solved this difficulty by invoking the concept of \"information\", treating information as independent of meaning. But a different solution to the puzzle was to recall that the laws of physics in themselves are never sufficient to explain natural phenomena. To explain, say, the unique physical and chemical characteristics of the planets in our solar system, scientists must work out how the laws of physics became constrained by particular sequences of events following the formation of the Sun.\n\nAccording to Howard Pattee — a pioneering figure in biosemiotics — the same principle applies to the evolution of life on earth, a process in which certain \"frozen accidents\" or \"natural constraints\" have from time to time drastically reduced the number of possible evolutionary outcomes. Codes, when they prove to be stable over evolutionary time, are constraints of this kind. The most fundamental such \"frozen accident\" was the emergence of DNA as a self-replicating molecule, but the history of life on earth has been characterised by a succession of comparably dramatic events, each of which can be conceptualised as the emergence of a new code. From this perspective, the evolutionary emergence of spoken language was one more event of essentially the same kind.\n\nIn 1975, the Israeli theoretical biologist Amotz Zahavi proposed a novel theory which, although controversial, has come to dominate Darwinian thinking on how signals evolve. Zahavi's \"handicap principle\" states that to be effective, signals must be reliable; to be reliable, the bodily investment in them must be so high as to make cheating unprofitable.\n\nParadoxically, if this logic is accepted, signals in nature evolve not to be efficient but, on the contrary, to be elaborate and wasteful of time and energy. A peacock's tail is the classic illustration. Zahavi's theory is that since peahens are on the look-out for male braggarts and cheats, they insist on a display of quality so costly that only a genuinely fit peacock could afford to pay. Needless to say, not all signals in the animal world are quite as elaborate as a peacock's tail. But if Zahavi is correct, all require some bodily investment — an expenditure of time and energy which \"handicaps\" the signaller in some way.\n\nAnimal vocalisations (according to Zahavi) are reliable because they are faithful reflections of the state of the signaller's body. To switch from an honest to a deceitful call, the animal would have to adopt a different bodily posture. Since every bodily action has its own optimal starting position, changing that position to produce a false message would interfere with the task of carrying out the action really intended. The gains made by cheating would not make up for the losses incurred by assuming an improper posture — and so the phony message turns out to be not worth its price. This may explain, in particular, why ape and monkey vocal signals have evolved to be so strikingly inflexible when compared with the varied speech sounds produced by the human tongue. The apparent inflexibility of chimpanzee vocalisations may strike the human observer as surprising until we realise that being inflexible is necessarily bound up with being perceptibly honest in the sense of \"hard-to-fake\".\n\nIf we accept this theory, the emergence of speech becomes theoretically impossible. Communication of this kind just cannot evolve. The problem is that words are cheap. Nothing about their acoustic features can reassure listeners that they are genuine and not fakes. Any strategy of reliance on someone else's tongue — perhaps the most flexible organ in the body — presupposes unprecedented levels of honesty and trust. To date, Darwinian thinkers have found it difficult to explain the requisite levels of community-wide cooperation and trust.\n\nAn influential standard textbook is \"Animal Signals,\" by John Maynard Smith and David Harper. These authors divide the costs of communication into two components, (1) the investment necessary to ensure transmission of a discernible signal; (2) the investment necessary to guarantee that each signal is reliable and not a fake. The authors point out that although costs in the second category may be relatively low, they are not zero. Even in relatively relaxed, cooperative social contexts — for example, when communication is occurring between genetic kin — some investment must be made to guarantee reliability. In short, the notion of super-efficient communication — eliminating all costs except those necessary for successful transmission — is biologically unrealistic. Yet speech comes precisely into this category.\n\nCognitive linguistics views linguistic structure as arising continuously out of usage. Speakers are forever discovering new ways to convey meanings by producing sounds, and in some cases these novel strategies become conventionalised. Between phonological structure and semantic structure there is no causal relationship. Instead, each novel pairing of sound and meaning involves an imaginative leap.\n\nIn their book, \"Metaphors We Live By,\" George Lakoff and Mark Johnson helped pioneer this approach, claiming that \"metaphor\" is what makes human thought special. All language, they argued, is permeated with metaphor, whose use in fact \"constitutes\" distinctively human — that is, distinctively abstract — thought. To conceptualise things which cannot be directly perceived — intangibles such as time, life, reason, mind, society or justice — we have no choice but to set out from more concrete and directly perceptible phenomena such as motion, location, distance, size and so forth. In all cultures across the world, according to Lakoff and Johnson, people resort to such familiar metaphors as \"ideas are locations, thinking is moving\" and \"mind is body\". For example, we might express the idea of \"arriving at a crucial point in our argument\" by proceeding as if literally travelling from one physical location to the next.\n\nMetaphors, by definition, are not literally true. Strictly speaking, they are fictions — from a pedantic standpoint, even falsehoods. But if we couldn't resort to metaphorical fictions, it's doubtful whether we could even form conceptual representations of such nebulous phenomena as \"ideas\", thoughts\", \"minds\", and so forth.\n\nThe bearing of these ideas on current thinking on speech origins remains unclear. One suggestion is that ape communication tends to resist metaphor for social reasons. Since they inhabit a Darwinian (as opposed to morally regulated) social world, these animals are under strong competitive pressure \"not\" to accept patent fictions as valid communicative currency. Ape vocal communication tends to be inflexible, marginalising the ultra-flexible tongue, precisely because listeners treat with suspicion any signal which might prove to be a fake. Such insistence on perceptible veracity is clearly incompatible with metaphoric usage. An implication is that neither articulate speech nor distinctively human abstract thought could have begun evolving until our ancestors had become more cooperative and trusting of one another's communicative intentions.\n\nWhen people converse with one another, according to the American philosopher John Searle, they're making moves, not in the real world which other species inhabit, but in a shared virtual realm peculiar to ourselves. Unlike the deployment of muscular effort to move a physical object, the deployment of illocutionary force requires no physical effort (except movement of the tongue/mouth to produce speech) and produces no effect which any measuring device could detect. Instead, our action takes place on a quite different level — that of \"social\" reality. This kind of reality is in one sense hallucinatory, being a product of collective intentionality. It consists, not of \"brute facts\" — facts which exist anyway, irrespective of anyone's belief — but of \"institutional facts\", which \"exist\" only if you believe in them. Government, marriage, citizenship and money are examples of \"institutional facts\". One can distinguish between \"brute\" facts and \"institutional\" ones by applying a simple test. Suppose no one believed in the fact — would it still be true? If the answer is \"yes\", it's \"brute\". If the answer is \"no\", it's \"institutional\".\n\nThe facts of language in general and of speech in particular are, from this perspective, \"institutional\" rather than \"brute\". The semantic meaning of a word, for example, is whatever its users imagine it to be. To \"do things with words\" is to operate in a virtual world which seems real because we share it in common. In this incorporeal world, the laws of physics, chemistry and biology do not apply. That explains why illocutionary force can be deployed without exerting muscular effort. Apes and monkeys inhabit the \"brute\" world. To make an impact, they must scream, bark, threaten, seduce or in other ways invest bodily effort. If they were invited to play chess, they would be unable to resist throwing their pieces at one another. Speech is not like that. A few movements of the tongue, under appropriate conditions, can be sufficient to open parliament, annul a marriage, confer a knighthood or declare war. To explain, on a Darwinian basis, how such apparent magic first began to work, we must ask how, when and why \"Homo sapiens\" succeeded in establishing the wider domain of institutional facts.\n\n\"Brute facts\", in the terminology of speech act philosopher John Searle, are facts which are true anyway, regardless of human belief. Suppose you don't believe in gravity: jump over a cliff and you'll still fall. Natural science is the study of facts of this kind. \"Institutional facts\" are fictions accorded factual status within human social institutions. Monetary and commercial facts are fictions of this kind. The complexities of today's global currency system are facts only while we believe in them: suspend the belief and the facts correspondingly dissolve. Yet although institutional facts rest on human belief, that doesn't make them mere distortions or hallucinations. Take my confidence that these two five-pound banknotes in my pocket are worth ten pounds. That's not merely my subjective belief: it's an objective, indisputable fact. But now imagine a collapse of public confidence in the currency system. Suddenly, the realities in my pocket dissolve.\n\nScholars who doubt the scientific validity of the notion of \"institutional facts\" include Noam Chomsky, for whom language is not social. In Chomsky's view, language is a natural object (a component of the individual brain) and its study, therefore, a branch of natural science. In explaining the origin of language, scholars in this intellectual camp invoke non-social developments — in Chomsky's case, a random genetic mutation. Chomsky argues that language might exist inside the brain of a single mutant gorilla even if no one else believed in it, even if no one else existed apart from the mutant — and even if the gorilla in question remained unaware of its existence, never actually speaking. In the opposite philosophical camp are those who, in the tradition of Ferdinand de Saussure, argue that if no one believed in words or rules, they simply would not exist. These scholars, correspondingly, regard language as essentially institutional, concluding that linguistics should be considered a topic within social science. In explaining the evolutionary emergence of language, scholars in this intellectual camp tend to invoke profound changes in social relationships.\n\nCriticism. Darwinian scientists today see little value in the traditional distinction between \"natural\" and \"social\" science. Darwinism in its modern form is the study of cooperation and competition in nature — a topic which is intrinsically social. Against this background, there is an increasing awareness among evolutionary linguists and Darwinian anthropologists that traditional inter-disciplinary barriers can have damaging consequences for investigations into the origins of speech.\n\n"}
{"id": "4574535", "url": "https://en.wikipedia.org/wiki?curid=4574535", "title": "Paremiology", "text": "Paremiology\n\nParemiology () is the collection and study of proverbs.\n\nParemiology can be dated back as far as Aristotle. Paremiography, on the other hand, is the collection of proverbs. The proverb scholar Wolfgang Mieder defines the term \"proverb\" as follows:\n\nAs well as actual proverbs, the following may be considered proverbial phrases:\n\nTypical stylistic features of proverbs (as Shirley Arora points out in her article, \"The Perception of Proverbiality\" (1984)) are:\n\n\nIn some languages, assonance, the repetition of a vowel, is also exploited in forming artistic proverbs, such as the following extreme example from Oromo, of Ethiopia.\nSimilarly, from Tajik:\nNotice that in all of these cases of complete assonance, the vowel is , the most common vowel in human languages.\n\nAlso in Amharic, complete assonance is not infrequent when verbs are in the 3rd person masculine singular, past tense. The vowel <ä> is the most frequent vowel in the language.\n\nInternal features that can be found quite frequently include:\n\nTo make the respective statement more general most proverbs are based on a metaphor. Further typical features of the proverb are its shortness and the fact that its author is generally unknown.\nIn the article \"Tensions in Proverbs: More Light on International Understanding\", Joseph Raymond comments on what common Russian proverbs from the 18th and 19th centuries portray: Potent antiauthoritarian proverbs reflected tensions between the Russian people and the Czar. The rollickingly malicious undertone of these folk verbalizations constitutes what might be labeled a \"paremiological revolt\". To avoid openly criticizing a given authority or cultural pattern, folk take recourse to proverbial expressions which voice personal tensions in a tone of generalized consent. Proverbs that speak to the political disgruntlement include: \"When the Czar spits into the soup dish, it fairly bursts with pride\"; \"If the Czar be a rhymester, woe be to the poets\"; and \"The hen of the Czarina herself does not lay swan's eggs\". While none of these proverbs state directly, \"I hate the Czar and detest my situation\" (which would have been incredibly dangerous), they do get their points across.\n\nProverbs are found in many parts of the world, but some areas seem to have richer stores of proverbs than others (such as West Africa), while others have hardly any (North and South America) (Mieder 2004b:108,109).\n\nProverbs are used by speakers for a variety of purposes. Sometimes they are used as a way of saying something gently, in a veiled way (Obeng 1996). Other times, they are used to carry more weight in a discussion, a weak person is able to enlist the tradition of the ancestors to support his position, or even to argue a legal case. Proverbs can also be used to simply make a conversation/discussion more lively. In many parts of the world, the use of proverbs is a mark of being a good orator.\n\nThe study of proverbs has application in a number of fields. Clearly, those who study folklore and literature are interested in them, but scholars from a variety of fields have found ways to profitably incorporate the study proverbs. For example, they have been used to study abstract reasoning of children, acculturation of immigrants, intelligence, the differing mental processes in mental illness, cultural themes, etc. Proverbs have also been incorporated into the strategies of social workers, teachers, preachers, and even politicians. (For the deliberate use of proverbs as a propaganda tool by Nazis, see Mieder 1982.)\n\nThere are collections of sayings that offer instructions on how to play certain games, such as dominoes (Borajo \"et al.\" 1990) and the Oriental board game go (Mitchell 2001). However, these are not prototypical proverbs in that their application is limited to one domain.\n\nOne of the most important developments in the study of proverbs (as in folklore scholarship more generally) was the shift to more ethnographic approaches in the 1960s. This approach attempted to explain proverb use in relation to the context of a speech event, rather than only in terms of the content and meaning of the proverb.\n\nAnother important development in scholarship on proverbs has been applying methods from cognitive science to understand the uses and effects of proverbs and proverbial metaphors in social relations.\n\n"}
{"id": "52798588", "url": "https://en.wikipedia.org/wiki?curid=52798588", "title": "Promotion of Putonghua", "text": "Promotion of Putonghua\n\nPromotion of Putonghua () is a movement led by the government of China since the Communist Party come to the power. Currently, the Promotion of Putonghua is led by State Language Work Committee (), a working organ which is the language regulator of People's Republic of China.\n\n"}
{"id": "2938373", "url": "https://en.wikipedia.org/wiki?curid=2938373", "title": "Proxy statement", "text": "Proxy statement\n\nA proxy statement is a statement required of a firm when soliciting shareholder votes. This statement is filed in advance of the annual meeting. The firm needs to file a proxy statement, otherwise known as a Form DEF 14A (Definitive Proxy Statement), with the U.S. Securities and Exchange Commission. This statement is useful in assessing how management is paid and potential conflict-of-interest issues with auditors. The statement includes:\n\nSEC proxy rules: The term \"proxy statement\" means the statement required by Section 240.14a-3(a) whether or not contained in a single document.\n\nIn many cases, shareholder votes—particularly institutional shareholder votes—are determined by proxy firms which advise the shareholders...\n\nTraditionally, broker-dealers have been permitted to vote for \"routine\" proposals on behalf of their shareholders if the shareholders do not return the proxy statement. This has been controversial, and in 2006 the NYSE Proxy Working Group recommended that the rules be modified so that uncontested director elections were not considered routine. The SEC approved the rule on July 1, 2009.\n\nIn July 2010, the SEC announced that it was seeking public comment on the efficiency of the proxy system.\n\nThere has been some controversy over \"proxy access\" which is a method to allow shareholders to nominate candidates which appear on the proxy statement. Currently, only the nominating board can place candidates on the proxy statement. The United States Dodd–Frank Wall Street Reform and Consumer Protection Act specifically allowed the SEC to rule on this issue. In 2010, the SEC passed a rule which allowed certain shareholders to place candidates on the proxy statement; however, the rule was struck down by the United States Court of Appeals for the District of Columbia Circuit in 2011.\n\n"}
{"id": "31614392", "url": "https://en.wikipedia.org/wiki?curid=31614392", "title": "Reallocation (media)", "text": "Reallocation (media)\n\nReallocation is a term in the media industry used to describe the practice of relocating an unsuccessful series that was originally developed for a broadcast network onto a cable network in hopes of gaining the attention and interest of a niche audience as well as growing a larger audience.\n\nReasons for reallocation are not always due to cancellation. Reallocation is also used to regain expenses lost due to production fees on under performing content. In some cases reallocation is used in order to promote an unsuccessful series.\n\nAlthough not a common practice,\n\n Lotz, Amanda D. (2007) \"The Television Will Be Revolutionized\". New York, NY: New York University Press. p. 126-128\n"}
{"id": "17731828", "url": "https://en.wikipedia.org/wiki?curid=17731828", "title": "Saam kap dai", "text": "Saam kap dai\n\nSaam kap dai （; IPA: ） is a writing style combining Classical Chinese, Cantonese and Standard Chinese. The articles and stories written in saam kap dai first appeared in Canton (Guangzhou) newspapers in the 1940s to 1950s. It became more popular in Hong Kong newspapers from the late 1940s to the 1960s.\n\nThe use of different style of Chinese gives the article varied tones. Sentences in Classical Chinese give the formal tone. Standard Chinese is usually neutral. Cantonese gives an impression of colloquial, local speech and slang.\n"}
{"id": "870352", "url": "https://en.wikipedia.org/wiki?curid=870352", "title": "Sentence (linguistics)", "text": "Sentence (linguistics)\n\nIn non-functional linguistics, a sentence is a textual unit consisting of one or more words that are grammatically linked. In functional linguistics, a sentence is a unit of written texts delimited by graphological features such as upper case letters and markers such as periods, question marks, and exclamation marks. This notion contrasts with a curve, which is delimited by phonologic features such as pitch and loudness and markers such as pauses; and with a clause, which is a sequence of words that represents some process going on throughout time. This entry is mainly about \"sentence\" in its non-functional sense, though much work in functional linguistics is indirectly cited or considered such as the categories of Speech Act Theory.\n\nA sentence can include words grouped meaningfully to express a statement, question, exclamation, request, command or suggestion.\nA sentence is a set of words that in principle tells a complete thought (although it may make little sense taken in isolation out of context) It may be a simple phrase, but it conveys enough meaning to imply a clause, even if it is not explicit; for example, \"Two\" as a sentence (in answer to the question \"How many were there?\") implies the clause \"There were two.\" Typically a sentence contains a subject and predicate. A sentence can also be defined purely in orthographic terms, as a group of words starting with a capital letter and ending in a full stop.\n\nIn the teaching of writing skills (composition skills), students are generally required to express (rather than imply) the elements of a sentence, leading to the schoolbook definition of a sentence as one that must [explicitly] include a subject and a verb. For example, in second-language acquisition, teachers often reject one-word answers that only imply a clause, commanding the student to \"give me a complete sentence,\" by which they mean an explicit one.\n\nAs with all language expressions, sentences might contain function and content words and contain properties such as characteristic intonation and timing patterns.\n\nSentences are generally characterized in most languages by the inclusion of a finite verb, e.g. \"The quick brown fox \"jumps\" over the lazy dog.\"\n\nIn non-functional linguistics, a simple complete sentence consists of a single clause. In functional linguistics, a sentence is typically associated with a clause and a clause can be either a clause simplex or a clause complex. A clause is a clause simplex if it represents a single process going on through time and it is a clause complex if it represents a logical relation between two or more processes and is thus composed of two or more clause simplexes.\n\nA clause (simplex) typically contains a predication structure with a subject noun phrase and a finite verb. Although the subject is usually a noun phrase, other kinds of phrases (such as gerund phrases) work as well, and some languages allow subjects to be omitted. In the examples below, the subject of the outmost clause simplex is in italics and the subject of \"boiling\" is in square brackets. Notice that there is clause embedding in the second and third examples.\n\nThere are two types of clauses: \"independent\" and \"non-independent\"/\"interdependent\". An independent clause realises a speech act such as a statement, a question, a command or an offer. A non-independent clause does not realise any act. A non-independent clause (simplex or complex) is usually logically related to other non-independent clauses. Together they usually constitute a single independent clause (complex). For that reason, non-independent clauses are also called \"interdependent\". For instance, the non-independent clause \"because I have no friends\" is related to the non-independent clause \"I don't go out\" in \"I don't go out, because I have no friends\". The whole clause complex is independent because it realises a statement. What is stated is the causal nexus between having no friend and not going out. When such a statement is acted out, the fact that the speaker doesn't go out is already established, therefore it cannot be stated. What is still open and under negotiation is the reason for that fact. The causal nexus is represented by the independent clause complex and not by the two interdependent clause simplexes.\n\nSee also copula for the consequences of the verb \"to be\" on the theory of sentence structure.\n\nOne traditional scheme for classifying English sentences is by clause structure, the number and types of clauses in the sentence with finite verbs.\n\nSentences can also be classified based on their purpose:\n\nA major sentence is a \"regular\" sentence; it has a subject and a predicate, e.g. \"I have a ball.\". In this sentence, one can change the persons, e.g. \"We have a ball.\". However, a minor sentence is an irregular type of sentence that does not contain a main clause, e.g. \"Mary!\", \"Precisely so.\", \"Next Tuesday evening after it gets dark.\". Other examples of minor sentences are headings (e.g. the heading of this entry), stereotyped expressions (\"Hello!\"), emotional expressions (\"Wow!\"), proverbs, etc. These can also include nominal sentences like \"The more, the merrier\". These mostly omit a main verb for the sake of conciseness, but may also do so in order to intensify the meaning around the nouns.\n\nSentences that comprise a single word are called word sentences, and the words themselves sentence words.\n\nThe 1980s saw a renewed surge in interest in sentence length, primarily in relation to \"other syntactic phenomena\".\n\nOne definition of the average sentence length of a prose passage is the ratio of the number of words to the number of sentences.\nThe textbook \"Mathematical linguistics\", by András Kornai, suggests that in \"journalistic prose the median sentence length is above 15 words.\"\nThe average length of a sentence generally serves as a measure of sentence difficulty or complexity. In general, as the average sentence length increases, the complexity of the sentences also increases.\n\nAnother definition of \"sentence length\" is the number of clauses in the sentence, whereas the \"clause length\" is the number of phones in the clause.\n\nResearch by Erik Schils and Pieter de Haan by sampling five texts showed that two adjacent sentences are more likely to have similar lengths than two non-adjacent sentences, and almost certainly have a similar length when in a work of fiction. This countered the theory that \"authors may aim at an alternation of long and short sentences.\"\nSentence length, as well as word difficulty, are both factors in the readability of a sentence; however, other factors, such as the presence of conjunctions, have been said to \"facilitate comprehension considerably\".\n\n"}
{"id": "48878990", "url": "https://en.wikipedia.org/wiki?curid=48878990", "title": "SkELL", "text": "SkELL\n\nSkELL is an abbreviation of Sketch Engine for Language Learning. It is a web interface for language learning. The main purpose is to help students and teachers of languages. SkELL has its own corpus that was gathered so that contained texts covering everyday, standard, formal, and professional language. In the corpus, there are a total of more than 60 million sentences and more than one billion words.\n\nThe SkELL interface provides features such as simple search showing words in context, but the maximum of displayed lines (concordances, in fact) is 40. However, the frequency of searched query is located below the search box and expressed with the number \"hits per million\". The second function is word sketch which enables showing collocates for a given word or words. The last one is named as similar words. It visualises similar words to searched word in a word cloud.\n\nThe tool has been available also for the Russian language (since 2015) and the Czech language (since 2017). \n\nSkELL offers three types of searches.\n\n\nThe corpus for English SkELL consists of English Wikipedia (special sorted out 130,000 articles), English collection of Project Gutenberg, a subset from the web corpus enTenTen14, the whole British National Corpus, and free new sources.\n\nAfter gathering and pre-cleaning (all structures have removed except sentences) data has run through processing pipe: normalization, tokenization, TreeTagger for English, and deduplication. The further process was a compilation of the corpus using manatee indexing library. In the end, all sentences were scored with the GDEX tool.\n\n"}
{"id": "39590024", "url": "https://en.wikipedia.org/wiki?curid=39590024", "title": "Tautology (language)", "text": "Tautology (language)\n\nIn literary criticism and rhetoric, a tautology is a statement which repeats the same idea, using near-synonymous morphemes, words, or phrases, that is, \"saying the same thing twice\". Tautology and pleonasm are not consistently differentiated in the literature.\n\nLike pleonasm, it is often considered a fault of style when unintentional. On the other hand, an intentional repetition may be an effective way to emphasize a thought, or help the listener or reader understand a point. \n\nSometimes logical tautologies like \"Boys will be boys\" are conflated with language tautologies, but in general, a rhetorical tautology is not inherently true.\n\nThe word was coined in Hellenistic Greek from ταὐτός (\"the same\") plus λόγος (\"word/idea\"), and transmitted through 3rd-century Latin \"tautologia\" and French \"tautologie\". It first appears in English in the 16th century. The use in the term logical tautology was introduced in English by Wittgenstein in 1919, perhaps following Auguste Comte's usage in 1835.\n\n\nIntentional repetition of meaning intends to amplify or emphasize a particular, usually significant, fact about what is being discussed. For example, a gift is, by definition, free of charge; using the phrase \"free gift\" might emphasize that there are no hidden conditions or fine print, be it the expectation of money or reciprocation, or that the gift is being given by volition.\n\nThis is related to the rhetorical device of hendiadys, where one concept is expressed through the use of two descriptive words or phrases. For example, \"goblets and gold\" meaning wealth, or \"this day and age\" meaning the present time. Superficially these expressions may seem tautological, but they are stylistically sound because the repeated meaning is just a way to emphasize the same idea.\n\nThe use of tautologies is, however, usually unintentional. For example, the phrases \"mental telepathy\", \"planned conspiracies\", and \"small dwarfs\" imply that there are such things as \"physical telepathy, spontaneous conspiracies, and giant dwarfs.\")\n\nParallelism is not tautology, but rather a particular stylistic device. Much Old Testament poetry is based on parallelism: the same thing said twice, but in slightly different ways (Fowler puts it as pleonasm). However, modern biblical study emphasizes that there are subtle distinctions and developments between the two lines, such that they are usually not truly the \"same thing.\" Parallelism can be found wherever there is poetry in the Bible: Psalms, the Books of the Prophets, and in other areas as well.\n\n"}
{"id": "27404794", "url": "https://en.wikipedia.org/wiki?curid=27404794", "title": "The Interpreter (Kim novel)", "text": "The Interpreter (Kim novel)\n\nThe Interpreter (2003) is Suki Kim’s first novel. In \"The Interpreter\", Kim creates a twenty-nine-year-old Korean American court interpreter named Suzy Park who makes a startling and ominous discovery during one court case which ultimately reveals the mystery of her parents' homicide. The award winning novel, mainly a murder mystery, breaks through the stereotypical images of the happy immigrant experience with a story of pain, loss, and murder.\n\nSuzy Park is a young, attractive, and achingly alone Korean American woman who works as a court interpreter for the New York City court system. She has had two rocky relationships with married men, worked a series of unsatisfying jobs, and cut ties with her family before her parents were shot dead in an unsolved double murder. The life she has chosen as an interpreter is a reflection of Suzy's searching for her own identity and trying to bridge the two cultures to both of which she feels a detachment. During one court case she discovers that her parents were not murdered by random violence, as the police had indicated, but instead had been shot by political enemies. The discovery provides the glint of a new lead for Park, and the novel tracks her investigation into what really happened, which ultimately reveals the mystery of her parents' homicide.\n\n\n\n"}
{"id": "34324140", "url": "https://en.wikipedia.org/wiki?curid=34324140", "title": "The Khovansky Foundation", "text": "The Khovansky Foundation\n\nThe Khovansky Foundation () is a non-profit organization, founded in Voronezh in 1899. It is financed by funds ownership and donations from the citizens and associations.\n\nThe motto of the foundation are the words of A. A. Khovansky: \"“The treasure of knowledge is invaluable, it also has the distinction, that the one who donate it doesn't lose it, moreover, a donating hier is also an acquisition — with the growing level of public education grow those who donate it.”\".\n\nAfter the death of the publisher A. Khovansky (January 29, 1899) there was organized the Foundation and the Prize for the best teachers of filology and history.\"\n\nIn 1917, the Foundation and the magazine Filologicheskie Zapiski ceased its activities almost for a century.\n\nThe Khovansky-fund was reconstituted on the occasion of the 110th anniversary (November 4, 2009), again with the aim to support teachers and publishers.\n\nThe objective of the fund is to determine in special contest \"The Living Word\" the best teachers of languages and history in order to provide material support to winners.\n\nAnother purpose of the Fund is to recognize a special annual premium for publishers.\n\nThe Foundation carries out research in the areas of: comparative linguistics, comparative mythology, local history, ethnic and social psychology, psycholinguistics and semiotics.\n\nThe Foundation has established new merit awards in commemoration of the 200th anniversary of Alexei Khovansky: medal \"The Living Word\" — for personal rewarding teachers of English and other languages and the statuette “Alexy” — for teaching and creative teams.\n\nThe \"Living Word\" medal has been awarded to several leading Russian academicians. In 2014, British translators Kit Bicket and Pascal Cissé became the first anglophone winners of the medal, for their work on a collection of essays on Russian and British heraldry and symbols \"The Semiotic Cycle: Signs of a Woman in Love\". The collection was specially translated by the Khovansky Foundation as part of the 2014 UK/Russia Year of Culture.\n"}
{"id": "34565118", "url": "https://en.wikipedia.org/wiki?curid=34565118", "title": "Usus", "text": "Usus\n\nUsus ( — usage; long-established rule, practice, custom) is a term referring to the common usage of linguistic units (words, idioms, forms) in a particular speech community.\n\nUsus can be contrasted with both occasional usage and prescribed standard usage. The term is used to designate usage that has wide currency and acceptance among speakers of a language, even if it diverges from the \"high\" literary standard. \n\nUsus is one of the crucial terms in the research of Danish linguists Otto Jespersen and Louis Hjelmslev.\n"}
{"id": "29584019", "url": "https://en.wikipedia.org/wiki?curid=29584019", "title": "Word family", "text": "Word family\n\nA word family is the base form of a word plus its inflected forms and derived forms made from affixes. In the English language, inflectional affixes include third person -\"s\", verbal \"-ed\" and \"-ing\", plural -\"s\", possessive -\"s\", comparative -\"er\" and superlative -\"est\". Derivational affixes include -\"able, -er, -ish, -less, -ly, -ness, -th, -y, non-, un-, -al, -ation, -ess, -ful, -ism, -ist, -ity, -ize/-ise, -ment, in-\". The idea is that a base word and its inflected forms support the same core meaning, and can be considered learned words if a learner knows both the base word and the affix.\nBauer and Nation proposed seven levels of affixes.\n\n"}
{"id": "32977", "url": "https://en.wikipedia.org/wiki?curid=32977", "title": "Writing", "text": "Writing\n\nWriting is a medium of human communication that represents language and emotion with signs and symbols. In most languages, writing is a complement to speech or spoken language. Writing is not a language, but a tool used to make languages be read. Within a language system, writing relies on many of the same structures as speech, such as vocabulary, grammar, and semantics, with the added dependency of a system of signs or symbols. The result of writing is called \"text\", and the recipient of text is called a \"reader\". Motivations for writing include publication, storytelling, correspondence, record keeping and diary. Writing has been instrumental in keeping history, maintaining culture, dissemination of knowledge through the media and the formation of legal systems.\n\nAs human societies emerged, the development of writing was driven by pragmatic exigencies such as exchanging information, maintaining financial accounts, codifying laws and recording history. Around the 4th millennium BC, the complexity of trade and administration in Mesopotamia outgrew human memory, and writing became a more dependable method of recording and presenting transactions in a permanent form. In both ancient Egypt and Mesoamerica, writing may have evolved through calendric and a political necessity for recording historical and environmental events.\n\nH.G. Wells argued that writing has the ability to \"put agreements, laws, commandments on record. It made the growth of states larger than the old city states possible. It made a continuous historical consciousness possible. The command of the priest or king and his seal could go far beyond his sight and voice and could survive his death\".\n\nThe major writing systems—methods of inscription—broadly fall into five categories: logographic, syllabic, alphabetic, featural, and ideographic (symbols for ideas). A sixth category, pictographic, is insufficient to represent language on its own, but often forms the core of logographies.\n\nA logogram is a written character which represents a word or morpheme. A vast number of logograms are needed to write Chinese characters, cuneiform, and Mayan, where a glyph may stand for a morpheme, a syllable, or both—(\"logoconsonantal\" in the case of hieroglyphs). Many logograms have an ideographic component (Chinese \"radicals\", hieroglyphic \"determiners\"). For example, in Mayan, the glyph for \"fin\", pronounced \"ka\", was also used to represent the syllable \"ka\" whenever the pronunciation of a logogram needed to be indicated, or when there was no logogram. In Chinese, about 90% of characters are compounds of a semantic (meaning) element called a \"radical\" with an existing character to indicate the pronunciation, called a \"phonetic\". However, such phonetic elements complement the logographic elements, rather than vice versa.\n\nThe main logographic system in use today is Chinese characters, used with some modification for the various languages or dialects of China, Japan, and sometimes in Korean despite the fact that in South and North Korea, the phonetic Hangul system is mainly used.\n\nA syllabary is a set of written symbols that represent (or approximate) syllables. A glyph in a syllabary typically represents a consonant followed by a vowel, or just a vowel alone, though in some scripts more complex syllables (such as consonant-vowel-consonant, or consonant-consonant-vowel) may have dedicated glyphs. Phonetically related syllables are not so indicated in the script. For instance, the syllable \"ka\" may look nothing like the syllable \"ki\", nor will syllables with the same vowels be similar.\n\nSyllabaries are best suited to languages with a relatively simple syllable structure, such as Japanese. Other languages that use syllabic writing include the Linear B script for Mycenaean Greek; Cherokee; Ndjuka, an English-based creole language of Surinam; and the Vai script of Liberia. Most logographic systems have a strong syllabic component. Ethiopic, though technically an abugida, has fused consonants and vowels together to the point where it is learned as if it were a syllabary.\n\nAn alphabet is a set of symbols, each of which represents or historically represented a phoneme of the language. In a perfectly phonological alphabet, the phonemes and letters would correspond perfectly in two directions: a writer could predict the spelling of a word given its pronunciation, and a speaker could predict the pronunciation of a word given its spelling.\n\nAs languages often evolve independently of their writing systems, and writing systems have been borrowed for languages they were not designed for, the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.\n\nIn most of the writing systems of the Middle East, it is usually only the consonants of a word that are written, although vowels may be indicated by the addition of various diacritical marks. Writing systems based primarily on marking the consonant phonemes alone date back to the hieroglyphs of ancient Egypt. Such systems are called \"abjads\", derived from the Arabic word for \"alphabet\".\n\nIn most of the alphabets of India and Southeast Asia, vowels are indicated through diacritics or modification of the shape of the consonant. These are called \"abugidas\". Some abugidas, such as Ethiopic and Cree, are learned by children as syllabaries, and so are often called \"syllabics\". However, unlike true syllabaries, there is not an independent glyph for each syllable.\n\nSometimes the term \"alphabet\" is restricted to systems with separate letters for consonants and vowels, such as the Latin alphabet, although abugidas and abjads may also be accepted as alphabets. Because of this use, Greek is often considered to be the first alphabet.\n\nA featural script notates the building blocks of the phonemes that make up a language. For instance, all sounds pronounced with the lips (\"labial\" sounds) may have some element in common. In the Latin alphabet, this is accidentally the case with the letters \"b\" and \"p\"; however, labial \"m\" is completely dissimilar, and the similar-looking \"q\" and \"d\" are not labial. In Korean hangul, however, all four labial consonants are based on the same basic element, but in practice, Korean is learned by children as an ordinary alphabet, and the featural elements tend to pass unnoticed.\n\nAnother featural script is SignWriting, the most popular writing system for many sign languages, where the shapes and movements of the hands and face are represented iconically. Featural scripts are also common in fictional or invented systems, such as J.R.R. Tolkien's Tengwar.\n\nHistorians draw a sharp distinction between prehistory and history, with history defined by the advent of writing. The cave paintings and petroglyphs of prehistoric peoples can be considered precursors of writing, but they are not considered true writing because they did not represent language directly.\n\nWriting systems develop and change based on the needs of the people who use them. Sometimes the shape, orientation, and meaning of individual signs changes over time. By tracing the development of a script, it is possible to learn about the needs of the people who used the script as well as how the script changed over time.\n\nThe many tools and writing materials used throughout history include stone tablets, clay tablets, bamboo slats, papyrus, wax tablets, vellum, parchment, paper, copperplate, styluses, quills, ink brushes, pencils, pens, and many styles of lithography. The Incas used knotted cords known as quipu (or khipu) for keeping records.\n\nThe typewriter and various forms of word processors have subsequently become widespread writing tools, and various studies have compared the ways in which writers have framed the experience of writing with such tools as compared with the pen or pencil.\n\nBy definition, the modern practice of history begins with written records. Evidence of human culture without writing is the realm of prehistory. The Dispilio Tablet (Greece), Jiahu symbols (China) and Tărtăria tablets (Romania), which have been carbon dated to the 6th millennium BC, are recent discoveries of the earliest known neolithic writings.\n\nWhile neolithic writing is a current research topic, conventional history assumes that the writing process first evolved from economic necessity in the ancient Near East. Writing most likely began as a consequence of political expansion in ancient cultures, which needed reliable means for transmitting information, maintaining financial accounts, keeping historical records, and similar activities. Around the 4th millennium BC, the complexity of trade and administration outgrew the power of memory, and writing became a more dependable method of recording and presenting transactions in a permanent form.\n\nArchaeologist Denise Schmandt-Besserat determined the link between previously uncategorized clay \"tokens\", the oldest of which have been found in the Zagros region of Iran, and the first known writing, Mesopotamian cuneiform. In approximately 8000 BC, the Mesopotamians began using clay tokens to count their agricultural and manufactured goods. Later they began placing these tokens inside large, hollow clay containers (bulla, or globular envelopes) which were then sealed. The quantity of tokens in each container came to be expressed by impressing, on the container's surface, one picture for each instance of the token inside. They next dispensed with the tokens, relying solely on symbols for the tokens, drawn on clay surfaces. To avoid making a picture for each instance of the same object (for example: 100 pictures of a hat to represent 100 hats), they 'counted' the objects by using various small marks. In this way the Sumerians added \"a system for enumerating objects to their incipient system of symbols\".\n\nThe original Mesopotamian writing system (believed to be the world's oldest) was derived around 3600 BC from this method of keeping accounts. By the end of the 4th millennium BC, the Mesopotamians were using a triangular-shaped stylus pressed into soft clay to record numbers. This system was gradually augmented with using a sharp stylus to indicate what was being counted by means of pictographs. Round-stylus and sharp-stylus writing was gradually replaced by writing using a wedge-shaped stylus (hence the term cuneiform), at first only for logograms, but by the 29th century BC also for phonetic elements. Around 2700 BC, cuneiform began to represent syllables of spoken Sumerian. About that time, Mesopotamian cuneiform became a general purpose writing system for logograms, syllables, and numbers. This script was adapted to another Mesopotamian language, the East Semitic Akkadian (Assyrian and Babylonian) around 2600 BC, and then to others such as Elamite, Hattian, Hurrian and Hittite. Scripts similar in appearance to this writing system include those for Ugaritic and Old Persian. With the adoption of Aramaic as the 'lingua franca' of the Neo-Assyrian Empire (911–609 BC), Old Aramaic was also adapted to Mesopotamian cuneiform. The last cuneiform scripts in Akkadian discovered thus far date from the 1st century AD.\n\nOver the centuries, three distinct Elamite scripts developed.\nProto-Elamite is the oldest known writing system from Iran. In use only for a brief time (c. 3200–2900 BC), clay tablets with Proto-Elamite writing have been found at different sites across Iran. The Proto-Elamite script is thought to have developed from early cuneiform (proto-cuneiform). The Proto-Elamite script consists of more than 1,000 signs and is thought to be partly logographic.\n\nLinear Elamite is a writing system attested in a few monumental inscriptions in Iran. It was used for a very brief period during the last quarter of the 3rd millennium BC. It is often claimed that Linear Elamite is a syllabic writing system derived from Proto-Elamite, although this cannot be proven since Linear-Elamite has not been deciphered. Several scholars have attempted to decipher the script, most notably and .\n\nThe Elamite cuneiform script was used from about 2500 to 331 BC, and was adapted from the Akkadian cuneiform. The Elamite cuneiform script consisted of about 130 symbols, far fewer than most other cuneiform scripts.\n\nCretan hieroglyphs are found on artifacts of Crete (early-to-mid-2nd millennium BC, MM I to MM III, overlapping with Linear A from MM IIA at the earliest). Linear B, the writing system of the Mycenaean Greeks, has been deciphered while Linear A has yet to be deciphered. The sequence and the geographical spread of the three overlapping, but distinct writing systems can be summarized as follows: Cretan hieroglyphs were used in Crete from c. 1625 to 1500 BC; Linear A was used in the Aegean Islands (Kea, Kythera, Melos, Thera), and the Greek mainland (Laconia) from c. 18th century to 1450 BC; and Linear B was used in Crete (Knossos), and mainland (Pylos, Mycenae, Thebes, Tiryns) from c. 1375 to 1200 BC.\n\nThe earliest surviving examples of writing in China—inscriptions on so-called \"oracle bones\", tortoise plastrons and ox scapulae used for divination—date from around 1200 BC in the late Shang dynasty. A small number of bronze inscriptions from the same period have also survived.\nHistorians have found that the type of media used had an effect on what the writing was documenting and how it was used.\n\nIn 2003, archaeologists reported discoveries of isolated tortoise-shell carvings dating back to the 7th millennium BC, but whether or not these symbols are related to the characters of the later oracle-bone script is disputed.\n\nThe earliest known hieroglyphic inscriptions are the Narmer Palette, dating to c. 3200 BC, and several recent discoveries that may be slightly older, though these glyphs were based on a much older artistic rather than written tradition. The hieroglyphic script was logographic with phonetic adjuncts that included an effective alphabet.\n\nWriting was very important in maintaining the Egyptian empire, and literacy was concentrated among an educated elite of scribes. Only people from certain backgrounds were allowed to train to become scribes, in the service of temple, pharaonic, and military authorities. The hieroglyph system was always difficult to learn, but in later centuries was purposely made even more so, as this preserved the scribes' status.\n\nThe world's oldest known alphabet appears to have been developed by Canaanite turquoise miners in the Sinai desert around the mid-19th century BC. Around 30 crude inscriptions have been found at a mountainous Egyptian mining site known as Serabit el-Khadem. This site was also home to a temple of Hathor, the \"Mistress of turquoise\". A later, two line inscription has also been found at Wadi el-Hol in Central Egypt. Based on hieroglyphic prototypes, but also including entirely new symbols, each sign apparently stood for a consonant rather than a word: the basis of an alphabetic system. It was not until the 12th to 9th centuries, however, that the alphabet took hold and became widely used.\n\nIndus script refers to short strings of symbols associated with the Indus Valley Civilization (which spanned modern-day Pakistan and North India) used between 2600 and 1900 BC. In spite of many attempts at decipherments and claims, it is as yet undeciphered. The term 'Indus script' is mainly applied to that used in the mature Harappan phase, which perhaps evolved from a few signs found in early Harappa after 3500 BC, and was followed by the mature Harappan script. The script is written from right to left, and sometimes follows a boustrophedonic style. Since the number of principal signs is about 400–600, midway between typical logographic and syllabic scripts, many scholars accept the script to be logo-syllabic (typically syllabic scripts have about 50–100 signs whereas logographic scripts have a very large number of principal signs). Several scholars maintain that structural analysis indicates that an agglutinative language underlies the script.\n\nIn 2001, archaeologists discovered that there was a civilization in Central Asia that used writing c. 2000 BC. An excavation near Ashgabat, the capital of Turkmenistan, revealed an inscription on a piece of stone that was used as a stamp seal.\n\nThe Proto-Sinaitic script, in which Proto-Canaanite is believed to have been first written, is attested as far back as the 19th century BC. The Phoenician writing system was adapted from the Proto-Canaanite script sometime before the 14th century BC, which in turn borrowed principles of representing phonetic information from Hieratic, Cuneiform and Egyptian hieroglyphs. This writing system was an odd sort of syllabary in which only consonants are represented. This script was adapted by the Greeks, who adapted certain consonantal signs to represent their vowels. The Cumae alphabet, a variant of the early Greek alphabet, gave rise to the Etruscan alphabet and its own descendants, such as the Latin alphabet and Runes. Other descendants from the Greek alphabet include Cyrillic, used to write Bulgarian, Russian and Serbian, among others. The Phoenician system was also adapted into the Aramaic script, from which the Hebrew and the Arabic scripts are descended.\n\nThe Tifinagh script (Berber languages) is descended from the Libyco-Berber script, which is assumed to be of Phoenician origin.\n\nA stone slab with 3,000-year-old writing, known as the Cascajal Block, was discovered in the Mexican state of Veracruz and is an example of the oldest script in the Western Hemisphere, preceding the oldest Zapotec writing by approximately 500 years. It is thought to be Olmec.\n\nOf several pre-Columbian scripts in Mesoamerica, the one that appears to have been best developed, and the only one to be deciphered, is the Maya script. The earliest inscription identified as Maya dates to the 3rd century BC. Maya writing used logograms complemented by a set of syllabic glyphs, somewhat similar in function to modern Japanese writing.\n\nThe Incas had no known script. Their quipu system of recording information—based on knots tied along one or many linked cords—was apparently used for inventory and accountancy purposes and could not encode textual information.\n\nThree stone slabs were found by Romanian archaeologist Nicolae Vlassa, in the mid-20th century (1961) in Tărtăria (present-day Alba County, Transylvania), Romania, ancient land of Dacia, inhabited by Dacians, which were a population who may have been related to the Getaes and Thracians.\nOne of the slabs contains 4 groups of pictographs divided by lines. Some of the characters are also found in Ancient Greek, as well as in Phoenician, Etruscan, Old Italic and Iberian.\nThe origin and the timing of the writings are disputed, because there are no precise evidence in situ, the slabs cannot be carbon dated, because of the bad treatment of the Cluj museum. There are indirect carbon dates found on a skeleton discovered near the slabs, that certifies the 5300–5500 BC period.\n\nIn the 21st century, writing has become an important part of daily life as technology has connected individuals from across the globe through systems such as e-mail and social media. Literacy has grown in importance as a factor for success in the modern world. In the United States, the ability to read and write are necessary for most jobs, and multiple programs are in place to aid both children and adults in improving their literacy skills. For example, the emergence of the writing center and community-wide literacy councils aim to help students and community members sharpen their writing skills. These resources, and many more, span across different age groups in order to offer each individual a better understanding of their language and how to express themselves via writing in order to perhaps improve their socioeconomic status.\n\nOther parts of the world have seen an increase in writing abilities as a result of programs such as the World Literacy Foundation and International Literacy Foundation, as well as a general push for increased global communication.\n\n\n"}
{"id": "714599", "url": "https://en.wikipedia.org/wiki?curid=714599", "title": "ǂKx'ao-ǁ'ae", "text": "ǂKx'ao-ǁ'ae\n\nǂKxʼao-ǁʼae (ǂKxʼauǁʼein, Auen, Kaukau) is a southeastern variety of the !Xuun dialect continuum, spoken in Botswana (the settlements of Groote Laagte, East Hanahai, Kanagas and Ghanzi in Ghanzi District and on the commercial farms) and in Namibia (the city of Gobabis and settlements along the C22 road to Otjinene as far as Eiseb, Omaheke Region) by about 7,000 people. In Botswana, most speakers are bilingual in Naro or Tswana.\n\nThere are numerous spellings of the name, including \"ǁAuǁei, ǁX’auǁ’e,\" and \"Auen\". Endonyms are \"Juǀʼhoan, ǃXuun \"in Namibia and \"ǂKx'ao||'ae\" (predominantly in Botswana) meaning \"northern people\" in Naro. It also goes by the names \"Gobabis ǃKung\" and \"Kaukau \"(which can take the noun class prefixes in Tswana to give \"Mokaukau \"for one person, \"Bakaukau\" for the group and \"Sekaukau\" for the language).\n\nIn Namibia, ǂKx'ao||'ae\" \"tends to refer literally to the !Xuun speakers to the north in the Caprivi area. With the exception of a few cultural traits, speakers of ǂKx'ao||'ae\" \"in Botswana and those of Ju|'hoan in Namibia argue that they are one and the same people, speaking one language, with some dialectal attributes.\n\nThe non-Latin characters used by the language predominantly refer to click consonants and follow the orthography by Patrick Dickens for Juǀ'hoan.\n\nThe limited data on these dialects is poorly transcribed, but as of 2015 fieldwork is in progress.\n\n\n"}
