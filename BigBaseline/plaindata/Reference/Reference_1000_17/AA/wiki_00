{"id": "1230569", "url": "https://en.wikipedia.org/wiki?curid=1230569", "title": "Acronym Finder", "text": "Acronym Finder\n\nAcronym Finder (AF) is a free online searchable dictionary and database of abbreviations (acronyms, initialisms, and others) and their meanings.\n\nThe entries are classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes. For abbreviations with multiple meanings they are listed by popularity with the most common one being listed first. it claims to have over a million \"human-edited\" and verified definitions.\n\nAcronym Finder was registered and the database put online by Michael K. Molloy of Colorado in 1997 but he began compiling it in 1985 working as a computer systems officer for the USAF. Molloy first saw the need of an acronym list while integrating computers at the Randolph Air Force Base in Texas His first acronym list running up-to 30 pages. When he had retired and put AF online in 1997, his list already had 43,000 acronyms. It began mainly as a list of Military/Government abbreviations before expanding to other areas.\n\nMolloy and his wife served as the editors of the website verifying user submissions for abbreviations and adding others they found to the database. Molloy has also provided opinions on abbreviations such as \"MSG\" which Madison Square Garden wanted as a domain name (\"msg.com\") claiming trademark to the abbreviated letters. He stated that MSG also stood for more common things such as monosodium glutamate and message among others. The Garden in the end settled out of court and came to own msg.com.\n\nThe website was maintained under Mountain Data Systems, LLC by Molloy before being sold off and eventually coming under the ownership of Farlex, Inc. publishers of Thefreedictionary.com.\n\nThe website contains a database of meanings and expansions for abbreviations, acronyms, initialisms mainly in English but includes some entries in other languages such as French, German, Spanish etc. as well. It is freely accessible. The entries are further classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes which are shown on a Map along with location information. Abbreviations with multiple expansions are listed by popularity with the most common one being presented first, these can be sorted alphabetically as well.\n\nAnyone can contribute to the database by submitting abbreviations and their meanings, these are reviewed by an by editor and categorized before being added to the database. While the database has been described as fairly accurate errors have been found in the meanings and expansions of abbreviations. The website does not list sources for the abbreviations and their meanings but it does identify people who have contributed more than 50 abbreviations to the database.\n\nThe database only contains abbreviations and their expansions and does not list other data such as grammatical category, context, source, field of the abbreviation etc.\n\nFarlex, Inc. the current owner of the website also publishes mobile apps for the Android and iOS operating systems.\n\nAcronym Finder also includes a \"Systematic Buzz Phrase Projector\", a light-hearted tool that randomly generates jargon-like phrases and abbreviations — usually initialisms that would be unpronounceable as acronyms — and meanings from 30 cleverly chosen buzz words.\n\nThe website is supported through advertisements.\n\nThe website is listed as a quick reference tool in directories like Stanford Library, Library of Congress, USC Library. It has been cited as the largest database of acronyms and has been used in computational studies for its database.\n\nListings of abbreviations on the website have also been used as a defense that an abbreviation is in public use and cannot be trademarked. While in some trademark cases citations for AF have been accepted it has been described as an unreliable reference in others.\n\nIt has garnered criticism for the fact that anyone can submit abbreviations to the site and the content is user generated. Mike Molloy the site's original owner had defended that each submission is verified before being added to the database.\n\n"}
{"id": "925519", "url": "https://en.wikipedia.org/wiki?curid=925519", "title": "Autogram", "text": "Autogram\n\nAn autogram (Greek: αὐτός = self, γράμμα = letter) is a sentence that describes itself in the sense of providing an inventory of its own characters. They were invented by Lee Sallows, who also coined the word \"autogram\". An essential feature is the use of full cardinal number names such as “one”, “two”, etc., in recording character counts. Autograms are also called ‘self-enumerating’ or ‘self-documenting’ sentences. Often, letter counts only are recorded while punctuation signs are ignored, as in this example:\n\nThe first autogram to be published was composed by Sallows in 1982 and appeared in Douglas Hofstadter's \"Metamagical Themas\" column in \"Scientific American\".\n\nThe task of producing an autogram is perplexing because the object to be described cannot be known until its description is first complete.\n\nA type of autogram that has attracted special interest is the autogramic pangram, a self-enumerating sentence in which every letter of the alphabet occurs at least once. Certain letters do not appear in either of the two autograms above, which are therefore not pangrams. The first ever self-enumerating pangram appeared in a Dutch newspaper and was composed by Rudy Kousbroek. Sallows, who lives in the Netherlands, was challenged by Kousbroek to produce a self-enumerating ‘translation’ of this pangram into English—an impossible-seeming task. This prompted Sallows to construct an electronic Pangram Machine. Eventually the machine succeeded, producing the example below which was published in Scientific American in October 1984:\n\nSallows wondered if one could produce a pangram that counts its letters as percentages of the whole sentence–a particularly difficult task since such percentages usually won't be exact integers. He mentioned the problem to Chris Patuzzo and in late 2015 Patuzzo produced the following solution:\n\nAutograms exist that exhibit extra self-descriptive features. Besides counting each letter, here the total number of letters appearing is also named:\n\nJust as an autogram is a sentence that describes itself, so there exist closed chains of sentences each of which describes its predecessor in the chain. Viewed thus, an autogram is such a chain of length 1. Here follows a chain of length 2:\nA special kind of autogram is the ‘reflexicon’ (short for “reflexive lexicon”), which is a self-descriptive word list that describes its own letter frequencies. The constraints on reflexicons are much tighter than on autograms because the freedom to choose alternative words such as “contains”, “comprises”, “employs”, and so on, is lost. However, a degree of freedom still exists through appending entries to the list that are strictly superfluous.\n\nFor example, “Sixteen e's, six f's, one g, three h's, nine i's, nine n's, five o's, five r's, sixteen s's, five t's, three u's, four v's, one w, four x's” is a reflexicon, but it includes what Sallows calls “dummy text” in the shape of “one g” and “one w”. The latter might equally be replaced with “one #”, where “#” can be any typographical sign not already listed. Sallows has made an extensive computer search and conjectures that there exist but three and only three pure (i.e., no dummy text) English reflexicons.\n\n"}
{"id": "928216", "url": "https://en.wikipedia.org/wiki?curid=928216", "title": "Autological word", "text": "Autological word\n\nAn autological word (also called homological word or autonym) is a word that expresses a property that it also possesses (e.g. the word \"short\" is short, \"noun\" is a noun, \"English\" is English, \"pentasyllabic\" has five syllables, \"word\" is a word). The opposite is a heterological word, one that does not apply to itself (e.g. \"long\" is not long, \"monosyllabic\" has five syllables).\n\nUnlike more general concepts of autology and self-reference, this particular distinction and opposition of \"autological\" and \"heterological words\" is uncommon in linguistics for describing linguistic phenomena or classes of words, but is current in logic and philosophy where it was introduced by Kurt Grelling and Leonard Nelson for describing a semantic paradox, later known as Grelling's paradox or the Grelling–Nelson paradox. \n\nOne source of autological words is ostensive definition: the reference to a class of words by an example of the member of the class, as it were by synecdoche: such as mondegreen, oxymoron, eggcorn, bahuvrihi, etc.\nA word's status as autological may change over time. For example, \"neologism\" was once an autological word but no longer is; similarly, \"protologism\" (a word invented recently by literary theorist Mikhail Epstein) may or may not lose its autological status depending on whether or not it gains wider usage.\n\n\n\n"}
{"id": "3181897", "url": "https://en.wikipedia.org/wiki?curid=3181897", "title": "Brand Book", "text": "Brand Book\n\nA Brand Book records all livestock brands registered with an organization. In the U.S. most states have branding laws that require brands to be registered before use. This may be a state agency (usually affiliated with each state's Department of Agriculture) or a private association regulated by the state. Most states with such laws have a Brand Book for the entire state. Texas, an exception, registers brands at the county level. These book are usually provided free to law enforcement personnel and County Extension Agents. Some states have their Brand Books available online.\n\nA typical Brand Book will usually have an image of the brand, the location of the brand on the animal, and the type of animal that will be branded, as well as the owner of the brand. Many Brand Books also record earmarks.\n\nBrand Books are used by law enforcement officials, brand inspectors, and association investigators to record and track livestock movement, deter loss of livestock by straying or theft, and prosecute thieves.\n\n\n"}
{"id": "6767022", "url": "https://en.wikipedia.org/wiki?curid=6767022", "title": "Carol Ballard", "text": "Carol Ballard\n\nCarol Ballard is an author of more than 80 non-fiction books. Specializing in informational books for children and teens, her focus is toward the 7- to 14-year-old group.\n\nAfter graduating from Leeds University in plant sciences, Ballard did post-graduate research and was awarded a PhD in Immunology. She has many years experience as a science teacher and co-ordinator and has written articles for teachers on various aspects of science teaching, and teachers' materials for classroon use.\n\nIn addition to her writing, Carol works as a freelance consultant for publishers on educational and scientific matters. She also has her own business, Kite Books, which produces worksheets and teachers' resources.\n\n\n"}
{"id": "1250078", "url": "https://en.wikipedia.org/wiki?curid=1250078", "title": "Cf.", "text": "Cf.\n\nThe abbreviation cf. (short for the , both meaning \"compare\") is used in writing to refer the reader to other material to make a comparison with the topic being discussed. It is used to form a contrast, for example: \"Abbott (2010) found supportive results in her memory experiment, unlike those of previous work (cf. Zeller & Williams, 2007).\" It is recommended that \"cf.\" be used only to suggest a comparison, and the word \"see\" be used to point to a source of information.\n\nIn biological naming conventions, cf. is commonly placed between the genus name and the species name to describe a specimen that is difficult to identify because of practical difficulties, such as the specimen being poorly preserved. For example, \"' cf. '\" indicates that the specimen is in the genus \"Barbus\", and believed to be \"\" but the actual species-level identification cannot be certain.\n\nCf. can also be used to express a possible identity, or at least a significant resemblance, such as between a newly observed specimen and a known species or taxon. Such a usage might suggest a specimen's membership of the same genus or possibly of a shared higher taxon, such as in, \", cf. \"\"\", where the author is confident of the order and family (Diptera: Tabanidae), but can only offer the genus (\"Tabanus\") as a suggestion and has no information favouring a particular species.\n\n"}
{"id": "17077434", "url": "https://en.wikipedia.org/wiki?curid=17077434", "title": "Comparative Toxicogenomics Database", "text": "Comparative Toxicogenomics Database\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool launched in November 2004 that curates scientific data describing relationships between chemicals/drugs, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules.\nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool that curates scientific data describing relationships between chemicals, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules, launched on November 12, 2004. \nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nOne of the primary goals of CTD is to advance the understanding of the effects of environmental chemicals on human health on the genetic level, a field called toxicogenomics.\n\nThe etiology of many chronic diseases involves interactions between environmental factors and genes that modulate important physiological processes. Chemicals are an important component of the environment. Conditions such as asthma, cancer, diabetes, hypertension, immunodeficiency, and Parkinson's disease are known to be influenced by the environment; however, the molecular mechanisms underlying these correlations are not well understood. CTD may help resolve these mechanisms. The most up-to-date extensive list of peer-reviewed scientific articles about CTD is available at their publications page\n\nCTD is a unique resource where biocurators read the scientific literature and manually curate four types of core data:\n\n\nBy integrating the above four data sets, CTD automatically constructs putative chemical-gene-phenotype-disease networks to illuminate molecular mechanisms underlying environmentally-influenced diseases.\n\nThese inferred relationships are statistically scored and ranked and can be used by scientists and computational biologists to generate and verify testable hypotheses about toxicogenomic mechanisms and how they relate to human health.\n\nUsers can search CTD to explore scientific data for chemicals, genes, diseases, or interactions between any of these three concepts. Currently, CTD integrates toxicogenomic data for vertebrates and invertebrates.\n\nCTD integrates data from or hyperlinks to these databases:\n\n"}
{"id": "14322444", "url": "https://en.wikipedia.org/wiki?curid=14322444", "title": "Comparative advertising", "text": "Comparative advertising\n\nComparative advertising or advertising war is an advertisement in which a particular product, or service, specifically mentions a competitor by name for the express purpose of showing why the competitor is inferior to the product naming it. Also referred to as \"knocking copy\", it is loosely defined as advertising where “the advertised brand is explicitly compared with one or more competing brands and the comparison is obvious to the audience.”\n\nThis should not be confused with parody advertisements, where a fictional product is being advertised for the purpose of poking fun at the particular advertisement, nor should it be confused with the use of a coined brand name for the purpose of comparing the product without actually naming an actual competitor. (\"Wikipedia tastes better and is less filling than the Encyclopedia Galactica.\")\n\nIn the United States, the Federal Trade Commission (FTC) defined comparative advertising as “advertisement that compares alternative brands on objectively measurable attributes or price, and identifies the alternative brand by name, illustration or other distinctive information.” This definition was used in the case Gillette Australia Pty Ltd v Energizer Australia Pty Ltd. Similarly, the Law Council of Australia recently suggested that comparative advertising refers to “advertising which include reference to a competitor’s trademark in a way which does not impute proprietorship in the mark to the advertiser.”\n\nComparative advertisements could be either indirectly or directly comparative, positive or negative, and seeks “to associate or differentiate the two competing brands”. Different countries apply differing views regarding the laws on comparative advertising.\n\nThe earliest court case concerning comparative advertising dates back to 1910 in the United States – Saxlehner v Wagner. Prior to the 1970s, comparative advertising was deemed unfeasible due to related risks. For instance, comparative advertising could invite misidentification of products, potential legal issues, and may even win public sympathy for their competitors as victims.\n\nIn 1972, the FTC began to encourage advertisers to make comparison with named competitors, with the broad, public welfare objective of creating more informative advertising. The FTC argued that this form of advertising could also stimulate comparison shopping, encourage product improvement and innovation, and foster a positive competitive environment. However, studies have shown that while comparative advertisements had increased since 1960, the relative amount of comparative advertising is still small.\n\nPrior to 1997, many European countries severely limited comparative claims as an advertising practice. For example, in Germany comparisons in advertising had since the 1930's been largely prohibited as an anti-competitive practice, with very limited exceptions for cases where the advertiser had a good reason for presenting a critical claim, and reference to a competitor was necessary in order to present that claim. Importantly, this only applied to \"critical\" claims - claims of equivalence were completely prohibited. A similar approach had been adopted in France, where comparative advertising was commonly seen as disparaging of competitors. However, the legalisation of comparative advertising in France in 1992, opened the door to a general legalisation of comparative advertising through EU law, which had first been proposed by the European Commission in 1978. The result was the adoption of Directive 97/55/EC, which came into force in the year 2000. The relevant provisions are now contained in Directive 2006/114/EC.\n\nThis Directive sets out rules that comparative advertising must comply with in order to be considered permissible. These include the requirements that the comparison concern goods and services that meet the same purpose, that it objectively compare the relevant characteristics of the products concerned and that it not cause confusion or denigrate the trademarks and other distinguishing signs of competitors. The Directive prohibits comparisons that take unfair advantage of the reputation of a competitor's distinguishing marks, or present goods or services as imitations of products covered by a protected trade mark or trade name. Additionally, any comparison aimed at promoting goods bearing a protected designation of origin must refer exclusively to other goods bearing the same designation. Directive 2006/114/EC constitutes a total harmonisation of the rules on comparative advertising, meaning that the Member States are neither allowed to permit comparisons that breach the requirements of the Directive, nor prohibit ones that do. \n\nFurther, while trademark rights can in principle be used to prevent comparative advertising that makes unauthorised use of a competitor's trademark, this is not the case where the comparative advertisement complies with all the requirements of Directive 2006/114/EC. Legitimate comparative advertising must therefore be seen as an exception to the exclusive rights of the trademark proprietor. However, the trademark proprietor can, thanks to the prohibition on taking unfair advantage of a trademark's reputation, oppose the use of their trademark where it is not aimed at distinguishing the products of the advertiser and trademark proprietor and to highlight their differences objectively, but rather at riding on the coat-tails of that mark in order to benefit from its reputation.\n\nThe requirements set out by the Directive have resulted in some controversy. This is particularly true of the \"per se\" prohibition on comparisons presenting goods and services as imitations of trademarked products. In this regard, EU law contrasts starkly with the US approach; the US courts have long held that traders are allowed to the trademarked names of products they have imitated in advertising. In contrast, in L'Oréal and others v. Bellure, the Court of Justice held that smell-alike perfumes marketed through comparison lists breached this condition. This decision was criticised both by the English courts and by scholars, who have considered that this places unjustified limits on advertising acts that are otherwise fully legal, such as copying that does not infringe intellectual property rights. \n\nIn the UK, most of the use of competitor’s registered trademark in a comparative advertisement was an infringement of the registration up till the end of 1994. However, the laws on comparative advertising were harmonized in 2000. The current rules on comparative advertising are regulated by a series of EU Directives. The Business Protection from Misleading Marketing Regulations 2008 implements provisions of Directive (EC) 2006/114 in the UK.\n\nOne of the classic cases of comparative advertising in the UK was the O2 v Hutchison case. The European Court of Justice (ECJ) held that there could have been a trademark infringement when a comparative advertiser used the registered trademark for the advertiser’s own goods and services. It was also held that a trademark proprietor could not prevent a competitor’s use of a sign similar or identical to his mark in a comparative advertisement, which satisfies all the conditions of the Comparative Advertising Directive. If the Advocate General's decision in the O2 case were followed by the ECJ, competitors will not be able to use trademark legislation either to prevent a comparative advertisement through an injunction or to charge in respect of its use. Conversely, in British Airways plc v Ryanair Ltd. a lenient approach was adopted by the UK courts. The use of competitors’ trademarks was no longer restricted for businesses competing within an industry, provided that compliance of the conditions set out in the legislation were performed. This meant that businesses are able to use the trademarks of other companies and trade names to distinguish the relative merits of their own products and services over those of their competitors.\n\nThe FTC and the National Advertising Division of the Council of Better Business Bureaus, Inc. (NAD), govern the laws of comparative advertising in the United States including the treatment of comparative advertising claims. FTC stated that comparative advertising could benefit consumers and encourages comparative advertising, provided that the comparisons are “clearly identified, truthful, and non-deceptive”. Although comparative advertising is encouraged, NAD has stated “claims that expressly or implicitly disparage a competing product should be held to the highest level of scrutiny in order to ensure that they are truthful, accurate, and narrowly drawn.” Another major law is the trademark protective Lanham Act, which states that one could incur liability when the message of the comparative advertisement is untrue or uncertain, but has the intention to deceive consumers through the implied message conveyed.\n\nIn Australia, no specific law governs comparative advertising although certain cases regarding this matter have occurred. Comparative advertising that is truthful, and does not lead to confusion is permitted.\n\nGenerally, Australian advertisers should make sure that the following are complied when exercising comparative advertising to avoid breaches regarding misleading advertising under Australia Consumer Law:\n\n\nThe law in Hong Kong regarding comparative advertising is the law that existed in the UK prior to the enactment of the UK Act 1994. Hong Kong has no legislation exclusively intended at limiting false or misleading advertisements. Still, the Trade Descriptions Ordinance (Cap 362) bans the use of false trade descriptions in advertisements. The tort of trade libel also exists to deal with false or misleading advertisements designed to injure the competitor. Consumer Council may have the authority to publish information with a perspective to amending false or misleading advertisements, while the Association of Accredited Advertising Agencies of Hong Kong have the authority to take action against members who organize advertisements that are inaccurate.\n\nIn Argentina, there is no specific statute dealing with comparative advertising (so it is not forbidden), but there are clear jurisprudential rules based on unfair competition law. If in some manner an advertisement is proven to be unfair or exceeds ethical standards by hiding the truth or omitting some essential aspect of the comparison, it is probable that an injunction will be granted and that the plaintiff will be able to obtain a final decision declaring the advertising illegal.\n\nNumerous cases follow international precedent in referring to the requirements of the European Union Directive on comparative advertising. By following these criteria, Argentine courts have developed standards very similar to European regulation. It is as if the judges wanted to validate the law created by the Courts with an external source. Similar conclusions reached elsewhere indicate the existence of universally accepted principles that accept that comparing products in commercial advertisements should be lawful.\n\nIn Brazil, the allow comparative advertising with certain restrictions. Its primary purpose shall be the clarification or consumer’s protection; it shall have as basic principle the objectiveness of the comparison since subjective data, psychological or emotionally based data does not constitute a valid comparison basis for consumers; the purposed or implemented comparison shall be capable of being supported byevidence; in the case of consumption goods, the comparison shall be made with models manufactured in the same year and no comparison shall be made between products manufactured in different years, unless it is only a reference to show evolution, in which case the evolution shall be clearly demonstrated; there shall be no confusion between the products and competitor’s brands; there shall be no unfair competition, denigration of the product’s image or another company’s product; and there shall be no unreasonable use of the corporate image or goodwill of third parties.\n\nLikewise, the majority of the Brazilian authors is inclined to say that its legitimacy depends to meet certain requirements, which, in general, would be stipulated by Article 3a of Directive 84/450/EEC \n\nIn an early Mercosur's rules through Resolution 126/96.\n\nComparative advertising has been increasingly implemented through the years, and the types of comparative advertising range from comparing a single attribute dimension, comparing an attribute unique to the target and absent in the referent and comparisons involving attributes unique to both brands. The contributing factors to the effectiveness of comparative advertising include believability, which refers to the extent a consumer can rely on the information provided in comparative advertisements, the level of involvement, and the convenience in evaluation, provided by spoon feeding the consumer with information that does not require extra effort in recall.\n\nComparative advertising is generally coupled with negativity, as evidenced by early industry condemnation. Stating reasons such as participation in comparative advertising damaged the honour and credibility of advertising. Studies have suggested that negative information can be stored more effectively, thus generating the impact that any advertisement is purposed for, and more importantly, strong recall. On the contrary, such negativity can either be transferred directly to the brand and the consumer’s impression of the brand, various studies through the years have proven that comparative advertising has been responded to negatively.\n\nComparative advertising has been used effectively by companies like The National Australia Bank (NAB), and its “break up” campaign has made such an impact it has won an award from Cannes, and a substantial increase in its consumer interest. Internationally acclaimed Apple Inc. has effectively utilized its Mac vs PC advertisements as part of its marketing efforts to increase its market share over the years. Such companies prove the academic view that comparative advertising is more successful when used by established brands, justified by the credibility and attention an established brand brings. Other famous examples include L’Oreal SA v Bellure NV and Coca Cola v Pepsi. Comparative advertising has to be executed with caution and deep consideration for the targeted markets as the novelty of the concept affects the effectiveness of the stipulated campaigns.\n\nIn the 1980s, during what has been referred to as the cola wars, soft-drink manufacturer Pepsi ran a series of advertisements where people, caught on hidden camera, in a blind taste test, chose Pepsi over rival Coca-Cola.\nThe use of comparative advertising has been well established in political campaigns, where typically one candidate will run ads where the record of the other candidate is displayed, for the purpose of disparaging the other candidate. The most famous of these type ads, which only ran once on TV, consisted of a child picking daisies in a field, while a voice which sounded like Barry Goldwater performed a countdown to zero before the launch of a nuclear weapon which explodes in a mushroom cloud. The ad, \"Daisy\", was produced by Lyndon B. Johnson's campaign in an attempt to prevent Goldwater from either winning the nomination of his party or being selected.\n\nAnother example took place throughout the late 1980s between the bitter rivals Nintendo and Sega. \"Genesis does what Nintendon't\" immediately became a catchphrase following the release of the Sega Genesis (known as Mega Drive in PAL countries).\n\nA 30-second commercial promoting sustainability, showing soda bottles exploding each time a person makes a drink using his Sodastream machine, was banned in the United Kingdom in 2012. Clearcast, the organization that preapproves TV advertising in the U.K., explained that they \"thought it was a denigration of the bottled drinks market.\" The same ad, crafted by Alex Bogusky, ran in the United States, Sweden, Australia, and other countries. An appeal by Sodastream to reverse Clearcast's decision to censor the commercial was rejected. A similar ad was expected to air during Super Bowl XLVII in February 2013 but was banned by CBS for jabbing at Coke and Pepsi (two of CBS's largest sponsors).\n\nIn 2012, Microsoft's Bing (formerly MSN Search) began to run a campaign about which search engine they prefer as it compared Bing to Google, and that more people preferred Bing over Google. The campaign was titled \"Bing It On\".\n"}
{"id": "12185843", "url": "https://en.wikipedia.org/wiki?curid=12185843", "title": "Comparative cognition", "text": "Comparative cognition\n\nComparative cognition is the comparative study of the mechanisms and origins of cognition in various species, and is sometimes seen as more general than, or similar to, comparative psychology.\nFrom a biological point of view, work is being done on the brains of fruit flies that should yield techniques precise enough to allow an understanding of the workings of the human brain on a scale appreciative of individual groups of neurons rather than the more regional scale previously used. Similarly, gene activity in the human brain is better understood through examination of the brains of mice by the Seattle-based Allen Institute for Brain Science (see link below), yielding the freely available Allen Brain Atlas. This type of study is related to comparative cognition, but better classified as one of comparative genomics. Increasing emphasis in psychology and ethology on the biological aspects of perception and behavior is bridging the gap between genomics and behavioral analysis.\n\nIn order for scientists to better understand cognitive function across a broad range of species they can systematically compare cognitive abilities between closely and distantly related species Through this process they can determine what kinds of selection pressure has led to different cognitive abilities across a broad range of animals. For example, it has been hypothesized that there is convergent evolution of the higher cognitive functions of corvids and apes, possibly due to both being omnivorous, visual animals that live in social groups.\n\n\n"}
{"id": "983601", "url": "https://en.wikipedia.org/wiki?curid=983601", "title": "Comparative genomic hybridization", "text": "Comparative genomic hybridization\n\nComparative genomic hybridization is a molecular cytogenetic method for analysing copy number variations (CNVs) relative to ploidy level in the DNA of a test sample compared to a reference sample, without the need for culturing cells. The aim of this technique is to quickly and efficiently compare two genomic DNA samples arising from two sources, which are most often closely related, because it is suspected that they contain differences in terms of either gains or losses of either whole chromosomes or subchromosomal regions (a portion of a whole chromosome). This technique was originally developed for the evaluation of the differences between the chromosomal complements of solid tumor and normal tissue, and has an improved resolution of 5–10 megabases compared to the more traditional cytogenetic analysis techniques of giemsa banding and fluorescence in situ hybridization (FISH) which are limited by the resolution of the microscope utilized.\n\nThis is achieved through the use of competitive fluorescence in situ hybridization. In short, this involves the isolation of DNA from the two sources to be compared, most commonly a test and reference source, independent labelling of each DNA sample with fluorophores (fluorescent molecules) of different colours (usually red and green), denaturation of the DNA so that it is single stranded, and the hybridization of the two resultant samples in a 1:1 ratio to a normal metaphase spread of chromosomes, to which the labelled DNA samples will bind at their locus of origin. Using a fluorescence microscope and computer software, the differentially coloured fluorescent signals are then compared along the length of each chromosome for identification of chromosomal differences between the two sources. A higher intensity of the test sample colour in a specific region of a chromosome indicates the gain of material of that region in the corresponding source sample, while a higher intensity of the reference sample colour indicates the loss of material in the test sample in that specific region. A neutral colour (yellow when the fluorophore labels are red and green) indicates no difference between the two samples in that location.\n\nCGH is only able to detect unbalanced chromosomal abnormalities. This is because balanced chromosomal abnormalities such as reciprocal translocations, inversions or ring chromosomes do not affect copy number, which is what is detected by CGH technologies. CGH does, however, allow for the exploration of all 46 human chromosomes in single test and the discovery of deletions and duplications, even on the microscopic scale which may lead to the identification of candidate genes to be further explored by other cytological techniques.\n\nThrough the use of DNA microarrays in conjunction with CGH techniques, the more specific form of array CGH (aCGH) has been developed, allowing for a locus-by-locus measure of CNV with increased resolution as low as 100 kilobases. This improved technique allows for the aetiology of known and unknown conditions to be discovered.\n\nThe motivation underlying the development of CGH stemmed from the fact that the available forms of cytogenetic analysis at the time (giemsa banding and FISH) were limited in their potential resolution by the microscopes necessary for interpretation of the results they provided. Furthermore, giemsa banding interpretation has the potential to be ambiguous and therefore has lowered reliability, and both techniques require high labour inputs which limits the loci which may be examined.\n\nThe first report of CGH analysis was by Kallioniemi and colleagues in 1992 at the University of California, San Francisco, who utilised CGH in the analysis of solid tumors. They achieved this by the direct application of the technique to both breast cancer cell lines and primary bladder tumors in order to establish complete copy number karyotypes for the cells. They were able to identify 16 different regions of amplification, many of which were novel discoveries.\n\nSoon after in 1993, du Manoir et al. reported virtually the same methodology. The authors painted a series of individual human chromosomes from a DNA library with two different fluorophores in different proportions to test the technique, and also applied CGH to genomic DNA from patients affected with either Downs syndrome or T-cell prolymphocytic leukemia as well as cells of a renal papillary carcinoma cell line. It was concluded that the fluorescence ratios obtained were accurate and that differences between genomic DNA from different cell types were detectable, and therefore that CGH was a highly useful cytogenetic analysis tool.\n\nInitially, the widespread use of CGH technology was difficult, as protocols were not uniform and therefore inconsistencies arose, especially due to uncertainties in the interpretation of data. However, in 1994 a review was published which described an easily understood protocol in detail and the image analysis software was made available commercially, which allowed CGH to be utilised all around the world.\nAs new techniques such as microdissection and degenerate oligonucleotide primed polymerase chain reaction (DOP-PCR) became available for the generation of DNA products, it was possible to apply the concept of CGH to smaller chromosomal abnormalities, and thus the resolution of CGH was improved.\n\nThe implementation of array CGH, whereby DNA microarrays are used instead of the traditional metaphase chromosome preparation, was pioneered by Solinas-Tolodo et al. in 1997 using tumor cells and Pinkel et al. in 1998 by use of breast cancer cells. This was made possible by the Human Genome Project which generated a library of cloned DNA fragments with known locations throughout the human genome, with these fragments being used as probes on the DNA microarray. Now probes of various origins such as cDNA, genomic PCR products and bacterial artificial chromosomes (BACs) can be used on DNA microarrays which may contain up to 2 million probes. Array CGH is automated, allows greater resolution (down to 100 kb) than traditional CGH as the probes are far smaller than metaphase preparations, requires smaller amounts of DNA, can be targeted to specific chromosomal regions if required and is ordered and therefore faster to analyse, making it far more adaptable to diagnostic uses.\n\nThe DNA on the slide is a reference sample, and is thus obtained from a karyotypically normal man or woman, though it is preferential to use female DNA as they possess two X chromosomes which contain far more genetic information than the male Y chromosome. Phytohaemagglutinin stimulated peripheral blood lymphocytes are used. 1mL of heparinised blood is added to 10ml of culture medium and incubated for 72 hours at 37 °C in an atmosphere of 5% CO. Colchicine is added to arrest the cells in mitosis, the cells are then harvested and treated with hypotonic potassium chloride and fixed in 3:1 methanol/acetic acid.\n\nOne drop of the cell suspension should then be dropped onto an ethanol cleaned slide from a distance of about 30 cm, optimally this should be carried out at room temperature at humidity levels of 60–70%. Slides should be evaluated by visualisation using a phase contrast microscope, minimal cytoplasm should be observed and chromosomes should not be overlapping and be 400–550 bands long with no separated chromatids and finally should appear dark rather than shiny. Slides then need to be air dried overnight at room temperature, and any further storage should be in groups of four at −20 °C with either silica beads or nitrogen present to maintain dryness. Different donors should be tested as hybridization may be variable. Commercially available slides may be used, but should always be tested first.\n\nStandard phenol extraction is used to obtain DNA from test or reference (karyotypically normal individual) tissue, which involves the combination of Tris-Ethylenediaminetetraacetic acid and phenol with aqueous DNA in equal amounts. This is followed by separation by agitation and centrifugation, after which the aqueous layer is removed and further treated using ether and finally ethanol precipitation is used to concentrate the DNA.\n\nMay be completed using DNA isolation kits available commercially which are based on affinity columns.\n\nPreferentially, DNA should be extracted from fresh or frozen tissue as this will be of the highest quality, though it is now possible to use archival material which is formalin fixed or paraffin wax embedded, provided the appropriate procedures are followed. 0.5-1 µg of DNA is sufficient for the CGH experiment, though if the desired amount is not obtained DOP-PCR may be applied to amplify the DNA, however it in this case it is important to apply DOP-PCR to both the test and reference DNA samples to improve reliability.\n\nNick translation is used to label the DNA and involves cutting DNA and substituting nucleotides labelled with fluorophores (direct labelling) or biotin or oxigenin to have fluophore conjugated antibodies added later (indirect labelling). It is then important to check fragment lengths of both test and reference DNA by gel electrophoresis, as they should be within the range of 500kb-1500kb for optimum hybridization.\n\nUnlabelled Life Technologies Corporation's Cot-1 DNA® (placental DNA enriched with repetitive sequences of length 50bp-100bp)is added to block normal repetitive DNA sequences, particularly at centromeres and telomeres, as these sequences, if detected, may reduce the fluorescence ratio and cause gains or losses to escape detection.\n\n8–12µl of each of labelled test and labelled reference DNA are mixed and 40 µg Cot-1 DNA® is added, then precipitated and subsequently dissolved in 6µl of hybridization mix, which contains 50% formamide to decrease DNA melting temperature and 10% dextran sulphate to increase the effective probe concentration in a saline sodium citrate (SSC) solution at a pH of 7.0.\n\nDenaturation of the slide and probes are carried out separately. The slide is submerged in 70% formamide/2xSSC for 5–10 minutes at 72 °C, while the probes are denatured by immersion in a water bath of 80 °C for 10 minutes and are immediately added to the metaphase slide preparation. This reaction is then covered with a coverslip and left for two to four days in a humid chamber at 40 °C.\n\nThe coverslip is then removed and 5 minute washes are applied, three using 2xSSC at room temperature, one at 45 °C with 0.1xSSC and one using TNT at room temperature. The reaction is then preincubated for 10 minutes then followed by a 60-minute, 37 °C incubation, three more 5 minute washes with TNT then one with 2xSSC at room temperature. The slide is then dried using an ethanol series of 70%/96%/100% before counterstaining with DAPI (0.35 μg/ml), for chromosome identification, and sealing with a coverslip.\n\nA fluorescence microscope with the appropriate filters for the DAPI stain as well as the two fluorophores utilised is required for visualisation, and these filters should also minimise the crosstalk between the fluorophores, such as narrow band pass filters. The microscope must provide uniform illumination without chromatic variation, be appropriately aligned and have a “plan” type of objective which is apochromatic and give a magnification of x63 or x100.\n\nThe image should be recorded using a camera with spatial resolution at least 0.1 µm at the specimen level and give an image of at least 600x600 pixels. The camera must also be able to integrate the image for at least 5 to 10 seconds, with a minimum photometric resolution of 8 bit.\n\nDedicated CGH software is commercially available for the image processing step, and is required to subtract background noise, remove and segment materials not of chromosomal origin, normalize the fluorescence ratio, carry out interactive karyotyping and chromosome scaling to standard length. A “relative copy number karyotype” which presents chromosomal areas of deletions or amplifications is generated by averaging the ratios of a number of high quality metaphases and plotting them along an ideogram, a diagram identifying chromosomes based on banding patterns. Interpretation of the ratio profiles is conducted either using fixed or statistical thresholds (confidence intervals). When using confidence intervals, gains or losses are identified when 95% of the fluorescence ratio does not contain 1.0.\n\nExtreme care must be taken to avoid contamination of any step involving DNA, especially with the test DNA as contamination of the sample with normal DNA will skew results closer to 1.0, thus abnormalities may go undetected. FISH, PCR and flow cytometry experiments may be employed to confirm results.\n\nArray comparative genomic hybridization (also microarray-based comparative genomic hybridization, matrix CGH, array CGH, aCGH) is a molecular cytogenetic technique for the detection of chromosomal copy number changes on a genome wide and high-resolution scale. Array CGH compares the patient's genome against a reference genome and identifies differences between the two genomes, and hence locates regions of genomic imbalances in the patient, utilizing the same principles of competitive fluorescence in situ hybridization as traditional CGH.\n\nWith the introduction of array CGH, the main limitation of conventional CGH, a low resolution, is overcome. In array CGH, the metaphase chromosomes are replaced by cloned DNA fragments (+100–200 kb) of which the exact chromosomal location is known. This allows the detection of aberrations in more detail and, moreover, makes it possible to map the changes directly onto the genomic sequence.\n\nArray CGH has proven to be a specific, sensitive, fast and highthroughput technique, with considerable advantages compared to other methods used for the analysis of DNA copy number changes making it more amenable to diagnostic applications. Using this method, copy number changes at a level of 5–10 kilobases of DNA sequences can be detected. , even high-resolution CGH (HR-CGH) arrays are accurate to detect structural variations (SV) at resolution of 200 bp. This method allows one to identify new recurrent chromosome changes such as microdeletions and duplications in human conditions such as cancer and birth defects due to chromosome aberrations.\n\nArray CGH is based on the same principle as conventional CGH. In both techniques, DNA from a reference (or control) sample and DNA from a test (or patient) sample are differentially labelled with two different fluorophores and used as probes that are cohybridized competitively onto nucleic acid targets. In conventional CGH, the target is a reference metaphase spread. In array CGH, these targets can be genomic fragments cloned in a variety of vectors (such as BACs or plasmids), cDNAs, or oligonucleotides.\n\nFigure 2. is a schematic overview of the array CGH technique. DNA from the sample to be tested is labeled with a red fluorophore (Cyanine 5) and a reference DNA sample is labeled with green fluorophore (Cyanine 3). Equal quantities of the two DNA samples are mixed and cohybridized to a DNA microarray of several thousand evenly spaced cloned DNA fragments or oligonucleotides, which have been spotted in triplicate on the array. After hybridization, digital imaging systems are used to capture and quantify the relative fluorescence intensities of each of the hybridized fluorophores. The resulting ratio of the fluorescence intensities is proportional to the ratio of the copy numbers of DNA sequences in the test and reference genomes. If the intensities of the flurochromes are equal on one probe, this region of the patient's genome is interpreted as having equal quantity of DNA in the test and reference samples; if there is an altered Cy3:Cy5 ratio this indicates a loss or a gain of the patient DNA at that specific genomic region.\n\nArray CGH has been implemented using a wide variety of techniques. Therefore, some of the advantages and limitations of array CGH are dependent on the technique chosen.\nThe initial approaches used arrays produced from large insert genomic DNA clones, such as BACs. The use of BACs provides sufficient intense signals to detect single-copy changes and to locate aberration boundaries accurately. However, initial DNA yields of isolated BAC clones are low and DNA amplification techniques are necessary. These techniques include ligation-mediated polymerase chain reaction (PCR), degenerate primer PCR using one or several sets of primers, and rolling circle amplification. Arrays can also be constructed using cDNA. These arrays currently yield a high spatial resolution, but the number of cDNAs is limited by the genes that are encoded on the chromosomes, and their sensitivity is low due to cross-hybridization. This results in the inability to detect single copy changes on a genome wide scale. The latest approach is spotting the arrays with short oligonucleotides. The amount of oligos is almost infinite, and the processing is rapid, cost-effective, and easy. Although oligonucleotides do not have the sensitivity to detect single copy changes, averaging of ratios from oligos that map next to each other on the chromosome can compensate for the reduced sensitivity. It is also possible to use arrays which have overlapping probes so that specific breakpoints may be uncovered.\n\nThere are two approaches to the design of microarrays for CGH applications: whole genome and targeted.\n\nWhole genome arrays are designed to cover the entire human genome. They often include clones that provide an extensive coverage across the genome; and arrays that have contiguous coverage, within the limits of the genome. Whole-genome arrays have been constructed mostly for research applications and have proven their outstanding worth in gene discovery. They are also very valuable in screening the genome for DNA gains and losses at an unprecedented resolution.\n\nTargeted arrays are designed for a specific region(s) of the genome for the purpose of evaluating that targeted segment. It may be designed to study a specific chromosome or chromosomal segment or to identify and evaluate specific DNA dosage abnormalities in individuals with suspected microdeletion syndromes or subtelomeric rearrangements. The crucial goal of a targeted microarray in medical practice is to provide clinically useful results for diagnosis, genetic counseling, prognosis, and clinical management of unbalanced cytogenetic abnormalities.\n\nConventional CGH has been used mainly for the identification of chromosomal regions that are recurrently lost or gained in tumors, as well as for the diagnosis and prognosis of cancer. This approach can also be used to study chromosomal aberrations in fetal and neonatal genomes. Furthermore, conventional CGH can be used in detecting chromosomal abnormalities and have been shown to be efficient in diagnosing complex abnormalities associated with human genetic disorders.\n\nCGH data from several studies of the same tumor type show consistent patterns of non-random genetic aberrations. Some of these changes appear to be common to various kinds of malignant tumors, while others are more tumor specific. For example, gains of chromosomal regions lq, 3q and 8q, as well as losses of 8p, 13q, 16q and 17p, are common to a number of tumor types, such as breast, ovarian, prostate, renal and bladder cancer (Figure. 3). Other alterations, such as 12p and Xp gains in testicular cancer, 13q gain 9q loss in bladder cancer, 14q loss in renal cancer and Xp loss in ovarian cancer are more specific, and might reflect the unique selection forces operating during cancer development in different organs. Array CGH is also frequently used in research and diagnostics of B cell malignancies, such as chronic lymphocytic leukemia.\n\nCri du Chat (CdC) is a syndrome caused by a partial deletion of the short arm of chromosome 5. Several studies have shown that conventional CGH is suitable to detect the deletion, as well as more complex chromosomal alterations. For example, Levy et al. (2002) reported an infant with a cat-like cry, the hallmark of CdC, but having an indistinct karyotype. CGH analysis revealed a loss of chromosomal material from 5p15.3 confirming the diagnosis clinically. These results demonstrate that conventional CGH is a reliable technique in detecting structural aberrations and, in specific cases, may be more efficient in diagnosing complex abnormalities.\n\nArray CGH applications are mainly directed at detecting genomic abnormalities in cancer. However, array CGH is also suitable for the analysis of DNA copy number aberrations that cause human genetic disorders. That is, array CGH is employed to uncover deletions, amplifications, breakpoints and ploidy abnormalities. Earlier diagnosis is of benefit to the patient as they may undergo appropriate treatments and counseling to improve their prognosis.\n\nGenetic alterations and rearrangements occur frequently in cancer and contribute to its pathogenesis. Detecting these aberrations by array CGH provides information on the locations of important cancer genes and can have clinical use in diagnosis, cancer classification and prognostification. However, not all of the losses of genetic material are pathogenetic, since some DNA material is physiologically lost during the rearrangement of immunoglobulin subgenes. In a recent study, array CGH has been implemented to identify regions of chromosomal aberration (copy-number variation) in several mouse models of breast cancer, leading to identification of cooperating genes during myc-induced oncogenesis.\n\nArray CGH may also be applied not only to the discovery of chromosomal abnormalities in cancer, but also to the monitoring of the progression of tumors. Differentiation between metastatic and mild lesions is also possible using FISH once the abnormalities have been identified by array CGH.\n\nPrader–Willi syndrome (PWS) is a paternal structural abnormality involving 15q11-13, while a maternal aberration in the same region causes Angelman syndrome (AS). In both syndromes, the majority of cases (75%) are the result of a 3–5 Mb deletion of the PWS/AS critical region. These small aberrations cannot be detected using cytogenetics or conventional CGH, but can be readily detected using array CGH. As a proof of principle Vissers et al. (2003) constructed a genome wide array with a 1 Mb resolution to screen three patients with known, FISH-confirmed microdeletion syndromes, including one with PWS. In all three cases, the abnormalities, ranging from 1.5 to 2.9Mb, were readily identified. Thus, array CGH was demonstrated to be a specific and sensitive approach in detecting submicroscopic aberrations.\n\nWhen using overlapping microarrays, it is also possible to uncover breakpoints involved in chromosomal aberrations.\n\nThough not yet a widely employed technique, the use of array CGH as a tool for preimplantation genetic screening is becoming an increasingly popular concept. It has the potential to detect CNVs and aneuploidy in eggs, sperm or embryos which may contribute to failure of the embryo to successfully implant, miscarriage or conditions such as Down syndrome (trisomy 21). This makes array CGH a promising tool to reduce the incidence of life altering conditions and improve success rates of IVF attempts. The technique involves whole genome amplification from a single cell which is then used in the array CGH method. It may also be used in couples carrying chromosomal translocations such as balanced reciprocal translocations or Robertsonian translocations, which have the potential to cause chromosomal imbalances in their offspring.\n\nA main disadvantage of conventional CGH is its inability to detect structural chromosomal aberrations without copy number changes, such as mosaicism, balanced chromosomal translocations, and inversions. CGH can also only detect gains and losses relative to the ploidy level. In addition, chromosomal regions with short repetitive DNA sequences are highly variable between individuals and can interfere with CGH analysis. Therefore, repetitive DNA regions like centromeres and telomeres need to be blocked with unlabeled repetitive DNA (e.g. Cot1 DNA) and/or can be omitted from screening. Furthermore, the resolution of conventional CGH is a major practical problem that limits its clinical applications. Although CGH has proven to be a useful and reliable technique in the research and diagnostics of both cancer and human genetic disorders, the applications involve only gross abnormalities. Because of the limited resolution of metaphase chromosomes, aberrations smaller than 5–10 Mb cannot be detected using conventional CGH.\nFor the detection of such abnormalities, a high-resolution technique is required.\nArray CGH overcomes many of these limitations. Array CGH is characterized by a high resolution, its major advantage with respect to conventional CGH. The standard resolution varies between 1 and 5 Mb, but can be increased up to approximately 40 kb by supplementing the array with extra clones. However, as in conventional CGH, the main disadvantage of array CGH is its inability to detect aberrations that do not result in copy number changes and is limited in its ability to detect mosaicism. The level of mosaicism that can be detected is dependent on the sensitivity and spatial resolution of the clones. At present, rearrangements present in approximately 50% of the cells is the detection limit. For the detection of such abnormalities, other techniques, such as SKY (Spectral karyotyping) or FISH have to still be used.\n\n\n"}
{"id": "917868", "url": "https://en.wikipedia.org/wiki?curid=917868", "title": "Comparative genomics", "text": "Comparative genomics\n\nComparative genomics is a field of biological research in which the genomic features of different organisms are compared. The genomic features may include the DNA sequence, genes, gene order, regulatory sequences, and other genomic structural landmarks. In this branch of genomics, whole or large parts of genomes resulting from genome projects are compared to study basic biological similarities and differences as well as evolutionary relationships between organisms. The major principle of comparative genomics is that common features of two organisms will often be encoded within the DNA that is evolutionarily conserved between them. Therefore, comparative genomic approaches start with making some form of alignment of genome sequences and looking for orthologous sequences (sequences that share a common ancestry) in the aligned genomes and checking to what extent those sequences are conserved. Based on these, genome and molecular evolution are inferred and this may in turn be put in the context of, for example, phenotypic evolution or population genetics.\n\nVirtually started as soon as the whole genomes of two organisms became available (that is, the genomes of the bacteria \"Haemophilus influenzae\" and \"Mycoplasma genitalium\") in 1995, comparative genomics is now a standard component of the analysis of every new genome sequence. With the explosion in the number of genome projects due to the advancements in DNA sequencing technologies, particularly the next-generation sequencing methods in late 2000s, this field has become more sophisticated, making it possible to deal with many genomes in a single study. Comparative genomics has revealed high levels of similarity between closely related organisms, such as humans and chimpanzees, and, more surprisingly, similarity between seemingly distantly related organisms, such as humans and the yeast \"Saccharomyces cerevisiae\". It has also showed the extreme diversity of the gene\ncomposition in different evolutionary lineages.\n\n\"See also\": History of genomics\n\nComparative genomics has a root in the comparison of virus genomes in the early 1980s. For example, small RNA viruses infecting animals (picornaviruses) and those infecting plants (cowpea mosaic virus) were compared and turned out to share significant sequence similarity and, in part, the order of their genes. In 1986, the first comparative genomic study at a larger scale was published, comparing the genomes of varicella-zoster virus and Epstein-Barr virus that contained more than 100 genes each.\n\nThe first complete genome sequence of a cellular organism, that of \"Haemophilus influenzae\" Rd, was published in 1995. The second genome sequencing paper was of the small parasitic bacterium \"Mycoplasma genitalium\" published in the same year. Starting from this paper, reports on new genomes inevitably became comparative-genomic studies.\n\nThe first high-resolution whole genome comparison system was developed in 1998 by Art Delcher, Simon Kasif and Steven Salzberg and applied to the comparison of entire highly related microbial organisms with their collaborators at the Institute for Genomic Research (TIGR). The system is called MUMMER and was described in a publication in Nucleic Acids Research in 1999. The system helps researchers to identify large rearrangements, single base mutations, reversals, tandem repeat expansions and other polymorphisms. In bacteria, MUMMER enables the identification of polymorphisms that are responsible for virulence, pathogenicity, and anti-biotic resistance. The system was also applied to the Minimal Organism Project at TIGR and subsequently to many other comparative genomics projects.\n\n\"Saccharomyces cerevisiae\", the baker's yeast, was the first eukaryote to have its complete genome sequence published in 1996. After the publication of the roundworm \"Caenorhabditis elegans\" genome in 1998 and together with the fruit fly \"Drosophila melanogaster\" genome in 2000, Gerald M. Rubin and his team published a paper titled \"Comparative Genomics of the Eukaryotes\", in which they compared the genomes of the eukaryotes \"D. melanogaster\", \"C. elegans\", and \"S. cerevisiae\", as well as the prokaryote \"H. influenzae\". At the same time, Bonnie Berger, Eric Lander, and their team published a paper on whole-genome comparison of human and mouse.\n\nWith the publication of the large genomes of vertebrates in the 2000s, including human, the Japanese pufferfish \"Takifugu rubripes\", and mouse, precomputed results of large genome comparisons have been released for downloading or for visualization in a genome browser. Instead of undertaking their own analyses, most biologists can access these large cross-species comparisons and avoid the impracticality caused by the size of the genomes.\n\nNext-generation sequencing methods, which were first introduced in 2007, have produced an enormous amount of genomic data and have allowed researchers to generate multiple (prokaryotic) draft genome sequences at once. These methods can also quickly uncover single-nucleotide polymorphisms, insertions and deletions by mapping unassembled reads against a well annotated reference genome, and thus provide a list of possible gene differences that may be the basis for any functional variation among strains.\n\nOne character of biology is evolution, evolutionary theory is also the theoretical foundation of comparative genomics, and at the same time the results of comparative genomics unprecedentedly enriched and developed the theory of evolution. When two or more of the genome sequence are compared, one can deduce the evolutionary relationships of the sequences in a phylogenetic tree. Based on a variety of biological genome data and the study of vertical and horizontal evolution processes, one can understand vital parts of the gene structure and its regulatory function.\n\nSimilarity of related genomes is the basis of comparative genomics. If two creatures have a recent common ancestor, the differences between the two species genomes are evolved from the ancestors’ genome. The closer the relationship between two organisms, the higher the similarities between their genomes. If there is close relationship between them, then their genome will display a linear behaviour (synteny), namely some or all of the genetic sequences are conserved. Thus, the genome sequences can be used to identify gene function, by analyzing their homology (sequence similarity) to genes of known function.\n\nOrthologous sequences are related sequences in different species: a gene exists in the original species, the species divided into two species, so genes in new species are orthologous to the sequence in the original species. Paralogous sequences are separated by gene cloning (gene duplication): if a particular gene in the genome is copied, then the copy of the two sequences is paralogous to the original gene. A pair of orthologous sequences is called orthologous pairs (orthologs), a pair of paralogous sequence is called collateral pairs (paralogs). Orthologous pairs usually have the same or similar function, which is not necessarily the case for collateral pairs. In collateral pairs, the sequences tend to evolve into having different functions.\nComparative genomics exploits both similarities and differences in the proteins, RNA, and regulatory regions of different organisms to infer how selection has acted upon these elements. Those elements that are responsible for similarities between different species should be conserved through time (stabilizing selection), while those elements responsible for differences among species should be divergent (positive selection). Finally, those elements that are unimportant to the evolutionary success of the organism will be unconserved (selection is neutral).\n\nOne of the important goals of the field is the identification of the mechanisms of eukaryotic genome evolution. It is however often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. For this reason comparative genomics studies of small model organisms (for example the model Caenorhabditis elegans and closely related Caenorhabditis briggsae) are of great importance to advance our understanding of general mechanisms of evolution.\n\nComputational approaches to genome comparison have recently become a common research topic in computer science. A public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while multiple courses will begin training students to be fluent in both topics.\n\nComputational tools for analyzing sequences and complete genomes are developing quickly due to the availability of large amount of genomic data. At the same time, comparative analysis tools are progressed and improved. In the challenges about these analyses, it is very important to visualize the comparative results.\n\nVisualization of sequence conservation is a tough task of comparative sequence analysis. As we know, it is highly inefficient to examine the alignment of long genomic regions manually. Internet-based genome browsers provide many useful tools for investigating genomic sequences due to integrating all sequence-based biological information on genomic regions. When we extract large amount of relevant biological data, they can be very easy to use and less time-consuming.\n\n\nAn advantage of using online tools is that these websites are being developed and updated constantly. There are many new settings and content can be used online to improve efficiency.\n\nAgriculture is a field that reaps the benefits of comparative genomics. Identifying the loci of advantageous genes is a key step in breeding crops that are optimized for greater yield, cost-efficiency, quality, and disease resistance. For example, one genome wide association study conducted on 517 rice landraces revealed 80 loci associated with several categories of agronomic performance, such as grain weight, amylose content, and drought tolerance. Many of the loci were previously uncharacterized. Not only is this methodology powerful, it is also quick. Previous methods of identifying loci associated with agronomic performance required several generations of carefully monitored breeding of parent strains, a time consuming effort that is unnecessary for comparative genomic studies.\n\nThe medical field also benefits from the study of comparative genomics. Vaccinology in particular has experienced useful advances in technology due to genomic approaches to problems. In an approach known as reverse vaccinology, researchers can discover candidate antigens for vaccine development by analyzing the genome of a pathogen or a family of pathogens. Applying a comparative genomics approach by analyzing the genomes of several related pathogens can lead to the development of vaccines that are multiprotective. A team of researchers employed such an approach to create a universal vaccine for Group B Streptococcus, a group of bacteria responsible for severe neonatal infection. Comparative genomics can also be used to generate specificity for vaccines against pathogens that are closely related to commensal microorganisms. For example, researchers used comparative genomic analysis of commensal and pathogenic strains of E. coli to identify pathogen specific genes as a basis for finding antigens that result in immune response against pathogenic strains but not commensal ones.\n\nComparative genomics also opens up new avenues in other areas of research. As DNA sequencing technology has become more accessible, the number of sequenced genomes has grown. With the increasing reservoir of available genomic data, the potency of comparative genomic inference has grown as well. A notable case of this increased potency is found in recent primate research. Comparative genomic methods have allowed researchers to gather information about genetic variation, differential gene expression, and evolutionary dynamics in primates that were indiscernible using previous data and methods. The Great Ape Genome Project used comparative genomic methods to investigate genetic variation with reference to the six great ape species, finding healthy levels of variation in their gene pool despite shrinking population size. Another study showed that patterns of DNA methylation, which are a known regulation mechanism for gene expression, differ in the prefrontal cortex of humans versus chimps, and implicated this difference in the evolutionary divergence of the two species.\n\n\n\n"}
{"id": "1826033", "url": "https://en.wikipedia.org/wiki?curid=1826033", "title": "Comparative neuropsychology", "text": "Comparative neuropsychology\n\nComparative neuropsychology refers to an approach used for understanding human brain functions. It involves the direct evaluation of clinical neurological populations by employing experimental methods originally developed for use with nonhuman animals.\n\nOver many decades of animal research, methods were perfected to study the effects of well-defined brain lesions on specific behaviors, and later the tasks were modified for human use. Generally the modifications involve changing the reward from food to money, but standard administration of the tasks in humans still involves minimal instructions, thus necessitating a degree of procedural learning in human and nonhuman animals alike.\n\nCurrently, comparative neuropsychology is used with neurological patients to link specific deficits with localized areas of the brain.\n\nThe comparative neuropsychological approach employs simple tasks that can be mastered without relying upon language skills. Precisely because these simple paradigms do not require linguistic strategies for solution, they are especially useful for working with patients whose language skills are compromised, or whose cognitive skills may be minimal.\n\nComparative neuropsychology contrasts with the traditional approach of using tasks that rely upon linguistic skills, and that were designed to study human cognition. Because important ambiguities about its heuristic value had not been addressed empirically, only recently has comparative neuropsychology become popular for implementation with brain-damaged patients.\n\nWithin the past decade, comparative neuropsychology has had prevalent use as a framework for comparing and contrasting the performances of disparate neurobehavioral populations on similar tasks.\n\nComparative neuropsychology involves the study of brain-behavior relationships by applying experimental paradigms, used extensively in animal laboratories, for testing human clinical populations. Popular paradigms include delayed reaction tasks, discrimination and reversal learning tasks, and matching- and nonmatching-to-sample. These tasks are used to test animals and relate them to human brain functioning. Such tasks were perfected on experimental animals having well defined brain lesions, and adapted for human neurological patients. The comparative aspects of such approach resides in the analogy between animals with brain lesions and human patients with lesions in homologous areas of the brain. One example is represented by the comparison between the brain of laboratory animals (primarily non human primates and mice) with the one of people with damages resulting from alcohol abuse.\n\nGeorge Ettlinger was one of the few who actively combined human and animal research, and he did so consistently throughout his scientific career. Ettinger work focused on the importance of the inferior temporal neocortex in visual discrimination learning and memory in macaque monkeys, and on the importance of ventral temporal lobe in vision. Ettinger animals models carried inferotemporal or latero-ventral prestriate ablation. In 1966 George Ettlinger, together with the psychologist Colin Blakemore and the neurosurgeon Murray Falconer, described the results of a study on correlation between pre-operative intelligence and the severity of mesial temporal sclerosis in temporal lobe specimens excised to treat intractable epilepsy. Such study It is known as a forerunner of what has become one of the potentially most interesting techniques for exploring the relationship between certain aspects of human memory and temporal lobe structures.\n\n\n"}
{"id": "2466507", "url": "https://en.wikipedia.org/wiki?curid=2466507", "title": "Comparative sociology", "text": "Comparative sociology\n\nComparative sociology involves comparison of the social processes between nation states, or across different types of society (for example capitalist and socialist). There are two main approaches to comparative sociology: some seek similarity across different countries and cultures whereas others seek variance. For example, structural Marxists have attempted to use comparative methods to discover the general processes that underlie apparently different social orderings in different societies. The danger of this approach is that the different social contexts are overlooked in the search for supposed universal structures.\n\nOne sociologist who employed comparative methods to understand variance was Max Weber, whose studies attempted to show how differences between cultures explained the different social orderings that had emerged (see for example \"The Protestant Ethic and the Spirit of Capitalism\" and Sociology of religion).\n\nThere is some debate within sociology regarding whether the label of 'comparative' is suitable. Emile Durkheim argued in \"The Rules of Sociological Method\" (1895) that all sociological research was in fact comparative since social phenomenon are always held to be typical, representative or unique, all of which imply some sort of comparison. In this sense, all sociological analysis is comparative and it has been suggested that what is normally referred to as comparative research, may be more appropriately called cross-national research.\n\n"}
{"id": "1266596", "url": "https://en.wikipedia.org/wiki?curid=1266596", "title": "Comparison of Dewey and Library of Congress subject classification", "text": "Comparison of Dewey and Library of Congress subject classification\n\nThis is a conversion chart showing how the Dewey Decimal and Library of Congress Classification systems organize resources by concept, in part for the purpose of assigning . These two systems account for over 95% of the classification in United States libraries, and are used widely around the world.\n\nThe chart includes all ninety-nine second level (two-digit) DDC classes (040 is not assigned), and should include all second level (two-digit) LCC classes. Where a class in one system maps to several classes in other system, it will be listed multiple times (e.g. DDC class 551).\n\nAdditional information on these classification plans is available at:\n\n\n"}
{"id": "13243925", "url": "https://en.wikipedia.org/wiki?curid=13243925", "title": "Comparison of Star Trek and Star Wars", "text": "Comparison of Star Trek and Star Wars\n\n\"Star Trek\" and \"Star Wars\" are American media franchises which present alternative scenarios of space adventure. The two franchises are dominant in this setting of storytelling and have offered various forms of media productions for decades that manage billions of dollars of intellectual property, providing employment and entertainment for billions of people around the world.\n\n\"Star Trek\" was introduced as in 1966 that lasted three years. \"\" commenced in 1973 (based directly on the original series) but lasted only two seasons with a combined total of 22 episodes. With the subsequent publication of novels, comics, animated series, toys and feature films, \"Star Trek\" grew into a popular media franchise.\n\n\"Star Wars\" was introduced as a feature film, \"A New Hope\" (1977). A novelization titled \"\", based on the original script of the film, was published about a year earlier. Upon the release of the first film, \"Star Wars\" quickly grew into a popular media franchise.\n\n\"Star Trek\" debuted in television. The franchise was conceived in the style of the television Western \"Wagon Train\" and the adventure stories of Horatio Hornblower, but evolved into an idealistic, utopian prospect of future human society. Inspired by \"Gulliver's Travels\", \"Star Trek\"s main focus is of space exploration and a galactic society consisting of multiple planets and species, where conflict occasionally occurs. \"Star Trek\" occurs in the relatively distant future, specifically the 22nd through 24th centuries, with occasional time travel and interdimensional travel. The Earth of the \"Star Trek\" universe shares most of its history with the real world.\n\n\"Star Wars\" debuted in film, despite the novel based on the film's original script having been published a year before the film itself. \"Star Wars\" mainly belongs to the space opera subgenre of science fiction that follows The Hero's Journey and was inspired by works such as Beowulf, King Arthur and other mythologies, world religions, as well as ancient and medieval history. It depicts a galactic society in constant conflict. Though there are periods of peace, these are only documented in novels, comics, video games, non-feature films and other spin-off media. \"Star Wars\" is set \"a long time ago, in a galaxy far, far away\", although many characters are human, occasionally use Earth metaphors and exhibit human character traits.\n\nAlthough both \"Star Trek\" and \"Star Wars\" populate various forms of media, not all types have been produced that are mutual to both franchises. \"Star Wars\" has not produced any live-action television series while \"Star Trek\" has produced seven live-action television series.\n\n\"Star Trek\" likewise has not produced any television films; whereas \"Star Wars\" has produced at least three live-action television films outside the \"Star Wars\" film saga. The \"Star Wars Holiday Special\", \"\" and \"\" are all live-action television spin-off films set in the \"Star Wars\" universe, but not considered part of the official \"Star Wars canon\".\n\n\nAside from both having the word \"star\" in their titles, the two franchises share many similarities and commonalities. Both franchises have their origins in the space western subgenre.\n\nBoth stories depict societies consisting of multiple planets and species. The main galaxy in \"Star Trek\" consists of various planets, each inhabited by different species, united into a single state, the United Federation of Planets. \"Star Wars\" depicts a galaxy that is mostly part of a single state known as the Old Republic, inhabited by humans and countless other species, which later became the Galactic Empire and was again later reformed into a new society called the New Republic after a series of wars.\n\nBoth franchises promote philosophical and political messages.\n\nThe primary philosophies of \"Star Trek\" convey the morals of exploration and interference and how to properly confront and ethically resolve a new situation. Creator Gene Roddenberry was inspired by morality tales such as \"Gulliver's Travels\".\n\nThe primary philosophical messages of \"Star Wars\" are the ethics of good against evil and how to distinguish them from one another. \"Star Wars\" preaches against totalitarian systems and favors societies that offer equality. In an interview on the \"Star Wars\" 20th Anniversary UK Programme aired in 1997 referring to the mythology of the original \"Star Wars\" trilogy, Patrick Stewart stated \"A belief in one's own powers; especially one's own powers to do good because the underlying morality of \"Star Wars\" is a very very positive one.\"\n\nThere have been actors from both franchises who have appeared on common television series such as \"The Outer Limits\" and \"Seaquest\".\n\nBoth franchises also derive significantly from history and ancient mythology, including Greco-Roman mythology. Many planets and alien species in \"Star Trek\", for instance, are named after ancient Roman deities. Several episodes from various \"Star Trek\" television series, such as \"Who Mourns for Adonais\", are directly based on ancient Greek-Roman themes and settings. The series also make references to Ancient Babylon and its mythic folklore. The Klingons and their warrior culture are a representation of the 12th-century Mongols.\n\nMuch of \"Star Wars\" story plots and character developments are based on ancient history, including classical Greece and Rome, such as the fall of the Old Republic in \"Star Wars\", followed by the rise of the Galactic Empire, which parallels the fall of the ancient Roman Republic followed by the rise of the Roman Empire.\n\nA 1983 documentary on the making of \"Star Wars Episode VI: Return of the Jedi\" was hosted by Leonard Nimoy, who also made mention of Lucas's original plan to do two other trilogies preceding and proceeding the original trilogy.\n\nJ. J. Abrams, director and producer of \"Star Trek\" (2009) and \"Star Trek Into Darkness\" (2013) and producer of \"Star Trek Beyond\" (2016), directed and produced \"\" (2015). \"Star Trek\" (2009) and \"Star Wars: The Force Awakens\" (2015) are each the first entries in expected trilogies. These films received favorable critical and commercial response and revived interest for both franchises. In addition to Abrams, actors such as Simon Pegg starred in both series.\n\n\"\" (2002) was poorly received and \"\" (2005) had capped off the prequel trilogy, which overall had a mixed to positive reception.\n\nThe newer films of the two franchises filmed major scenes in the United Arab Emirates. The desert scenes on the planet Jakku in \"Star Wars: The Force Awakens\" (2015) were filmed in the Emirate of Abu Dhabi, while scenes for cities in the film \"Star Trek Beyond\" (2016) were filmed in the Emirate of Dubai.\n\nThe two franchises now offer almost all forms of media ranging from novels, television series, comic books, toys for younger audience, magazines, themed merchandise, board games and video games, as well as fan works. These include canonical and non-canonical works, including works made both by series producers and fans jointly.\n\nDespite the difference in the numbers of films, the profit made by the \"Star Wars\" film series exceed the profit of the \"Star Trek\" film series by almost five times, while the entire franchise outgrosses the other by four times. It is difficult to accurately judge the total worth of each franchise as television series, memorabilia and video games must be taken into account.\n\nScience fiction writer David Brin criticized \"Star Wars\" at the time of the release of \"The Phantom Menace\", arguing that while the \"Star Wars\" movies provide special effects and action/adventure, audiences are not encouraged to engage with their overriding themes. Among his issues with \"Star Wars\" and George Lucas, whom he accused of \"having an agenda\", is that the \"Star Wars\" galaxy is too \"elitist\", with arbitrary rulers on both the evil and good sides, replacing one another without any involvement of the population. He criticizes both sides of the Galactic Civil War as part of the \"same genetically superior royal family\". He finds the \"Star Wars\" universe flawed with additional forms of absolutism, such as justified emotions leading a good person to evil - for example citing the idea that Luke Skywalker killing Palpatine would somehow turn him to the dark side, despite the act potentially saving millions of lives.\n\nAmong the many other flaws he sees with \"Star Wars\" is that Anakin Skywalker becomes a hero in the ending of \"Return of the Jedi\" simply because he saved his son's life, while the atrocities he committed during his time in power go largely ignored. In contrast, he argues that, despite its flaws, \"Star Trek\" is \"democratic\" and follows genuine issues and strong questioning.\n\nWilliam Shatner argues that \"Star Trek\" is superior to \"Star Wars\". According to him, \"\"Star Trek\" had relationships and conflict among the relationships and stories that involved humanity and philosophical questions.\" Shatner believes that \"Star Wars\" was only better than \"Star Trek\" in terms of special effects, and that once J.J. Abrams became involved, \"Star Trek\" was able to \"supersede \"Star Wars\" on every level\".\n\nTim Russ, who played Tuvok on \"\", claims that it is difficult to find common enough elements to be able to compare the two. Among those common elements are their similar settings of unique characters and technologies. He echoed Shatner that \"Star Trek\" reflects common human issues, the morals of exploration and considers ethical questions. \"Star Wars\" in his view is a classic medieval tale dressed up as action-adventure, and that embraces the Eastern philosophy of inner-strength. Russ concludes that despite both their success and popularity, \"Star Trek\" comes out as the better of the two, as it is set in \"our\" galaxy and therefore people can relate better to it, whereas \"Star Wars\" takes place in another galaxy. He acknowledged that he could be biased.\n\nJeremy Bulloch is best known for his role as Boba Fett in the original \"Star Wars\" trilogy. He is a huge fan of \"Star Trek: The Original Series\". He argued that while both franchises are popular, \"Star Wars\" comes out as the superior, for its soundtracks and special effects.\n\nContrasting the focus of the two franchises, contributor J.C. Herzthe of \"The New York Times\" argued, \"\"Trek\" fandom revolves around technology because the \"Star Trek\" universe was founded on ham-fisted dialogue and \"Gong Show\"-caliber acting. But the fictional science has always been brilliant. The science in \"Star Wars\" is nonsense, and everyone knows it. But no one cares because \"Star Wars\" isn't about science. It's epic drama. It's about those incredibly well-developed characters and the moral decisions they face. People don't get into debates about how the second Death Star works. They get into debates about the ethics of blowing it up.\"\n\nJohn Wenzel of \"The Denver Post\" highlighted two differences in approach, noting the \"swashbuckling\" and \"gunslinger\" style of \"Star Wars\" compared with \"Star Trek\"s \"broader themes of utopian living, justice and identity\" and that the spiritual aspect of \"Star Wars\" contrasts with the balance of emotion and logic seen in \"Star Trek\".\n\nBillionaire Peter Thiel told Dowd \"I'm a capitalist. \"Star Wars\" is the capitalist show. \"Star Trek\" is the communist one\". He further stated \"There is no money in \"Star Trek\" because you just have the transporter machine that can make anything you need. The whole plot of \"Star Wars\" starts with Han Solo having this debt that he owes and so the plot in \"Star Wars\" is driven by money.\"\n\nArchived footage in \"Trek Nation\" showed Gene Roddenberry saying, \"I like \"Star Wars\". It was young King Arthur growing up, slaying the evil emperor finally. There's nothing wrong with that kind of entertainment - everything doesn't have to create a philosophy for you - for your whole life. You can also have fun.\"\n\nThe two franchises have a \"symbiotic relationship\" stated Shatner, who credits \"Star Wars\" for launching the \"Star Trek\" films. He repeated this sentiment at a 2016 \"Star Trek\" fan convention in Las Vegas by stating \"\"Star Wars\" created \"Star Trek\"\". He clarified this statement by explaining that at the time of the release of the first \"Star Wars\" film (\"A New Hope\"), Paramount, then under new management, was struggling to come up with something that could compete with it. A \"Star Trek\" relaunch was the choice. Since then, public interest has returned to \"Star Trek\". \"It was \"Star Wars\" that thrust \"Star Trek\" into the people of Paramount's consciousness\" Shatner stated.\n\nThe documentary \"Trek Nation\" features interviews where both Lucas and Roddenberry praise each other's respective franchises, with the former stating that \"Star Trek\" was an influence while writing the original screenplay for \"Star Wars\". He explained that while both franchises were so \"far out\", \"Star Trek\" produced a fanbase that \"softened up the entertainment arena\" so that \"Star Wars\" could \"come along and stand on its shoulders.\" This is also acknowledged by Shatner, who went as far as to call \"Star Wars\" a \"derivative\" of \"Star Trek\".\n\nA few references to \"Star Wars\" have been inserted into \"Star Trek\" films. For fleeting moments, one can see ships and droids from \"Star Wars\" in both \"Star Trek\" (2009) and \"Star Trek Into Darkness\" (2013). Some \"Star Trek\" films and television episodes used the \"Star Wars\" animation shop, Industrial Light & Magic, for their special effects.\n\nWhen Roddenberry was honored at a \"Star Trek\" convention late in life, a congratulatory letter from Lucas was presented by an actor dressed as Darth Vader. A few years earlier, Roddenberry had contributed an entry in honor of \"Star Wars\" and Lucas at a convention honoring the latter.\n\nWilliam Shatner was a presenter at Lucas' American Film Institute Lifetime Achievement Award ceremony in 2007 and did a comical stage performance honoring Lucas.\n\nAt a live concert, Shatner dressed as an imperial stormtrooper singing \"Girl Crush\" alongside Carrie Underwood and Brad Paisley.\n\nIn 2011, Shatner and Carrie Fisher posted a series of humorous YouTube videos satirizing each other's franchises.\n\nIn a 2016 interview, Shatner commented that Captain Kirk and Princess Leia eloping and running off into the sunset would be the \"perfect union\" between \"Star Trek\" and \"Star Wars\".\n\nShatner has also posted a number of humorous tweets on his Twitter account mocking \"Star Wars\". Amongst them were commemorating the 35th anniversary of the poorly received \"Star Wars Holiday Special\". It was then that \"Star Wars\" actor Peter Mayhew posted a \"retaliation\" tweet congratulating Shatner for the directing of \"\", another poorly received film.\n\nBoth franchises are set to grow throughout the next decade.\n\n\"Star Trek\" was rebooted with a series of feature films starting with the \"Star Trek\" reboot (2009), which was followed by \"Star Trek Into Darkness\" (2013) and \"Star Trek Beyond\" (2016) and a number of sequels are set to follow. A new television series based in the original timeline, subtitled \"\", serving as a prequel to the original series, debuted on CBS All Access, an online streaming platform, in 2017.\n\n\"Star Wars\" picked up from where \"Return of the Jedi\" left off, with \"\" the first in the new trilogy, and \"\" following two years later. Additionally, more spin-off media is also underway after the debut of \"Star Wars Rebels\", a television series set in between the \"Star Wars\" prequels and the original trilogy, and an anthology of stand-alone \"Star Wars\" films, starting with \"Rogue One\", which was released in December 2016, and \"\" following in May 2018.\n\nAside from official works by the producers of \"Star Trek\" and \"Star Wars\", many fan films and webisodes set in the two universes of the franchises are also constantly produced and posted on the Internet by fans, but are not officially considered canon in relation to either franchise.\n\n"}
{"id": "32639223", "url": "https://en.wikipedia.org/wiki?curid=32639223", "title": "Comparison of online charity donation services in the United Kingdom", "text": "Comparison of online charity donation services in the United Kingdom\n\nThe page is a comparison of notable online charity donation services in the UK.\n\nThe table below gives examples of the various transaction fees for a £10 donation using each organisation, assuming they claim back the tax for the charity using gift aid. (Charities may also be charged set-up fees and monthly fees as detailed above.)\n\n\n"}
{"id": "590473", "url": "https://en.wikipedia.org/wiki?curid=590473", "title": "Contraction (grammar)", "text": "Contraction (grammar)\n\nA contraction is a shortened version of the written and spoken forms of a word, syllable, or word group, created by omission of internal letters and sounds.\n\nIn linguistic analysis, contractions should not be confused with crasis, abbreviations nor acronyms (including initialisms), with which they share some semantic and phonetic functions, though all three are connoted by the term \"abbreviation\" in loose parlance. Contraction is also distinguished from clipping, where beginnings and endings are omitted.\n\nThe definition overlaps with the term portmanteau (a linguistic \"blend\"), but a distinction can be made between a portmanteau and a contraction by noting that contractions are formed from words that would otherwise appear together in sequence, such as \"do\" and \"not\", whereas a portmanteau word is formed by combining two or more existing words that all relate to a singular concept which the portmanteau describes.\n\nEnglish has a number of contractions, mostly involving the elision of a vowel (which is replaced by an apostrophe in writing), as in \"I'm\" for \"I am\", and sometimes other changes as well, as in \"won't\" for \"will not\" or \"ain't\" for \"am not\". These contractions are commonly used in speech and in informal writing, though tend to be avoided in more formal writing (with limited exceptions, such as the mandatory form of \"o'clock\").\n\nThe main contractions are listed in the following table (for more explanation see English auxiliaries and contractions).\nSome other simplified pronunciations of common word groups, which can often equally be described as cases of elision, may also be considered (non-standard) contractions (not enshrined into the written standard language, but frequently expressed in written form anyway), such as \"wanna\" for \"want to\", \"gonna\" for \"going to\", \"y'all\" for \"you all\", \"ya'll\" for \"ya all\" in the Southern United States and others common forms in colloquial speech.\n\nIn subject–auxiliary inversion, the contracted negative forms behave as if they were auxiliaries themselves, changing place with the subject. For example, the interrogative form of \"He won't go\" is \"Won't he go\", whereas the uncontracted equivalent is \"Will he not go?\", with \"not\" following the subject.\n\nContractions exist in Classical Chinese, some of which are used in modern Chinese.\nContractions also appear in Cantonese, for example, 乜嘢 and 咩.\n\nThe French language has a variety of contractions, similar to English but mandatory, as in \"C'est la vie\" (\"That's life\"), where \"c'est\" stands for \"ce\" + \"est\" (\"that is\"). The formation of these contractions is called elision.\n\nIn general, any monosyllabic word ending in \"e caduc\" (schwa) will contract if the following word begins with a vowel, \"h\" or \"y\" (as \"h\" is silent and absorbed by the sound of the succeeding vowel; \"y\" sounds like \"i\"). In addition to \"ce\" → \"c'-\" (demonstrative pronoun \"that\"), these words are \"que\" → \"qu'-\" (conjunction, relative pronoun, or interrogative pronoun \"that\"), \"ne\" → \"n'-\" (\"not\"), \"se\" → \"s'-\" (\"himself\", \"herself\", \"itself\", \"oneself\" before a verb), \"je\" → \"j'-\" (\"I\"), \"me\" → \"m'-\" (\"me\" before a verb), \"te\" → \"t'- \" (informal singular \"you\" before a verb), \"le\" or \"la\" → \"l'-\" (\"the\"; or \"he/she\", \"it\" before a verb or after an imperative verb and before the word \"y\" or \"en\"), and \"de\" → \"d'-\" (\"of\"). Unlike with English contractions, however, these contractions are mandatory: one would never say (or write) \"*ce est\" or \"*que elle\".\n\n\"Moi\" (\"myself\") and \"toi\" (informal \"yourself\") mandatorily contract to \"m'-\" and \"t'-\" respectively after an imperative verb and before the word \"y\" or \"en\".\n\nIt is also mandatory to avoid the repetition of a sound when the conjunction \"si\" (\"if\") is followed by \"il\" (\"he\", \"it\") or \"ils\" (\"they\"), which begin with the same vowel sound \"i\": \"*si il\" → \"s'il\" (\"if it\", if he\"); \"*si ils\" → \"s'ils\" (\"if they\").\n\nCertain prepositions are also mandatorily merged with masculine and plural direct articles: \"au\" for \"à le\", \"aux\" for \"à les\", \"du\" for \"de le\", and \"des\" for \"de les\". However, the contraction of \"cela\" (demonstrative pronoun \"that\") to \"ça\" is optional and informal.\n\nIn informal speech, a personal pronoun may sometimes be contracted onto a following verb. For example, \"je ne sais pas\" (, \"I don't know\") may be pronounced roughly \"chais pas\" (), with the \"ne\" being completely elided and the of \"je\" being mixed with the of \"sais\". It is also common in informal contexts to contract \"tu\" to \"t'-\" before a vowel, e.g., \"t'as mangé\" for \"tu as mangé\".\n\nIn Modern Hebrew, the prepositional prefixes -בְּ /bə-/ 'in' and -לְ /lə-/ 'to' contract with the definite article prefix -ה (/ha-/) to form the prefixes -ב /ba/ 'in the' and -ל /la/ 'to the'. In colloquial Israeli Hebrew, the preposition את (/ʔet/), which indicates a definite direct object, and the definite article prefix -ה (/ha-/) are often contracted to 'ת (/ta-/) when the former immediately precedes the latter. Thus ראיתי את הכלב (/ʁaˈʔiti ʔet haˈkelev/, \"I saw the dog\") may become ראיתי ת'כלב (/ʁaˈʔiti taˈkelev/).\n\nIn Italian, prepositions merge with direct articles in predictable ways. The prepositions \"a\", \"da\", \"di\", \"in\", \"su\", \"con\" and \"per\" combine with the various forms of the definite article, namely \"il\", \"lo\", \"la\", \"l',\" \"i\", \"gli\", \"gl',\" and \"le\".\n\n\nThe words \"ci\" and \"è\" (form of \"essere\", to be) and the words \"vi\" and \"è\" are contracted into \"c'è\" and \"v'è\" (both meaning \"there is\").\n\nThe words \"dove\" and any word that begins with \"e\" are contracted into one single, deleting the e of the principal word, dove (dov'). Equally \"come\" does be made so.\nAs well other words may be contracted the same these two, like \"quale\", and other ones, etcetera.\n\nSpanish has two mandatory phonetic contractions between prepositions and articles: \"al\" (to the) for \"a el\", and \"del\" (of the) for \"de el\" (not to be confused with \"a él\", meaning \"to him\", and \"de él\", meaning \"his\" or, more literally, \"of him\").\n\nOther contractions were common in writing until the 17th century, the most usual being \"de\" + personal and demonstrative pronouns: \"destas\" for \"de estas\" (of these, fem.), \"daquel\" for \"de aquel\" (of that, masc.), \"dél\" for \"de él\" (of him) etc.; and the feminine article before words beginning with \"a-\": \"l'alma\" for \"la alma\", now \"el alma\" (the soul). Several sets of demonstrative pronouns originated as contractions of \"aquí\" (here) + pronoun, or pronoun + \"otro/a\" (other): \"aqueste\", \"aqueso\", \"estotro\" etc. The modern \"aquel\" (that, masc.) is the only survivor of the first pattern; the personal pronouns \"nosotros\" (we) and \"vosotros\" (pl. you) are remnants of the second. In medieval texts unstressed words very often appear contracted: \"todol\" for \"todo el\" (all the, masc.), \"ques\" for \"que es\" (which is); etc. including with common words, like d'ome (d'home/d'homme) instead de ome (home/homme), and so on.\n\nThough not strictly a contraction, a special form is used when combining con with mí, ti or sí which is written as \"conmigo\" for *\"con mí\" (with me), \"contigo\" for *\"con ti\" (with you sing.), \"consigo\" for *\"con sí\" (with himself/herself/itself/themselves (themself).\n\nFinally, one can hear \"pa\"' for \"para\", deriving as \"pa'l\" for \"para el\", but these forms are only considered appropriate in informal speech.\n\nIn Portuguese, contractions are common and much more numerous than those in Spanish. Several prepositions regularly contract with certain articles and pronouns. For instance, \"de\" (of) and \"por\" (by; formerly \"per\") combine with the definite articles \"o\" and \"a\" (masculine and feminine forms of \"the\" respectively), producing \"do\", \"da\" (of the), \"pelo\", \"pela\" (by the). The preposition \"de\" contracts with the pronouns \"ele\" and \"ela\" (he, she), producing \"dele\", \"dela\" (his, her). In addition, some verb forms contract with enclitic object pronouns: e.g., the verb \"amar\" (to love) combines with the pronoun \"a\" (her), giving \"amá-la\" (to love her).\n\nAnother contraction in portuguese which is similar to English ones is the combination of the pronoun \"da\" with words starting in \"a\", resulting in changing the first letter \"a\" for an apostrophe and joining both words. Examples: \"Estrela d'alva\" (A popular phrase to refer to Venus that means \"Alb star\", as a reference to its brightness) ; \"Caixa d'água\" (water tank).\n\nIn informal, spoken German prepositional phrases, one can often merge the preposition and the article; for example, \"von dem\" becomes \"vom\", \"zu dem\" becomes \"zum\", or \"an das\" becomes \"ans\". Some of these are so common that they are mandatory. In informal speech, \"aufm\" for \"auf dem\", \"unterm\" for \"unter dem\", etc. are also used, but would be considered to be incorrect if written, except maybe in quoted direct speech, in appropriate context and style.\n\nThe pronoun \"es\" often contracts to \"s\" (usually written with the apostrophe) in certain contexts. For example, the greeting \"Wie geht es?\" is usually encountered in the contracted form \"Wie geht's?\".\n\nRegional dialects of German, and various local languages which usually were already used long before today's Standard German was created, do use contractions usually more frequently than German, but varying widely between different local languages. The informally spoken German contractions are observed almost everywhere, most often accompanied by additional ones, such as \"in den\" becoming \"in'n\" (sometimes \"im\") or \"haben wir\" becoming \"hamwer\", \"hammor\", \"hemmer\", or \"hamma\" depending on local intonation preferences. Bavarian German features several more contractions such as \"gesund sind wir\" becoming \"xund samma\" which are schematically applied to all word or combinations of similar sound. (One must remember, however, that German \"wir\" exists alongside Bavarian \"mir\", or \"mia\", with the same meaning.) The Munich-born footballer Franz Beckenbauer has as his catchphrase \"Schau mer mal\" (\"Schauen wir einmal\" - in English \"let's have a look\"). A book about his career had as its title the slightly longer version of the phrase, \"Schau'n Mer Mal\".\n\nSuch features are found in all central and southern language regions. A sample from Berlin: \"Sag einmal, Meister, kann man hier einmal hinein?\" is spoken as \"Samma, Meesta, kamma hier ma rin?\"\n\nSeveral West Central German dialects along the Rhine River have built contraction patterns involving long phrases and entire sentences. In speech, words are often concatenated, and frequently the process of \"liaison\" is used. So, \"[Dat] kriegst Du nicht\" may become \"Kressenit\", or \"Lass mich gehen, habe ich gesagt\" may become \"Lomejon haschjesaat\".\n\nMostly, there are no binding orthographies for local dialects of German, hence writing is left to a great extent to authors and their publishers. Outside quotations, at least, they usually pay little attention to print more than the most commonly spoken contractions, so as not to degrade their readability. The use of apostrophes to indicate omissions is a varying and considerably less frequent process than in English-language publications.\n\nThe use of contractions is not allowed in any form of standard Norwegian spelling, however, it is fairly common to shorten or contract words in spoken language. Yet, the commonness varies from dialect to dialect and from sociolect to sociolect—it depends on the formality etc. of the setting. Some common, and quite drastic, contractions found in Norwegian speech are \"jakke\" for \"jeg har ikke\", meaning \"I do not have\" and \"dække\" for \"det er ikke\", meaning \"there is not\". The most frequently used of these contractions—usually consisting of two or three words contracted into one word, contain short, common and often monosyllabic words like , , , , or . The use of the apostrophe (') is much less common than in English, but is sometimes used in contractions to show where letters have been dropped.\n\nIn extreme cases, long, entire sentences may be written as one word. An example of this is \"Det ordner seg av seg selv\" in standard written Bokmål, meaning \"It will sort itself out\" could become \"dånesæsæsjæl\" (note the letters Å and Æ, and the word \"sjæl\", as an eye dialect spelling of ). R-dropping, being present in the example, is especially common in speech in many areas of Norway , but plays out in different ways, as does elision of word-final phonemes like .\n\nBecause of the many dialects of Norwegian and their widespread use it is often difficult to distinguish between non-standard writing of standard Norwegian and eye dialect spelling. It is almost universally true that these spellings try to convey the way each word is pronounced, but it is rare to see language written that does not adhere to at least some of the rules of the official orthography. Reasons for this include words spelled unphonemically, ignorance of conventional spelling rules, or adaptation for better transcription of that dialect's phonemes.\n\nLatin contains several examples of contractions. One such case is preserved in the verb \"nolo\" (I am unwilling/do not want) which was formed by a contraction of \"non volo\" (\"volo\" meaning “I want”). Similarly this is observed in the first person plural and third person plural forms (nolumus and nolunt respectively).\n\nSome contractions in rapid speech include ～っす (\"-ssu\") for です (\"desu\") and すいません (\"suimasen\") for すみません (\"sumimasen\"). では (\"dewa\") is often contracted to じゃ (\"ja\"). In certain grammatical contexts the particle の (\"no\") is contracted to simply ん (\"n\").\n\nWhen used after verbs ending in the conjunctive form ～て (\"-te\"), certain auxiliary verbs and their derivations are often abbreviated. Examples:\n<nowiki>*</nowiki> this abbreviation is never used in the polite conjugation, to avoid the resultant ambiguity between an abbreviated \"ikimasu\" (go) and the verb \"kimasu\" (come).\n\nThe ending ～なければ (\"-nakereba\") can be contracted to ～なきゃ (\"-nakya\") when it is used to indicate obligation. It is often used without an auxiliary, e.g., 行かなきゃ（いけない） (\"ikanakya (ikenai)\") \"I have to go.\"\n\nOther times, contractions are made to create new words or to give added or altered meaning:\n\nVarious dialects of Japanese also use their own specific contractions which are often unintelligible to speakers of other dialects.\n\nIn the Polish language pronouns have contracted forms which are more prevalent in their colloquial usage. Examples are \"go\" and \"mu\". The non-contracted forms are \"jego\" (unless it is used as a possessive pronoun) and \"jemu\", respectively. The clitic \"-ń\" which stands for \"niego\" (him) as in \"dlań\" (\"dla niego\") is more common in literature. The non-contracted forms are generally used as a means to accentuate.\n\nUyghur, a Turkic language spoken in Central Asia, includes some verbal suffixes that are actually contracted forms of compound verbs (serial verbs). For instance, \"sëtip alidu\" (sell-manage, \"manage to sell\") is usually written and pronounced \"sëtivaldu\", with the two words forming a contraction and the [p] leniting into a [v] or [w].\n\nIn Filipino, most contractions need other words to be contracted correctly. Only words that end with vowels can make a contraction with words like \"at\" and \"ay.\" In this chart, the \"@\" represents any vowel.\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "1833848", "url": "https://en.wikipedia.org/wiki?curid=1833848", "title": "Cross-reference", "text": "Cross-reference\n\nThe term cross-reference can refer to either:\n\nIn a document, especially those authored in a Content management system,\na cross-reference has two major aspects:\n\nThe visible form contains text, graphics, and other indications that:\n\nThe technical mechanism that resides within the system:\n\nIf the cross reference mechanism is well designed, the reader will be able to follow each cross reference to the referenced content whether the content is presented in print or electronically.\n\nAn author working in a content management system is responsible for identifying subjects of interest that cross documents, and creating appropriate systems of cross references to support readers who seek to understand those subjects. For an individual cross reference, an author should ensure that location and content of the target of the cross reference are clearly identified, and the reader can easily determine how to follow the cross reference in each medium in which publication is supported.\n\nContent strategy practitioners (known as content strategists) specialize in planning content to meet business needs, taking into account the processes for creating and maintaining the content, and the systems that support the content.\n\n"}
{"id": "58632079", "url": "https://en.wikipedia.org/wiki?curid=58632079", "title": "Encyclopedia of Forensic and Legal Medicine 2nd Edition", "text": "Encyclopedia of Forensic and Legal Medicine 2nd Edition\n\nThe Encyclopedia of Forensic and Legal Medicine 2nd Edition is a reference source and pioneering 4 set encyclopedia of forensics and medico-legal knowledge published by Academic Press, Elsevier in 2016. This has been edited by the renowned British forensic specialist Jason Payne-James and Australian forensic pathologist Roger W. Byard and an international editorial board. \nThis reference work includes more than 300 articles contributed by forensic medicine and forensic science experts from all over the world. The encyclopedia is a complete reference source of articles covering from forensics, criminal investigations, health-care, legal, judicial, ballistics, toxicology,fingerprinting, DNA typing, disaster victim identification to autopsy and postmortem examination.\n\nThe encyclopedia is especially meant for forensic, medical, chemistry, physics, laboratory technologists and anthropology students and specialists such as forensic experts, lawyers, judicial officers, judges, police and investigating offices, nurses, medical officers etc. All the articles of the encyclopedia are available through Science direct and Scopus.\n"}
{"id": "1720724", "url": "https://en.wikipedia.org/wiki?curid=1720724", "title": "Fumblerules", "text": "Fumblerules\n\nA fumblerule is a rule of language or linguistic style, humorously written in such a way that it breaks this rule. Fumblerules are a form of self-reference.\n\nThe science editor George L. Trigg published a list of such rules in 1979. The term \"fumblerules\" was coined in a list of such rules compiled by William Safire on Sunday, 4 November 1979, in his column \"On Language\" in the \"New York Times\". Safire later authored a book titled \"Fumblerules: A Lighthearted Guide to Grammar and Good Usage\", which was reprinted in 2005 as \"\".\n\n\n\n"}
{"id": "33487458", "url": "https://en.wikipedia.org/wiki?curid=33487458", "title": "Guide to information sources", "text": "Guide to information sources\n\nA Guide to information sources (or a bibliographic guide, a literature guide, a guide to reference materials, a subject gateway, etc.) is a kind of metabibliography. Ideally it is not just a listing of bibliographies, reference works and other information sources, but more like a textbook introducing users to the information sources in a given field (in general).\n\nSuch guides may have many different forms: Comprehensive or highly selective, printed or electronic sources, annoteted listings or written chapters etc.\n\nOften used as curriculum tools for bibliographic instruction, the guides help library users find materials or help those unfamiliar with a discipline understand the key sources.\n\nAby, Stephen H., Nalen, James & Fielding, Lori (2005). Sociology; a guide to reference and information sources. 3rd ed. Westport, Conn.: Libraries Unlimited.\n\nAdams, Stephen R. (2005). \"Information Sources in Patents\"; 2nd ed. (Guides to Information Sources). München: K. G. Saur \n\nBlewett, Daniel K (2008). American military history; a guide to reference and information sources. 2nd ed. Westport, CT : Libraries Unlimited.\n\nJacoby, JoAnn & Kibbee, Josephine Z. (2007). Cultural anthropology; a guide to reference and information sources. 2nd ed. Westport, Conn.: Libraries Unlimited.\n\nSchmidt, Diane & Bell, George H. (2003). Guide to reference and information sources in the zoological sciences. Westport, Conn. : Libraries Unlimited.\n\nO'Hare, Christine (2007). \"Business Information Sources\". London: Library Assn Pub Ltd\n\nOstwald, W (1919). Die chemische Literatur und die Organisation der Wissenschaft. Leipzig : W. Ostwald & C. Drucker. (This is considered the first \"guide to information sources\").\n\nStebbins, Leslie F. (2006). Student guide to research in the digital age; how to locate and evaluate information sources. Westport, Conn.: Libraries Unlimited.\n\nWebb, W. H. et al. (Ed.). (1986). Sources of information in the social sciences. A Guide to the literature. 3. ed. Chicago : American Library Association.\n\nZell, Hans M. (ed.). (2003). The African studies companion; a guide to information sources. 3rd rev. and expanded ed. Glais Bheinn : Hans Zell.\n\n\n"}
{"id": "4491358", "url": "https://en.wikipedia.org/wiki?curid=4491358", "title": "Handbook", "text": "Handbook\n\nA handbook is a type of reference work, or other collection of instructions, that is intended to provide ready reference. The term originally applied to a small or portable book containing information useful for its owner, but the Oxford English Dictionary defines the current sense as \"any book...giving information such as facts on a particular subject, guidance in some art or occupation, instructions for operating a machine, or information for tourists.\" \n\nA handbook is sometimes referred to as a vade mecum (Latin, \"go with me\") or pocket reference. It may also be referred to as an enchiridion.\n\nHandbooks may deal with any topic, and are generally compendiums of information in a particular field or about a particular technique. They are designed to be easily consulted and provide quick answers in a certain area. For example, the MLA Handbook for Writers of Research Papers is a reference for how to cite works in MLA style, among other things. Examples of engineering handbooks include \"Perry's Chemical Engineers' Handbook\", \"Marks Standard Handbook for Mechanical Engineers\", and the \"CRC Handbook of Chemistry and Physics\".\n\n"}
{"id": "57442907", "url": "https://en.wikipedia.org/wiki?curid=57442907", "title": "Handbook of Middle American Indians", "text": "Handbook of Middle American Indians\n\nHandbook of Middle American Indians (HMAI) is a sixteen-volume compendium on Mesoamerica , from the prehispanic to the late twentieth century. Volumes on particular topics were published from the 1960s and 1970s under the general editorship of Robert Wauchope. Separate volumes with particular volume editors deal with a number of general topics, including archeology, cultural anthropology, physical anthropology, linguistics, with the last four substantive volumes treating various topics in Mesoamerican ethnohistory, under the editorship of Howard F. Cline. Select volumes have become available in e-book format.\n\nA retrospective review of the HMAI by two anthropologists discusses its history and evaluates it. One review calls it a fundamental work. Another reviewer says \"since the first volume of the HMAI appeared in 1964 is far and away the most comprehensive and erudite coverage of native cultures of any region in the Americas.\" A review in the journal \"Science\" says that \"There can be little doubt that, like the \"Handbook of South American Indians\", this monumental synthesis will provide a sound basis for new generalizations and will stimulate additional research to fill the gaps in knowledge and understanding that will become apparent.\n\nStarting in 1981, six volumes in the Supplement to the Handbook of Middle American Indians were published under the general editorship of Victoria Bricker.\n\nVolume 1. Natural Environment and Early Cultures, Robert C. West, volume editor. 1. Geohistory and Paleogeography of Middle America (Manuel Maldonado-Koerdell); 2. Surface Configuration and Associated Geology of Middle America (Robert C. West); 3. The Hydrography of Middle America (Jorge L. Tamayo, in collaboration with Robert C. West); 4. The American Mediterranean (Albert Collier); 5. Oceanography and Marine Life along the Pacific Coast (Carl L. Hubbs and Gunnar I. Roden); 6. Weather and Climate of Mexico and Central America (Jorge A. Vivo Escoto); 7. Natural Vegetation of Middle America (Philip L. Wagner); 8. The Soils of Middle America and their Relation to Indian Peoples and Cultures (Rayfred L. Stevens); 9. Fauna of Middle America (L. C. Stuart); 10. The Natural Regions of Middle America (Robert C. West); 11. The Primitive Hunters (Luis Aveleyra Arroyo de Anda); 12. The Food-gathering and Incipient Agriculture Stage of Prehistoric Middle America (Richard S. MacNeish); 13. Origins of Agriculture in Middle America (Paul C. Mangelsdorf, Richard S. MacNeish, and Gordon R. Willey); 14. The Patterns of Farming Life and Civilization (Gordon R. Willey, Gordon F. Ekholm, and Rene F. Millon)\n\nVolumes 2-3. Archeology of Southern Mesoamerica, Gordon R. Wiley, volume editor.\n\nVolume 4. ‘’Archeological Frontiers and External Connections G.F. Ekholm and G. R. Wiley, volume editors.\n\nVolume 5. ‘’Linguistics, Norman A. McQuown, volume editor.\n\nVolume 6. Social Anthropology, Manning Nash, volume editor. 1.Introduction, Manning Nash; 2. Indian Population and its Identification, Anselmo Marino Flores; 3.Agricultural Systems and Food Patterns, Angel Palerm; 4. Settlement Patterns, William T. Sanders; 5. Indian Economies, Manning Nash; 6. Contemporary Pottery and Basketry, George M. Foster; 7. Laquer, Katharine D. Jenkins; 8. Textiles and Costume, A.H. Gayton; 9. Drama, Dance and Music, Gertrude Prokosch Kurath; 10. Play: Games, Gossip, and Humor; 11. Kinship and Family, A. Kimball Romney; 12. Compadrinazgo, Robert Ravicz; 13. Local and Territoria Units, Eva Hunt and June Nash; 14. Political and Religious Organizations, Frank Cancian; 15. Levels of Communal Relations, Eric R. Wolf; 16. Annual Cycle and Fiesta Cycle, Ruben E. Reina; 17. Sickness and Social Relations, Richard N. Adams and Arthur J. Rubel; 18. Narrative Folklore, Munro S. Edmonson; 19. Religious Syncretism, William Madsen; 20. Ritual and Mythology, E. Michael Mendelson; 21. Psychological Orientations, Benjamin N. Colby; 22. Ethnic Relationships, Julio de la Fuente; 23. Acculturation, Ralph L. Beals; 24. Nationalization, Richard N. Adams; 25. Directed Change, Robert H. Ewald; 26. Urbanization and Industrialization, Arden R. King\n\nVolumes 7-8, Ethnology, Evan Z. Vogt, volume editor. Volume 7. Introduction (Evon Z. Vogt)Section I: The Maya 2; The Maya: Introduction (Evon Z. Vogt); 3. Guatemalan Highlands (Manning Nash); 4. The Maya of Northwestern Guatemala (Charles Wagley); 5. The Maya of the Midwestern Highlands (Sol Tax and Robert Hinshaw); 6. Eastern Guatemalan Highlands: The Pokomames and Chorti (Ruben E. Reina); 7. Chiapas Highlands (Evon Z. Vogt); 8. The Tzotzil (Robert M. Laughlin); 9. The Tzeltal (Alfonso Villa Rojas); 10. The Tojolabal (Roberta Montagu); 11. Maya Lowlands: The Chontal, Chol, and Kekchi (Alfonso Villa Rojas); 12. The Maya of Yucatan (Alfonso Villa Rojas); 13. The Lacandon (Gertrude Duby and Frans Blom); 14. The Huastec (Robert M. Laughlin); Section II: Southern Mexican Highlands and Adjacent Coastal Regions15. Southern Mexican Highlands and Adjacent Coastal Regions: Introduction (Ralph L. Reals); 16. The Zapotec of Oaxaca (Laura Nader); 17. The Chatino (Gabriel DeCicco); 18. The Mixtec (Robert Ravicz and A. Kimball Romney); 19. The Trique of Oaxaca (Laura Nader);20. The Amuzgo (Robert Ravicz and A. Kimball Romney); 21. The Cuicatec (Roberto J. Weitlaner); 22. The Mixe, Zoque, and Popoluca (George M. Foster); 23. The Huave (A. Richard Diebold, Jr.); 24. The Popoloca (Walter A. Hoppe, Andres Medina, and Roberto J. Weitlaner); 25. The Ichcatec (Walter A. Hoppe and Roberto J. Weitlaner); 26. The Chocho (Walter A. Hoppe and Roberto J. Weitlaner); 27. The Mazatec (Roberto J. Weitlaner and Walter A. Hoppe); 28. The Chinantec (Roberto J. Weitlaner and Howard F. Cline); 29. The Tequistlatec and Tlapanec (D. L. Olmsted); 30. The Cuitlatec (Susana Drucker, Roberto Escalante, and Roberto J. Weitlaner); Volume 8, Section III: Central Mexican Highlands; 31. Central Mexican Highlands: Introduction (Pedro Carrasco); 32. The Nahua (William Madsen); 33. The Totonac (H. R. Harvey and Isabel Kelly); 34. The Otomi (Leonardo Manrique C.); Section IV: Western Mexico 35. The Tarascans (Ralph L. Beals); Section V: Northwest Mexico; 36. Northwest Mexico: Introduction (Edward H. Spicer); 37. The Huichol and Cora (Joseph E. Grimes and Thomas B. Hinton); 38. The Southern Tepehuan and Tepecano (Carroll L. Riley); 39. The Northern Tepehuan (Elman R. Service); 40. The Yaqui and Mayo (Edward H. Spicer); 41. The Tarahumara (Jacob Fried); 42. Contemporary Ethnography of Baja California, Mexico (Roger C. Owen); 43. Remnant Tribes of Sonora: Opata, Pima, Papago, and Seri (Thomas B. Hinton).\n\nVolumes 6 & 7 were reviewed when the appeared. One reviewer highlights several articles, including those by Eric R. Wolf, Angel Palerm, and Willilam Sanders, but he goes on to say \"These volumes are ... more valuable for reference than for reading. Sections dealing with distribution, history, and bibliography are very useful, but sections dealing with social structure or the character of the peoples generally fail to provide integrated analyses indicating the essential features.\"\n\nVolume 9. Physical Anthropology, T.D. Stewart, volume editor.\n\nVolume 10-11. Archeology of Northern Mesoamerica, G. F. Ekholm and Ignacio Bernal, volume editors.\n\nVolumes 12-15, Guide to Ethnohistorical Sources, Howard F. Cline, Volume editor.\n\nVolume 12, Guide to Ethnohistorical Sources, Part 1. (1972) 1.“Introductory Notes on Territorial Divisions of Middle America” , Howard F. Cline, pp. 17–62; 2. “Colonial New Spain, 1519-1786: Historical Notes on the Evolution of Minor Political Jurisdictions”, Peter Gerhard, pp. 63–137; 3. “Viceroyalty to Republics, 1786-1952: Historical Notes on the Evolution of Middle American Political Units,” Howard F. Cline, pp. 138–165; 4.“Ethnohistorical Regions of Middle America,” Howard F. Cline, pp. 166–182; 5.“The \"Relaciones Geográficas\" of the Spanish Indies, 1577-1648,” Howard F. Cline, pp. 183–242; 6.“The Pinturas (Maps) of the Relaciones Geográficas, with Catalogue,” Donald Robertson, pp. 243–278; 7.“The Relaciones Geográficas, 1579-1586: Native Languages,” H.R. Harvey, pp. 279–323; 8.“A Census of the Relaciones Geográficas of New Spain, 1579-1612,” Howard F. Cline, pp. 324–369; 9.“The Relaciones Geográficas of Spain, New Spain, and the Spanish Indies: An Annotated Bibliography,” Howard F. Cline, pp. 370–395; 10.“The Relaciones Geográficas of Mexico and Central America, 1740-1792,” Robert C. West, pp. 396–452.\nVolume 13. Guide to Ethnohistorical Sources, Part 2. (1973) 11, “Published Collections of Documents Relating to Middle American Ethnohistory”, Charles Gibson; 12, “An Introductory Survey of Secular Writings in the European Tradition on Colonial Middle America, 1503-1818,” J. Benedict Warren, pp. 42–137; 13. “Religious Chronicles and Historians: A Summary and Annotated Bibliography,” Ernest J. Burrus, S.J.; 14. “Bernardino de Sahagún, 1499-1590A. “Sahagún and His Works,” Nicolau d’Olwer and Howard F. Cline, 186-206; B. “Sahagún’s “Primeros Memoriales.” Tepepulco, H. B. Nicholson, pp. 207–217; C. “Sahagún’s Materials and Studies,” Howard F. Cline, pp. 218–239; 15. “Antonio de Herrera, 1549-1625,” Manuel Ballesteros Gaibrois, pp. 240–255; 16. “Juan de Torquemada, 1564-1624,” José Alcina Franch, pp. 256–275; 17. “Francisco Javier Clavigero, 1731-1787, “ Charles F. Ronan, S. J., pp. 276–297; 18. “Charles Etienne Brasseur de Bourbourg, 1814-1874,” Carroll Edward Mace, pp. 298–325; 19. “Hubert Howe Bancroft, 1832-1918,” Howard F. Cline, pp. 326–347; 20. “Eduard Georg Seler, 1849-1922,” H. B. Nicholson, pp. 348–369; 21, “Select Nineteenth-Century Mexican Writers on Ethnohistory,” Howard F. Cline, pp. 370–403. Carlos María de Bustamante, José Fernando Ramírez, Manuel Orozco y Berra, Joaquín García Icazbalceta, Alfredo Chavero, Francisco del Paso y Troncoso\n\nVolume 14. Guide to Ethnohistorical Sources Part 3. (1975) 22. “A Survey of Native Middle American Pictorial Manuscripts,” John B. Glass, pp. 3–80; 23. “A Census of Native Middle American Pictorial Manuscripts,” John B. Glass with Donald Robertson, pp. 81–252; 24. “Techialoyan Manuscripts and Paintings with a Catalog,” Donald Robertson, pp. 253–280; 25. “A Census of Middle American Testerian Manuscripts,” John B. Glass, pp. 281–296; 26. “A Catalogue of Falsified Middle American Pictorial Manuscripts,” John B. Glass, pp. 297–309; Illustrations and maps, 1-103\n\nVolume 15. Guide to Ethnohistorical Sources Part 4. (1975) 27A. “Prose Sources in the Native Historical Tradition,” Charles Gibson, pp 312–319; 27B. “A Census of Middle American Prose Manuscripts in the Native Historical Tradition,” Charles Gibson and John B. Glass, pp. 322–400; 28. “A Checklist of Institutional Holdings of Middle American Manuscripts in the Native Historical Tradition,” John B. Glass, pp. 401–472; 29. “The Boturini Collection,” John B. Glass, pp. 473–486; 30. “Middle American Ethnohistory: An Overview,” H. B. Nicholson, pp. 487–505; 31.”Index of Authors, Titles, and Synonyms,” John B. Glass, pp. 506–536; 32. “Annotated References,” John B. Glass, pp. 537–724.\nVolume 16. Handbook of Middle American Indians. Margaret A.L. Harrison, volume editor. (1976) – Bibliography for all volumes.\n\nGeneral Editor, Victoria Bricker\n\n"}
{"id": "1865442", "url": "https://en.wikipedia.org/wiki?curid=1865442", "title": "Hofstadter's law", "text": "Hofstadter's law\n\nHofstadter's law is a self-referential time-related adage, coined by Douglas Hofstadter and named after him.\n\nHofstadter's law was a part of Douglas Hofstadter's 1979 book \"Gödel, Escher, Bach: An Eternal Golden Braid\". The \"law\" is a statement regarding the difficulty of accurately estimating the time it will take to complete tasks of substantial complexity. It is often cited by programmers, especially in discussions of techniques to improve productivity, such as \"The Mythical Man-Month\" or extreme programming. The recursive nature of the law is a reflection of the widely experienced difficulty of estimating complex tasks despite all best efforts, including knowing that the task is complex.\n\nThe law was initially introduced in connection with a discussion of chess-playing computers, where top-level players were continually beating machines, even though the machines outweighed the players in recursive analysis. The intuition was that the players were able to focus on particular positions instead of following every possible line of play to its conclusion. Hofstadter wrote in 1979, \"In the early days of computer chess, people used to estimate that it would be ten years until a computer (or program) was world champion. But after ten years had passed, it seemed that the day a computer would become world champion was still more than ten years away ... This is just one more piece of evidence for the rather recursive Hofstadter's Law:\" Notably, that day did indeed come, when Deep Blue defeated Garry Kasparov in 1997, which indicates that the Law of Accelerating Returns may take effect when the task is repeated, thus counteracting -- and [in some cases] overpowering -- Hofstadter's law.\n\n"}
{"id": "161388", "url": "https://en.wikipedia.org/wiki?curid=161388", "title": "Indirect self-reference", "text": "Indirect self-reference\n\nIndirect self-reference describes an object referring to itself \"indirectly\".\n\nFor example, define the function f such that f(x) = x(x). Any function passed as an argument to f is invoked with itself as an argument, and thus in any use of that argument is indirectly referring to itself.\n\nThis example is similar to the Scheme expression \"((lambda(x)(x x)) (lambda(x)(x x)))\", which is expanded to itself by beta reduction, and so its evaluation loops indefinitely despite the lack of explicit looping constructs. An equivalent example can be formulated in lambda calculus.\n\nIndirect self-reference is special in that its self-referential quality is not explicit, as it is in the sentence \"this sentence is false.\" The phrase \"this sentence\" refers directly to the sentence as a whole. An indirectly self-referential sentence would replace the phrase \"this sentence\" with an expression that effectively still referred to the sentence, but did not use the pronoun \"this.\"\n\nAn example will help to explain this. Suppose we define the quine of a phrase to be the quotation of the phrase followed by the phrase itself. So, the quine of:\nwould be:\nwhich, incidentally, is a true statement.\n\nNow consider the sentence:\n\nThe quotation here, plus the phrase \"when quined,\" indirectly refers to the entire sentence. The importance of this fact is that the remainder of the sentence, the phrase \"makes quite a statement,\" can now make a statement about the sentence as a whole. If we had used a pronoun for this, we could have written something like \"this sentence makes quite a statement.\"\n\nIt seems silly to go through this trouble when pronouns will suffice (and when they make more sense to the casual reader), but in systems of mathematical logic, there is generally no analog of the pronoun. It is somewhat surprising, in fact, that self-reference can be achieved at all in these systems.\n\nUpon closer inspection, it can be seen that in fact, the Scheme example above uses a quine, and f(x) is actually the quine function itself.\n\nIndirect self-reference was studied in great depth by W. V. Quine (after whom the operation above is named), and occupies a central place in the proof of Gödel's incompleteness theorem. Among the paradoxical statements developed by Quine is the following:\n\n"}
{"id": "6487324", "url": "https://en.wikipedia.org/wiki?curid=6487324", "title": "Leishu", "text": "Leishu\n\nThe leishu () is a genre of reference books historically compiled in China and other countries of the Sinosphere. The term is generally translated as \"encyclopedia\", although the \"leishu\" are quite different from the modern notion of encyclopedia.\n\nThe \"leishu\" are composed of sometimes lengthy citations from other works, and often contain copies of entire works, not just excerpts. The works are classified by a systematic set of categories, which are further divided into subcategories. \"Leishu\" may be considered anthologies, but are encyclopedic in the sense that they may comprise the entire realm of knowledge at the time of compilation.\n\nApproximately 600 \"leishu\" were compiled from the early third century until the eighteenth century, of which 200 have survived. The largest \"leishu\" ever compiled was the 1408 \"Yongle Dadian\", containing 370 million Chinese characters, and the largest ever printed was the \"Gujin Tushu Jicheng\", containing 100 million characters and 852,408 pages.\n\nThe genre first appeared in the early third century. The earliest known was the \"Huanglan\" (\"Emperor's mirror\"). Sponsored by the emperor of Cao Wei, it was compiled around 220, but has since been lost. However, the term \"leishu\" was not used until the Song dynasty (960–1279).\n\nIn later imperial China dynasties, such as the Ming and Qing, emperors sponsored monumental projects to compile all known human knowledge into a single \"leishu\", in which entire works, rather than excerpts, were copied and classified by category. The largest \"leishu\" ever compiled, on the order of the Yongle Emperor of Ming, was the \"Yongle Dadian\" containing a total of 370 million Chinese characters. The project involved 2,169 scholars, who worked for four years under general editor Yao Guangxiao. It was completed in 1408, but never printed, as the imperial treasury had run out of money.\n\nThe \"Qinding Gujin Tushu Jicheng\" (Imperially approved synthesis of books and illustrations past and present) is by far the largest \"leishu\" ever printed, containing 100 million characters and 852,408 pages. It was compiled by a team of scholars led by Chen Menglei, and printed between 1726 and 1728, during the Qing dynasty.\n\nThe \"riyong leishu\" (encyclopedias for daily use), containing practical information for people who were literate but below the Confucian elite, were also compiled in the later imperial era. Today, they provide scholars with valuable information on non-elite culture and attitudes.\n\nAccording to Jean-Pierre Diény, the Jiaqing reign (1796–1820) of the Qing dynasty saw the end of the publication of \"leishu\".\n\nOther countries of the Sinosphere also adopted the genre of \"leishu\". In 1712, the \"Sancai Tuhui\", a richly illustrated \"leishu\" compiled by Ming scholar Wang Qi (王圻) in the early 17th century, was printed in Japan as \"Wakan Sansai Zue\". The Japanese version was edited by Terajima Ryōan (寺島良安), a physician born in Osaka.\n\nThe \"leishu\" have played an important role in the preservation of ancient works, many of which have been lost, only preserved completely or partially as part of a \"leishu\" compilation. The 7th-century \"Yiwen Leiju\" is especially valuable. It contains excerpts from 1,400 pre-7th century works, 90% of which have been otherwise lost. Even though the \"Yongle Dadian\" is itself largely lost, the remnants still contain 385 complete books that have been otherwise lost. The \"leishu\" also provide a unique view of the transmission of knowledge and education, and an easy way to locate traditional materials on any given subject.\n\nApproximately 600 \"leishu\" were compiled, from the Cao Wei period (early third century) until the 18th century, of which 200 have survived. Among the most important, in chronological order, are:\n\n\n"}
{"id": "4104172", "url": "https://en.wikipedia.org/wiki?curid=4104172", "title": "List of Latin abbreviations", "text": "List of Latin abbreviations\n\nThis is a list of common Latin abbreviations. Nearly all the abbreviations below have been adopted by Modern English. However, with some exceptions (for example, \"versus\" or \"modus operandi\"), most of the Latin referent words and phrases are perceived as foreign to English. In a few cases, English referents have replaced the original Latin ones (e.g., \"rest in peace\" for RIP and \"post script\" for PS).\n\nLatin was once the universal academic language in Europe. From the 18th century authors started using their mother tongues to write books, papers or proceedings. Even when Latin fell out of use, many Latin abbreviations continued to be used due to their precise simplicity and Latin's status as a learnèd language.\n\nIn July 2016, the government of the United Kingdom announced that its websites would no longer use Latin abbreviations.\n\nAll abbreviations are given with full stops, although these are omitted or included as a personal preference in most situations.\n\nWords and abbreviations that have been in general use, but are currently used less often:\n\n\n\n\n"}
{"id": "4807639", "url": "https://en.wikipedia.org/wiki?curid=4807639", "title": "Microsoft Bookshelf", "text": "Microsoft Bookshelf\n\nMicrosoft Bookshelf was a reference collection introduced in 1987 as part of Microsoft's extensive work in promoting CD-ROM technology as a distribution medium for electronic publishing. The original MS-DOS version showcased the massive storage capacity of CD-ROM technology, and was accessed while the user was using one of 13 different word processor programs that Bookshelf supported. Subsequent versions were produced for Windows and became a commercial success as part of the Microsoft Home brand. It was often bundled with personal computers as a cheaper alternative to the Encarta Suite. The Encarta Deluxe Suite / Reference Library versions also bundled Bookshelf.\n\nMicrosoft Bookshelf was discontinued in 2000. In later editions of the Encarta suite (Encarta 2000 and onwards), Bookshelf was replaced with a dedicated \"Encarta Dictionary\", a superset of the printed edition. There has been some controversy over the decision, since the dictionary lacks the other books provided in Bookshelf which many found to be a useful reference, such as the dictionary of quotations (replaced with a quotations section in \"Encarta\" that links to relevant articles and people) and the Internet Directory, although the directory is now a moot point since many of the sites listed in offline directories no longer exist.\n\nThe original 1987 edition contained \"The Original Roget's Thesaurus of English Words and Phrases\", \"The American Heritage Dictionary of the English Language\", World Almanac and Book of Facts, Bartlett's Familiar Quotations, The Chicago Manual of Style (13th Edition), the U.S. ZIP Code Directory, Houghton Mifflin Usage Alert, Houghton Mifflin Spelling Verifier and Corrector, Business Information Sources, and Forms and Letters. Titles in non-US versions of Bookshelf were different. For example, the 1997 UK edition included the Chambers Dictionary, Bloomsbury Treasury of Quotations, and Hutchinson Concise Encyclopedia.\n\nThe Windows release of Bookshelf added a number of new reference titles, including \"The Concise Columbia Encyclopedia\" and an Internet Directory. Other titles were added and some were dropped in subsequent years. By 1994, the English-language version also contained the \"Columbia Dictionary of Quotations\"; \"The Concise Columbia Encyclopedia\"; the \"Hammond Intermediate World Atlas\"; and \"The People's Chronology\". By 2000, the collection came to include the \"Encarta Desk Encyclopedia\", the \"Encarta Desk Atlas\", the \"Encarta Style Guide\" and a specialized \"Computer and Internet Dictionary\" by Microsoft Press.\n\nBookshelf 1.0 used a proprietary hypertext engine that Microsoft acquired when it bought the company Cytation in 1986. Also used for Microsoft Stat Pack and Microsoft Small Business Consultant, it was a Terminate and Stay Resident (TSR) program that ran alongside a dominant program, unbeknownst to the dominant program. Like Apple's similar Hypercard reader, Bookshelf engine's files used a single compound document, containing large numbers of subdocuments (\"cards\" or \"articles\"). They both differ from current browsers which normally treat each \"page\" or \"article\" as a separate file.\n\nThough similar to Apple's Hypercard reader in many ways, the Bookshelf engine had several key differences. Unlike Hypercard files, Bookshelf files required compilation and complex markup codes. This made the files more difficult to pirate, addressing a key concern of early electronic publishers. Furthermore, Bookshelf's engine was designed to run as fast as possible on slow first-generation CD-ROM drives, some of which required as much as a half-second to move the drive head. Such hardware constraints made Hypercard impractical for high-capacity CD-ROMs. Bookshelf also had full text searching capability, which made it easy to find needed information.\n\nCollaborating with DuPont, the Microsoft CD-ROM division developed a Windows version of its engine for applications as diverse as document management, online help, and a CD-ROM encyclopedia. In a skunkworks project, these developers worked secretly with Multimedia Division developers so that the engine would be usable for more ambitious multimedia applications. Thus they integrated a multimedia markup language, full text search, and extensibility using software objects, all of which are commonplace in modern internet browsing.\n\nIn 1992, Microsoft started selling the Bookshelf engine to third-party developers, marketing the product as Microsoft Multimedia Viewer. The idea was that such a tool would help a burgeoning growth of CD-ROM titles that would spur demand for Windows. Although the engine had multimedia capabilities that would not be matched by Web browsers until the late 1990s, Microsoft Viewer did not enjoy commercial success as a standalone product. However, Microsoft continued to use the engine for its Encarta and WinHelp applications, though the multimedia functions are rarely used in Windows help files.\n\nIn 1993, the developers who were working on the next generation viewer were moved to the Cairo systems group which was charged with delivering Bill Gates' 'vision' of 'Information at your fingertips'. This advanced browser was a fully componentized application using what are now known as Component Object Model objects, designed for hypermedia browsing across large networks and whose main competitor was thought to be Lotus Notes. Long before Netscape appeared, this team, known as the WEB (web enhanced browser) team had already shipped a network capable hypertext browser capable of doing everything that HTML browsers would not be able to do until the turn of the century. Nearly all technologies of Cairo shipped. The WEB browser was not one of them, though it influenced the design of many other common Microsoft technologies.\n\n\"BYTE\" in 1989 listed Microsoft Bookshelf as among the \"Excellence\" winners of the BYTE Awards, stating that it \"is the first substantial application of CD-ROM technology\" and \"a harbinger of personal library systems to come\".\n\n"}
{"id": "5629066", "url": "https://en.wikipedia.org/wiki?curid=5629066", "title": "Nomina sacra", "text": "Nomina sacra\n\nIn Christian scribal practice, nomina sacra (singular: nomen sacrum from Latin sacred name) is the abbreviation of several frequently occurring divine names or titles, especially in Greek manuscripts of Holy Scripture. A nomen sacrum consists of two or more letters from the original word spanned by an overline.\n\nMetzger lists 15 such expressions from Greek papyri: the Greek counterparts of \"God\", \"Lord\", \"Jesus\", \"Christ\", \"Son\", \"Spirit\", \"David\", \"Cross\", \"Mother\", \"Father\", \"Israel\", \"Savior\", \"Man\", \"Jerusalem\", and \"Heaven\". These \"nomina sacra\" are all found in Greek manuscripts of the 3rd century and earlier, except \"Mother\", which appears in the 4th.\n\n\"Nomina sacra\" also occur in some form in Latin, Coptic, Armenian (indicated by the \"pativ\"), Gothic, Old Nubian, and Cyrillic (indicated by the \"titlo\").\n\n\"Nomina sacra\" are consistently observed in even the earliest extant Christian writings, along with the codex form rather than the roll, implying that when these were written, in approximately the second century, the practice had already been established for some time. However, it is not known precisely when and how the \"nomina sacra\" first arose.\n\nThe initial system of \"nomina sacra\" apparently consisted of just four or five words, called \"nomina divina\": the Greek words for \"Jesus\", \"Christ\", \"Lord\", \"God\", and possibly \"Spirit\". The practice quickly expanded to a number of other words regarded as sacred.\n\nIn the system of \"nomina sacra\" that came to prevail, abbreviation is by \"contraction\", meaning that the first and last letter (at least) of each word are used. In a few early cases, an alternate practice is seen of abbreviation by \"suspension\", meaning that the initial two letters (at least) of the word are used; e.g., the opening verses of Revelation in write (\"Jesus Christ\") as . Contraction, however, offered the practical advantage of indicating the case of the abbreviated noun.\n\nIt is evident that the use of \"nomina sacra\" was an act of reverence rather than a purely practical space-saving device, as they were employed even where well-established abbreviations of far more frequent words such as \"and\" were avoided, and the \"nomen sacrum\" itself was written with generous spacing. Furthermore, early scribes often distinguished between mundane and sacred occurrences of the same word, e.g. a \"spirit\" vs. the \"Spirit\", and applied \"nomina sacra\" only to the latter (at times necessarily revealing an exegetical choice), although later scribes would mechanically abbreviate all occurrences.\n\nScholars have advanced a number of theories on the origin of the \"nomina sacra\". An obvious parallel that likely offered some inspiration is the Jewish practice of writing the divine name of God, commonly rendered as Jehovah or Yahweh in English, as the Hebrew tetragrammaton (transliterated as YHWH) even in Greek Scriptures. The Septuagint manuscript LXX P.Oxy.VII.1007 uses two Paleo-Hebrew \"yodh's\" with a horizontal line through them for YHWH (an abbreviated form of the Name of God translitered as ). Pavlos Vasileiadis, a Doctor of Theology at the Aristotle University of Thessaloniki, quoting Gerard Gertoux, states that \"the subsequent use of the contracted forms of the original nomina sacra κ[ύριο]ς [()] and θ[εό]ς [()] within Christian manuscripts probably reflects the Jewish practice of replacing the Tetragrammaton by י[הו]ה.\", transliterated in koine Greek as ιά.\n\nGreek culture also employed a number of ways of abbreviating even proper names, though none in quite the same form as the \"nomina sacra\". Inspiration for the contracted forms (using the first and last letter) has also been seen in Revelation, where Jesus speaks of himself as \"the beginning and the end\" and \"the first and the last\" as well \"the Alpha and the Omega\". Greek numerals have been suggested as the origin of the overline spanning the whole \"nomen sacrum\", with the suspended form being simply the ordinary way of writing \"eighteen\", for example.\n\n"}
{"id": "2673834", "url": "https://en.wikipedia.org/wiki?curid=2673834", "title": "Numeronym", "text": "Numeronym\n\nA numeronym is a number-based word.\n\nMost commonly, a numeronym is a word where a number is used to form an abbreviation (albeit not an acronym or an initialism). Pronouncing the letters and numbers may sound similar to the full word: \"K9\" for \"canine\" (phonetically: \"kay\" + \"nine\").\n\nAlternatively, the letters between the first and last are replaced with a number representing the number of letters omitted, such as \"i18n\" for \"internationalization\". Sometimes the last letter is also counted and omitted. These word shortenings are sometimes called \"alphanumeric acronyms\", \"alphanumeric abbreviations\", or \"numerical contractions\".\n\nAccording to Tex Texin, the first numeronym of this kind was \"S12n\", the electronic mail account name given to Digital Equipment Corporation (DEC) employee Jan Scherpenhuizen by a system administrator because his surname was too long to be an account name. By 1985, colleagues who found Jan's name unpronounceable often referred to him verbally as \"S12n\" (\"ess-twelve-en\"). The use of such numeronyms became part of DEC corporate culture.\n\nA number may also denote how many times the character before or after it is repeated. This is typically used to represent a name or phrase in which several consecutive words start with the same letter, as in W3 (World Wide Web) or W3C (World Wide Web Consortium).\n\nSome numeronyms are composed entirely of numbers, such as \"212\" for \"New Yorker\", \"4-1-1\" for \"information\", \"9-1-1\" for \"help\", and \"101\" for \"basic introduction to a subject\". Words of this type have existed for decades, including those in 10-code, which has been in use since before World War II.\n\nChapter or title numbers of some jurisdictions' statutes have become numeronyms, for example 5150 and 187 from California's penal code. Largely because the production of many American movies and television programs are based in California, usage of these terms has spread beyond its original location and user population.\n\nThe concept of incorporating numbers into words can also be found in Leet-speak, where numbers are frequently substituted for orthographically similar letters (e.g. \"H4CK3D\" for \"HACKED\").\n\nAnne H. Soukhanov, editor of the new \"Microsoft Encarta College Dictionary\", gives the original meaning of the term as \"a telephone number that spells a word or a name\" on a telephone dial.\n\nWhere words have multiple meanings, abbreviations such as these are almost always used to refer to their computing sense; for example, \"G11n\" for \"globalization\" refers to software preparedness for global distribution, and not the social trend of globalization. In some cases, the use of appropriate case makes it easier to distinguish between letters such as uppercase I/i and lower case L/l.\n\n"}
{"id": "10550174", "url": "https://en.wikipedia.org/wiki?curid=10550174", "title": "Observer's Books", "text": "Observer's Books\n\nThe Observer's Books were a series of small, pocket-sized books, published by Frederick Warne & Co in the United Kingdom from 1937 to 2003. They covered a variety of topics including hobbies, art, history and wildlife. The aim of these books was to interest the observer and they have also been popular amongst children. Some of them have become collector's items. For the dedicated collector this could be a lifetime's work as there are over 800 variations, some of which are now rare. The values of the books can vary from 50 pence to hundreds of pounds. \n\nThe books were produced with paper dust covers up until 1969. Each one had a unique pattern of squiggly lines at the top but these were not especially practical because they were easy to rip and stain. From 1970, the covers were protected with a glossy coating. These types are often referred to as \"Glossies\". From the late 1970s, Warne decided to laminate the covers to the actual books to make them sturdier and more resistant to wear.\n\nThe first Observer's guide was published in 1937, and was on the subject of British birds. This is now rare, and a mint copy with a dust cover is worth hundreds of pounds. The same year, Warne published a second Observer's book on British wild flowers. A mint copy of this book is worth around £220. When the popularity of these was recognized, several more titles were added 'uniform in the series', but during World War II production was limited due to paper and labour shortages. Even so, by 1941 Warne had published the first six Observer's books.\n\nIn 1942 a special edition book was brought out on \"airplanes\" . This book had no number in the series, as it was brought out to help people spot enemy warplanes. It was reprinted in 1943 and 1945. \n\nThe first few Observer's titles had focused on nature, but gradually subjects like geology, music and architecture were introduced. 'Spotter' titles like \"Aircraft\", \"Automobiles\" and \"Railway Locomotives\" proved popular. During the 1950s and 60s collecting sets of these books was popular among children and adults alike. \n\nWhen Warne was acquired by Penguin books in 1983, Warne brought out new editions of the Observer's books. These were slightly bigger than the original books, and were in paperback, not hardback. The same year Penguin, with permission of Warne, started printing their own, more up-to-date Observer's books. These again were slightly larger than the originals, but were hardbacks. Like the later original Observer's books, the dust covers were laminated to the actual book. There were two types of the Penguin Observer's books: Bloomsbury Observer's, and Claremont Observer's, (of which there were only 12 different editions).\n\nAfter a hiatus of 17 years, Peregrine Books published the appropriately titled \"Observer's Book of Observer's Books\" in 1999, in a format that matched the original editions and was numbered 99 so as to follow on from the last 'official' title. As the title implies, it is a guide to the series with details of its history, authors, and print-runs. As a sign of the series' popularity, this potentially obscure book has been reprinted no fewer than six times. More recently the series has been rounded up to 100 with the publication of \"Wayside and Woodland\" in 2003.\n"}
{"id": "177891", "url": "https://en.wikipedia.org/wiki?curid=177891", "title": "Primary source", "text": "Primary source\n\nIn the study of history as an academic discipline, a primary source (also called an original source) is an artifact, document, diary, manuscript, autobiography, recording, or any other source of information that was created at the time under study. It serves as an original source of information about the topic. Similar definitions can be used in library science, and other areas of scholarship, although different fields have somewhat different definitions. In journalism, a primary source can be a person with direct knowledge of a situation, or a document written by such a person.\n\nPrimary sources are distinguished from secondary sources, which cite, comment on, or build upon primary sources. Generally, accounts written after the fact with the benefit (and possible distortions) of hindsight are secondary. A secondary source may also be a primary source depending on how it is used. For example, a memoir would be considered a primary source in research concerning its author or about his or her friends characterized within it, but the same memoir would be a secondary source if it were used to examine the culture in which its author lived. \"Primary\" and \"secondary\" should be understood as relative terms, with sources categorized according to specific historical contexts and what is being studied.\n\nIn scholarly writing, an important objective of classifying sources is to determine their independence and reliability. In contexts such as historical writing, it is almost always advisable to use primary sources and that \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" Sreedharan believes that primary sources have the most direct connection to the past and that they \"speak for themselves\" in ways that cannot be captured through the filter of secondary sources.\n\nIn scholarly writing, the objective of classifying sources is to determine the independence and reliability of sources. Though the terms \"primary source\" and \"secondary source\" originated in historiography as a way to trace the history of historical ideas, they have been applied to many other fields. For example, these ideas may be used to trace the history of scientific theories, literary elements and other information that is passed from one author to another.\n\nIn scientific literature, a primary source is the original publication of a scientist's new data, results and theories. In political history, primary sources are documents such as official reports, speeches, pamphlets, posters, or letters by participants, official election returns and eyewitness accounts. In the history of ideas or intellectual history, the main primary sources are books, essays and letters written by intellectuals; these intellectuals may include historians, whose books and essays are therefore considered primary sources for the intellectual historian, though they are secondary sources in their own topical fields. In religious history, the primary sources are religious texts and descriptions of religious ceremonies and rituals.\n\nA study of cultural history could include fictional sources such as novels or plays. In a broader sense primary sources also include artifacts like photographs, newsreels, coins, paintings or buildings created at the time. Historians may also take archaeological artifacts and oral reports and interviews into consideration. Written sources may be divided into three types.\n\n\nIn historiography, when the study of history is subject to historical scrutiny, a secondary source becomes a primary source. For a biography of a historian, that historian's publications would be primary sources. Documentary films can be considered a secondary source or primary source, depending on how much the filmmaker modifies the original sources.\n\nThe Lafayette College Library, provides a synopsis of primary sources in several areas of study:\n\"The definition of a primary source varies depending upon the academic discipline and the context in which it is used.<br>\n\nAlthough many primary sources remain in private hands, others are located in archives, libraries, museums, historical societies, and special collections. These can be public or private. Some are affiliated with universities and colleges, while others are government entities. Materials relating to one area might be spread over a large number of different institutions. These can be distant from the original source of the document. For example, the Huntington Library in California houses a large number of documents from the United Kingdom.\n\nIn the US, digital copies of primary sources can be retrieved from a number of places. The Library of Congress maintains several digital collections where they can be retrieved. Some examples are American Memory and Chronicling America. The National Archives and Records Administration also has digital collections in Digital Vaults. The Digital Public Library of America searches across the digitized primary source collections of many libraries, archives, and museums. The Internet Archive also has primary source materials in many formats.\n\nIn the UK, the National Archives provides a consolidated search of its own catalogue and a wide variety of other archives listed on the Access to Archives index. Digital copies of various classes of documents at the National Archives (including wills) are available from DocumentsOnline. Most of the available documents relate to England and Wales. Some digital copies of primary sources are available from the National Archives of Scotland. Many County Record Offices collections are included in Access to Archives, while others have their own on-line catalogues. Many County Record Offices will supply digital copies of documents.\n\nIn other regions, Europeana has digitized materials from across Europe while the World Digital Library and Flickr Commons have items from all over the world. Trove has primary sources from Australia.\n\nMost primary source materials are not digitized and may only be represented online with a record or finding aid. Both digitized and not digitized materials can be found through catalogs such as WorldCat, the Library of Congress catalog, the National Archives catalog, and so on.\n\nHistory as an academic discipline is based on primary sources, as evaluated by the community of scholars, who report their findings in books, articles and papers. Arthur Marwick says \"Primary sources are absolutely fundamental to history.\" Ideally, a historian will use all available primary sources that were created by the people involved at the time being studied. In practice some sources have been destroyed, while others are not available for research. Perhaps the only eyewitness reports of an event may be memoirs, autobiographies, or oral interviews taken years later. Sometimes the only evidence relating to an event or person in the distant past was written or copied decades or centuries later. Manuscripts that are sources for classical texts can be copies of documents, or fragments of copies of documents. This is a common problem in classical studies, where sometimes only a summary of a book or letter has survived. Potential difficulties with primary sources have the result that history is usually taught in schools using secondary sources.\n\nHistorians studying the modern period with the intention of publishing an academic article prefer to go back to available primary sources and to seek new (in other words, forgotten or lost) ones. Primary sources, whether accurate or not, offer new input into historical questions and most modern history revolves around heavy use of archives and special collections for the purpose of finding useful primary sources. A work on history is not likely to be taken seriously as scholarship if it only cites secondary sources, as it does not indicate that original research has been done.\n\nHowever, primary sources – particularly those from before the 20th century – may have hidden challenges. \"Primary sources, in fact, are usually fragmentary, ambiguous and very difficult to analyse and interpret.\" Obsolete meanings of familiar words and social context are among the traps that await the newcomer to historical studies. For this reason, the interpretation of primary texts is typically taught as part of an advanced college or postgraduate history course, although advanced self-study or informal training is also possible.\n\nThe following questions are asked about primary sources:\n\nIn many fields and contexts, such as historical writing, it is almost always advisable to use primary sources if possible, and \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" In addition, primary sources avoid the problem inherent in secondary sources in which each new author may distort and put a new spin on the findings of prior cited authors.\n\nHowever, a primary source is not necessarily more of an authority or better than a secondary source. There can be bias and tacit unconscious views which twist historical information.\n\nParticipants and eyewitnesses may misunderstand events or distort their reports, deliberately or not, to enhance their own image or importance. Such effects can increase over time, as people create a narrative that may not be accurate. For any source, primary or secondary, it is important for the researcher to evaluate the amount and direction of bias. As an example, a government report may be an accurate and unbiased description of events, but it may be censored or altered for propaganda or cover-up purposes. The facts can be distorted to present the opposing sides in a negative light. Barristers are taught that evidence in a court case may be truthful but may still be distorted to support or oppose the position of one of the parties.\n\nMany sources can be considered either primary or secondary, depending on the context in which they are examined. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. A book review, when it contains the opinion of the reviewer about the book rather than a summary of the book, becomes a primary source.\n\nIf a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion. Examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source; but if the letter is later found, it may then be considered \"secondary\"\n\nIn some instances, the reason for identifying a text as the \"primary source\" may devolve from the fact that no copy of the original source material exists, or that it is the oldest extant source for the information cited.\n\nHistorians must occasionally contend with forged documents that purport to be primary sources. These forgeries have usually been constructed with a fraudulent purpose, such as promulgating legal rights, supporting false pedigrees, or promoting particular interpretations of historic events. The investigation of documents to determine their authenticity is called diplomatics.\n\nFor centuries, Popes used the forged Donation of Constantine to bolster the Papacy's secular power. Among the earliest forgeries are false Anglo-Saxon charters, a number of 11th- and 12th-century forgeries produced by monasteries and abbeys to support a claim to land where the original document had been lost or never existed. One particularly unusual forgery of a primary source was perpetrated by Sir Edward Dering, who placed false monumental brasses in a parish church. In 1986, Hugh Trevor-Roper \"authenticated\" the Hitler Diaries, which were later proved to be forgeries. Recently, forged documents have been placed within the UK National Archives in the hope of establishing a false provenance. However, historians dealing with recent centuries rarely encounter forgeries of any importance.\n\n</div>\n\n\n\n"}
{"id": "1130951", "url": "https://en.wikipedia.org/wiki?curid=1130951", "title": "Scribal abbreviation", "text": "Scribal abbreviation\n\nScribal abbreviations or sigla (singular: siglum) are the abbreviations used by ancient and medieval scribes writing in Latin, and later in Greek and Old Norse. In modern manuscript editing (substantive and mechanical) \"sigla\" are the symbols used to indicate the source manuscript (e.g. variations in text between different such manuscripts) and to identify the copyist(s) of a work. See Critical apparatus.\n\nAbbreviated writing, using \"sigla\", arose partly from the limitations of the workable nature of the materials (stone, metal, parchment, etc.) employed in record-making and partly from their availability. Thus, lapidaries, engravers, and copyists made the most of the available writing space. Scribal abbreviations were infrequent when writing materials were plentiful, but by the 3rd and 4th centuries AD, writing materials were scarce and costly.\n\nDuring the Roman Republic, several abbreviations, known as \"sigla\" (plural of \"siglum\" = symbol or abbreviation), were in common use in inscriptions, and they increased in number during the Roman Empire. Additionally, in this period shorthand entered general usage. The earliest known Western shorthand system was that employed by the Greek historian Xenophon in the memoir of Socrates, and it was called \"notae socratae\". In the late Roman Republic, the Tironian notes were developed possibly by Marcus Tullius Tiro, Cicero's amanuensis, in 63 BC to record information with fewer symbols; Tironian notes include a shorthand/syllabic alphabet notation different from the Latin minuscule hand and square and rustic capital letters. The notation was akin to modern stenographic writing systems. It used symbols for whole words or word roots and grammatical modifier marks, and it could be used to write either whole passages in shorthand or only certain words. In medieval times, the symbols to represent words were widely used; and the initial symbols, as few as 140 according to some sources, were increased to 14,000 by the Carolingians, who used them in conjunction with other abbreviations. However, the alphabet notation had a \"murky existence\" (C. Burnett), as it was often associated with witchcraft and magic, and it was eventually forgotten. Interest in it was rekindled by the Archbishop of Canterbury Thomas Becket in the 12th century and later in the 15th century, when it was rediscovered by Johannes Trithemius, abbot of the Benedictine abbey of Sponheim, in a psalm written entirely in Tironian shorthand and a Ciceronian lexicon, which was discovered in a Benedictine monastery (\"notae benenses\").\n\nTo learn the Tironian note system, scribes required formal schooling in some 4,000 symbols; this later increased to some 5,000 symbols and then to some 13,000 in the medieval period (4th to 15th centuries AD); the meanings of some characters remain uncertain. \"Sigla\" were mostly used in lapidary inscriptions; in some places and historical periods (such as medieval Spain) scribal abbreviations were overused to the extent that some are indecipherable.\n\nThe abbreviations were not constant but changed from region to region. Scribal abbreviations increased in usage and reached their height in the Carolingian Renaissance (8th to 10th centuries). The most common abbreviations, called \"notae communes\", were used across most of Europe, but others appeared in certain regions. In legal documents, legal abbreviations, called \"notae juris\", appear but also capricious abbreviations, which scribes manufactured \"ad hoc\" to avoid repeating names and places in a given document.\n\nScribal abbreviations can be found in epigraphy, sacred and legal manuscripts, written in Latin or in a vernacular tongue (but less frequently and with fewer abbreviations), either calligraphically or not.\n\nIn epigraphy, common abbreviations were comprehended in two observed classes:\n\nBoth forms of abbreviation are called \"suspensions\" (as the scribe suspends the writing of the word). A separate form of abbreviation is by \"contraction\" and was mostly a Christian usage for sacred words, Nomina Sacra; non-Christian sigla usage usually limited the number of letters the abbreviation comprised and omitted no intermediate letter. One practice was rendering an overused, formulaic phrase only as a siglum: DM for \"Dis Manibus\" (\"Dedicated to the Manes\"); IHS from the first three letters of \"ΙΗΣΟΥΣ\"; and RIP for \"requiescat in pace\" (\"Rest in Peace\") because the long-form written usage of the abbreviated phrase, by itself, was rare. According to Trabe, these abbreviations are not really meant to lighten the burden of the scribe but rather to shroud in reverent obscurity the holiest words of the Christian religion.\n\nAnother practice was repeating the abbreviation's final consonant a given number of times to indicate a group of as many persons: AVG denoted \"Augustus\", thus, AVGG denoted \"Augusti duo\"; however, lapidaries took typographic liberties with that rule, and instead of using COSS to denote \"Consulibus duobus\", they invented the CCSS form. Still, when occasion required referring to three or four persons, the complex doubling of the final consonant yielded to the simple plural siglum. To that effect, a \"vinculum\" (overbar) above a letter or a letter-set also was so used, becoming a universal medieval typographic usage. Likewise the \"tilde\" (~), an undulated, curved-end line, came into standard late-medieval usage.\n\nBesides the \"tilde\" and macron marks, above and below letters, modifying cross-bars and extended strokes were employed as scribal abbreviation marks, mostly for prefixes and verb, noun and adjective suffixes. The \"typographic\" abbreviations should not be confused with the \"phrasal\" abbreviations: i.e. (\"id est\" — \"that is\"); loc. cit. (\"loco citato\" — \"in the passage already cited\"); viz. (\"vide licet\" — \"namely\", \"that is to say\", \"in other words\" — formed with \"vi\" and the \"yogh\"-like glyph [Ꝫ], [ꝫ], the siglum for the suffix -et and the conjunction et) and et cetera.\n\nMoreover, besides scribal abbreviations, ancient texts also contained variant typographic characters, including ligatures (e.g. Æ, Œ, etc.), the long s (ſ), and the half r, resembling an Arabic numeral two (\"2\"). The \"u\" and \"v\" characters originated as scribal variants for their respective letters, likewise the \"i\" and \"j\" pair. Modern publishers printing Latin-language works replace variant typography and sigla with full-form Latin spellings; the convention of using \"u\" and \"i\" for vowels and \"v\" and \"j\" for consonants is a late typographic development.\n\nSome ancient and medieval sigla are still used in English and other European languages; the Latin ampersand (&) replaces the conjunction \"and\" in English, \"et\" in Latin and French, and \"y\" in Spanish (but its use in Spanish is frowned upon, since the \"y\" is already smaller and easier to write). The Tironian sign ⁊, resembling the digit seven (\"7\"), represents the conjunction \"et\" and is written only to the x-height; in current Irish language usage, the siglum denotes the conjunction \"agus\" (\"and\"). Other scribal abbreviations in modern typographic use are the percentage sign (%), from the Italian \"per cento\" (\"per hundred\"); the permille sign (‰), from the Italian \"per mille\" (\"per thousand\"); the pound sign (₤, £ and #, all descending from ℔ or lb, \"librum\") and the dollar sign ($), which possibly derives from the Spanish word \"Peso\". The commercial at symbol (@), originally denoting \"at the rate/price of\", is a ligature derived from the English preposition \"at\"; from the 1990s, its use outside commerce became widespread, as part of e-mail addresses.\n\nTypographically, the ampersand (\"&\"), representing the word \"et\", is a space-saving ligature of the letters \"e\" and \"t\", its component graphemes. Since the establishment of movable-type printing in the 15th century, founders have created many such ligatures for each set of record type (font) to communicate much information with fewer symbols. Moreover, during the Renaissance (14th to 17th centuries), when Ancient Greek language manuscripts introduced that tongue to Western Europe, its scribal abbreviations were converted to ligatures in imitation of the Latin scribal writing to which readers were accustomed. Later, in the 16th century, when the culture of publishing included Europe's vernacular languages, Graeco-Roman scribal abbreviations disappeared, an ideologic deletion ascribed to the anti-Latinist Protestant Reformation (1517–1648).\n\nThe common abbreviation \"Xmas,\" for Christmas, is a remnant of an old scribal abbreviation that substituted the Greek letter chi (Χ, resembling Latin X and representing the first letter in the Greek word for Christ, Χριστος) for the word Christ.\n\nAfter the invention of printing, manuscript copying abbreviations continued to be employed in Church Slavonic and are still in use in printed books as well as on icons and inscriptions. Many common long roots and nouns describing sacred persons are abbreviated and written under the special diacritic symbol titlo, as shown in the figure at the right. That corresponds to the Nomina sacra (Latin: \"Sacred names\") tradition of using contractions for certain frequently-occurring names in Greek ecclesiastical texts. However, sigla for personal nouns are restricted to \"good\" beings and the same words, when referring to \"bad\" beings, are spelled out; for example, while \"God\" in the sense of the one true God is abbreviated as \"\", \"god\" referring to \"false\" gods is spelled out. Likewise, the word for \"angel\" is generally abbreviated as \"\", but the word for \"angels\" is spelled out for \"performed by evil angels\" in Psalm 77.\n\nAdriano Cappelli's \"Lexicon Abbreviaturarum\", enumerates the various medieval brachigraphic signs found in Latin and Italian vulgar texts, which originate from the Roman sigla, a symbol to express a word, and Tironian notes. Quite rarely, abbreviations did not carry marks to indicate that an abbreviation has occurred: if they did, they were often copying errors. For example, \"e.g.\" is written with periods, but modern terms, such as \"PC\", may be written in uppercase.\n\nIt should be noted that the original manuscripts were not written in a modern sans-serif or serif font but in Roman capitals, rustic, uncial, insular, Carolingian or blackletter styles. For more, refer to Western calligraphy or a beginner's guide.\n\nAdditionally, the abbreviations employed varied across Europe. In Nordic texts, for instance, two runes were used in text written in the Latin alphabet, which are ᚠ for \"fé\" \"cattle, goods\" and ᛘ for \"maðr\" \"man\".\n\nCappelli divides abbreviations into six overlapping categories:\n\nSuspended terms are those of which only the first part is written, and the last part is substituted by a mark, which can be of two types:\n\nThe largest class of suspensions consists of single letters standing in for words that begin with that letter.\n\nA dot at the baseline after a capital letter may stand for a title if it is used such as in front of names or a person's name in medieval legal documents. However, not all sigla use the beginning of the word.\nFor plural words, the siglum is often doubled: \"F.\" = \"frater\" and \"FF.\" = \"fratres\". Tripled sigla often stand for three: \"DDD\" = \"domini tres\".\n\nLetters lying on their sides, or mirrored (backwards), often indicate female titles, but a mirrored C, Ↄ, stands generally for \"con\" or \"contra\" (the latter sometimes with a macron above, \"Ↄ̄\").\n\nTo avoid confusion with abbreviations and numerals, the latter are often written with a bar above. In some contexts, however, numbers with a line above indicate that number is to be multiplied by a thousand, and several other abbreviations also have a line above them, such as \"ΧΡ\" (Greek letters chi+rho) = \"Christus\" or \"IHS\" = \"Jesus\".\n\nStarting in the 8th or the 9th century, single letter sigla grew less common and were replaced by longer, less-ambiguous sigla, with bars above them.\n\nAbbreviations by contraction have one or more middle letters omitted. They were often represented with a general mark of abbreviation (above), such as a line above. They can be divided into two subtypes:\n\nSuch marks inform the reader of the identity of the missing part of the word without affecting (\"independent\" of) the meaning. Some of them may be interpreted as alternative contextual glyphs of their respective letters.\n\nThe meaning of the marks depends on the letter on which they appear.\n\nA superscript letter generally referred to the letter omitted, but, in some instances, as in the case of vowel letters, it could refer to a missing vowel combined with the letter \"r\", before or after it. It is only in some English dialects that the letter \"r\" before another consonant largely silent and the preceding vowel is \"r-coloured\".\n\nHowever, \"a\", \"i\", and \"o\" above \"g\" meant \"gͣ\" \"gna\", \"gͥ\" \"gni\" and \"gͦ\" \"gno\" respectively. Although in English, the \"g\" is silent in \"gn\", but in other languages, it is pronounced. Vowel letters above \"q\" meant \"qu\" + vowel: \"qͣ\", \"qͤ\", \"qͥ\", \"qͦ\", \"qͧ\".\n\n\nVowels were the most common superscripts, but consonants could be placed above letters without ascenders; the most common were \"c\", e.g. \"nͨ\". A cut \"l\" above an \"n\", \"nᷝ\", meant \"nihil\" for instance.\n\nThese marks are nonalphabetic letters carrying a particular meaning. Several of them continue in modern usage, as in the case of monetary symbols. In Unicode, they are referred to as \"letter-like glyphs\". Additionally, several authors are of the view that the Roman numerals themselves were, for example, nothing less than abbreviations of the words for those numbers. Other examples of symbols still in some use are alchemical and zodiac symbols, which were, in any case, employed only in alchemy and astrology texts, which made their appearance beyond that special context rare.\n\nIn addition to the signs used to signify abbreviations, medieval manuscripts feature some glyphs that are now uncommon but were not sigla.\nMany more ligatures were used to reduce the space occupied, a characteristic that is particularly prominent in blackletter scripts.\nSome such as r rotunda, long s and uncial or insular variants (Insular G), Claudian letters were in common use, as well as letters derived from other scripts such as Nordic runes: thorn (þ=th) and eth (ð=dh).\nAn illuminated manuscript would feature miniatures, decorated initials or \"littera notabilior\", which later resulted in the bicamerality of the script (case distinction).\n\nVarious typefaces have been designed to allow scribal abbreviations and other archaic glyphs to be replicated in print. They include \"record type\", which was first developed in the 1770s to publish Domesday Book and was fairly widely used for the publication of medieval records in Britain until the end of the 19th century.\n\nIn the Unicode Standard v. 5.1 (4 April 2008), 152 medieval and classical glyphs were given specific locations outside of the Private Use Area. Specifically, they are located in the charts \"Combining Diacritical Marks Supplement\" (26 characters), \"Latin Extended Additional\" (10 characters), \"Supplemental Punctuation\" (15 characters), \"Ancient Symbols\" (12 characters) and especially \"Latin Extended-D\" (89 characters).\nThese consist in both precomposed characters and modifiers for other characters, called combining diacritical marks (such as writing in LaTeX or using overstrike in MS Word).\n\nCharacters are \"the smallest components of written language that have semantic value\" but glyphs are \"the shapes that characters can have when they are rendered or displayed\".\n\n\n\n"}
{"id": "49632241", "url": "https://en.wikipedia.org/wiki?curid=49632241", "title": "Self-perpetuation", "text": "Self-perpetuation\n\nSelf-perpetuation, the capability of something to cause itself to continue to exist, is one of the main characteristics of life. Organisms' capability of reproduction leads to self-perpetuation of the species, if not to the individual. Populations self-perpetuate and grow. Entire ecosystems show homeostasis, and thus perpetuate themselves. The slow modifying effect of succession and similar shifts in the composition of the system can, however, not be neglected in the long run. Overall, life's object's capabilities of self-perpetuation are always accompanied by evolution, a perfect steady state of the biological system is never reached. Sexual reproduction is also a form of imperfect self-replication and thus imperfect self-perpetuation because of recombination and mutation. Organisms are not like self-replicating machine but amass random modifications from generation to generation. The property of self-perpetuation in the strict sense thus only applies to life itself.\n\nIn a social context, self-perpetuation is tied to reflexivity and (usually) positive feedback loops:\nDepending on the time scope or the context, self-perpetuation either depends on self-sustainability, or is equivalent to it. While we may talk about the self-sustainability of an ecosystem, this depends amongst other factor on the self-perpetuation of its constituting species.\n\nIn computer science, self-reproducing programs constitute an incomplete metaphor for self-perpetuation. A better analogue can be seen in computer viruses which are actually able to self-reproduce - given a suitable computing environment.\n\n"}
{"id": "16122539", "url": "https://en.wikipedia.org/wiki?curid=16122539", "title": "Self-reference puzzle", "text": "Self-reference puzzle\n\nA self-reference puzzle is a type of logical puzzle where the question in the puzzle refers to the attributes of the puzzle itself.\nA common example is that a \"fill in the blanks\" style sentence is given, but what is filled in the blanks can contribute to the sentence itself. An example is \"There are _____ e's in this sentence.\", for which a solution is \"eight\" (since including the \"eight\", there are 8 e's in the sentence).\n\n"}
{"id": "858507", "url": "https://en.wikipedia.org/wiki?curid=858507", "title": "Self-referential humor", "text": "Self-referential humor\n\nSelf-referential humor, also known as self-reflexive humor or meta humor, is a type of comedic expression that—either directed toward some other subject, or openly directed toward itself—intentionally alludes to the very person who is expressing the humor in a comedic fashion, or to some specific aspect of that same comedic expression. Self-referential humor expressed discreetly and surrealistically is a form of bathos. In general, self-referential humor often uses hypocrisy, oxymoron, or paradox to create a contradictory or otherwise absurd situation that is humorous to the audience. \n\nSelf-referential humor is sometimes combined with breaking the fourth wall to explicitly make the reference directly to the audience, or make self-reference to an element of the medium that the characters should not be aware of.\n\nOld Comedy of Classical Athens is held to be the first—in the extant sources—form of self-referential comedy. Aristophanes, whose plays form the only remaining fragments of Old Comedy, used fantastical plots, grotesque and inhuman masks and status reversals of characters to slander prominent politicians and court his audience's approval.\n\nRAS syndrome refers to the redundant use of one or more of the words that make up an acronym or initialism with the abbreviation itself, thus in effect repeating one or more words. However, \"RAS\" stands for Redundant Acronym Syndrome; therefore, the full phrase yields \"Redundant Acronym Syndrome syndrome\" and is self-referencing in a comical manner. It also reflects an excessive use of TLAs (Three Letter Acronyms).\n\nMeta has come to be used, particularly in art, to refer to something that is self-referential. Popularised by Douglas Hofstadter who wrote several books on himself and the subject of self-reference, meta-jokes are a popular form of humor.\n\n"}
{"id": "2636884", "url": "https://en.wikipedia.org/wiki?curid=2636884", "title": "Source criticism", "text": "Source criticism\n\nSource criticism (or information evaluation) is the process of evaluating an information source, i.e. a document, a person, a speech, a fingerprint, a photo, an observation, or anything used in order to obtain knowledge. In relation to a given purpose, a given information source may be more or less valid, reliable or relevant. Broadly, \"source criticism\" is the interdisciplinary study of how information sources are evaluated for given tasks.\n\nProblems in translation: The Danish word \"kildekritik\" like the Norwegian word \"kildekritikk\" and the Swedish word \"källkritik\" derived from the German \"Quellenkritik\" and is closely associated with the German historian Leopold von Ranke (1795–1886). Hardtwig writes: \"His [Ranke's] first work \"Geschichte der romanischen und germanischen Völker\" von 1494–1514 (History of the Latin and Teutonic Nations from 1494 to 1514) (1824) was a great success. It already showed some of the basic characteristics of his conception of Europe, and was of historiographical importance particularly because Ranke made an exemplary critical analysis of his sources in a separate volume, \"Zur Kritik neuerer Geschichtsschreiber\" (On the Critical Methods of Recent Historians). In this work he raised the method of textual criticism used in the late eighteenth century, particularly in classical philology to the standard method of scientific historical writing\" (Hardtwig, 2001, p. 12739).\nThe larger part of the nineteenth and twentieth\ncenturies would be dominated by the research-oriented\nconception of historical method of the so-called\nHistorical School in Germany, led by historians as\nLeopold Ranke and Berthold Niebuhr. Their conception\nof history, long been regarded as the beginning\nof modern, 'scientific' history, harked back to the\n'narrow' conception of historical method, limiting the\nmethodical character of history to source criticism\" (Lorenz, 2001).\nBible studies dominate the use of \"source criticism\" in America (cf. Hjørland, 2008). The term is thus relatively seldom used in English about historical methods and historiography (cf. Hjørland, 2008). This difference between European and American use of \"source criticism\" is somewhat strange considering the influence of Ranke on both sides of the Atlantic Ocean. It has been suggested that differences in the use of the term are not accidental but due to different views of the historical method. In the German/Scandinavian tradition this subject is seen as important, whereas in the Anglo-American tradition it is believed that historical methods must be specific and associated with the subject studied, for which reason there is no general field of \"source criticism\".\n\nIn the Scandinavian countries and elsewhere source evaluation (or information evaluation) is also studied interdisciplinarily from many different points of view, partly caused by the influence of the Internet. It is a growing field in, among other fields, library and information science. In this context source criticism is studied from a broader perspective than just, for example, history or biblical studies.\n\nThe following principles are cited from two Scandinavian textbooks on source criticism, Olden-Jørgensen (1998) and Thurén (1997) written by historians:\n\nWe may add the following principles:\n\n\"Because each source teaches you more and more about your subject, you will be able to judge with ever-increasing precision the usefulness and value of any prospective source. In other words, the more you know about the subject, the more precisely you can identify what you must still find out\". (Bazerman, 1995, p. 304).\n\"The empirical case study showed that most people find it difficult to assess questions of cognitive authority and media credibility in a general sense, for example, by comparing the overall credibility of newspapers and the Internet. Thus these assessments tend to be situationally sensitive. Newspapers, television and the Internet were frequently used as sources of orienting information, but their credibility varied depending on the actual topic at hand\" (Savolainen, 2007).\nThe following questions are often good ones to ask about any source according to the American Library Association (1994) and Engeldinger (1988):\n\n\nFor literary sources we might add complementing criteria:\n\n\nHow general are principles of source criticism?\nSome principles are universal, other principles are specific for certain kinds of information sources. One may ask whether principles of source criticism are unique to the humanities?\n\nThere is today no consensus about the similarities and differences between natural science and humanities. Logical positivism claimed that all fields of knowledge were based on the same principles. Much of the criticism of logical positivism claimed that positivism is the basis of the sciences, whereas hermeneutics is the basis of the humanities. This was, for example, the position of Jürgen Habermas. A newer position, in accordance with, among others, Hans-Georg Gadamer and Thomas Kuhn understands both science and humanities as determined by researchers' preunderstanding and paradigms. Hermeneutics is thus a universal theory. The difference is, however, that the sources of the humanities are themselves products of human interests and preunderstanding, whereas the sources of the natural sciences are not. Humanities are thus \"doubly hermeneutic\".\n\nNatural scientists, however, are also using human products (such as scientific papers) which are products of preunderstanding (and, for example, academic fraud).\n\nEpistemological theories are the basic theories about how knowledge is obtained and thus the most general theories about how to evaluate information sources. Empiricism evaluates sources by considering the observations (or sensations) on which they are based. Sources without basis in experience are not seen as valid. Rationalism provides low priority to sources based on observations. In order to be meaningful observations must be grasped by clear ideas or concepts. It is the logical structure and the well definedness that is in focus in evaluating information sources from the rationalist point of view. Historicism evaluates information sources on the basis of their reflection of their sociocultural context and their theoretical development. Pragmatism evaluate sources on the basis of how their values and usefulness to accomplish certain outcomes. Pragmatism is skeptical about claimed neutral information sources.\n\nThe evaluation of knowledge or information sources cannot be more certain than is the construction of knowledge. If we accept the principle of fallibilism we also have to accept that source criticism can never 100% verify knowledge claims. As discussed in the next section is source criticism intimately linked to scientific methods.\n\nThe presence of fallacies of argument in sources is another kind of philosophical criteria for evaluating sources. Fallacies are presented by Walton (1998). Among the fallacies are the 'ad hominem fallacy' (the use of personal attack to try to undermine or refute a person's argument) and the 'straw man fallacy' (when one arguer misrepresents another's position to make it appear less plausible than it really is, in order more easily to criticize or refute it.) See also fallacy.\n\nResearch methods are methods used to produce scholarly knowledge. The methods that are relevant for producing knowledge are also relevant for evaluating knowledge. An example of a book that turns methodology upside-down and uses it to evaluate produced knowledge is Katzer; Cook & Crouch (1998). See also Unobtrusive measures, Triangulation (social science).\n\nStudies of quality evaluation processes such as peer review, book reviews and of the normative criteria used in evaluation of scientific and scholarly research. Another field is the study of scientific misconduct.\n\nHarris (1979) provides a case study of how a famous experiment in psychology, Little Albert, has been distorted throughout the history of psychology, starting with the author (Watson) himself, general textbook authors, behavior therapists, and a prominent learning theorist. Harris proposes possible causes for these distortions and analyzes the Albert study as an example of myth making in the history of psychology. Studies of this kind may be regarded a special kind of reception history (how Watson's paper was received). It may also be regarded as a kind of critical history (opposed to ceremonial history of psychology, cf. Harris, 1980). Such studies are important for source criticism in revealing the bias introduced by referring to classical studies.\n\nSee also Hjørland (2008): Empirical studies of the quality of science.\n\nTextual criticism (or broader: text philology) is a part of philology, which is not just devoted to the study of texts, but also to edit and produce \"scientific editions\", \"scholarly editions\", \"standard editions\", \"historical editions\", \"reliable editions\", \"reliable texts\", \"text editions\" or \"critical editions\", which are editions in which careful scholarship has been employed to ensure that the information contained within is as close to the author's/composer's original intentions as possible (and which allows the user to compare and judge changes in editions published under influence by the author/composer). The relation between these kinds of works and the concept \"source criticism\" is evident in Danish, where they may be termed \"kildekritisk udgave\" (directly translated \"source critical edition\").\n\nIn other words, it is assumed that most editions of a given works is filled with noise and errors provided by publishers, why it is important to produce \"scholarly editions\". The work provided by text philology is an important part of source criticism in the humanities.\n\n\ncomplete works and monumental editions\n\nThe study of eyewitness testimony is an important field of study used, among other purposes, to evaluate testimony in courts. The basics of eyewitness fallibility includes factors such as poor viewing conditions, brief exposure, and stress. More subtle factors, such as expectations, biases, and personal stereotypes can intervene to create erroneous reports. Loftus (1996) discuss all such factors and also shows that eyewitness memory is chronically inaccurate in surprising ways. An ingenious series of experiments reveals that memory can be radically altered by the way an eyewitness is questioned after the fact. New memories can be implanted and old ones unconsciously altered under interrogation.\n\nAnderson (1978) and Anderson & Pichert (1977) reported an elegant experiment demonstrating how change in perspective affected people's ability to recall information that was unrecallable from another perspective.\n\nIn psychoanalysis the concept of defence mechanism is important and may be considered a contribution to the theory of source criticism because it explains psychological mechanisms, which distort the reliability of human information sources.\n\nIn schools of library and information science (LIS) is source criticism of taught as part of the growing field Information literacy.\nStudy issues like relevance, quality indicators for documents, kinds of documents and their qualities (e.g. scholarly editions) and related issues are studied in LIS and are relevant for source criticism. Bibliometrics is often used to find the most influential journal, authors, countries and institutions. The study of book reviews and their function in evaluating books should also be mentioned. The well-known comparison of Wikipedia and Encyclopædia Britannica (Giles, 2005) - although not done by information scientists - contained an interview with an information scientist (Michael Twidale) and should be obvious to include in LIS.\n\nIt could be argued that library and information education should provide teaching in source criticism at least at the same level as is taught in Upper Secondary School (see Gudmundsson, 2007).\n\nIn library and information science the checklist approach has often been used. A criticism of this approach is given by Meola (2004): \"Chucking the checklist\".\n\nLibraries sometimes provide advice on how their users may evaluate sources.\n\nThe Library of Congress has a \"Teaching with Primary Sources\" (TPS) program.\n\nSource criticism is also about ethical behavior and culture. It is about a free press and an open society, including the protecting information sources from being persecuted (cf., Whistleblower).\n\nPhotos are often manipulated during wars and for political purposes. One well known example is Joseph Stalin's manipulation of a photograph from May 5, 1920 on which Stalin's predecessor Lenin held a speech for Soviet troops that Leon Trotsky attended. Stalin had later Trotsky retouched out of this photograph. (cf. King, 1997). A recent example is reported by Healy (2008) about North Korean leader Kim Jong Il.\n\nMuch interest in evaluating Internet sources (such as Wikipedia) is reflected in the scholarly literature of Library and information science and in other fields. Mintz (2002) is an edited volume about this issue. Examples of literature examining Internet sources include Chesney (2006), Fritch & Cromwell (2001), Leth & Thurén (2000) and Wilkinson, Bennett, & Oliver (1997).\n\n\"In history, the term historical method was first introduced in a systematic way in the sixteenth century by Jean Bodin in his treatise of source criticism, \"Methodus ad facilem historiarium cognitionem\" (1566). Characteristically, Bodin's treatise intended to establish the ways by which reliable knowledge of the past could be established by checking sources against one another and by so assessing the reliability of the information conveyed by them, relating them to the interests involved.\" (Lorenz, 2001, p. 6870).\n\nAs written above, modern source criticism in history is closely associated with the German historian Leopold von Ranke (1795–1886), who influenced historical methods on both sides of the Atlantic Ocean, although in rather different ways. American history developed in a more empirist and antiphilosophical way (cf., Novick, 1988).\n\nTwo of the best-known rule books from History's childhood are Bernheim (1889) and Langlois & Seignobos (1898). These books provided a seven-step procedure (here quoted from Howell & Prevenier, 2001, p. 70-71):\n\nGudmundsson (2007, p. 38) writes: \"Source criticism should not totally dominate later courses. Other important perspectives, for example, philosophy of history/view of history, should not suffer by being neglected\" (Translated by BH). This quote makes a distinction between source criticism on the one hand and historical philosophy on the other hand. However, different views of history and different specific theories about the field being studied may have important consequences for how sources are selected, interpreted and used. Feminist scholars may, for example, select sources made by women and may interpret sources from a feminist perspective. Epistemology should thus be considered a part of source criticism. It is in particular related to \"tendency analysis\".\n\nIn archaeology, radiocarbon dating is an important technique to establish the age of information sources. Methods of this kind were the ideal when history established itself as both a scientific discipline and as a profession based on \"scientific\" principles in the last part of the 1880s (although radiocarbon dating is a more recent example of such methods). The empiricist movement in history brought along both \"source criticism\" as a research method and also in many countries large scale publishing efforts to make valid editions of \"source materials\" such as important letters and official documents (e.g. as facsimiles or transcriptions).\n\nHistoriography and Historical method include the study of the reliability of the sources used, in terms of, for example, authorship, credibility of the author, and the authenticity or corruption of the text.\n\nSource criticism, as the term is used in biblical criticism, refers to the attempt to establish the sources used by the author and/or redactor of the final text. The term \"literary criticism\" is occasionally used as a synonym.\n\nBiblical source criticism originated in the 18th century with the work of Jean Astruc, who adapted the methods already developed for investigating the texts of Classical antiquity (Homer's Iliad in particular) to his own investigation into the sources of the Book of Genesis. It was subsequently considerably developed by German scholars in what was known as \"the Higher Criticism\", a term no longer in widespread use. The ultimate aim of these scholars was to reconstruct the history of the biblical text, as well as the religious history of ancient Israel.\n\nRelated to Source Criticism is Redaction Criticism which seeks to determine how and why the redactor (editor) put the sources together the way he did. Also related is form criticism and tradition history which try to reconstruct the oral prehistory behind the identified written sources.\n\nJournalists often work with strong time pressure and have access to only a limited number of information sources such as news bureaus, persons which may be interviewed, newspapers, journals and so on (see journalism sourcing). Journalists' possibility for conducting serious source criticism is thus limited compared to, for example, historians' possibilities.\n\nThe most important legal sources are created by parliaments, governments, courts, and legal researchers. They may be written or informal and based on established practices. Views concerning the quality of sources differ among legal philosophies: Legal positivism is the view that the text of the law should be considered in isolation, while legal realism, interpretivism (legal), critical legal studies and feminist legal criticism interprets the law on a broader cultural basis.\n\n\n"}
{"id": "54226143", "url": "https://en.wikipedia.org/wiki?curid=54226143", "title": "The Crime Book", "text": "The Crime Book\n\nThe Crime Book (Big Ideas Simply Explained) is a non-fiction volume co-authored by American crime writers Cathy Scott, Shanna Hogan, Rebecca Morris, Canadian author and historian Lee Mellor, and United Kingdom author Michael Kerrigan, with a foreword for the U.S. edition by Scott and the U.K. edition by crime-fiction author Peter James. It was released by DK Books under its Big Ideas Learning imprint in May 2017.\n\nThe publisher describes \"The Crime Book\" as a guide to criminology that explores the most infamous cases of all time, from serial killers to mob hits to war crimes and more.\n\nIt includes a variety of crimes committed by more than 100 of the world's most notorious criminals. From Jack the Ripper to Jeffrey Dahmer, the book is a study of international true-crime history that covers shocking stories through infographics and research that lays out key facts and details. It examines the science, psychology and sociology of criminal behavior. It profiles of villains, victims and detectives. Each clue is listed for readers to follow investigations from start to finish, and studies the police and detective work for each case.\n\nIn a Q&A article for CrimeCon's blog with Scott, the author described the crimes detailed in the book as having \"such diversity that there is something for everyone. ... I can’t think of one crime that’s not represented in The Crime Book. It runs the gamut—from nonviolent cons to gangland-style criminals, to white-collar offenders—with a complete representation starting with the first known homicide committed against a Neanderthal man. Simply put, you can’t make this stuff up.\"\n\n\"Rolling Stone\" magazine's description, in an August 2017 interview with co-author Scott about the book, wrote that it is \"an encyclopedic treatment of the topic (that) makes for excellent companion reading. A compelling compilation of human trickery and awfulness, it covers crimes from arson, art forgery and kidnapping to bank robbery, drug trafficking and, of course, murder, with many of the entries accompanied by helpful illustrations.\"\n\n\"Reader's Digest\" listed it as one of its \"Best New Books You Should Read This April,\" describing it as \"everything you ever wanted to know about some of the most audacious, hideous, hilarious and mysterious acts of crime in one explosive book, filled with graphs, illustrations, quotes and timelines. This highly addictive encyclopedia of crime ... is a trivia goldmine and a helpful guide allowing you to put events into context.\"\n\n\"Culture Magazine\" in Germany had this to say: \"The level of expertise is quite high,\" noting that the book \"is lushly illustrated, readable and entertaining.\"\n\nIn its review, \"Crime Fiction Lover\" wrote that \"a crack team of true-crime experts helped put it together.\"\n\n\"Crimespree Magazine\" wrote, \"The crimes covered are all over from serial killers to gangsters and outlaws to kidnappers and elderly Brit bank robbers. This is a great book.\"\n\n"}
{"id": "39726999", "url": "https://en.wikipedia.org/wiki?curid=39726999", "title": "The Rough Guide to True Crime", "text": "The Rough Guide to True Crime\n\nThe Rough Guide to True Crime is a non-fiction paperback reference guide to national and international true crime cases by American crime writer Cathy Scott. It was released in the UK and US in August 2009 by Penguin Books through its Rough Guides imprint.\n\n\"The Rough Guide to True Crime\" is a compilation of a variety of cases, including historic crimes, with sections broken down by the type of offenses and who committed them. It includes black-and-white photos as illustration. Psychological profiles are included throughout by forensic expert Dr. Louis B. Schlesinger, who explains the psychology of serial killers, murderers, hit men and burglars. The book features serial killer Jeffrey Dahmer, mob hitman Richard \"The Iceman\" Kuklinski, John Wayne Glover \"The Granny Killer,\" and British \"Doctor of Death\" Harold Shipman.\n\nScott's story from \"The Rough Guide to True Crime\" about mob enforcer Herbert Blitzstein was selected for inclusion in the July 2012 retrospective of crime writing, \"Masters of True Crime: Chilling Stories of Murder and the Macabre\".\n\nThe author appeared on BlogTalkRadio's \"True Murder\" show and described some of the crimes included in the book that were committed in the 19th century as \"a different time in America, where people like Billy the Kid could walk in and just rob a bank\" and get away with it. And while \"there was nothing glamorous about what they did, they are a part of lore.\"\n\nThe book was featured at BookExpo America 2009's trade fair in DK Publishing's booth in New York City.\n\nIn a review, \"True Crime Book Reviews\" wrote, \"From the Moors murders and Harold Shipman, to the murder of 2pac, this guide illuminates the psychology in play behind the most intriguing crimes in history, from the absurd to the appalling. \"The Rough Guide to True Crime\" explores the best of the haunting genre of True Crime.\"\n\n\n"}
{"id": "58757675", "url": "https://en.wikipedia.org/wiki?curid=58757675", "title": "William Chaffers", "text": "William Chaffers\n\nWilliam Chaffers (28 September 1811 – 12 April 1892) was an English antiquary and writer of reference works on hallmarks, and marks on ceramics. His \"Marks and Monograms on Pottery and Porcelain\", first published in 1863, has appeared in many later editions.\n\nChaffers was the son of William Chaffers and wife Sarah, and was born in Watling Street, London, in 1811; he was descended from a brother of Richard Chaffers (1731–1765), a manufacturer of Liverpool porcelain. He was educated at Margate and at Merchant Taylors' School, where he was entered in 1824.\n\nHe was attracted to antiquarian studies while a clerk in the city of London, by the discovery of Roman and medieval antiquities in the foundations of the Royal Exchange during 1838–9. At the same time he began to concentrate attention upon the study of gold and silver plate and ceramics, especially in regard to the official and other marks by which dates and places of fabrication can be distinguished. In 1863 Chaffers published two important works:\n\nOther publications are \"The Keramic Gallery\", in 2 volumes, with 500 illustrations (1872); a handbook abridged from \"Marks and Monograms\" (1874); \"Gilda Aurifabrorum\", a history of goldsmiths and plate workers and their marks (1883); also a priced catalogue of coins, and other minor catalogues.\n\nHis reputation was furthered in organizing exhibitions of art treasures, at Manchester in 1857, South Kensington in 1862, Leeds in 1869, Dublin in 1872, Wrexham in 1876, and Hanley (at the great Staffordshire exhibition of ceramics) in 1890. Chaffers was elected Fellow of the Society of Antiquaries of London in 1843, and he was a frequent contributor to \"Archæologia\", to \"Notes and Queries\", and to various learned periodicals upon the two subjects of which he had particular knowledge.\n\nIn 1841 he married Charlotte Matilda, daughter of John Hewett. About 1870 he retired from Fitzroy Square to a house in Willesden Lane, and later moved to West Hampstead, where he died on 12 April 1892.\n\nAttribution\n"}
