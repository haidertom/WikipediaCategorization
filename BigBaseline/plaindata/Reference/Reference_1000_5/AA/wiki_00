{"id": "56753217", "url": "https://en.wikipedia.org/wiki?curid=56753217", "title": "APLL", "text": "APLL\n\nAPLL is an acronym and may refer to:\n"}
{"id": "624684", "url": "https://en.wikipedia.org/wiki?curid=624684", "title": "Annotation", "text": "Annotation\n\nAn annotation is a metadatum (e.g. a post, explanation, markup) attached to location or other data.\n\nTextual scholarship is a discipline that often uses the technique of annotation to describe or add additional historical context to texts and physical documents.\n\nStudents often highlight passages in books in order to refer back to key phrases easily, or add marginalia to aid studying. One educational technique when analyzing prose literature is to have students or teachers circle the names of characters and put rectangular boxes around phrases identifying the setting of a given scene.\n\nAnnotated bibliographies add commentary on the relevance or quality of each source, in addition to the usual bibliographic information that merely identifies the source.\n\nFrom a cognitive perspective annotation has an important role in learning and instruction. As part of guided noticing it involves highlighting, naming or labelling and commenting aspects of visual representations to help focus learners' attention on specific visual aspects. In other words, it means the assignment of typological representations (culturally meaningful categories), to topological representations (e.g. images). This is especially important when experts, such as medical doctors, interpret visualizations in detail and explain their interpretations to others, for example by means of digital technology. Here, annotation can be a way to establish common ground between interactants with different levels of knowledge. The value of annotation has been empirically confirmed, for example, in a study which shows that in computer-based teleconsultations the integration of image annotation and speech leads to significantly improved knowledge exchange compared with the use of images and speech without annotation.\n\nMarkup languages like XML and HTML annotate text in a way that is syntactically distinguishable from that text. They can be used to add information about the desired visual presentation, or machine-readable semantic information, as in the semantic web.\n\nThe \"annotate\" function (also known as \"blame\" or \"praise\") used in source control systems such as Git, Team Foundation Server and Subversion determines who committed changes to the source code into the repository. This outputs a copy of the source code where each line is annotated with the name of the last contributor to edit that line (and possibly a revision number). This can help establish blame in the event a change caused a malfunction, or identify the author of brilliant code.\n\nA special case is the Java programming language, where annotations can be used as a special form of syntactic metadata in the source code. Classes, methods, variables, parameters and packages may be annotated. The annotations can be embedded in class files generated by the compiler and may be retained by the Java virtual machine and thus influence the run-time behaviour of an application. It is possible to create meta-annotations out of the existing ones in Java.\n\nSince the 1980s, molecular biology and bioinformatics have created the need for DNA annotation. DNA annotation or genome annotation is the process of identifying the locations of genes and all of the coding regions in a genome and determining what those genes do. An annotation (irrespective of the context) is a note added by way of explanation or commentary. Once a genome is sequenced, it needs to be annotated to make sense of it.\n\nIn the digital imaging community the term annotation is commonly used for visible metadata superimposed on an image without changing the underlying master image, such as sticky notes, virtual laser pointers, circles, arrows, and black-outs (cf. redaction).\n\nIn the medical imaging community, an annotation is often referred to as a region of interest and is encoded in DICOM format.\n\nIn the United States, legal publishers such as Thomson West and Lexis Nexis publish annotated versions of statutes, providing information about court cases that have interpreted the statutes. Both the federal United States Code and state statutes are subject to interpretation by the courts, and the annotated statutes are valuable tools in legal research.\n\nIn linguistics, annotations include comments and metadata; these non-transcriptional annotations are also non-linguistic. A collection of texts with linguistic annotations is known as a corpus (plural \"corpora\"). The Linguistic Annotation Wiki describes tools and formats for creating and managing linguistic annotations.\n\n"}
{"id": "1837830", "url": "https://en.wikipedia.org/wiki?curid=1837830", "title": "Apocope", "text": "Apocope\n\nIn phonology, apocope () is the loss (elision) of one or more sounds from the end of a word, especially the loss of an unstressed vowel.\n\n\"Apocope\" comes from the Greek () from () \"cutting off\", from () \"away from\" and () \"to cut\".\n\nIn historical linguistics, \"apocope\" is often the loss of an unstressed vowel.\n\n\n\nIn Estonian and the Sami languages, apocopes explain the forms of grammatical cases. For example, a nominative is described as having apocope of the final vowel, but the genitive does not. Throughout its history, however, the genitive case marker has also undergone apocope: Estonian (\"a city\") and (\"of a city\") are derived from and respectively, as can still be seen in the corresponding Finnish word. In the genitive form, the final , while it was being deleted, blocked the loss of . In colloquial Finnish, the final vowel is sometimes omitted from case markers.\n\nSome languages have apocopations that are internalized as mandatory forms. In Spanish and Italian, for example, some adjectives that come before the noun lose the final vowel or syllable if they precede a noun (mainly) in the masculine singular form. In Spanish, some adverbs and cardinal and ordinal numbers have apocopations as well.\n\n\nVarious numerous sorts of informal abbreviations might be classed as apocope:\n\nFor a list of similar apocopations in the English language, see List of English apocopations.\n\nDiminutives in Australian English lists many apocopations.\n\nThe process is also linguistically subsumed under one called \"clipping\", or \"truncation\".\n\n\n\n"}
{"id": "26334944", "url": "https://en.wikipedia.org/wiki?curid=26334944", "title": "Auxiliary sciences of history", "text": "Auxiliary sciences of history\n\nAuxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. \"History\" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.\n\nAuxiliary sciences of history include, but are not limited to:\n\n"}
{"id": "2887701", "url": "https://en.wikipedia.org/wiki?curid=2887701", "title": "Bible (screenwriting)", "text": "Bible (screenwriting)\n\nA bible (also known as a story bible, show bible, series bible, or pitch bible) is a reference document used by screenwriters for information on a television series' characters, settings, and other elements.\n\nShow bibles are updated with information on the characters after the information has been established on screen. For example, the \"Frasier\" show bible was \"scrupulously maintained\", and anything established on air — \"the name of Frasier's mother, Niles' favorite professor, Martin's favorite bar...even a list of Maris' [dozens of] food allergies\" — was reflected in the bible. The updated bible then serves as a resource for writers to keep everything within the series consistent. \n\nOther show bibles are used as sales documents to help a television network or studio understand a series, and are sometimes given to new writers when they join the writing staff for the same reason. These types of bibles discuss the backstories of the main characters and the history of the series' fictional universe.\n\nTelevision series often rely on writers' assistants and script coordinators to serve as \"walking bibles\" in remembering details about a series.\n\nIn the United States, writing the show bible of a produced series earns that writer the 24 units of required credit necessary to qualify for membership in the Writers Guild of America.\n\n\n"}
{"id": "3181897", "url": "https://en.wikipedia.org/wiki?curid=3181897", "title": "Brand Book", "text": "Brand Book\n\nA Brand Book records all livestock brands registered with an organization. In the U.S. most states have branding laws that require brands to be registered before use. This may be a state agency (usually affiliated with each state's Department of Agriculture) or a private association regulated by the state. Most states with such laws have a Brand Book for the entire state. Texas, an exception, registers brands at the county level. These book are usually provided free to law enforcement personnel and County Extension Agents. Some states have their Brand Books available online.\n\nA typical Brand Book will usually have an image of the brand, the location of the brand on the animal, and the type of animal that will be branded, as well as the owner of the brand. Many Brand Books also record earmarks.\n\nBrand Books are used by law enforcement officials, brand inspectors, and association investigators to record and track livestock movement, deter loss of livestock by straying or theft, and prosecute thieves.\n\n\n"}
{"id": "27778631", "url": "https://en.wikipedia.org/wiki?curid=27778631", "title": "Breviograph", "text": "Breviograph\n\nA breviograph or brevigraph (from , short, and Greek \"grapho\", to write) is a type of scribal abbreviation in the form of an easily written symbol, character, flourish or stroke, based on a modified letter form to take the place of a common letter combination, especially those occurring at the beginning or end of a word. Breviographs were used frequently by stenographers, law clerks and scriveners, and they were also found in early printed books and tracts. Their use declined after the 17th century.\n\nExamples of breviographs:\n\n\n"}
{"id": "6767022", "url": "https://en.wikipedia.org/wiki?curid=6767022", "title": "Carol Ballard", "text": "Carol Ballard\n\nCarol Ballard is an author of more than 80 non-fiction books. Specializing in informational books for children and teens, her focus is toward the 7- to 14-year-old group.\n\nAfter graduating from Leeds University in plant sciences, Ballard did post-graduate research and was awarded a PhD in Immunology. She has many years experience as a science teacher and co-ordinator and has written articles for teachers on various aspects of science teaching, and teachers' materials for classroon use.\n\nIn addition to her writing, Carol works as a freelance consultant for publishers on educational and scientific matters. She also has her own business, Kite Books, which produces worksheets and teachers' resources.\n\n\n"}
{"id": "1250078", "url": "https://en.wikipedia.org/wiki?curid=1250078", "title": "Cf.", "text": "Cf.\n\nThe abbreviation cf. (short for the , both meaning \"compare\") is used in writing to refer the reader to other material to make a comparison with the topic being discussed. It is used to form a contrast, for example: \"Abbott (2010) found supportive results in her memory experiment, unlike those of previous work (cf. Zeller & Williams, 2007).\" It is recommended that \"cf.\" be used only to suggest a comparison, and the word \"see\" be used to point to a source of information.\n\nIn biological naming conventions, cf. is commonly placed between the genus name and the species name to describe a specimen that is difficult to identify because of practical difficulties, such as the specimen being poorly preserved. For example, \"' cf. '\" indicates that the specimen is in the genus \"Barbus\", and believed to be \"\" but the actual species-level identification cannot be certain.\n\nCf. can also be used to express a possible identity, or at least a significant resemblance, such as between a newly observed specimen and a known species or taxon. Such a usage might suggest a specimen's membership of the same genus or possibly of a shared higher taxon, such as in, \", cf. \"\"\", where the author is confident of the order and family (Diptera: Tabanidae), but can only offer the genus (\"Tabanus\") as a suggestion and has no information favouring a particular species.\n\n"}
{"id": "1047161", "url": "https://en.wikipedia.org/wiki?curid=1047161", "title": "Chapters and verses of the Bible", "text": "Chapters and verses of the Bible\n\nThe Bible is a compilation of many shorter books written at different times by a variety of authors, and later assembled into the biblical canon. Since the early 13th century, most copies and editions of the Bible present all but the shortest of these books with divisions into chapters, generally a page or so in length. Since the mid-16th century editors have further subdivided each chapter into verses - each consisting of a few short lines or sentences. Sometimes a sentence spans more than one verse, as in the case of , and sometimes there is more than one sentence in a single verse, as in the case of .\n\nAs the chapter and verse divisions did not appear in the original texts, they form part of the paratext of the Bible.\n\nThe Jewish divisions of the Hebrew text differ at various points from those used by Christians. For instance, in Jewish tradition, the ascriptions to many Psalms are regarded as independent verses or parts of the subsequent verses, making 116 more verses, whereas established Christian practice treats each Psalm ascription as independent and unnumbered. Some chapter divisions also occur in different places, e.g. Hebrew Bibles have where Christian translations have .\n\nEarly manuscripts of the biblical texts did not contain the chapter and verse divisions in the numbered form familiar to modern readers. In antiquity Hebrew texts were divided into paragraphs (parashot) that were identified by two letters of the Hebrew alphabet. Peh פ indicated an \"open\" paragraph that began on a new line, while Samekh ס indicated a \"closed\" paragraph that began on the same line after a small space. These two letters begin the Hebrew words open (patuach\") and closed (sagoor\"), and are, themselves, open פ and closed ס. The earliest known copies of the Book of Isaiah from the Dead Sea Scrolls used parashot divisions, although they differ slightly from the Masoretic divisions. (This is different from the use of consecutive letters of the Hebrew alphabet to structure certain poetic compositions, known as acrostics, such as several of the Psalms and most of the Book of Lamentations.)\n\nThe Hebrew Bible was also divided into some larger sections. In Israel the Torah (its first five books) were divided into 154 sections so that they could be read through aloud in weekly worship over the course of three years. In Babylonia it was divided into 53 or 54 sections (Parashat ha-Shavua) so it could be read through in one year. The New Testament was divided into topical sections known as \"kephalaia\" by the fourth century. Eusebius of Caesarea divided the gospels into parts that he listed in tables or \"canons\". Neither of these systems corresponds with modern chapter divisions. (See fuller discussions below.)\n\nChapter divisions, with titles, are also found in the 9th century Tours manuscript, Paris Bibliothèque Nationale MS Lat. 3, the so-called Bible of Rorigo.\n\nArchbishop Stephen Langton and Cardinal Hugo de Sancto Caro developed different schemas for systematic division of the Bible in the early 13th century. It is the system of Archbishop Langton on which the modern chapter divisions are based.\n\nWhile chapter divisions have become nearly universal, editions of the Bible have sometimes been published without them. Such editions, which typically use thematic or literary criteria to divide the biblical books instead, include John Locke's \"Paraphrase and Notes on the Epistles of St. Paul\" (1707), Alexander Campbell's \"The Sacred Writings\" (1826), Daniel Berkeley Updike’s fourteen-volume \"The Holy Bible Containing the Old and New Testaments and the Apocrypha,\" Richard Moulton's \"The Modern Reader's Bible\" (1907), Ernest Sutherland Bates's \"The Bible Designed to Be Read as Living Literature\" (1936), \"The Books of the Bible\" (2007) from the International Bible Society (Biblica), Adam Lewis Greene’s five-volume \"Bibliotheca\" (2014), and the six-volume ESV Reader's Bible (2016) from Crossway Books.\n\nSince at least 916 the Tanakh has contained an extensive system of multiple levels of section, paragraph, and phrasal divisions that were indicated in Masoretic vocalization and cantillation markings. One of the most frequent of these was a special type of punctuation, the \"sof passuq\", symbol for a full stop or sentence break, resembling the colon (:) of English and Latin orthography. With the advent of the printing press and the translation of the Bible into English, Old Testament versifications were made that correspond predominantly with the existing Hebrew full stops, with a few isolated exceptions. Most attribute these to Rabbi Isaac Nathan ben Kalonymus's work for the first Hebrew Bible concordance around 1440.\n\nThe first person to divide New Testament chapters into verses was Italian Dominican biblical scholar Santi Pagnini (1470–1541), but his system was never widely adopted. His verse divisions in the New Testament were far longer than those known today. Robert Estienne created an alternate numbering in his 1551 edition of the Greek New Testament which was also used in his 1553 publication of the Bible in French. Estienne's system of division was widely adopted, and it is this system which is found in almost all modern Bibles. Estienne produced a 1555 Vulgate that is the first Bible to include the verse numbers integrated into the text. Before this work, they were printed in the margins.\n\nThe first English New Testament to use the verse divisions was a 1557 translation by William Whittingham (c. 1524–1579). The first Bible in English to use both chapters and verses was the Geneva Bible published shortly afterwards in 1560. These verse divisions soon gained acceptance as a standard way to notate verses, and have since been used in nearly all English Bibles and the vast majority of those in other languages. (Nevertheless, some Bibles have removed the verse numbering, including the ones noted above that also removed chapter numbers; a recent example of an edition that removed only verses, not chapters, is \"The Message: The Bible in Contemporary Language\" by Eugene H. Peterson.)\n\nThe Hebrew Masoretic text of the Bible notes several different kinds of subdivisions within the biblical books:\n\nMost important are the verse endings. According to the Talmudic tradition, the division of the text into verses is of ancient origin. In Masoretic versions of the Bible, the end of a verse is indicated by a small mark in its final word called a \"silluq\" (which means \"stop\"). Less formally, verse endings are usually also indicated by two horizontal dots following the word with a \"silluq\".\n\nThe Masoretic textual tradition also contains section endings called \"parashot\", which are usually indicated by a space within a line (a \"closed\" section) or a new line beginning (an \"open\" section). The division of the text reflected in the \"parashot\" is usually thematic. Unlike chapters, the \"parashot\" are not numbered, but some of them have special titles.\n\nIn early manuscripts (most importantly in Tiberian Masoretic manuscripts, such as the Aleppo codex), an \"open\" section may also be represented by a blank line, and a \"closed\" section by a new line that is slightly indented (the preceding line may also not be full). These latter conventions are no longer used in Torah scrolls and printed Hebrew Bibles. In this system, the one rule differentiating \"open\" and \"closed\" sections is that \"open\" sections must \"always\" start at the beginning of a new line, while \"closed\" sections \"never\" start at the beginning of a new line.\n\nAnother division of the biblical books found in the Masoretic text is the division of the \"sedarim\". This division is not thematic, but is almost entirely based upon the \"quantity\" of text. For the Torah, this division reflects the triennial cycle of reading that was practiced by the Jews of the Land of Israel.\n\nThe Byzantines also introduced a concept roughly similar to chapter divisions, called \"kephalaia\" (singular \"kephalaion\", literally meaning \"heading\"). This system, which was in place no later than the 5th century, is not identical to the present chapters. Unlike the modern chapters, which tend to be of roughly similar length, the distance from one \"kephalaion\" mark to the next varied greatly in length both within a book and from one book to the next. For example, the Sermon on the Mount, comprising three chapters in the modern system, has but one \"kephalaion\" mark, while the single modern chapter 8 of the Gospel of Matthew has several, one per miracle. Moreover, there were far fewer \"kephalaia\" in the Gospel of John than in the Gospel of Mark, even though the latter is the shorter text. In the manuscripts, the \"kephalaia\" with their numbers, their standard titles (\"titloi\") and their page numbers would be listed at the beginning of each biblical book; in the book's main body, they would be marked only with arrow-shaped or asterisk-like symbols in the margin, not in the text itself.\n\nThe titles usually referred to the first event or the first theological point of the section only, and some \"kephalaia\" are manifestly incomplete if one stops reading at the point where the next \"kephalaion\" begins (for example, the combined accounts of the miracles of the Daughter of Jairus and of the healing of the woman with a haemorrhage gets two marked \"kephalaia\", one titled \"of the daughter of the synagogue ruler\" at the beginning when the ruler approaches Jesus and one titled \"of the woman with the flow of blood\" where the woman enters the picture – well before the ruler's daughter is healed and the storyline of the previous \"kephalaion\" is thus properly concluded). Thus the \"kephalaia\" marks are rather more like a system of bookmarks or links into a continuous text, helping a reader to quickly find one of several well-known episodes, than like a true system of chapter divisions.\n\nCardinal Hugo de Sancto Caro is often given credit for first dividing the Latin Vulgate into chapters in the real sense, but it is the arrangement of his contemporary and fellow cardinal Stephen Langton who in 1205 created the chapter divisions which are used today. They were then inserted into Greek manuscripts of the New Testament in the 16th century. Robert Estienne (Robert Stephanus) was the first to number the verses within each chapter, his verse numbers entering printed editions in 1551 (New Testament) and 1571 (Hebrew Bible).\n\nThe division of the Bible into chapters and verses has received criticism from some traditionalists and modern scholars. Critics state that the text is often divided in an incoherent way, or at inappropriate rhetorical points, and that it encourages citing passages out of context. Nevertheless, the chapter and verse numbers have become indispensable as technical references for Bible study.\n\nSeveral modern publications of the Bible have eliminated numbering of chapters and verses. Biblica published such a version of the NIV in 2007 and 2011. In 2014, Crossway published the ESV Reader's Bible and \"Bibliotheca\" published a modified ASV. Projects such as Icthus also exist which strip chapter and verse numbers from existing translations.\n\nThe number of words can vary depending upon aspects such as whether the Hebrew alphabet in Psalm 119, the superscriptions listed in some of the Psalms, and the subscripts traditionally found at the end of the Pauline epistles, are included.\nExcept where stated, the following apply to the King James Version of the Bible in its modern 66-book Protestant form including the New Testament and the protocanonical Old Testament, not the deuterocanonical books.\n\n\n\n\n\n"}
{"id": "54083241", "url": "https://en.wikipedia.org/wiki?curid=54083241", "title": "Citation dynamics", "text": "Citation dynamics\n\nCitation dynamics describes the number of references received by the article or other scientific work over time. The citation dynamics is usually described by the bang, that take place 2–3 years after the work has been published, and the burst size spans several orders of magnitude. The presence of bursts is not consistent with other models based on preferential attachment. Those models are able to account for the skewed citation distribution but their reference accumulation is gradual.\n\nThe dynamics of scientific production has changed significantly over the past years. Due to technological progress, the number of published papers has been increasing exponentially until now. This, along with a much shorter time needed for the article to be published, has affected the citation dynamics of the modern papers. Furthermore, if the reference list of the study includes papers published in different years, older papers tend to have more citations. This may not necessarily because they are better but just because they had more time to accumulate those references.\n\nIt has been found that citation distributions are best described by a shifted power-law. The probability that paper formula_1 is cited at time formula_2 after publication as:\n\nwhere formula_4 serves as the outcome variable for each particular paper formula_1 at time formula_2. Fitness, formula_7, captures the inherent differences between papers, accounting for the perceived novelty and importance of a discovery. formula_8 represents the cumulative number of citations acquired by a paper formula_1 at time formula_2 and formula_11 is a log-normal survival probability. The probability is equal\n\nwhere formula_2 is time; formula_14 is longevity, capturing the decay rate; and formula_15 indicates immediacy, governing the time for a paper to reach its citation peak.\nThe ultimate impact formula_16 represents the total number or citations that the paper receives during its lifetime.\n\nWhere formula_15 is a global parameter that has the same value for all publications. formula_19 represents the relative fitness of the paper. From the above formula, we can see that the total number of references that the paper can receive during its lifetime depends only on its relative fitness which is very hard to quantify.\n\n"}
{"id": "983601", "url": "https://en.wikipedia.org/wiki?curid=983601", "title": "Comparative genomic hybridization", "text": "Comparative genomic hybridization\n\nComparative genomic hybridization is a molecular cytogenetic method for analysing copy number variations (CNVs) relative to ploidy level in the DNA of a test sample compared to a reference sample, without the need for culturing cells. The aim of this technique is to quickly and efficiently compare two genomic DNA samples arising from two sources, which are most often closely related, because it is suspected that they contain differences in terms of either gains or losses of either whole chromosomes or subchromosomal regions (a portion of a whole chromosome). This technique was originally developed for the evaluation of the differences between the chromosomal complements of solid tumor and normal tissue, and has an improved resolution of 5–10 megabases compared to the more traditional cytogenetic analysis techniques of giemsa banding and fluorescence in situ hybridization (FISH) which are limited by the resolution of the microscope utilized.\n\nThis is achieved through the use of competitive fluorescence in situ hybridization. In short, this involves the isolation of DNA from the two sources to be compared, most commonly a test and reference source, independent labelling of each DNA sample with fluorophores (fluorescent molecules) of different colours (usually red and green), denaturation of the DNA so that it is single stranded, and the hybridization of the two resultant samples in a 1:1 ratio to a normal metaphase spread of chromosomes, to which the labelled DNA samples will bind at their locus of origin. Using a fluorescence microscope and computer software, the differentially coloured fluorescent signals are then compared along the length of each chromosome for identification of chromosomal differences between the two sources. A higher intensity of the test sample colour in a specific region of a chromosome indicates the gain of material of that region in the corresponding source sample, while a higher intensity of the reference sample colour indicates the loss of material in the test sample in that specific region. A neutral colour (yellow when the fluorophore labels are red and green) indicates no difference between the two samples in that location.\n\nCGH is only able to detect unbalanced chromosomal abnormalities. This is because balanced chromosomal abnormalities such as reciprocal translocations, inversions or ring chromosomes do not affect copy number, which is what is detected by CGH technologies. CGH does, however, allow for the exploration of all 46 human chromosomes in single test and the discovery of deletions and duplications, even on the microscopic scale which may lead to the identification of candidate genes to be further explored by other cytological techniques.\n\nThrough the use of DNA microarrays in conjunction with CGH techniques, the more specific form of array CGH (aCGH) has been developed, allowing for a locus-by-locus measure of CNV with increased resolution as low as 100 kilobases. This improved technique allows for the aetiology of known and unknown conditions to be discovered.\n\nThe motivation underlying the development of CGH stemmed from the fact that the available forms of cytogenetic analysis at the time (giemsa banding and FISH) were limited in their potential resolution by the microscopes necessary for interpretation of the results they provided. Furthermore, giemsa banding interpretation has the potential to be ambiguous and therefore has lowered reliability, and both techniques require high labour inputs which limits the loci which may be examined.\n\nThe first report of CGH analysis was by Kallioniemi and colleagues in 1992 at the University of California, San Francisco, who utilised CGH in the analysis of solid tumors. They achieved this by the direct application of the technique to both breast cancer cell lines and primary bladder tumors in order to establish complete copy number karyotypes for the cells. They were able to identify 16 different regions of amplification, many of which were novel discoveries.\n\nSoon after in 1993, du Manoir et al. reported virtually the same methodology. The authors painted a series of individual human chromosomes from a DNA library with two different fluorophores in different proportions to test the technique, and also applied CGH to genomic DNA from patients affected with either Downs syndrome or T-cell prolymphocytic leukemia as well as cells of a renal papillary carcinoma cell line. It was concluded that the fluorescence ratios obtained were accurate and that differences between genomic DNA from different cell types were detectable, and therefore that CGH was a highly useful cytogenetic analysis tool.\n\nInitially, the widespread use of CGH technology was difficult, as protocols were not uniform and therefore inconsistencies arose, especially due to uncertainties in the interpretation of data. However, in 1994 a review was published which described an easily understood protocol in detail and the image analysis software was made available commercially, which allowed CGH to be utilised all around the world.\nAs new techniques such as microdissection and degenerate oligonucleotide primed polymerase chain reaction (DOP-PCR) became available for the generation of DNA products, it was possible to apply the concept of CGH to smaller chromosomal abnormalities, and thus the resolution of CGH was improved.\n\nThe implementation of array CGH, whereby DNA microarrays are used instead of the traditional metaphase chromosome preparation, was pioneered by Solinas-Tolodo et al. in 1997 using tumor cells and Pinkel et al. in 1998 by use of breast cancer cells. This was made possible by the Human Genome Project which generated a library of cloned DNA fragments with known locations throughout the human genome, with these fragments being used as probes on the DNA microarray. Now probes of various origins such as cDNA, genomic PCR products and bacterial artificial chromosomes (BACs) can be used on DNA microarrays which may contain up to 2 million probes. Array CGH is automated, allows greater resolution (down to 100 kb) than traditional CGH as the probes are far smaller than metaphase preparations, requires smaller amounts of DNA, can be targeted to specific chromosomal regions if required and is ordered and therefore faster to analyse, making it far more adaptable to diagnostic uses.\n\nThe DNA on the slide is a reference sample, and is thus obtained from a karyotypically normal man or woman, though it is preferential to use female DNA as they possess two X chromosomes which contain far more genetic information than the male Y chromosome. Phytohaemagglutinin stimulated peripheral blood lymphocytes are used. 1mL of heparinised blood is added to 10ml of culture medium and incubated for 72 hours at 37 °C in an atmosphere of 5% CO. Colchicine is added to arrest the cells in mitosis, the cells are then harvested and treated with hypotonic potassium chloride and fixed in 3:1 methanol/acetic acid.\n\nOne drop of the cell suspension should then be dropped onto an ethanol cleaned slide from a distance of about 30 cm, optimally this should be carried out at room temperature at humidity levels of 60–70%. Slides should be evaluated by visualisation using a phase contrast microscope, minimal cytoplasm should be observed and chromosomes should not be overlapping and be 400–550 bands long with no separated chromatids and finally should appear dark rather than shiny. Slides then need to be air dried overnight at room temperature, and any further storage should be in groups of four at −20 °C with either silica beads or nitrogen present to maintain dryness. Different donors should be tested as hybridization may be variable. Commercially available slides may be used, but should always be tested first.\n\nStandard phenol extraction is used to obtain DNA from test or reference (karyotypically normal individual) tissue, which involves the combination of Tris-Ethylenediaminetetraacetic acid and phenol with aqueous DNA in equal amounts. This is followed by separation by agitation and centrifugation, after which the aqueous layer is removed and further treated using ether and finally ethanol precipitation is used to concentrate the DNA.\n\nMay be completed using DNA isolation kits available commercially which are based on affinity columns.\n\nPreferentially, DNA should be extracted from fresh or frozen tissue as this will be of the highest quality, though it is now possible to use archival material which is formalin fixed or paraffin wax embedded, provided the appropriate procedures are followed. 0.5-1 µg of DNA is sufficient for the CGH experiment, though if the desired amount is not obtained DOP-PCR may be applied to amplify the DNA, however it in this case it is important to apply DOP-PCR to both the test and reference DNA samples to improve reliability.\n\nNick translation is used to label the DNA and involves cutting DNA and substituting nucleotides labelled with fluorophores (direct labelling) or biotin or oxigenin to have fluophore conjugated antibodies added later (indirect labelling). It is then important to check fragment lengths of both test and reference DNA by gel electrophoresis, as they should be within the range of 500kb-1500kb for optimum hybridization.\n\nUnlabelled Life Technologies Corporation's Cot-1 DNA® (placental DNA enriched with repetitive sequences of length 50bp-100bp)is added to block normal repetitive DNA sequences, particularly at centromeres and telomeres, as these sequences, if detected, may reduce the fluorescence ratio and cause gains or losses to escape detection.\n\n8–12µl of each of labelled test and labelled reference DNA are mixed and 40 µg Cot-1 DNA® is added, then precipitated and subsequently dissolved in 6µl of hybridization mix, which contains 50% formamide to decrease DNA melting temperature and 10% dextran sulphate to increase the effective probe concentration in a saline sodium citrate (SSC) solution at a pH of 7.0.\n\nDenaturation of the slide and probes are carried out separately. The slide is submerged in 70% formamide/2xSSC for 5–10 minutes at 72 °C, while the probes are denatured by immersion in a water bath of 80 °C for 10 minutes and are immediately added to the metaphase slide preparation. This reaction is then covered with a coverslip and left for two to four days in a humid chamber at 40 °C.\n\nThe coverslip is then removed and 5 minute washes are applied, three using 2xSSC at room temperature, one at 45 °C with 0.1xSSC and one using TNT at room temperature. The reaction is then preincubated for 10 minutes then followed by a 60-minute, 37 °C incubation, three more 5 minute washes with TNT then one with 2xSSC at room temperature. The slide is then dried using an ethanol series of 70%/96%/100% before counterstaining with DAPI (0.35 μg/ml), for chromosome identification, and sealing with a coverslip.\n\nA fluorescence microscope with the appropriate filters for the DAPI stain as well as the two fluorophores utilised is required for visualisation, and these filters should also minimise the crosstalk between the fluorophores, such as narrow band pass filters. The microscope must provide uniform illumination without chromatic variation, be appropriately aligned and have a “plan” type of objective which is apochromatic and give a magnification of x63 or x100.\n\nThe image should be recorded using a camera with spatial resolution at least 0.1 µm at the specimen level and give an image of at least 600x600 pixels. The camera must also be able to integrate the image for at least 5 to 10 seconds, with a minimum photometric resolution of 8 bit.\n\nDedicated CGH software is commercially available for the image processing step, and is required to subtract background noise, remove and segment materials not of chromosomal origin, normalize the fluorescence ratio, carry out interactive karyotyping and chromosome scaling to standard length. A “relative copy number karyotype” which presents chromosomal areas of deletions or amplifications is generated by averaging the ratios of a number of high quality metaphases and plotting them along an ideogram, a diagram identifying chromosomes based on banding patterns. Interpretation of the ratio profiles is conducted either using fixed or statistical thresholds (confidence intervals). When using confidence intervals, gains or losses are identified when 95% of the fluorescence ratio does not contain 1.0.\n\nExtreme care must be taken to avoid contamination of any step involving DNA, especially with the test DNA as contamination of the sample with normal DNA will skew results closer to 1.0, thus abnormalities may go undetected. FISH, PCR and flow cytometry experiments may be employed to confirm results.\n\nArray comparative genomic hybridization (also microarray-based comparative genomic hybridization, matrix CGH, array CGH, aCGH) is a molecular cytogenetic technique for the detection of chromosomal copy number changes on a genome wide and high-resolution scale. Array CGH compares the patient's genome against a reference genome and identifies differences between the two genomes, and hence locates regions of genomic imbalances in the patient, utilizing the same principles of competitive fluorescence in situ hybridization as traditional CGH.\n\nWith the introduction of array CGH, the main limitation of conventional CGH, a low resolution, is overcome. In array CGH, the metaphase chromosomes are replaced by cloned DNA fragments (+100–200 kb) of which the exact chromosomal location is known. This allows the detection of aberrations in more detail and, moreover, makes it possible to map the changes directly onto the genomic sequence.\n\nArray CGH has proven to be a specific, sensitive, fast and highthroughput technique, with considerable advantages compared to other methods used for the analysis of DNA copy number changes making it more amenable to diagnostic applications. Using this method, copy number changes at a level of 5–10 kilobases of DNA sequences can be detected. , even high-resolution CGH (HR-CGH) arrays are accurate to detect structural variations (SV) at resolution of 200 bp. This method allows one to identify new recurrent chromosome changes such as microdeletions and duplications in human conditions such as cancer and birth defects due to chromosome aberrations.\n\nArray CGH is based on the same principle as conventional CGH. In both techniques, DNA from a reference (or control) sample and DNA from a test (or patient) sample are differentially labelled with two different fluorophores and used as probes that are cohybridized competitively onto nucleic acid targets. In conventional CGH, the target is a reference metaphase spread. In array CGH, these targets can be genomic fragments cloned in a variety of vectors (such as BACs or plasmids), cDNAs, or oligonucleotides.\n\nFigure 2. is a schematic overview of the array CGH technique. DNA from the sample to be tested is labeled with a red fluorophore (Cyanine 5) and a reference DNA sample is labeled with green fluorophore (Cyanine 3). Equal quantities of the two DNA samples are mixed and cohybridized to a DNA microarray of several thousand evenly spaced cloned DNA fragments or oligonucleotides, which have been spotted in triplicate on the array. After hybridization, digital imaging systems are used to capture and quantify the relative fluorescence intensities of each of the hybridized fluorophores. The resulting ratio of the fluorescence intensities is proportional to the ratio of the copy numbers of DNA sequences in the test and reference genomes. If the intensities of the flurochromes are equal on one probe, this region of the patient's genome is interpreted as having equal quantity of DNA in the test and reference samples; if there is an altered Cy3:Cy5 ratio this indicates a loss or a gain of the patient DNA at that specific genomic region.\n\nArray CGH has been implemented using a wide variety of techniques. Therefore, some of the advantages and limitations of array CGH are dependent on the technique chosen.\nThe initial approaches used arrays produced from large insert genomic DNA clones, such as BACs. The use of BACs provides sufficient intense signals to detect single-copy changes and to locate aberration boundaries accurately. However, initial DNA yields of isolated BAC clones are low and DNA amplification techniques are necessary. These techniques include ligation-mediated polymerase chain reaction (PCR), degenerate primer PCR using one or several sets of primers, and rolling circle amplification. Arrays can also be constructed using cDNA. These arrays currently yield a high spatial resolution, but the number of cDNAs is limited by the genes that are encoded on the chromosomes, and their sensitivity is low due to cross-hybridization. This results in the inability to detect single copy changes on a genome wide scale. The latest approach is spotting the arrays with short oligonucleotides. The amount of oligos is almost infinite, and the processing is rapid, cost-effective, and easy. Although oligonucleotides do not have the sensitivity to detect single copy changes, averaging of ratios from oligos that map next to each other on the chromosome can compensate for the reduced sensitivity. It is also possible to use arrays which have overlapping probes so that specific breakpoints may be uncovered.\n\nThere are two approaches to the design of microarrays for CGH applications: whole genome and targeted.\n\nWhole genome arrays are designed to cover the entire human genome. They often include clones that provide an extensive coverage across the genome; and arrays that have contiguous coverage, within the limits of the genome. Whole-genome arrays have been constructed mostly for research applications and have proven their outstanding worth in gene discovery. They are also very valuable in screening the genome for DNA gains and losses at an unprecedented resolution.\n\nTargeted arrays are designed for a specific region(s) of the genome for the purpose of evaluating that targeted segment. It may be designed to study a specific chromosome or chromosomal segment or to identify and evaluate specific DNA dosage abnormalities in individuals with suspected microdeletion syndromes or subtelomeric rearrangements. The crucial goal of a targeted microarray in medical practice is to provide clinically useful results for diagnosis, genetic counseling, prognosis, and clinical management of unbalanced cytogenetic abnormalities.\n\nConventional CGH has been used mainly for the identification of chromosomal regions that are recurrently lost or gained in tumors, as well as for the diagnosis and prognosis of cancer. This approach can also be used to study chromosomal aberrations in fetal and neonatal genomes. Furthermore, conventional CGH can be used in detecting chromosomal abnormalities and have been shown to be efficient in diagnosing complex abnormalities associated with human genetic disorders.\n\nCGH data from several studies of the same tumor type show consistent patterns of non-random genetic aberrations. Some of these changes appear to be common to various kinds of malignant tumors, while others are more tumor specific. For example, gains of chromosomal regions lq, 3q and 8q, as well as losses of 8p, 13q, 16q and 17p, are common to a number of tumor types, such as breast, ovarian, prostate, renal and bladder cancer (Figure. 3). Other alterations, such as 12p and Xp gains in testicular cancer, 13q gain 9q loss in bladder cancer, 14q loss in renal cancer and Xp loss in ovarian cancer are more specific, and might reflect the unique selection forces operating during cancer development in different organs. Array CGH is also frequently used in research and diagnostics of B cell malignancies, such as chronic lymphocytic leukemia.\n\nCri du Chat (CdC) is a syndrome caused by a partial deletion of the short arm of chromosome 5. Several studies have shown that conventional CGH is suitable to detect the deletion, as well as more complex chromosomal alterations. For example, Levy et al. (2002) reported an infant with a cat-like cry, the hallmark of CdC, but having an indistinct karyotype. CGH analysis revealed a loss of chromosomal material from 5p15.3 confirming the diagnosis clinically. These results demonstrate that conventional CGH is a reliable technique in detecting structural aberrations and, in specific cases, may be more efficient in diagnosing complex abnormalities.\n\nArray CGH applications are mainly directed at detecting genomic abnormalities in cancer. However, array CGH is also suitable for the analysis of DNA copy number aberrations that cause human genetic disorders. That is, array CGH is employed to uncover deletions, amplifications, breakpoints and ploidy abnormalities. Earlier diagnosis is of benefit to the patient as they may undergo appropriate treatments and counseling to improve their prognosis.\n\nGenetic alterations and rearrangements occur frequently in cancer and contribute to its pathogenesis. Detecting these aberrations by array CGH provides information on the locations of important cancer genes and can have clinical use in diagnosis, cancer classification and prognostification. However, not all of the losses of genetic material are pathogenetic, since some DNA material is physiologically lost during the rearrangement of immunoglobulin subgenes. In a recent study, array CGH has been implemented to identify regions of chromosomal aberration (copy-number variation) in several mouse models of breast cancer, leading to identification of cooperating genes during myc-induced oncogenesis.\n\nArray CGH may also be applied not only to the discovery of chromosomal abnormalities in cancer, but also to the monitoring of the progression of tumors. Differentiation between metastatic and mild lesions is also possible using FISH once the abnormalities have been identified by array CGH.\n\nPrader–Willi syndrome (PWS) is a paternal structural abnormality involving 15q11-13, while a maternal aberration in the same region causes Angelman syndrome (AS). In both syndromes, the majority of cases (75%) are the result of a 3–5 Mb deletion of the PWS/AS critical region. These small aberrations cannot be detected using cytogenetics or conventional CGH, but can be readily detected using array CGH. As a proof of principle Vissers et al. (2003) constructed a genome wide array with a 1 Mb resolution to screen three patients with known, FISH-confirmed microdeletion syndromes, including one with PWS. In all three cases, the abnormalities, ranging from 1.5 to 2.9Mb, were readily identified. Thus, array CGH was demonstrated to be a specific and sensitive approach in detecting submicroscopic aberrations.\n\nWhen using overlapping microarrays, it is also possible to uncover breakpoints involved in chromosomal aberrations.\n\nThough not yet a widely employed technique, the use of array CGH as a tool for preimplantation genetic screening is becoming an increasingly popular concept. It has the potential to detect CNVs and aneuploidy in eggs, sperm or embryos which may contribute to failure of the embryo to successfully implant, miscarriage or conditions such as Down syndrome (trisomy 21). This makes array CGH a promising tool to reduce the incidence of life altering conditions and improve success rates of IVF attempts. The technique involves whole genome amplification from a single cell which is then used in the array CGH method. It may also be used in couples carrying chromosomal translocations such as balanced reciprocal translocations or Robertsonian translocations, which have the potential to cause chromosomal imbalances in their offspring.\n\nA main disadvantage of conventional CGH is its inability to detect structural chromosomal aberrations without copy number changes, such as mosaicism, balanced chromosomal translocations, and inversions. CGH can also only detect gains and losses relative to the ploidy level. In addition, chromosomal regions with short repetitive DNA sequences are highly variable between individuals and can interfere with CGH analysis. Therefore, repetitive DNA regions like centromeres and telomeres need to be blocked with unlabeled repetitive DNA (e.g. Cot1 DNA) and/or can be omitted from screening. Furthermore, the resolution of conventional CGH is a major practical problem that limits its clinical applications. Although CGH has proven to be a useful and reliable technique in the research and diagnostics of both cancer and human genetic disorders, the applications involve only gross abnormalities. Because of the limited resolution of metaphase chromosomes, aberrations smaller than 5–10 Mb cannot be detected using conventional CGH.\nFor the detection of such abnormalities, a high-resolution technique is required.\nArray CGH overcomes many of these limitations. Array CGH is characterized by a high resolution, its major advantage with respect to conventional CGH. The standard resolution varies between 1 and 5 Mb, but can be increased up to approximately 40 kb by supplementing the array with extra clones. However, as in conventional CGH, the main disadvantage of array CGH is its inability to detect aberrations that do not result in copy number changes and is limited in its ability to detect mosaicism. The level of mosaicism that can be detected is dependent on the sensitivity and spatial resolution of the clones. At present, rearrangements present in approximately 50% of the cells is the detection limit. For the detection of such abnormalities, other techniques, such as SKY (Spectral karyotyping) or FISH have to still be used.\n\n\n"}
{"id": "380406", "url": "https://en.wikipedia.org/wiki?curid=380406", "title": "Comparative psychology", "text": "Comparative psychology\n\nComparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area addresses many different issues, uses many different methods and explores the behavior of many different species from insects to primates.\n\nComparative psychology is sometimes assumed to emphasize cross-species comparisons, including those between humans and animals. However, some researchers feel that direct comparisons should not be the sole focus of comparative psychology and that intense focus on a single organism to understand its behavior is just as desirable; if not more so. Donald Dewsbury reviewed the works of several psychologists and their definitions and concluded that the object of comparative psychology is to establish principles of generality focusing on both proximate and ultimate causation. \n\nUsing a comparative approach to behavior allows one to evaluate the target behavior from four different, complementary perspectives, developed by Niko Tinbergen. First, one may ask how pervasive the behavior is across species (i.e. how common is the behavior between animal species?). Second, one may ask how the behavior contributes to the lifetime reproductive success of the individuals demonstrating the behavior (i.e. does the behavior result in animals producing more offspring than animals not displaying the behavior)? Theories addressing the ultimate causes of behavior are based on the answers to these two questions.\n\nThird, what mechanisms are involved in the behavior (i.e. what physiological, behavioral, and environmental components are necessary and sufficient for the generation of the behavior)? Fourth, a researcher may ask about the development of the behavior within an individual (i.e. what maturational, learning, social experiences must an individual undergo in order to demonstrate a behavior)? Theories addressing the proximate causes of behavior are based on answers to these two questions. For more details see Tinbergen's four questions.\n\nThe 9th century scholar al-Jahiz wrote works on the social organization and communication methods of animals like ants. The 11th century Arabic writer Ibn al-Haytham (Alhazen) wrote the \"Treatise on the Influence of Melodies on the Souls of Animals\", an early treatise dealing with the effects of music on an imals. In the treatise, he demonstrates how a camel's pace could be hastened or retarded with the use of music, and shows other examples of how music can affect animal behavior, experimenting with horses, birds and reptiles. Through to the 19th century, a majority of scholars in the Western world continued to believe that music was a distinctly human phenomenon, but experiments since then have vindicated Ibn al-Haytham's view that music does indeed have an effect on animals.\n\nCharles Darwin was central in the development of comparative psychology; it is thought that psychology should be spoken in terms of \"pre-\" and \"post-Darwin\" because his contributions were so influential. theory led to several hypotheses, one being that the factors that set humans apart, such as higher mental, moral and spiritual faculties, could be accounted for by evolutionary principles. In response to the vehement opposition to Darwinism was the \"anecdotal movement\" led by George Romanes who set out to demonstrate that animals possessed a \"rudimentary human mind\". Romanes is most famous for two major flaws in his work: his focus on anecdotal observations and entrenched anthropomorphism.\n\nNear the end of the 19th century, several scientists existed whose work was also very influential. Douglas Alexander Spalding was called the \"first experimental biologist\", and worked mostly with birds; studying instinct, imprinting, and visual and auditory development. Jacques Loeb emphasized the importance of objectively studying behavior, Sir John Lubbock is credited with first using mazes and puzzle devices to study learning and Conwy Lloyd Morgan is thought to be \"the first ethologist in the sense in which we presently use the word\".\n\nThroughout the long history of comparative psychology, repeated attempts have been made to enforce a more disciplined approach, in which similar studies are carried out on animals of different species, and the results interpreted in terms of their different phylogenetic or ecological backgrounds. Behavioral ecology in the 1970s gave a more solid base of knowledge against which a true comparative psychology could develop. However, the broader use of the term \"comparative psychology\" is enshrined in the names of learned societies and academic journals, not to mention in the minds of psychologists of other specialisms, so the label of the field is never likely to disappear completely.\n\nA persistent question with which comparative psychologists have been faced is the relative intelligence of different species of animal. Indeed, some early attempts at a genuinely comparative psychology involved evaluating how well animals of different species could learn different tasks. These attempts floundered; in retrospect it can be seen that they were not sufficiently sophisticated, either in their analysis of the demands of different tasks, or in their choice of species to compare. However, the definition of \"intelligence\" in comparative psychology is deeply affected by anthropomorphism, and focuses on simple tasks, complex problems, reversal learning, learning sets, and delayed alternation are plagued with practical and theoretical problems. In the literature, \"intelligence\" is defined as whatever is closest to human performance and neglects behaviors that humans are usually incapable of (e.g. echolocation). Specifically, comparative researchers encounter problems associated with individual differences, differences in motivation, differences in reinforcement, differences in sensory function, differences in motor capacities, and species-typical preparedness (i.e. some species have evolved to acquire some behaviors quicker than other behaviors).\n\nA wide variety of species have been studied by comparative psychologists. However, a small number have dominated the scene. Ivan Pavlov's early work used dogs; although they have been the subject of occasional studies, since then they have not figured prominently. Increasing interest in the study of abnormal animal behavior has led to a return to the study of most kinds of domestic animal. Thorndike began his studies with cats, but American comparative psychologists quickly shifted to the more economical rat, which remained the almost invariable subject for the first half of the 20th century and continues to be used.\n\nSkinner introduced the use of pigeons, and they continue to be important in some fields. There has always been interest in studying various species of primate; important contributions to social and developmental psychology were made by Harry F. Harlow's studies of maternal deprivation in rhesus monkeys. Cross-fostering studies have shown similarities between human infants and infant chimpanzees. Kellogg and Kellogg (1933) aimed to look at heredity and environmental effects of young primates. They found that a cross-fostered chimpanzee named Gua was better at recognizing human smells and clothing and that the Kelloggs' infant (Donald) recognised humans better by their faces. The study ended 9 months after it had begun, after the infant began to imitate the noises of Gua.\n\nNonhuman primates have also been used to show the development of language in comparison with human development. For example, Gardner (1967) successfully taught the female chimpanzee Washoe 350 words in American Sign Language. Washoe subsequently passed on some of this teaching to her adopted offspring, Loulis. A criticism of Washoe's acquisition of sign language focused on the extent to which she actually understood what she was signing. Her signs may have just based on an association to get a reward, such as food or a toy. Other studies concluded that apes do not understand linguistic input, but may form an intended meaning of what is being communicated. All great apes have been reported to have the capacity of allospecific symbolic production.\n\nInterest in primate studies has increased with the rise in studies of animal cognition. Other animals thought to be intelligent have also been increasingly studied. Examples include various species of corvid, parrots — especially the grey parrot — and dolphins. Alex (Avian Learning EXperiment) is a well known case study (1976–2007) which was developed by Pepperberg, who found that the African gray parrot Alex did not only mimic vocalisations but understood the concepts of same and different between objects. The study of non-human mammals has also included the study of dogs. Due to their domestic nature and personalities, dogs have lived closely with humans, and parallels in communication and cognitive behaviours have therefore been recognised and further researched. Joly-Mascheroni and colleagues (2008) demonstrated that dogs may be able to catch human yawns and suggested a level of empathy in dogs, a point that is strongly debated. Pilley and Reid found that a Border Collie named Chaser was able to successfully identify and retrieve 1022 distinct objects/toys.\n\nResearchers who study animal cognition are interested in understanding the mental processes that control complex behavior, and much of their work parallels that of cognitive psychologists working with humans. For example, there is extensive research with animals on attention, categorization, concept formation, memory, spatial cognition, and time estimation. Much research in these and other areas is related directly or indirectly to behaviors important to survival in natural settings, such as navigation, tool use, and numerical competence. Thus, comparative psychology and animal cognition are heavily overlapping research categories.\n\nVeterinary surgeons recognize that the psychological state of a captive or domesticated animal must be taken into account if its behavior and health are to be understood and optimized.\n\nCommon causes of disordered behavior in captive or pet animals are lack of stimulation, inappropriate stimulation, or overstimulation. These conditions can lead to disorders, unpredictable and unwanted behavior, and sometimes even physical symptoms and diseases. For example, rats who are exposed to loud music for a long period will ultimately develop unwanted behaviors that have been compared with human psychosis, like biting their owners.\n\nThe way dogs behave when understimulated is widely believed to depend on the breed as well as on the individual animal's character. For example, huskies have been known to ruin gardens and houses if they are not allowed enough activity. Dogs are also prone to psychological damage if they are subjected to violence. If they are treated very badly, they may become dangerous.\n\nThe systematic study of disordered animal behavior draws on research in comparative psychology, including the early work on conditioning and instrumental learning, but also on ethological studies of natural behavior. However, at least in the case of familiar domestic animals, it also draws on the accumulated experience of those who have worked closely with the animals.\n\nThe relationship between humans and animals has long been of interest to anthropologists as one pathway to an understanding the evolution of human behavior. Similarities between the behavior of humans and animals have sometimes been used in an attempt to understand the evolutionary significance of particular behaviors. Differences in the treatment of animals have been said to reflect a society's understanding of human nature and the place of humans and animals in the scheme of things. Domestication has been of particular interest. For example, it has been argued that, as animals became domesticated, humans treated them as property and began to see them as inferior or fundamentally different from humans.\nIngold remarks that in all societies children have to learn to differentiate and separate themselves from others. In this process, strangers may be seen as \"not people,\" and like animals. Ingold quoted Sigmund Freud: \"Children show no trace of arrogance which urges adult civilized men to draw a hard-and-fast line between their own nature and that of all other animals. Children have no scruples over allowing animals to rank as their full equals.\" With maturity however, humans find it hard to accept that they themselves are animals, so they categorize, separating humans from animals, and animals into wild animals and tame animals, and tame animals into house pets and livestock. Such divisions can be seen as similar to categories of humans: who is part of a human community and someone who isn't, that is, the outsider.\n\n\"The New York Times\" ran an article that showed the psychological benefits of animals, more specifically of children with their pets. It's been proven that having a pet does in fact improve kids' social skills. In the article, Dr. Sue Doescher, a psychologist involved in the study, stated, \"It made the children more cooperative and sharing.\" It was also shown that these kids were more confident with themselves and able to be more empathic with other children.\n\nFurthermore, in an edition of \"Social Science and Medicine\" it was stated, \"A random survey of 339 residents from Perth, Western Australia were selected from three suburbs and interviewed by telephone. Pet ownership was found to be positively associated with some forms of social contact and interaction, and with perceptions of neighborhood friendliness. After adjustment for demographic variables, pet owners scored higher on social capital and civic engagement scales.\" Results like these let us know that owning a pet provides opportunities for neighborly interaction, among many other chances for socialization among people.\n\nNoted comparative psychologists, in this broad sense, include:\n\nMany of these were active in fields other than animal psychology; this is characteristic of comparative psychologists.\n\nFields of psychology and other disciplines that draw upon, or overlap with, comparative psychology include:\n\n\n"}
{"id": "2466507", "url": "https://en.wikipedia.org/wiki?curid=2466507", "title": "Comparative sociology", "text": "Comparative sociology\n\nComparative sociology involves comparison of the social processes between nation states, or across different types of society (for example capitalist and socialist). There are two main approaches to comparative sociology: some seek similarity across different countries and cultures whereas others seek variance. For example, structural Marxists have attempted to use comparative methods to discover the general processes that underlie apparently different social orderings in different societies. The danger of this approach is that the different social contexts are overlooked in the search for supposed universal structures.\n\nOne sociologist who employed comparative methods to understand variance was Max Weber, whose studies attempted to show how differences between cultures explained the different social orderings that had emerged (see for example \"The Protestant Ethic and the Spirit of Capitalism\" and Sociology of religion).\n\nThere is some debate within sociology regarding whether the label of 'comparative' is suitable. Emile Durkheim argued in \"The Rules of Sociological Method\" (1895) that all sociological research was in fact comparative since social phenomenon are always held to be typical, representative or unique, all of which imply some sort of comparison. In this sense, all sociological analysis is comparative and it has been suggested that what is normally referred to as comparative research, may be more appropriately called cross-national research.\n\n"}
{"id": "13243925", "url": "https://en.wikipedia.org/wiki?curid=13243925", "title": "Comparison of Star Trek and Star Wars", "text": "Comparison of Star Trek and Star Wars\n\n\"Star Trek\" and \"Star Wars\" are American media franchises which present alternative scenarios of space adventure. The two franchises are dominant in this setting of storytelling and have offered various forms of media productions for decades that manage billions of dollars of intellectual property, providing employment and entertainment for billions of people around the world.\n\n\"Star Trek\" was introduced as in 1966 that lasted three years. \"\" commenced in 1973 (based directly on the original series) but lasted only two seasons with a combined total of 22 episodes. With the subsequent publication of novels, comics, animated series, toys and feature films, \"Star Trek\" grew into a popular media franchise.\n\n\"Star Wars\" was introduced as a feature film, \"A New Hope\" (1977). A novelization titled \"\", based on the original script of the film, was published about a year earlier. Upon the release of the first film, \"Star Wars\" quickly grew into a popular media franchise.\n\n\"Star Trek\" debuted in television. The franchise was conceived in the style of the television Western \"Wagon Train\" and the adventure stories of Horatio Hornblower, but evolved into an idealistic, utopian prospect of future human society. Inspired by \"Gulliver's Travels\", \"Star Trek\"s main focus is of space exploration and a galactic society consisting of multiple planets and species, where conflict occasionally occurs. \"Star Trek\" occurs in the relatively distant future, specifically the 22nd through 24th centuries, with occasional time travel and interdimensional travel. The Earth of the \"Star Trek\" universe shares most of its history with the real world.\n\n\"Star Wars\" debuted in film, despite the novel based on the film's original script having been published a year before the film itself. \"Star Wars\" mainly belongs to the space opera subgenre of science fiction that follows The Hero's Journey and was inspired by works such as Beowulf, King Arthur and other mythologies, world religions, as well as ancient and medieval history. It depicts a galactic society in constant conflict. Though there are periods of peace, these are only documented in novels, comics, video games, non-feature films and other spin-off media. \"Star Wars\" is set \"a long time ago, in a galaxy far, far away\", although many characters are human, occasionally use Earth metaphors and exhibit human character traits.\n\nAlthough both \"Star Trek\" and \"Star Wars\" populate various forms of media, not all types have been produced that are mutual to both franchises. \"Star Wars\" has not produced any live-action television series while \"Star Trek\" has produced seven live-action television series.\n\n\"Star Trek\" likewise has not produced any television films; whereas \"Star Wars\" has produced at least three live-action television films outside the \"Star Wars\" film saga. The \"Star Wars Holiday Special\", \"\" and \"\" are all live-action television spin-off films set in the \"Star Wars\" universe, but not considered part of the official \"Star Wars canon\".\n\n\nAside from both having the word \"star\" in their titles, the two franchises share many similarities and commonalities. Both franchises have their origins in the space western subgenre.\n\nBoth stories depict societies consisting of multiple planets and species. The main galaxy in \"Star Trek\" consists of various planets, each inhabited by different species, united into a single state, the United Federation of Planets. \"Star Wars\" depicts a galaxy that is mostly part of a single state known as the Old Republic, inhabited by humans and countless other species, which later became the Galactic Empire and was again later reformed into a new society called the New Republic after a series of wars.\n\nBoth franchises promote philosophical and political messages.\n\nThe primary philosophies of \"Star Trek\" convey the morals of exploration and interference and how to properly confront and ethically resolve a new situation. Creator Gene Roddenberry was inspired by morality tales such as \"Gulliver's Travels\".\n\nThe primary philosophical messages of \"Star Wars\" are the ethics of good against evil and how to distinguish them from one another. \"Star Wars\" preaches against totalitarian systems and favors societies that offer equality. In an interview on the \"Star Wars\" 20th Anniversary UK Programme aired in 1997 referring to the mythology of the original \"Star Wars\" trilogy, Patrick Stewart stated \"A belief in one's own powers; especially one's own powers to do good because the underlying morality of \"Star Wars\" is a very very positive one.\"\n\nThere have been actors from both franchises who have appeared on common television series such as \"The Outer Limits\" and \"Seaquest\".\n\nBoth franchises also derive significantly from history and ancient mythology, including Greco-Roman mythology. Many planets and alien species in \"Star Trek\", for instance, are named after ancient Roman deities. Several episodes from various \"Star Trek\" television series, such as \"Who Mourns for Adonais\", are directly based on ancient Greek-Roman themes and settings. The series also make references to Ancient Babylon and its mythic folklore. The Klingons and their warrior culture are a representation of the 12th-century Mongols.\n\nMuch of \"Star Wars\" story plots and character developments are based on ancient history, including classical Greece and Rome, such as the fall of the Old Republic in \"Star Wars\", followed by the rise of the Galactic Empire, which parallels the fall of the ancient Roman Republic followed by the rise of the Roman Empire.\n\nA 1983 documentary on the making of \"Star Wars Episode VI: Return of the Jedi\" was hosted by Leonard Nimoy, who also made mention of Lucas's original plan to do two other trilogies preceding and proceeding the original trilogy.\n\nJ. J. Abrams, director and producer of \"Star Trek\" (2009) and \"Star Trek Into Darkness\" (2013) and producer of \"Star Trek Beyond\" (2016), directed and produced \"\" (2015). \"Star Trek\" (2009) and \"Star Wars: The Force Awakens\" (2015) are each the first entries in expected trilogies. These films received favorable critical and commercial response and revived interest for both franchises. In addition to Abrams, actors such as Simon Pegg starred in both series.\n\n\"\" (2002) was poorly received and \"\" (2005) had capped off the prequel trilogy, which overall had a mixed to positive reception.\n\nThe newer films of the two franchises filmed major scenes in the United Arab Emirates. The desert scenes on the planet Jakku in \"Star Wars: The Force Awakens\" (2015) were filmed in the Emirate of Abu Dhabi, while scenes for cities in the film \"Star Trek Beyond\" (2016) were filmed in the Emirate of Dubai.\n\nThe two franchises now offer almost all forms of media ranging from novels, television series, comic books, toys for younger audience, magazines, themed merchandise, board games and video games, as well as fan works. These include canonical and non-canonical works, including works made both by series producers and fans jointly.\n\nDespite the difference in the numbers of films, the profit made by the \"Star Wars\" film series exceed the profit of the \"Star Trek\" film series by almost five times, while the entire franchise outgrosses the other by four times. It is difficult to accurately judge the total worth of each franchise as television series, memorabilia and video games must be taken into account.\n\nScience fiction writer David Brin criticized \"Star Wars\" at the time of the release of \"The Phantom Menace\", arguing that while the \"Star Wars\" movies provide special effects and action/adventure, audiences are not encouraged to engage with their overriding themes. Among his issues with \"Star Wars\" and George Lucas, whom he accused of \"having an agenda\", is that the \"Star Wars\" galaxy is too \"elitist\", with arbitrary rulers on both the evil and good sides, replacing one another without any involvement of the population. He criticizes both sides of the Galactic Civil War as part of the \"same genetically superior royal family\". He finds the \"Star Wars\" universe flawed with additional forms of absolutism, such as justified emotions leading a good person to evil - for example citing the idea that Luke Skywalker killing Palpatine would somehow turn him to the dark side, despite the act potentially saving millions of lives.\n\nAmong the many other flaws he sees with \"Star Wars\" is that Anakin Skywalker becomes a hero in the ending of \"Return of the Jedi\" simply because he saved his son's life, while the atrocities he committed during his time in power go largely ignored. In contrast, he argues that, despite its flaws, \"Star Trek\" is \"democratic\" and follows genuine issues and strong questioning.\n\nWilliam Shatner argues that \"Star Trek\" is superior to \"Star Wars\". According to him, \"\"Star Trek\" had relationships and conflict among the relationships and stories that involved humanity and philosophical questions.\" Shatner believes that \"Star Wars\" was only better than \"Star Trek\" in terms of special effects, and that once J.J. Abrams became involved, \"Star Trek\" was able to \"supersede \"Star Wars\" on every level\".\n\nTim Russ, who played Tuvok on \"\", claims that it is difficult to find common enough elements to be able to compare the two. Among those common elements are their similar settings of unique characters and technologies. He echoed Shatner that \"Star Trek\" reflects common human issues, the morals of exploration and considers ethical questions. \"Star Wars\" in his view is a classic medieval tale dressed up as action-adventure, and that embraces the Eastern philosophy of inner-strength. Russ concludes that despite both their success and popularity, \"Star Trek\" comes out as the better of the two, as it is set in \"our\" galaxy and therefore people can relate better to it, whereas \"Star Wars\" takes place in another galaxy. He acknowledged that he could be biased.\n\nJeremy Bulloch is best known for his role as Boba Fett in the original \"Star Wars\" trilogy. He is a huge fan of \"Star Trek: The Original Series\". He argued that while both franchises are popular, \"Star Wars\" comes out as the superior, for its soundtracks and special effects.\n\nContrasting the focus of the two franchises, contributor J.C. Herzthe of \"The New York Times\" argued, \"\"Trek\" fandom revolves around technology because the \"Star Trek\" universe was founded on ham-fisted dialogue and \"Gong Show\"-caliber acting. But the fictional science has always been brilliant. The science in \"Star Wars\" is nonsense, and everyone knows it. But no one cares because \"Star Wars\" isn't about science. It's epic drama. It's about those incredibly well-developed characters and the moral decisions they face. People don't get into debates about how the second Death Star works. They get into debates about the ethics of blowing it up.\"\n\nJohn Wenzel of \"The Denver Post\" highlighted two differences in approach, noting the \"swashbuckling\" and \"gunslinger\" style of \"Star Wars\" compared with \"Star Trek\"s \"broader themes of utopian living, justice and identity\" and that the spiritual aspect of \"Star Wars\" contrasts with the balance of emotion and logic seen in \"Star Trek\".\n\nBillionaire Peter Thiel told Dowd \"I'm a capitalist. \"Star Wars\" is the capitalist show. \"Star Trek\" is the communist one\". He further stated \"There is no money in \"Star Trek\" because you just have the transporter machine that can make anything you need. The whole plot of \"Star Wars\" starts with Han Solo having this debt that he owes and so the plot in \"Star Wars\" is driven by money.\"\n\nArchived footage in \"Trek Nation\" showed Gene Roddenberry saying, \"I like \"Star Wars\". It was young King Arthur growing up, slaying the evil emperor finally. There's nothing wrong with that kind of entertainment - everything doesn't have to create a philosophy for you - for your whole life. You can also have fun.\"\n\nThe two franchises have a \"symbiotic relationship\" stated Shatner, who credits \"Star Wars\" for launching the \"Star Trek\" films. He repeated this sentiment at a 2016 \"Star Trek\" fan convention in Las Vegas by stating \"\"Star Wars\" created \"Star Trek\"\". He clarified this statement by explaining that at the time of the release of the first \"Star Wars\" film (\"A New Hope\"), Paramount, then under new management, was struggling to come up with something that could compete with it. A \"Star Trek\" relaunch was the choice. Since then, public interest has returned to \"Star Trek\". \"It was \"Star Wars\" that thrust \"Star Trek\" into the people of Paramount's consciousness\" Shatner stated.\n\nThe documentary \"Trek Nation\" features interviews where both Lucas and Roddenberry praise each other's respective franchises, with the former stating that \"Star Trek\" was an influence while writing the original screenplay for \"Star Wars\". He explained that while both franchises were so \"far out\", \"Star Trek\" produced a fanbase that \"softened up the entertainment arena\" so that \"Star Wars\" could \"come along and stand on its shoulders.\" This is also acknowledged by Shatner, who went as far as to call \"Star Wars\" a \"derivative\" of \"Star Trek\".\n\nA few references to \"Star Wars\" have been inserted into \"Star Trek\" films. For fleeting moments, one can see ships and droids from \"Star Wars\" in both \"Star Trek\" (2009) and \"Star Trek Into Darkness\" (2013). Some \"Star Trek\" films and television episodes used the \"Star Wars\" animation shop, Industrial Light & Magic, for their special effects.\n\nWhen Roddenberry was honored at a \"Star Trek\" convention late in life, a congratulatory letter from Lucas was presented by an actor dressed as Darth Vader. A few years earlier, Roddenberry had contributed an entry in honor of \"Star Wars\" and Lucas at a convention honoring the latter.\n\nWilliam Shatner was a presenter at Lucas' American Film Institute Lifetime Achievement Award ceremony in 2007 and did a comical stage performance honoring Lucas.\n\nAt a live concert, Shatner dressed as an imperial stormtrooper singing \"Girl Crush\" alongside Carrie Underwood and Brad Paisley.\n\nIn 2011, Shatner and Carrie Fisher posted a series of humorous YouTube videos satirizing each other's franchises.\n\nIn a 2016 interview, Shatner commented that Captain Kirk and Princess Leia eloping and running off into the sunset would be the \"perfect union\" between \"Star Trek\" and \"Star Wars\".\n\nShatner has also posted a number of humorous tweets on his Twitter account mocking \"Star Wars\". Amongst them were commemorating the 35th anniversary of the poorly received \"Star Wars Holiday Special\". It was then that \"Star Wars\" actor Peter Mayhew posted a \"retaliation\" tweet congratulating Shatner for the directing of \"\", another poorly received film.\n\nBoth franchises are set to grow throughout the next decade.\n\n\"Star Trek\" was rebooted with a series of feature films starting with the \"Star Trek\" reboot (2009), which was followed by \"Star Trek Into Darkness\" (2013) and \"Star Trek Beyond\" (2016) and a number of sequels are set to follow. A new television series based in the original timeline, subtitled \"\", serving as a prequel to the original series, debuted on CBS All Access, an online streaming platform, in 2017.\n\n\"Star Wars\" picked up from where \"Return of the Jedi\" left off, with \"\" the first in the new trilogy, and \"\" following two years later. Additionally, more spin-off media is also underway after the debut of \"Star Wars Rebels\", a television series set in between the \"Star Wars\" prequels and the original trilogy, and an anthology of stand-alone \"Star Wars\" films, starting with \"Rogue One\", which was released in December 2016, and \"\" following in May 2018.\n\nAside from official works by the producers of \"Star Trek\" and \"Star Wars\", many fan films and webisodes set in the two universes of the franchises are also constantly produced and posted on the Internet by fans, but are not officially considered canon in relation to either franchise.\n\n"}
{"id": "16759434", "url": "https://en.wikipedia.org/wiki?curid=16759434", "title": "Comparison of the Amundsen and Scott Expeditions", "text": "Comparison of the Amundsen and Scott Expeditions\n\nBetween December 1911 and January 1912, both Roald Amundsen (leading his South Pole expedition) and Robert Falcon Scott (leading the Terra Nova Expedition) reached the South Pole within a month of each other. But while Scott and his four companions died on the return journey, Amundsen's party managed to reach the geographic south pole first and subsequently return to their base camp at Framheim without loss of life, suggesting that they were better prepared for the expedition. The contrasting fates of the two teams seeking the same prize at the same time invites comparison.\n\nThe outcomes of the two expeditions were as follows.\n\nHistorically, several factors have been discussed and many contributing factors claimed, including:\n\nSullivan states that it was the last factor that probably was decisive. he states \"Man is a poor beast of burden, as was shown in the terrible experience of Scott, Shackleton, and Wilson in their thrust to the south of 1902–3. However, Scott relied chiefly on man-hauling in 1911–12 because ponies could not ascend the glacier midway to the Pole. The Norwegians correctly estimated that dog teams could go all the way. Furthermore, they used a simple plan, based on their native skill with skis and on dog-driving methods that were tried and true. In a similar fashion to the way the moon was reached by expending a succession of rocket stages and then casting each aside; the Norwegians used the same strategy, sacrificing the weaker animals along the journey to feed the other animals and the men themselves.\"\n\nScott and his financial backers saw the expedition as having a scientific basis, while also wishing to reach the pole. However, it was recognised by all involved that the South Pole was the primary objective (\"The Southern Journey involves the most important object of the Expedition\" – Scott), and had priority in terms of resources, such as the best ponies and all the dogs and motor sledges as well as involvement of the vast majority of the expedition personnel. Scott and his team knew the expedition would be judged on his attainment of the pole (\"The ... public will gauge the result of the scientific work of the expedition largely in accordance with the success or failure of the main object\" – Scott). He was prepared to make a second attempt the following year (1912–13) if this attempt failed and had Indian Army mules and additional dogs delivered in anticipation. In fact the mules were used by the team that discovered the dead bodies of Scott, Henry Robertson Bowers, and Edward Adrian Wilson in November 1912, but proved even less useful than the ponies, according to Cherry-Garrard.\n\nAmundsen's expedition was planned to reach the South Pole. This was a plan he conceived in 1909. Amundsen's expedition did conduct geographical work under Kristian Prestrud who conducted an expedition to King Edward VII Land while Amundsen was undertaking his attempt at the pole.\n\nAmundsen camped on the Ross Ice Shelf at the Bay of Whales which is 60 miles (96 km) closer to the pole than Scott's camp (which was 350 miles west of Amundsen). Amundsen had deduced that, as the Trans-Antarctic Mountains ran northwest to southeast then if he were to meet a mountain range on his route then the time spent at the high altitude of the Antarctic plateau would be less than Scott's.\nScott's base was at Cape Evans on Ross Island, with access to the Trans-Antarctic mountain range to the west, and was a better base for geological exploration. He had based his previous expedition in the same area. However, he knew it to be poor as a route to the pole as he had to start before sea ice melted and had suffered delay in returning while waiting for the sea ice to freeze. They also had to make detours around Ross Island and its known crevassed areas which meant a longer journey. The crossing of the Ross Ice Shelf was an onerous task for the ponies. Scott had advanced considerable stores across the ice shelf the year before to allow the ponies to carry lighter loads over the early passage across the ice. Even so, he had to delay the departure of the ponies until 1 November rather than 24 October when the dogs and motor sledges set off.\nConsequently, the Motor Party spent 6 days at the Mount Hooper Depot waiting for Scott to arrive.\n\nThe major comparison between Scott and Amundsen has focused on the choice of draft transport —dog versus pony/man-hauling. In fact Scott took dogs, ponies and three \"motor sledges\". Scott spent nearly seven times the amount of money on his motor sledges than on the dogs and horses combined. They were therefore a vital part of the expedition. Unfortunately, Scott decided to leave behind the engineer, Lieutenant Commander Reginald William Skelton who had created and trialled the motor sledges. This was due to the selection of Lieutenant E.R.G.R. \"Teddy\" Evans as the expedition's second in command. As Evans was junior in rank to Skelton, he insisted that Skelton could not come on the expedition. Scott agreed to this request and Skelton's experience and knowledge was lost. One of the original three motor sledges was a failure even before the expedition set out; the heavy sledge was lost through thin ice on unloading it from the ship. The two remaining motor sledges failed relatively early in the main expedition because of repeated faults. Skelton's experience might have been valuable in overcoming the failures.\n\nScott had used dogs on his first (Discovery) expedition and felt they had failed. On that journey, Scott, Shackleton, and Wilson started with three sledges and 13 dogs. But on that expedition, the men had not properly understood how to travel on snow with the use of dogs. The party had skis but were too inexperienced to make good use of them. As a result, the dogs travelled so fast that the men could not keep up with them. The Discovery expedition had to increase their loads to slow the dogs down. Additionally, the dogs were fed Norwegian dried fish, which did not agree with them and soon they began to deteriorate. The whole team of dogs eventually died (and were eaten), and the men took over hauling the sleds.\n\nScott's opinion was reinforced by Shackleton's experience on his Nimrod expedition that got to within of the pole. Shackleton used ponies. Scott planned to use ponies only to the base of the Beardmore Glacier (one-quarter of the total journey) and man-haul the rest of the journey. Scott's team had developed snow shoes for his ponies, and trials showed they could significantly increase daily progress. However, Lawrence Oates, whom Scott had made responsible for the ponies, was reluctant to use the snow shoes and Scott failed to insist on their use.\n\nThere was plenty of evidence that dogs could succeed in the achievements of William Speirs Bruce in his Arctic, Antarctic, and Scottish National Antarctic Expedition, Amundsen in the \"Gjøa\" North West passage expedition, Fridtjof Nansen's crossing of Greenland, Robert Peary's three attempts at the North Pole, Eivind Astrup's work supporting Peary, Frederick Cook's discredited North Pole expedition, and Otto Sverdrup's explorations of Ellesmere Island. Moreover, Scott ignored the direct advice he received (while attending trials of the motor sledges in Norway) from Nansen, the most famous explorer of the day, who told Scott to take \"dogs, dogs and more dogs\".\n\nAt the time of the events, the expert view in England had been that dogs were of dubious value as a means of Antarctic transport. Broadly speaking, Scott saw two ways in which dogs may be used—they may be taken with the idea of bringing them all back safe and sound, or they may be treated as pawns in the game, from which the best value is to be got regardless of their lives. He stated that if, and only if, the comparison was made with a dog sledge journey which aimed to preserve the dogs' lives, 'I am inclined to state my belief that in the polar regions properly organised parties of men will perform as extended journeys as teams of dogs.' On the other hand, if the lives of the dogs were to be sacrificed, then 'the dog-team is invested with a capacity for work which is beyond the emulation of men. To appreciate this is a matter of simple arithmetic'. But efficiency notwithstanding, he expressed \"reluctance\" to use dogs in this way: \"One cannot calmly contemplate the murder of animals which possess such intelligence and individuality, which have frequently such endearing qualities, and which very possibly one has learnt to regard as friends and companions.\"\n\nAmundsen, by contrast, took an entirely utilitarian approach. Amundsen planned from the start to have weaker animals killed to feed the other animals and the men themselves. He expressed the opinion that it was less cruel to feed and work dogs correctly before shooting them, than it would be to starve and overwork them to the point of collapse. Amundsen and his team had similar affection for their dogs as those expressed above by the English, but they \"also had agreed to shrink from nothing in order to achieve our goal\". The British thought such a procedure was distasteful, though they were willing to eat their ponies.\n\nAmundsen had used the opportunity of learning from the Inuit while on his \"Gjøa\" North West passage expedition of 1905. He recruited experienced dog drivers. To make the most of the dogs he paced them and deliberately kept daily mileages shorter than he need have for 75 percent of the journey, and his team spent up to 16 hours a day resting. His dogs could eat seals and penguins hunted in the Antarctic while Scott's pony fodder had to be brought all the way from England in their ship. It has been later shown that seal meat with the blubber attached is the ideal food for a sledge dog. Amundsen went with 52 dogs, and came back with 11.\n\nWhat Scott did not realise is a sledge dog, if it is to do the same work as a man, will require the same amount of food. Furthermore, when sledge dogs are given insufficient food they become difficult to handle. The advantage of the sledge dog is its greater mobility. Not only were the Norwegians accustomed to skiing, which enabled them to keep up with their dogs, but they also understood how to feed them and not overwork them.\n\nScott took the Norwegian pilot and skier Tryggve Gran to the Antarctic on the recommendation of Nansen to train his expedition to ski, but although a few of his party began to learn, he made no arrangements for compulsory training for the full party. Gran (possibly because he was Norwegian) was not included in the South Pole party, which could have made a difference. Gran was, one year later, the first to locate the deceased Scott and his remaining companions in their tent just some 18 km (11 miles) short of One Ton depot, that might have saved their lives had they reached it.\n\nScott would subsequently complain in his diary, while well into his journey and therefore too late to take any corrective action and after over 10 years since the Discovery expedition, that \"Skis are the thing, and here are my tiresome fellow countrymen too prejudiced to have prepared themselves for the event\".\n\nAmundsen on his side recruited a team of well experienced skiers, all Norwegians who had skied from an early age. He also recruited a champion skier, Olav Bjaaland, as the front runner. The Amundsen party gained weight on their return travel from the South Pole.\n\nScott and Shackleton's experience in 1903 and 1907 gave them first-hand experience of average conditions in Antarctica. Simpson, Scott's meteorologist 1910–1912, charted the weather during their expedition, often taking two readings a day. On their return to the Ross Ice Shelf, Scott's group experienced prolonged low temperatures from 27 February until 10 March which have only been matched once in 15 years of current records. The exceptional severity of the weather meant they failed to make the daily distances they needed to get to the next depot. This was a serious position as they were short of fuel and food. When Scott, Wilson, and Bowers died (Petty Officer Edgar Evans and Lawrence Oates had died earlier during the return from the South Pole) they were short of One-Ton Depot, which was from Corner Camp, where they would have been safe.\n\nOn the other hand, Cherry-Garrard had travelled nearly in the same area, during the same time period and same temperatures, using a dog team. Scott also blamed \"a prolonged blizzard\". But while there is evidence to support the low temperatures, there is only evidence for a \"normal\" two- to four-day blizzard, and not the ten days that Scott claims.\n\nDuring depot laying in February 1911, Roald Amundsen had his first (and last) of his route marked like a Norwegian ski course using marker flags initially every eight miles. He added to this by using food containers painted black, resulting in a marker every mile. From 82 degrees on, Amundsen built a cairn every three miles with a note inside recording the cairn's position, the distance to the next depot, and direction to the next cairn. In order not to miss a depot considering the snow and great distances, Amundsen took precautions. Each depot laid out up to 85 degrees (laid out every degree of latitude) had a line of bamboo flags laid out transversely every half-mile for five miles on either side of the depot, ensuring that the returning party could locate the designated depot.\n\nScott relied on depots much less frequently laid out. For one distance where Amundsen laid seven depots, Scott laid only two. Routes were marked by the walls made at lunch and evening stops to protect the ponies. Depots had a single flag. As a result, Scott has much concern recorded in his diaries over route finding, and experienced close calls about finding depots. It is also clear that Scott's team did not travel on several days, because the swirling snow hid their three-month-old outward tracks. With better depot and route marking they would have been able to travel on more days with a following wind which would have filled the sail attached to their sledge, and so travel further, and might have reached safety.\n\nBy the time they arrived at the pole, the health of Scott's team had significantly deteriorated, whereas Amundsen's team actually gained weight during the expedition. While Scott's team managed to maintain the scheduled pace for most of the return leg, and hence was virtually always on full rations, their condition continued to worsen rapidly. (The only delay occurred when they were held for four days by a blizzard, and had to open their summit rations early as a consequence.)\n\nApsley Cherry-Garrard in his analysis of the expedition estimated that even under optimistic assumptions the summit rations contained only a little more than half the calories actually required for the man-hauling of sledges. A carefully planned 2006 re-enactment of both Amundsen's and Scott's travels, sponsored by the BBC, confirmed Cherry-Garrard's theory. The British team had to abort their tour due to the severe weight loss of all members. The experts hinted that Scott's reports of unusually bad surfaces and weather conditions might in part have been due to their exhausted state which made them feel the sledge weights and the chill more severely.\n\nScott's calculations for the supply requirements were based on a number of expeditions, both by members of his team (e.g., Wilson's trip with Cherry-Garrard and Bowers to the Emperor penguin colony which had each man on a different type of experimental ration), and by Shackleton. Apparently, Scott didn't take the strain of prolonged man-hauling at high altitudes sufficiently into account.\n\nSince the rations contained no B and C vitamins, the only source of these vitamins during the trek was from the slaughter of ponies or dogs. This made the men progressively malnourished, manifested most clearly in the form of scurvy.\n\nScott also had to fight with a shortage of fuel due to leakage from stored fuel cans which used leather washers. This was a phenomenon that had been noticed previously by other expeditions, but Scott took no measures to prevent it. Amundsen, in contrast, had learned the lesson and had his fuel cans soldered closed. A fuel depot he left on Betty's Knoll was found 50 years later still full.\n\nDehydration may also have been a factor. Amundsen's team had plenty of fuel due to better planning and soldered fuel cans. Scott had a shortage of fuel and was unable to melt as much water as Amundsen. At the same time Scott's team were more physically active in man-hauling the sledges.\n\nIt has been said (by the present-day explorer Ranulph Fiennes amongst others) that Scott's team was appropriately dressed for man-hauling in their woolen and wind-proof clothing, and as Amundsen was skiing it was appropriate he wore furs. Skiing at the pace of a dog team is a strenuous activity. Yet Amundsen never complained about the clothing being too hot. That is because the furs are worn loosely so air circulates and sweat evaporates. Scott's team, on the other hand, made regular complaints about the cold.\n\nAmundsen's team did initially have problems with their boots. However, the depot-laying trips of January and February 1911 and an abortive departure to the South Pole on 8 September 1911 allowed changes to be made before it was too late.\n\nScott's team suffered regularly from snow blindness and sometimes this affected over half the team at any one time. By contrast, there was no recorded case of snow blindness during the whole of Amundsen's expedition. On the return journey, Amundsen's team rested during the \"day\" (when the sun was in front of them) and travelled during the \"night\" (when the sun was behind them) to minimise the effects of snow blindness.\n\nIn 1921, 'Teddy' Evans wrote in his book \"South with Scott\" that Scott had left the following written orders at Cape Evans.\n\nHe did however place a lesser importance upon this journey than that of replenishing the food rations at One Ton Depot.\n\nHe continued his instructions in the next paragraph \"You will of course understand that whilst the object of your third journey is important, that of the second is vital. At all hazards three X.S. units of provision must be got to One Ton Camp by the date named (19th January), and if the dogs are unable to perform this task, a man party must be organised.\" with that qualification he closed his notes regarding his instructions for the dogs.\n\nExpedition member Apsley Cherry-Garrard did not mention Scott's order in his 1922 book \"The Worst Journey in the World\". However, in the 1948 preface to his book, he discusses Scott's order. Cherry-Garrard writes that he and Edward Atkinson reached Cape Evans on 28 January. Scott had estimated Atkinson would reach camp by 13 January. Atkinson, now the senior officer discovered that the dog handler Cecil Meares had resigned from the expedition and that neither Meares nor anyone else had resupplied dog food to the depots. Cherry-Garrard also wrote \"In my opinion he [Atkinson] would not have been fit to take out the dogs in the first week of February\".\n\nOn 13 February, Atkinson set off on the first lap southwards to Hut Point with the dog assistant, Dimitri Gerov, and the dogs to avoid being cut off by disintegrating sea ice. Atkinson and Gerov were still at Hut Point when, on 19 February, Tom Crean arrived on foot from the Barrier and reported that Lt Edward Evans was lying seriously ill in a tent some to the south, and in urgent need of rescue. Atkinson decided that this mission was his priority, and set out with the dogs to bring Evans back. This was achieved; the party was back at Hut Point on 22 February.\n\nAtkinson sent a note back to the Cape Evans base camp requesting either the meteorologist Wright or Cherry-Garrard to take over the task of meeting Scott with the dogs. Chief meteorologist Simpson was unwilling to release Wright from his scientific work, and Atkinson therefore selected Apsley Cherry-Garrard. It was still not in Atkinson's mind that Cherry-Garrard's was a relief mission, and according to Cherry-Garrard's account, told him to \"use his judgement\" as to what to do in the event of not meeting the polar party by One Ton, and that Scott's orders were that the dogs must not be risked. Cherry-Garrard left with Gerov and the dogs on 26 February, carrying extra rations for the polar party to be added to the depot and 24 days' of dog food. They arrived at One Ton Depot on 4 March and did not proceed further south. Instead, he and Gerov, after waiting there for Scott for several days, apparently mostly in blizzard conditions (although no blizzard was recorded by Scott some 100 miles further south until 10 March), they returned to Hut Point on 16 March, in poor physical condition and without news of the polar party.\n\nOn the return journey from the pole, Scott reached the 82.30°S meeting point for the dog teams three days ahead of schedule, around 27 February 1912. Scott's diary for that day notes \"We are naturally always discussing possibility of meeting dogs, where and when, etc. It is a critical position. We may find ourselves in safety at the next depot, but there is a horrid element of doubt.\" By 10 March it became clear that the dog teams were not coming: \"The dogs which would have been our salvation have evidently failed. Meares [the dog-driver] had a bad trip home I suppose. It's a miserable jumble.\"\n\nAround 25 March, awaiting death in his tent at latitude 79.30°S, Scott speculated, in a farewell letter to his expedition treasurer Sir Edgar Speyer, that he had overshot the meeting point with the dog relief teams, writing \"We very nearly came through, and it's a pity to have missed it, but lately I have felt that we have overshot our mark. No-one is to blame and I hope no attempt will be made to suggest that we had lacked support.\" (Farewell letter to Sir Edgar Speyer, cited from Karen May 2012.)\n\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "327803", "url": "https://en.wikipedia.org/wiki?curid=327803", "title": "Crossword abbreviations", "text": "Crossword abbreviations\n\nCryptic crosswords often use abbreviations to clue individual letters or short fragments of the overall solution. These include:\n\n\nThe abbreviation is not always a short form of the word used in the clue. For example:\n\n\nTaking this one stage further, the clue word can hint at the word or words to be abbreviated rather than giving the word itself. For example:\n\n\nMore obscure clue words of this variety include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7931", "url": "https://en.wikipedia.org/wiki?curid=7931", "title": "Dictionary", "text": "Dictionary\n\nA dictionary, sometimes known as a wordbook, is a collection of words in one or more specific languages, often arranged alphabetically (or by radical and stroke for ideographic languages), which may include information on definitions, usage, etymologies, pronunciations, translation, etc. or a book of words in one language with their equivalents in another, sometimes known as a lexicon. It is a lexicographical reference that shows inter-relationships among the data.\n\nA broad distinction is made between general and specialized dictionaries. Specialized dictionaries include words in specialist fields, rather than a complete range of words in the language. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that do not fit neatly into the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), and rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a general purpose monolingual dictionary.\n\nThere is also a contrast between \"prescriptive\" or \"descriptive\" dictionaries; the former reflect what is seen as correct use of the language while the latter reflect recorded actual use. Stylistic indications (e.g. \"informal\" or \"vulgar\") in many modern dictionaries are also considered by some to be less than objectively descriptive.\n\nAlthough the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of \"astonishing\" lack of method and critical-self reflection.\n\nThe oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE \"Urra=hubullu\" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE \"Erya\", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a \"dictionary\", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary \"Disorderly Words\" (Ἄτακτοι γλῶσσαι, \"\") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the \"Nihon Shoki\", the first Japanese dictionary was the long-lost 682 CE \"Niina\" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE \"Tenrei Banshō Meigi\", was also a glossary of written Chinese. In \"Frahang-i Pahlavig\", Aramaic heterograms are listed together with their translation in Middle Persian language and phonetic transcription in Pazand alphabet. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compiled the Khaliq-e-bari which mainly dealt with Hindustani and Persian words.\nArabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the \"Lisan al-`Arab\" (13th century, still the best-known large-scale dictionary of Arabic) and \"al-Qamus al-Muhit\" (14th century) listed words in the alphabetical order of the radicals. The \"Qamus al-Muhit\" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the \"Lisan\" and the \"Oxford English Dictionary\".\nIn medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The \"Catholicon\" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's \"Dictionarium\" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the \"Thesaurus linguae latinae\" and in 1572 his son Henri Estienne published the \"Thesaurus linguae graecae\", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in Europe was the Spanish, written by Sebastián Covarrubias' \"Tesoro de la lengua castellana o española\", published in 1611 in Madrid, Spain. In 1612 the first edition of the \"Vocabolario degli Accademici della Crusca\", for Italian, was published. It served as the model for similar works in French and English. In 1690 in Rotterdam was published, posthumously, the \"Dictionnaire Universel\" by Antoine Furetière for French. In 1694 appeared the first edition of the \"Dictionnaire de l'Académie française\". Between 1712 and 1721 was published the \"Vocabulario portughez e latino\" written by Raphael Bluteau. The Real Academia Española published the first edition of the \"Diccionario de la lengua española\" in 1780, but their \"Diccionario de Autoridades\", which included quotes taken from literary works, was published in 1726. The \"Totius Latinitatis lexicon\" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.\n\nThe first edition of \"A Greek-English Lexicon\" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the \"Dizionario della lingua italiana\" by Niccolò Tommaseo. Between 1862 and 1874 was published the six volumes of \"A magyar nyelv szótára\" (Dictionary of Hungarian Language) by Gergely Czuczor and János Fogarasi. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the \"Woordenboek der Nederlandsche Taal\" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the \"Explanatory Dictionary of the Living Great Russian Language\". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. The decision to start work on the \"Svenska Akademiens ordbok\" was taken in 1787.\n\nThe earliest dictionaries in the English language were glossaries of French, Spanish or Latin words along with their definitions in English. The word \"dictionary\" was invented by an Englishman called John of Garland in 1220 — he had written a book \"Dictionarius\" to help with Latin \"diction\". An early non-alphabetical list of 8000 English words was the \"Elementarie\", created by Richard Mulcaster in 1582.\n\nThe first purely English alphabetical dictionary was \"A Table Alphabeticall\", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. This dictionary, and the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is \"a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title.\" \n\nIn 1616, John Bullokar described the history of the dictionary with his \"English Expositor\". \"Glossographia\" by Thomas Blount, published in 1656, contains more than 10,000 words along with their etymologies or histories. Edward Phillips wrote another dictionary in 1658, entitled \"The New World of English Words: Or a General Dictionary\" which boldly plagiarized Blount's work, and the two denounced each other. This created more interest in the dictionaries. John Wilkins' 1668 essay on philosophical language contains a list of 11,500 words with careful distinctions, compiled by William Lloyd. Elisha Coles published his \"English Dictionary\" in 1676.\n\nIt was not until Samuel Johnson's \"A Dictionary of the English Language\" (1755) that a more reliable English dictionary was produced. Many people today mistakenly believe that Johnson wrote the first English dictionary: a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first \"modern\" dictionary.\n\nJohnson's dictionary remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the \"Oxford English Dictionary\" in short fascicles from 1884 onwards. It took nearly 50 years to complete this huge work, and they finally released the complete \"OED\" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.\n\nIn 1806, American Noah Webster published his first dictionary, \"\". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, \"An American Dictionary of the English Language;\" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.\n\nWebster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing \"colour\" with \"color\", substituting \"wagon\" for \"waggon\", and printing \"center\" instead of \"centre\". He also added American words, like \"skunk\" and \"squash\", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.\n\nIn a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.\n\nIn many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the \"New Oxford American Dictionary\" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.\n\nAccording to the \"Manual of Specialized Lexicographies\", a specialized dictionary, also referred to as a technical dictionary, is a dictionary that focuses upon a specific subject field. Following the description in \"The Bilingual LSP Dictionary\", lexicographers categorize specialized dictionaries into three types: A multi-field dictionary broadly covers several subject fields (e.g. a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g. law), and a sub-field dictionary covers a more specialized field (e.g. constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the coverage distinction between \"minimizing dictionaries\" and \"maximizing dictionaries\", multi-field dictionaries tend to minimize coverage across subject fields (for instance, \"Oxford Dictionary of World Religions\" and \"Yadgar Dictionary of Computer and Internet Terms\") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field (\"The Oxford Dictionary of English Etymology\").\n\nAnother variant is the glossary, an alphabetical list of defined terms in a specialized field, such as medicine (medical dictionary).\n\nThe simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.\n\nLexicographers apply two basic philosophies to the defining of words: \"prescriptive\" or \"descriptive\". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling \"color\" while the rest of the English-speaking world prefers \"colour\". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)\n\nLarge 20th-century dictionaries such as the \"Oxford English Dictionary\" (OED) and \"Webster's Third\" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, and information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. \"Merriam-Webster\" is subtle, only adding italicized notations such as, \"sometimes offensive\" or \"stand\" (nonstandard). \"American Heritage\" goes further, discussing issues separately in numerous \"usage notes.\" \"Encarta\" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, \"an offensive term for...\" or \"a taboo term meaning...\".\n\nBecause of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to \"El otro, el mismo\": \"It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature.\"\n\nSometimes the same dictionary can be descriptive in some domains and prescriptive in others. For example, according to Ghil'ad Zuckermann, the \"Oxford English-Hebrew Dictionary\" is \"at war with itself\": whereas its coverage (lexical items) and glosses (definitions) are descriptive and colloquial, its vocalization is prescriptive. This internal conflict results in absurd sentences such as \"hi taharóg otí kshetiré me asíti lamkhonít\" (she'll tear me apart when she sees what I've done to the car). Whereas \"hi taharóg otí\", literally 'she will kill me', is colloquial, me (a variant of ma 'what') is archaic, resulting in a combination that is unutterable in real life.\n\nA historical dictionary is a specific kind of descriptive dictionary which describes the development of words and senses over time, usually using citations to original source material to support its conclusions.\n\nIn contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. The final user is a human being but the direct user is a program. Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex network (see Diathesis alternation). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.\n\n\nIn many languages, such as the English language, the pronunciation of some words is not consistently apparent from their spelling. In these languages, dictionaries usually provide the pronunciation. For example, the definition for the word \"dictionary\" might be followed by the International Phonetic Alphabet spelling . American English dictionaries often use their ownpronunciation respelling systems with diacritics, for example \"dictionary\" is respelled as \"dĭk′shə-nĕr′ē\" in the American Heritage Dictionary. The IPA is more commonly used within the British Commonwealth countries. Yet others use their own pronunciation respelling systems without diacritics: for example, \"dictionary\" may be respelled as . Some online or electronic dictionaries provide audio recordings of words being spoken.\n\nHistories and descriptions of the dictionaries of other languages on Wikipedia include:\n\n\nThe age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that \"Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well.\"\nThere exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:\n\n\n\n"}
{"id": "1902180", "url": "https://en.wikipedia.org/wiki?curid=1902180", "title": "Digital reference", "text": "Digital reference\n\nDigital reference (or virtual reference) is a service by which a library reference service is conducted online, and the reference transaction is a computer-mediated communication. It is the remote, NextNextcomputer-mediated delivery of reference information provided by library professionals to users who cannot access or do not want face-to-face communication. Virtual reference service is most often an extension of a library's existing reference service program. The word \"reference\" in this context refers to the task of providing assistance to library users in finding information, answering questions, and otherwise fulfilling users’ information needs. Reference work often but not always involves using reference works, such as dictionaries, encyclopedias, etc. This form of reference work expands reference services from the physical reference desk to a \"virtual\" reference desk where the patron could be writing from home, work or a variety of other locations.\n\nThe terminology surrounding virtual reference services may involve multiple terms used for the same definition. The preferred term for remotely delivered, computer-mediated reference services is \"virtual reference\", with the secondary non-preferred term \"digital reference\" having gone out of use in recent years. \"Chat reference\" is often used interchangeably with virtual reference, although it represents only one aspect of virtual reference. Virtual reference includes the use of both synchronous (i.e., IM, videoconferencing) and asynchronous communication (i.e., texting and email). Here, \"synchronous virtual reference\" refers to any real-time computer-mediated communication between patron and information professional. Asynchronous virtual reference is all computer-mediated communication that is sent and received at different times.\n\nThe earliest digital reference services were launched in the mid-1980s, primarily by academic and medical libraries, and provided by e-mail. These early-adopter libraries launched digital reference services for two main reasons: to extend the hours that questions could be submitted to the reference desk, and to explore the potential of campus-wide networks, which at that time was a new technology.\n\nWith the advent of the graphical World Wide Web, libraries quickly adopted webforms for question submission. Since then, the percentage of questions submitted to services via webforms has outstripped the percentage submitted via email.\n\nIn the early- to mid-1990s, digital reference services began to appear that were not affiliated with any library. These digital reference services are often referred to as \"AskA\" services. Examples of AskA services are the Internet Public Library, Ask Dr. Math, and Ask Joan of Art.\n\nProviding remote-based services for patrons has been a steady practice of libraries over the years. For example, before the widespread use of chat software, reference questions were often answered via phone, fax, email and audio conferencing. Email is the oldest type of virtual reference service used by libraries. Library services in America and the UK are just now gaining visibility in their use of virtual reference services using chat software. However, a survey in America revealed that by 2001 over 200 libraries were using chat reference services. \nThe rapid global proliferation of information technology (IT) often leaves libraries at a disadvantage in terms of keeping their services current. However, libraries are always striving to understand their user demographics in order to provide the best possible services. Therefore, libraries continue to take notes from current cyberculture and are continually incorporating a diversified range of interactive technologies in their service repertoires. Virtual reference represents only one small part of a larger library mission to meet the needs of a new generation, sometimes referred to as the \"Google Generation\", of users who have grown up with the internet. For instance, virtual reference may be used in conjunction with embedded Web 2.0 (online social media such as Facebook, YouTube, blogs, del.icio.us, Flickr, etc.) applications in a library's suite of online services. As technological innovations continue, libraries will be watching to find new, more personalized ways of interacting with remote reference users.\n\nThe range of cost-per-transaction of reference interactions has been found to be large, due to the differences in librarian salaries and infrastructural costs required by reference interviews.\n\nWebforms are created for digital reference services in order to help the patron be more productive in asking their question. This document helps the librarian locate exactly what the patron is asking for. Creation of webforms requires design consideration. Because webforms substitute for the reference interview, receiving as much information as possible from the patron is a key function.\n\nAspects commonly found within webforms:\n\n\nSeveral applications exist for providing chat-based reference. Some of these applications are: QuestionPoint, OmniReference, Tutor.com, LibraryH3lp, AspiringKidz.com, and Vienova.com. These applications bear a resemblance to commercial help desk applications. These applications possess functionality such as: chat, co-browsing of webpages, webpage and document pushing, customization of pre-scripted messages, storage of chat transcripts, and statistical reporting.\n\nInstant messaging (IM) services are used by some libraries as a low-cost means of offering chat-based reference, since most IM services are free. Utilizing IM for reference services allows a patron to contact the library from any location via the internet. This service is like the traditional reference interview because it is a live interaction between the patron and the librarian. On the other side the reference interview is different because the conversation does not float away but instead is in print on the screen for the librarian to review if needed to better understand the patron. IM reference services may be for the use of in-house patrons as well as patrons unable to go to the library. If library computers support IM chat programs, patrons may IM from within the library to avoid losing their use of a computer or avoid making embarrassing questions public.\n\nSuccessful IM reference services will:\n\nAt times, IM becomes challenging because of lack of non-verbal cues such as eye contact, and the perceived time pressure. Moreover, formulating the question online without the give and take of nonverbal cues and face to face conversation presents an added obstacle. In addition, to provide effective reference service through IM, it is important to meet higher level of information literacy standards. These standards include evaluating the information and its source, synthesizing the information to create new ideas or products, and understanding the societal, legal, and economic issues surrounding its use.\n\nThe article Live, Digital Reference Marketplace by Buff Hirko contains a comparison of the features of applications for chat-based reference.\n\nSee the entries in the Library Success Wiki's Online Reference Section, including software recommended for web-based chat reference, IM reference, SMS (text messaging) reference, and other types like digital audio or video reference.\n\nVirtual service software programs offered by libraries are often unique, and tailored to the individual library's needs. However, each program may have several distinct features. A knowledge base is a chunk of information that users can access independently. An example of this is a serialized listing of frequently asked questions (FAQ) that a user can read and use at his or her leisure.\n\nOnline chat, or instant messaging (IM) has become a very popular Web-based feature. Instant messaging is a real time conversation that utilizes typed text instead of language. Users may feel a sense of satisfaction with the use of this tool because of their personalized interaction with staff.\n\nThe use of electronic mail (email) in responding to reference questions in libraries has been in use for years. Also, in some cases with the IM feature, a question may be asked that cannot be resolved in online chat. In this instance the staff member may document the inquiring patron’s email address and will the user a response.\n\nWith the increase in use of text messaging (Short Message Service or SMS), some libraries are also adopting text messaging in their virtual reference services. Librarians can use mobile phones, text-to-instant messaging or web-based services to respond to reference questions via text messaging.\n\nCo-browsing, or cooperative browsing, is a virtual reference function that involves interactive control of a user’s web browser. This function enables the librarian to see what the patron has on his or her computer screen. Several types of co-browsing have been offered in mobile devices of late; libraries may have software that incorporates dual modes of co-browsing in a variety of formats. For instance, it is possible to browse on a mobile device within and between documents (such as Word), webpages, and images.\n\nVirtual reference services are growing in popularity in the UK with more institutions accepting queries via email, instant messaging and other chat based services. A study of the use of virtual reference within UK academic institutions showed that 25% currently offer a form of virtual reference, with 54% of academic institutions surveyed considering adding this service.\n\nUK public libraries were instrumental in some of the first steps towards UK-wide internet collaboration amongst libraries with the EARL Consortium (Electronic Access to Resources in Libraries) in 1995, in a time where internet access was a rare commodity for both library staff and the public. Resources were collated and lines of communication opened between libraries across the UK, paving the way for services all over the world to follow suit. There are now a number of area-specific reference services across the UK including Ask A Librarian (UK-wide, established in 1997), Ask Cymru (Welsh and English language service), Enquire (Government funded through the People's Network, also UK-wide), and Ask Scotland. Ask Scotland was created by the Scottish Government's advisory body on libraries, SLIC (Scottish Library and Information Council), and funded by the Public Library Quality Improvement Fund (PLQIF) in June 2009. It uses the Online Computer Library Center's QuestionPoint software.\n\nThe definition formulated by the American Library Association's (ALA) 2004 MARS Digital Reference Guidelines Ad Hoc Committee contains three components:\n\n\nIn January 2011 QuestionPoint and the American Library Association were in talks about offering a National Ask A Librarian service across the whole United States of America. At present the Ask services in the US are run at a local level.\n\nIn Europe some countries offer services in both their own national language and in English. European countries include: Finland, the Netherlands (in Dutch only), Denmark, and France.\n\nOther countries which offer virtual reference services include: Australia, New Zealand, Canada, and the state of Colorado in the United States.\n\nA collaboration between UK and Australian library services, entitled Chasing the Sun, has been initiated using QuestionPoint software so that an all-hours digital reference chat service can be offered. Targeted at health libraries where reference queries from health professionals could occur at any time of the day or night due to medical emergencies, the collaboration between the two countries means that someone will be on hand to field the query at any time. Although the UK libraries involved are currently based in England the programme may expand to other countries and health services if successful.\n\n\n\n\nThe following provide software and technology infrastructure for digital/virtual reference.\n\n\n\n\n\n"}
{"id": "1054566", "url": "https://en.wikipedia.org/wiki?curid=1054566", "title": "Ditloid", "text": "Ditloid\n\nA ditloid is a type of word puzzle, in which a phrase, quotation, date, or fact must be deduced from the numbers and abbreviated letters in the clue. Common words such as 'the', 'in', 'a', 'an', 'of', 'to', etc. are not normally abbreviated. The name 'ditloid' was given by the \"Daily Express\" newspaper, originating from the clue: 1 = DitLoID ≡ \"1 Day in the Life of Ivan Denisovich\".\n\nWill Shortz originated the current form of this puzzle and first published it in the May–June 1981 issue of \"Games\" magazine, calling it the Equation Analysis Test. In its annual 1981 issue of \"What's hot and what's not,\" \"Us\" magazine named the Equation Analysis Test in the \"what's hot\" category – the only nonperson so recognized. Shortz reports:\nSome anonymous person had retyped the puzzle from \"Games\" (word for word, except for my byline),\nphotocopied it, and passed it along. This page was then rephotocopied ad infinitum, like a chain letter,\nand circulated around the country. \"Games\" readers who hadn't seen the original even started sending\nit back to \"Games\" as something the magazine ought to consider publishing!\nShortz based the puzzle on the Formula Analysis Test - Revised Form published in Morgan Worthy's 1975 book \"AHA! A Puzzle Approach to Creative Thinking\" (Chicago: Nelson Hall). Worthy's equations were in a different format, for example:\n\nWorthy gives the source of his inspiration and speculates about the perennial popularity\nof this puzzle:\nI got the idea for linguistic equations from graffiti someone had\nwritten in the form of an obscene formula on a restroom wall at the\nUniversity of Florida. When the answer suddenly came to me, I realized\nthe format was a good one for eliciting the \"aha effect\". After that I\nused such items as exercise material when teaching workshops on\ncreative thinking.\nMy guess is that one reason a person enjoys linguistic equations is\nthat the answer hits him or her all at once rather than being solved in\nan incremental fashion. It is similar to what happens when we suddenly\nsee an embedded figure pop into focus; the satisfaction is visceral\nrather than just intellectual. My experience was that people often had\nthe answer to an item come to them when they were not consciously\nthinking about the puzzles, but relaxed, such as in the shower or about\nto fall asleep.\nAnother factor is that with well-written items, success does not hinge\non obscure information. Ideally, a person should never have to feel, \"I\ncould never have gotten that one no matter how long I worked on it.\"\nThere is something ego enhancing about knowing you have the answer\ninside and just need to find it.\n"}
{"id": "2330597", "url": "https://en.wikipedia.org/wiki?curid=2330597", "title": "Errors and omissions excepted", "text": "Errors and omissions excepted\n\nErrors and omissions excepted (E&OE) is a phrase used in an attempt to reduce legal liability for potentially incorrect or incomplete information supplied in a contractually related document such as a quotation or specification.\n\nIt is often applied as a disclaimer in situations in which the information to which it is applied is relatively fast-moving. In legal terms, it seeks to make a statement that information cannot be relied upon, or may have changed by the time of use.\n\nIt is regularly used in accounting, to \"excuse slight mistakes or oversights.\"\n\nIt is also used when a large amount of information is listed against a product, to state that—to the best of the supplier's knowledge—the information is correct, but that they will not be held responsible if an error has been committed.\n"}
{"id": "57442907", "url": "https://en.wikipedia.org/wiki?curid=57442907", "title": "Handbook of Middle American Indians", "text": "Handbook of Middle American Indians\n\nHandbook of Middle American Indians (HMAI) is a sixteen-volume compendium on Mesoamerica , from the prehispanic to the late twentieth century. Volumes on particular topics were published from the 1960s and 1970s under the general editorship of Robert Wauchope. Separate volumes with particular volume editors deal with a number of general topics, including archeology, cultural anthropology, physical anthropology, linguistics, with the last four substantive volumes treating various topics in Mesoamerican ethnohistory, under the editorship of Howard F. Cline. Select volumes have become available in e-book format.\n\nA retrospective review of the HMAI by two anthropologists discusses its history and evaluates it. One review calls it a fundamental work. Another reviewer says \"since the first volume of the HMAI appeared in 1964 is far and away the most comprehensive and erudite coverage of native cultures of any region in the Americas.\" A review in the journal \"Science\" says that \"There can be little doubt that, like the \"Handbook of South American Indians\", this monumental synthesis will provide a sound basis for new generalizations and will stimulate additional research to fill the gaps in knowledge and understanding that will become apparent.\n\nStarting in 1981, six volumes in the Supplement to the Handbook of Middle American Indians were published under the general editorship of Victoria Bricker.\n\nVolume 1. Natural Environment and Early Cultures, Robert C. West, volume editor. 1. Geohistory and Paleogeography of Middle America (Manuel Maldonado-Koerdell); 2. Surface Configuration and Associated Geology of Middle America (Robert C. West); 3. The Hydrography of Middle America (Jorge L. Tamayo, in collaboration with Robert C. West); 4. The American Mediterranean (Albert Collier); 5. Oceanography and Marine Life along the Pacific Coast (Carl L. Hubbs and Gunnar I. Roden); 6. Weather and Climate of Mexico and Central America (Jorge A. Vivo Escoto); 7. Natural Vegetation of Middle America (Philip L. Wagner); 8. The Soils of Middle America and their Relation to Indian Peoples and Cultures (Rayfred L. Stevens); 9. Fauna of Middle America (L. C. Stuart); 10. The Natural Regions of Middle America (Robert C. West); 11. The Primitive Hunters (Luis Aveleyra Arroyo de Anda); 12. The Food-gathering and Incipient Agriculture Stage of Prehistoric Middle America (Richard S. MacNeish); 13. Origins of Agriculture in Middle America (Paul C. Mangelsdorf, Richard S. MacNeish, and Gordon R. Willey); 14. The Patterns of Farming Life and Civilization (Gordon R. Willey, Gordon F. Ekholm, and Rene F. Millon)\n\nVolumes 2-3. Archeology of Southern Mesoamerica, Gordon R. Wiley, volume editor.\n\nVolume 4. ‘’Archeological Frontiers and External Connections G.F. Ekholm and G. R. Wiley, volume editors.\n\nVolume 5. ‘’Linguistics, Norman A. McQuown, volume editor.\n\nVolume 6. Social Anthropology, Manning Nash, volume editor. 1.Introduction, Manning Nash; 2. Indian Population and its Identification, Anselmo Marino Flores; 3.Agricultural Systems and Food Patterns, Angel Palerm; 4. Settlement Patterns, William T. Sanders; 5. Indian Economies, Manning Nash; 6. Contemporary Pottery and Basketry, George M. Foster; 7. Laquer, Katharine D. Jenkins; 8. Textiles and Costume, A.H. Gayton; 9. Drama, Dance and Music, Gertrude Prokosch Kurath; 10. Play: Games, Gossip, and Humor; 11. Kinship and Family, A. Kimball Romney; 12. Compadrinazgo, Robert Ravicz; 13. Local and Territoria Units, Eva Hunt and June Nash; 14. Political and Religious Organizations, Frank Cancian; 15. Levels of Communal Relations, Eric R. Wolf; 16. Annual Cycle and Fiesta Cycle, Ruben E. Reina; 17. Sickness and Social Relations, Richard N. Adams and Arthur J. Rubel; 18. Narrative Folklore, Munro S. Edmonson; 19. Religious Syncretism, William Madsen; 20. Ritual and Mythology, E. Michael Mendelson; 21. Psychological Orientations, Benjamin N. Colby; 22. Ethnic Relationships, Julio de la Fuente; 23. Acculturation, Ralph L. Beals; 24. Nationalization, Richard N. Adams; 25. Directed Change, Robert H. Ewald; 26. Urbanization and Industrialization, Arden R. King\n\nVolumes 7-8, Ethnology, Evan Z. Vogt, volume editor. Volume 7. Introduction (Evon Z. Vogt)Section I: The Maya 2; The Maya: Introduction (Evon Z. Vogt); 3. Guatemalan Highlands (Manning Nash); 4. The Maya of Northwestern Guatemala (Charles Wagley); 5. The Maya of the Midwestern Highlands (Sol Tax and Robert Hinshaw); 6. Eastern Guatemalan Highlands: The Pokomames and Chorti (Ruben E. Reina); 7. Chiapas Highlands (Evon Z. Vogt); 8. The Tzotzil (Robert M. Laughlin); 9. The Tzeltal (Alfonso Villa Rojas); 10. The Tojolabal (Roberta Montagu); 11. Maya Lowlands: The Chontal, Chol, and Kekchi (Alfonso Villa Rojas); 12. The Maya of Yucatan (Alfonso Villa Rojas); 13. The Lacandon (Gertrude Duby and Frans Blom); 14. The Huastec (Robert M. Laughlin); Section II: Southern Mexican Highlands and Adjacent Coastal Regions15. Southern Mexican Highlands and Adjacent Coastal Regions: Introduction (Ralph L. Reals); 16. The Zapotec of Oaxaca (Laura Nader); 17. The Chatino (Gabriel DeCicco); 18. The Mixtec (Robert Ravicz and A. Kimball Romney); 19. The Trique of Oaxaca (Laura Nader);20. The Amuzgo (Robert Ravicz and A. Kimball Romney); 21. The Cuicatec (Roberto J. Weitlaner); 22. The Mixe, Zoque, and Popoluca (George M. Foster); 23. The Huave (A. Richard Diebold, Jr.); 24. The Popoloca (Walter A. Hoppe, Andres Medina, and Roberto J. Weitlaner); 25. The Ichcatec (Walter A. Hoppe and Roberto J. Weitlaner); 26. The Chocho (Walter A. Hoppe and Roberto J. Weitlaner); 27. The Mazatec (Roberto J. Weitlaner and Walter A. Hoppe); 28. The Chinantec (Roberto J. Weitlaner and Howard F. Cline); 29. The Tequistlatec and Tlapanec (D. L. Olmsted); 30. The Cuitlatec (Susana Drucker, Roberto Escalante, and Roberto J. Weitlaner); Volume 8, Section III: Central Mexican Highlands; 31. Central Mexican Highlands: Introduction (Pedro Carrasco); 32. The Nahua (William Madsen); 33. The Totonac (H. R. Harvey and Isabel Kelly); 34. The Otomi (Leonardo Manrique C.); Section IV: Western Mexico 35. The Tarascans (Ralph L. Beals); Section V: Northwest Mexico; 36. Northwest Mexico: Introduction (Edward H. Spicer); 37. The Huichol and Cora (Joseph E. Grimes and Thomas B. Hinton); 38. The Southern Tepehuan and Tepecano (Carroll L. Riley); 39. The Northern Tepehuan (Elman R. Service); 40. The Yaqui and Mayo (Edward H. Spicer); 41. The Tarahumara (Jacob Fried); 42. Contemporary Ethnography of Baja California, Mexico (Roger C. Owen); 43. Remnant Tribes of Sonora: Opata, Pima, Papago, and Seri (Thomas B. Hinton).\n\nVolumes 6 & 7 were reviewed when the appeared. One reviewer highlights several articles, including those by Eric R. Wolf, Angel Palerm, and Willilam Sanders, but he goes on to say \"These volumes are ... more valuable for reference than for reading. Sections dealing with distribution, history, and bibliography are very useful, but sections dealing with social structure or the character of the peoples generally fail to provide integrated analyses indicating the essential features.\"\n\nVolume 9. Physical Anthropology, T.D. Stewart, volume editor.\n\nVolume 10-11. Archeology of Northern Mesoamerica, G. F. Ekholm and Ignacio Bernal, volume editors.\n\nVolumes 12-15, Guide to Ethnohistorical Sources, Howard F. Cline, Volume editor.\n\nVolume 12, Guide to Ethnohistorical Sources, Part 1. (1972) 1.“Introductory Notes on Territorial Divisions of Middle America” , Howard F. Cline, pp. 17–62; 2. “Colonial New Spain, 1519-1786: Historical Notes on the Evolution of Minor Political Jurisdictions”, Peter Gerhard, pp. 63–137; 3. “Viceroyalty to Republics, 1786-1952: Historical Notes on the Evolution of Middle American Political Units,” Howard F. Cline, pp. 138–165; 4.“Ethnohistorical Regions of Middle America,” Howard F. Cline, pp. 166–182; 5.“The \"Relaciones Geográficas\" of the Spanish Indies, 1577-1648,” Howard F. Cline, pp. 183–242; 6.“The Pinturas (Maps) of the Relaciones Geográficas, with Catalogue,” Donald Robertson, pp. 243–278; 7.“The Relaciones Geográficas, 1579-1586: Native Languages,” H.R. Harvey, pp. 279–323; 8.“A Census of the Relaciones Geográficas of New Spain, 1579-1612,” Howard F. Cline, pp. 324–369; 9.“The Relaciones Geográficas of Spain, New Spain, and the Spanish Indies: An Annotated Bibliography,” Howard F. Cline, pp. 370–395; 10.“The Relaciones Geográficas of Mexico and Central America, 1740-1792,” Robert C. West, pp. 396–452.\nVolume 13. Guide to Ethnohistorical Sources, Part 2. (1973) 11, “Published Collections of Documents Relating to Middle American Ethnohistory”, Charles Gibson; 12, “An Introductory Survey of Secular Writings in the European Tradition on Colonial Middle America, 1503-1818,” J. Benedict Warren, pp. 42–137; 13. “Religious Chronicles and Historians: A Summary and Annotated Bibliography,” Ernest J. Burrus, S.J.; 14. “Bernardino de Sahagún, 1499-1590A. “Sahagún and His Works,” Nicolau d’Olwer and Howard F. Cline, 186-206; B. “Sahagún’s “Primeros Memoriales.” Tepepulco, H. B. Nicholson, pp. 207–217; C. “Sahagún’s Materials and Studies,” Howard F. Cline, pp. 218–239; 15. “Antonio de Herrera, 1549-1625,” Manuel Ballesteros Gaibrois, pp. 240–255; 16. “Juan de Torquemada, 1564-1624,” José Alcina Franch, pp. 256–275; 17. “Francisco Javier Clavigero, 1731-1787, “ Charles F. Ronan, S. J., pp. 276–297; 18. “Charles Etienne Brasseur de Bourbourg, 1814-1874,” Carroll Edward Mace, pp. 298–325; 19. “Hubert Howe Bancroft, 1832-1918,” Howard F. Cline, pp. 326–347; 20. “Eduard Georg Seler, 1849-1922,” H. B. Nicholson, pp. 348–369; 21, “Select Nineteenth-Century Mexican Writers on Ethnohistory,” Howard F. Cline, pp. 370–403. Carlos María de Bustamante, José Fernando Ramírez, Manuel Orozco y Berra, Joaquín García Icazbalceta, Alfredo Chavero, Francisco del Paso y Troncoso\n\nVolume 14. Guide to Ethnohistorical Sources Part 3. (1975) 22. “A Survey of Native Middle American Pictorial Manuscripts,” John B. Glass, pp. 3–80; 23. “A Census of Native Middle American Pictorial Manuscripts,” John B. Glass with Donald Robertson, pp. 81–252; 24. “Techialoyan Manuscripts and Paintings with a Catalog,” Donald Robertson, pp. 253–280; 25. “A Census of Middle American Testerian Manuscripts,” John B. Glass, pp. 281–296; 26. “A Catalogue of Falsified Middle American Pictorial Manuscripts,” John B. Glass, pp. 297–309; Illustrations and maps, 1-103\n\nVolume 15. Guide to Ethnohistorical Sources Part 4. (1975) 27A. “Prose Sources in the Native Historical Tradition,” Charles Gibson, pp 312–319; 27B. “A Census of Middle American Prose Manuscripts in the Native Historical Tradition,” Charles Gibson and John B. Glass, pp. 322–400; 28. “A Checklist of Institutional Holdings of Middle American Manuscripts in the Native Historical Tradition,” John B. Glass, pp. 401–472; 29. “The Boturini Collection,” John B. Glass, pp. 473–486; 30. “Middle American Ethnohistory: An Overview,” H. B. Nicholson, pp. 487–505; 31.”Index of Authors, Titles, and Synonyms,” John B. Glass, pp. 506–536; 32. “Annotated References,” John B. Glass, pp. 537–724.\nVolume 16. Handbook of Middle American Indians. Margaret A.L. Harrison, volume editor. (1976) – Bibliography for all volumes.\n\nGeneral Editor, Victoria Bricker\n\n"}
{"id": "190975", "url": "https://en.wikipedia.org/wiki?curid=190975", "title": "Ibid.", "text": "Ibid.\n\nIbid is an abbreviation for the Latin word \"ibīdem\", meaning \"in the same place\", commonly used in an endnote, footnote, bibliography citation, or scholarly reference to refer to the source cited in the preceding note or list item. This is similar to \"īdem\", literally meaning \"the same\", abbreviated \"Id.\", which is commonly used in legal citation.\n\nIbid. may also be used in the Harvard (name-date) system for in-text references where there has been a close previous citation from the same source material. The previous reference should be immediately visible, e.g. within the same paragraph or page. Some academic publishers now prefer that \"ibid.\" not be italicized, as it is a commonly found term.\n\nSince ibid. is an abbreviation where the last two letters of the word are omitted, it takes a full stop (period) in both British and American usage.\n\nReference 2 is the same as reference 1: E. Vijh, \"Latin for Dummies\" on page 23, whereas reference 3 refers to the same work but at a different location, namely page 29. Intervening entries require a reference to the original citation in the form Ibid. <citation #>, as in reference 5.\n\n\n\n"}
{"id": "6487324", "url": "https://en.wikipedia.org/wiki?curid=6487324", "title": "Leishu", "text": "Leishu\n\nThe leishu () is a genre of reference books historically compiled in China and other countries of the Sinosphere. The term is generally translated as \"encyclopedia\", although the \"leishu\" are quite different from the modern notion of encyclopedia.\n\nThe \"leishu\" are composed of sometimes lengthy citations from other works, and often contain copies of entire works, not just excerpts. The works are classified by a systematic set of categories, which are further divided into subcategories. \"Leishu\" may be considered anthologies, but are encyclopedic in the sense that they may comprise the entire realm of knowledge at the time of compilation.\n\nApproximately 600 \"leishu\" were compiled from the early third century until the eighteenth century, of which 200 have survived. The largest \"leishu\" ever compiled was the 1408 \"Yongle Dadian\", containing 370 million Chinese characters, and the largest ever printed was the \"Gujin Tushu Jicheng\", containing 100 million characters and 852,408 pages.\n\nThe genre first appeared in the early third century. The earliest known was the \"Huanglan\" (\"Emperor's mirror\"). Sponsored by the emperor of Cao Wei, it was compiled around 220, but has since been lost. However, the term \"leishu\" was not used until the Song dynasty (960–1279).\n\nIn later imperial China dynasties, such as the Ming and Qing, emperors sponsored monumental projects to compile all known human knowledge into a single \"leishu\", in which entire works, rather than excerpts, were copied and classified by category. The largest \"leishu\" ever compiled, on the order of the Yongle Emperor of Ming, was the \"Yongle Dadian\" containing a total of 370 million Chinese characters. The project involved 2,169 scholars, who worked for four years under general editor Yao Guangxiao. It was completed in 1408, but never printed, as the imperial treasury had run out of money.\n\nThe \"Qinding Gujin Tushu Jicheng\" (Imperially approved synthesis of books and illustrations past and present) is by far the largest \"leishu\" ever printed, containing 100 million characters and 852,408 pages. It was compiled by a team of scholars led by Chen Menglei, and printed between 1726 and 1728, during the Qing dynasty.\n\nThe \"riyong leishu\" (encyclopedias for daily use), containing practical information for people who were literate but below the Confucian elite, were also compiled in the later imperial era. Today, they provide scholars with valuable information on non-elite culture and attitudes.\n\nAccording to Jean-Pierre Diény, the Jiaqing reign (1796–1820) of the Qing dynasty saw the end of the publication of \"leishu\".\n\nOther countries of the Sinosphere also adopted the genre of \"leishu\". In 1712, the \"Sancai Tuhui\", a richly illustrated \"leishu\" compiled by Ming scholar Wang Qi (王圻) in the early 17th century, was printed in Japan as \"Wakan Sansai Zue\". The Japanese version was edited by Terajima Ryōan (寺島良安), a physician born in Osaka.\n\nThe \"leishu\" have played an important role in the preservation of ancient works, many of which have been lost, only preserved completely or partially as part of a \"leishu\" compilation. The 7th-century \"Yiwen Leiju\" is especially valuable. It contains excerpts from 1,400 pre-7th century works, 90% of which have been otherwise lost. Even though the \"Yongle Dadian\" is itself largely lost, the remnants still contain 385 complete books that have been otherwise lost. The \"leishu\" also provide a unique view of the transmission of knowledge and education, and an easy way to locate traditional materials on any given subject.\n\nApproximately 600 \"leishu\" were compiled, from the Cao Wei period (early third century) until the 18th century, of which 200 have survived. Among the most important, in chronological order, are:\n\n\n"}
{"id": "4807639", "url": "https://en.wikipedia.org/wiki?curid=4807639", "title": "Microsoft Bookshelf", "text": "Microsoft Bookshelf\n\nMicrosoft Bookshelf was a reference collection introduced in 1987 as part of Microsoft's extensive work in promoting CD-ROM technology as a distribution medium for electronic publishing. The original MS-DOS version showcased the massive storage capacity of CD-ROM technology, and was accessed while the user was using one of 13 different word processor programs that Bookshelf supported. Subsequent versions were produced for Windows and became a commercial success as part of the Microsoft Home brand. It was often bundled with personal computers as a cheaper alternative to the Encarta Suite. The Encarta Deluxe Suite / Reference Library versions also bundled Bookshelf.\n\nMicrosoft Bookshelf was discontinued in 2000. In later editions of the Encarta suite (Encarta 2000 and onwards), Bookshelf was replaced with a dedicated \"Encarta Dictionary\", a superset of the printed edition. There has been some controversy over the decision, since the dictionary lacks the other books provided in Bookshelf which many found to be a useful reference, such as the dictionary of quotations (replaced with a quotations section in \"Encarta\" that links to relevant articles and people) and the Internet Directory, although the directory is now a moot point since many of the sites listed in offline directories no longer exist.\n\nThe original 1987 edition contained \"The Original Roget's Thesaurus of English Words and Phrases\", \"The American Heritage Dictionary of the English Language\", World Almanac and Book of Facts, Bartlett's Familiar Quotations, The Chicago Manual of Style (13th Edition), the U.S. ZIP Code Directory, Houghton Mifflin Usage Alert, Houghton Mifflin Spelling Verifier and Corrector, Business Information Sources, and Forms and Letters. Titles in non-US versions of Bookshelf were different. For example, the 1997 UK edition included the Chambers Dictionary, Bloomsbury Treasury of Quotations, and Hutchinson Concise Encyclopedia.\n\nThe Windows release of Bookshelf added a number of new reference titles, including \"The Concise Columbia Encyclopedia\" and an Internet Directory. Other titles were added and some were dropped in subsequent years. By 1994, the English-language version also contained the \"Columbia Dictionary of Quotations\"; \"The Concise Columbia Encyclopedia\"; the \"Hammond Intermediate World Atlas\"; and \"The People's Chronology\". By 2000, the collection came to include the \"Encarta Desk Encyclopedia\", the \"Encarta Desk Atlas\", the \"Encarta Style Guide\" and a specialized \"Computer and Internet Dictionary\" by Microsoft Press.\n\nBookshelf 1.0 used a proprietary hypertext engine that Microsoft acquired when it bought the company Cytation in 1986. Also used for Microsoft Stat Pack and Microsoft Small Business Consultant, it was a Terminate and Stay Resident (TSR) program that ran alongside a dominant program, unbeknownst to the dominant program. Like Apple's similar Hypercard reader, Bookshelf engine's files used a single compound document, containing large numbers of subdocuments (\"cards\" or \"articles\"). They both differ from current browsers which normally treat each \"page\" or \"article\" as a separate file.\n\nThough similar to Apple's Hypercard reader in many ways, the Bookshelf engine had several key differences. Unlike Hypercard files, Bookshelf files required compilation and complex markup codes. This made the files more difficult to pirate, addressing a key concern of early electronic publishers. Furthermore, Bookshelf's engine was designed to run as fast as possible on slow first-generation CD-ROM drives, some of which required as much as a half-second to move the drive head. Such hardware constraints made Hypercard impractical for high-capacity CD-ROMs. Bookshelf also had full text searching capability, which made it easy to find needed information.\n\nCollaborating with DuPont, the Microsoft CD-ROM division developed a Windows version of its engine for applications as diverse as document management, online help, and a CD-ROM encyclopedia. In a skunkworks project, these developers worked secretly with Multimedia Division developers so that the engine would be usable for more ambitious multimedia applications. Thus they integrated a multimedia markup language, full text search, and extensibility using software objects, all of which are commonplace in modern internet browsing.\n\nIn 1992, Microsoft started selling the Bookshelf engine to third-party developers, marketing the product as Microsoft Multimedia Viewer. The idea was that such a tool would help a burgeoning growth of CD-ROM titles that would spur demand for Windows. Although the engine had multimedia capabilities that would not be matched by Web browsers until the late 1990s, Microsoft Viewer did not enjoy commercial success as a standalone product. However, Microsoft continued to use the engine for its Encarta and WinHelp applications, though the multimedia functions are rarely used in Windows help files.\n\nIn 1993, the developers who were working on the next generation viewer were moved to the Cairo systems group which was charged with delivering Bill Gates' 'vision' of 'Information at your fingertips'. This advanced browser was a fully componentized application using what are now known as Component Object Model objects, designed for hypermedia browsing across large networks and whose main competitor was thought to be Lotus Notes. Long before Netscape appeared, this team, known as the WEB (web enhanced browser) team had already shipped a network capable hypertext browser capable of doing everything that HTML browsers would not be able to do until the turn of the century. Nearly all technologies of Cairo shipped. The WEB browser was not one of them, though it influenced the design of many other common Microsoft technologies.\n\n\"BYTE\" in 1989 listed Microsoft Bookshelf as among the \"Excellence\" winners of the BYTE Awards, stating that it \"is the first substantial application of CD-ROM technology\" and \"a harbinger of personal library systems to come\".\n\n"}
{"id": "6908619", "url": "https://en.wikipedia.org/wiki?curid=6908619", "title": "Museum of Comparative Zoology", "text": "Museum of Comparative Zoology\n\nThe Museum of Comparative Zoology, full name \"The Louis Agassiz Museum of Comparative Zoology\", often abbreviated simply to \"MCZ\", is the zoology museum located on the grounds of Harvard University in Cambridge, Massachusetts. It is one of three natural history research museums at Harvard whose public face is the Harvard Museum of Natural History. Harvard MCZ's collections consist of some 21 million specimens, of which several thousand are on rotating display at the public museum. The current director of the Museum of Comparative Zoology is James Hanken, the Louis Agassiz Professor of Zoology at Harvard University.\n\nMany of the exhibits in the public museum have not only zoological interest but also historical significance. Past exhibits have included a fossil sand dollar which was found by Charles Darwin in 1834, Captain Cook's mamo, and two pheasants that once belonged to George Washington, now on loan to Mount Vernon in Virginia.\n\nThe Harvard Museum of Natural History is physically connected to the Peabody Museum of Archaeology and Ethnology; for visitors, one admission ticket grants access to both museums. The research collections of the Museum of Comparative Zoology are not open to the public.\n\nThe Museum of Comparative Zoology was founded in 1859 through the efforts of zoologist Louis Agassiz, and the museum used to be referred to as \"The Agassiz\" after its founder. Agassiz designed the collection to illustrate the variety and comparative relationships of animal life.\n\nThe Radcliffe Zoological Laboratory was created in 1894 when Radcliffe College rented a space on the fifth floor of the Museum of Comparative Zoology at Harvard University to convert into a women's laboratory. Prior to this acquisition, Radcliffe science laboratories were taught using inadequate facilities, converting spaces such as bathrooms in old houses into physics laboratories, which Harvard professors often refused to teach in.The laboratory space was converted from an office or storage closet, and was sandwiched between other invertebrate storage rooms on the fifth floor.\n\nThe museum comprises twelve departments: Biological Oceanography, Entomology, Herpetology, Ichthyology, Invertebrate Paleontology, Invertebrate Zoology, Mammalogy, Marine invertebrates, Malacology, Ornithology, Population Genetics, and Vertebrate Paleontology. The Ernst Mayr Library and its archives join in supporting the work of the museum. The Ernst Mayr Library is a founding member of the Biodiversity Heritage Library.\n\nThe museum publishes two journals: the \"Bulletin of the Museum of Comparative Zoology at Harvard College\", first published in 1869, and \"Breviora\", first published in 1956.\n\nIn contrast to numerous more modern museums, the Harvard Museum of Natural History has many hundreds of stuffed animals on display, from the collections of the Museum of Comparative Zoology. Notable exhibits include whale skeletons, the largest turtle shell ever found (eight feet long), \"the Harvard mastodon\", a long \"Kronosaurus\" skeleton, the skeleton of a dodo, and a coelacanth preserved in fluid. The two-story Great Mammal Hall was renovated in 2009 in celebration of the 150th anniversary of founding of the Museum of Comparative Zoology.\n\nNew and changing exhibitions in the Harvard Museum of Natural History include \"Evolution\" (2008); \"The Language of Color\" (2008 to 2013); \"Arthropods: Creatures that Rule\" (2006); \"New England Forests\" (2011); and \"Mollusks: Shelled Masters of the Marine Realm\" (2012).\n\n"}
{"id": "5629066", "url": "https://en.wikipedia.org/wiki?curid=5629066", "title": "Nomina sacra", "text": "Nomina sacra\n\nIn Christian scribal practice, nomina sacra (singular: nomen sacrum from Latin sacred name) is the abbreviation of several frequently occurring divine names or titles, especially in Greek manuscripts of Holy Scripture. A nomen sacrum consists of two or more letters from the original word spanned by an overline.\n\nMetzger lists 15 such expressions from Greek papyri: the Greek counterparts of \"God\", \"Lord\", \"Jesus\", \"Christ\", \"Son\", \"Spirit\", \"David\", \"Cross\", \"Mother\", \"Father\", \"Israel\", \"Savior\", \"Man\", \"Jerusalem\", and \"Heaven\". These \"nomina sacra\" are all found in Greek manuscripts of the 3rd century and earlier, except \"Mother\", which appears in the 4th.\n\n\"Nomina sacra\" also occur in some form in Latin, Coptic, Armenian (indicated by the \"pativ\"), Gothic, Old Nubian, and Cyrillic (indicated by the \"titlo\").\n\n\"Nomina sacra\" are consistently observed in even the earliest extant Christian writings, along with the codex form rather than the roll, implying that when these were written, in approximately the second century, the practice had already been established for some time. However, it is not known precisely when and how the \"nomina sacra\" first arose.\n\nThe initial system of \"nomina sacra\" apparently consisted of just four or five words, called \"nomina divina\": the Greek words for \"Jesus\", \"Christ\", \"Lord\", \"God\", and possibly \"Spirit\". The practice quickly expanded to a number of other words regarded as sacred.\n\nIn the system of \"nomina sacra\" that came to prevail, abbreviation is by \"contraction\", meaning that the first and last letter (at least) of each word are used. In a few early cases, an alternate practice is seen of abbreviation by \"suspension\", meaning that the initial two letters (at least) of the word are used; e.g., the opening verses of Revelation in write (\"Jesus Christ\") as . Contraction, however, offered the practical advantage of indicating the case of the abbreviated noun.\n\nIt is evident that the use of \"nomina sacra\" was an act of reverence rather than a purely practical space-saving device, as they were employed even where well-established abbreviations of far more frequent words such as \"and\" were avoided, and the \"nomen sacrum\" itself was written with generous spacing. Furthermore, early scribes often distinguished between mundane and sacred occurrences of the same word, e.g. a \"spirit\" vs. the \"Spirit\", and applied \"nomina sacra\" only to the latter (at times necessarily revealing an exegetical choice), although later scribes would mechanically abbreviate all occurrences.\n\nScholars have advanced a number of theories on the origin of the \"nomina sacra\". An obvious parallel that likely offered some inspiration is the Jewish practice of writing the divine name of God, commonly rendered as Jehovah or Yahweh in English, as the Hebrew tetragrammaton (transliterated as YHWH) even in Greek Scriptures. The Septuagint manuscript LXX P.Oxy.VII.1007 uses two Paleo-Hebrew \"yodh's\" with a horizontal line through them for YHWH (an abbreviated form of the Name of God translitered as ). Pavlos Vasileiadis, a Doctor of Theology at the Aristotle University of Thessaloniki, quoting Gerard Gertoux, states that \"the subsequent use of the contracted forms of the original nomina sacra κ[ύριο]ς [()] and θ[εό]ς [()] within Christian manuscripts probably reflects the Jewish practice of replacing the Tetragrammaton by י[הו]ה.\", transliterated in koine Greek as ιά.\n\nGreek culture also employed a number of ways of abbreviating even proper names, though none in quite the same form as the \"nomina sacra\". Inspiration for the contracted forms (using the first and last letter) has also been seen in Revelation, where Jesus speaks of himself as \"the beginning and the end\" and \"the first and the last\" as well \"the Alpha and the Omega\". Greek numerals have been suggested as the origin of the overline spanning the whole \"nomen sacrum\", with the suspended form being simply the ordinary way of writing \"eighteen\", for example.\n\n"}
{"id": "3653726", "url": "https://en.wikipedia.org/wiki?curid=3653726", "title": "Psc (military)", "text": "Psc (military)\n\npsc is a post-nominal for \"passed Staff College\" in the Commonwealth militaries of Britain, Bangladesh, Indian, Sri Lanka and Pakistan. It indicates that an officer has undertaken the staff officer course at a Staff College.\n\nThe practice originated in the British Army where the initials \"psc\" appeared in the service lists denoting that the officer had attended the Staff College, Camberley. Royal Navy offers who attended the staff course at Royal Naval College, Greenwich also used the qualification. Since the 1997 amalgamation of staff training officers now receive the letters psc(j) from the Joint Services Command and Staff College.\n\nPSC is used for Bangladeshi Army officers who have attended the Defence Services Command & Staff College (DSCSC), Bangladesh.\n\nInitials psc is used by officers who attended the Defence Services Staff College, Wellington, India.\n\nIn Pakistan initials psc is used by officers who attended the Command and Staff College, Quetta.\n\nOfficers graduated from the Malaysian Armed Forces Staff College, Kuala Lumpur use the initials psc.\n\nIn Sri Lanka, the initials psc are used by Army, Navy and Air Force officers who have gained the Pass Staff College status from a recognized a staff college such as the Defence Services Command and Staff College and the Sri Lanka Air Force Junior Command & Staff College. Such officers are eligible to wear the psc badge.\n"}
{"id": "18134289", "url": "https://en.wikipedia.org/wiki?curid=18134289", "title": "Qualitative comparative analysis", "text": "Qualitative comparative analysis\n\nIn statistics, qualitative comparative analysis (QCA) is a data analysis technique for determining which logical conclusions a data set supports. The analysis begins with listing and counting all the combinations of variables observed in the data set, followed by applying the rules of logical inference to determine which descriptive inferences or implications the data supports. The technique was originally developed by Charles Ragin in 1987.\n\nIn the case of categorical variables, QCA begins by listing and counting all types of cases which occur, where each type of case is defined by its unique combination of values of its independent and dependent variables. For instance, if there were four categorical variables of interest, {A,B,C,D}, and A and B were dichotomous (could take on two values), C could take on five values, and D could take on three, then there would be 60 possible types of observations determined by the possible combinations of variables, not all of which would necessarily occur in real life. By counting the number of observations that exist for each of the 60 unique combination of variables, QCA can determine which descriptive inferences or implications are empirically supported by a data set. Thus, the input to QCA is a data set of any size, from small-N to large-N, and the output of QCA is a set of descriptive inferences or implications the data supports.\n\nIn QCA's next step, inferential logic or Boolean algebra is used to simplify or reduce the number of inferences to the minimum set of inferences supported by the data. This reduced set of inferences is termed the \"prime implicates\" by QCA adherents. For instance, if the presence of conditions A and B is always associated with the presence of a particular value of D, regardless of the observed value of C, then the value that C takes is irrelevant. Thus, all five inferences involving A and B and any of the five values of C may be replaced by the single descriptive inference \"(A and B) implies the particular value of D\".\n\nTo establish that the prime implicants or descriptive inferences derived from the data by the QCA method are causal requires establishing the existence of causal mechanism using another method such as process tracing, formal logic, intervening variables, or established multidisciplinary knowledge. The method is used in social science and is based on the binary logic of Boolean algebra, and attempts to ensure that all possible combinations of variables that can be made across the cases under investigation are considered.\n\nThe technique of listing case types by potential variable combinations assists with case selection by making investigators aware of all possible case types that would need to be investigated, at a minimum, if they exist, in order to test a certain hypothesis or to derive new inferences from an existing data set. In situations where the available observations constitute the entire population of cases, this method alleviates the small N problem by allowing inferences to be drawn by evaluating and comparing the number of cases exhibiting each combination of variables. The small N problem arises when the number of units of analysis (e.g. countries) available is inherently limited. For example: a study where countries are the unit of analysis is limited in that are only a limited number of countries in the world (less than 200), less than necessary for some (probabilistic) statistical techniques. By maximizing the number of comparisons that can be made across the cases under investigation, causal inferences are according to Ragin possible. This technique allows the identification of multiple causal pathways and interaction effects that may not be detectable via statistical analysis that typically requires its data set to conform to one model. Thus, it is the first step to identifying subsets of a data set conforming to particular causal pathway based on the combinations of covariates prior to quantitative statistical analyses testing conformance to a model; and helps qualitative researchers to correctly limit the scope of claimed findings to the type of observations they analyze.\n\nAs this is a logical (deterministic) and not a statistical (probabilistic) technique, with \"crisp-set\" QCA (csQCA), the original application of QCA, variables can only have two values, which is problematic as the researcher has to determine the values of each variable. For example: GDP per capita has to be divided by the researcher in two categories (e.g. low = 0 and high = 1). But as this variable is essentially a continuous variable, the division will always be arbitrary. A second, related problem is that the technique does not allow an assessment of the effect of the relative strengths of the independent variables (as they can only have two values). Ragin, and other scholars such as Lasse Cronqvist, have tried to deal with these issues by developing new tools that extend QCA, such as multi-value QCA (mvQCA) and fuzzy set QCA (fsQCA). Note: Multi-value QCA is simply QCA applied to observations having categorical variables with more than two values. Crisp-Set QCA can be considered a special case of Multi-value QCA. \n\nStatistical methodologists have argued that QCA's strong assumptions render its findings both fragile and prone to type I error. Simon Hug argues that deterministic hypotheses and error-free measures are exceedingly rare in social science and uses Monte Carlo simulations to demonstrate the fragility of QCA results if either assumption is violated. Chris Krogslund, Donghyun Danny Choi, and Mathias Poertner further demonstrate that QCA results are highly sensitive to minor parametric and model-susceptibility changes and are vulnerable to type I error. Bear F. Braumoeller further explores the vulnerability of the QCA family of techniques to both type I error and multiple inference. Braumoeller also offers a formal test of the null hypothesis and demonstrates that even very convincing QCA findings may be the result of chance.\n\nQCA can be performed probabilistically or deterministically with observations of categorical variables. For instance, the existence of a descriptive inference or implication is supported deterministically by the absence of any counter-example cases to the inference; i.e. if a researcher claims condition X implies condition Y, then, deterministically, there must not exist any counterexample cases having condition X, but not condition Y. However, if the researcher wants to claim that condition X is a probabilistic 'predictor' of condition Y, in another similar set of cases, then the proportion of counterexample cases to an inference to the proportion of cases having that same combination of conditions can be set at a threshold value of for example 80% or higher. For each prime implicant that QCA outputs via its logical inference reduction process, the \"coverage\" — percentage out of all observations that exhibit that implication or inference — and the \"consistency\" — the percentage of observations conforming to that combination of variables having that particular value of the dependent variable or outcome — are calculated and reported, and can be used as indicators of the strength of such a explorative probabilistic inference. In real-life complex societal processes, QCA enables the identification of multiple sets of conditions that are consistently associated with a particular output value in order to explore for causal predictors.\n\nFuzzy set QCA aims to handle variables, such as GDP per capita, where the number of categories, decimal values of monetary units, becomes too large to use mvQCA, or in cases were uncertainty or ambiguity or measurement error in the classification of a case needs to be acknowledged.\n\nQCA has now become used in many more fields than political science which Ragin first developed the method for. Today the method has been used in:\n"}
{"id": "25407", "url": "https://en.wikipedia.org/wiki?curid=25407", "title": "Recursion", "text": "Recursion\n\nRecursion occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no loop or infinite chain of references can occur.\n\nIn mathematics and computer science, a class of objects or methods exhibit recursive behavior when they can be defined by two properties:\n\nFor example, the following is a recursive definition of a person's ancestors:\n\nThe Fibonacci sequence is a classic example of recursion:\n\nformula_1\n\nformula_2\n\nformula_3\n\nMany mathematical axioms are based upon recursive rules. For example, the formal definition of the natural numbers by the Peano axioms can be described as: \"0 is a natural number, and each natural number has a successor, which is also a natural number.\" By this base case and recursive rule, one can generate the set of all natural numbers.\n\nRecursively defined mathematical objects include functions, sets, and especially fractals.\n\nThere are various more tongue-in-cheek \"definitions\" of recursion; see recursive humor.\n\nRecursion is the process a procedure goes through when one of the steps of the procedure involves invoking the procedure itself. A procedure that goes through recursion is said to be 'recursive'.\n\nTo understand recursion, one must recognize the distinction between a procedure and the running of a procedure. A procedure is a set of steps based on a set of rules. The running of a procedure involves actually following the rules and performing the steps. An analogy: a procedure is like a written recipe; running a procedure is like actually preparing the meal.\n\nRecursion is related to, but not the same as, a reference within the specification of a procedure to the execution of some other procedure. For instance, a recipe might refer to cooking vegetables, which is another procedure that in turn requires heating water, and so forth. However, a recursive procedure is where (at least) one of its steps calls for a new instance of the very same procedure, like a sourdough recipe calling for some dough left over from the last time the same recipe was made. This immediately creates the possibility of an endless loop; recursion can only be properly used in a definition if the step in question is skipped in certain cases so that the procedure can complete, like a sourdough recipe that also tells you how to get some starter dough in case you've never made it before. Even if properly defined, a recursive procedure is not easy for humans to perform, as it requires distinguishing the new from the old (partially executed) invocation of the procedure; this requires some administration of how far various simultaneous instances of the procedures have progressed. For this reason recursive definitions are very rare in everyday situations. An example could be the following procedure to find a way through a maze. Proceed forward until reaching either an exit or a branching point (a dead end is considered a branching point with 0 branches). If the point reached is an exit, terminate. Otherwise try each branch in turn, using the procedure recursively; if every trial fails by reaching only dead ends, return on the path that led to this branching point and report failure. Whether this actually defines a terminating procedure depends on the nature of the maze: it must not allow loops. In any case, executing the procedure requires carefully recording all currently explored branching points, and which of their branches have already been exhaustively tried.\n\nLinguist Noam Chomsky among many others has argued that the lack of an upper bound on the number of grammatical sentences in a language, and the lack of an upper bound on grammatical sentence length (beyond practical constraints such as the time available to utter one), can be explained as the consequence of recursion in natural language. This can be understood in terms of a recursive definition of a syntactic category, such as a sentence. A sentence can have a structure in which what follows the verb is another sentence: \"Dorothy thinks witches are dangerous\", in which the sentence \"witches are dangerous\" occurs in the larger one. So a sentence can be defined recursively (very roughly) as something with a structure that includes a noun phrase, a verb, and optionally another sentence. This is really just a special case of the mathematical definition of recursion.\n\nThis provides a way of understanding the creativity of language—the unbounded number of grammatical sentences—because it immediately predicts that sentences can be of arbitrary length: \"Dorothy thinks that Toto suspects that Tin Man said that...\". There are many structures apart from sentences that can be defined recursively, and therefore many ways in which a sentence can embed instances of one category inside another. Over the years, languages in general have proved amenable to this kind of analysis.\n\nRecently, however, the generally accepted idea that recursion is an essential property of human language has been challenged by Daniel Everett on the basis of his claims about the Pirahã language. Andrew Nevins, David Pesetsky and Cilene Rodrigues are among many who have argued against this. Literary self-reference can in any case be argued to be different in kind from mathematical or logical recursion.\n\nRecursion plays a crucial role not only in syntax, but also in natural language semantics. The word \"and\", for example, can be construed as a function that can apply to sentence meanings to create new sentences, and likewise for noun phrase meanings, verb phrase meanings, and others. It can also apply to intransitive verbs, transitive verbs, or ditransitive verbs. In order to provide a single denotation for it that is suitably flexible, \"and\" is typically defined so that it can take any of these different types of meanings as arguments. This can be done by defining it for a simple case in which it combines sentences, and then defining the other cases recursively in terms of the simple one. \n\nA recursive grammar is a formal grammar that contains recursive production rules.\n\nRecursion is sometimes used humorously in computer science, programming, philosophy, or mathematics textbooks, generally by giving a circular definition or self-reference, in which the putative recursive step does not get closer to a base case, but instead leads to an infinite regress. It is not unusual for such books to include a joke entry in their glossary along the lines of:\n\nA variation is found on page 269 in the index of some editions of Brian Kernighan and Dennis Ritchie's book \"The C Programming Language\"; the index entry recursively references itself (\"recursion 86, 139, 141, 182, 202, 269\"). The earliest version of this joke was in \"Software Tools\" by Kernighan and Plauger, and also appears in \"The UNIX Programming Environment\" by Kernighan and Pike. It did not appear in the first edition of \"The C Programming Language\".\n\nAnother joke is that \"To understand recursion, you must understand recursion.\" In the English-language version of the Google web search engine, when a search for \"recursion\" is made, the site suggests \"Did you mean: \"recursion\".\" An alternative form is the following, from Andrew Plotkin: \"If you already know what recursion is, just remember the answer. Otherwise, find someone who is standing closer to Douglas Hofstadter than you are; then ask him or her what recursion is.\"\n\nRecursive acronyms can also be examples of recursive humor. PHP, for example, stands for \"PHP Hypertext Preprocessor\", WINE stands for \"WINE Is Not an Emulator.\" and GNU stands for \"GNU's not Unix\".\n\nThe canonical example of a recursively defined set is given by the natural numbers:\n\nAnother interesting example is the set of all \"true reachable\" propositions in an axiomatic system.\n\n\nThis set is called 'true reachable propositions' because in non-constructive approaches to the foundations of mathematics, the set of true propositions may be larger than the set recursively constructed from the axioms and rules of inference. See also Gödel's incompleteness theorems.\n\nFinite subdivision rules are a geometric form of recursion, which can be used to create fractal-like images. A subdivision rule starts with a collection of polygons labelled by finitely many labels, and then each polygon is subdivided into smaller labelled polygons in a way that depends only on the labels of the original polygon. This process can be iterated. The standard `middle thirds' technique for creating the Cantor set is a subdivision rule, as is barycentric subdivision.\n\nA function may be partly defined in terms of itself. A familiar example is the Fibonacci number sequence: \"F\"(\"n\") = \"F\"(\"n\" − 1) + \"F\"(\"n\" − 2). For such a definition to be useful, it must lead to non-recursively defined values, in this case \"F\"(0) = 0 and \"F\"(1) = 1.\n\nA famous recursive function is the Ackermann function, which—unlike the Fibonacci sequence—cannot easily be expressed without recursion.\n\nApplying the standard technique of proof by cases to recursively defined sets or functions, as in the preceding sections, yields structural induction, a powerful generalization of mathematical induction widely used to derive proofs in mathematical logic and computer science.\n\nDynamic programming is an approach to optimization that restates a multiperiod or multistep optimization problem in recursive form. The key result in dynamic programming is the Bellman equation, which writes the value of the optimization problem at an earlier time (or earlier step) in terms of its value at a later time (or later step).\n\nIn set theory, this is a theorem guaranteeing that recursively defined functions exist. Given a set \"X\", an element \"a\" of \"X\" and a function formula_7, the theorem states that there is a unique function formula_8 (where formula_4 denotes the set of natural numbers including zero) such that\nfor any natural number \"n\".\n\nTake two functions formula_8 and formula_13 such that:\n\nwhere \"a\" is an element of \"X\".\n\nIt can be proved by mathematical induction that formula_18 for all natural numbers \"n\":\n\nBy induction, formula_18 for all formula_25.\n\nA common method of simplification is to divide a problem into subproblems of the same type. As a computer programming technique, this is called divide and conquer and is key to the design of many important algorithms. Divide and conquer serves as a top-down approach to problem solving, where problems are solved by solving smaller and smaller instances. A contrary approach is dynamic programming. This approach serves as a bottom-up approach, where problems are solved by solving larger and larger instances, until the desired size is reached.\n\nA classic example of recursion is the definition of the factorial function, given here in C code:\n\nIf not having reached the base case and returning with value every instantiation of the above function creates a new instance of the function, passing to it an input reduced by (), and returns the result of this (recursive) call, multiplied by its own value of , analogously to the mathematical definition of the factorial.\n\nRecursion in computer programming is exemplified when a function is defined in terms of simpler, often smaller versions of itself. The solution to the problem is then devised by combining the solutions obtained from the simpler versions of the problem. One example application of recursion is in parsers for programming languages. The great advantage of recursion is that an infinite set of possible sentences, designs or other data can be defined, parsed or produced by a finite computer program.\n\nRecurrence relations are equations to define one or more sequences recursively. Some specific kinds of recurrence relation can be \"solved\" to obtain a non-recursive definition.\n\nUse of recursion in an algorithm has both advantages and disadvantages. The main advantage is usually simplicity. The main disadvantage is often that the algorithm may require large amounts of memory if the depth of the recursion is very large.\n\nThe Russian Doll or Matryoshka Doll is a physical artistic example of the recursive concept.\n\nRecursion has been used in paintings since Giotto's \"Stefaneschi Triptych\", made in 1320. Its central panel contains the kneeling figure of Cardinal Stefaneschi, holding up the triptych itself as an offering.\n\nM. C. Escher's \"Print Gallery\" (1956) is a print which depicts a distorted city which contains a gallery which recursively contains the picture, and so \"ad infinitum\".\n\n\n"}
{"id": "20110874", "url": "https://en.wikipedia.org/wiki?curid=20110874", "title": "Reference", "text": "Reference\n\nReference is a relation between objects in which one object designates, or acts as a means by which to connect to or link to, another object. The first object in this relation is said to \"refer to\" the second object. It is called a \"name\" for the second object. The second object, the one to which the first object refers, is called the \"referent\" of the first object. A name is usually a phrase or expression, or some other symbolic representation. Its referent may be anything – a material object, a person, an event, an activity, or an abstract concept.\n\nReferences can take on many forms, including: a thought, a sensory perception that is audible (onomatopoeia), visual (text), olfactory, or tactile, emotional state, relationship with other, spacetime coordinate, symbolic or alpha-numeric, a physical object or an energy projection. In some cases, methods are used that intentionally hide the reference from some observers, as in cryptography.\n\nReferences feature in many spheres of human activity and knowledge, and the term adopts shades of meaning particular to the contexts in which it is used. Some of them are described in the sections below.\n\nThe word \"reference\" is derived from Middle English \"referren\", from Middle French \"référer\", from Latin \"referre\", \"to carry back\", formed from the prefix \"re\"- and \"ferre\", \"to bear\". A number of words derive from the same root, including \"refer\", \"referee\", \"referential\", \"referent\", \"referendum\".\n\nThe verb \"refer (to)\" and its derivatives may carry the sense of \"link to\" or \"connect to\", as in the meanings of \"reference\" described in this article. Another sense is \"consult\"; this is reflected in such expressions as reference work, reference desk, job reference, etc.\n\nIn semantics, reference is generally construed as the relationships between nouns or pronouns and objects that are named by them. Hence, the word \"John\" refers to the person John. The word \"it\" refers to some previously specified object. The object referred to is called the \"referent\" of the word. Sometimes the word-object relation is called \"denotation\"; the word denotes the object. The converse relation, the relation from object to word, is called \"exemplification\"; the object exemplifies what the word denotes. In syntactic analysis, if a word refers to a previous word, the previous word is called the \"antecedent\".\n\nGottlob Frege argued that reference cannot be treated as identical with meaning: \"Hesperus\" (an ancient Greek name for the evening star) and \"Phosphorus\" (an ancient Greek name for the morning star) both refer to Venus, but the astronomical fact that '\"Hesperus\" is \"Phosphorus\"' can still be informative, even if the \"meanings\" of \"Hesperus\" and \"Phosphorus\" are already known. This problem led Frege to distinguish between the sense and reference of a word. Some cases seem to be too complicated to be classified within this framework; the acceptance of the notion of secondary reference may be necessary to fill the gap. See also Opaque context.\n\nThe very concept of the linguistic sign is the combination of content and expression, the former of which may refer entities in the world or refer more abstract concepts, e.g. thought.\nCertain parts of speech exist only to express reference, namely anaphora such as pronouns. The subset of reflexives expresses co-reference of two participants in a sentence. These could be the agent (actor) and patient (acted on), as in \"The man washed himself\", the theme and recipient, as in \"I showed Mary to herself\", or various other possible combinations.\n\nIn computer science, references are data types that refer to an object elsewhere in memory and are used to construct a wide variety of data structures, such as linked lists. Generally, a reference is a value that enables a program to directly access the particular data item. Most programming languages support some form of reference. For the specific type of reference used in the C++ language, see reference (C++).\n\nThe notion of reference is also important in relational database theory; see referential integrity.\n\nReferences to many types of printed matter may come in an electronic or machine-readable form. For books, there exists the ISBN and for journal articles, the Digital object identifier (DOI) is gaining relevance. Information on the Internet may be referred to by a Uniform Resource Identifier (URI).\n\nIn terms of mental processing, a self-reference is used in psychology to establish identification with a mental state during self-analysis. This seeks to allow the individual to develop own frames of reference in a greater state of immediate awareness. However, it can also lead to circular reasoning, preventing evolution of thought.\n\nAccording to Perceptual Control Theory (PCT), a reference condition is the state toward which a control system's output tends to alter a controlled quantity. The main proposition is that \"All behavior is oriented all of the time around the control of certain quantities with respect to specific reference conditions.\"\n\nIn academics and scholarship, an author-title-date information in bibliographies and footnotes, specifying complete works of other people. Copying of material by another author without proper citation or without required permissions is plagiarism.\n\nKeeping a diary allows an individual to use references for personal organization, whether or not anyone else understands the systems of reference used. However, scholars have studied methods of reference because of their key role in communication and co-operation between \"different\" people, and also because of misunderstandings that can arise. Modern academic study of reference has been developing since the 19th century.\n\nIn scholarship, a reference may be a citation of a text that has been used in the creation of a piece of work such as an essay, report, or oration. Its primary purpose is to allow people who read such work to examine the author's sources, either for validity or to learn more about the subject. Such items are often listed at the end of an article or book in a section marked \"Bibliography\" or \"References\". A bibliographical section often contains works not cited by the author, but used as background reading or listed as potentially useful to the reader. A reference section contains only those works cited by the author(s) in the main text.\n\nIn patent law, a reference is a document that can be used to show the state of knowledge at a given time and that therefore may make a claimed invention obvious or anticipated. Examples of references are patents of any country, magazine articles, Ph.D. theses that are indexed and thus accessible to those interested in finding information about the subject matter, and to some extent Internet material that is similarly accessible.\n\nIn art, a reference is an item from which a work is based. This may include:\nAnother example of reference is samples of various musical works being incorporated into a new one.\n\n\n"}
{"id": "17892", "url": "https://en.wikipedia.org/wiki?curid=17892", "title": "Reference desk", "text": "Reference desk\n\nThe reference desk or information desk of a library is a public service counter where professional librarians provide library users with direction to library materials, advice on library collections and services, and expertise on multiple kinds of information from multiple sources.\n\nLibrary users can consult the staff at the reference desk for help in finding information. Using a structured reference interview, the librarian works with the library user to clarify their needs and determine what information sources will fill them. To borrow a medical analogy, reference librarians diagnose and treat information deficiencies.\n\nThe ultimate help provided may consist of reading material in the form of a book or journal article, instruction in the use of specific searchable information resources such as the library's online catalog or subscription bibliographic/fulltext databases, or simply factual information drawn from the library's print or online reference collection. Information is also provided to patrons through electronic resources. Typically, a reference desk can be consulted either in person, by telephone, through email or online chat, although a library user may be asked to come to the library in person for help with more involved research questions. A staffed and knowledgeable reference desk is an essential part of a library.\n\nThe services that are provided at a reference desk may vary depending on the type of library, its purpose, its resources, and its staff.\n\nReference services did not become commonplace in libraries until the late 1800s. These services initially began in public libraries. At first librarians were hesitant to offer reference services because many libraries did not have a large enough staff to provide the services without other duties being neglected. Beginning in 1883 with the Boston Public Library, libraries began to hire librarians whose primary duty was to provide reference services.\n\nOne of the earliest proponents of references services was Samuel Swett Green. He wrote an article titled \"Personal Relations Between Librarians and Readers\" which had a large impact on the future of reference services.\nthen, it operated to incorporate ... making the following variables relevant in offering reference services: the user's query; the reference librarian; and, the reference sources. Until hitherto the communication between the reference librarian and the user are through direct contact. Hence, Utor (2008), defined reference services as a direct personal assistance to readers seeking information. That is during the traditional era. towards the later decades of 19th century, however, reference and information services witnessed an insidious yet drastic paradigm-shift following the incorporation of information communication technology in reference services (and in library operations, by extension). Thus leading to an entirely new era, otherwise known as digital era with different information technologies coming in to aid the work of a reference librarian; changing information sources, reference processes and communication medium.\n\nResources that are often kept at a library reference desk may include:\n\nServices that are often available at a library reference desk include:\n\nThe librarian who staffs the reference desk can usually do the following by virtue of their professional training and experience:\n\nIn the United States, those who staff library reference desks are usually required to have a master's degree in library science from a program accredited by the American Library Association. However, if there is a lack of qualified applicants, particularly in rural areas, a person with an associate degree, a certificate in library technology, or a bachelor's degree in library science may perform these duties. In many academic libraries, student assistants are used as the primary contact, sometimes at an \"information desk.\"\n\nIn Sri Lanka, librarians at reference desks typically have master's degrees from the Sri Lankan Library Association's accredited programs.\n\nWith the development of the Web, digital reference services are beginning to take over some of the roles of the traditional reference desk in a library. There is disagreement over whether or not this development is desirable or inevitable.\n\n\n"}
{"id": "8912106", "url": "https://en.wikipedia.org/wiki?curid=8912106", "title": "Reference scenario", "text": "Reference scenario\n\nA reference scenario is an imagined situation where a library patron brings a question to a librarian and there is then a conversation, called in the field a reference interview, where the librarian works to help the patron find what he or she wants. These scenarios are used in training future librarians how to help patrons. Basically, a scenario is as short as a couple of sentences, including a question and a situation that underlies that question.\n\nA great deal of reference teaching puts students to researching the answers to made-up questions. This focuses the student on learning about the reference sources at hand by using them to answer those questions. Scenarios are something different. They focus the student on the interaction with patrons. In class practice sessions, one student can be the patron and the other the librarian, as long as the one practicing as the librarian doesn't know the whole scenario in advance.\n\nScenarios are valued because often the question asked is not the end of the patron's information hunt, but the start. Patrons often start by voicing a question that they think the library can answer, rather than the question they are really seeking to answer. Or they pose a question that the librarian doesn't understand. Reference librarian skills are very much about mediating a gap between what the patron wants and what the library can provide. This can involve the librarian making him or herself a partner in the patron's search, teaching them what the library really has to offer, or even just clarifying a confusing word: Does the patron want information about soaps to clean with or soaps as in soap operas?\n\n\n"}
{"id": "1130951", "url": "https://en.wikipedia.org/wiki?curid=1130951", "title": "Scribal abbreviation", "text": "Scribal abbreviation\n\nScribal abbreviations or sigla (singular: siglum) are the abbreviations used by ancient and medieval scribes writing in Latin, and later in Greek and Old Norse. In modern manuscript editing (substantive and mechanical) \"sigla\" are the symbols used to indicate the source manuscript (e.g. variations in text between different such manuscripts) and to identify the copyist(s) of a work. See Critical apparatus.\n\nAbbreviated writing, using \"sigla\", arose partly from the limitations of the workable nature of the materials (stone, metal, parchment, etc.) employed in record-making and partly from their availability. Thus, lapidaries, engravers, and copyists made the most of the available writing space. Scribal abbreviations were infrequent when writing materials were plentiful, but by the 3rd and 4th centuries AD, writing materials were scarce and costly.\n\nDuring the Roman Republic, several abbreviations, known as \"sigla\" (plural of \"siglum\" = symbol or abbreviation), were in common use in inscriptions, and they increased in number during the Roman Empire. Additionally, in this period shorthand entered general usage. The earliest known Western shorthand system was that employed by the Greek historian Xenophon in the memoir of Socrates, and it was called \"notae socratae\". In the late Roman Republic, the Tironian notes were developed possibly by Marcus Tullius Tiro, Cicero's amanuensis, in 63 BC to record information with fewer symbols; Tironian notes include a shorthand/syllabic alphabet notation different from the Latin minuscule hand and square and rustic capital letters. The notation was akin to modern stenographic writing systems. It used symbols for whole words or word roots and grammatical modifier marks, and it could be used to write either whole passages in shorthand or only certain words. In medieval times, the symbols to represent words were widely used; and the initial symbols, as few as 140 according to some sources, were increased to 14,000 by the Carolingians, who used them in conjunction with other abbreviations. However, the alphabet notation had a \"murky existence\" (C. Burnett), as it was often associated with witchcraft and magic, and it was eventually forgotten. Interest in it was rekindled by the Archbishop of Canterbury Thomas Becket in the 12th century and later in the 15th century, when it was rediscovered by Johannes Trithemius, abbot of the Benedictine abbey of Sponheim, in a psalm written entirely in Tironian shorthand and a Ciceronian lexicon, which was discovered in a Benedictine monastery (\"notae benenses\").\n\nTo learn the Tironian note system, scribes required formal schooling in some 4,000 symbols; this later increased to some 5,000 symbols and then to some 13,000 in the medieval period (4th to 15th centuries AD); the meanings of some characters remain uncertain. \"Sigla\" were mostly used in lapidary inscriptions; in some places and historical periods (such as medieval Spain) scribal abbreviations were overused to the extent that some are indecipherable.\n\nThe abbreviations were not constant but changed from region to region. Scribal abbreviations increased in usage and reached their height in the Carolingian Renaissance (8th to 10th centuries). The most common abbreviations, called \"notae communes\", were used across most of Europe, but others appeared in certain regions. In legal documents, legal abbreviations, called \"notae juris\", appear but also capricious abbreviations, which scribes manufactured \"ad hoc\" to avoid repeating names and places in a given document.\n\nScribal abbreviations can be found in epigraphy, sacred and legal manuscripts, written in Latin or in a vernacular tongue (but less frequently and with fewer abbreviations), either calligraphically or not.\n\nIn epigraphy, common abbreviations were comprehended in two observed classes:\n\nBoth forms of abbreviation are called \"suspensions\" (as the scribe suspends the writing of the word). A separate form of abbreviation is by \"contraction\" and was mostly a Christian usage for sacred words, Nomina Sacra; non-Christian sigla usage usually limited the number of letters the abbreviation comprised and omitted no intermediate letter. One practice was rendering an overused, formulaic phrase only as a siglum: DM for \"Dis Manibus\" (\"Dedicated to the Manes\"); IHS from the first three letters of \"ΙΗΣΟΥΣ\"; and RIP for \"requiescat in pace\" (\"Rest in Peace\") because the long-form written usage of the abbreviated phrase, by itself, was rare. According to Trabe, these abbreviations are not really meant to lighten the burden of the scribe but rather to shroud in reverent obscurity the holiest words of the Christian religion.\n\nAnother practice was repeating the abbreviation's final consonant a given number of times to indicate a group of as many persons: AVG denoted \"Augustus\", thus, AVGG denoted \"Augusti duo\"; however, lapidaries took typographic liberties with that rule, and instead of using COSS to denote \"Consulibus duobus\", they invented the CCSS form. Still, when occasion required referring to three or four persons, the complex doubling of the final consonant yielded to the simple plural siglum. To that effect, a \"vinculum\" (overbar) above a letter or a letter-set also was so used, becoming a universal medieval typographic usage. Likewise the \"tilde\" (~), an undulated, curved-end line, came into standard late-medieval usage.\n\nBesides the \"tilde\" and macron marks, above and below letters, modifying cross-bars and extended strokes were employed as scribal abbreviation marks, mostly for prefixes and verb, noun and adjective suffixes. The \"typographic\" abbreviations should not be confused with the \"phrasal\" abbreviations: i.e. (\"id est\" — \"that is\"); loc. cit. (\"loco citato\" — \"in the passage already cited\"); viz. (\"vide licet\" — \"namely\", \"that is to say\", \"in other words\" — formed with \"vi\" and the \"yogh\"-like glyph [Ꝫ], [ꝫ], the siglum for the suffix -et and the conjunction et) and et cetera.\n\nMoreover, besides scribal abbreviations, ancient texts also contained variant typographic characters, including ligatures (e.g. Æ, Œ, etc.), the long s (ſ), and the half r, resembling an Arabic numeral two (\"2\"). The \"u\" and \"v\" characters originated as scribal variants for their respective letters, likewise the \"i\" and \"j\" pair. Modern publishers printing Latin-language works replace variant typography and sigla with full-form Latin spellings; the convention of using \"u\" and \"i\" for vowels and \"v\" and \"j\" for consonants is a late typographic development.\n\nSome ancient and medieval sigla are still used in English and other European languages; the Latin ampersand (&) replaces the conjunction \"and\" in English, \"et\" in Latin and French, and \"y\" in Spanish (but its use in Spanish is frowned upon, since the \"y\" is already smaller and easier to write). The Tironian sign ⁊, resembling the digit seven (\"7\"), represents the conjunction \"et\" and is written only to the x-height; in current Irish language usage, the siglum denotes the conjunction \"agus\" (\"and\"). Other scribal abbreviations in modern typographic use are the percentage sign (%), from the Italian \"per cento\" (\"per hundred\"); the permille sign (‰), from the Italian \"per mille\" (\"per thousand\"); the pound sign (₤, £ and #, all descending from ℔ or lb, \"librum\") and the dollar sign ($), which possibly derives from the Spanish word \"Peso\". The commercial at symbol (@), originally denoting \"at the rate/price of\", is a ligature derived from the English preposition \"at\"; from the 1990s, its use outside commerce became widespread, as part of e-mail addresses.\n\nTypographically, the ampersand (\"&\"), representing the word \"et\", is a space-saving ligature of the letters \"e\" and \"t\", its component graphemes. Since the establishment of movable-type printing in the 15th century, founders have created many such ligatures for each set of record type (font) to communicate much information with fewer symbols. Moreover, during the Renaissance (14th to 17th centuries), when Ancient Greek language manuscripts introduced that tongue to Western Europe, its scribal abbreviations were converted to ligatures in imitation of the Latin scribal writing to which readers were accustomed. Later, in the 16th century, when the culture of publishing included Europe's vernacular languages, Graeco-Roman scribal abbreviations disappeared, an ideologic deletion ascribed to the anti-Latinist Protestant Reformation (1517–1648).\n\nThe common abbreviation \"Xmas,\" for Christmas, is a remnant of an old scribal abbreviation that substituted the Greek letter chi (Χ, resembling Latin X and representing the first letter in the Greek word for Christ, Χριστος) for the word Christ.\n\nAfter the invention of printing, manuscript copying abbreviations continued to be employed in Church Slavonic and are still in use in printed books as well as on icons and inscriptions. Many common long roots and nouns describing sacred persons are abbreviated and written under the special diacritic symbol titlo, as shown in the figure at the right. That corresponds to the Nomina sacra (Latin: \"Sacred names\") tradition of using contractions for certain frequently-occurring names in Greek ecclesiastical texts. However, sigla for personal nouns are restricted to \"good\" beings and the same words, when referring to \"bad\" beings, are spelled out; for example, while \"God\" in the sense of the one true God is abbreviated as \"\", \"god\" referring to \"false\" gods is spelled out. Likewise, the word for \"angel\" is generally abbreviated as \"\", but the word for \"angels\" is spelled out for \"performed by evil angels\" in Psalm 77.\n\nAdriano Cappelli's \"Lexicon Abbreviaturarum\", enumerates the various medieval brachigraphic signs found in Latin and Italian vulgar texts, which originate from the Roman sigla, a symbol to express a word, and Tironian notes. Quite rarely, abbreviations did not carry marks to indicate that an abbreviation has occurred: if they did, they were often copying errors. For example, \"e.g.\" is written with periods, but modern terms, such as \"PC\", may be written in uppercase.\n\nIt should be noted that the original manuscripts were not written in a modern sans-serif or serif font but in Roman capitals, rustic, uncial, insular, Carolingian or blackletter styles. For more, refer to Western calligraphy or a beginner's guide.\n\nAdditionally, the abbreviations employed varied across Europe. In Nordic texts, for instance, two runes were used in text written in the Latin alphabet, which are ᚠ for \"fé\" \"cattle, goods\" and ᛘ for \"maðr\" \"man\".\n\nCappelli divides abbreviations into six overlapping categories:\n\nSuspended terms are those of which only the first part is written, and the last part is substituted by a mark, which can be of two types:\n\nThe largest class of suspensions consists of single letters standing in for words that begin with that letter.\n\nA dot at the baseline after a capital letter may stand for a title if it is used such as in front of names or a person's name in medieval legal documents. However, not all sigla use the beginning of the word.\nFor plural words, the siglum is often doubled: \"F.\" = \"frater\" and \"FF.\" = \"fratres\". Tripled sigla often stand for three: \"DDD\" = \"domini tres\".\n\nLetters lying on their sides, or mirrored (backwards), often indicate female titles, but a mirrored C, Ↄ, stands generally for \"con\" or \"contra\" (the latter sometimes with a macron above, \"Ↄ̄\").\n\nTo avoid confusion with abbreviations and numerals, the latter are often written with a bar above. In some contexts, however, numbers with a line above indicate that number is to be multiplied by a thousand, and several other abbreviations also have a line above them, such as \"ΧΡ\" (Greek letters chi+rho) = \"Christus\" or \"IHS\" = \"Jesus\".\n\nStarting in the 8th or the 9th century, single letter sigla grew less common and were replaced by longer, less-ambiguous sigla, with bars above them.\n\nAbbreviations by contraction have one or more middle letters omitted. They were often represented with a general mark of abbreviation (above), such as a line above. They can be divided into two subtypes:\n\nSuch marks inform the reader of the identity of the missing part of the word without affecting (\"independent\" of) the meaning. Some of them may be interpreted as alternative contextual glyphs of their respective letters.\n\nThe meaning of the marks depends on the letter on which they appear.\n\nA superscript letter generally referred to the letter omitted, but, in some instances, as in the case of vowel letters, it could refer to a missing vowel combined with the letter \"r\", before or after it. It is only in some English dialects that the letter \"r\" before another consonant largely silent and the preceding vowel is \"r-coloured\".\n\nHowever, \"a\", \"i\", and \"o\" above \"g\" meant \"gͣ\" \"gna\", \"gͥ\" \"gni\" and \"gͦ\" \"gno\" respectively. Although in English, the \"g\" is silent in \"gn\", but in other languages, it is pronounced. Vowel letters above \"q\" meant \"qu\" + vowel: \"qͣ\", \"qͤ\", \"qͥ\", \"qͦ\", \"qͧ\".\n\n\nVowels were the most common superscripts, but consonants could be placed above letters without ascenders; the most common were \"c\", e.g. \"nͨ\". A cut \"l\" above an \"n\", \"nᷝ\", meant \"nihil\" for instance.\n\nThese marks are nonalphabetic letters carrying a particular meaning. Several of them continue in modern usage, as in the case of monetary symbols. In Unicode, they are referred to as \"letter-like glyphs\". Additionally, several authors are of the view that the Roman numerals themselves were, for example, nothing less than abbreviations of the words for those numbers. Other examples of symbols still in some use are alchemical and zodiac symbols, which were, in any case, employed only in alchemy and astrology texts, which made their appearance beyond that special context rare.\n\nIn addition to the signs used to signify abbreviations, medieval manuscripts feature some glyphs that are now uncommon but were not sigla.\nMany more ligatures were used to reduce the space occupied, a characteristic that is particularly prominent in blackletter scripts.\nSome such as r rotunda, long s and uncial or insular variants (Insular G), Claudian letters were in common use, as well as letters derived from other scripts such as Nordic runes: thorn (þ=th) and eth (ð=dh).\nAn illuminated manuscript would feature miniatures, decorated initials or \"littera notabilior\", which later resulted in the bicamerality of the script (case distinction).\n\nVarious typefaces have been designed to allow scribal abbreviations and other archaic glyphs to be replicated in print. They include \"record type\", which was first developed in the 1770s to publish Domesday Book and was fairly widely used for the publication of medieval records in Britain until the end of the 19th century.\n\nIn the Unicode Standard v. 5.1 (4 April 2008), 152 medieval and classical glyphs were given specific locations outside of the Private Use Area. Specifically, they are located in the charts \"Combining Diacritical Marks Supplement\" (26 characters), \"Latin Extended Additional\" (10 characters), \"Supplemental Punctuation\" (15 characters), \"Ancient Symbols\" (12 characters) and especially \"Latin Extended-D\" (89 characters).\nThese consist in both precomposed characters and modifiers for other characters, called combining diacritical marks (such as writing in LaTeX or using overstrike in MS Word).\n\nCharacters are \"the smallest components of written language that have semantic value\" but glyphs are \"the shapes that characters can have when they are rendered or displayed\".\n\n\n\n"}
{"id": "28545", "url": "https://en.wikipedia.org/wiki?curid=28545", "title": "Self-reference", "text": "Self-reference\n\nSelf-reference occurs in natural or formal languages when a sentence, idea or formula refers to itself. The reference may be expressed either directly—through some intermediate sentence or formula—or by means of some encoding. In philosophy, it also refers to the ability of a subject to speak of or refer to itself: to have the kind of thought expressed by the first person nominative singular pronoun, the word \"I\" in English.\n\nSelf-reference is studied and has applications in mathematics, philosophy, computer programming, and linguistics. Self-referential statements are sometimes paradoxical, and can also be considered recursive.\n\nIn classical philosophy, paradoxes were created by self-referential concepts such as the omnipotence paradox of asking if it was possible for a being to exist so powerful that it could create a stone that it could not lift. The Epimenides paradox, 'All Cretans are liars' when uttered by an ancient Greek Cretan was one of the first recorded versions. Contemporary philosophy sometimes employs the same technique to demonstrate that a supposed concept is meaningless or ill-defined.\n\nIn mathematics and computability theory, self-reference (also known as Impredicativity) is the key concept in proving limitations of many systems. Gödel's theorem uses it to show that no formal consistent system of mathematics can ever contain all possible mathematical truths, because it cannot prove some truths about its own structure. The halting problem equivalent, in computation theory, shows that there is always some task that a computer cannot perform, namely reasoning about itself. These proofs relate to a long tradition of mathematical paradoxes such as Russell's paradox and Berry's paradox, and ultimately to classical philosophical paradoxes.\n\nIn game theory undefined behaviors can occur where two players must model each other's mental states and behaviors, leading to infinite regress.\n\nIn computer programming, self-reference occurs in reflection, where a program can read or modify its own instructions like any other data. Numerous programming languages support reflection to some extent with varying degrees of expressiveness. Additionally, self-reference is seen in recursion (related to the mathematical recurrence relation) in functional programming, where a code structure refers back to itself during computation. 'Taming' self-reference from potentially paradoxical concepts into well-behaved recursions has been one of the great successes of computer science, and is now used routinely in, for example, writing compilers using the 'meta-language' ML. Using a compiler to compile itself is known as bootstrapping. Self-modifying code is possible to write (programs which operate on themselves), both with assembler and with functional languages such as Lisp, but is generally discouraged in real-world programming. Computing hardware makes fundamental use of self-reference in flip-flops, the basic units of digital memory, which convert potentially paradoxical logical self-relations into memory by expanding their terms over time. Thinking in terms of self-reference is a pervasive part of programmer culture, with many programs and acronyms named self-referentially as a form of humor, such as GNU ('Gnu's not Unix') and PINE ('Pine is not Elm'). The GNU Hurd is named for a pair of mutually self-referential acronyms.\n\nTupper's self-referential formula is a mathematical curiosity which plots an image of its own formula.\n\nThe biology of self-replication is self-referential, as embodied by DNA and RNA replication mechanisms. Models of self-replication are found in the computational Game of life, and have inspired engineering systems such as the RepRap self-replicating 3d printer.\n\nSelf-reference occurs in literature and film when an author refers to his or her own work in the context of the work itself. Examples include Cervantes's \"Don Quixote\", Shakespeare's \"A Midsummer Night's Dream\", \"The Tempest\" and \"Twelfth Night\", Denis Diderot's \"Jacques le fataliste et son maître\", Italo Calvino's \"If on a winter's night a traveler\", many stories by Nikolai Gogol, \"Lost in the Funhouse\" by John Barth, Luigi Pirandello's \"Six Characters in Search of an Author\" and Federico Fellini's \"8½\". Perhaps the earliest example is in Homer's Iliad, where Helen of Troy laments: \"for generations still unborn/we will live in song\" (appearing in the song itself).\n\nSelf-reference in art is closely related to the concepts of breaking the fourth wall and meta-reference, which often involve self-reference. The short stories of Jorge Luis Borges play with self-reference and related paradoxes in many ways. Samuel Beckett's Krapp's Last Tape consists entirely of the protagonist listening to and making recordings of himself, mostly about other recordings. During the 1990s and 2000s filmic self-reference was a popular part of the rubber reality movement, notably in Charlie Kaufman's films Being John Malkovich and Adaptation, the latter pushing the concept arguably to its breaking point as it attempts to portray its own creation.\n\nVarious creation myths invoke self-reference to solve the problem of what created the creator. For example the Egyptian creation myth has a god swallowing his own semen to create himself. Ouroboros is a mythical dragon which eats itself.\n\nThe surrealist painter René Magritte is famous for his self-referential works. His painting \"The Treachery of Images\", includes the words \"this is not a pipe\", the truth of which depends entirely on whether the word \"ceci\" (in English, \"this\") refers to the pipe depicted—or to the painting or the word or sentence itself. M.C. Escher's art also contains many self-referential concepts such as hands drawing themselves.\n\nA word that describes itself is called an \"autological word\" (or \"autonym\"). This generally applies to adjectives, for example sesquipedalian (i.e. \"sesquipedalian\" is a sesquipedalian word), but can also apply to other parts of speech, such as TLA, as a three-letter abbreviation for \"three-letter abbreviation\".\n\nA sentence which inventories its own letters and punctuation marks is called an autogram.\n\nThere is a special case of meta-sentence in which the content of the sentence in the metalanguage and the content of the sentence in the object language are the same. Such a sentence is referring to itself. However some meta-sentences of this type can lead to paradoxes. \"This is a sentence.\" can be considered to be a self-referential meta-sentence which is obviously true. However \"This sentence is false\" is a meta-sentence which leads to a self-referential paradox. Such sentences can lead to problems, for example, in law, where statements bringing laws into existence can contradict one another or themselves. Kurt Gödel claimed to have found such a paradox in the US constitution at his citizenship ceremony.\n\nSelf-reference occasionally occurs in the media when it is required to write about itself, for example the BBC reporting on job cuts at the BBC. Notable encyclopedias may be required to feature articles about themselves, such as Wikipedia's article on Wikipedia.\n\nFumblerules are a list of rules of good grammar and writing, demonstrated through sentences that violate those very rules, such as \"Avoid cliches like the plague\" and \"Don't use no double negatives\". The term was coined in a published list of such rules by William Safire.\n\nSeveral academic disciplines are sometime required to study themselves in forms of self-reference, for example historiography (or \"meta-history\") is history's study of its own past; meta-sociology occurs when sociologists study the power structures in their own academic institutions; and meta-mathematics is the study of mathematics itself as a formal system, using its own methods. In law, self-reference may become an issue when laws are required to regulate the making of new laws, especially around constitutional issues. (The game of nomic begins as a model of this process.) The prefix \"meta\" is often used to denote this type of self-reference.\n\n\n"}
{"id": "16122539", "url": "https://en.wikipedia.org/wiki?curid=16122539", "title": "Self-reference puzzle", "text": "Self-reference puzzle\n\nA self-reference puzzle is a type of logical puzzle where the question in the puzzle refers to the attributes of the puzzle itself.\nA common example is that a \"fill in the blanks\" style sentence is given, but what is filled in the blanks can contribute to the sentence itself. An example is \"There are _____ e's in this sentence.\", for which a solution is \"eight\" (since including the \"eight\", there are 8 e's in the sentence).\n\n"}
{"id": "2636884", "url": "https://en.wikipedia.org/wiki?curid=2636884", "title": "Source criticism", "text": "Source criticism\n\nSource criticism (or information evaluation) is the process of evaluating an information source, i.e. a document, a person, a speech, a fingerprint, a photo, an observation, or anything used in order to obtain knowledge. In relation to a given purpose, a given information source may be more or less valid, reliable or relevant. Broadly, \"source criticism\" is the interdisciplinary study of how information sources are evaluated for given tasks.\n\nProblems in translation: The Danish word \"kildekritik\" like the Norwegian word \"kildekritikk\" and the Swedish word \"källkritik\" derived from the German \"Quellenkritik\" and is closely associated with the German historian Leopold von Ranke (1795–1886). Hardtwig writes: \"His [Ranke's] first work \"Geschichte der romanischen und germanischen Völker\" von 1494–1514 (History of the Latin and Teutonic Nations from 1494 to 1514) (1824) was a great success. It already showed some of the basic characteristics of his conception of Europe, and was of historiographical importance particularly because Ranke made an exemplary critical analysis of his sources in a separate volume, \"Zur Kritik neuerer Geschichtsschreiber\" (On the Critical Methods of Recent Historians). In this work he raised the method of textual criticism used in the late eighteenth century, particularly in classical philology to the standard method of scientific historical writing\" (Hardtwig, 2001, p. 12739).\nThe larger part of the nineteenth and twentieth\ncenturies would be dominated by the research-oriented\nconception of historical method of the so-called\nHistorical School in Germany, led by historians as\nLeopold Ranke and Berthold Niebuhr. Their conception\nof history, long been regarded as the beginning\nof modern, 'scientific' history, harked back to the\n'narrow' conception of historical method, limiting the\nmethodical character of history to source criticism\" (Lorenz, 2001).\nBible studies dominate the use of \"source criticism\" in America (cf. Hjørland, 2008). The term is thus relatively seldom used in English about historical methods and historiography (cf. Hjørland, 2008). This difference between European and American use of \"source criticism\" is somewhat strange considering the influence of Ranke on both sides of the Atlantic Ocean. It has been suggested that differences in the use of the term are not accidental but due to different views of the historical method. In the German/Scandinavian tradition this subject is seen as important, whereas in the Anglo-American tradition it is believed that historical methods must be specific and associated with the subject studied, for which reason there is no general field of \"source criticism\".\n\nIn the Scandinavian countries and elsewhere source evaluation (or information evaluation) is also studied interdisciplinarily from many different points of view, partly caused by the influence of the Internet. It is a growing field in, among other fields, library and information science. In this context source criticism is studied from a broader perspective than just, for example, history or biblical studies.\n\nThe following principles are cited from two Scandinavian textbooks on source criticism, Olden-Jørgensen (1998) and Thurén (1997) written by historians:\n\nWe may add the following principles:\n\n\"Because each source teaches you more and more about your subject, you will be able to judge with ever-increasing precision the usefulness and value of any prospective source. In other words, the more you know about the subject, the more precisely you can identify what you must still find out\". (Bazerman, 1995, p. 304).\n\"The empirical case study showed that most people find it difficult to assess questions of cognitive authority and media credibility in a general sense, for example, by comparing the overall credibility of newspapers and the Internet. Thus these assessments tend to be situationally sensitive. Newspapers, television and the Internet were frequently used as sources of orienting information, but their credibility varied depending on the actual topic at hand\" (Savolainen, 2007).\nThe following questions are often good ones to ask about any source according to the American Library Association (1994) and Engeldinger (1988):\n\n\nFor literary sources we might add complementing criteria:\n\n\nHow general are principles of source criticism?\nSome principles are universal, other principles are specific for certain kinds of information sources. One may ask whether principles of source criticism are unique to the humanities?\n\nThere is today no consensus about the similarities and differences between natural science and humanities. Logical positivism claimed that all fields of knowledge were based on the same principles. Much of the criticism of logical positivism claimed that positivism is the basis of the sciences, whereas hermeneutics is the basis of the humanities. This was, for example, the position of Jürgen Habermas. A newer position, in accordance with, among others, Hans-Georg Gadamer and Thomas Kuhn understands both science and humanities as determined by researchers' preunderstanding and paradigms. Hermeneutics is thus a universal theory. The difference is, however, that the sources of the humanities are themselves products of human interests and preunderstanding, whereas the sources of the natural sciences are not. Humanities are thus \"doubly hermeneutic\".\n\nNatural scientists, however, are also using human products (such as scientific papers) which are products of preunderstanding (and, for example, academic fraud).\n\nEpistemological theories are the basic theories about how knowledge is obtained and thus the most general theories about how to evaluate information sources. Empiricism evaluates sources by considering the observations (or sensations) on which they are based. Sources without basis in experience are not seen as valid. Rationalism provides low priority to sources based on observations. In order to be meaningful observations must be grasped by clear ideas or concepts. It is the logical structure and the well definedness that is in focus in evaluating information sources from the rationalist point of view. Historicism evaluates information sources on the basis of their reflection of their sociocultural context and their theoretical development. Pragmatism evaluate sources on the basis of how their values and usefulness to accomplish certain outcomes. Pragmatism is skeptical about claimed neutral information sources.\n\nThe evaluation of knowledge or information sources cannot be more certain than is the construction of knowledge. If we accept the principle of fallibilism we also have to accept that source criticism can never 100% verify knowledge claims. As discussed in the next section is source criticism intimately linked to scientific methods.\n\nThe presence of fallacies of argument in sources is another kind of philosophical criteria for evaluating sources. Fallacies are presented by Walton (1998). Among the fallacies are the 'ad hominem fallacy' (the use of personal attack to try to undermine or refute a person's argument) and the 'straw man fallacy' (when one arguer misrepresents another's position to make it appear less plausible than it really is, in order more easily to criticize or refute it.) See also fallacy.\n\nResearch methods are methods used to produce scholarly knowledge. The methods that are relevant for producing knowledge are also relevant for evaluating knowledge. An example of a book that turns methodology upside-down and uses it to evaluate produced knowledge is Katzer; Cook & Crouch (1998). See also Unobtrusive measures, Triangulation (social science).\n\nStudies of quality evaluation processes such as peer review, book reviews and of the normative criteria used in evaluation of scientific and scholarly research. Another field is the study of scientific misconduct.\n\nHarris (1979) provides a case study of how a famous experiment in psychology, Little Albert, has been distorted throughout the history of psychology, starting with the author (Watson) himself, general textbook authors, behavior therapists, and a prominent learning theorist. Harris proposes possible causes for these distortions and analyzes the Albert study as an example of myth making in the history of psychology. Studies of this kind may be regarded a special kind of reception history (how Watson's paper was received). It may also be regarded as a kind of critical history (opposed to ceremonial history of psychology, cf. Harris, 1980). Such studies are important for source criticism in revealing the bias introduced by referring to classical studies.\n\nSee also Hjørland (2008): Empirical studies of the quality of science.\n\nTextual criticism (or broader: text philology) is a part of philology, which is not just devoted to the study of texts, but also to edit and produce \"scientific editions\", \"scholarly editions\", \"standard editions\", \"historical editions\", \"reliable editions\", \"reliable texts\", \"text editions\" or \"critical editions\", which are editions in which careful scholarship has been employed to ensure that the information contained within is as close to the author's/composer's original intentions as possible (and which allows the user to compare and judge changes in editions published under influence by the author/composer). The relation between these kinds of works and the concept \"source criticism\" is evident in Danish, where they may be termed \"kildekritisk udgave\" (directly translated \"source critical edition\").\n\nIn other words, it is assumed that most editions of a given works is filled with noise and errors provided by publishers, why it is important to produce \"scholarly editions\". The work provided by text philology is an important part of source criticism in the humanities.\n\n\ncomplete works and monumental editions\n\nThe study of eyewitness testimony is an important field of study used, among other purposes, to evaluate testimony in courts. The basics of eyewitness fallibility includes factors such as poor viewing conditions, brief exposure, and stress. More subtle factors, such as expectations, biases, and personal stereotypes can intervene to create erroneous reports. Loftus (1996) discuss all such factors and also shows that eyewitness memory is chronically inaccurate in surprising ways. An ingenious series of experiments reveals that memory can be radically altered by the way an eyewitness is questioned after the fact. New memories can be implanted and old ones unconsciously altered under interrogation.\n\nAnderson (1978) and Anderson & Pichert (1977) reported an elegant experiment demonstrating how change in perspective affected people's ability to recall information that was unrecallable from another perspective.\n\nIn psychoanalysis the concept of defence mechanism is important and may be considered a contribution to the theory of source criticism because it explains psychological mechanisms, which distort the reliability of human information sources.\n\nIn schools of library and information science (LIS) is source criticism of taught as part of the growing field Information literacy.\nStudy issues like relevance, quality indicators for documents, kinds of documents and their qualities (e.g. scholarly editions) and related issues are studied in LIS and are relevant for source criticism. Bibliometrics is often used to find the most influential journal, authors, countries and institutions. The study of book reviews and their function in evaluating books should also be mentioned. The well-known comparison of Wikipedia and Encyclopædia Britannica (Giles, 2005) - although not done by information scientists - contained an interview with an information scientist (Michael Twidale) and should be obvious to include in LIS.\n\nIt could be argued that library and information education should provide teaching in source criticism at least at the same level as is taught in Upper Secondary School (see Gudmundsson, 2007).\n\nIn library and information science the checklist approach has often been used. A criticism of this approach is given by Meola (2004): \"Chucking the checklist\".\n\nLibraries sometimes provide advice on how their users may evaluate sources.\n\nThe Library of Congress has a \"Teaching with Primary Sources\" (TPS) program.\n\nSource criticism is also about ethical behavior and culture. It is about a free press and an open society, including the protecting information sources from being persecuted (cf., Whistleblower).\n\nPhotos are often manipulated during wars and for political purposes. One well known example is Joseph Stalin's manipulation of a photograph from May 5, 1920 on which Stalin's predecessor Lenin held a speech for Soviet troops that Leon Trotsky attended. Stalin had later Trotsky retouched out of this photograph. (cf. King, 1997). A recent example is reported by Healy (2008) about North Korean leader Kim Jong Il.\n\nMuch interest in evaluating Internet sources (such as Wikipedia) is reflected in the scholarly literature of Library and information science and in other fields. Mintz (2002) is an edited volume about this issue. Examples of literature examining Internet sources include Chesney (2006), Fritch & Cromwell (2001), Leth & Thurén (2000) and Wilkinson, Bennett, & Oliver (1997).\n\n\"In history, the term historical method was first introduced in a systematic way in the sixteenth century by Jean Bodin in his treatise of source criticism, \"Methodus ad facilem historiarium cognitionem\" (1566). Characteristically, Bodin's treatise intended to establish the ways by which reliable knowledge of the past could be established by checking sources against one another and by so assessing the reliability of the information conveyed by them, relating them to the interests involved.\" (Lorenz, 2001, p. 6870).\n\nAs written above, modern source criticism in history is closely associated with the German historian Leopold von Ranke (1795–1886), who influenced historical methods on both sides of the Atlantic Ocean, although in rather different ways. American history developed in a more empirist and antiphilosophical way (cf., Novick, 1988).\n\nTwo of the best-known rule books from History's childhood are Bernheim (1889) and Langlois & Seignobos (1898). These books provided a seven-step procedure (here quoted from Howell & Prevenier, 2001, p. 70-71):\n\nGudmundsson (2007, p. 38) writes: \"Source criticism should not totally dominate later courses. Other important perspectives, for example, philosophy of history/view of history, should not suffer by being neglected\" (Translated by BH). This quote makes a distinction between source criticism on the one hand and historical philosophy on the other hand. However, different views of history and different specific theories about the field being studied may have important consequences for how sources are selected, interpreted and used. Feminist scholars may, for example, select sources made by women and may interpret sources from a feminist perspective. Epistemology should thus be considered a part of source criticism. It is in particular related to \"tendency analysis\".\n\nIn archaeology, radiocarbon dating is an important technique to establish the age of information sources. Methods of this kind were the ideal when history established itself as both a scientific discipline and as a profession based on \"scientific\" principles in the last part of the 1880s (although radiocarbon dating is a more recent example of such methods). The empiricist movement in history brought along both \"source criticism\" as a research method and also in many countries large scale publishing efforts to make valid editions of \"source materials\" such as important letters and official documents (e.g. as facsimiles or transcriptions).\n\nHistoriography and Historical method include the study of the reliability of the sources used, in terms of, for example, authorship, credibility of the author, and the authenticity or corruption of the text.\n\nSource criticism, as the term is used in biblical criticism, refers to the attempt to establish the sources used by the author and/or redactor of the final text. The term \"literary criticism\" is occasionally used as a synonym.\n\nBiblical source criticism originated in the 18th century with the work of Jean Astruc, who adapted the methods already developed for investigating the texts of Classical antiquity (Homer's Iliad in particular) to his own investigation into the sources of the Book of Genesis. It was subsequently considerably developed by German scholars in what was known as \"the Higher Criticism\", a term no longer in widespread use. The ultimate aim of these scholars was to reconstruct the history of the biblical text, as well as the religious history of ancient Israel.\n\nRelated to Source Criticism is Redaction Criticism which seeks to determine how and why the redactor (editor) put the sources together the way he did. Also related is form criticism and tradition history which try to reconstruct the oral prehistory behind the identified written sources.\n\nJournalists often work with strong time pressure and have access to only a limited number of information sources such as news bureaus, persons which may be interviewed, newspapers, journals and so on (see journalism sourcing). Journalists' possibility for conducting serious source criticism is thus limited compared to, for example, historians' possibilities.\n\nThe most important legal sources are created by parliaments, governments, courts, and legal researchers. They may be written or informal and based on established practices. Views concerning the quality of sources differ among legal philosophies: Legal positivism is the view that the text of the law should be considered in isolation, while legal realism, interpretivism (legal), critical legal studies and feminist legal criticism interprets the law on a broader cultural basis.\n\n\n"}
{"id": "4159251", "url": "https://en.wikipedia.org/wiki?curid=4159251", "title": "Source text", "text": "Source text\n\nA source text is a text (sometimes oral) from which information or ideas are derived. In translation, a source text is the original text that is to be translated into another language.\n\nIn historiography, distinctions are commonly made between three kinds of source texts:\n\nPrimary sources are firsthand written evidence of history made at the time of the event by someone who was present. They have been described as those sources closest to the origin of the information or idea under study. These types of sources have been said to provide researchers with \"direct, unmediated information about the object of study.\" Primary sources are sources which, usually, are recorded by someone who participated in, witnessed, or lived through the event. These are also usually authoritative and fundamental documents concerning the subject under consideration. This includes published original accounts, published original works, or published original research. They may contain original research or new information not previously published elsewhere. They have been distinguished from secondary sources, which often cite, comment on, or build upon primary sources. They serve as an original source of information or new ideas about the topic. \"Primary\" and \"secondary\", however, are relative terms, and any given source may be classified as primary or secondary, depending on how it is used. Physical objects can be primary sources.\n\nSecondary sources are written accounts of history based upon the evidence from primary sources. These are sources which, usually, are accounts, works, or research that analyze, assimilate, evaluate, interpret, and/or synthesize primary sources. These are not as authoritative and are supplemental documents concerning the subject under consideration. These documents or people summarize other material, usually primary source material. They are academics, journalists, and other researchers, and the papers and books they produce. This includes published accounts, published works, or published research. For example a history book drawing upon diary and newspaper records. \n\nTertiary sources are compilations based upon primary and secondary sources. These are sources which, on average, do not fall into the above two levels. They consist of generalized research of a specific subject under consideration. Tertiary sources are analyzed, assimilated, evaluated, interpreted, and/or synthesized from secondary sources, also. These are not authoritative and are just supplemental documents concerning the subject under consideration. These are often meant to present known information in a convenient form with no claim to originality. Common examples are encyclopedias and textbooks.\n\nThe distinction between \"primary source\" and \"secondary source\" is standard in historiography, while the distinction between these sources and \"tertiary sources\" is more peripheral, and is more relevant to the scholarly research work than to the published content itself.\n\nBelow are types of sources that most generally, but not absolutely, fall into a certain level. The letters after an item describes \"generally\" the type it is (though this can vary pending the exact source). \"P\" is for Primary sources, \"S\" is for Secondary sources, and \"T\" is for Tertiary sources. (ed., those with \"?\"s are indeterminate.)\nIn translation, a source text (ST) is a text written in a given source language which is to be, or has been, translated into another language. In translation the source text (ST) is transformed into a target text (TT), written in a given target language. According to Jeremy Munday's definition of translation, \"the process of translation between two different written languages involves the changing of an original written text (the source text or ST) in the original verbal language (the source language or SL) into a written text (the target text or TT) in a different verbal language (the target language or TL)\". \n\nTranslation scholars including Eugene Nida and Peter Newmark have represented the different approaches to translation as falling broadly into source-text-oriented or target-text-oriented categories.\n"}
{"id": "3423601", "url": "https://en.wikipedia.org/wiki?curid=3423601", "title": "Stumpers-L", "text": "Stumpers-L\n\nThe Stumpers-L electronic mailing list, was a resource available for librarians and others to discuss reference questions which they were unable to answer using available resources. It was succeeded by the similar Project Wombat.\n\nStumpers-L began in 1992, created by Ann Feeney, a library school graduate student at Rosary College in River Forest, Illinois, in the United States. It was moved to Concordia University, Chicago, then back to Rosary, which was then renamed Dominican University. From 2002 to 2005 it was maintained by the Dominican University Graduate School of Library and Information Science program. At the end of 2005 Dominican University ceased hosting the list. A replacement list, known as Project Wombat, commenced in January 2006, and is hosted by Project Gutenberg.\n\nOriginally the Stumpers-L archive was a gopher resource, but migrated to the World Wide Web once the web became more universally used in the mid-1990s.\n\nTypical Stumpers-L topics include:\n\nA book of Stumpers-L questions and answers was published in 1998 by Random House, edited by Fred Shapiro of Yale and titled \"Stumpers! Answers to Hundreds of Questions That Stumped The Experts\" (). Shapiro was an active member; other prominent members include Barbara and David P. Mikkelson, the co-editors of \"Snopes.com.\n\nThe unofficial mascot of the Stumpers-L list is the wombat.\n\n"}
{"id": "1007243", "url": "https://en.wikipedia.org/wiki?curid=1007243", "title": "Tertiary source", "text": "Tertiary source\n\nA tertiary source is an index or textual consolidation of primary and secondary sources. Some tertiary sources are not to be used for academic research, unless they can also be used as secondary sources, or to find other sources.\n\nDepending on the topic of research, a scholar may use a bibliography, dictionary, or encyclopedia as either a tertiary or a secondary source. This causes difficulty in defining many sources as either one type or the other.\n\nIn some academic disciplines the differentiation between a secondary and tertiary source is relative. \n\nIn the United Nations International Scientific Information System (UNISIST) model, a secondary source is a bibliography, whereas a tertiary source is a synthesis of primary sources.\n\nAs tertiary sources, encyclopedias, textbooks, and compendia attempt to summarize, collect, and consolidate the source materials into an overview, but may also present subjective, or biased commentary and analysis (which are characteristics of secondary sources).\n\nIndexes, bibliographies, concordances, and databases may not provide much textual information, but as aggregates of primary and secondary sources, they are often considered tertiary sources. So although tertiary sources are both primary and secondary, they are more towards a secondary source because of commentary and bias.\n\nAlmanacs, travel guides, field guides, and timelines are also examples of tertiary sources.\n\nSurvey or overview articles are usually tertiary, though review articles in peer-reviewed academic journals are secondary (not be confused with film, book, etc. reviews, which are primary-source opinions).\n\nSome usually primary sources, such as user guides and manuals, are secondary or tertiary (depending on the nature of the material) when written by third parties.\n\n"}
{"id": "26681002", "url": "https://en.wikipedia.org/wiki?curid=26681002", "title": "Text annotation", "text": "Text annotation\n\nText Annotation is the practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links. Text annotations can include notes written for a reader's private purposes, as well as shared annotations written for the purposes of collaborative writing and editing, commentary, or social reading and sharing. In some fields, text annotation is comparable to metadata insofar as it is added post hoc and provides information about a text without fundamentally altering that original text. Text annotations are sometimes referred to as marginalia, though some reserve this term specifically for hand-written notes made in the margins of books or manuscripts. Annotations are extremely useful and help to develop knowledge of English literature.\n\nThis article covers both private and socially shared text annotations, including hand-written and information technology-based annotation. For information on annotation of Web content, including images and other non-textual content, see also Web annotation.\n\nText annotation may be as old as writing on media, where it was possible to produce an additional copy with a reasonable effort. It became a prominent activity around 1000 AD in Talmudic commentaries and Arabic rhetorics treaties. In the Medieval era, scribes who copied manuscripts often made marginal annotations that then circulated with the manuscripts and were thus shared with the community; sometimes annotations were copied over to new versions when such manuscripts were later recopied.\n\nWith the rise of the printing press and the relative ease of circulating and purchasing individual (rather than shared) copies of texts, the prevalence of socially shared annotations declined and text annotation became a more private activity consisting of a reader interacting with a text. Annotations made on shared copies of texts (such as library books) are sometimes seen as devaluing the text, or as an act of defacement. Thus, print technologies support the circulation of annotations primarily as formal scholarly commentary or textual footnotes or endnotes rather than marginal, handwritten comments made by private readers, though handwritten comments or annotations were common in collaborative writing or editing.\n\nComputer-based technologies have provided new opportunities for individual and socially shared text annotations that support multiple purposes, including readers’ individual reading goals, learning, social reading, writing and editing, and other practices. Text annotation in Information Technology (IT) systems raises technical issues of access, linkage, and storage that are generally not relevant to paper-based text annotation, and thus research and development of such systems often addresses these areas.\n\nText annotations can serve a variety of functions for both private and public reading and communication practices. In their article \"From the Margins to the Center: The Future of Annotation,\" scholars Joanna Wolfe and Christine Neuwirth identify four primary functions that text annotations commonly serve in the modern era, including: (1)\"facilitat[ing] reading and later writing tasks,\" which includes annotations that support reading for both personal and professional purposes; (2)\"eavesdrop[ping] on the insights of other readers,\" which involves sharing of annotations; (3)\"provid[ing] feedback to writers or promote communication with collaborators,\" which can include personal, professional, and education-related feedback; and (4)\"call[ing] attention to topics and important passages,\" for which scholarly annotations, footnotes, and call-outs often function. Regarding the ways that annotations can support individual reading tasks, Catherine Marshall points out that the ways that readers annotate texts depends on the purpose, motivation, and context of reading. Readers may annotate to help interpret a text, to call attention to a section for future reference or reading, to support memory and recall, to help focus attention on the text as they read, to work out a problem related to the text, or create annotations not specifically related to the text at all.\n\nEducational research in text annotation has examined the role that both private and shared text annotations can play in supporting learning goals and communication. Much educational research examines how students’ private annotation of texts supports comprehension and memory; for example, research indicates that annotating texts causes more in-depth processing of information, which results in greater recall of information.\n\nOther areas of educational research investigate the benefits of socially shared text annotations for collaborative learning, both for paper-based and IT-based annotation sharing. For example, studies by Joanna Wolfe have investigated the benefits of exposure to others’ annotations on student readers and writers. In a 2000 study, Wolfe found that exposing students to others’ annotations influenced their perceptions of the annotators, which in turn shaped their responses to the material and their written products. In a later study, Wolfe found that viewing others’ written comments on a paper text, especially pairs of annotations that present opposing responses to the text, can help students engage in the type of critical reading and stance-taking necessary for effective argumentative writing.\n\nWhile shared annotations can benefit individual readers, it is important to note that, \"since the 1920s, literacy theory has increasingly emphasized the importance of social factors in the development of literacy.\" Thus, shared annotations can not only help one to better understand the content of a particular text, but may also aid in the acquirement of literacy skills. For example, a mother may leave marks inside a book to draw the attention of her child to a particular theme or concept; thanks to the development of audio annotations, parents may now leave notes for children who are just starting to read and may struggle with textual annotations.\n\nMore recent research in the effects of shared text annotations has focused on the learning applications for web-based annotation systems, some of which were developed based on design recommendations from studies outlined above. For example, Ananda Gunawardena, Aaron Tan, and David Kaufer conducted a pilot study to examine whether annotating documents in Classroom Salon, a web-based annotation and social reading platform, encouraged active reading, error detection, and collaboration in a computer science course at Carnegie Mellon University. This study suggested a correlation between students’ overall performance in the course and their ability to identify errors in a text that they annotated in Classroom Salon; it also found that students were likely to change their annotations in response to annotations made by others in the course.\n\nSimilarly, the web-based annotation tool HyLighter was used in a first-year writing course and shown to improve the development of students’ mental models of texts, including supporting reading comprehension, critical thinking, and the ability to develop a thesis. The collaboration with peers and experts around a shared text improved these skills and brought the communities’ understanding closer together.\n\nA meta-analysis of empirical studies into the higher-education uses of social annotation (SA) tools indicates such tools have been tested in several courses, among them English, sport psychology, and hypermedia. Studies have indicated that social annotation functions, including commenting, information sharing, and highlighting, can support instruction designed to foster collaborative learning and communication, as well as reading comprehension, metacognition, and critical analysis. Several studies indicated that students enjoyed using social annotation tools, and that it improved motivation in the course.\n\nText annotations have long been used in writing and revision processes as a way for reviewers to suggest changes and communicate about a text. In book publishing, for example, the collaboration of authors and editors to develop and revise a manuscript frequently involves exchanges of both in-line revisions or notes as well as marginal annotations. Similarly, copyeditors often make marginal annotations or notes that explain or suggest revisions or are directed at the author as questions or suggestions (commonly called \"queries\"). Asynchronous collaborative writing and document development often depend on text annotations as a way not only to suggest revisions but also to exchange ideas during document development or to facilitate group decision making, though such processes are often complicated by the use of different communication technologies (such as phone calls or emails as well as document sharing) for distinct tasks. Text annotations can also function to allow group or community members to communicate about a shared text, such as a doctor annotating a patient's chart.\n\nMuch research into the functionality and design of collaborative IT-based writing systems, which often support text annotation, has occurred in the area of computer-supported cooperative work.\n\nResearch in the design and development of annotation systems uses specific terminology to refer to distinct structural components of annotations and also distinguishes among options for digital annotation displays.\n\nThe structural components of any annotation can be roughly divided into three primary elements: a \"body\", an \"anchor\", and a \"marker\". The body of an annotation includes reader-generated symbols and text, such as handwritten commentary or stars in the margin. The anchor is what indicates the extent of the original text to which the body of the annotation refers; it may include circles around sections, brackets, highlights, underlines, and so on. Annotations may be anchored to very broad stretches of text (such as an entire document) or very narrow sections (such as a specific letter, word, or phrase). The marker is the visual appearance of the anchor, such as whether it is a grey underline or a yellow highlight. An annotation that has a body (such as a comment in the margin) but no specific anchor has no marker.\n\nIT-based annotation systems utilize a variety of display options for annotations, including:\nAnnotation interfaces may also allow highlighting or underlining, as well as threaded discussions. Sharing and communicating through annotations anchored to specific documents is sometimes referred to as \"anchored discussion\".\n\nIT-based annotation systems include standalone and client-server systems. In the 1980s and 1990s, a number of such systems were built in the context of libraries, patent offices, and legal text processing. Their design led researchers to produce taxonomies of annotation forms. Text annotation research has taken place at several institutions, including Xerox research centers in Palo Alto and Grenoble (France), the Hitachi Central Research Lab (in particular for annotation of patents), and in relation with the construction of the new French National Library between 1989 and 1995 at the Institut de Recherche en Informatique de Toulouse and in the company AIS (Advanced Innovation Systems).\n\nAnnotation functionality has been present in text processing software for many years through inline notes displayed as pop-ups, footnotes, and endnotes; however, it is only recently that functionality for displaying annotations as marginalia has appeared in programs such as OpenOffice.org/LibreOffice Writer and Microsoft Word. Personal or standalone annotation include word processing software that supports embedded or anchored text annotations as well as Adobe Acrobat, which in addition to commenting allows highlights, stamps, and other types of markup.\n\nTim Berners-Lee had already implemented the concept of directly editing web documents in 1990 in WorldWideWeb, the first web browser, but later ported versions removed this collaborative ability. An early version of NCSA Mosaic in 1993 also included a collaborative annotation capability, though it was quickly removed. Web Distributed Authoring and Versioning, WebDAV, was then reintroduced as an extension.\n\nA different approach to distributed authoring consists in first gathering many annotations from a wide public, and then integrate them all in order to produce a further version of a document. This approach was pioneered by Stet, the system put in place to gather comments on drafts of version 3 of the GNU General Public License. This system arose after a specific requirement, which it served egregiously, but was not so easily configurable as to be convenient for annotating any other document on the web. The co-ment system uses annotation interface concepts similar to Stet's, but it is based on an entirely new implementation, using Django/Python on the server side and various AJAX libraries such as JQuery on the client side. Both Stet and co-ment are licensed under the GNU Affero General Public License.\n\nSince 2011, the non-profit Hypothes Is Project has offered the free, open web annotation service Hypothes.is. The service features annotation via a Chrome extension, bookmarklet or proxy server, as well as integration into a LMS or CMS. Both webpages and PDFs can be annotated. Other web-based text annotation systems are collaborative software for distributed text editing and versioning, which also feature annotation and commenting interfaces. For example, HyLighter supports synchronous and asynchronous interactions, general commenting, comment tagging, threaded discussions and comment filtering. Other annotation tools under these category are more focused on NLP tasks as Named-entity recognition, relationship extraction or normalization. Some tools support manual tagging of data or automatic annotations via supervised learning.\n\nSpecialized Web-based text annotations exist in the context of scientific publication, either for refereeing or post-publication. The on-line journal PLoS ONE, published by the Public Library of Science, has developed its own Web-based system where scientists and the public can comment on published articles. The annotations are displayed as pop-ups with an anchor in the text.\n\n\n"}
{"id": "9032406", "url": "https://en.wikipedia.org/wiki?curid=9032406", "title": "Tupper's self-referential formula", "text": "Tupper's self-referential formula\n\nTupper's self-referential formula is a formula that visually represents itself when graphed at a specific location in the (\"x\", \"y\") plane.\n\nThe formula was defined by Jeff Tupper and appears as an example in Tupper's 2001 SIGGRAPH paper on reliable two-dimensional computer graphing algorithms.\n\nAlthough the formula is called \"self-referential\", Tupper did not name it as such.\n\nThe formula is an inequality defined as:\n\nor, as plaintext,\nwhere ⌊ ⌋ denotes the floor function, and mod is the modulo operation.\n\nLet \"k\" equal the following 543-digit integer:\n\nIf one graphs the set of points (\"x\", \"y\") in 0 ≤ \"x\" < 106 and \"k\" ≤ \"y\" < \"k\" + 17 satisfying the inequality given above, the resulting graph looks like this (the axes in this plot have been reversed, otherwise the picture would be upside-down and mirrored):\n\nThe formula is a general-purpose method of decoding a bitmap stored in the constant \"k\", and it could actually be used to draw any other image. When applied to the unbounded positive range 0 ≤ \"y\", the formula tiles a vertical swath of the plane with a pattern that contains all possible 17-pixel-tall bitmaps. One horizontal slice of that infinite bitmap depicts the drawing formula itself, but this is not remarkable, since other slices depict all other possible formulae that might fit in a 17-pixel-tall bitmap. Tupper has created extended versions of his original formula that rule out all but one slice.\n\nThe constant \"k\" is a simple monochrome bitmap image of the formula treated as a binary number and multiplied by 17. If \"k\" is divided by 17, the least significant bit encodes the upper-right corner (\"k\", 0); the 17 least significant bits encode the rightmost column of pixels; the next 17 least significant bits encode the 2nd-rightmost column, and so on.\n\n\n"}
