{"id": "56753217", "url": "https://en.wikipedia.org/wiki?curid=56753217", "title": "APLL", "text": "APLL\n\nAPLL is an acronym and may refer to:\n"}
{"id": "1230569", "url": "https://en.wikipedia.org/wiki?curid=1230569", "title": "Acronym Finder", "text": "Acronym Finder\n\nAcronym Finder (AF) is a free online searchable dictionary and database of abbreviations (acronyms, initialisms, and others) and their meanings.\n\nThe entries are classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes. For abbreviations with multiple meanings they are listed by popularity with the most common one being listed first. it claims to have over a million \"human-edited\" and verified definitions.\n\nAcronym Finder was registered and the database put online by Michael K. Molloy of Colorado in 1997 but he began compiling it in 1985 working as a computer systems officer for the USAF. Molloy first saw the need of an acronym list while integrating computers at the Randolph Air Force Base in Texas His first acronym list running up-to 30 pages. When he had retired and put AF online in 1997, his list already had 43,000 acronyms. It began mainly as a list of Military/Government abbreviations before expanding to other areas.\n\nMolloy and his wife served as the editors of the website verifying user submissions for abbreviations and adding others they found to the database. Molloy has also provided opinions on abbreviations such as \"MSG\" which Madison Square Garden wanted as a domain name (\"msg.com\") claiming trademark to the abbreviated letters. He stated that MSG also stood for more common things such as monosodium glutamate and message among others. The Garden in the end settled out of court and came to own msg.com.\n\nThe website was maintained under Mountain Data Systems, LLC by Molloy before being sold off and eventually coming under the ownership of Farlex, Inc. publishers of Thefreedictionary.com.\n\nThe website contains a database of meanings and expansions for abbreviations, acronyms, initialisms mainly in English but includes some entries in other languages such as French, German, Spanish etc. as well. It is freely accessible. The entries are further classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes which are shown on a Map along with location information. Abbreviations with multiple expansions are listed by popularity with the most common one being presented first, these can be sorted alphabetically as well.\n\nAnyone can contribute to the database by submitting abbreviations and their meanings, these are reviewed by an by editor and categorized before being added to the database. While the database has been described as fairly accurate errors have been found in the meanings and expansions of abbreviations. The website does not list sources for the abbreviations and their meanings but it does identify people who have contributed more than 50 abbreviations to the database.\n\nThe database only contains abbreviations and their expansions and does not list other data such as grammatical category, context, source, field of the abbreviation etc.\n\nFarlex, Inc. the current owner of the website also publishes mobile apps for the Android and iOS operating systems.\n\nAcronym Finder also includes a \"Systematic Buzz Phrase Projector\", a light-hearted tool that randomly generates jargon-like phrases and abbreviations — usually initialisms that would be unpronounceable as acronyms — and meanings from 30 cleverly chosen buzz words.\n\nThe website is supported through advertisements.\n\nThe website is listed as a quick reference tool in directories like Stanford Library, Library of Congress, USC Library. It has been cited as the largest database of acronyms and has been used in computational studies for its database.\n\nListings of abbreviations on the website have also been used as a defense that an abbreviation is in public use and cannot be trademarked. While in some trademark cases citations for AF have been accepted it has been described as an unreliable reference in others.\n\nIt has garnered criticism for the fact that anyone can submit abbreviations to the site and the content is user generated. Mike Molloy the site's original owner had defended that each submission is verified before being added to the database.\n\n"}
{"id": "46407896", "url": "https://en.wikipedia.org/wiki?curid=46407896", "title": "Bibliography of C. Northcote Parkinson", "text": "Bibliography of C. Northcote Parkinson\n\n"}
{"id": "1250078", "url": "https://en.wikipedia.org/wiki?curid=1250078", "title": "Cf.", "text": "Cf.\n\nThe abbreviation cf. (short for the , both meaning \"compare\") is used in writing to refer the reader to other material to make a comparison with the topic being discussed. It is used to form a contrast, for example: \"Abbott (2010) found supportive results in her memory experiment, unlike those of previous work (cf. Zeller & Williams, 2007).\" It is recommended that \"cf.\" be used only to suggest a comparison, and the word \"see\" be used to point to a source of information.\n\nIn biological naming conventions, cf. is commonly placed between the genus name and the species name to describe a specimen that is difficult to identify because of practical difficulties, such as the specimen being poorly preserved. For example, \"' cf. '\" indicates that the specimen is in the genus \"Barbus\", and believed to be \"\" but the actual species-level identification cannot be certain.\n\nCf. can also be used to express a possible identity, or at least a significant resemblance, such as between a newly observed specimen and a known species or taxon. Such a usage might suggest a specimen's membership of the same genus or possibly of a shared higher taxon, such as in, \", cf. \"\"\", where the author is confident of the order and family (Diptera: Tabanidae), but can only offer the genus (\"Tabanus\") as a suggestion and has no information favouring a particular species.\n\n"}
{"id": "97585", "url": "https://en.wikipedia.org/wiki?curid=97585", "title": "Citation", "text": "Citation\n\nA citation is a reference to a published or unpublished source. More precisely, a citation is an abbreviated alphanumeric expression embedded in the body of an intellectual work that denotes an entry in the bibliographic references section of the work for the purpose of acknowledging the relevance of the works of others to the topic of discussion at the spot where the citation appears. Generally the combination of both the in-body citation and the bibliographic entry constitutes what is commonly thought of as a citation (whereas bibliographic entries by themselves are not). References to single, machine-readable assertions in electronic scientific articles are known as nanopublications, a form of microattribution.\n\nCitations have several important purposes: to uphold intellectual honesty (or avoiding plagiarism), to attribute prior or unoriginal work and ideas to the correct sources, to allow the reader to determine independently whether the referenced material supports the author's argument in the claimed way, and to help the reader gauge the strength and validity of the material the author has used. As Roark and Emerson have argued, citations relate to the way authors perceive the substance of their work, their position in the academic system, and the moral equivalency of their place, substance, and words. Despite these attributes, many drawbacks and shortcoming of citation practices have been reported, including for example honorary citations, circumstantial citations, discriminatory citations, selective and arbitrary citations.\n\nThe forms of citations generally subscribe to one of the generally accepted citations systems, such as the Oxford, Harvard, MLA, American Sociological Association (ASA), American Psychological Association (APA), and other citations systems, because their syntactic conventions are widely known and easily interpreted by readers. Each of these citation systems has its advantages and disadvantages. Editors often specify the citation system to use.\n\nBibliographies, and other list-like compilations of references, are generally not considered citations because they do not fulfill the true spirit of the term: deliberate acknowledgement by other authors of the priority of one's ideas.\n\nA bibliographic citation is a reference to a book, article, web page, or other published item. Citations should supply detail to identify the item uniquely. Different citation systems and styles are used in scientific citation, legal citation, prior art, the arts, and the humanities.\n\nCitation content can vary depending on the type of source and may include:\n\nAlong with information such as author(s), date of publication, title and page numbers, citations may also include unique identifiers depending on the type of work being referred to.\n\nBroadly speaking, there are two types of citation systems, the Vancouver system and parenthetical referencing. However, the Council of Science Editors (CSE) adds a third, the\" citation-name system\".\n\nThe Vancouver system uses sequential numbers in the text, either bracketed or superscript or both. The numbers refer to either footnotes (notes at the end of the page) or endnotes (notes on a page at the end of the paper) that provide source detail. The notes system may or may not require a full bibliography, depending on whether the writer has used a full-note form or a shortened-note form.\n\nFor example, an excerpt from the text of a paper using a notes system \"without\" a full bibliography could look like:\n\nThe note, located either at the foot of the page (footnote) or at the end of the paper (endnote) would look like this:\n\nIn a paper with a full bibliography, the shortened note might look like:\n\nThe bibliography entry, which is required with a shortened note, would look like this:\n\nIn the humanities, many authors also use footnotes or endnotes to supply anecdotal information. In this way, what looks like a citation is actually supplementary material, or suggestions for further reading.\n\nParenthetical referencing, also known as Harvard referencing, has full or partial, in-text, citations enclosed in circular brackets and embedded in the paragraph.\n\nAn example of a parenthetical reference:\n\nDepending on the choice of style, fully cited parenthetical references may require no end section. Other styles include a list of the citations, with complete bibliographical references, in an end section, sorted alphabetically by author. This section is often called \"References\", \"Bibliography\", \"Works cited\" or \"Works consulted\".\n\nIn-text references for online publications may differ from conventional parenthetical referencing. A full reference can be hidden, only displayed when wanted by the reader, in the form of a tooltip. This style makes citing easier and improves the reader's experience.\n\nSuperscripted numbers are inserted at the point of reference, just as in the citation‐sequence system, but the citations are numbered according to the order of cited works at the end of the paper or book; this list is often sorted alphabetically by author.\n\nCitation styles can be broadly divided into styles common to the Humanities and the Sciences, though there is considerable overlap. Some style guides, such as the Chicago Manual of Style, are quite flexible and cover both parenthetical and note citation systems. Others, such as MLA and APA styles, specify formats within the context of a single citation system. These may be referred to as citation formats as well as citation styles. The various guides thus specify order of appearance, for example, of publication date, title, and page numbers following the author name, in addition to conventions of punctuation, use of italics, emphasis, parenthesis, quotation marks, etc., particular to their style.\n\nA number of organizations have created styles to fit their needs; consequently, a number of different guides exist. Individual publishers often have their own in-house variations as well, and some works are so long-established as to have their own citation methods too: Stephanus pagination for Plato; Bekker numbers for Aristotle; citing the Bible by book, chapter and verse; or Shakespeare notation by play.\n\n\nIn some areas of the Humanities, footnotes are used exclusively for references, and their use for conventional footnotes (explanations or examples) is avoided. In these areas, the term \"footnote\" is actually used as a synonym for \"reference\", and care must be taken by editors and typesetters to ensure that they understand how the term is being used by their authors.\n\n\n\n\nIn their research on footnotes in scholarly journals in the field of communication, Michael Bugeja and Daniela V. Dimitrova have found that citations to online sources have a rate of decay (as cited pages are taken down), which they call a \"half-life\", that renders footnotes in those journals less useful for scholarship over time.\n\nOther experts have found that published replications do not have as many citations as original publications.\n\nAnother important issue is citation errors, which often occur due to carelessness on either the researcher or journal editor's part in the publication procedure. Experts have found that simple precautions, such as consulting the author of a cited source about proper citations, reduce the likelihood of citation errors and thus increase the quality of research.\n\nResearch suggests the impact of an article can be, partly, explained by superficial factors and not only by the scientific merits of an article. Field-dependent factors are usually listed as an issue to be tackled not only when comparison across disciplines are made, but also when different fields of research of one discipline are being compared. For example, in medicine, among other factors, the number of authors, the number of references, the article length, and the presence of a colon in the title influence the impact; while in sociology the number of references, the article length, and title length are among the factors.\n\nCitation patterns are also known to be affected by unethical behavior of both the authors and journal staff. Such behavior is called impact factor boosting, and was reported to involve even the top-tier journals. Specifically the high-ranking journals of medical science, including the Lancet, JAMA and New England Journal of Medicine, are thought to be associated with such behavior, with up to 30% of citations to these journals being generated by commissioned opinion articles. On the other hand, the phenomenon of citation cartels is rising. Citation cartels are defined as groups of authors that cite each other disproportionately more than they do other groups of authors who work on the same subject.\n\n"}
{"id": "10018490", "url": "https://en.wikipedia.org/wiki?curid=10018490", "title": "Citation Style Language", "text": "Citation Style Language\n\nThe Citation Style Language (CSL) is an open XML-based language to describe the formatting of citations and bibliographies. Reference management programs using CSL include Zotero, Mendeley and Papers.\n\nCSL was created by Bruce D'Arcus for use with OpenOffice.org, and an XSLT-based \"CiteProc\" CSL processor. CSL was further developed in collaboration with Zotero developer Simon Kornblith. Since 2008, the core development team consists of D'Arcus, Frank Bennett and Rintze Zelle.\n\nThe releases of CSL are 0.8 (March 21, 2009), 0.8.1 (February 1, 2010), 1.0 (March 22, 2010), and 1.0.1 (September 3, 2012). CSL 1.0 was a backward-incompatible release, but styles in the 0.8.1 format can be automatically updated to the CSL 1.0 format.\n\nOn its release in 2006, Zotero became the first application to adopt CSL. In 2008 Mendeley was released with CSL support, and in 2011, Papers and Qiqqa gained support for CSL-based citation formatting.\n\n\nThe CSL project maintains a CSL 1.0 style repository, which contains over 9000 styles (more than 1700 unique styles).\n\n"}
{"id": "14322444", "url": "https://en.wikipedia.org/wiki?curid=14322444", "title": "Comparative advertising", "text": "Comparative advertising\n\nComparative advertising or advertising war is an advertisement in which a particular product, or service, specifically mentions a competitor by name for the express purpose of showing why the competitor is inferior to the product naming it. Also referred to as \"knocking copy\", it is loosely defined as advertising where “the advertised brand is explicitly compared with one or more competing brands and the comparison is obvious to the audience.”\n\nThis should not be confused with parody advertisements, where a fictional product is being advertised for the purpose of poking fun at the particular advertisement, nor should it be confused with the use of a coined brand name for the purpose of comparing the product without actually naming an actual competitor. (\"Wikipedia tastes better and is less filling than the Encyclopedia Galactica.\")\n\nIn the United States, the Federal Trade Commission (FTC) defined comparative advertising as “advertisement that compares alternative brands on objectively measurable attributes or price, and identifies the alternative brand by name, illustration or other distinctive information.” This definition was used in the case Gillette Australia Pty Ltd v Energizer Australia Pty Ltd. Similarly, the Law Council of Australia recently suggested that comparative advertising refers to “advertising which include reference to a competitor’s trademark in a way which does not impute proprietorship in the mark to the advertiser.”\n\nComparative advertisements could be either indirectly or directly comparative, positive or negative, and seeks “to associate or differentiate the two competing brands”. Different countries apply differing views regarding the laws on comparative advertising.\n\nThe earliest court case concerning comparative advertising dates back to 1910 in the United States – Saxlehner v Wagner. Prior to the 1970s, comparative advertising was deemed unfeasible due to related risks. For instance, comparative advertising could invite misidentification of products, potential legal issues, and may even win public sympathy for their competitors as victims.\n\nIn 1972, the FTC began to encourage advertisers to make comparison with named competitors, with the broad, public welfare objective of creating more informative advertising. The FTC argued that this form of advertising could also stimulate comparison shopping, encourage product improvement and innovation, and foster a positive competitive environment. However, studies have shown that while comparative advertisements had increased since 1960, the relative amount of comparative advertising is still small.\n\nPrior to 1997, many European countries severely limited comparative claims as an advertising practice. For example, in Germany comparisons in advertising had since the 1930's been largely prohibited as an anti-competitive practice, with very limited exceptions for cases where the advertiser had a good reason for presenting a critical claim, and reference to a competitor was necessary in order to present that claim. Importantly, this only applied to \"critical\" claims - claims of equivalence were completely prohibited. A similar approach had been adopted in France, where comparative advertising was commonly seen as disparaging of competitors. However, the legalisation of comparative advertising in France in 1992, opened the door to a general legalisation of comparative advertising through EU law, which had first been proposed by the European Commission in 1978. The result was the adoption of Directive 97/55/EC, which came into force in the year 2000. The relevant provisions are now contained in Directive 2006/114/EC.\n\nThis Directive sets out rules that comparative advertising must comply with in order to be considered permissible. These include the requirements that the comparison concern goods and services that meet the same purpose, that it objectively compare the relevant characteristics of the products concerned and that it not cause confusion or denigrate the trademarks and other distinguishing signs of competitors. The Directive prohibits comparisons that take unfair advantage of the reputation of a competitor's distinguishing marks, or present goods or services as imitations of products covered by a protected trade mark or trade name. Additionally, any comparison aimed at promoting goods bearing a protected designation of origin must refer exclusively to other goods bearing the same designation. Directive 2006/114/EC constitutes a total harmonisation of the rules on comparative advertising, meaning that the Member States are neither allowed to permit comparisons that breach the requirements of the Directive, nor prohibit ones that do. \n\nFurther, while trademark rights can in principle be used to prevent comparative advertising that makes unauthorised use of a competitor's trademark, this is not the case where the comparative advertisement complies with all the requirements of Directive 2006/114/EC. Legitimate comparative advertising must therefore be seen as an exception to the exclusive rights of the trademark proprietor. However, the trademark proprietor can, thanks to the prohibition on taking unfair advantage of a trademark's reputation, oppose the use of their trademark where it is not aimed at distinguishing the products of the advertiser and trademark proprietor and to highlight their differences objectively, but rather at riding on the coat-tails of that mark in order to benefit from its reputation.\n\nThe requirements set out by the Directive have resulted in some controversy. This is particularly true of the \"per se\" prohibition on comparisons presenting goods and services as imitations of trademarked products. In this regard, EU law contrasts starkly with the US approach; the US courts have long held that traders are allowed to the trademarked names of products they have imitated in advertising. In contrast, in L'Oréal and others v. Bellure, the Court of Justice held that smell-alike perfumes marketed through comparison lists breached this condition. This decision was criticised both by the English courts and by scholars, who have considered that this places unjustified limits on advertising acts that are otherwise fully legal, such as copying that does not infringe intellectual property rights. \n\nIn the UK, most of the use of competitor’s registered trademark in a comparative advertisement was an infringement of the registration up till the end of 1994. However, the laws on comparative advertising were harmonized in 2000. The current rules on comparative advertising are regulated by a series of EU Directives. The Business Protection from Misleading Marketing Regulations 2008 implements provisions of Directive (EC) 2006/114 in the UK.\n\nOne of the classic cases of comparative advertising in the UK was the O2 v Hutchison case. The European Court of Justice (ECJ) held that there could have been a trademark infringement when a comparative advertiser used the registered trademark for the advertiser’s own goods and services. It was also held that a trademark proprietor could not prevent a competitor’s use of a sign similar or identical to his mark in a comparative advertisement, which satisfies all the conditions of the Comparative Advertising Directive. If the Advocate General's decision in the O2 case were followed by the ECJ, competitors will not be able to use trademark legislation either to prevent a comparative advertisement through an injunction or to charge in respect of its use. Conversely, in British Airways plc v Ryanair Ltd. a lenient approach was adopted by the UK courts. The use of competitors’ trademarks was no longer restricted for businesses competing within an industry, provided that compliance of the conditions set out in the legislation were performed. This meant that businesses are able to use the trademarks of other companies and trade names to distinguish the relative merits of their own products and services over those of their competitors.\n\nThe FTC and the National Advertising Division of the Council of Better Business Bureaus, Inc. (NAD), govern the laws of comparative advertising in the United States including the treatment of comparative advertising claims. FTC stated that comparative advertising could benefit consumers and encourages comparative advertising, provided that the comparisons are “clearly identified, truthful, and non-deceptive”. Although comparative advertising is encouraged, NAD has stated “claims that expressly or implicitly disparage a competing product should be held to the highest level of scrutiny in order to ensure that they are truthful, accurate, and narrowly drawn.” Another major law is the trademark protective Lanham Act, which states that one could incur liability when the message of the comparative advertisement is untrue or uncertain, but has the intention to deceive consumers through the implied message conveyed.\n\nIn Australia, no specific law governs comparative advertising although certain cases regarding this matter have occurred. Comparative advertising that is truthful, and does not lead to confusion is permitted.\n\nGenerally, Australian advertisers should make sure that the following are complied when exercising comparative advertising to avoid breaches regarding misleading advertising under Australia Consumer Law:\n\n\nThe law in Hong Kong regarding comparative advertising is the law that existed in the UK prior to the enactment of the UK Act 1994. Hong Kong has no legislation exclusively intended at limiting false or misleading advertisements. Still, the Trade Descriptions Ordinance (Cap 362) bans the use of false trade descriptions in advertisements. The tort of trade libel also exists to deal with false or misleading advertisements designed to injure the competitor. Consumer Council may have the authority to publish information with a perspective to amending false or misleading advertisements, while the Association of Accredited Advertising Agencies of Hong Kong have the authority to take action against members who organize advertisements that are inaccurate.\n\nIn Argentina, there is no specific statute dealing with comparative advertising (so it is not forbidden), but there are clear jurisprudential rules based on unfair competition law. If in some manner an advertisement is proven to be unfair or exceeds ethical standards by hiding the truth or omitting some essential aspect of the comparison, it is probable that an injunction will be granted and that the plaintiff will be able to obtain a final decision declaring the advertising illegal.\n\nNumerous cases follow international precedent in referring to the requirements of the European Union Directive on comparative advertising. By following these criteria, Argentine courts have developed standards very similar to European regulation. It is as if the judges wanted to validate the law created by the Courts with an external source. Similar conclusions reached elsewhere indicate the existence of universally accepted principles that accept that comparing products in commercial advertisements should be lawful.\n\nIn Brazil, the allow comparative advertising with certain restrictions. Its primary purpose shall be the clarification or consumer’s protection; it shall have as basic principle the objectiveness of the comparison since subjective data, psychological or emotionally based data does not constitute a valid comparison basis for consumers; the purposed or implemented comparison shall be capable of being supported byevidence; in the case of consumption goods, the comparison shall be made with models manufactured in the same year and no comparison shall be made between products manufactured in different years, unless it is only a reference to show evolution, in which case the evolution shall be clearly demonstrated; there shall be no confusion between the products and competitor’s brands; there shall be no unfair competition, denigration of the product’s image or another company’s product; and there shall be no unreasonable use of the corporate image or goodwill of third parties.\n\nLikewise, the majority of the Brazilian authors is inclined to say that its legitimacy depends to meet certain requirements, which, in general, would be stipulated by Article 3a of Directive 84/450/EEC \n\nIn an early Mercosur's rules through Resolution 126/96.\n\nComparative advertising has been increasingly implemented through the years, and the types of comparative advertising range from comparing a single attribute dimension, comparing an attribute unique to the target and absent in the referent and comparisons involving attributes unique to both brands. The contributing factors to the effectiveness of comparative advertising include believability, which refers to the extent a consumer can rely on the information provided in comparative advertisements, the level of involvement, and the convenience in evaluation, provided by spoon feeding the consumer with information that does not require extra effort in recall.\n\nComparative advertising is generally coupled with negativity, as evidenced by early industry condemnation. Stating reasons such as participation in comparative advertising damaged the honour and credibility of advertising. Studies have suggested that negative information can be stored more effectively, thus generating the impact that any advertisement is purposed for, and more importantly, strong recall. On the contrary, such negativity can either be transferred directly to the brand and the consumer’s impression of the brand, various studies through the years have proven that comparative advertising has been responded to negatively.\n\nComparative advertising has been used effectively by companies like The National Australia Bank (NAB), and its “break up” campaign has made such an impact it has won an award from Cannes, and a substantial increase in its consumer interest. Internationally acclaimed Apple Inc. has effectively utilized its Mac vs PC advertisements as part of its marketing efforts to increase its market share over the years. Such companies prove the academic view that comparative advertising is more successful when used by established brands, justified by the credibility and attention an established brand brings. Other famous examples include L’Oreal SA v Bellure NV and Coca Cola v Pepsi. Comparative advertising has to be executed with caution and deep consideration for the targeted markets as the novelty of the concept affects the effectiveness of the stipulated campaigns.\n\nIn the 1980s, during what has been referred to as the cola wars, soft-drink manufacturer Pepsi ran a series of advertisements where people, caught on hidden camera, in a blind taste test, chose Pepsi over rival Coca-Cola.\nThe use of comparative advertising has been well established in political campaigns, where typically one candidate will run ads where the record of the other candidate is displayed, for the purpose of disparaging the other candidate. The most famous of these type ads, which only ran once on TV, consisted of a child picking daisies in a field, while a voice which sounded like Barry Goldwater performed a countdown to zero before the launch of a nuclear weapon which explodes in a mushroom cloud. The ad, \"Daisy\", was produced by Lyndon B. Johnson's campaign in an attempt to prevent Goldwater from either winning the nomination of his party or being selected.\n\nAnother example took place throughout the late 1980s between the bitter rivals Nintendo and Sega. \"Genesis does what Nintendon't\" immediately became a catchphrase following the release of the Sega Genesis (known as Mega Drive in PAL countries).\n\nA 30-second commercial promoting sustainability, showing soda bottles exploding each time a person makes a drink using his Sodastream machine, was banned in the United Kingdom in 2012. Clearcast, the organization that preapproves TV advertising in the U.K., explained that they \"thought it was a denigration of the bottled drinks market.\" The same ad, crafted by Alex Bogusky, ran in the United States, Sweden, Australia, and other countries. An appeal by Sodastream to reverse Clearcast's decision to censor the commercial was rejected. A similar ad was expected to air during Super Bowl XLVII in February 2013 but was banned by CBS for jabbing at Coke and Pepsi (two of CBS's largest sponsors).\n\nIn 2012, Microsoft's Bing (formerly MSN Search) began to run a campaign about which search engine they prefer as it compared Bing to Google, and that more people preferred Bing over Google. The campaign was titled \"Bing It On\".\n"}
{"id": "14308145", "url": "https://en.wikipedia.org/wiki?curid=14308145", "title": "Comparative bullet-lead analysis", "text": "Comparative bullet-lead analysis\n\nComparative bullet-lead analysis (CBLA), also known as compositional bullet-lead analysis, is a now discredited and abandoned forensic technique which used chemistry to link crime scene bullets to ones possessed by suspects on the theory that each batch of lead had a unique elemental makeup.\n\nThe technique was first used after U.S. President John F. Kennedy's assassination in 1963. From the early 1980s through 2004 the US Federal Bureau of Investigation conducted about 2,500 analyses on cases submitted by law-enforcement groups. The results of these analyses had often been questioned by defence lawyers and the press, so the FBI finally asked the United States National Academy of Science's Board on Science, Technology, and Economic Policy to research the scientific merit of the process.\n\nIn 2004 the Board's study was summarized in \"Forensic Analysis: Weighing Bullet Lead Evidence.\" The Board determined that the chemical analyses were being performed correctly and were probably sufficient to determine correlation between two bullets from separate sources (the analysis used plasma-optical emission spectroscopy to identify trace elements in the bullets). The report also concluded that the seven trace elements selected for the analyses (arsenic, antimony, tin, copper, bismuth, silver and cadmium) are acceptable for sample correlation. The report finally concluded that the procedure is the best available method for such correlations. The greatest caveat in the report was that the statistical tests as applied by the FBI could cause confusion and misinterpretation when transmitted to prosecutors or when explained to a trial jury. Because of the significance of this weakness, the report concluded that the analysis should be used with caution. This report helped the FBI decide in 2004 to voluntarily cease offering the analysis to law-enforcement entities. The National Academy of Sciences never required that the FBI stop using the test.\n\n\"CNN PRESENTS Encore Presentation: Reasonable Doubt\" examined the unreliability of this technique. It has been discontinued as of September 1, 2005.\n\nThe U.S. government has fought releasing the list of the estimated 2,500 cases over three decades in which it performed the analysis, which may have led to false convictions. According to the FBI, only 20% of the 2,500 tests performed introduced the CBLA results into evidence at trial.\n\nOn 17 December 2008, Jimmy Ates was released from a Florida prison after serving ten years on the conviction of having murdered his wife, a conviction obtained largely on the strength of a bullet-lead analysis. His conviction was overturned as a consequence of the 2004 report.\n\n"}
{"id": "4481195", "url": "https://en.wikipedia.org/wiki?curid=4481195", "title": "Comparative cultural studies", "text": "Comparative cultural studies\n\nComparative cultural studies is a contextual approach to the study of culture in a global and intercultural context. Focus is placed on the theory, method, and application of the study process(es) rather than on the \"what\" of the object(s) of study.\n\nIn comparative cultural studies, selected tenets of comparative literature are merged with selected tenets of the field of cultural studies (including culture theories, (radical) constructivism, communication theories, and systems theories) with the objective to study culture and culture products (including but not restricted to literature, communication, media, art, etc.). This is performed in a contextual and relational construction and with a plurality of methods and approaches, interdisciplinary, and, if and when required, including teamwork. In comparative cultural studies, it is the processes of communicative action(s) in culture and the how of these processes that constitute the main objectives of research and study. However, scholarship in comparative cultural studies does not exclude textual analysis proper of other established fields of study. In comparative cultural studies, ideally, the framework of and methodologies available in the systemic and empirical study of culture are favored. Scholarship in comparative cultural studies includes the theoretical, as well as methodological and applied postulate to move and to dialogue between cultures, languages, literature, and disciplines: attention to other cultures against essentialist notions and practices and beyond the paradigm of the nation-state is a basic and founding element of the framework and its application.\n\n\n"}
{"id": "983601", "url": "https://en.wikipedia.org/wiki?curid=983601", "title": "Comparative genomic hybridization", "text": "Comparative genomic hybridization\n\nComparative genomic hybridization is a molecular cytogenetic method for analysing copy number variations (CNVs) relative to ploidy level in the DNA of a test sample compared to a reference sample, without the need for culturing cells. The aim of this technique is to quickly and efficiently compare two genomic DNA samples arising from two sources, which are most often closely related, because it is suspected that they contain differences in terms of either gains or losses of either whole chromosomes or subchromosomal regions (a portion of a whole chromosome). This technique was originally developed for the evaluation of the differences between the chromosomal complements of solid tumor and normal tissue, and has an improved resolution of 5–10 megabases compared to the more traditional cytogenetic analysis techniques of giemsa banding and fluorescence in situ hybridization (FISH) which are limited by the resolution of the microscope utilized.\n\nThis is achieved through the use of competitive fluorescence in situ hybridization. In short, this involves the isolation of DNA from the two sources to be compared, most commonly a test and reference source, independent labelling of each DNA sample with fluorophores (fluorescent molecules) of different colours (usually red and green), denaturation of the DNA so that it is single stranded, and the hybridization of the two resultant samples in a 1:1 ratio to a normal metaphase spread of chromosomes, to which the labelled DNA samples will bind at their locus of origin. Using a fluorescence microscope and computer software, the differentially coloured fluorescent signals are then compared along the length of each chromosome for identification of chromosomal differences between the two sources. A higher intensity of the test sample colour in a specific region of a chromosome indicates the gain of material of that region in the corresponding source sample, while a higher intensity of the reference sample colour indicates the loss of material in the test sample in that specific region. A neutral colour (yellow when the fluorophore labels are red and green) indicates no difference between the two samples in that location.\n\nCGH is only able to detect unbalanced chromosomal abnormalities. This is because balanced chromosomal abnormalities such as reciprocal translocations, inversions or ring chromosomes do not affect copy number, which is what is detected by CGH technologies. CGH does, however, allow for the exploration of all 46 human chromosomes in single test and the discovery of deletions and duplications, even on the microscopic scale which may lead to the identification of candidate genes to be further explored by other cytological techniques.\n\nThrough the use of DNA microarrays in conjunction with CGH techniques, the more specific form of array CGH (aCGH) has been developed, allowing for a locus-by-locus measure of CNV with increased resolution as low as 100 kilobases. This improved technique allows for the aetiology of known and unknown conditions to be discovered.\n\nThe motivation underlying the development of CGH stemmed from the fact that the available forms of cytogenetic analysis at the time (giemsa banding and FISH) were limited in their potential resolution by the microscopes necessary for interpretation of the results they provided. Furthermore, giemsa banding interpretation has the potential to be ambiguous and therefore has lowered reliability, and both techniques require high labour inputs which limits the loci which may be examined.\n\nThe first report of CGH analysis was by Kallioniemi and colleagues in 1992 at the University of California, San Francisco, who utilised CGH in the analysis of solid tumors. They achieved this by the direct application of the technique to both breast cancer cell lines and primary bladder tumors in order to establish complete copy number karyotypes for the cells. They were able to identify 16 different regions of amplification, many of which were novel discoveries.\n\nSoon after in 1993, du Manoir et al. reported virtually the same methodology. The authors painted a series of individual human chromosomes from a DNA library with two different fluorophores in different proportions to test the technique, and also applied CGH to genomic DNA from patients affected with either Downs syndrome or T-cell prolymphocytic leukemia as well as cells of a renal papillary carcinoma cell line. It was concluded that the fluorescence ratios obtained were accurate and that differences between genomic DNA from different cell types were detectable, and therefore that CGH was a highly useful cytogenetic analysis tool.\n\nInitially, the widespread use of CGH technology was difficult, as protocols were not uniform and therefore inconsistencies arose, especially due to uncertainties in the interpretation of data. However, in 1994 a review was published which described an easily understood protocol in detail and the image analysis software was made available commercially, which allowed CGH to be utilised all around the world.\nAs new techniques such as microdissection and degenerate oligonucleotide primed polymerase chain reaction (DOP-PCR) became available for the generation of DNA products, it was possible to apply the concept of CGH to smaller chromosomal abnormalities, and thus the resolution of CGH was improved.\n\nThe implementation of array CGH, whereby DNA microarrays are used instead of the traditional metaphase chromosome preparation, was pioneered by Solinas-Tolodo et al. in 1997 using tumor cells and Pinkel et al. in 1998 by use of breast cancer cells. This was made possible by the Human Genome Project which generated a library of cloned DNA fragments with known locations throughout the human genome, with these fragments being used as probes on the DNA microarray. Now probes of various origins such as cDNA, genomic PCR products and bacterial artificial chromosomes (BACs) can be used on DNA microarrays which may contain up to 2 million probes. Array CGH is automated, allows greater resolution (down to 100 kb) than traditional CGH as the probes are far smaller than metaphase preparations, requires smaller amounts of DNA, can be targeted to specific chromosomal regions if required and is ordered and therefore faster to analyse, making it far more adaptable to diagnostic uses.\n\nThe DNA on the slide is a reference sample, and is thus obtained from a karyotypically normal man or woman, though it is preferential to use female DNA as they possess two X chromosomes which contain far more genetic information than the male Y chromosome. Phytohaemagglutinin stimulated peripheral blood lymphocytes are used. 1mL of heparinised blood is added to 10ml of culture medium and incubated for 72 hours at 37 °C in an atmosphere of 5% CO. Colchicine is added to arrest the cells in mitosis, the cells are then harvested and treated with hypotonic potassium chloride and fixed in 3:1 methanol/acetic acid.\n\nOne drop of the cell suspension should then be dropped onto an ethanol cleaned slide from a distance of about 30 cm, optimally this should be carried out at room temperature at humidity levels of 60–70%. Slides should be evaluated by visualisation using a phase contrast microscope, minimal cytoplasm should be observed and chromosomes should not be overlapping and be 400–550 bands long with no separated chromatids and finally should appear dark rather than shiny. Slides then need to be air dried overnight at room temperature, and any further storage should be in groups of four at −20 °C with either silica beads or nitrogen present to maintain dryness. Different donors should be tested as hybridization may be variable. Commercially available slides may be used, but should always be tested first.\n\nStandard phenol extraction is used to obtain DNA from test or reference (karyotypically normal individual) tissue, which involves the combination of Tris-Ethylenediaminetetraacetic acid and phenol with aqueous DNA in equal amounts. This is followed by separation by agitation and centrifugation, after which the aqueous layer is removed and further treated using ether and finally ethanol precipitation is used to concentrate the DNA.\n\nMay be completed using DNA isolation kits available commercially which are based on affinity columns.\n\nPreferentially, DNA should be extracted from fresh or frozen tissue as this will be of the highest quality, though it is now possible to use archival material which is formalin fixed or paraffin wax embedded, provided the appropriate procedures are followed. 0.5-1 µg of DNA is sufficient for the CGH experiment, though if the desired amount is not obtained DOP-PCR may be applied to amplify the DNA, however it in this case it is important to apply DOP-PCR to both the test and reference DNA samples to improve reliability.\n\nNick translation is used to label the DNA and involves cutting DNA and substituting nucleotides labelled with fluorophores (direct labelling) or biotin or oxigenin to have fluophore conjugated antibodies added later (indirect labelling). It is then important to check fragment lengths of both test and reference DNA by gel electrophoresis, as they should be within the range of 500kb-1500kb for optimum hybridization.\n\nUnlabelled Life Technologies Corporation's Cot-1 DNA® (placental DNA enriched with repetitive sequences of length 50bp-100bp)is added to block normal repetitive DNA sequences, particularly at centromeres and telomeres, as these sequences, if detected, may reduce the fluorescence ratio and cause gains or losses to escape detection.\n\n8–12µl of each of labelled test and labelled reference DNA are mixed and 40 µg Cot-1 DNA® is added, then precipitated and subsequently dissolved in 6µl of hybridization mix, which contains 50% formamide to decrease DNA melting temperature and 10% dextran sulphate to increase the effective probe concentration in a saline sodium citrate (SSC) solution at a pH of 7.0.\n\nDenaturation of the slide and probes are carried out separately. The slide is submerged in 70% formamide/2xSSC for 5–10 minutes at 72 °C, while the probes are denatured by immersion in a water bath of 80 °C for 10 minutes and are immediately added to the metaphase slide preparation. This reaction is then covered with a coverslip and left for two to four days in a humid chamber at 40 °C.\n\nThe coverslip is then removed and 5 minute washes are applied, three using 2xSSC at room temperature, one at 45 °C with 0.1xSSC and one using TNT at room temperature. The reaction is then preincubated for 10 minutes then followed by a 60-minute, 37 °C incubation, three more 5 minute washes with TNT then one with 2xSSC at room temperature. The slide is then dried using an ethanol series of 70%/96%/100% before counterstaining with DAPI (0.35 μg/ml), for chromosome identification, and sealing with a coverslip.\n\nA fluorescence microscope with the appropriate filters for the DAPI stain as well as the two fluorophores utilised is required for visualisation, and these filters should also minimise the crosstalk between the fluorophores, such as narrow band pass filters. The microscope must provide uniform illumination without chromatic variation, be appropriately aligned and have a “plan” type of objective which is apochromatic and give a magnification of x63 or x100.\n\nThe image should be recorded using a camera with spatial resolution at least 0.1 µm at the specimen level and give an image of at least 600x600 pixels. The camera must also be able to integrate the image for at least 5 to 10 seconds, with a minimum photometric resolution of 8 bit.\n\nDedicated CGH software is commercially available for the image processing step, and is required to subtract background noise, remove and segment materials not of chromosomal origin, normalize the fluorescence ratio, carry out interactive karyotyping and chromosome scaling to standard length. A “relative copy number karyotype” which presents chromosomal areas of deletions or amplifications is generated by averaging the ratios of a number of high quality metaphases and plotting them along an ideogram, a diagram identifying chromosomes based on banding patterns. Interpretation of the ratio profiles is conducted either using fixed or statistical thresholds (confidence intervals). When using confidence intervals, gains or losses are identified when 95% of the fluorescence ratio does not contain 1.0.\n\nExtreme care must be taken to avoid contamination of any step involving DNA, especially with the test DNA as contamination of the sample with normal DNA will skew results closer to 1.0, thus abnormalities may go undetected. FISH, PCR and flow cytometry experiments may be employed to confirm results.\n\nArray comparative genomic hybridization (also microarray-based comparative genomic hybridization, matrix CGH, array CGH, aCGH) is a molecular cytogenetic technique for the detection of chromosomal copy number changes on a genome wide and high-resolution scale. Array CGH compares the patient's genome against a reference genome and identifies differences between the two genomes, and hence locates regions of genomic imbalances in the patient, utilizing the same principles of competitive fluorescence in situ hybridization as traditional CGH.\n\nWith the introduction of array CGH, the main limitation of conventional CGH, a low resolution, is overcome. In array CGH, the metaphase chromosomes are replaced by cloned DNA fragments (+100–200 kb) of which the exact chromosomal location is known. This allows the detection of aberrations in more detail and, moreover, makes it possible to map the changes directly onto the genomic sequence.\n\nArray CGH has proven to be a specific, sensitive, fast and highthroughput technique, with considerable advantages compared to other methods used for the analysis of DNA copy number changes making it more amenable to diagnostic applications. Using this method, copy number changes at a level of 5–10 kilobases of DNA sequences can be detected. , even high-resolution CGH (HR-CGH) arrays are accurate to detect structural variations (SV) at resolution of 200 bp. This method allows one to identify new recurrent chromosome changes such as microdeletions and duplications in human conditions such as cancer and birth defects due to chromosome aberrations.\n\nArray CGH is based on the same principle as conventional CGH. In both techniques, DNA from a reference (or control) sample and DNA from a test (or patient) sample are differentially labelled with two different fluorophores and used as probes that are cohybridized competitively onto nucleic acid targets. In conventional CGH, the target is a reference metaphase spread. In array CGH, these targets can be genomic fragments cloned in a variety of vectors (such as BACs or plasmids), cDNAs, or oligonucleotides.\n\nFigure 2. is a schematic overview of the array CGH technique. DNA from the sample to be tested is labeled with a red fluorophore (Cyanine 5) and a reference DNA sample is labeled with green fluorophore (Cyanine 3). Equal quantities of the two DNA samples are mixed and cohybridized to a DNA microarray of several thousand evenly spaced cloned DNA fragments or oligonucleotides, which have been spotted in triplicate on the array. After hybridization, digital imaging systems are used to capture and quantify the relative fluorescence intensities of each of the hybridized fluorophores. The resulting ratio of the fluorescence intensities is proportional to the ratio of the copy numbers of DNA sequences in the test and reference genomes. If the intensities of the flurochromes are equal on one probe, this region of the patient's genome is interpreted as having equal quantity of DNA in the test and reference samples; if there is an altered Cy3:Cy5 ratio this indicates a loss or a gain of the patient DNA at that specific genomic region.\n\nArray CGH has been implemented using a wide variety of techniques. Therefore, some of the advantages and limitations of array CGH are dependent on the technique chosen.\nThe initial approaches used arrays produced from large insert genomic DNA clones, such as BACs. The use of BACs provides sufficient intense signals to detect single-copy changes and to locate aberration boundaries accurately. However, initial DNA yields of isolated BAC clones are low and DNA amplification techniques are necessary. These techniques include ligation-mediated polymerase chain reaction (PCR), degenerate primer PCR using one or several sets of primers, and rolling circle amplification. Arrays can also be constructed using cDNA. These arrays currently yield a high spatial resolution, but the number of cDNAs is limited by the genes that are encoded on the chromosomes, and their sensitivity is low due to cross-hybridization. This results in the inability to detect single copy changes on a genome wide scale. The latest approach is spotting the arrays with short oligonucleotides. The amount of oligos is almost infinite, and the processing is rapid, cost-effective, and easy. Although oligonucleotides do not have the sensitivity to detect single copy changes, averaging of ratios from oligos that map next to each other on the chromosome can compensate for the reduced sensitivity. It is also possible to use arrays which have overlapping probes so that specific breakpoints may be uncovered.\n\nThere are two approaches to the design of microarrays for CGH applications: whole genome and targeted.\n\nWhole genome arrays are designed to cover the entire human genome. They often include clones that provide an extensive coverage across the genome; and arrays that have contiguous coverage, within the limits of the genome. Whole-genome arrays have been constructed mostly for research applications and have proven their outstanding worth in gene discovery. They are also very valuable in screening the genome for DNA gains and losses at an unprecedented resolution.\n\nTargeted arrays are designed for a specific region(s) of the genome for the purpose of evaluating that targeted segment. It may be designed to study a specific chromosome or chromosomal segment or to identify and evaluate specific DNA dosage abnormalities in individuals with suspected microdeletion syndromes or subtelomeric rearrangements. The crucial goal of a targeted microarray in medical practice is to provide clinically useful results for diagnosis, genetic counseling, prognosis, and clinical management of unbalanced cytogenetic abnormalities.\n\nConventional CGH has been used mainly for the identification of chromosomal regions that are recurrently lost or gained in tumors, as well as for the diagnosis and prognosis of cancer. This approach can also be used to study chromosomal aberrations in fetal and neonatal genomes. Furthermore, conventional CGH can be used in detecting chromosomal abnormalities and have been shown to be efficient in diagnosing complex abnormalities associated with human genetic disorders.\n\nCGH data from several studies of the same tumor type show consistent patterns of non-random genetic aberrations. Some of these changes appear to be common to various kinds of malignant tumors, while others are more tumor specific. For example, gains of chromosomal regions lq, 3q and 8q, as well as losses of 8p, 13q, 16q and 17p, are common to a number of tumor types, such as breast, ovarian, prostate, renal and bladder cancer (Figure. 3). Other alterations, such as 12p and Xp gains in testicular cancer, 13q gain 9q loss in bladder cancer, 14q loss in renal cancer and Xp loss in ovarian cancer are more specific, and might reflect the unique selection forces operating during cancer development in different organs. Array CGH is also frequently used in research and diagnostics of B cell malignancies, such as chronic lymphocytic leukemia.\n\nCri du Chat (CdC) is a syndrome caused by a partial deletion of the short arm of chromosome 5. Several studies have shown that conventional CGH is suitable to detect the deletion, as well as more complex chromosomal alterations. For example, Levy et al. (2002) reported an infant with a cat-like cry, the hallmark of CdC, but having an indistinct karyotype. CGH analysis revealed a loss of chromosomal material from 5p15.3 confirming the diagnosis clinically. These results demonstrate that conventional CGH is a reliable technique in detecting structural aberrations and, in specific cases, may be more efficient in diagnosing complex abnormalities.\n\nArray CGH applications are mainly directed at detecting genomic abnormalities in cancer. However, array CGH is also suitable for the analysis of DNA copy number aberrations that cause human genetic disorders. That is, array CGH is employed to uncover deletions, amplifications, breakpoints and ploidy abnormalities. Earlier diagnosis is of benefit to the patient as they may undergo appropriate treatments and counseling to improve their prognosis.\n\nGenetic alterations and rearrangements occur frequently in cancer and contribute to its pathogenesis. Detecting these aberrations by array CGH provides information on the locations of important cancer genes and can have clinical use in diagnosis, cancer classification and prognostification. However, not all of the losses of genetic material are pathogenetic, since some DNA material is physiologically lost during the rearrangement of immunoglobulin subgenes. In a recent study, array CGH has been implemented to identify regions of chromosomal aberration (copy-number variation) in several mouse models of breast cancer, leading to identification of cooperating genes during myc-induced oncogenesis.\n\nArray CGH may also be applied not only to the discovery of chromosomal abnormalities in cancer, but also to the monitoring of the progression of tumors. Differentiation between metastatic and mild lesions is also possible using FISH once the abnormalities have been identified by array CGH.\n\nPrader–Willi syndrome (PWS) is a paternal structural abnormality involving 15q11-13, while a maternal aberration in the same region causes Angelman syndrome (AS). In both syndromes, the majority of cases (75%) are the result of a 3–5 Mb deletion of the PWS/AS critical region. These small aberrations cannot be detected using cytogenetics or conventional CGH, but can be readily detected using array CGH. As a proof of principle Vissers et al. (2003) constructed a genome wide array with a 1 Mb resolution to screen three patients with known, FISH-confirmed microdeletion syndromes, including one with PWS. In all three cases, the abnormalities, ranging from 1.5 to 2.9Mb, were readily identified. Thus, array CGH was demonstrated to be a specific and sensitive approach in detecting submicroscopic aberrations.\n\nWhen using overlapping microarrays, it is also possible to uncover breakpoints involved in chromosomal aberrations.\n\nThough not yet a widely employed technique, the use of array CGH as a tool for preimplantation genetic screening is becoming an increasingly popular concept. It has the potential to detect CNVs and aneuploidy in eggs, sperm or embryos which may contribute to failure of the embryo to successfully implant, miscarriage or conditions such as Down syndrome (trisomy 21). This makes array CGH a promising tool to reduce the incidence of life altering conditions and improve success rates of IVF attempts. The technique involves whole genome amplification from a single cell which is then used in the array CGH method. It may also be used in couples carrying chromosomal translocations such as balanced reciprocal translocations or Robertsonian translocations, which have the potential to cause chromosomal imbalances in their offspring.\n\nA main disadvantage of conventional CGH is its inability to detect structural chromosomal aberrations without copy number changes, such as mosaicism, balanced chromosomal translocations, and inversions. CGH can also only detect gains and losses relative to the ploidy level. In addition, chromosomal regions with short repetitive DNA sequences are highly variable between individuals and can interfere with CGH analysis. Therefore, repetitive DNA regions like centromeres and telomeres need to be blocked with unlabeled repetitive DNA (e.g. Cot1 DNA) and/or can be omitted from screening. Furthermore, the resolution of conventional CGH is a major practical problem that limits its clinical applications. Although CGH has proven to be a useful and reliable technique in the research and diagnostics of both cancer and human genetic disorders, the applications involve only gross abnormalities. Because of the limited resolution of metaphase chromosomes, aberrations smaller than 5–10 Mb cannot be detected using conventional CGH.\nFor the detection of such abnormalities, a high-resolution technique is required.\nArray CGH overcomes many of these limitations. Array CGH is characterized by a high resolution, its major advantage with respect to conventional CGH. The standard resolution varies between 1 and 5 Mb, but can be increased up to approximately 40 kb by supplementing the array with extra clones. However, as in conventional CGH, the main disadvantage of array CGH is its inability to detect aberrations that do not result in copy number changes and is limited in its ability to detect mosaicism. The level of mosaicism that can be detected is dependent on the sensitivity and spatial resolution of the clones. At present, rearrangements present in approximately 50% of the cells is the detection limit. For the detection of such abnormalities, other techniques, such as SKY (Spectral karyotyping) or FISH have to still be used.\n\n\n"}
{"id": "917868", "url": "https://en.wikipedia.org/wiki?curid=917868", "title": "Comparative genomics", "text": "Comparative genomics\n\nComparative genomics is a field of biological research in which the genomic features of different organisms are compared. The genomic features may include the DNA sequence, genes, gene order, regulatory sequences, and other genomic structural landmarks. In this branch of genomics, whole or large parts of genomes resulting from genome projects are compared to study basic biological similarities and differences as well as evolutionary relationships between organisms. The major principle of comparative genomics is that common features of two organisms will often be encoded within the DNA that is evolutionarily conserved between them. Therefore, comparative genomic approaches start with making some form of alignment of genome sequences and looking for orthologous sequences (sequences that share a common ancestry) in the aligned genomes and checking to what extent those sequences are conserved. Based on these, genome and molecular evolution are inferred and this may in turn be put in the context of, for example, phenotypic evolution or population genetics.\n\nVirtually started as soon as the whole genomes of two organisms became available (that is, the genomes of the bacteria \"Haemophilus influenzae\" and \"Mycoplasma genitalium\") in 1995, comparative genomics is now a standard component of the analysis of every new genome sequence. With the explosion in the number of genome projects due to the advancements in DNA sequencing technologies, particularly the next-generation sequencing methods in late 2000s, this field has become more sophisticated, making it possible to deal with many genomes in a single study. Comparative genomics has revealed high levels of similarity between closely related organisms, such as humans and chimpanzees, and, more surprisingly, similarity between seemingly distantly related organisms, such as humans and the yeast \"Saccharomyces cerevisiae\". It has also showed the extreme diversity of the gene\ncomposition in different evolutionary lineages.\n\n\"See also\": History of genomics\n\nComparative genomics has a root in the comparison of virus genomes in the early 1980s. For example, small RNA viruses infecting animals (picornaviruses) and those infecting plants (cowpea mosaic virus) were compared and turned out to share significant sequence similarity and, in part, the order of their genes. In 1986, the first comparative genomic study at a larger scale was published, comparing the genomes of varicella-zoster virus and Epstein-Barr virus that contained more than 100 genes each.\n\nThe first complete genome sequence of a cellular organism, that of \"Haemophilus influenzae\" Rd, was published in 1995. The second genome sequencing paper was of the small parasitic bacterium \"Mycoplasma genitalium\" published in the same year. Starting from this paper, reports on new genomes inevitably became comparative-genomic studies.\n\nThe first high-resolution whole genome comparison system was developed in 1998 by Art Delcher, Simon Kasif and Steven Salzberg and applied to the comparison of entire highly related microbial organisms with their collaborators at the Institute for Genomic Research (TIGR). The system is called MUMMER and was described in a publication in Nucleic Acids Research in 1999. The system helps researchers to identify large rearrangements, single base mutations, reversals, tandem repeat expansions and other polymorphisms. In bacteria, MUMMER enables the identification of polymorphisms that are responsible for virulence, pathogenicity, and anti-biotic resistance. The system was also applied to the Minimal Organism Project at TIGR and subsequently to many other comparative genomics projects.\n\n\"Saccharomyces cerevisiae\", the baker's yeast, was the first eukaryote to have its complete genome sequence published in 1996. After the publication of the roundworm \"Caenorhabditis elegans\" genome in 1998 and together with the fruit fly \"Drosophila melanogaster\" genome in 2000, Gerald M. Rubin and his team published a paper titled \"Comparative Genomics of the Eukaryotes\", in which they compared the genomes of the eukaryotes \"D. melanogaster\", \"C. elegans\", and \"S. cerevisiae\", as well as the prokaryote \"H. influenzae\". At the same time, Bonnie Berger, Eric Lander, and their team published a paper on whole-genome comparison of human and mouse.\n\nWith the publication of the large genomes of vertebrates in the 2000s, including human, the Japanese pufferfish \"Takifugu rubripes\", and mouse, precomputed results of large genome comparisons have been released for downloading or for visualization in a genome browser. Instead of undertaking their own analyses, most biologists can access these large cross-species comparisons and avoid the impracticality caused by the size of the genomes.\n\nNext-generation sequencing methods, which were first introduced in 2007, have produced an enormous amount of genomic data and have allowed researchers to generate multiple (prokaryotic) draft genome sequences at once. These methods can also quickly uncover single-nucleotide polymorphisms, insertions and deletions by mapping unassembled reads against a well annotated reference genome, and thus provide a list of possible gene differences that may be the basis for any functional variation among strains.\n\nOne character of biology is evolution, evolutionary theory is also the theoretical foundation of comparative genomics, and at the same time the results of comparative genomics unprecedentedly enriched and developed the theory of evolution. When two or more of the genome sequence are compared, one can deduce the evolutionary relationships of the sequences in a phylogenetic tree. Based on a variety of biological genome data and the study of vertical and horizontal evolution processes, one can understand vital parts of the gene structure and its regulatory function.\n\nSimilarity of related genomes is the basis of comparative genomics. If two creatures have a recent common ancestor, the differences between the two species genomes are evolved from the ancestors’ genome. The closer the relationship between two organisms, the higher the similarities between their genomes. If there is close relationship between them, then their genome will display a linear behaviour (synteny), namely some or all of the genetic sequences are conserved. Thus, the genome sequences can be used to identify gene function, by analyzing their homology (sequence similarity) to genes of known function.\n\nOrthologous sequences are related sequences in different species: a gene exists in the original species, the species divided into two species, so genes in new species are orthologous to the sequence in the original species. Paralogous sequences are separated by gene cloning (gene duplication): if a particular gene in the genome is copied, then the copy of the two sequences is paralogous to the original gene. A pair of orthologous sequences is called orthologous pairs (orthologs), a pair of paralogous sequence is called collateral pairs (paralogs). Orthologous pairs usually have the same or similar function, which is not necessarily the case for collateral pairs. In collateral pairs, the sequences tend to evolve into having different functions.\nComparative genomics exploits both similarities and differences in the proteins, RNA, and regulatory regions of different organisms to infer how selection has acted upon these elements. Those elements that are responsible for similarities between different species should be conserved through time (stabilizing selection), while those elements responsible for differences among species should be divergent (positive selection). Finally, those elements that are unimportant to the evolutionary success of the organism will be unconserved (selection is neutral).\n\nOne of the important goals of the field is the identification of the mechanisms of eukaryotic genome evolution. It is however often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. For this reason comparative genomics studies of small model organisms (for example the model Caenorhabditis elegans and closely related Caenorhabditis briggsae) are of great importance to advance our understanding of general mechanisms of evolution.\n\nComputational approaches to genome comparison have recently become a common research topic in computer science. A public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while multiple courses will begin training students to be fluent in both topics.\n\nComputational tools for analyzing sequences and complete genomes are developing quickly due to the availability of large amount of genomic data. At the same time, comparative analysis tools are progressed and improved. In the challenges about these analyses, it is very important to visualize the comparative results.\n\nVisualization of sequence conservation is a tough task of comparative sequence analysis. As we know, it is highly inefficient to examine the alignment of long genomic regions manually. Internet-based genome browsers provide many useful tools for investigating genomic sequences due to integrating all sequence-based biological information on genomic regions. When we extract large amount of relevant biological data, they can be very easy to use and less time-consuming.\n\n\nAn advantage of using online tools is that these websites are being developed and updated constantly. There are many new settings and content can be used online to improve efficiency.\n\nAgriculture is a field that reaps the benefits of comparative genomics. Identifying the loci of advantageous genes is a key step in breeding crops that are optimized for greater yield, cost-efficiency, quality, and disease resistance. For example, one genome wide association study conducted on 517 rice landraces revealed 80 loci associated with several categories of agronomic performance, such as grain weight, amylose content, and drought tolerance. Many of the loci were previously uncharacterized. Not only is this methodology powerful, it is also quick. Previous methods of identifying loci associated with agronomic performance required several generations of carefully monitored breeding of parent strains, a time consuming effort that is unnecessary for comparative genomic studies.\n\nThe medical field also benefits from the study of comparative genomics. Vaccinology in particular has experienced useful advances in technology due to genomic approaches to problems. In an approach known as reverse vaccinology, researchers can discover candidate antigens for vaccine development by analyzing the genome of a pathogen or a family of pathogens. Applying a comparative genomics approach by analyzing the genomes of several related pathogens can lead to the development of vaccines that are multiprotective. A team of researchers employed such an approach to create a universal vaccine for Group B Streptococcus, a group of bacteria responsible for severe neonatal infection. Comparative genomics can also be used to generate specificity for vaccines against pathogens that are closely related to commensal microorganisms. For example, researchers used comparative genomic analysis of commensal and pathogenic strains of E. coli to identify pathogen specific genes as a basis for finding antigens that result in immune response against pathogenic strains but not commensal ones.\n\nComparative genomics also opens up new avenues in other areas of research. As DNA sequencing technology has become more accessible, the number of sequenced genomes has grown. With the increasing reservoir of available genomic data, the potency of comparative genomic inference has grown as well. A notable case of this increased potency is found in recent primate research. Comparative genomic methods have allowed researchers to gather information about genetic variation, differential gene expression, and evolutionary dynamics in primates that were indiscernible using previous data and methods. The Great Ape Genome Project used comparative genomic methods to investigate genetic variation with reference to the six great ape species, finding healthy levels of variation in their gene pool despite shrinking population size. Another study showed that patterns of DNA methylation, which are a known regulation mechanism for gene expression, differ in the prefrontal cortex of humans versus chimps, and implicated this difference in the evolutionary divergence of the two species.\n\n\n\n"}
{"id": "9435784", "url": "https://en.wikipedia.org/wiki?curid=9435784", "title": "Comparative physiology", "text": "Comparative physiology\n\nComparative physiology is a subdiscipline of physiology that studies and exploits the diversity of functional characteristics of various kinds of organisms. It is closely related to evolutionary physiology and environmental physiology. Many universities offer undergraduate courses that cover comparative aspects of animal physiology. According to Clifford Ladd Prosser, \"Comparative Physiology\nis not so much a defined discipline as a viewpoint, a philosophy.\"\n\nOriginally, physiology focused primarily on human beings, in large part from a desire to improve medical practices. When physiologists first began comparing different species it was sometimes out of simple curiosity to understand how organisms work but also stemmed from a desire to discover basic physiological principles. This use of specific organisms convenient to study specific questions is known as the Krogh Principle.\n\nC. Ladd Prosser, a founder of modern comparative physiology, outlined a broad agenda for comparative physiology in his 1950 edited volume (see summary and discussion in Garland and Carter):\n\n1. To describe how different kinds of animals meet their needs.\n\n2. The use of physiological information to reconstruct phylogenetic relationships of organisms.\n\n3. To elucidate how physiology mediates interactions between organisms and their environments.\n\n4. To identify \"model systems\" for studying particular physiological functions.\n\n5. To use the \"kind of animal\" as an experimental variable.\n\nComparative physiologists often study organisms that live in \"extreme\" environments (e.g., deserts) because they expect to find especially clear examples of evolutionary adaptation. One example is the study of water balance in desert-inhabiting mammals, which have been found to exhibit kidney specializations.\n\nSimilarly, comparative physiologists have been attracted to \"unusual\" organisms, such as very large or small ones. As an example, of the latter, hummingbirds have been studied. As another example, giraffe have been studied because of their long necks and the expectation that this would lead to specializations related to the regulation of blood pressure. More generally, ectothermic vertebrates have been studied to determine how blood acid-base balance and pH change as body temperature changes.\n\nIn the United States, research in comparative physiology is funded by both the National Institutes of Health and the National Science Foundation.\n\nA number of scientific societies feature sections on comparative physiology, including:\n\nKnut Schmidt-Nielsen (1915–2007) was a major figure in vertebrate comparative physiology, serving on the faculty at Duke University for many years and training a large number of students (obituary). He also authored several books, including an influential text, all known for their accessible writing style.\n\nGrover C. Stephens (1925–2003) was a well-known invertebrate comparative physiologist, serving on the faculty of the University of Minnesota until becoming the founding chairman of the Department of Organismic Biology at the University of California at Irvine in 1964. He was the mentor for numerous graduate students, many of whom have gone on to further build the field (obituary). He authored several books and in addition to being an accomplished biologist was also an accomplished pianist and philosopher.\n\n\n\n"}
{"id": "380406", "url": "https://en.wikipedia.org/wiki?curid=380406", "title": "Comparative psychology", "text": "Comparative psychology\n\nComparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area addresses many different issues, uses many different methods and explores the behavior of many different species from insects to primates.\n\nComparative psychology is sometimes assumed to emphasize cross-species comparisons, including those between humans and animals. However, some researchers feel that direct comparisons should not be the sole focus of comparative psychology and that intense focus on a single organism to understand its behavior is just as desirable; if not more so. Donald Dewsbury reviewed the works of several psychologists and their definitions and concluded that the object of comparative psychology is to establish principles of generality focusing on both proximate and ultimate causation. \n\nUsing a comparative approach to behavior allows one to evaluate the target behavior from four different, complementary perspectives, developed by Niko Tinbergen. First, one may ask how pervasive the behavior is across species (i.e. how common is the behavior between animal species?). Second, one may ask how the behavior contributes to the lifetime reproductive success of the individuals demonstrating the behavior (i.e. does the behavior result in animals producing more offspring than animals not displaying the behavior)? Theories addressing the ultimate causes of behavior are based on the answers to these two questions.\n\nThird, what mechanisms are involved in the behavior (i.e. what physiological, behavioral, and environmental components are necessary and sufficient for the generation of the behavior)? Fourth, a researcher may ask about the development of the behavior within an individual (i.e. what maturational, learning, social experiences must an individual undergo in order to demonstrate a behavior)? Theories addressing the proximate causes of behavior are based on answers to these two questions. For more details see Tinbergen's four questions.\n\nThe 9th century scholar al-Jahiz wrote works on the social organization and communication methods of animals like ants. The 11th century Arabic writer Ibn al-Haytham (Alhazen) wrote the \"Treatise on the Influence of Melodies on the Souls of Animals\", an early treatise dealing with the effects of music on an imals. In the treatise, he demonstrates how a camel's pace could be hastened or retarded with the use of music, and shows other examples of how music can affect animal behavior, experimenting with horses, birds and reptiles. Through to the 19th century, a majority of scholars in the Western world continued to believe that music was a distinctly human phenomenon, but experiments since then have vindicated Ibn al-Haytham's view that music does indeed have an effect on animals.\n\nCharles Darwin was central in the development of comparative psychology; it is thought that psychology should be spoken in terms of \"pre-\" and \"post-Darwin\" because his contributions were so influential. theory led to several hypotheses, one being that the factors that set humans apart, such as higher mental, moral and spiritual faculties, could be accounted for by evolutionary principles. In response to the vehement opposition to Darwinism was the \"anecdotal movement\" led by George Romanes who set out to demonstrate that animals possessed a \"rudimentary human mind\". Romanes is most famous for two major flaws in his work: his focus on anecdotal observations and entrenched anthropomorphism.\n\nNear the end of the 19th century, several scientists existed whose work was also very influential. Douglas Alexander Spalding was called the \"first experimental biologist\", and worked mostly with birds; studying instinct, imprinting, and visual and auditory development. Jacques Loeb emphasized the importance of objectively studying behavior, Sir John Lubbock is credited with first using mazes and puzzle devices to study learning and Conwy Lloyd Morgan is thought to be \"the first ethologist in the sense in which we presently use the word\".\n\nThroughout the long history of comparative psychology, repeated attempts have been made to enforce a more disciplined approach, in which similar studies are carried out on animals of different species, and the results interpreted in terms of their different phylogenetic or ecological backgrounds. Behavioral ecology in the 1970s gave a more solid base of knowledge against which a true comparative psychology could develop. However, the broader use of the term \"comparative psychology\" is enshrined in the names of learned societies and academic journals, not to mention in the minds of psychologists of other specialisms, so the label of the field is never likely to disappear completely.\n\nA persistent question with which comparative psychologists have been faced is the relative intelligence of different species of animal. Indeed, some early attempts at a genuinely comparative psychology involved evaluating how well animals of different species could learn different tasks. These attempts floundered; in retrospect it can be seen that they were not sufficiently sophisticated, either in their analysis of the demands of different tasks, or in their choice of species to compare. However, the definition of \"intelligence\" in comparative psychology is deeply affected by anthropomorphism, and focuses on simple tasks, complex problems, reversal learning, learning sets, and delayed alternation are plagued with practical and theoretical problems. In the literature, \"intelligence\" is defined as whatever is closest to human performance and neglects behaviors that humans are usually incapable of (e.g. echolocation). Specifically, comparative researchers encounter problems associated with individual differences, differences in motivation, differences in reinforcement, differences in sensory function, differences in motor capacities, and species-typical preparedness (i.e. some species have evolved to acquire some behaviors quicker than other behaviors).\n\nA wide variety of species have been studied by comparative psychologists. However, a small number have dominated the scene. Ivan Pavlov's early work used dogs; although they have been the subject of occasional studies, since then they have not figured prominently. Increasing interest in the study of abnormal animal behavior has led to a return to the study of most kinds of domestic animal. Thorndike began his studies with cats, but American comparative psychologists quickly shifted to the more economical rat, which remained the almost invariable subject for the first half of the 20th century and continues to be used.\n\nSkinner introduced the use of pigeons, and they continue to be important in some fields. There has always been interest in studying various species of primate; important contributions to social and developmental psychology were made by Harry F. Harlow's studies of maternal deprivation in rhesus monkeys. Cross-fostering studies have shown similarities between human infants and infant chimpanzees. Kellogg and Kellogg (1933) aimed to look at heredity and environmental effects of young primates. They found that a cross-fostered chimpanzee named Gua was better at recognizing human smells and clothing and that the Kelloggs' infant (Donald) recognised humans better by their faces. The study ended 9 months after it had begun, after the infant began to imitate the noises of Gua.\n\nNonhuman primates have also been used to show the development of language in comparison with human development. For example, Gardner (1967) successfully taught the female chimpanzee Washoe 350 words in American Sign Language. Washoe subsequently passed on some of this teaching to her adopted offspring, Loulis. A criticism of Washoe's acquisition of sign language focused on the extent to which she actually understood what she was signing. Her signs may have just based on an association to get a reward, such as food or a toy. Other studies concluded that apes do not understand linguistic input, but may form an intended meaning of what is being communicated. All great apes have been reported to have the capacity of allospecific symbolic production.\n\nInterest in primate studies has increased with the rise in studies of animal cognition. Other animals thought to be intelligent have also been increasingly studied. Examples include various species of corvid, parrots — especially the grey parrot — and dolphins. Alex (Avian Learning EXperiment) is a well known case study (1976–2007) which was developed by Pepperberg, who found that the African gray parrot Alex did not only mimic vocalisations but understood the concepts of same and different between objects. The study of non-human mammals has also included the study of dogs. Due to their domestic nature and personalities, dogs have lived closely with humans, and parallels in communication and cognitive behaviours have therefore been recognised and further researched. Joly-Mascheroni and colleagues (2008) demonstrated that dogs may be able to catch human yawns and suggested a level of empathy in dogs, a point that is strongly debated. Pilley and Reid found that a Border Collie named Chaser was able to successfully identify and retrieve 1022 distinct objects/toys.\n\nResearchers who study animal cognition are interested in understanding the mental processes that control complex behavior, and much of their work parallels that of cognitive psychologists working with humans. For example, there is extensive research with animals on attention, categorization, concept formation, memory, spatial cognition, and time estimation. Much research in these and other areas is related directly or indirectly to behaviors important to survival in natural settings, such as navigation, tool use, and numerical competence. Thus, comparative psychology and animal cognition are heavily overlapping research categories.\n\nVeterinary surgeons recognize that the psychological state of a captive or domesticated animal must be taken into account if its behavior and health are to be understood and optimized.\n\nCommon causes of disordered behavior in captive or pet animals are lack of stimulation, inappropriate stimulation, or overstimulation. These conditions can lead to disorders, unpredictable and unwanted behavior, and sometimes even physical symptoms and diseases. For example, rats who are exposed to loud music for a long period will ultimately develop unwanted behaviors that have been compared with human psychosis, like biting their owners.\n\nThe way dogs behave when understimulated is widely believed to depend on the breed as well as on the individual animal's character. For example, huskies have been known to ruin gardens and houses if they are not allowed enough activity. Dogs are also prone to psychological damage if they are subjected to violence. If they are treated very badly, they may become dangerous.\n\nThe systematic study of disordered animal behavior draws on research in comparative psychology, including the early work on conditioning and instrumental learning, but also on ethological studies of natural behavior. However, at least in the case of familiar domestic animals, it also draws on the accumulated experience of those who have worked closely with the animals.\n\nThe relationship between humans and animals has long been of interest to anthropologists as one pathway to an understanding the evolution of human behavior. Similarities between the behavior of humans and animals have sometimes been used in an attempt to understand the evolutionary significance of particular behaviors. Differences in the treatment of animals have been said to reflect a society's understanding of human nature and the place of humans and animals in the scheme of things. Domestication has been of particular interest. For example, it has been argued that, as animals became domesticated, humans treated them as property and began to see them as inferior or fundamentally different from humans.\nIngold remarks that in all societies children have to learn to differentiate and separate themselves from others. In this process, strangers may be seen as \"not people,\" and like animals. Ingold quoted Sigmund Freud: \"Children show no trace of arrogance which urges adult civilized men to draw a hard-and-fast line between their own nature and that of all other animals. Children have no scruples over allowing animals to rank as their full equals.\" With maturity however, humans find it hard to accept that they themselves are animals, so they categorize, separating humans from animals, and animals into wild animals and tame animals, and tame animals into house pets and livestock. Such divisions can be seen as similar to categories of humans: who is part of a human community and someone who isn't, that is, the outsider.\n\n\"The New York Times\" ran an article that showed the psychological benefits of animals, more specifically of children with their pets. It's been proven that having a pet does in fact improve kids' social skills. In the article, Dr. Sue Doescher, a psychologist involved in the study, stated, \"It made the children more cooperative and sharing.\" It was also shown that these kids were more confident with themselves and able to be more empathic with other children.\n\nFurthermore, in an edition of \"Social Science and Medicine\" it was stated, \"A random survey of 339 residents from Perth, Western Australia were selected from three suburbs and interviewed by telephone. Pet ownership was found to be positively associated with some forms of social contact and interaction, and with perceptions of neighborhood friendliness. After adjustment for demographic variables, pet owners scored higher on social capital and civic engagement scales.\" Results like these let us know that owning a pet provides opportunities for neighborly interaction, among many other chances for socialization among people.\n\nNoted comparative psychologists, in this broad sense, include:\n\nMany of these were active in fields other than animal psychology; this is characteristic of comparative psychologists.\n\nFields of psychology and other disciplines that draw upon, or overlap with, comparative psychology include:\n\n\n"}
{"id": "1719952", "url": "https://en.wikipedia.org/wiki?curid=1719952", "title": "Comparative research", "text": "Comparative research\n\nComparative research is a research methodology in the social sciences that aims to make comparisons across different countries or cultures. A major problem in comparative research is that the data sets in different countries may not use the same categories, or define categories differently (for example by using different definitions of poverty).\n\nAs Moutsios argues, cross-cultural and comparative research should be seen as part of the scientific spirit that arose in Greece in the 6th century and the overall appreciation of knowledge and learning that was characteristic of the 5th century. In other words, it is part of the emergence of \"episteme\" and \"philo-sophia\", as a love for knowledge that is independent from material benefits. \"Episteme\", as a form and activity in the field of \"logos\", marked the break of cognitive closure and advanced empirical inquiry, logical argumentation and the search for truth. And the high esteem for intellectual activity gave rise to a genuine curiosity about other cultures – which has lain thereafter at the heart of comparative inquiry.\n\nMoreover, behind the Greek comparative gaze also was the philosophical and political questioning which characterised the life of the democratic \"polis\". Philosophical inquiry, from the Milesians down to the Sophists, questioned the representations and the cognitive traditions of their own people; the inquiry of the traditions of other peoples was, as Herodotus’ \"Histories\" demonstrate, an activity associated with the ethos of philosophical critique that characterised democratic life in Greece. Similarly, questioning of the Greek laws and institutions and its related values and practices (e.g. \"isegoria\" and \"parrhesia\"), as part of Greek politics, is associated with the effort of the first historians to reflect on home institutions through researching those of others.\n\nAccording also to Karl Deutsch, we have been using this form of investigation for over 2,000 years. Comparing things is essential to basic scientific and philosophic inquiry, which has been done for a long time. Most authors are more conservative in their estimate of how long comparative research has been with us. It is largely an empty debate over the definition of the tradition with those questioning whether comparing things counts as comparative research.\n\nTextbooks on this form of study were beginning to appear by the 1880s, but its rise to extreme popularity began after World War II. There are numerous reasons that comparative research has come to take a place of honour in the toolbox of the social scientist. Globalization has been a major factor, increasing the desire and possibility for educational exchanges and intellectual curiosity about other cultures. Information technology has enabled greater production of quantitative data for comparison, and international communications technology has facilitated this information to be easily spread.\n\nComparative research, simply put, is the act of comparing two or more things with a view to discovering something about one or all of the things being compared. This technique often utilizes multiple disciplines in one study. When it comes to method, the majority agreement is that there is no methodology peculiar to comparative research. The multidisciplinary approach is good for the flexibility it offers, yet comparative programs do have a case to answer against the call that their research lacks a \"seamless whole.\" \n\nThere are certainly methods that are far more common than others in comparative studies, however. Quantitative analysis is much more frequently pursued than qualitative, and this is seen by the majority of comparative studies which use quantitative data. The general method of comparing things is the same for comparative research as it is in our everyday practice of comparison. Like cases are treated alike, and different cases are treated differently; the extent of difference determines how differently cases are to be treated. If one is able to sufficiently distinguish two carry the research conclusions will not be very helpful. \n\nSecondary analysis of quantitative data is relatively widespread in comparative research, undoubtedly in part because of the cost of obtaining primary data for such large things as a country's policy environment. This study is generally aggregate data analysis. Comparing large quantities of data (especially government sourced) is prevalent. A typical method of comparing welfare states is to take balance of their levels of spending on social welfare.\n\nIn line with how a lot of theorizing has gone in the last century, comparative research does not tend to investigate \"grand theories,\" such as Marxism. It instead occupies itself with middle-range theories that do not purport to describe our social system in its entirety, but a subset of it. A good example of this is the common research program that looks for differences between two or more social systems, then looks at these differences in relation to some other variable coexisting in those societies to see if it is related. The classic case of this is Esping-Andersen's research on social welfare systems. He noticed there was a difference in types of social welfare systems, and compared them based on their level of decommodification of social welfare goods. He found that he was able to class welfare states into three types, based on their level of decommodification. He further theorized from this that decommodification was based on a combination of class coalitions and mobilization, and regime legacy. Here, Esping-Andersen is using comparative research: he takes many western countries and compares their level of decommodification, then develops a theory of the divergence based on his findings.\n\nComparative research can take many forms. Two key factors are space and time. Spatially, cross-national comparisons are by far the most common, although comparisons within countries, contrasting different areas, cultures or governments also subsist and are very constructive, especially in a country like New Zealand, where policy often changes depending on which race it pertains to. Recurrent interregional studies include comparing similar or different countries or sets of countries, comparing one's own country to others or to the whole world.\n\nThe historical comparative research involves comparing different time-frames. The two main choices within this model are comparing two stages in time (either snapshots or time-series), or just comparing the same thing over time, to see if a policy's effects differ over a stretch of time.\n\nWhen it comes to subject matter of comparative inquiries, many contend there is none unique to it. This may indeed be true, but a brief perusal of comparative endeavours reveals there are some topics more recurrent than others. Determining whether socioeconomic or political factors are more important in explaining government action is a familiar theme. In general, however, the only thing that is certain in comparative research issues is the existence of differences to be analysed.\n\n\n"}
{"id": "31994535", "url": "https://en.wikipedia.org/wiki?curid=31994535", "title": "Comparison of Nazism and Stalinism", "text": "Comparison of Nazism and Stalinism\n\nA number of authors have carried out comparisons of Nazism and Stalinism, in which they have considered the similarities and differences of the two ideologies and political systems, what relationship existed between the two regimes, and why both of them came to prominence at the same time. During the 20th century, the comparison of Stalinism and Nazism was made on the topics of totalitarianism, ideology, and personality cult. Both regimes were seen in contrast to the liberal West, with an emphasis on the similarities between the two. The American political scientists Zbigniew Brzezinski, Hannah Arendt and Carl Friedrich and historian Robert Conquest were prominent advocates of applying the \"totalitarian\" concept to compare Nazism and Stalinism.\n\nOne of the first scholars to publish a comparative study of Nazi Germany and Stalin’s Soviet Union was Hannah Arendt. In her 1951 work, \"The Origins of Totalitarianism\", Arendt puts forward the idea of totalitarianism as a distinct type of political movement and form of government, which “differs essentially from other forms of political oppression known to us such as despotism, tyranny and dictatorship.” Furthermore, Arendt distinguishes between a totalitarian movement (such as a political party with totalitarian aims) and a totalitarian government. Not all totalitarian movements succeed in creating totalitarian governments once they gain power. In Arendt’s view, although many totalitarian movements existed in Europe in the 1920s and 1930s, only the governments of Stalin and Hitler succeeded in fully implementing their totalitarian aims. \n\nArendt traced the origin of totalitarian movements to the nineteenth century, focusing especially on antisemitism and imperialism. She emphasized the connection between the rise of European nation-states and the growth of antisemitism, which was due to the fact that the Jews represented an “inter-European, non-national element in a world of growing or existing nations.” Conspiracy theories abounded, and the Jews were accused of being part of various international schemes to ruin European nations. Small antisemitic political parties formed in response to this perceived Jewish threat, and, according to Arendt, these were the first political organizations in Europe that claimed to represent the interests of the whole nation as opposed to the interests of a class or other social group. The later totalitarian movements would copy or inherit this claim to speak for the whole nation, with the implication that any opposition to them constituted treason.\n\nEuropean imperialism of the nineteenth century also paved the way for totalitarianism, by legitimizing the concept of endless expansion. After Europeans had engaged in imperialist expansion on other continents, political movements developed which aimed to copy the methods of imperialism on the European continent itself. Arendt refers specifically to the “pan-movements” of pan-Germanism and pan-Slavism, which promised continental empires to nations that had little hope of overseas expansion. According to Arendt, “Nazism and Bolshevism owe more to Pan-Germanism and Pan-Slavism (respectively) than to any other ideology or political movement.”\n\nArendt argues that both the Nazi and Bolshevik movements “recruited their members from [a] mass of apparently indifferent people whom all other parties had given up,” and who “had reason to be equally hostile to all parties.” For this reason, totalitarian movements did not need to use debate or persuasion, and did not need to refute the arguments of the other parties. Their target audience did not have to be persuaded to despise the other parties or the democratic system, because it consisted of people who already despised mainstream politics. As a result, totalitarian movements were free to use violence and terror against their opponents without fear that this might alienate their own supporters. Instead of arguing against their opponents, they adopted deterministic views of human behavior and presented opposing ideas as “originating in deep natural, social, or psychological sources beyond the control of the individual and therefore beyond the power of reason.” The Nazis in particular, during the years before their rise to power, engaged in “killing small socialist functionaries or influential members of opposing parties” both as a means to intimidate opponents and as a means of demonstrating to their supporters that they were a party of action, “different from the ‘idle talkers’ of other parties.”\n\nTotalitarian governments make extensive use of propaganda, and are often characterized by having a strong distinction between what they tell their own supporters and the propaganda they produce for others. Arendt distinguishes these two categories as \"indoctrination\" and \"propaganda\". Indoctrination consists of the message that a totalitarian government promotes internally, to the members of the ruling party and that segment of the population which supports the government. Propaganda consists of the message that a totalitarian government seeks to promote in the outside world, and also among those parts of its own society which may not support the government. Thus, “the necessities for propaganda are always dictated by the outside world,” while the opportunities for indoctrination depend on “the totalitarian governments’ isolation and security from outside interference.” \n\nThe type of indoctrination used by the Soviets and the Nazis was characterized by claims of “scientific” truth, and appeals to “objective laws of nature.” Both movements took a deterministic view of human society and claimed that their ideologies were based on scientific discoveries regarding race (in the case of the Nazis) or the forces governing human history (in the case of the Soviets). Arendt identifies this as being in certain ways similar to modern advertising, in which companies claim that scientific research shows their products to be superior, but more generally she argues that it is an extreme version of “that obsession with science which has characterized the Western world since the rise of mathematics and physics in the sixteenth century.” By their use of pseudoscience as the main justification for their actions, Nazism and Stalinism are distinguished from earlier historical despotic regimes, who appealed instead to religion or sometimes did not try to justify themselves at all. According to Arendt, totalitarian governments did not merely use these appeals to supposed scientific laws as propaganda to manipulate others. Rather, totalitarian leaders like Hitler and Stalin genuinely believed that they were acting in accordance with immutable natural laws, to such an extent that they were willing to sacrifice the self-interest of their regimes for the sake of enacting those supposed laws. For instance, the Nazis treated the inhabitants of occupied territories with extreme brutality and planned to depopulate Eastern Europe in order to make way for colonists from the German “master race,” despite the fact that this actively harmed their war effort. Stalin repeatedly purged the Communist Party of people who deviated even slightly from the party line, even when this weakened the party or the Soviet government, because he believed that they represented the interests of “dying classes” and their demise was historically inevitable.\n\nArendt also identifies the central importance of an all-powerful leader in totalitarian movements. As in other areas, she distinguishes between totalitarian leaders (such as Hitler and Stalin) and non-totalitarian dictators or autocratic leaders. The totalitarian leader does not rise to power by personally using violence or through any special organizational skills, but rather by controlling appointments of personnel within the party, so that all other prominent party members owe their positions to him. With loyalty to the leader becoming the primary criterion for promotion, ambitious party members compete with each other in trying to express their loyalty, and a cult of personality develops around the leader. Even when the leader is not particularly competent and the members of his inner circle are aware of his deficiencies, they remain committed to him out of fear that without him the entire power structure would collapse.\n\nOnce in power, according to Arendt, totalitarian movements face a major dilemma: they built their support on the basis of anger against the status quo and on impossible or dishonest promises, but now they have become the new status quo and are expected to carry out their promises. They deal with this problem by engaging in a constant struggle against external and internal enemies, real or imagined, so as to enable them to say that, in a sense, they have not yet gained the power they need to fulfill their promises. According to Arendt, totalitarian governments must be constantly fighting enemies in order to survive. This explains their apparently irrational behavior, for example when Hitler continued to make territorial demands even after he was offered everything he asked for in the Munich Agreement, or when Stalin unleashed the Great Terror despite the fact that he faced no significant internal opposition.\n\nArendt points out the widespread use of concentration camps by totalitarian governments, arguing that they are the most important manifestation of the need to find enemies to fight against, and are therefore “more essential to the preservation of the regime’s power than any of its other institutions.” Although forced labor was commonly imposed on inmates of concentration camps, Arendt argues that their primary purpose was not any kind of material gain for the regime: “The only permanent economic function of the camps has been the financing of their own supervisory apparatus; thus from the economic point of view the concentration camps exist mostly for their own sake.” The Nazis in particular carried this to the point of “open anti-utility,” by expending large sums of money, resources and manpower – during a war – for the purpose of building and staffing extermination camps and transporting people to them. This sets apart the concentration camps of totalitarian regimes from older human institutions that bear some similarity to them, such as slavery. Slaves were abused and killed for the sake of profit; concentration camp inmates were abused and killed because a totalitarian government needed to justify its existence. Finally, Arendt points out that concentration camps under both Hitler and Stalin included large numbers of inmates who were innocent of any crime – not only in the ordinary sense of the word, but even by the standards of the regimes themselves. That is to say, most of the inmates had not actually committed any action against the regime.\n\nThroughout her analysis, Arendt emphasized the modernity and novelty of the governmental structures set up by Stalin and Hitler, arguing that they represented “an entirely new form of government” which is likely to manifest itself again in various other forms in the future. She also cautioned against the belief that future totalitarian movements would necessarily share the ideological foundations of Nazism or Stalinism, writing that “all ideologies contain totalitarian elements.”\n\nThe totalitarian paradigm in the comparative study of Nazi Germany and the Soviet Union was further developed by Carl Friedrich and Zbigniew Brzezinski, who wrote extensively on this topic both individually and in collaboration. Similar to Hannah Arendt, they state that “totalitarian dictatorship is a new phenomenon; there has never been anything quite like it before.” Friedrich and Brzezinski classify totalitarian dictatorship as a type of autocracy, but argue that it is different in important ways from most other historical autocracies. In particular, it is distinguished by a reliance on modern technology and mass legitimation. Unlike Arendt, Friedrich and Brzezinski apply the notion of totalitarian dictatorship not only to the regimes of Hitler and Stalin, but also to the USSR throughout its entire existence, as well as the regime of Benito Mussolini in Italy and the People’s Republic of China under Mao Zedong.\n\nCarl Friedrich noted that the “possibility of equating the dictatorship of Stalin in the Soviet Union and that of Hitler in Germany” has been a deeply controversial topic and a subject of debate almost from the beginning of those dictatorships. Various other aspects of the two regimes have also been the subject of intense scholarly debate, such as whether Nazi and Stalinist ideologies were genuinely believed and pursued by the respective governments, or whether the ideologies were merely convenient justifications for dictatorial rule. Friedrich himself argues in favor of the former view.\n\nFriedrich and Brzezinski argue that Nazism and Stalinism are not only similar to each other, but also represent a continuation or a return to the tradition of European absolute monarchy on certain levels. In the absolute monarchies of the seventeenth and eighteenth centuries, the monarch ultimately held all decisional power, and was considered accountable only to God. In Stalinism and Nazism, the leader likewise held all real power, and was considered accountable only to various intangible entities such as “the people”, “the masses” or “the Volk.” Thus the common feature of autocracies – whether monarchical or totalitarian – is the concentration of power in the hands of a leader who cannot be held accountable by any legal mechanisms, and who is supposed to be the embodiment of the will of an abstract entity. Friedrich and Brzezinski also identify other features common to all autocracies, such as “the oscillation between tight and loose control.” The regime alternates between periods of intense repression and periods of relative freedom, often represented by different leaders. This depends in part on the personal character of different leaders, but Friedrich and Brzezinski believe that there is also an underlying political cycle, in which rising discontent leads to increased repression up to the point at which the opposition is eliminated, then controls are relaxed until the next time that popular dissatisfaction begins to grow.\n\nThus, placing Stalinism and Nazism within the broader historical tradition of autocratic government, Friedrich and Brzezinski hold that “totalitarian dictatorship, in a sense, is the adaptation of autocracy to twentieth-century industrial society.” However, at the same time, they insist that totalitarian dictatorship is a “\"novel\" type of autocracy” and argue that twentieth century totalitarian regimes (such as those of Hitler and Stalin) had more in common with each other than with any other form of government, including historical autocracies of the past. Totalitarianism can only exist after the creation of modern technology, because such technology is essential for propaganda, for surveillance of the population, and for the operation of a secret police. Furthermore, when speaking of the differences and similarities between fascist and communist regimes, Friedrich and Brzezinski insist that the two kinds of totalitarian governments are “basically alike” but “not wholly alike” – they are more similar to each other than to other forms of government, but they are not the same. Among the major differences between them, Friedrich and Brzezinski identify in particular the fact that communists seek “the world revolution of the proletariat,” while fascists wish to “establish the imperial predominance of a particular nation or race.” \n\nIn terms of the similarities between Nazism and Stalinism, Friedrich lists five main aspects that they hold in common: First, an official ideology that is supposed to be followed by all members of society, at least passively, and which promises to serve as a perfect guide towards some ultimate goal. Second, a single political party, composed of the most enthusiastic supporters of the official ideology, representing an elite group within society (no more than 10 percent of the population), and organized along strictly regimented lines. Third, “a technologically conditioned near-complete monopoly of control of all means of effective armed combat” in the hands of the party or its representatives. Fourth, a similar monopoly held by the party over the mass media and all technological forms of communication. Fifth, “a system of terroristic police control” that is not only used to defend the regime against real enemies, but also to persecute various groups of people who are only suspected of being enemies or who may potentially become enemies in the future.\n\nTwo first pillars of any totalitarian government, according to Friedrich and Brzezinski, are the dictator and the Party. The dictator, whether Stalin, Hitler or Mussolini, holds supreme power. Friedrich and Brzezinski explicitly reject the claim that the Party, or any other institution, could provide a significant counterweight to the power of the dictator in Nazism or Stalinism. The dictator needs the Party in order to be able to rule, so he may be careful not to make decisions that would go directly against the wishes of other leading Party members, but ultimate authority rests with him and not with them. Like Arendt, Friedrich and Brzezinski also identify the cult of personality surrounding the leader as an essential element of a totalitarian dictatorship, and reference Stalin’s personality cult in particular. They also draw attention to the fact that Hitler and Stalin were expected to provide ideological direction for their governments and not merely practical leadership. Friedrich and Brzezinski write that “unlike military dictators in the past, but like certain types of primitive chieftains, the totalitarian dictator is both ruler and high priest.” That is to say, he not only governs, but also provides the principles on which his government is to be based. This is partly due to the way that totalitarian governments arise. They come about when a militant ideological movement seizes power, so the first leader of a totalitarian government is usually the ideologue who built the movement that seized power, and subsequent leaders try to emulate him.\n\nThe totalitarian dictator needs loyal lieutenants to carry out his orders faithfully and with a reasonable degree of efficiency. Friedrich and Brzezinski identify parallels between the men in Hitler and Stalin’s entourage, arguing that both dictators used similar people to perform similar tasks. Thus, for example, Martin Bormann and Georgy Malenkov were both capable administrators and bureaucrats, while Heinrich Himmler and Lavrentiy Beria were ruthless secret police chiefs responsible for suppressing any potential challenge to the dictator’s power. Both Hitler and Stalin promoted rivalry and distrust among their lieutenants so as to ensure that none of them would become powerful enough to challenge the dictator himself. This is the cause of an important weakness of the totalitarian regimes: the problem of succession. Friedrich points out that neither the Nazi nor the Stalinist government ever established any official line of succession or any mechanism to decide who would replace the dictator after his death. The dictator, being the venerated “father of the people,” was regarded as irreplaceable. There could never be any heir apparent, because such an heir would have been a threat to the power of the dictator while he was alive. Thus the dictator’s inevitable death would always leave behind a major power vacuum and cause a political crisis. In the case of the Nazi regime, since Hitler died mere days before the final defeat of Germany in the war, this never became a major issue. In the case of the USSR, Stalin’s death led to a prolonged power struggle.\n\nFriedrich and Brzezinski also identify key similarities between the Nazi and Stalinist political parties, which set them apart from other types of political parties. Both the Nazi Party and the CPSU under Stalin had very strict membership requirements and did not accept members on the basis of mere agreement with the Party’s ideology and goals. Rather, they strictly tested potential members, in a manner similar to exclusive clubs, and often engaged in political purges of the membership, expelling large numbers of people from their ranks (and sometimes arresting and executing those expelled, such as in the Great Purge or the Night of the Long Knives). Thus, the totalitarian party cultivates the idea that to be a member is a privilege which needs to be earned, and total obedience to the leader is required in order to maintain this privilege. While both Nazism and Stalinism required party members to display such total loyalty in practice, they differed in the way they dealt with it in theory. Nazism openly proclaimed the hierarchical ideal of absolute obedience to the Führer as one of its key ideological principles (the \"Führerprinzip\"). Stalinism, meanwhile, denied that it did anything similar, and claimed instead to uphold democratic principles, with the Party Congress (made up of elected delegates) supposedly being the highest authority. However, Stalinist elections typically featured only a single candidate, and the Party Congress met very rarely and simply approved Stalin’s decisions. Thus, regardless of the differences in their underlying ideological claims, the Nazi and Stalinist parties were organized in practice along similar lines, with a rigid hierarchy and centralized leadership.\n\nEach totalitarian party and dictator is supported by a specific totalitarian ideology. Friedrich and Brzezinski argue, in agreement with Arendt, that Nazi and Stalinist leaders really believed in their respective ideologies and did not merely use them as tools to gain power. Several major policies, such as the Stalinist collectivization of agriculture or the Nazi “final solution”, cannot be explained by anything other than a genuine commitment to achieve ideological goals, even at great cost. The ideologies were different and their goals were different, but what they had in common was a utopian commitment to reshaping the world, and a determination to fight by any means necessary against a real or imagined enemy. This stereotyped enemy could be described as “the fat rich Jew or the Jewish Bolshevik” for the Nazis, or “the war-mongering, atom-bomb-wielding American Wallstreeter” for the Soviets.\n\nAccording to Friedrich and Brzezinski, the most important difference between Nazi and Stalinist ideology lies in the degree of universality involved. Stalinism, and communist ideology in general, is universal in its appeal and addresses itself to all the “workers of the world.” Nazism, on the other hand, and fascist ideology in general, can only address itself to one particular race or nation – the “master race” that is destined to dominate all others. Therefore, “in communism social justice appears to be the ultimate value, unless it be the classless society that is its essential condition; in fascism, the highest value is dominion, eventually world dominion, and the strong and pure nation-race is \"its\" essential condition, as seen by its ideology.” This means that fascist or Nazi movements from different countries will be natural enemies, rather than natural allies, as they each seek to extend the dominion of their own nation at the expense of others. Friedrich and Brzezinski see this as a weakness inherent in fascist and Nazi ideology, while communist universalism is a source of ideological strength for Stalinism.\n\nFriedrich and Brzezinski also draw attention to the symbols used by Nazis and Stalinists to represent themselves. The Soviet Union adopted the hammer and sickle, a newly-created symbol, “invented by the leaders of the movement and pointing to the future.” Meanwhile, Nazi Germany used the swastika, “a ritual symbol of uncertain origin, quite common in primitive societies.” Thus, one is trying to project itself as being oriented towards a radically new future, while the other is appealing to a mythical heroic past.\n\nTotalitarian dictatorships maintain themselves in power through the use of propaganda and terror, which Friedrich and Brzezinski believe to be closely connected. Terror may be enforced with arrests and executions of dissenters, but it can also take more subtle forms, such as the threat of losing one’s job, social stigma and defamation. “Terror” can refer to any widespread method used to intimidate people into submission as a matter of daily life. According to Friedrich and Brzezinski, the most effective terror is invisible to the people it affects. They simply develop a habit of acting in a conformist manner and not questioning authority, without necessarily being aware that this is what they are doing. Thus, terror creates a society dominated by apparent consensus, where the vast majority of the population appears to support the government. Propaganda is then used to maintain this appearance of popular consent. \n\nTotalitarian propaganda is one of the features that distinguishes totalitarian regimes as modern forms of government and separates them from older autocracies, since a totalitarian government holds complete control over all means of communication (not only public communication such as the mass media, but also private communication such as letters and telephone calls, which are strictly monitored). The methods of propaganda were very similar in the Stalinist USSR and in Nazi Germany. Both Joseph Goebbels and Soviet propagandists sought to demonize their enemies and present a picture of a united people standing behind its leader to confront foreign threats. In both cases there was no attempt to convey complex ideological nuances to the masses, with the message being instead about a simplistic struggle between good and evil. Both Nazi and Stalinist regimes produced two very different sets of propaganda – one for internal consumption and one for potential sympathizers in other countries. And both regimes would sometimes radically change their propaganda line as they made peace with a former enemy or got into a war with a former ally. Yet, paradoxically, a totalitarian government’s complete control over communications renders that government highly misinformed. With no way for anyone to express criticism, the dictator has no way of knowing how much support he actually has among the general populace. With all government policies always declared successful in propaganda, officials are unable to determine what actually worked and what didn’t. Both Stalinism and Nazism suffered from this problem, especially during the war between them. As the war turned against Germany, there was growing opposition to Hitler’s rule, including within the ranks of the military, but Hitler was never aware of this until it was too late (see: 20 July plot). In 1948, during the early days of the Berlin Blockade, the Soviet leadership apparently believed that the population of West Berlin was sympathetic to Soviet Communism and that they would request to join the Soviet zone. Given enough time, the gap between real public opinion and what the totalitarian government believes about public opinion can grow so wide that the government is no longer able to even produce effective propaganda, because it does not know what the people actually think and so it does not know what to tell them. Friedrich and Brzezinski refer to this as the “ritualization of propaganda”: the totalitarian regime continues to produce propaganda as a political ritual, with little real impact on public opinion.\n\nThe totalitarian use of mass arrests, executions and concentration camps – also noted by Arendt – was analyzed at length by Friedrich and Brzezinski. They hold that “totalitarian terror maintains, in institutionalized form, the civil war that originally produced the totalitarian movement and by means of which the regime is able to proceed with its program, first of social disintegration and then of social reconstruction.” Both Stalinism and Nazism saw themselves as engaging in a life-or-death struggle against implacable enemies. But to declare that the struggle had been won would have meant to declare that most of the totalitarian features of the government were no longer needed. A secret police force, for instance, has no reason to exist if there are no dangerous traitors who need to be found. Thus the struggle, or “civil war” against internal enemies, must be institutionalized and must continue indefinitely. In the Stalinist USSR, the repressive apparatus was eventually turned against members of the Communist Party itself in the Great Purge and the show trials that accompanied it. Nazism, by contrast, had a much shorter lifespan in power, and Nazi terror generally maintained an outward focus, with the extermination of the Jews always given top priority. The Nazis did not turn inward towards purging their own party except in a limited way on two occasions (the Night of the Long Knives and the aftermath of the 20 July plot). \n\nThe peak of totalitarian terror was reached with the Nazi concentration camps. These ranged from labor camps to extermination camps, and they are described by Friedrich and Brzezinski as aiming to “eliminate all actual, potential, and imagined enemies of the regime.” As the field of Holocaust studies was still in its early stages at the time of their writing, they do not describe the conditions in detail, but do refer to the camps as involving “extreme viciousness.” They also compare these camps with the Soviet Gulag system, and highlight the use of concentration camps as a method of punishment and execution by Nazi and Stalinist regimes alike. However, unlike Hannah Arendt, who held that the Gulag camps served no economic purpose, Friedrich and Brzezinski argue that they provided an important source of cheap labor for the Stalinist economy.\n\nThe comparative study of Nazism and Stalinism was carried further by other groups of scholars, such as Moshe Lewin and Ian Kershaw together with their collaborators. Writing after the dissolution of the USSR, Lewin and Kershaw take a longer historical perspective and regard Nazism and Stalinism not so much as examples of a new type of society (like Arendt, Friedrich and Brzezinski did), but more as historical “anomalies” – unusual deviations from the typical path of development that most industrial societies are expected to follow. Therefore, the task of comparing Nazism and Stalinism is, to them, a task of explaining why Germany and Russia (along with other countries) deviated from the historical norm. At the outset, Lewin and Kershaw identify similarities between the historical situations in Germany and Russia prior to the First World War and during that war. Both countries were ruled by authoritarian monarchies, who were under pressure to make concessions to popular demands. Both countries had “powerful bureaucracies and strong military traditions.” Both had “powerful landowning classes,” while also being in the process of rapid industrialization and modernization. And both countries had expansionist foreign policies with a particular interest in Central and Eastern Europe. Lewin and Kershaw do not claim that these factors made Stalinism or Nazism inevitable, but rather that they help to explain why the Stalinist and Nazi regimes developed similar features.\n\nIan Kershaw admitted that Stalinism and Nazism are comparable in “the nature and extent of their inhumanity,” but noted that the two regimes were different in a number of aspects Lewin and Kershaw question the usefulness of grouping the Stalinist and Nazi regimes together under a “totalitarian” category, saying that it remains an open question whether the similarities between them are greater or smaller than the differences. In particular, they criticize what they see as the ideologically-motivated attempt to determine which regime killed more people, saying that apologists of each regime are trying to defend their side by claiming the other was responsible for more deaths.\n\nLewin and Kershaw place the cult of personality at the center of their comparison of Nazism and Stalinism, writing that both regimes “represented a new genre of political system centred upon the artificial construct of a leadership cult – the ‘heroic myth’ of the ‘great leader’, no longer a king or emperor but a ‘man of the people.” With regard to Stalinism, they emphasize its bureaucratic character, and its “merging of the most modern with the most archaic traits” by combining modern technology and the latest methods of administration and propaganda with the ancient practice of arbitrary rule by a single man. They compare this with the Prussian military tradition in Germany, which had been called “bureaucratic absolutism” in the eighteenth century, and which played a significant role in the organization of the Nazi state in the twentieth century.\n\nKershaw agrees with Mommsen that there was a fundamental difference between Nazism and Stalinism regarding the importance of the leader. Stalinism had an absolute leader, but he was not essential. He could be replaced by another. Nazism, on the other hand, was a “classic charismatic leadership movement,” defined entirely by its leader. Stalinism had an ideology which existed independently of Stalin. But for Nazism, “Hitler \"was\" ideological orthodoxy” – Nazi ideals were by definition whatever Hitler said they were. In Stalinism, the bureaucratic apparatus was the foundation of the system, while in Nazism, the person of the leader was the foundation.\n\nMoshe Lewin also focuses on the comparison between the personality cults of Hitler and Stalin, and their respective roles in Nazi Germany and the Soviet Union. He refers to them as the “Hitler myth” and the “Stalin myth,” and argues that they served different functions within their two regimes. The function of the “Hitler myth” was to legitimize Nazi rule. The function of the “Stalin myth” was to legitimize not Soviet rule itself, but Stalin’s leadership within the Party. Stalin’s personality cult existed precisely because Stalin knew that he was replaceable, and feared that he might be replaced, and so needed to bolster his authority as much as possible. While the “Hitler myth” was essential to Nazi Germany, the “Stalin myth” was essential only to Stalin, not to the Soviet Union itself.\n\nTogether with fellow historian Hans Mommsen, Lewin argues that the Stalinist and Nazi regimes featured an “intrinsic structural contradiction” which led to “inherent self-destructiveness”: they depended on a highly organized state bureaucracy which was trying to set up complex rules and procedures for every aspect of life, yet this bureaucracy was under the complete personal control of a despot who made policy decisions as he saw fit, routinely changing his mind on major issues, without any regard for the rules and institutions which his own bureaucracy had set up. The bureaucracy and the leader needed each other, but also undermined each other with their different priorities. Mommsen sees this as being a much greater problem in Nazi Germany than in Stalin’s Soviet Union, as the Nazis inherited large parts of the traditional German bureaucracy, while the Soviets largely built their own bureaucracy from the ground up. He argues that many of the irrational features of the Nazi regime – such as wasting resources on exterminating undesirable populations instead of using those resources in the war effort – were caused by the dysfunction of the Nazi state rather than by fanatical commitment to Nazi ideology. In accordance with the Führerprinzip, all decisional power in the Nazi state ultimately rested with Hitler. But Hitler often issued only vague and general directives, forcing other Nazi leaders lower down in the hierarchy to guess what precisely the Führer wanted. This confusion produced competition between Nazi officials, as each of them attempted to prove that he was a more dedicated Nazi than his rivals, by engaging in ever more extreme policies. This competition to please Hitler was, according to Mommsen, the real cause of Nazi irrationality. Hitler was aware of it, and deliberately encouraged it out of a “social-darwinist conviction that the best man would ultimately prevail.” Mommsen argues that this represents a structural difference between the regimes of Hitler and Stalin. In spite of its purges, Stalin’s regime was more effective in building a stable bureaucracy, such that it was possible for the system to sustain itself and continue even without Stalin. The Nazi regime, on the other hand, was much more personalized and depended entirely on Hitler, being unable to build any lasting institutions.\n\nKershaw also saw major personal differences between Stalin and Hitler and their respective styles of rule. He describes Stalin as “a committee man, chief oligarch, man of the machine” and a “creature of his party,” who came to power only thanks to his party and his ability to manipulate the levers of power within that party. Hitler, by contrast, came to power based on his charisma and mass appeal, and in the Nazi regime it was the leader that created the party instead of the other way around. According to Kershaw, “Stalin was a highly interventionist dictator, sending a stream of letters and directives determining or interfering with policy,” while Hitler “was a non-interventionist dictator as far as government administration was concerned,” preferring to involve himself in military affairs and plans for conquest rather than the daily routine of government work, and giving only broad verbal instructions to his subordinates regarding civilian affairs, which they were expected to translate into policy. Furthermore, although both regimes featured all-pervasive cults of personality, there was a qualitative difference between those cults. Stalin’s personality cult was “superimposed upon the Marxist-Leninist ideology and Communist Party,” and could be abandoned (or replaced with a personality cult around some other leader) without major changes to the regime. On the other hand, “the ‘Hitler myth’ was structurally indispensable to, in fact the very basis of, and scarcely distinguishable from, the Nazi Movement and its \"Weltanschauung\".” The belief in the person of Adolf Hitler as the unique savior of the German nation was the very foundation of Nazism, to such an extent that Nazism found it impossible to even imagine a successor to Hitler. Thus, in Kershaw’s analysis, Stalinism was a fundamentally bureaucratic system while Nazism was the embodiment of “charismatic authority” as described by Max Weber. Stalinism could exist without its leader. Nazism could not. \n\nThe topic of comparisons between Nazism and Stalinism was also studied in the 1990s and 2000s by historians Henry Rousso, Nicolas Werth and Philippe Burrin.\n\nRousso defends the work of Carl Friedrich by pointing out that Friedrich himself had only said that Stalinism and Nazism were comparable, not that they were identical. Rousso also argues that the popularity of the concept of totalitarianism (the way that large numbers of people have come to routinely refer to certain governments as “totalitarian”) should be seen as evidence that the concept is useful, that it really describes a specific type of government which is different from other dictatorships. At the same time, however, Rousso notes that the concept of totalitarianism is descriptive rather than analytical: the regimes described as totalitarian do not have a common origin and did not arise in similar ways. Nazism is unique among totalitarian regimes in having taken power in “a country endowed with an advanced industrial economy and with a system of political democracy (and an even older political pluralism).” All other examples of totalitarianism (including the Stalinist regime) took power, according to Rousso, “in an agrarian economy, in a poor society without a tradition of political pluralism, not to mention democracy, and where diverse forms of tyranny had traditionally prevailed.” He sees this as a weakness of the concept of totalitarianism, because it merely describes the similarities between Stalinism and Nazism without dealing with the very different ways they came to power. On the other hand, Rousso agrees with Hannah Arendt that “totalitarian regimes constitute something new in regard to classical tyranny, authoritarian regimes, or other forms of ancient and medieval dictatorships,” and he says that the main strength of the concept of totalitarianism is the way it highlights this inherent novelty of the regimes involved.\n\nNicolas Werth and Philippe Burrin have worked together on comparative assessments of Stalinism and Nazism, with Werth covering the Stalinist regime and Burrin covering Nazi Germany. One of the topics they have studied is the question of how much power the dictator really held in the two regimes. Werth identifies two main historiographical approaches in the study of the Stalinist regime: Those who emphasize the power and control exercised by Joseph Stalin himself, attributing most of the actions of the Soviet government to deliberate plans and decisions made by him, and those who argue that Stalin had no pre-determined course of action in mind, that he was reacting to events as they unfolded, and that the Soviet bureaucracy had its own agenda which often differed from Stalin’s wishes. Werth regards these as two mistaken extremes, one making Stalin seem all-powerful, the other making him seem like a weak dictator. But he believes that the competing perspectives are useful in drawing attention to the tension between two different forms of organization in the Stalinist USSR: an “administrative system of command,” bureaucratic and resistant to change but effective in running the Soviet state, and the strategy of “running the country in a crudely despotic way by Stalin and his small cadre of directors.” Thus, Werth agrees with Lewin that there was an inherent conflict between the priorities of the Soviet bureaucracy and Stalin’s accumulation of absolute power in his own hands. According to Werth, it was this unresolved and unstated conflict that led to the Great Purge and to the use of terror by Stalin’s regime against its own party and state cadres.\n\nIn studying similar issues with regard to the Nazi regime, Philippe Burrin draws attention to the debate between the “Intentionalist” and “Functionalist” schools of thought, which dealt with the question of whether the Nazi regime represented an extension of Hitler’s autocratic will, faithfully obeying his wishes, or whether it was an essentially chaotic and uncontrollable system that functioned on its own with little direct input from the Führer. Like Kershaw and Lewin, Burrin says that the relationship between the leader and his party’s ideology was different in Nazism compared to Stalinism: “One can rightly state that Nazism cannot be dissociated from Hitlerism, something that is difficult to affirm for Bolshevism and Stalinism.” Unlike Stalin, who inherited an existing system with an existing ideology and presented himself as the heir to the Leninist political tradition, Hitler created both his movement and its ideology by himself, claiming to be “someone sent by Providence, a Messiah whom the German people had been expecting for centuries, even for two thousand years, as Heinrich Himmler enjoyed saying.” Thus, there could be no real conflict between the Party and the leader in Nazi Germany, because the Nazi Party’s entire reason for existence was to support and follow Hitler. However, there was a potential for division between the leader and the state bureaucracy, due to the way that Nazism came to power – as part of an alliance with traditional conservative elites, industrialists, and the army. Unlike the USSR, Nazi Germany did not build its own state, but rather inherited the state machinery of the previous government. This provided the Nazis with an immediate supply of capable and experienced managers and military commanders, but on the other hand it also meant that the Nazi regime had to rely on the cooperation of people who had not been Nazis prior to Hitler’s rise to power, and whose loyalty was questionable. It was only during the war, when Nazi Germany conquered large territories and had to create Nazi administrations for them, that brand new Nazi bureaucracies were created without any input or participation from traditional German elites. This produced a surprising difference between Nazism and Stalinism: When the Stalinist USSR conquered territory, it created smaller copies of itself and installed them as the governments of the occupied countries. When Nazi Germany conquered territory, on the other hand, it did not attempt to create copies of the German government back home. Instead, it experimented with different power structures and policies, often reflecting a “far more ample Nazification of society than what the balance of power authorized in the Reich.”\n\nAnother major topic investigated by Werth and Burrin was the violence and terror employed by the regimes of Hitler and Stalin. Werth reports that the Stalinist USSR underwent an “extraordinary brutalization of the relations between state and society” for the purpose of rapid modernization and industrialization, to “gain one hundred years in one decade, and to metamorphose the country into a great industrial power.” This transformation was accomplished at the cost of massive violence and a sociopolitical regression into what Werth calls “military-feudal exploitation.” The types of violence employed by the Stalinist regime included loss of civil rights, mass arrests, deportations of entire ethnic groups from one part of the USSR to another, forced labor in the Gulag, mass executions (especially during the Great Terror of 1937-38), and most of all the great famine of 1932-33, known as the Holodomor. All levels of Soviet society were affected by Stalinist repression, from the top to the bottom. At the top, high-ranking members of the Communist Party were arrested and executed under the claim that they had plotted against Stalin (and in some cases they were forced to confess to imaginary crimes in show trials). At the bottom, the peasantry suffered the Holodomor famine (especially in Ukraine), and even outside of the famine years they were faced with very high grain quotas.\n\nWerth identifies four categories of people that became the targets of Stalinist violence in the USSR. He lists them from smallest to largest. The first and smallest group consisted of many of Stalin’s former comrades-in-arms, who had participated in the revolution and were known as “Old Bolsheviks.” They were dangerous to Stalin because they had known him before his rise to power and could expose the many false claims made by his personality cult. The second group consisted of mid-level Communist Party officials, who were subject to mass arrests and executions in the late 1930s, particularly during the Great Purge. Eliminating them served a dual purpose: It helped Stalin to centralize power in the Kremlin (as opposed to regional centers), and it also provided him with “corrupt officials” that he could blame for earlier repressions and unpopular policies. Werth draws parallels between this and the old Tsarist tradition of blaming “bad bureaucrats” – rather than the Tsar – for unpopular government actions. The third group was made up of ordinary citizens from all walks of life who resorted to petty crime in order to provide for themselves in the face of worsening living standards (for example by taking home some wheat from the fields or tools from the factory). This type of petty crime became very widespread, and was often punished as if it were intentional sabotage motivated by political opposition to the USSR. The fourth and largest category consisted of ethnic groups that were subject to deportation, famine, or arbitrary arrests under the suspicion of being collectively disloyal to Stalin or to the Soviet state. This included the Holodomor famine directed at the Ukrainians, the deportation of ethnic groups suspected of pro-German sympathies (such as the Volga Germans, the Crimean Tatars, the Chechens and others), and eventually also persecution of ethnic Jews, especially as Stalin grew increasingly antisemitic near the end of his life.\n\nBurrin’s study of violence carried out by the Nazi regime begins with the observation that “violence is at the heart of Nazism,” and that Nazi violence is “established as a doctrine and exalted in speech.” This marks a point of difference between Nazism and Stalinism, according to Burrin. In Stalinism, there was a gulf between ideology and reality when it came to violence. The Soviet regime continuously denied that it was repressive, proclaimed itself a defender of peace, and sought to conceal all the evidence to the contrary. In Nazism, on the other hand, “doctrine and reality were fused from the start.” Nazism not only practiced violent repression and war, but advocated it in principle as well, considering war to be a positive force in human civilization and openly seeking ”living space” and the domination of the European continent by ethnic Germans.\n\nBurrin identifies three motivations for Nazi violence: political repression, exclusion and social repression, and racial politics. The first of these, political repression, is common in many dictatorships. The Nazis aimed to eliminate their real or imagined political opponents, first in the Reich and later in the occupied territories during the war. Some of these opponents were executed, while others were imprisoned in concentration camps. The first targets of political repression, immediately after Hitler’s rise to power in 1933, were the parties of the Left in general and the Communists in particular. Then, after the mid-1930s, repression was extended to members of the clergy, and later to the conservative opposition as well (especially after the failed attempt to assassinate Hitler in 1944). The death penalty was used on a wide scale, even before the war. During the war, political repression was greatly expanded both inside Germany and especially in the newly occupied territories. Political prisoners in the concentration camps numbered only about 25,000 at the beginning of the war. By January 1945 they had swelled to 714,211 – most of them non-Germans accused of plotting against the Reich.\n\nThe second type of Nazi violence, motivated by exclusion and social repression, was the violence aimed at purging German society of people whose lifestyle was considered incompatible with the social norms of the Nazi regime (even if the people involved were racially pure and able-bodied). Such people were divided into two categories: homosexuals and “asocials.” The “asocials” were only vaguely defined, and included “Gypsies, tramps, beggars, prostitutes, alcoholics, the jobless who refused any employment, and those who left their work frequently or for no reason.”\n\nThe third and final type of Nazi violence, by far the most extensive, was violence motivated by Nazi racial policies. This was aimed both inward, to cleanse the “Aryan race” of “degenerate” elements and life unworthy of life, as well as outward, to seek the extermination of “inferior races”. Germans considered physically or mentally unfit were among the first victims. One of the first laws of the Nazi regime mandated the forced sterilization of people suffering from physical handicaps or who had psychiatric conditions deemed to be hereditary. Later, sterilization was replaced by murder of the mentally ill and of people with severe disabilities, as part of a “euthanasia” program called Aktion T4. Burrin notes that this served no practical political purpose – the people being murdered could not have possibly been political opponents of the regime – so the motivation was purely a matter of racial ideology. The most systematic and by far the most large-scale acts of Nazi violence, however, were directed at “racially inferior” non-German populations. As laid out in \"Generalplan Ost\", the Nazis wished to eliminate most of the Slavic populations of Eastern Europe, partly through deportation and partly through murder, in order to secure land for ethnic German settlement and colonization. But even more urgently, the Nazis wished to exterminate the Jews of Europe, whom they regarded as the implacable racial enemy of the Germans. This culminated in the Holocaust, the Nazi genocide of the Jews. Unlike in the case of all other target populations, the Jews were to be exterminated completely, with no individual exceptions for any reason.\n\nIn \"Beyond Totalitarianism: Stalinism and Nazism Compared\", editors Michael Geyer and Sheila Fitzpatrick disputed the concept of totalitarianism, noting that the term entered political discourse first as a term of self-description by the Italian Fascists and was only later used as a framework to compare Nazi Germany with the Soviet Union. They argued that the totalitarian states were not as monolithic or as ideology-driven as they seemed. Geyer and Fitzpatrick describe Nazi Germany and the Stalinist USSR as “immensely powerful, threatening, and contagious dictatorships” who “shook the world in their antagonism.” Without calling them totalitarian, they identified their common features, including genocide, an all-powerful party, a charismatic leader, and pervasive invasion of privacy. However, they argue that Stalinism and Nazism did not represent a new and unique type of government, but rather that they can be placed in the broader context of the turn to dictatorship in Europe in the interwar period. The reason they appear extraordinary is because they were the “most prominent, most hard-headed, and most violent” of the European dictatorships of the 20th century. They are comparable because of their “shock and awe” and sheer ruthlessness, but underneath superficial similarities they were fundamentally different and that “when it comes to one-on-one comparison, the two societies and regimes may as well have hailed from different worlds.”\n\nAccording to Geyer and Fitzpatrick, the similarities between Nazism and Stalinism stem from the fact that they were both “ideology driven” and sought to subordinate all aspects of life to their respective ideologies. The differences stem from the fact that their ideologies were opposed to each other and regarded each other as enemies. Another major difference is that Stalin created a stable and long-lasting regime, while Nazi Germany had a “short-lived, explosive nature.” Notably, the stable state created by Stalinism was based on an entirely new elite, while Nazism, despite having the support of the traditional elite, failed to achieve stability.\n\nHowever, the two regimes did borrow ideas from one another, especially regarding propaganda techniques (most of all in architecture and cinema), but also in terms of state surveillance and antisemitism. At the same time, they both vigorously denied borrowing anything from each other. While their methods of propaganda were similar, the content was different. For instance, Soviet wartime propaganda revolved around the idea of resisting imperial aggression, while Nazi propaganda was about wars of racial conquest. Geyer and Fitzpatrick also take note of the fact that both Stalinism and Nazism sought to create a New Man, an “entirely modern, illiberal, and self-fashioned personage,” even though they had different visions about what being a “New Man” would mean.\n\nAmong the other authors contributing to the volume edited by Geyer and Fitzpatrick, David Hoffmann and Annette Timm discuss biopolitics and the pro-natalist policies of the Nazi and Stalinist regimes. Both governments were highly concerned over low fertility rates in their respective populations, and applied extensive and intrusive social engineering techniques to increase the number of births. Reproductive policies in the Soviet Union and Nazi Germany were administered through their health care systems—both regimes saw health care as a key pillar to their designs to develop a new society. While the Soviet Union had to design a public health care system from scratch, Nazi Germany built upon the pre-existing public health care system in Germany that had existed since 1883, when Otto von Bismarck's legislation had created the world's first national public health care program. The Nazis centralized the German health care system in order to enforce Nazi ideological components upon it, and replaced existing voluntary and government welfare agencies with new ones that were devoted to racial hygiene and other components of Nazi ideology.\n\nThe Nazi and Stalinist attempt to control family size was not unique, as many other European states practiced eugenics at this time, and the Stalinist and Nazi ideals were vastly different. In fact, they had more in common with third parties than with each other: Nazi Germany’s policies were rather similar to those in Scandinavia at the time, while the USSR’s policies resembled those in Catholic countries.The common point between Nazi and Stalinist practices was the connection of reproduction policies with the ideological goals of the state — \"part of the project of a rational, hypermodern vision for the re-organization of society\". There were nevertheless substantial differences between the two regimes' approaches. Stalin's Soviet Union never officially supported eugenics as the Nazis did—the Soviet government called eugenics a \"fascist science\"—although there were in fact Soviet eugenicists. The two regimes also had different approaches to the relationship between family and paid labor—Nazism promoted the male single-breadwinner family while Stalinism promoted the dual-wage-earner household.\n\nIn another contribution to the same volume, Christian Gerlach and Nicolas Werth discuss the topic of mass violence, and the way that it was used by both Stalinism and Nazism. Both Stalin's Soviet Union and Nazi Germany were violent societies where mass violence was accepted by the state, such as in the Great Terror of 1937 to 1938 in the Soviet Union and the Holocaust in Nazi Germany and its occupied territories in World War II.\n\nBoth the Stalinist Soviet Union and Nazi Germany utilized internment camps led by agents of the state – the NKVD in the Soviet Union and the SS in Nazi Germany. They also both engaged in violence against minorities based on xenophobia – the xenophobic violence of the Nazis was outspoken but rationalized as being against \"asocial\" elements while the xenophobic violence of the Stalinists was disguised as being against \"anti-soviet\", \"counter-revolutionary\" and \"socially harmful\" elements – a term which often targeted diaspora nationalities. The Stalinist Soviet Union established \"special settlements\" where the \"socially harmful\" or \"socially dangerous\" who included ex-convicts, criminals, vagrants, the disenfranchised and \"declassed elements\" were expelled to. These \"special settlements\" were largely in Siberia, the far north, the Urals, or other inhospitable territories. In July 1933, the Soviet Union made a mass arrest of 5000 Romani people effectively on the basis of their ethnicity, who were deported that month to the \"special settlements\" in Western Siberia. In 1935, the Soviet Union arrested 160,000 homeless people and juvenile delinquents and sent many of them to NKVD labor colonies where they did forced labor.\n\nThe Nazi regime was founded upon a racialist view of politics and envisioned the deportation or extermination of the majority of the population of Eastern Europe in order to open up “living space” for ethnic German settlers. This was mainly intended to be carried out after an eventual German victory in the war, but steps had already started being taken while the war was still ongoing. For instance, by the end of 1942, the Nazis had deported 365,000 Poles and Jews from their original homes in western Poland (now German-annexed) and into the General Government. A further 194,000 Poles were internally displaced (not deported to another territory but expelled from their homes). The Nazis had also deported 100,000 persons from Alsace, Lorraine, and Luxembourg, as well as 54,000 Slovenians.\n\nStalinism in practice in the Soviet Union pursued ethnic deportations from the 1930s to the early 1950s, with a total of 3 million Soviet citizens being subjected to ethnic-based resettlement. The first major ethnic deportation took place from December 1932 to January 1933, during which some 60,000 Kuban Cossacks were collectively criminally charged as a whole with association with resistance to socialism and affiliation with Ukrainian nationalism. From 1935 to 1936, the Soviet Union deported Soviet citizens of Polish and German origins living in the western districts of Ukraine, and Soviet citizens of Finnish origins living on the Finland-Soviet Union border. These deportations from 1935 to 1936 affected tens of thousands of families. From September to October 1937, Soviet authorities deported the Korean minority from its Far Eastern region that bordered on Japanese-controlled Korea. Soviet authorities claimed the territory was \"rich soil for the Japanese to till\" – implying a Soviet suspicion that the Koreans could potentially join forces with the Japanese to unite the land with Japanese-held Korea. Over 170,000 Koreans were deported to remote parts of Soviet Central Asia from September to October 1937. These ethnically-based deportations reflected a new trend in Stalinist policy, a \"Soviet xenophobia\" based on ideological grounds that suspected that these people were susceptible to foreign influence, and which was also based on a resurgent Russian nationalism.\n\nAfter Nazi Germany declared war on the Soviet Union in 1941, the Soviet Union initiated another major round of ethnic deportations. The first group targeted were Soviet Germans. Between September 1941 and February 1942, 900,000 people – over 70 percent of the entire Soviet German community – were deported to Kazakhstan and Siberia in mass operations. A second wave of mass deportations took place between November 1943 and May 1944, in which Soviet authorities expelled six ethnic groups (the Balkars, Chechens, Crimean Tatars, Ingush, Karachai, and Kalmyks) that together numbered 900,000. There were also smaller-scale operations involving ethnic cleansing of diaspora minorities during and after World War II, in which tens of thousands of Crimean Bulgarians, Greeks, Iranians, Khemshils, Kurds, and Meskhetian Turks were deported from the Black Sea and Transcaucasian border regions.\n\nTwo ethnic groups that were specifically targeted for persecution by Stalin's Soviet Union were the Chechens and the Ingush. Unlike the other nationalities that could be suspected of connection to foreign states which shared their ethnic background, the Chechens and the Ingush were completely indigenous people of the Soviet Union. Rather than being accused of collaboration with foreign enemies, these two ethnic groups were considered to have cultures which did not fit in with Soviet culture – such as accusing Chechens of being associated with “banditism” – and the authorities claimed that the Soviet Union had to intervene in order to “remake” and “reform” these cultures. In practice this meant heavily armed punitive operations carried out against Chechen “bandits” that failed to achieve forced assimilation, culminating in an ethnic cleansing operation in 1944, which involved the arrests and deportation of over 500,000 Chechens and Ingush from the Caucasus to Central Asia and Kazakhstan. The deportations of the Chechens and Ingush also involved the outright massacre of thousands of people, and severe conditions placed upon the deportees – they were put in unsealed train cars, with little to no food for a four-week journey during which many died from hunger and exhaustion.\n\nThe main difference between Nazi and Stalinist deportations was in their purpose: while Nazi Germany sought ethnic cleansing to allow settlement by Germans into the cleansed territory, Stalin's Soviet Union pursued ethnic cleansing in order to remove minorities from strategically important areas.\n\nOther historians and political scientists have also made comparisons between Nazism and Stalinism as part of their work.\n\nStanley Payne, in his work on fascism, said that although the Nazi Party was ideologically opposed to communism, Adolf Hitler and other Nazi leaders frequently expressed recognition that only in Soviet Russia were their revolutionary and ideological counterparts to be found. Both placed a major emphasis on creating a \"party-army,\" with the regular armed forces controlled by the party. In the case of the Soviet Union this was done through the political commissars, while Nazi Germany introduced a roughly equivalent leadership role for \"National Socialist Guidance Officers\" in 1943.\n\nFrançois Furet, in his work on communism, noted that Hitler personally admired Soviet leader Joseph Stalin, and on numerous occasions publicly praised Stalin for seeking to purify the Communist Party of the Soviet Union of Jewish influences, especially by purging Jewish communists such as Leon Trotsky, Grigory Zinoviev, Lev Kamenev and Karl Radek.\n\nRichard Pipes draws attention to Stalin and his antisemitism in a parallel with Nazi antisemitism. He notes that soon after the 1917 October Revolution, the Soviet Union undertook practices to break up Jewish culture, religion and language. In the fall of 1918, the Soviet Communist Party set up the Jewish section Yevsektsiya, with a stated mission of “destruction of traditional Jewish life, the Zionist movement, and Hebrew culture.” By 1919, the Bolsheviks began to confiscate Jewish properties, Hebrew schools, libraries, books, and synagogues in accordance with newly imposed anti-religious laws, turning their buildings into \"Communist centers, clubs or restaurants.\" After Joseph Stalin rose to power, antisemitism continued to be endemic throughout Russia, although official Soviet policy condemned it. On August 12, 1952, Stalin's personal antisemitism became more visible, as he ordered the execution of the most prominent Yiddish authors in the Soviet Union, in an event known as the \"Night of the Murdered Poets\". Shortly before his death, Stalin also organized the anti-Semitic campaign known as the Doctors' plot.\n\nA number of research institutions are focusing on the analysis of fascism/Nazism and Stalinism/communism, and the comparative approach, including the Hannah Arendt Institute for the Research on Totalitarianism in Germany, the Institute for the Study of Totalitarian Regimes in the Czech Republic and the Institute of National Remembrance in Poland.\n\nIn comparing the deaths caused by both Stalin and Hitler's policies, some historians have asserted that archival evidence released after the collapse of the USSR confirms that Stalin did not kill more people than Hitler. American historian Timothy D. Snyder, for example, after assessing such data, says that while the Nazi regime killed approximately 11 million non-combatants (which rises to above 12 million if \"foreseeable deaths from deportation, hunger, and sentences in concentration camps are included\"), Stalin's deliberately killed about 6 million (rising to 9 million if foreseeable deaths arising from policies are taken into account). Australian historian and archival researcher Stephen G. Wheatcroft posits that \"The Stalinist regime was consequently responsible for about a million purposive killings, and through its criminal neglect and irresponsibility it was probably responsible for the premature deaths of about another two million more victims amongst the repressed population, i.e. in the camps, colonies, prisons, exile, in transit and in the POW camps for Germans. These are clearly much lower figures than those for whom Hitler's regime was responsible.\" Wheatcroft also says that, unlike Hitler, Stalin's \"purposive killings\" fit more closely into the category of \"execution\" than \"murder\", given he thought the accused were indeed guilty of crimes against the state and insisted on documentation, whereas Hitler simply wanted to kill Jews and communists because of who they were, and insisted on no documentation and was indifferent at even a pretence of legality for these actions.\n\nKristen R. Ghodsee, an ethnographer of post-Cold War Eastern Europe, contends that the efforts to institutionalize the \"double genocide thesis\", or the moral equivalence between the Nazi Holocaust (race murder) and the victims of communism (class murder), and in particular the recent push at the beginning of the global financial crisis for commemoration of the latter in Europe, can be seen as the response by economic and political elites to fears of a leftist resurgence in the face of devastated economies and extreme inequalities in both the East and West as the result of neoliberal capitalism. She notes that any discussion of the achievements under communism, including literacy, education, women’s rights, and social security is usually silenced, and any discourse on the subject of communism is focused almost exclusively on Stalin's crimes and the \"double genocide thesis\", an intellectual paradigm summed up as such: \"1) any move towards redistribution and away from a completely free market is seen as communist; 2) anything communist inevitably leads to class murder; and 3) class murder is the moral equivalent of the Holocaust.\" By linking all leftist and socialist ideals to the excesses of Stalinism, Ghodsee concludes, the elites in the West hope to discredit and marginalize all political ideologies that could \"threaten the primacy of private property and free markets.\"\n\nThe comparison of Stalinism and Nazism remains a neglected field of academic study.\n\nThe comparison of Nazism and Stalinism has long provoked political controversy, and it led to the historians' dispute within Germany in the 1980s.\n\nIn the 1920s, the Social Democratic Party of Germany (SPD), under the leadership of Chancellor Hermann Müller, adopted the view that \"red equals brown\", i.e. that the communists and Nazis posed an equal danger to liberal democracy. In 1930, Kurt Schumacher said that the two movements enabled each other. He argued that the Communist Party of Germany, which was staunchly Stalinist, were \"red-painted Nazis.\" This comparison was mirrored by the social fascism theory advanced by the Soviet government and the Comintern (including the Communist Party of Germany), which accused social democracy of enabling fascism and went as far as to call social democrats \"social fascists.\" After the 1939 Molotov–Ribbentrop Pact was announced, \"The New York Times\" published an editorial arguing that \"Hitlerism is brown communism, Stalinism is red fascism.\"\n\nMarxist theories of fascism have seen fascism as a form of reaction to socialism and a feature of capitalism. Several modern historians have tried to pay more attention to the economic, political and ideological differences between these two regimes than to their similarities. \n\nThe 2008 Prague Declaration on European Conscience and Communism, initiated by the Czech government and signed by figures such as Václav Havel, called for \"a common approach regarding crimes of totalitarian regimes, inter alia Communist regimes\" and for\nThe Communist Party of Greece opposes the Prague Declaration and has criticized \"the new escalation of the anti-communist hysteria led by the EU council, the European Commission and the political staff of the bourgeois class in the European Parliament.\" The Communist Party of Britain opined that the Prague Declaration \"is a rehash of the persistent attempts by reactionary historians to equate Soviet Communism and Hitlerite Fascism, echoing the old slanders of British authors George Orwell and Robert Conquest.\"\n\nThe 2008 documentary film \"The Soviet Story\", commissioned by the Union for Europe of the Nations group in the European Parliament, published archival records which listed thousands of German Jews who were arrested in the Soviet Union by the NKVD (People's Commissariat for Internal Affairs) from 1937 to 1941 and handed over to Gestapo or SS officials in Germany. These German Jews had originally sought asylum in the USSR. The documentary film accuses Stalin's regime of being an accomplice in Hitler's Holocaust by arresting these asylum seekers and sending them back to Germany.\n\nSince 2009, the European Union has officially commemorated the European Day of Remembrance for Victims of Stalinism and Nazism, proclaimed by the European Parliament in 2008 and endorsed by the Organization for Security and Co-operation in Europe in 2009, and officially known as the Black Ribbon Day in some countries (including Canada).\n\nThe former President of the European Parliament and Christian Democratic Union member, Hans-Gert Pöttering, argued that \"both totalitarian systems (Stalinism and Nazism) are comparable and terrible.\"\n\nIn some Eastern European countries the denial of both Nazi and Communist crimes has been explicitly outlawed, and Czech foreign minister Karel Schwarzenberg has argued that \"there is a fundamental concern here that totalitarian systems be measured by the same standard.\" However, the European Commission rejected calls for similar EU-wide legislation, due to the lack of consensus among member states.\n\nA statement adopted by Russia's legislature said that comparisons of Nazism and Stalinism are \"blasphemous towards all of the anti-fascist movement veterans, Holocaust victims, concentration camp prisoners and tens of millions of people ... who sacrificed their lives for the sake of the fight against the Nazis' anti-human racial theory.\"\n\nBritish journalist Seumas Milne posits that the impact of the post-Cold War narrative that Stalin and Hitler were twin evils, and therefore Communism is as monstrous as Nazism, \"has been to relativise the unique crimes of Nazism, bury those of colonialism and feed the idea that any attempt at radical social change will always lead to suffering, killing and failure.\"\n\n\n"}
{"id": "16759434", "url": "https://en.wikipedia.org/wiki?curid=16759434", "title": "Comparison of the Amundsen and Scott Expeditions", "text": "Comparison of the Amundsen and Scott Expeditions\n\nBetween December 1911 and January 1912, both Roald Amundsen (leading his South Pole expedition) and Robert Falcon Scott (leading the Terra Nova Expedition) reached the South Pole within a month of each other. But while Scott and his four companions died on the return journey, Amundsen's party managed to reach the geographic south pole first and subsequently return to their base camp at Framheim without loss of life, suggesting that they were better prepared for the expedition. The contrasting fates of the two teams seeking the same prize at the same time invites comparison.\n\nThe outcomes of the two expeditions were as follows.\n\nHistorically, several factors have been discussed and many contributing factors claimed, including:\n\nSullivan states that it was the last factor that probably was decisive. he states \"Man is a poor beast of burden, as was shown in the terrible experience of Scott, Shackleton, and Wilson in their thrust to the south of 1902–3. However, Scott relied chiefly on man-hauling in 1911–12 because ponies could not ascend the glacier midway to the Pole. The Norwegians correctly estimated that dog teams could go all the way. Furthermore, they used a simple plan, based on their native skill with skis and on dog-driving methods that were tried and true. In a similar fashion to the way the moon was reached by expending a succession of rocket stages and then casting each aside; the Norwegians used the same strategy, sacrificing the weaker animals along the journey to feed the other animals and the men themselves.\"\n\nScott and his financial backers saw the expedition as having a scientific basis, while also wishing to reach the pole. However, it was recognised by all involved that the South Pole was the primary objective (\"The Southern Journey involves the most important object of the Expedition\" – Scott), and had priority in terms of resources, such as the best ponies and all the dogs and motor sledges as well as involvement of the vast majority of the expedition personnel. Scott and his team knew the expedition would be judged on his attainment of the pole (\"The ... public will gauge the result of the scientific work of the expedition largely in accordance with the success or failure of the main object\" – Scott). He was prepared to make a second attempt the following year (1912–13) if this attempt failed and had Indian Army mules and additional dogs delivered in anticipation. In fact the mules were used by the team that discovered the dead bodies of Scott, Henry Robertson Bowers, and Edward Adrian Wilson in November 1912, but proved even less useful than the ponies, according to Cherry-Garrard.\n\nAmundsen's expedition was planned to reach the South Pole. This was a plan he conceived in 1909. Amundsen's expedition did conduct geographical work under Kristian Prestrud who conducted an expedition to King Edward VII Land while Amundsen was undertaking his attempt at the pole.\n\nAmundsen camped on the Ross Ice Shelf at the Bay of Whales which is 60 miles (96 km) closer to the pole than Scott's camp (which was 350 miles west of Amundsen). Amundsen had deduced that, as the Trans-Antarctic Mountains ran northwest to southeast then if he were to meet a mountain range on his route then the time spent at the high altitude of the Antarctic plateau would be less than Scott's.\nScott's base was at Cape Evans on Ross Island, with access to the Trans-Antarctic mountain range to the west, and was a better base for geological exploration. He had based his previous expedition in the same area. However, he knew it to be poor as a route to the pole as he had to start before sea ice melted and had suffered delay in returning while waiting for the sea ice to freeze. They also had to make detours around Ross Island and its known crevassed areas which meant a longer journey. The crossing of the Ross Ice Shelf was an onerous task for the ponies. Scott had advanced considerable stores across the ice shelf the year before to allow the ponies to carry lighter loads over the early passage across the ice. Even so, he had to delay the departure of the ponies until 1 November rather than 24 October when the dogs and motor sledges set off.\nConsequently, the Motor Party spent 6 days at the Mount Hooper Depot waiting for Scott to arrive.\n\nThe major comparison between Scott and Amundsen has focused on the choice of draft transport —dog versus pony/man-hauling. In fact Scott took dogs, ponies and three \"motor sledges\". Scott spent nearly seven times the amount of money on his motor sledges than on the dogs and horses combined. They were therefore a vital part of the expedition. Unfortunately, Scott decided to leave behind the engineer, Lieutenant Commander Reginald William Skelton who had created and trialled the motor sledges. This was due to the selection of Lieutenant E.R.G.R. \"Teddy\" Evans as the expedition's second in command. As Evans was junior in rank to Skelton, he insisted that Skelton could not come on the expedition. Scott agreed to this request and Skelton's experience and knowledge was lost. One of the original three motor sledges was a failure even before the expedition set out; the heavy sledge was lost through thin ice on unloading it from the ship. The two remaining motor sledges failed relatively early in the main expedition because of repeated faults. Skelton's experience might have been valuable in overcoming the failures.\n\nScott had used dogs on his first (Discovery) expedition and felt they had failed. On that journey, Scott, Shackleton, and Wilson started with three sledges and 13 dogs. But on that expedition, the men had not properly understood how to travel on snow with the use of dogs. The party had skis but were too inexperienced to make good use of them. As a result, the dogs travelled so fast that the men could not keep up with them. The Discovery expedition had to increase their loads to slow the dogs down. Additionally, the dogs were fed Norwegian dried fish, which did not agree with them and soon they began to deteriorate. The whole team of dogs eventually died (and were eaten), and the men took over hauling the sleds.\n\nScott's opinion was reinforced by Shackleton's experience on his Nimrod expedition that got to within of the pole. Shackleton used ponies. Scott planned to use ponies only to the base of the Beardmore Glacier (one-quarter of the total journey) and man-haul the rest of the journey. Scott's team had developed snow shoes for his ponies, and trials showed they could significantly increase daily progress. However, Lawrence Oates, whom Scott had made responsible for the ponies, was reluctant to use the snow shoes and Scott failed to insist on their use.\n\nThere was plenty of evidence that dogs could succeed in the achievements of William Speirs Bruce in his Arctic, Antarctic, and Scottish National Antarctic Expedition, Amundsen in the \"Gjøa\" North West passage expedition, Fridtjof Nansen's crossing of Greenland, Robert Peary's three attempts at the North Pole, Eivind Astrup's work supporting Peary, Frederick Cook's discredited North Pole expedition, and Otto Sverdrup's explorations of Ellesmere Island. Moreover, Scott ignored the direct advice he received (while attending trials of the motor sledges in Norway) from Nansen, the most famous explorer of the day, who told Scott to take \"dogs, dogs and more dogs\".\n\nAt the time of the events, the expert view in England had been that dogs were of dubious value as a means of Antarctic transport. Broadly speaking, Scott saw two ways in which dogs may be used—they may be taken with the idea of bringing them all back safe and sound, or they may be treated as pawns in the game, from which the best value is to be got regardless of their lives. He stated that if, and only if, the comparison was made with a dog sledge journey which aimed to preserve the dogs' lives, 'I am inclined to state my belief that in the polar regions properly organised parties of men will perform as extended journeys as teams of dogs.' On the other hand, if the lives of the dogs were to be sacrificed, then 'the dog-team is invested with a capacity for work which is beyond the emulation of men. To appreciate this is a matter of simple arithmetic'. But efficiency notwithstanding, he expressed \"reluctance\" to use dogs in this way: \"One cannot calmly contemplate the murder of animals which possess such intelligence and individuality, which have frequently such endearing qualities, and which very possibly one has learnt to regard as friends and companions.\"\n\nAmundsen, by contrast, took an entirely utilitarian approach. Amundsen planned from the start to have weaker animals killed to feed the other animals and the men themselves. He expressed the opinion that it was less cruel to feed and work dogs correctly before shooting them, than it would be to starve and overwork them to the point of collapse. Amundsen and his team had similar affection for their dogs as those expressed above by the English, but they \"also had agreed to shrink from nothing in order to achieve our goal\". The British thought such a procedure was distasteful, though they were willing to eat their ponies.\n\nAmundsen had used the opportunity of learning from the Inuit while on his \"Gjøa\" North West passage expedition of 1905. He recruited experienced dog drivers. To make the most of the dogs he paced them and deliberately kept daily mileages shorter than he need have for 75 percent of the journey, and his team spent up to 16 hours a day resting. His dogs could eat seals and penguins hunted in the Antarctic while Scott's pony fodder had to be brought all the way from England in their ship. It has been later shown that seal meat with the blubber attached is the ideal food for a sledge dog. Amundsen went with 52 dogs, and came back with 11.\n\nWhat Scott did not realise is a sledge dog, if it is to do the same work as a man, will require the same amount of food. Furthermore, when sledge dogs are given insufficient food they become difficult to handle. The advantage of the sledge dog is its greater mobility. Not only were the Norwegians accustomed to skiing, which enabled them to keep up with their dogs, but they also understood how to feed them and not overwork them.\n\nScott took the Norwegian pilot and skier Tryggve Gran to the Antarctic on the recommendation of Nansen to train his expedition to ski, but although a few of his party began to learn, he made no arrangements for compulsory training for the full party. Gran (possibly because he was Norwegian) was not included in the South Pole party, which could have made a difference. Gran was, one year later, the first to locate the deceased Scott and his remaining companions in their tent just some 18 km (11 miles) short of One Ton depot, that might have saved their lives had they reached it.\n\nScott would subsequently complain in his diary, while well into his journey and therefore too late to take any corrective action and after over 10 years since the Discovery expedition, that \"Skis are the thing, and here are my tiresome fellow countrymen too prejudiced to have prepared themselves for the event\".\n\nAmundsen on his side recruited a team of well experienced skiers, all Norwegians who had skied from an early age. He also recruited a champion skier, Olav Bjaaland, as the front runner. The Amundsen party gained weight on their return travel from the South Pole.\n\nScott and Shackleton's experience in 1903 and 1907 gave them first-hand experience of average conditions in Antarctica. Simpson, Scott's meteorologist 1910–1912, charted the weather during their expedition, often taking two readings a day. On their return to the Ross Ice Shelf, Scott's group experienced prolonged low temperatures from 27 February until 10 March which have only been matched once in 15 years of current records. The exceptional severity of the weather meant they failed to make the daily distances they needed to get to the next depot. This was a serious position as they were short of fuel and food. When Scott, Wilson, and Bowers died (Petty Officer Edgar Evans and Lawrence Oates had died earlier during the return from the South Pole) they were short of One-Ton Depot, which was from Corner Camp, where they would have been safe.\n\nOn the other hand, Cherry-Garrard had travelled nearly in the same area, during the same time period and same temperatures, using a dog team. Scott also blamed \"a prolonged blizzard\". But while there is evidence to support the low temperatures, there is only evidence for a \"normal\" two- to four-day blizzard, and not the ten days that Scott claims.\n\nDuring depot laying in February 1911, Roald Amundsen had his first (and last) of his route marked like a Norwegian ski course using marker flags initially every eight miles. He added to this by using food containers painted black, resulting in a marker every mile. From 82 degrees on, Amundsen built a cairn every three miles with a note inside recording the cairn's position, the distance to the next depot, and direction to the next cairn. In order not to miss a depot considering the snow and great distances, Amundsen took precautions. Each depot laid out up to 85 degrees (laid out every degree of latitude) had a line of bamboo flags laid out transversely every half-mile for five miles on either side of the depot, ensuring that the returning party could locate the designated depot.\n\nScott relied on depots much less frequently laid out. For one distance where Amundsen laid seven depots, Scott laid only two. Routes were marked by the walls made at lunch and evening stops to protect the ponies. Depots had a single flag. As a result, Scott has much concern recorded in his diaries over route finding, and experienced close calls about finding depots. It is also clear that Scott's team did not travel on several days, because the swirling snow hid their three-month-old outward tracks. With better depot and route marking they would have been able to travel on more days with a following wind which would have filled the sail attached to their sledge, and so travel further, and might have reached safety.\n\nBy the time they arrived at the pole, the health of Scott's team had significantly deteriorated, whereas Amundsen's team actually gained weight during the expedition. While Scott's team managed to maintain the scheduled pace for most of the return leg, and hence was virtually always on full rations, their condition continued to worsen rapidly. (The only delay occurred when they were held for four days by a blizzard, and had to open their summit rations early as a consequence.)\n\nApsley Cherry-Garrard in his analysis of the expedition estimated that even under optimistic assumptions the summit rations contained only a little more than half the calories actually required for the man-hauling of sledges. A carefully planned 2006 re-enactment of both Amundsen's and Scott's travels, sponsored by the BBC, confirmed Cherry-Garrard's theory. The British team had to abort their tour due to the severe weight loss of all members. The experts hinted that Scott's reports of unusually bad surfaces and weather conditions might in part have been due to their exhausted state which made them feel the sledge weights and the chill more severely.\n\nScott's calculations for the supply requirements were based on a number of expeditions, both by members of his team (e.g., Wilson's trip with Cherry-Garrard and Bowers to the Emperor penguin colony which had each man on a different type of experimental ration), and by Shackleton. Apparently, Scott didn't take the strain of prolonged man-hauling at high altitudes sufficiently into account.\n\nSince the rations contained no B and C vitamins, the only source of these vitamins during the trek was from the slaughter of ponies or dogs. This made the men progressively malnourished, manifested most clearly in the form of scurvy.\n\nScott also had to fight with a shortage of fuel due to leakage from stored fuel cans which used leather washers. This was a phenomenon that had been noticed previously by other expeditions, but Scott took no measures to prevent it. Amundsen, in contrast, had learned the lesson and had his fuel cans soldered closed. A fuel depot he left on Betty's Knoll was found 50 years later still full.\n\nDehydration may also have been a factor. Amundsen's team had plenty of fuel due to better planning and soldered fuel cans. Scott had a shortage of fuel and was unable to melt as much water as Amundsen. At the same time Scott's team were more physically active in man-hauling the sledges.\n\nIt has been said (by the present-day explorer Ranulph Fiennes amongst others) that Scott's team was appropriately dressed for man-hauling in their woolen and wind-proof clothing, and as Amundsen was skiing it was appropriate he wore furs. Skiing at the pace of a dog team is a strenuous activity. Yet Amundsen never complained about the clothing being too hot. That is because the furs are worn loosely so air circulates and sweat evaporates. Scott's team, on the other hand, made regular complaints about the cold.\n\nAmundsen's team did initially have problems with their boots. However, the depot-laying trips of January and February 1911 and an abortive departure to the South Pole on 8 September 1911 allowed changes to be made before it was too late.\n\nScott's team suffered regularly from snow blindness and sometimes this affected over half the team at any one time. By contrast, there was no recorded case of snow blindness during the whole of Amundsen's expedition. On the return journey, Amundsen's team rested during the \"day\" (when the sun was in front of them) and travelled during the \"night\" (when the sun was behind them) to minimise the effects of snow blindness.\n\nIn 1921, 'Teddy' Evans wrote in his book \"South with Scott\" that Scott had left the following written orders at Cape Evans.\n\nHe did however place a lesser importance upon this journey than that of replenishing the food rations at One Ton Depot.\n\nHe continued his instructions in the next paragraph \"You will of course understand that whilst the object of your third journey is important, that of the second is vital. At all hazards three X.S. units of provision must be got to One Ton Camp by the date named (19th January), and if the dogs are unable to perform this task, a man party must be organised.\" with that qualification he closed his notes regarding his instructions for the dogs.\n\nExpedition member Apsley Cherry-Garrard did not mention Scott's order in his 1922 book \"The Worst Journey in the World\". However, in the 1948 preface to his book, he discusses Scott's order. Cherry-Garrard writes that he and Edward Atkinson reached Cape Evans on 28 January. Scott had estimated Atkinson would reach camp by 13 January. Atkinson, now the senior officer discovered that the dog handler Cecil Meares had resigned from the expedition and that neither Meares nor anyone else had resupplied dog food to the depots. Cherry-Garrard also wrote \"In my opinion he [Atkinson] would not have been fit to take out the dogs in the first week of February\".\n\nOn 13 February, Atkinson set off on the first lap southwards to Hut Point with the dog assistant, Dimitri Gerov, and the dogs to avoid being cut off by disintegrating sea ice. Atkinson and Gerov were still at Hut Point when, on 19 February, Tom Crean arrived on foot from the Barrier and reported that Lt Edward Evans was lying seriously ill in a tent some to the south, and in urgent need of rescue. Atkinson decided that this mission was his priority, and set out with the dogs to bring Evans back. This was achieved; the party was back at Hut Point on 22 February.\n\nAtkinson sent a note back to the Cape Evans base camp requesting either the meteorologist Wright or Cherry-Garrard to take over the task of meeting Scott with the dogs. Chief meteorologist Simpson was unwilling to release Wright from his scientific work, and Atkinson therefore selected Apsley Cherry-Garrard. It was still not in Atkinson's mind that Cherry-Garrard's was a relief mission, and according to Cherry-Garrard's account, told him to \"use his judgement\" as to what to do in the event of not meeting the polar party by One Ton, and that Scott's orders were that the dogs must not be risked. Cherry-Garrard left with Gerov and the dogs on 26 February, carrying extra rations for the polar party to be added to the depot and 24 days' of dog food. They arrived at One Ton Depot on 4 March and did not proceed further south. Instead, he and Gerov, after waiting there for Scott for several days, apparently mostly in blizzard conditions (although no blizzard was recorded by Scott some 100 miles further south until 10 March), they returned to Hut Point on 16 March, in poor physical condition and without news of the polar party.\n\nOn the return journey from the pole, Scott reached the 82.30°S meeting point for the dog teams three days ahead of schedule, around 27 February 1912. Scott's diary for that day notes \"We are naturally always discussing possibility of meeting dogs, where and when, etc. It is a critical position. We may find ourselves in safety at the next depot, but there is a horrid element of doubt.\" By 10 March it became clear that the dog teams were not coming: \"The dogs which would have been our salvation have evidently failed. Meares [the dog-driver] had a bad trip home I suppose. It's a miserable jumble.\"\n\nAround 25 March, awaiting death in his tent at latitude 79.30°S, Scott speculated, in a farewell letter to his expedition treasurer Sir Edgar Speyer, that he had overshot the meeting point with the dog relief teams, writing \"We very nearly came through, and it's a pity to have missed it, but lately I have felt that we have overshot our mark. No-one is to blame and I hope no attempt will be made to suggest that we had lacked support.\" (Farewell letter to Sir Edgar Speyer, cited from Karen May 2012.)\n\n"}
{"id": "46842976", "url": "https://en.wikipedia.org/wiki?curid=46842976", "title": "Diels–Kranz numbering", "text": "Diels–Kranz numbering\n\nDiels–Kranz (DK) numbering is the standard system for referencing the works of the ancient Greek pre-Socratic philosophers, based on the collection of quotations from and reports of their work, \"Die Fragmente der Vorsokratiker\" (The Fragments of the Pre-Socratics), by Hermann Alexander Diels. The \"Fragmente\" was first published in 1903, was later revised and expanded three times by Diels, and was finally revised in a fifth edition (1934–7) by Walther Kranz and again in a sixth edition (1952). In Diels-Kranz, each passage, or item, is assigned a number which is used to uniquely identify the ancient personality with which it is concerned, and the type of item given. Diels-Kranz is used in academia to cite pre-Socratic philosophers, and the system also encompasses Sophists and pre-Homeric poets such as Orpheus.\n\nStephanus pagination is the comparable system for referring to Plato, and Bekker numbering is the comparable system for referring to Aristotle.\n\nThe works of the pre-Socratics have not survived extant to the present day. Our knowledge of them exists only through references in the works of later philosophers (known as doxography) in the form of quotations and paraphrases. For example, our knowledge of Thales of Miletus comes largely from the works of Aristotle, who lived centuries after him. Another interesting example of such a source is Hippolytus of Rome, whose polemic \"Refutation of All Heresies\" is a source of many direct quotations of Heraclitus as well as of other philosophers, thereby perpetuating the work of those he was refuting.\n\nThese quotations, paraphrases and other references to pre-Socratic philosophers were collected by Diels and Kranz in their book, which became a standard text in modern pre-Socratic education and scholarship. Because of its influence, Diels-Kranz numbering became the standard way of referencing the material: in literature, conferences, and even in conversation.\n\nThe number corresponding to an item was made up of three parts:\n\n\nWhy, take the case of Thales, Theodorus. While he was studying the stars and looking upwards, he fell into a pit, and a neat, witty Thracian servant girl jeered at him, they say, because he was so eager to know the things in the sky that he could not see what was there before him at his very feet.\n\nThe above text has a DK number of 11A9, since it refers to Thales who is, as mentioned above, chapter 11's subject. The source is \"Theaetetus\" (one of Plato's dialogues), and gives an account of Thales' life, hence it is a \"testimonium\", represented by the letter \"A\". Finally, it is the ninth item in its chapter, giving it the overall number of DK 11A9.\n\nSometimes, the chapter (personality) number may simply be replaced by the name, which can be helpful in cases where the former is the same as the passage number, to avoid ambiguity. For example:\n\nThose who seek for gold dig up much earth and find a little.\n\nRather than \"22B22\" the above may also instead be referred to as \"Heraclitus B22\" as it is a direct transmission of the words of Heraclitus (thus, B) and is the 22nd item in the chapter about Heraclitus (whose chapter number is also 22) in the \"Fragmente\".\n\nThe following table gives the Diels-Kranz numbering of Pre-Socratic philosophers. Note that the numbering scheme presented is that of the fifth edition of \"Die Fragmente der Vorsokratiker\", the first to be revised by Kranz. The fifth edition's numbering is the scheme which has since gained the most traction in modern Pre-Socratic scholarship, and it is the one used consistently throughout this article. It should not be confused with the numberings given in other versions, which changed frequently depending on the particular edition of the \"Fragmente\".\n\nMost entries (78) are concerned with a single, named individual, while the remaining minority of entries (12) have more complex context. Of these latter, eight (10, 19, 39, 46, 53-56) are each concerned with groups of named personalties, who typically have a clear relationship of some kind to justify their association in each entry. Two entries (58, 79) are devoted not to individuals, but to schools of thought (Pythagoreanism and Sophism), and the last two (89, 90) reproduce contemporaneous anonymous texts. Although \"the Seven Sages of Greece\" implies a clearly defined set of seven people, historical disagreement renders intractable the problem of exactly who they were, with multiple sources suggesting several different candidates. If one takes the Seven Sages as a group of seven and includes the later Iamblichus, Diels-Kranz encompasses 106 named personalities and two anonymous authors. The chapter on Sophism is concerned with the named sophists who take up most of the rest of the scheme, and \nper Freeman with regard to the chapter on Pythagoreanism, a catalogue due to Iamblichus lists 218 named men and 17 named women as Pythagoreans, along with other probable, anonymous adherents.\n\nIn several cases, the personalities listed are so obscure that they are merely mentioned by name in other sources, commonly with hints as to their geographical and philosophical associations, and without even surviving \"paraphrases\" of any of their ideas, or what they might have written. That is, these more obscure personalities survive in the historical record only as names cited by others, and so came to be included in Diels-Kranz for the sake of scholarly completeness.\n\n\n"}
{"id": "1902180", "url": "https://en.wikipedia.org/wiki?curid=1902180", "title": "Digital reference", "text": "Digital reference\n\nDigital reference (or virtual reference) is a service by which a library reference service is conducted online, and the reference transaction is a computer-mediated communication. It is the remote, NextNextcomputer-mediated delivery of reference information provided by library professionals to users who cannot access or do not want face-to-face communication. Virtual reference service is most often an extension of a library's existing reference service program. The word \"reference\" in this context refers to the task of providing assistance to library users in finding information, answering questions, and otherwise fulfilling users’ information needs. Reference work often but not always involves using reference works, such as dictionaries, encyclopedias, etc. This form of reference work expands reference services from the physical reference desk to a \"virtual\" reference desk where the patron could be writing from home, work or a variety of other locations.\n\nThe terminology surrounding virtual reference services may involve multiple terms used for the same definition. The preferred term for remotely delivered, computer-mediated reference services is \"virtual reference\", with the secondary non-preferred term \"digital reference\" having gone out of use in recent years. \"Chat reference\" is often used interchangeably with virtual reference, although it represents only one aspect of virtual reference. Virtual reference includes the use of both synchronous (i.e., IM, videoconferencing) and asynchronous communication (i.e., texting and email). Here, \"synchronous virtual reference\" refers to any real-time computer-mediated communication between patron and information professional. Asynchronous virtual reference is all computer-mediated communication that is sent and received at different times.\n\nThe earliest digital reference services were launched in the mid-1980s, primarily by academic and medical libraries, and provided by e-mail. These early-adopter libraries launched digital reference services for two main reasons: to extend the hours that questions could be submitted to the reference desk, and to explore the potential of campus-wide networks, which at that time was a new technology.\n\nWith the advent of the graphical World Wide Web, libraries quickly adopted webforms for question submission. Since then, the percentage of questions submitted to services via webforms has outstripped the percentage submitted via email.\n\nIn the early- to mid-1990s, digital reference services began to appear that were not affiliated with any library. These digital reference services are often referred to as \"AskA\" services. Examples of AskA services are the Internet Public Library, Ask Dr. Math, and Ask Joan of Art.\n\nProviding remote-based services for patrons has been a steady practice of libraries over the years. For example, before the widespread use of chat software, reference questions were often answered via phone, fax, email and audio conferencing. Email is the oldest type of virtual reference service used by libraries. Library services in America and the UK are just now gaining visibility in their use of virtual reference services using chat software. However, a survey in America revealed that by 2001 over 200 libraries were using chat reference services. \nThe rapid global proliferation of information technology (IT) often leaves libraries at a disadvantage in terms of keeping their services current. However, libraries are always striving to understand their user demographics in order to provide the best possible services. Therefore, libraries continue to take notes from current cyberculture and are continually incorporating a diversified range of interactive technologies in their service repertoires. Virtual reference represents only one small part of a larger library mission to meet the needs of a new generation, sometimes referred to as the \"Google Generation\", of users who have grown up with the internet. For instance, virtual reference may be used in conjunction with embedded Web 2.0 (online social media such as Facebook, YouTube, blogs, del.icio.us, Flickr, etc.) applications in a library's suite of online services. As technological innovations continue, libraries will be watching to find new, more personalized ways of interacting with remote reference users.\n\nThe range of cost-per-transaction of reference interactions has been found to be large, due to the differences in librarian salaries and infrastructural costs required by reference interviews.\n\nWebforms are created for digital reference services in order to help the patron be more productive in asking their question. This document helps the librarian locate exactly what the patron is asking for. Creation of webforms requires design consideration. Because webforms substitute for the reference interview, receiving as much information as possible from the patron is a key function.\n\nAspects commonly found within webforms:\n\n\nSeveral applications exist for providing chat-based reference. Some of these applications are: QuestionPoint, OmniReference, Tutor.com, LibraryH3lp, AspiringKidz.com, and Vienova.com. These applications bear a resemblance to commercial help desk applications. These applications possess functionality such as: chat, co-browsing of webpages, webpage and document pushing, customization of pre-scripted messages, storage of chat transcripts, and statistical reporting.\n\nInstant messaging (IM) services are used by some libraries as a low-cost means of offering chat-based reference, since most IM services are free. Utilizing IM for reference services allows a patron to contact the library from any location via the internet. This service is like the traditional reference interview because it is a live interaction between the patron and the librarian. On the other side the reference interview is different because the conversation does not float away but instead is in print on the screen for the librarian to review if needed to better understand the patron. IM reference services may be for the use of in-house patrons as well as patrons unable to go to the library. If library computers support IM chat programs, patrons may IM from within the library to avoid losing their use of a computer or avoid making embarrassing questions public.\n\nSuccessful IM reference services will:\n\nAt times, IM becomes challenging because of lack of non-verbal cues such as eye contact, and the perceived time pressure. Moreover, formulating the question online without the give and take of nonverbal cues and face to face conversation presents an added obstacle. In addition, to provide effective reference service through IM, it is important to meet higher level of information literacy standards. These standards include evaluating the information and its source, synthesizing the information to create new ideas or products, and understanding the societal, legal, and economic issues surrounding its use.\n\nThe article Live, Digital Reference Marketplace by Buff Hirko contains a comparison of the features of applications for chat-based reference.\n\nSee the entries in the Library Success Wiki's Online Reference Section, including software recommended for web-based chat reference, IM reference, SMS (text messaging) reference, and other types like digital audio or video reference.\n\nVirtual service software programs offered by libraries are often unique, and tailored to the individual library's needs. However, each program may have several distinct features. A knowledge base is a chunk of information that users can access independently. An example of this is a serialized listing of frequently asked questions (FAQ) that a user can read and use at his or her leisure.\n\nOnline chat, or instant messaging (IM) has become a very popular Web-based feature. Instant messaging is a real time conversation that utilizes typed text instead of language. Users may feel a sense of satisfaction with the use of this tool because of their personalized interaction with staff.\n\nThe use of electronic mail (email) in responding to reference questions in libraries has been in use for years. Also, in some cases with the IM feature, a question may be asked that cannot be resolved in online chat. In this instance the staff member may document the inquiring patron’s email address and will the user a response.\n\nWith the increase in use of text messaging (Short Message Service or SMS), some libraries are also adopting text messaging in their virtual reference services. Librarians can use mobile phones, text-to-instant messaging or web-based services to respond to reference questions via text messaging.\n\nCo-browsing, or cooperative browsing, is a virtual reference function that involves interactive control of a user’s web browser. This function enables the librarian to see what the patron has on his or her computer screen. Several types of co-browsing have been offered in mobile devices of late; libraries may have software that incorporates dual modes of co-browsing in a variety of formats. For instance, it is possible to browse on a mobile device within and between documents (such as Word), webpages, and images.\n\nVirtual reference services are growing in popularity in the UK with more institutions accepting queries via email, instant messaging and other chat based services. A study of the use of virtual reference within UK academic institutions showed that 25% currently offer a form of virtual reference, with 54% of academic institutions surveyed considering adding this service.\n\nUK public libraries were instrumental in some of the first steps towards UK-wide internet collaboration amongst libraries with the EARL Consortium (Electronic Access to Resources in Libraries) in 1995, in a time where internet access was a rare commodity for both library staff and the public. Resources were collated and lines of communication opened between libraries across the UK, paving the way for services all over the world to follow suit. There are now a number of area-specific reference services across the UK including Ask A Librarian (UK-wide, established in 1997), Ask Cymru (Welsh and English language service), Enquire (Government funded through the People's Network, also UK-wide), and Ask Scotland. Ask Scotland was created by the Scottish Government's advisory body on libraries, SLIC (Scottish Library and Information Council), and funded by the Public Library Quality Improvement Fund (PLQIF) in June 2009. It uses the Online Computer Library Center's QuestionPoint software.\n\nThe definition formulated by the American Library Association's (ALA) 2004 MARS Digital Reference Guidelines Ad Hoc Committee contains three components:\n\n\nIn January 2011 QuestionPoint and the American Library Association were in talks about offering a National Ask A Librarian service across the whole United States of America. At present the Ask services in the US are run at a local level.\n\nIn Europe some countries offer services in both their own national language and in English. European countries include: Finland, the Netherlands (in Dutch only), Denmark, and France.\n\nOther countries which offer virtual reference services include: Australia, New Zealand, Canada, and the state of Colorado in the United States.\n\nA collaboration between UK and Australian library services, entitled Chasing the Sun, has been initiated using QuestionPoint software so that an all-hours digital reference chat service can be offered. Targeted at health libraries where reference queries from health professionals could occur at any time of the day or night due to medical emergencies, the collaboration between the two countries means that someone will be on hand to field the query at any time. Although the UK libraries involved are currently based in England the programme may expand to other countries and health services if successful.\n\n\n\n\nThe following provide software and technology infrastructure for digital/virtual reference.\n\n\n\n\n\n"}
{"id": "58632079", "url": "https://en.wikipedia.org/wiki?curid=58632079", "title": "Encyclopedia of Forensic and Legal Medicine 2nd Edition", "text": "Encyclopedia of Forensic and Legal Medicine 2nd Edition\n\nThe Encyclopedia of Forensic and Legal Medicine 2nd Edition is a reference source and pioneering 4 set encyclopedia of forensics and medico-legal knowledge published by Academic Press, Elsevier in 2016. This has been edited by the renowned British forensic specialist Jason Payne-James and Australian forensic pathologist Roger W. Byard and an international editorial board. \nThis reference work includes more than 300 articles contributed by forensic medicine and forensic science experts from all over the world. The encyclopedia is a complete reference source of articles covering from forensics, criminal investigations, health-care, legal, judicial, ballistics, toxicology,fingerprinting, DNA typing, disaster victim identification to autopsy and postmortem examination.\n\nThe encyclopedia is especially meant for forensic, medical, chemistry, physics, laboratory technologists and anthropology students and specialists such as forensic experts, lawyers, judicial officers, judges, police and investigating offices, nurses, medical officers etc. All the articles of the encyclopedia are available through Science direct and Scopus.\n"}
{"id": "2330597", "url": "https://en.wikipedia.org/wiki?curid=2330597", "title": "Errors and omissions excepted", "text": "Errors and omissions excepted\n\nErrors and omissions excepted (E&OE) is a phrase used in an attempt to reduce legal liability for potentially incorrect or incomplete information supplied in a contractually related document such as a quotation or specification.\n\nIt is often applied as a disclaimer in situations in which the information to which it is applied is relatively fast-moving. In legal terms, it seeks to make a statement that information cannot be relied upon, or may have changed by the time of use.\n\nIt is regularly used in accounting, to \"excuse slight mistakes or oversights.\"\n\nIt is also used when a large amount of information is listed against a product, to state that—to the best of the supplier's knowledge—the information is correct, but that they will not be held responsible if an error has been committed.\n"}
{"id": "500948", "url": "https://en.wikipedia.org/wiki?curid=500948", "title": "Field guide", "text": "Field guide\n\nA field guide is a book designed to help the reader identify wildlife (plants or animals) or other objects of natural occurrence (e.g. minerals). It is generally designed to be brought into the 'field' or local area where such objects exist to help distinguish between similar objects. Field guides are often designed to help users distinguish animals and plants that may be similar in appearance but are not necessarily closely related.\n\nIt will typically include a description of the objects covered, together with paintings or photographs and an index. More serious and scientific field identification books, including those intended for students, will probably include identification keys to assist with identification, but the publicly accessible field guide is more often a browsable picture guide organized by family, colour, shape, location or other descriptors.\n\nPopular interests in identifying things in nature probably were strongest in bird and plant guides. Perhaps the first popular field guide to plants in the United States was the 1893 \"How to Know the Wildflowers\" by \"Mrs. William Starr Dana\" (Frances Theodora Parsons). In 1890, Florence Merriam published \"Birds Through an Opera-Glass\", describing 70 common species. Focused on living birds observed in the field, the book is considered the first in the tradition of modern, illustrated bird guides. In 1902, now writing as Florence Merriam Bailey (having married the zoologist Vernon Bailey), she published \"Handbook of Birds of the Western United States\". By contrast, the \"Handbook\" is designed as a comprehensive reference for the lab rather a portable book for the field. It was arranged by taxonomic order and had clear descriptions of species size, distribution, feeding, and nesting habits.\n\nFrom this point into the 1930s, features of field guides were introduced by Chester A. Reed and others such as changing the size of the book to fit the pocket, including colour plates, and producing guides in uniform editions that covered subjects such as garden and woodland flowers, mushrooms, insects, and dogs.\n\nIn 1934, Roger Tory Peterson, using his fine skill as an artist, changed the way modern field guides approached identification. Using color plates with paintings of similar species together – and marked with arrows showing the differences – people could use his bird guide in the field to compare species quickly to make identification easier. This technique, the \"Peterson Identification System\", was used in most of Peterson's Field Guides from animal tracks to seashells and has been widely adopted by other publishers and authors as well.\n\nToday, each field guide has its own range, focus and organization. Specialist publishers such as Croom Helm, along with organisations like the Audubon Society, the RSPB, the Field Studies Council, National Geographic, HarperCollins, and many others all produce quality field guides.\n\nIt is somewhat difficult to generalise about how field guides are intended to be used, because this varies from one guide to another, partly depending on how expert the targeted reader is expected to be.\n\nFor general public use, the main function of a field guide is to help the reader identify a bird, plant, rock, butterfly or other natural object down to at least the popular naming level. To this end some field guides employ simple keys and other techniques: the reader is usually encouraged to scan illustrations looking for a match, and to compare similar-looking choices using information on their differences. Guides are often designed to first lead readers to the appropriate section of the book, where the choices are not so overwhelming in number.\n\nGuides for students often introduce the concept of identification keys. Plant field guides such as \"Newcomb's Wildflower Guide\" (which is limited in scope to the wildflowers of northeastern North America) frequently have an abbreviated key that helps limit the search. Insect guides tend to limit identification to Order or Family levels rather than individual species, due to their diversity.\n\nMany taxa show variability and it is often difficult to capture the constant features using a small number of photographs. Illustrations by artists or post processing of photographs help in emphasising specific features needed to for reliable identification. Peterson introduced the idea of lines to point to these key features. He also noted the advantages of illustrations over photographs:\n\nField guides aid in improving the state of knowledge of various taxa. By making the knowledge of experienced museum specialists available to amateurs, they increase the gathering of information by amateurs from a wider geographic area and increasing the communication of these findings to the specialists.\n\n"}
{"id": "1720724", "url": "https://en.wikipedia.org/wiki?curid=1720724", "title": "Fumblerules", "text": "Fumblerules\n\nA fumblerule is a rule of language or linguistic style, humorously written in such a way that it breaks this rule. Fumblerules are a form of self-reference.\n\nThe science editor George L. Trigg published a list of such rules in 1979. The term \"fumblerules\" was coined in a list of such rules compiled by William Safire on Sunday, 4 November 1979, in his column \"On Language\" in the \"New York Times\". Safire later authored a book titled \"Fumblerules: A Lighthearted Guide to Grammar and Good Usage\", which was reprinted in 2005 as \"\".\n\n\n\n"}
{"id": "33487458", "url": "https://en.wikipedia.org/wiki?curid=33487458", "title": "Guide to information sources", "text": "Guide to information sources\n\nA Guide to information sources (or a bibliographic guide, a literature guide, a guide to reference materials, a subject gateway, etc.) is a kind of metabibliography. Ideally it is not just a listing of bibliographies, reference works and other information sources, but more like a textbook introducing users to the information sources in a given field (in general).\n\nSuch guides may have many different forms: Comprehensive or highly selective, printed or electronic sources, annoteted listings or written chapters etc.\n\nOften used as curriculum tools for bibliographic instruction, the guides help library users find materials or help those unfamiliar with a discipline understand the key sources.\n\nAby, Stephen H., Nalen, James & Fielding, Lori (2005). Sociology; a guide to reference and information sources. 3rd ed. Westport, Conn.: Libraries Unlimited.\n\nAdams, Stephen R. (2005). \"Information Sources in Patents\"; 2nd ed. (Guides to Information Sources). München: K. G. Saur \n\nBlewett, Daniel K (2008). American military history; a guide to reference and information sources. 2nd ed. Westport, CT : Libraries Unlimited.\n\nJacoby, JoAnn & Kibbee, Josephine Z. (2007). Cultural anthropology; a guide to reference and information sources. 2nd ed. Westport, Conn.: Libraries Unlimited.\n\nSchmidt, Diane & Bell, George H. (2003). Guide to reference and information sources in the zoological sciences. Westport, Conn. : Libraries Unlimited.\n\nO'Hare, Christine (2007). \"Business Information Sources\". London: Library Assn Pub Ltd\n\nOstwald, W (1919). Die chemische Literatur und die Organisation der Wissenschaft. Leipzig : W. Ostwald & C. Drucker. (This is considered the first \"guide to information sources\").\n\nStebbins, Leslie F. (2006). Student guide to research in the digital age; how to locate and evaluate information sources. Westport, Conn.: Libraries Unlimited.\n\nWebb, W. H. et al. (Ed.). (1986). Sources of information in the social sciences. A Guide to the literature. 3. ed. Chicago : American Library Association.\n\nZell, Hans M. (ed.). (2003). The African studies companion; a guide to information sources. 3rd rev. and expanded ed. Glais Bheinn : Hans Zell.\n\n\n"}
{"id": "652730", "url": "https://en.wikipedia.org/wiki?curid=652730", "title": "Inc.", "text": "Inc.\n\nInc. or inc may refer to:\n\n\n"}
{"id": "161388", "url": "https://en.wikipedia.org/wiki?curid=161388", "title": "Indirect self-reference", "text": "Indirect self-reference\n\nIndirect self-reference describes an object referring to itself \"indirectly\".\n\nFor example, define the function f such that f(x) = x(x). Any function passed as an argument to f is invoked with itself as an argument, and thus in any use of that argument is indirectly referring to itself.\n\nThis example is similar to the Scheme expression \"((lambda(x)(x x)) (lambda(x)(x x)))\", which is expanded to itself by beta reduction, and so its evaluation loops indefinitely despite the lack of explicit looping constructs. An equivalent example can be formulated in lambda calculus.\n\nIndirect self-reference is special in that its self-referential quality is not explicit, as it is in the sentence \"this sentence is false.\" The phrase \"this sentence\" refers directly to the sentence as a whole. An indirectly self-referential sentence would replace the phrase \"this sentence\" with an expression that effectively still referred to the sentence, but did not use the pronoun \"this.\"\n\nAn example will help to explain this. Suppose we define the quine of a phrase to be the quotation of the phrase followed by the phrase itself. So, the quine of:\nwould be:\nwhich, incidentally, is a true statement.\n\nNow consider the sentence:\n\nThe quotation here, plus the phrase \"when quined,\" indirectly refers to the entire sentence. The importance of this fact is that the remainder of the sentence, the phrase \"makes quite a statement,\" can now make a statement about the sentence as a whole. If we had used a pronoun for this, we could have written something like \"this sentence makes quite a statement.\"\n\nIt seems silly to go through this trouble when pronouns will suffice (and when they make more sense to the casual reader), but in systems of mathematical logic, there is generally no analog of the pronoun. It is somewhat surprising, in fact, that self-reference can be achieved at all in these systems.\n\nUpon closer inspection, it can be seen that in fact, the Scheme example above uses a quine, and f(x) is actually the quine function itself.\n\nIndirect self-reference was studied in great depth by W. V. Quine (after whom the operation above is named), and occupies a central place in the proof of Gödel's incompleteness theorem. Among the paradoxical statements developed by Quine is the following:\n\n"}
{"id": "17878314", "url": "https://en.wikipedia.org/wiki?curid=17878314", "title": "Information source", "text": "Information source\n\nAn information source is a person, thing, or place from which information comes, arises, or is obtained. Information souces can be known as primary or secondary. That source might then inform a person about something or provide knowledge about it. Information sources are divided into separate distinct categories, primary, secondary, tertiary, and so on.\n\n"}
{"id": "15293025", "url": "https://en.wikipedia.org/wiki?curid=15293025", "title": "Informationsdienst Wissenschaft", "text": "Informationsdienst Wissenschaft\n\nInformationsdienst Wissenschaft e.V. or idw (The Science Information Service) operates an Internet platform, which bundles the press reports and dates of important events from about 1,000 scientific institutions, including universities, technical colleges, governmental and non-governmental research institutes and institutes to support research or scientific administration. idw (a registered charitable society) also operates an expert broker, the idw expert finder, which is exclusively for journalists. This makes idw one of the most comprehensive sources of science news in the German-speaking area. Foreign journalists and institutions (mostly European) now use idw as well. \n\nThe two main objectives of idw are:\n\nThe information in idw can be accessed free of charge - either directly on idw’s www pages, or by using an individually configurable RSS feed or as an e-mail subscriber. Any user can request the information covering the topics and regions which interest him. All idw services can be used free of cost - the current news ticker, the science calendar, research in the archive (which contains more than 350,000 press releases), and the list of institutions linked to idw. idw also provides journalists with instruments for contacting experts, and maintains a database with science photos.\nThe members' press offices have various possibilities of communicating with journalists. Membership is only offered to German or foreign institutions which perform research or teaching, or which support science or are active in science in some other way.\n\nThe original idea of idw was to provide experts for journalists. Using the American ProfNet as example, the press officers of Universitaet Bayreuth, the Ruhr University Bochum and the Clausthal University of Technology, in collaboration with Computing Centre of Clausthal University of Technology/TU Clausthal, developed a concept for a German language network, by means of the new media. The concept was technically implemented by the staff of the Computing Centre of the Clausthal University of Technology. A total of nine staff members in Bayreuth, Bochum and Clausthal are responsible for programming, maintaining and developing the idw operating system, for user services and further development of the content.\n\nThe initial phase (1996–1999) was guaranteed by project support from the Federal Ministry for Education and Research (BMBF). The technical development of the idw was supported by the Ministry, together with the Stifterverband fuer die Deutsche Wissenschaft (Donor Association for German Science). idw has been working closely for years with the initiative Wissenschaft im Dialog (Science in Dialogue). idw has been economically independent since 2000 and is financed by contributions from member institutions. It has been organised as a registered charitable society (gemeinnütziger e. V.) since 2002.\n\nidw has developed as a recognised and accepted source for German language science and for science journalism. It has become an instrument for public relations work for scientific institutions. \nAbout 37,000 subscribers (figure for June 2018) receive regular reports from idw, including some 7,900 journalists. About 1,000 institutions publish their press reports and dates of important events via idw.\n\n"}
{"id": "41908", "url": "https://en.wikipedia.org/wiki?curid=41908", "title": "Key Word in Context", "text": "Key Word in Context\n\nKWIC is an acronym for Key Word In Context, the most common format for concordance lines. The term KWIC was first coined by Hans Peter Luhn. The system was based on a concept called \"keyword in titles\" which was first proposed for Manchester libraries in 1864 by Andrea Crestadoro.\n\nA KWIC index is formed by sorting and aligning the words within an article title to allow each word (except the stop words) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized full text search became common.\n\nFor example, a search query including all of the words in the title statement of this article (\"KWIC is an acronym for Key Word In Context, the most common format for concordance lines\") and the in English (\"the free encyclopedia\"), searched against this very web page, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).\n\nA KWIC index is a special case of a \"permuted index\". This term refers to the fact that it indexes all cyclic permutations of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of manual pages, often ended with a permuted index section, allowing the reader to easily find a section by any word from its heading. This practice, also known as KWOC (“Key Word Out of Context”), is no longer common.\n\n\"Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all.\"\n\n\n"}
{"id": "6487324", "url": "https://en.wikipedia.org/wiki?curid=6487324", "title": "Leishu", "text": "Leishu\n\nThe leishu () is a genre of reference books historically compiled in China and other countries of the Sinosphere. The term is generally translated as \"encyclopedia\", although the \"leishu\" are quite different from the modern notion of encyclopedia.\n\nThe \"leishu\" are composed of sometimes lengthy citations from other works, and often contain copies of entire works, not just excerpts. The works are classified by a systematic set of categories, which are further divided into subcategories. \"Leishu\" may be considered anthologies, but are encyclopedic in the sense that they may comprise the entire realm of knowledge at the time of compilation.\n\nApproximately 600 \"leishu\" were compiled from the early third century until the eighteenth century, of which 200 have survived. The largest \"leishu\" ever compiled was the 1408 \"Yongle Dadian\", containing 370 million Chinese characters, and the largest ever printed was the \"Gujin Tushu Jicheng\", containing 100 million characters and 852,408 pages.\n\nThe genre first appeared in the early third century. The earliest known was the \"Huanglan\" (\"Emperor's mirror\"). Sponsored by the emperor of Cao Wei, it was compiled around 220, but has since been lost. However, the term \"leishu\" was not used until the Song dynasty (960–1279).\n\nIn later imperial China dynasties, such as the Ming and Qing, emperors sponsored monumental projects to compile all known human knowledge into a single \"leishu\", in which entire works, rather than excerpts, were copied and classified by category. The largest \"leishu\" ever compiled, on the order of the Yongle Emperor of Ming, was the \"Yongle Dadian\" containing a total of 370 million Chinese characters. The project involved 2,169 scholars, who worked for four years under general editor Yao Guangxiao. It was completed in 1408, but never printed, as the imperial treasury had run out of money.\n\nThe \"Qinding Gujin Tushu Jicheng\" (Imperially approved synthesis of books and illustrations past and present) is by far the largest \"leishu\" ever printed, containing 100 million characters and 852,408 pages. It was compiled by a team of scholars led by Chen Menglei, and printed between 1726 and 1728, during the Qing dynasty.\n\nThe \"riyong leishu\" (encyclopedias for daily use), containing practical information for people who were literate but below the Confucian elite, were also compiled in the later imperial era. Today, they provide scholars with valuable information on non-elite culture and attitudes.\n\nAccording to Jean-Pierre Diény, the Jiaqing reign (1796–1820) of the Qing dynasty saw the end of the publication of \"leishu\".\n\nOther countries of the Sinosphere also adopted the genre of \"leishu\". In 1712, the \"Sancai Tuhui\", a richly illustrated \"leishu\" compiled by Ming scholar Wang Qi (王圻) in the early 17th century, was printed in Japan as \"Wakan Sansai Zue\". The Japanese version was edited by Terajima Ryōan (寺島良安), a physician born in Osaka.\n\nThe \"leishu\" have played an important role in the preservation of ancient works, many of which have been lost, only preserved completely or partially as part of a \"leishu\" compilation. The 7th-century \"Yiwen Leiju\" is especially valuable. It contains excerpts from 1,400 pre-7th century works, 90% of which have been otherwise lost. Even though the \"Yongle Dadian\" is itself largely lost, the remnants still contain 385 complete books that have been otherwise lost. The \"leishu\" also provide a unique view of the transmission of knowledge and education, and an easy way to locate traditional materials on any given subject.\n\nApproximately 600 \"leishu\" were compiled, from the Cao Wei period (early third century) until the 18th century, of which 200 have survived. Among the most important, in chronological order, are:\n\n\n"}
{"id": "51388883", "url": "https://en.wikipedia.org/wiki?curid=51388883", "title": "Life spans of home appliances", "text": "Life spans of home appliances\n\nThis page lists the average life spans of home appliances (major and small).\n\n"}
{"id": "4807639", "url": "https://en.wikipedia.org/wiki?curid=4807639", "title": "Microsoft Bookshelf", "text": "Microsoft Bookshelf\n\nMicrosoft Bookshelf was a reference collection introduced in 1987 as part of Microsoft's extensive work in promoting CD-ROM technology as a distribution medium for electronic publishing. The original MS-DOS version showcased the massive storage capacity of CD-ROM technology, and was accessed while the user was using one of 13 different word processor programs that Bookshelf supported. Subsequent versions were produced for Windows and became a commercial success as part of the Microsoft Home brand. It was often bundled with personal computers as a cheaper alternative to the Encarta Suite. The Encarta Deluxe Suite / Reference Library versions also bundled Bookshelf.\n\nMicrosoft Bookshelf was discontinued in 2000. In later editions of the Encarta suite (Encarta 2000 and onwards), Bookshelf was replaced with a dedicated \"Encarta Dictionary\", a superset of the printed edition. There has been some controversy over the decision, since the dictionary lacks the other books provided in Bookshelf which many found to be a useful reference, such as the dictionary of quotations (replaced with a quotations section in \"Encarta\" that links to relevant articles and people) and the Internet Directory, although the directory is now a moot point since many of the sites listed in offline directories no longer exist.\n\nThe original 1987 edition contained \"The Original Roget's Thesaurus of English Words and Phrases\", \"The American Heritage Dictionary of the English Language\", World Almanac and Book of Facts, Bartlett's Familiar Quotations, The Chicago Manual of Style (13th Edition), the U.S. ZIP Code Directory, Houghton Mifflin Usage Alert, Houghton Mifflin Spelling Verifier and Corrector, Business Information Sources, and Forms and Letters. Titles in non-US versions of Bookshelf were different. For example, the 1997 UK edition included the Chambers Dictionary, Bloomsbury Treasury of Quotations, and Hutchinson Concise Encyclopedia.\n\nThe Windows release of Bookshelf added a number of new reference titles, including \"The Concise Columbia Encyclopedia\" and an Internet Directory. Other titles were added and some were dropped in subsequent years. By 1994, the English-language version also contained the \"Columbia Dictionary of Quotations\"; \"The Concise Columbia Encyclopedia\"; the \"Hammond Intermediate World Atlas\"; and \"The People's Chronology\". By 2000, the collection came to include the \"Encarta Desk Encyclopedia\", the \"Encarta Desk Atlas\", the \"Encarta Style Guide\" and a specialized \"Computer and Internet Dictionary\" by Microsoft Press.\n\nBookshelf 1.0 used a proprietary hypertext engine that Microsoft acquired when it bought the company Cytation in 1986. Also used for Microsoft Stat Pack and Microsoft Small Business Consultant, it was a Terminate and Stay Resident (TSR) program that ran alongside a dominant program, unbeknownst to the dominant program. Like Apple's similar Hypercard reader, Bookshelf engine's files used a single compound document, containing large numbers of subdocuments (\"cards\" or \"articles\"). They both differ from current browsers which normally treat each \"page\" or \"article\" as a separate file.\n\nThough similar to Apple's Hypercard reader in many ways, the Bookshelf engine had several key differences. Unlike Hypercard files, Bookshelf files required compilation and complex markup codes. This made the files more difficult to pirate, addressing a key concern of early electronic publishers. Furthermore, Bookshelf's engine was designed to run as fast as possible on slow first-generation CD-ROM drives, some of which required as much as a half-second to move the drive head. Such hardware constraints made Hypercard impractical for high-capacity CD-ROMs. Bookshelf also had full text searching capability, which made it easy to find needed information.\n\nCollaborating with DuPont, the Microsoft CD-ROM division developed a Windows version of its engine for applications as diverse as document management, online help, and a CD-ROM encyclopedia. In a skunkworks project, these developers worked secretly with Multimedia Division developers so that the engine would be usable for more ambitious multimedia applications. Thus they integrated a multimedia markup language, full text search, and extensibility using software objects, all of which are commonplace in modern internet browsing.\n\nIn 1992, Microsoft started selling the Bookshelf engine to third-party developers, marketing the product as Microsoft Multimedia Viewer. The idea was that such a tool would help a burgeoning growth of CD-ROM titles that would spur demand for Windows. Although the engine had multimedia capabilities that would not be matched by Web browsers until the late 1990s, Microsoft Viewer did not enjoy commercial success as a standalone product. However, Microsoft continued to use the engine for its Encarta and WinHelp applications, though the multimedia functions are rarely used in Windows help files.\n\nIn 1993, the developers who were working on the next generation viewer were moved to the Cairo systems group which was charged with delivering Bill Gates' 'vision' of 'Information at your fingertips'. This advanced browser was a fully componentized application using what are now known as Component Object Model objects, designed for hypermedia browsing across large networks and whose main competitor was thought to be Lotus Notes. Long before Netscape appeared, this team, known as the WEB (web enhanced browser) team had already shipped a network capable hypertext browser capable of doing everything that HTML browsers would not be able to do until the turn of the century. Nearly all technologies of Cairo shipped. The WEB browser was not one of them, though it influenced the design of many other common Microsoft technologies.\n\n\"BYTE\" in 1989 listed Microsoft Bookshelf as among the \"Excellence\" winners of the BYTE Awards, stating that it \"is the first substantial application of CD-ROM technology\" and \"a harbinger of personal library systems to come\".\n\n"}
{"id": "24673687", "url": "https://en.wikipedia.org/wiki?curid=24673687", "title": "Polymath (disambiguation)", "text": "Polymath (disambiguation)\n\nA polymath is a person whose expertise spans a significant number of different subject areas and who has extraordinarily broad and comprehensive knowledge.\n\nPolymath may also refer to:\n\n"}
{"id": "3653726", "url": "https://en.wikipedia.org/wiki?curid=3653726", "title": "Psc (military)", "text": "Psc (military)\n\npsc is a post-nominal for \"passed Staff College\" in the Commonwealth militaries of Britain, Bangladesh, Indian, Sri Lanka and Pakistan. It indicates that an officer has undertaken the staff officer course at a Staff College.\n\nThe practice originated in the British Army where the initials \"psc\" appeared in the service lists denoting that the officer had attended the Staff College, Camberley. Royal Navy offers who attended the staff course at Royal Naval College, Greenwich also used the qualification. Since the 1997 amalgamation of staff training officers now receive the letters psc(j) from the Joint Services Command and Staff College.\n\nPSC is used for Bangladeshi Army officers who have attended the Defence Services Command & Staff College (DSCSC), Bangladesh.\n\nInitials psc is used by officers who attended the Defence Services Staff College, Wellington, India.\n\nIn Pakistan initials psc is used by officers who attended the Command and Staff College, Quetta.\n\nOfficers graduated from the Malaysian Armed Forces Staff College, Kuala Lumpur use the initials psc.\n\nIn Sri Lanka, the initials psc are used by Army, Navy and Air Force officers who have gained the Pass Staff College status from a recognized a staff college such as the Defence Services Command and Staff College and the Sri Lanka Air Force Junior Command & Staff College. Such officers are eligible to wear the psc badge.\n"}
{"id": "18134289", "url": "https://en.wikipedia.org/wiki?curid=18134289", "title": "Qualitative comparative analysis", "text": "Qualitative comparative analysis\n\nIn statistics, qualitative comparative analysis (QCA) is a data analysis technique for determining which logical conclusions a data set supports. The analysis begins with listing and counting all the combinations of variables observed in the data set, followed by applying the rules of logical inference to determine which descriptive inferences or implications the data supports. The technique was originally developed by Charles Ragin in 1987.\n\nIn the case of categorical variables, QCA begins by listing and counting all types of cases which occur, where each type of case is defined by its unique combination of values of its independent and dependent variables. For instance, if there were four categorical variables of interest, {A,B,C,D}, and A and B were dichotomous (could take on two values), C could take on five values, and D could take on three, then there would be 60 possible types of observations determined by the possible combinations of variables, not all of which would necessarily occur in real life. By counting the number of observations that exist for each of the 60 unique combination of variables, QCA can determine which descriptive inferences or implications are empirically supported by a data set. Thus, the input to QCA is a data set of any size, from small-N to large-N, and the output of QCA is a set of descriptive inferences or implications the data supports.\n\nIn QCA's next step, inferential logic or Boolean algebra is used to simplify or reduce the number of inferences to the minimum set of inferences supported by the data. This reduced set of inferences is termed the \"prime implicates\" by QCA adherents. For instance, if the presence of conditions A and B is always associated with the presence of a particular value of D, regardless of the observed value of C, then the value that C takes is irrelevant. Thus, all five inferences involving A and B and any of the five values of C may be replaced by the single descriptive inference \"(A and B) implies the particular value of D\".\n\nTo establish that the prime implicants or descriptive inferences derived from the data by the QCA method are causal requires establishing the existence of causal mechanism using another method such as process tracing, formal logic, intervening variables, or established multidisciplinary knowledge. The method is used in social science and is based on the binary logic of Boolean algebra, and attempts to ensure that all possible combinations of variables that can be made across the cases under investigation are considered.\n\nThe technique of listing case types by potential variable combinations assists with case selection by making investigators aware of all possible case types that would need to be investigated, at a minimum, if they exist, in order to test a certain hypothesis or to derive new inferences from an existing data set. In situations where the available observations constitute the entire population of cases, this method alleviates the small N problem by allowing inferences to be drawn by evaluating and comparing the number of cases exhibiting each combination of variables. The small N problem arises when the number of units of analysis (e.g. countries) available is inherently limited. For example: a study where countries are the unit of analysis is limited in that are only a limited number of countries in the world (less than 200), less than necessary for some (probabilistic) statistical techniques. By maximizing the number of comparisons that can be made across the cases under investigation, causal inferences are according to Ragin possible. This technique allows the identification of multiple causal pathways and interaction effects that may not be detectable via statistical analysis that typically requires its data set to conform to one model. Thus, it is the first step to identifying subsets of a data set conforming to particular causal pathway based on the combinations of covariates prior to quantitative statistical analyses testing conformance to a model; and helps qualitative researchers to correctly limit the scope of claimed findings to the type of observations they analyze.\n\nAs this is a logical (deterministic) and not a statistical (probabilistic) technique, with \"crisp-set\" QCA (csQCA), the original application of QCA, variables can only have two values, which is problematic as the researcher has to determine the values of each variable. For example: GDP per capita has to be divided by the researcher in two categories (e.g. low = 0 and high = 1). But as this variable is essentially a continuous variable, the division will always be arbitrary. A second, related problem is that the technique does not allow an assessment of the effect of the relative strengths of the independent variables (as they can only have two values). Ragin, and other scholars such as Lasse Cronqvist, have tried to deal with these issues by developing new tools that extend QCA, such as multi-value QCA (mvQCA) and fuzzy set QCA (fsQCA). Note: Multi-value QCA is simply QCA applied to observations having categorical variables with more than two values. Crisp-Set QCA can be considered a special case of Multi-value QCA. \n\nStatistical methodologists have argued that QCA's strong assumptions render its findings both fragile and prone to type I error. Simon Hug argues that deterministic hypotheses and error-free measures are exceedingly rare in social science and uses Monte Carlo simulations to demonstrate the fragility of QCA results if either assumption is violated. Chris Krogslund, Donghyun Danny Choi, and Mathias Poertner further demonstrate that QCA results are highly sensitive to minor parametric and model-susceptibility changes and are vulnerable to type I error. Bear F. Braumoeller further explores the vulnerability of the QCA family of techniques to both type I error and multiple inference. Braumoeller also offers a formal test of the null hypothesis and demonstrates that even very convincing QCA findings may be the result of chance.\n\nQCA can be performed probabilistically or deterministically with observations of categorical variables. For instance, the existence of a descriptive inference or implication is supported deterministically by the absence of any counter-example cases to the inference; i.e. if a researcher claims condition X implies condition Y, then, deterministically, there must not exist any counterexample cases having condition X, but not condition Y. However, if the researcher wants to claim that condition X is a probabilistic 'predictor' of condition Y, in another similar set of cases, then the proportion of counterexample cases to an inference to the proportion of cases having that same combination of conditions can be set at a threshold value of for example 80% or higher. For each prime implicant that QCA outputs via its logical inference reduction process, the \"coverage\" — percentage out of all observations that exhibit that implication or inference — and the \"consistency\" — the percentage of observations conforming to that combination of variables having that particular value of the dependent variable or outcome — are calculated and reported, and can be used as indicators of the strength of such a explorative probabilistic inference. In real-life complex societal processes, QCA enables the identification of multiple sets of conditions that are consistently associated with a particular output value in order to explore for causal predictors.\n\nFuzzy set QCA aims to handle variables, such as GDP per capita, where the number of categories, decimal values of monetary units, becomes too large to use mvQCA, or in cases were uncertainty or ambiguity or measurement error in the classification of a case needs to be acknowledged.\n\nQCA has now become used in many more fields than political science which Ragin first developed the method for. Today the method has been used in:\n"}
{"id": "47089125", "url": "https://en.wikipedia.org/wiki?curid=47089125", "title": "Roving reference", "text": "Roving reference\n\nRoving reference, also called roaming reference, is a library service model in which, instead of being positioned at a static reference desk, a librarian moves throughout the library to locate patrons with questions or concerns and offer them help in finding or using library resources.\n\nRoving reference as a library service practice was first formalized in the late 1980s and early 1990s. A 1999 report from the International Federation of Library Associations identified several advantages and disadvantages with roving reference in the pre-mobile era. The roving model allowed librarians to engage with \"the majority of users who have questions in mind [who] do not approach the reference desk for assistance\". However, libraries reported that some staff were uncomfortable with the practice, and that there were concerns about user privacy.\n\nBeginning in the 2000s, librarians used laptops or laptop carts to engage in technology-supported roaming reference. Since the development of mobile technologies, roving reference can be facilitated with the use of such technologies, such as tablet computers, which allow librarians to readily check the online public access catalogue or the library's electronic databases while away from their desk. This has contributed to the increased popularization of roving-reference programs as supplements for more traditional reference desks. The model has also been extended to service beyond the library building (library outreach), for example in a dormitory or faculty building at an academic institution.\n"}
{"id": "1130951", "url": "https://en.wikipedia.org/wiki?curid=1130951", "title": "Scribal abbreviation", "text": "Scribal abbreviation\n\nScribal abbreviations or sigla (singular: siglum) are the abbreviations used by ancient and medieval scribes writing in Latin, and later in Greek and Old Norse. In modern manuscript editing (substantive and mechanical) \"sigla\" are the symbols used to indicate the source manuscript (e.g. variations in text between different such manuscripts) and to identify the copyist(s) of a work. See Critical apparatus.\n\nAbbreviated writing, using \"sigla\", arose partly from the limitations of the workable nature of the materials (stone, metal, parchment, etc.) employed in record-making and partly from their availability. Thus, lapidaries, engravers, and copyists made the most of the available writing space. Scribal abbreviations were infrequent when writing materials were plentiful, but by the 3rd and 4th centuries AD, writing materials were scarce and costly.\n\nDuring the Roman Republic, several abbreviations, known as \"sigla\" (plural of \"siglum\" = symbol or abbreviation), were in common use in inscriptions, and they increased in number during the Roman Empire. Additionally, in this period shorthand entered general usage. The earliest known Western shorthand system was that employed by the Greek historian Xenophon in the memoir of Socrates, and it was called \"notae socratae\". In the late Roman Republic, the Tironian notes were developed possibly by Marcus Tullius Tiro, Cicero's amanuensis, in 63 BC to record information with fewer symbols; Tironian notes include a shorthand/syllabic alphabet notation different from the Latin minuscule hand and square and rustic capital letters. The notation was akin to modern stenographic writing systems. It used symbols for whole words or word roots and grammatical modifier marks, and it could be used to write either whole passages in shorthand or only certain words. In medieval times, the symbols to represent words were widely used; and the initial symbols, as few as 140 according to some sources, were increased to 14,000 by the Carolingians, who used them in conjunction with other abbreviations. However, the alphabet notation had a \"murky existence\" (C. Burnett), as it was often associated with witchcraft and magic, and it was eventually forgotten. Interest in it was rekindled by the Archbishop of Canterbury Thomas Becket in the 12th century and later in the 15th century, when it was rediscovered by Johannes Trithemius, abbot of the Benedictine abbey of Sponheim, in a psalm written entirely in Tironian shorthand and a Ciceronian lexicon, which was discovered in a Benedictine monastery (\"notae benenses\").\n\nTo learn the Tironian note system, scribes required formal schooling in some 4,000 symbols; this later increased to some 5,000 symbols and then to some 13,000 in the medieval period (4th to 15th centuries AD); the meanings of some characters remain uncertain. \"Sigla\" were mostly used in lapidary inscriptions; in some places and historical periods (such as medieval Spain) scribal abbreviations were overused to the extent that some are indecipherable.\n\nThe abbreviations were not constant but changed from region to region. Scribal abbreviations increased in usage and reached their height in the Carolingian Renaissance (8th to 10th centuries). The most common abbreviations, called \"notae communes\", were used across most of Europe, but others appeared in certain regions. In legal documents, legal abbreviations, called \"notae juris\", appear but also capricious abbreviations, which scribes manufactured \"ad hoc\" to avoid repeating names and places in a given document.\n\nScribal abbreviations can be found in epigraphy, sacred and legal manuscripts, written in Latin or in a vernacular tongue (but less frequently and with fewer abbreviations), either calligraphically or not.\n\nIn epigraphy, common abbreviations were comprehended in two observed classes:\n\nBoth forms of abbreviation are called \"suspensions\" (as the scribe suspends the writing of the word). A separate form of abbreviation is by \"contraction\" and was mostly a Christian usage for sacred words, Nomina Sacra; non-Christian sigla usage usually limited the number of letters the abbreviation comprised and omitted no intermediate letter. One practice was rendering an overused, formulaic phrase only as a siglum: DM for \"Dis Manibus\" (\"Dedicated to the Manes\"); IHS from the first three letters of \"ΙΗΣΟΥΣ\"; and RIP for \"requiescat in pace\" (\"Rest in Peace\") because the long-form written usage of the abbreviated phrase, by itself, was rare. According to Trabe, these abbreviations are not really meant to lighten the burden of the scribe but rather to shroud in reverent obscurity the holiest words of the Christian religion.\n\nAnother practice was repeating the abbreviation's final consonant a given number of times to indicate a group of as many persons: AVG denoted \"Augustus\", thus, AVGG denoted \"Augusti duo\"; however, lapidaries took typographic liberties with that rule, and instead of using COSS to denote \"Consulibus duobus\", they invented the CCSS form. Still, when occasion required referring to three or four persons, the complex doubling of the final consonant yielded to the simple plural siglum. To that effect, a \"vinculum\" (overbar) above a letter or a letter-set also was so used, becoming a universal medieval typographic usage. Likewise the \"tilde\" (~), an undulated, curved-end line, came into standard late-medieval usage.\n\nBesides the \"tilde\" and macron marks, above and below letters, modifying cross-bars and extended strokes were employed as scribal abbreviation marks, mostly for prefixes and verb, noun and adjective suffixes. The \"typographic\" abbreviations should not be confused with the \"phrasal\" abbreviations: i.e. (\"id est\" — \"that is\"); loc. cit. (\"loco citato\" — \"in the passage already cited\"); viz. (\"vide licet\" — \"namely\", \"that is to say\", \"in other words\" — formed with \"vi\" and the \"yogh\"-like glyph [Ꝫ], [ꝫ], the siglum for the suffix -et and the conjunction et) and et cetera.\n\nMoreover, besides scribal abbreviations, ancient texts also contained variant typographic characters, including ligatures (e.g. Æ, Œ, etc.), the long s (ſ), and the half r, resembling an Arabic numeral two (\"2\"). The \"u\" and \"v\" characters originated as scribal variants for their respective letters, likewise the \"i\" and \"j\" pair. Modern publishers printing Latin-language works replace variant typography and sigla with full-form Latin spellings; the convention of using \"u\" and \"i\" for vowels and \"v\" and \"j\" for consonants is a late typographic development.\n\nSome ancient and medieval sigla are still used in English and other European languages; the Latin ampersand (&) replaces the conjunction \"and\" in English, \"et\" in Latin and French, and \"y\" in Spanish (but its use in Spanish is frowned upon, since the \"y\" is already smaller and easier to write). The Tironian sign ⁊, resembling the digit seven (\"7\"), represents the conjunction \"et\" and is written only to the x-height; in current Irish language usage, the siglum denotes the conjunction \"agus\" (\"and\"). Other scribal abbreviations in modern typographic use are the percentage sign (%), from the Italian \"per cento\" (\"per hundred\"); the permille sign (‰), from the Italian \"per mille\" (\"per thousand\"); the pound sign (₤, £ and #, all descending from ℔ or lb, \"librum\") and the dollar sign ($), which possibly derives from the Spanish word \"Peso\". The commercial at symbol (@), originally denoting \"at the rate/price of\", is a ligature derived from the English preposition \"at\"; from the 1990s, its use outside commerce became widespread, as part of e-mail addresses.\n\nTypographically, the ampersand (\"&\"), representing the word \"et\", is a space-saving ligature of the letters \"e\" and \"t\", its component graphemes. Since the establishment of movable-type printing in the 15th century, founders have created many such ligatures for each set of record type (font) to communicate much information with fewer symbols. Moreover, during the Renaissance (14th to 17th centuries), when Ancient Greek language manuscripts introduced that tongue to Western Europe, its scribal abbreviations were converted to ligatures in imitation of the Latin scribal writing to which readers were accustomed. Later, in the 16th century, when the culture of publishing included Europe's vernacular languages, Graeco-Roman scribal abbreviations disappeared, an ideologic deletion ascribed to the anti-Latinist Protestant Reformation (1517–1648).\n\nThe common abbreviation \"Xmas,\" for Christmas, is a remnant of an old scribal abbreviation that substituted the Greek letter chi (Χ, resembling Latin X and representing the first letter in the Greek word for Christ, Χριστος) for the word Christ.\n\nAfter the invention of printing, manuscript copying abbreviations continued to be employed in Church Slavonic and are still in use in printed books as well as on icons and inscriptions. Many common long roots and nouns describing sacred persons are abbreviated and written under the special diacritic symbol titlo, as shown in the figure at the right. That corresponds to the Nomina sacra (Latin: \"Sacred names\") tradition of using contractions for certain frequently-occurring names in Greek ecclesiastical texts. However, sigla for personal nouns are restricted to \"good\" beings and the same words, when referring to \"bad\" beings, are spelled out; for example, while \"God\" in the sense of the one true God is abbreviated as \"\", \"god\" referring to \"false\" gods is spelled out. Likewise, the word for \"angel\" is generally abbreviated as \"\", but the word for \"angels\" is spelled out for \"performed by evil angels\" in Psalm 77.\n\nAdriano Cappelli's \"Lexicon Abbreviaturarum\", enumerates the various medieval brachigraphic signs found in Latin and Italian vulgar texts, which originate from the Roman sigla, a symbol to express a word, and Tironian notes. Quite rarely, abbreviations did not carry marks to indicate that an abbreviation has occurred: if they did, they were often copying errors. For example, \"e.g.\" is written with periods, but modern terms, such as \"PC\", may be written in uppercase.\n\nIt should be noted that the original manuscripts were not written in a modern sans-serif or serif font but in Roman capitals, rustic, uncial, insular, Carolingian or blackletter styles. For more, refer to Western calligraphy or a beginner's guide.\n\nAdditionally, the abbreviations employed varied across Europe. In Nordic texts, for instance, two runes were used in text written in the Latin alphabet, which are ᚠ for \"fé\" \"cattle, goods\" and ᛘ for \"maðr\" \"man\".\n\nCappelli divides abbreviations into six overlapping categories:\n\nSuspended terms are those of which only the first part is written, and the last part is substituted by a mark, which can be of two types:\n\nThe largest class of suspensions consists of single letters standing in for words that begin with that letter.\n\nA dot at the baseline after a capital letter may stand for a title if it is used such as in front of names or a person's name in medieval legal documents. However, not all sigla use the beginning of the word.\nFor plural words, the siglum is often doubled: \"F.\" = \"frater\" and \"FF.\" = \"fratres\". Tripled sigla often stand for three: \"DDD\" = \"domini tres\".\n\nLetters lying on their sides, or mirrored (backwards), often indicate female titles, but a mirrored C, Ↄ, stands generally for \"con\" or \"contra\" (the latter sometimes with a macron above, \"Ↄ̄\").\n\nTo avoid confusion with abbreviations and numerals, the latter are often written with a bar above. In some contexts, however, numbers with a line above indicate that number is to be multiplied by a thousand, and several other abbreviations also have a line above them, such as \"ΧΡ\" (Greek letters chi+rho) = \"Christus\" or \"IHS\" = \"Jesus\".\n\nStarting in the 8th or the 9th century, single letter sigla grew less common and were replaced by longer, less-ambiguous sigla, with bars above them.\n\nAbbreviations by contraction have one or more middle letters omitted. They were often represented with a general mark of abbreviation (above), such as a line above. They can be divided into two subtypes:\n\nSuch marks inform the reader of the identity of the missing part of the word without affecting (\"independent\" of) the meaning. Some of them may be interpreted as alternative contextual glyphs of their respective letters.\n\nThe meaning of the marks depends on the letter on which they appear.\n\nA superscript letter generally referred to the letter omitted, but, in some instances, as in the case of vowel letters, it could refer to a missing vowel combined with the letter \"r\", before or after it. It is only in some English dialects that the letter \"r\" before another consonant largely silent and the preceding vowel is \"r-coloured\".\n\nHowever, \"a\", \"i\", and \"o\" above \"g\" meant \"gͣ\" \"gna\", \"gͥ\" \"gni\" and \"gͦ\" \"gno\" respectively. Although in English, the \"g\" is silent in \"gn\", but in other languages, it is pronounced. Vowel letters above \"q\" meant \"qu\" + vowel: \"qͣ\", \"qͤ\", \"qͥ\", \"qͦ\", \"qͧ\".\n\n\nVowels were the most common superscripts, but consonants could be placed above letters without ascenders; the most common were \"c\", e.g. \"nͨ\". A cut \"l\" above an \"n\", \"nᷝ\", meant \"nihil\" for instance.\n\nThese marks are nonalphabetic letters carrying a particular meaning. Several of them continue in modern usage, as in the case of monetary symbols. In Unicode, they are referred to as \"letter-like glyphs\". Additionally, several authors are of the view that the Roman numerals themselves were, for example, nothing less than abbreviations of the words for those numbers. Other examples of symbols still in some use are alchemical and zodiac symbols, which were, in any case, employed only in alchemy and astrology texts, which made their appearance beyond that special context rare.\n\nIn addition to the signs used to signify abbreviations, medieval manuscripts feature some glyphs that are now uncommon but were not sigla.\nMany more ligatures were used to reduce the space occupied, a characteristic that is particularly prominent in blackletter scripts.\nSome such as r rotunda, long s and uncial or insular variants (Insular G), Claudian letters were in common use, as well as letters derived from other scripts such as Nordic runes: thorn (þ=th) and eth (ð=dh).\nAn illuminated manuscript would feature miniatures, decorated initials or \"littera notabilior\", which later resulted in the bicamerality of the script (case distinction).\n\nVarious typefaces have been designed to allow scribal abbreviations and other archaic glyphs to be replicated in print. They include \"record type\", which was first developed in the 1770s to publish Domesday Book and was fairly widely used for the publication of medieval records in Britain until the end of the 19th century.\n\nIn the Unicode Standard v. 5.1 (4 April 2008), 152 medieval and classical glyphs were given specific locations outside of the Private Use Area. Specifically, they are located in the charts \"Combining Diacritical Marks Supplement\" (26 characters), \"Latin Extended Additional\" (10 characters), \"Supplemental Punctuation\" (15 characters), \"Ancient Symbols\" (12 characters) and especially \"Latin Extended-D\" (89 characters).\nThese consist in both precomposed characters and modifiers for other characters, called combining diacritical marks (such as writing in LaTeX or using overstrike in MS Word).\n\nCharacters are \"the smallest components of written language that have semantic value\" but glyphs are \"the shapes that characters can have when they are rendered or displayed\".\n\n\n\n"}
{"id": "23499848", "url": "https://en.wikipedia.org/wiki?curid=23499848", "title": "Secondary source", "text": "Secondary source\n\nIn scholarship, a secondary source is a document or recording that relates or discusses information originally presented elsewhere. A secondary source contrasts with a primary source, which is an original source of the information being discussed; a primary source can be a person with direct knowledge of a situation, or a document created by such a person. \n\nA secondary source is one that gives information about a primary source. In this source, the original information is selected, modified and arranged in a suitable format. Secondary sources involve generalization, analysis, interpretation, or evaluation of the original information. \nThe most accurate classification for any given source is not always obvious. \"Primary\" and \"secondary\" are relative terms, and some sources may be classified as primary or secondary, depending on how they are used. A third level, the tertiary source, such as an encyclopedia or dictionary, resembles a secondary source in that it contains analysis, but attempts to provide a broad introductory overview of a topic.\n\nInformation can be taken from a wide variety of objects, but this classification system is only useful for a class of sources that are called symbolic sources. Symbolic sources are sources that are intended to communicate information to someone. Common symbolic sources include written documents such as letters and notes, but not, for example, bits of broken pottery and scraps of food excavated from a midden, regardless of how much information can be extracted from an ancient trash heap, or how little can be extracted from a written document. \n\nMany sources can be considered either primary or secondary, depending on the context in which they are used. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. For example, if a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion, but a secondary source of information found in the old documents. Other examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source, but if the letter is later found, it may then be considered \"secondary\".\n\nAttempts to map or model scientific and scholarly communication need the concepts of primary, secondary and further \"levels\". One such model is the UNISIST model of information dissemination. Within such a model these concepts are defined in relation to each other, and the acceptance of this way of defining the concepts are connected to the acceptance of the model.\n\nSome other modern languages use more than one word for the English word \"source\". German usually uses \"Sekundärliteratur\" (\"secondary literature\") for secondary sources for historical facts, leaving \"Sekundärquelle\" (\"secondary source\") to historiography. A \"Sekundärquelle\" is a source which can tell about a lost \"Primärquelle\" (\"primary source\"), such as a letter quoting from minutes which are no longer known to exist, and so cannot be consulted by the historian.\n\nIn general, secondary sources are self-described as review articles or meta-analysis.\n\nPrimary source materials are typically defined as \"original research papers written by the scientists who actually conducted the study.\" An example of primary source material is the Purpose, Methods, Results, Conclusions sections of a research paper (in IMRAD style) in a scientific journal by the authors who conducted the study. In some fields, a secondary source may include a summary of the literature in the Introduction of a scientific paper, a description of what is known about a disease or treatment in a chapter in a reference book, or a synthesis written to review available literature. A survey of previous work in the field in a primary peer-reviewed source is secondary source information. This allows secondary sourcing of recent findings in areas where full review articles have not yet been published.\n\nA book review that contains the judgment of the reviewer about the book is a primary source for the reviewer's opinion, and a secondary source for the contents of the book. A summary of the book within a review is a secondary source.\n\nIn library and information sciences, secondary sources are generally regarded as those sources that summarize or add commentary to primary sources in the context of the particular information or idea under study.\n\nAn important use of secondary sources in the field of mathematics has been to make difficult mathematical ideas and proofs from primary sources more accessible to the public; in other sciences tertiary sources are expected to fulfill the introductory role.\n\nSecondary sources in history and humanities are usually books or scholarly journals, from the perspective of a later interpreter, especially by a later scholar. In the humanities, a peer reviewed article is always a secondary source.\nThe delineation of sources as primary and secondary first arose in the field of historiography, as historians attempted to identify and classify the sources of historical writing. In scholarly writing, an important objective of classifying sources is to determine the independence and reliability of sources. In original scholarly writing, historians rely on primary sources, read in the context of the scholarly interpretations.\n\nFollowing the Rankean model established by German scholarship in the 19th century, historians use archives of primary sources. Most undergraduate research projects rely on secondary source material, with perhaps snippets of primary sources.\n\nIn the legal field, source classification is important because the persuasiveness of a source usually depends upon its history. Primary sources may include cases, constitutions, statutes, administrative regulations, and other sources of binding legal authority, while secondary legal sources may include books, the headnotes of case reports, articles, and encyclopedias. Legal writers usually prefer to cite primary sources because only primary sources are authoritative and precedential, while secondary sources are only persuasive at best.\n\n\"A secondary source is a record or statement of an event or circumstance made by a non-eyewitness or by someone not closely connected with the event or circumstances, recorded or stated verbally either at or sometime after the event, or by an eye-witness at a time after the event when the fallibility of memory is an important factor.\" Consequently, according to this definition, a first-hand account written long after the event \"when the fallibility of memory is an important factor\" is a secondary source, even though it may be the first published description of that event.\n\nAn autobiography can be a secondary source in history or the humanities when used for information about topics other than its subject. For example, many first hand accounts of events in World War I written in the post-war years were influenced by the then prevailing perception of the war which was significantly different from contemporary opinion.\n\n\n"}
{"id": "16122539", "url": "https://en.wikipedia.org/wiki?curid=16122539", "title": "Self-reference puzzle", "text": "Self-reference puzzle\n\nA self-reference puzzle is a type of logical puzzle where the question in the puzzle refers to the attributes of the puzzle itself.\nA common example is that a \"fill in the blanks\" style sentence is given, but what is filled in the blanks can contribute to the sentence itself. An example is \"There are _____ e's in this sentence.\", for which a solution is \"eight\" (since including the \"eight\", there are 8 e's in the sentence).\n\n"}
{"id": "29663614", "url": "https://en.wikipedia.org/wiki?curid=29663614", "title": "Species affinis", "text": "Species affinis\n\nSpecies affinis (commonly abbreviated to: sp. aff., aff., or affin.) is taxonomic terminology in zoology and botany. In open nomenclature it indicates that available material or evidence suggests that the proposed species is \"related to\", has an \"affinity\" to, but is \"not identical to\", the species with the binomial name that follows. The Latin word \"affinis\" can be translated as \"closely related to\", or \"akin to\".\n\nAn author who inserts \"n.sp.,\" or \"sp. nov., aff\" before a species name thereby states the opinion that the specimen is a new, previously undescribed species, but that there may not (yet) be enough information to complete a formal description. To use aff. alone, implies that the specimen differs suggestively from the holotype but that further progress is necessary to confirm that it is a novel species.\n\nAn example would be: a gastropod shell listed as \"Lucapina\" aff. \"aegis\" would mean that this shell somewhat resembles the shell of \"Lucapina aegis\", but is thought more likely to be another species, either closely related to, or closely resembling \"Lucapina aegis\". In a suitable context it also may suggest the possibility that the shell belongs to a species that has not yet been described.\n\nThe use of aff. is similar to other indicators of open nomenclature such as cf., sp., or ?, but the latter indicate that the species is \"uncertain\" rather than undescribed.\n\n"}
{"id": "3423601", "url": "https://en.wikipedia.org/wiki?curid=3423601", "title": "Stumpers-L", "text": "Stumpers-L\n\nThe Stumpers-L electronic mailing list, was a resource available for librarians and others to discuss reference questions which they were unable to answer using available resources. It was succeeded by the similar Project Wombat.\n\nStumpers-L began in 1992, created by Ann Feeney, a library school graduate student at Rosary College in River Forest, Illinois, in the United States. It was moved to Concordia University, Chicago, then back to Rosary, which was then renamed Dominican University. From 2002 to 2005 it was maintained by the Dominican University Graduate School of Library and Information Science program. At the end of 2005 Dominican University ceased hosting the list. A replacement list, known as Project Wombat, commenced in January 2006, and is hosted by Project Gutenberg.\n\nOriginally the Stumpers-L archive was a gopher resource, but migrated to the World Wide Web once the web became more universally used in the mid-1990s.\n\nTypical Stumpers-L topics include:\n\nA book of Stumpers-L questions and answers was published in 1998 by Random House, edited by Fred Shapiro of Yale and titled \"Stumpers! Answers to Hundreds of Questions That Stumped The Experts\" (). Shapiro was an active member; other prominent members include Barbara and David P. Mikkelson, the co-editors of \"Snopes.com.\n\nThe unofficial mascot of the Stumpers-L list is the wombat.\n\n"}
{"id": "1007243", "url": "https://en.wikipedia.org/wiki?curid=1007243", "title": "Tertiary source", "text": "Tertiary source\n\nA tertiary source is an index or textual consolidation of primary and secondary sources. Some tertiary sources are not to be used for academic research, unless they can also be used as secondary sources, or to find other sources.\n\nDepending on the topic of research, a scholar may use a bibliography, dictionary, or encyclopedia as either a tertiary or a secondary source. This causes difficulty in defining many sources as either one type or the other.\n\nIn some academic disciplines the differentiation between a secondary and tertiary source is relative. \n\nIn the United Nations International Scientific Information System (UNISIST) model, a secondary source is a bibliography, whereas a tertiary source is a synthesis of primary sources.\n\nAs tertiary sources, encyclopedias, textbooks, and compendia attempt to summarize, collect, and consolidate the source materials into an overview, but may also present subjective, or biased commentary and analysis (which are characteristics of secondary sources).\n\nIndexes, bibliographies, concordances, and databases may not provide much textual information, but as aggregates of primary and secondary sources, they are often considered tertiary sources. So although tertiary sources are both primary and secondary, they are more towards a secondary source because of commentary and bias.\n\nAlmanacs, travel guides, field guides, and timelines are also examples of tertiary sources.\n\nSurvey or overview articles are usually tertiary, though review articles in peer-reviewed academic journals are secondary (not be confused with film, book, etc. reviews, which are primary-source opinions).\n\nSome usually primary sources, such as user guides and manuals, are secondary or tertiary (depending on the nature of the material) when written by third parties.\n\n"}
{"id": "39726999", "url": "https://en.wikipedia.org/wiki?curid=39726999", "title": "The Rough Guide to True Crime", "text": "The Rough Guide to True Crime\n\nThe Rough Guide to True Crime is a non-fiction paperback reference guide to national and international true crime cases by American crime writer Cathy Scott. It was released in the UK and US in August 2009 by Penguin Books through its Rough Guides imprint.\n\n\"The Rough Guide to True Crime\" is a compilation of a variety of cases, including historic crimes, with sections broken down by the type of offenses and who committed them. It includes black-and-white photos as illustration. Psychological profiles are included throughout by forensic expert Dr. Louis B. Schlesinger, who explains the psychology of serial killers, murderers, hit men and burglars. The book features serial killer Jeffrey Dahmer, mob hitman Richard \"The Iceman\" Kuklinski, John Wayne Glover \"The Granny Killer,\" and British \"Doctor of Death\" Harold Shipman.\n\nScott's story from \"The Rough Guide to True Crime\" about mob enforcer Herbert Blitzstein was selected for inclusion in the July 2012 retrospective of crime writing, \"Masters of True Crime: Chilling Stories of Murder and the Macabre\".\n\nThe author appeared on BlogTalkRadio's \"True Murder\" show and described some of the crimes included in the book that were committed in the 19th century as \"a different time in America, where people like Billy the Kid could walk in and just rob a bank\" and get away with it. And while \"there was nothing glamorous about what they did, they are a part of lore.\"\n\nThe book was featured at BookExpo America 2009's trade fair in DK Publishing's booth in New York City.\n\nIn a review, \"True Crime Book Reviews\" wrote, \"From the Moors murders and Harold Shipman, to the murder of 2pac, this guide illuminates the psychology in play behind the most intriguing crimes in history, from the absurd to the appalling. \"The Rough Guide to True Crime\" explores the best of the haunting genre of True Crime.\"\n\n\n"}
{"id": "270906", "url": "https://en.wikipedia.org/wiki?curid=270906", "title": "Three-letter acronym", "text": "Three-letter acronym\n\nA three-letter acronym (TLA), or three-letter abbreviation, is an abbreviation, specifically an acronym, alphabetism, or initialism, consisting of three letters. These are usually the initial letters of the words of the phrase abbreviated, and are written in capital letters (upper case); three-letter abbreviations such as \"etc.\" and \"Mrs.\" are not three-letter acronyms, but \"TLA\" is a TLA (an example of an autological abbreviation).\n\nMost three-letter abbreviations are \"initialisms\": all the letters are pronounced as the names of letters, as in \"APA\" . Some are acronyms pronounced as a word; computed axial tomography, CAT, is almost always pronounced as the animal's name in \"CAT scan\".\n\n\nThe exact phrase \"three-letter acronym\" appeared in the sociology literature in 1975. Three-letter acronyms were used as mnemonics in biological sciences, from 1977 and their practical advantage was promoted by Weber in 1982. They are used in many other fields, but the term TLA is particularly associated with computing. In 1980, the manual for the Sinclair ZX81 home computer used and explained TLA. The specific generation of three-letter acronyms in computing was mentioned in a JPL report of 1982. In 1988, in a paper titled \"On the cruelty of really teaching computer science\", eminent computer scientist Edsger W. Dijkstra wrote \n\"Because no endeavour is respectable these days without a TLA ...\" By 1992 it was in a Microsoft handbook.\n\nThe number of possible three-letter abbreviations (or permutations) using the 26 letters of the alphabet from A to Z (AAA, AAB ... to ZZY, ZZZ) is 26 × 26 × 26 = 17,576. Another 26 × 26 × 10 = 6760 can be produced if the third element is allowed to be a digit 0-9, giving a total of 24,336.\n\nIn English, WWW is the longest possible TLA to pronounce, typically requiring nine syllables. The usefulness of TLAs typically comes from how it is quicker to say the acronym instead of than the phrase they represent, however saying 'WWW' in English requires three times as many syllables than the phrase it is meant to abbreviate (World Wide Web). Consequently, \"www\" is sometimes abbreviated as \"dubdubdub\" in speech.\n\n\n"}
