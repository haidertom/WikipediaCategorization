{"id": "1171", "url": "https://en.wikipedia.org/wiki?curid=1171", "title": "Abbreviation", "text": "Abbreviation\n\nAn abbreviation (from Latin \"brevis\", meaning \"short\" ) is a shortened form of a word or phrase. It consists of a group of letters taken from the word or phrase. For example, the word \"abbreviation\" can itself be represented by the abbreviation \"abbr.\", \"abbrv.\", or \"abbrev.\"\n\nIn strict analysis, abbreviations should not be confused with contractions, crasis, acronyms, or initialisms, with which they share some semantic and phonetic functions, though all four are connected by the term \"abbreviation\" in loose parlance.An abbreviation is a shortening by any method; a contraction is a reduction of size by the drawing together of the parts. A contraction of a word is made by omitting certain letters or syllables and bringing together the first and last letters or elements; an abbreviation may be made by omitting certain portions from the interior or by cutting off a part. A contraction is an abbreviation, but an abbreviation is not necessarily a contraction. Acronyms and initialisms are regarded as subsets of abbreviations (e.g. by the Council of Science Editors). They are abbreviations that consist of the initial letters or parts of words.\n\nAbbreviations have a long history, created so that spelling out a whole word could be avoided. This might be done to save time and space, and also to provide secrecy. Shortened words were used and initial letters were commonly used to represent words in specific applications. In classical Greece and Rome, the reduction of words to single letters was common. In Roman inscriptions, \"Words were commonly abbreviated by using the initial letter or letters of words, and most inscriptions have at least one abbreviation.\" However, \"some could have more than one meaning, depending on their context. (For example, \"A\" can be an abbreviation for many words, such as \"ager\", \"amicus\", \"annus\", \"as\", \"Aulus\", \"Aurelius\", \"aurum\" and \"avus\".)\"\n\nAbbreviations in English were frequently used from its earliest days. Manuscripts of copies of the old English poem \"Beowulf\" used many abbreviations, for example \"7\" or \"&\" for \"and\", and \"y\" for \"since\", so that \"not much space is wasted\". The standardisation of English in the 15th through 17th centuries included such a growth in the use of abbreviations. At first, abbreviations were sometimes represented with various suspension signs, not only periods. For example, sequences like ‹er› were replaced with ‹ɔ›, as in ‹mastɔ› for \"master\" and ‹exacɔbate› for \"exacerbate\". While this may seem trivial, it was symptomatic of an attempt by people manually reproducing academic texts to reduce the copy time. An example from the Oxford University Register, 1503:\n\nThe Early Modern English period, between the 15th and 17th centuries, had abbreviations like \"y\" for \"Þ\", used for the word \"the\": \"hence, by later misunderstanding, Ye Olde Tea Shoppe.\"\n\nDuring the growth of philological linguistic theory in academic Britain, abbreviating became very fashionable. The use of abbreviation for the names of J. R. R. Tolkien and his friend C. S. Lewis, and other members of the Oxford literary group known as the Inklings, are sometimes cited as symptomatic of this. Likewise, a century earlier in Boston, a fad of abbreviation started that swept the United States, with the globally popular term OK generally credited as a remnant of its influence.\n\nAfter World War II, the British greatly reduced the use of the full stop and other punctuation points after abbreviations in at least semi-formal writing, while the Americans more readily kept such use until more recently, and still maintain it more than Britons. The classic example, considered by their American counterparts quite curious, was the maintenance of the internal comma in a British organisation of secret agents called the \"Special Operations, Executive\"—\"S.O., E\"—which is not found in histories written after about 1960.\n\nBut before that, many Britons were more scrupulous at maintaining the French form. In French, the period only follows an abbreviation if the last letter in the abbreviation is \"not\" the last letter of its antecedent: \"M.\" is the abbreviation for \"monsieur\" while \"Mme\" is that for \"madame\". Like many other cross-channel linguistic acquisitions, many Britons readily took this up and followed this rule themselves, while the Americans took a simpler rule and applied it rigorously.\n\nOver the years, however, the lack of convention in some style guides has made it difficult to determine which two-word abbreviations should be abbreviated with periods and which should not. The U.S. media tend to use periods in two-word abbreviations like United States (U.S.), but not personal computer (PC) or television (TV). Many British publications have gradually done away with the use of periods in abbreviations.\n\nMinimization of punctuation in typewritten material became economically desirable in the 1960s and 1970s for the many users of carbon-film ribbons since a period or comma consumed the same length of non-reusable expensive ribbon as did a capital letter.\n\nWidespread use of electronic communication through mobile phones and the Internet during the 1990s allowed for a marked rise in colloquial abbreviation. This was due largely to increasing popularity of textual communication services such as instant- and text messaging. SMS, for instance, supports message lengths of 160 characters at most (using the GSM 03.38 character set). This brevity gave rise to an informal abbreviation scheme sometimes called Textese, with which 10% or more of the words in a typical SMS message are abbreviated. More recently Twitter, a popular social networking service, began driving abbreviation use with 140 character message limits.\n\nIn modern English, there are several conventions for abbreviations, and the choice may be confusing. The only rule universally accepted is that one should be \"consistent\", and to make this easier, publishers express their preferences in a style guide. Questions which arise include those in the following subsections.\n\nIf the original word was capitalized then the first letter of its abbreviation should retain the capital, for example Lev. for \"Leviticus\". When a word is abbreviated to more than a single letter and was originally spelled with lower case letters then there is no need for capitalization. However, when abbreviating a phrase where only the first letter of each word is taken, then all letters should be capitalized, as in YTD for \"year-to-date\", PCB for \"printed circuit board\" and FYI for \"for your information\". However, see the following section regarding abbreviations that have become common vocabulary: these are no longer written with capital letters.\n\nA period (full stop) is often used to signify an abbreviation, but opinion is divided as to when and if this should happen.\n\nAccording to Hart's Rules, the traditional rule is that abbreviations (in the narrow sense that includes only words with the ending, and not the middle, dropped) terminate with a full stop, whereas contractions (in the sense of words missing a middle part) do not, but there are exceptions. Fowler's Modern English Usage says full stops are used to mark both abbreviations and contractions, but recommends against this practice: advising them only for abbreviations and lower-case initialisms and not for upper-case initialisms and contractions.\n\nIn American English, the period is usually included regardless of whether or not it is a contraction, e.g. \"Dr.\" or \"Mrs.\". In some cases, periods are optional, as in either \"US\" or \"U.S.\" for \"United States\", \"EU\" or \"E.U.\" for \"European Union\", and \"UN\" or \"U.N.\" for \"United Nations\". There are some house styles, however—American ones included—that remove the periods from almost all abbreviations. For example:\n\nAcronyms that were originally capitalized (with or without periods) but have since entered the vocabulary as generic words are no longer written with capital letters nor with any periods. Examples are sonar, radar, lidar, laser, snafu, and scuba.\n\nToday, spaces are generally not used between single-letter abbreviations of words in the same phrase, so one almost never encounters \"U. S.\"\n\nWhen an abbreviation appears at the end of a sentence, only one period is used: \"The capital of the United States is Washington, D.C\".\n\nThere is a question about how to pluralize abbreviations, particularly acronyms. Often a writer will add an 's' following an apostrophe, as in \"PC's\". However, this style is not preferred by many style guides. For instance, Kate Turabian, writing about style in academic writings, allows for an apostrophe to form plural acronyms \"only when an abbreviation contains internal periods or both capital and lowercase letters\". Turabian would therefore prefer \"DVDs\" and \"URLs\" and \"Ph.D.'s\", while the Modern Language Association explicitly says, \"do not use an apostrophe to form the plural of an abbreviation\". Also, the American Psychological Association specifically says, \"without an apostrophe\".\n\nHowever, the 1999 style guide for \"The New York Times\" states that the addition of an apostrophe is necessary when pluralizing all abbreviations, preferring \"PC's, TV's and VCR's\".\n\nFollowing those who would generally omit the apostrophe, to form the plural of run batted in, simply add an s to the end of RBI.\n\n\nFor all other rules, see below:\n\nTo form the plural of an abbreviation, a number, or a capital letter used as a noun, simply add a lowercase \"s\" to the end. Apostrophes following decades and single letters are also common.\n\nTo indicate the plural of the abbreviation or symbol of a unit of measure, the same form is used as in the singular.\n\nWhen an abbreviation contains more than one full point, \"Hart's Rules\" recommends putting the \"s\" after the final one.\nHowever, subject to any house style or consistency requirement, the same plurals may be rendered less formally as:\n\nAccording to \"Hart's Rules\", an apostrophe may be used in rare cases where clarity calls for it, for example when letters or symbols are referred to as objects.\nHowever, the apostrophe can be dispensed with if the items are set in italics or quotes:\n\nIn Latin, and continuing to the derivative forms in European languages as well as English, single-letter abbreviations had the plural being a doubling of the letter for note-taking. Most of these deal with writing and publishing. A few longer abbreviations use this as well.\n\nPublications based in the U.S. tend to follow the style guides of \"The Chicago Manual of Style\" and the Associated Press. The U.S. Government follows a style guide published by the U.S. Government Printing Office. The National Institute of Standards and Technology sets the style for abbreviations of units.\n\nMany British publications follow some of these guidelines in abbreviation:\n\n\nWriters often use shorthand to denote units of measure. Such shorthand can be an abbreviation, such as \"in\" for \"inch\" or can be a symbol such as \"km\" for \"kilometre/kilometer\".\n\nThe shorthand \"in\" applies to English only—in Afrikaans for example, the shorthand \"dm\" is used for the equivalent Afrikaans word \"duim\". Since both \"in\" and \"dm\" are contractions of the same word, but in different languages, they are abbreviations. A symbol on the other hand, defined as \"Mark or character taken as the conventional sign of some object or idea or process\" applies the appropriate shorthand by \"substitution\" rather than by \"contraction\". Since the shorthand for kilometre/kilometer (\"\" in Portuguese or \"\" in Greek) is \"km\" in both languages and the letter \"k\" does not appear in the expansion of either translation, \"km\" is a symbol as it is a substitution rather than a contraction. It is a logogram rather than an abbreviation.\n\nIn the International System of Units (SI) manual the word \"symbol\" is used consistently to define the shorthand used to represent the various SI units of measure. The manual also defines the way in which units should be written, the principal rules being:\n\nA syllabic abbreviation is usually formed from the initial syllables of several words, such as \"Interpol\" = International\" + police\". It is a variant of the acronym. Syllabic abbreviations are usually written using lower case, sometimes starting with a capital letter, and are always pronounced as words rather than letter by letter. Syllabic abbreviations should be distinguished from portmanteaus, which combine two words without necessarily taking whole syllables from each.\n\nSyllabic abbreviations are not widely used in English. Some UK government ministries such as Ofcom (Office of Communications\") and Oftel (Office of Telecommunications\") use this style.\n\nNew York City has various neighborhoods named by syllabic abbreviation, such as Tribeca (Triangle below Canal Street\") and SoHo (South of Houston Street\"). This usage has spread into other American cities, giving SoMa, San Francisco (South of Market\") and LoDo, Denver (Lower Downtown\"), among others.\n\nOn the other hand, syllabic abbreviations prevailed both in Germany under the Nazis and in the Soviet Union for naming the plethora of new bureaucratic organisations. For example, \"Gestapo\" stands for Geheime Staats-Polizei\", or \"secret state police\". Similarly, Leninist organisations such as the \"Comintern\" (\"Communist International\") and \"Komsomol\" (Kommunisticheskii Soyuz Molodyozhi\", or \"Communist youth union\") used Russian language syllabic abbreviations. This has given syllabic abbreviations negative connotations in some countries, (as in Orwell's Newspeak), notwithstanding that such abbreviations were used in Germany even before the Nazis came to power, e.g., \"\" for \"Schutzpolizei\", and are still used, e.g. \"\" for \"\".\n\nIn the modern Russian language words like \"Minoborony\" (from Ministerstvo oborony — Ministry of Defence) and \"Minobrnauki\" (from Ministerstvo obrazovaniya i nauki — Ministry of Education and Science) are still commonly used.\n\nSyllabic abbreviations were also typical for the German language used in the German Democratic Republic, e.g. \"Stasi\" for Staatssicherheit\" (\"state security\", the secret police) or \"Vopo\" for \"Volkspolizist\" (\"people's policeman\"). Other uses are in company or product names such as Aldi, from the name of the founder, Theo Albrecht, and the German word Diskont\" (discount) or Haribo, from the name of the founder and the headquarters of the company, Hans Riegl Bonn.\n\nSyllabic abbreviations are \"de rigueur\" in Spanish; examples abound in organization names such as Pemex for Petróleos Mexicanos\" (\"Mexican Petroleums\") or Fonafifo for Fondo Nacional de Financimiento Forestal\" (National Forestry Financing Fund).\n\nEast Asian languages whose writing systems use Chinese characters form abbreviations similarly by using key Chinese characters from a term or phrase. For example, in Japanese the term for the United Nations, \"kokusai rengō\" (国際連合) is often abbreviated to \"kokuren\" (国連). (Such abbreviations are called (略語) in Japanese; see also Japanese abbreviated and contracted words). The syllabic abbreviation is frequently used for universities: for instance, \"Běidà\" (北大) for \"Běijīng Dàxué\" (北京大学, Peking University) and \"Tōdai\" (東大) for \"Tōkyō daigaku\" (東京大学, University of Tokyo). The English phrase \"Gung ho\" originated as a Chinese abbreviation.\n\nPartially syllabic abbreviations are preferred by the US Navy, as it increases readability amidst the large number of initialisms that would otherwise have to fit into the same acronyms. Hence \"DESRON 6\" is used (in the full capital form) to mean \"Destroyer Squadron 6\", while \"COMNAVAIRLANT\" would be \"Commander, Naval Air Force (in the) Atlantic.\"\n\n"}
{"id": "1230569", "url": "https://en.wikipedia.org/wiki?curid=1230569", "title": "Acronym Finder", "text": "Acronym Finder\n\nAcronym Finder (AF) is a free online searchable dictionary and database of abbreviations (acronyms, initialisms, and others) and their meanings.\n\nThe entries are classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes. For abbreviations with multiple meanings they are listed by popularity with the most common one being listed first. it claims to have over a million \"human-edited\" and verified definitions.\n\nAcronym Finder was registered and the database put online by Michael K. Molloy of Colorado in 1997 but he began compiling it in 1985 working as a computer systems officer for the USAF. Molloy first saw the need of an acronym list while integrating computers at the Randolph Air Force Base in Texas His first acronym list running up-to 30 pages. When he had retired and put AF online in 1997, his list already had 43,000 acronyms. It began mainly as a list of Military/Government abbreviations before expanding to other areas.\n\nMolloy and his wife served as the editors of the website verifying user submissions for abbreviations and adding others they found to the database. Molloy has also provided opinions on abbreviations such as \"MSG\" which Madison Square Garden wanted as a domain name (\"msg.com\") claiming trademark to the abbreviated letters. He stated that MSG also stood for more common things such as monosodium glutamate and message among others. The Garden in the end settled out of court and came to own msg.com.\n\nThe website was maintained under Mountain Data Systems, LLC by Molloy before being sold off and eventually coming under the ownership of Farlex, Inc. publishers of Thefreedictionary.com.\n\nThe website contains a database of meanings and expansions for abbreviations, acronyms, initialisms mainly in English but includes some entries in other languages such as French, German, Spanish etc. as well. It is freely accessible. The entries are further classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes which are shown on a Map along with location information. Abbreviations with multiple expansions are listed by popularity with the most common one being presented first, these can be sorted alphabetically as well.\n\nAnyone can contribute to the database by submitting abbreviations and their meanings, these are reviewed by an by editor and categorized before being added to the database. While the database has been described as fairly accurate errors have been found in the meanings and expansions of abbreviations. The website does not list sources for the abbreviations and their meanings but it does identify people who have contributed more than 50 abbreviations to the database.\n\nThe database only contains abbreviations and their expansions and does not list other data such as grammatical category, context, source, field of the abbreviation etc.\n\nFarlex, Inc. the current owner of the website also publishes mobile apps for the Android and iOS operating systems.\n\nAcronym Finder also includes a \"Systematic Buzz Phrase Projector\", a light-hearted tool that randomly generates jargon-like phrases and abbreviations — usually initialisms that would be unpronounceable as acronyms — and meanings from 30 cleverly chosen buzz words.\n\nThe website is supported through advertisements.\n\nThe website is listed as a quick reference tool in directories like Stanford Library, Library of Congress, USC Library. It has been cited as the largest database of acronyms and has been used in computational studies for its database.\n\nListings of abbreviations on the website have also been used as a defense that an abbreviation is in public use and cannot be trademarked. While in some trademark cases citations for AF have been accepted it has been described as an unreliable reference in others.\n\nIt has garnered criticism for the fact that anyone can submit abbreviations to the site and the content is user generated. Mike Molloy the site's original owner had defended that each submission is verified before being added to the database.\n\n"}
{"id": "26334944", "url": "https://en.wikipedia.org/wiki?curid=26334944", "title": "Auxiliary sciences of history", "text": "Auxiliary sciences of history\n\nAuxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. \"History\" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.\n\nAuxiliary sciences of history include, but are not limited to:\n\n"}
{"id": "46842956", "url": "https://en.wikipedia.org/wiki?curid=46842956", "title": "Bekker numbering", "text": "Bekker numbering\n\nBekker numbering or Bekker pagination is the standard form of citation to the works of Aristotle. It is based on the page numbers used in the Prussian Academy of Sciences edition of the complete works of Aristotle and takes its name from the editor of that edition, the classical philologist August Immanuel Bekker (1785-1871); because the Academy was located in Berlin, the system is occasionally referred to by the alternative name Berlin numbering or Berlin pagination.\n\nBekker numbers consist of up to three ordered coordinates, or pieces of information: a number, the letter a or b, and another number, which refer respectively to the page number of Bekker's edition of the Greek text of Aristotle's works, the page column (a standard page of Bekker's edition has exactly two columns), and the line number (total lines typically ranging from 20-40 on a given column or page in Bekker's edition). For example, the Bekker number denoting the beginning of Aristotle's Nicomachean Ethics is \"1094a1\", which corresponds to page 1094 of Bekker's edition, first column (column a), line 1.\n\nAll modern editions or translations of Aristotle intended for scholarly readers use Bekker numbers, in addition to or instead of page numbers. Contemporary scholars writing on Aristotle use the Bekker number so that the author's citations can be checked by readers without having to use the same edition or translation that the author used.\n\nWhile Bekker numbers are the dominant method used to refer to the works of Aristotle, Catholic or Thomist scholars often use the medieval method of reference by book, chapter, and sentence, albeit generally in addition to Bekker numbers.\n\nStephanus pagination is the comparable system for referring to the works of Plato, and Diels-Kranz numbering is the comparable system for Pre-Socratic philosophy. Unlike Stephanus pagination, which is based upon a three-volume translation of Plato's works and which recycles low page numbers across the three volumes, introducing the possibility for ambiguity if the Platonic work or volume is not specified, Bekker page numbers cycle from 1 through the end of the \"Corpus Aristotelicum\" regardless of volume, without starting over for some other given volume. Bekker numbering therefore has the advantage that its notation is unambiguous as compact numerical information, although it relies upon the ordering of Aristotle's works as presented in Bekker's edition.\n\nThe following list is complete. The titles are given in accordance with the standard set by the Revised Oxford Translation. Latin titles, still often used by scholars, are also given.\n\nThe \"Constitution of the Athenians\" (or \"Athenaiōn Politeia\") was not included in Bekker's edition because it was first edited in 1891 from papyrus rolls acquired in 1890 by the British Museum. The standard reference to it is by section (and subsection) numbers.\n\nSurviving fragments of the many lost works of Aristotle were included in the fifth volume of Bekker's edition, edited by Valentin Rose. These are not cited by Bekker numbers, however, but according to fragment numbers. Rose's first edition of the fragments of Aristotle was \"Aristoteles Pseudepigraphus\" (1863). As the title suggests, Rose considered these all to be spurious. The numeration of the fragments in a revised edition by Rose, published in the Teubner series, \"Aristotelis qui ferebantur librorum fragmenta\", Leipzig, 1886, is still commonly used (indicated by \"R\"), although there is a more current edition with a different numeration by Olof Gigon (published in 1987 as a new vol. 3 in Walter de Gruyter's reprint of the Bekker edition), and a new de Gruyter edition by Eckart Schütrumpf is in preparation.\n\nFor a selection of the fragments in English translation, see W.D. Ross, \"Select Fragments\" (Oxford 1952), and Jonathan Barnes (ed.), \"The Complete Works of Aristotle: The Revised Oxford Translation\", vol. 2, Princeton 1984, pp. 2384–2465.\n\nThe works surviving only in fragments include the dialogues \"On Philosophy\" (or \"On the Good\"), \"Eudemus\" (or \"On the Soul\"), \"On Justice\", and \"On Good Birth\". The possibly spurious work, \"On Ideas\" survives in quotations by Alexander of Aphrodisias in his commentary on Aristotle's \"Metaphysics\". For the dialogues, see also the editions of Richard Rudolf Walzer, \"Aristotelis Dialogorum fragmenta, in usum scholarum\" (Florence 1934), and Renato Laurenti, \"Aristotele: I frammenti dei dialoghi\" (2 vols.), Naples: Luigi Loffredo, 1987.\n\n"}
{"id": "3181897", "url": "https://en.wikipedia.org/wiki?curid=3181897", "title": "Brand Book", "text": "Brand Book\n\nA Brand Book records all livestock brands registered with an organization. In the U.S. most states have branding laws that require brands to be registered before use. This may be a state agency (usually affiliated with each state's Department of Agriculture) or a private association regulated by the state. Most states with such laws have a Brand Book for the entire state. Texas, an exception, registers brands at the county level. These book are usually provided free to law enforcement personnel and County Extension Agents. Some states have their Brand Books available online.\n\nA typical Brand Book will usually have an image of the brand, the location of the brand on the animal, and the type of animal that will be branded, as well as the owner of the brand. Many Brand Books also record earmarks.\n\nBrand Books are used by law enforcement officials, brand inspectors, and association investigators to record and track livestock movement, deter loss of livestock by straying or theft, and prosecute thieves.\n\n\n"}
{"id": "27778631", "url": "https://en.wikipedia.org/wiki?curid=27778631", "title": "Breviograph", "text": "Breviograph\n\nA breviograph or brevigraph (from , short, and Greek \"grapho\", to write) is a type of scribal abbreviation in the form of an easily written symbol, character, flourish or stroke, based on a modified letter form to take the place of a common letter combination, especially those occurring at the beginning or end of a word. Breviographs were used frequently by stenographers, law clerks and scriveners, and they were also found in early printed books and tracts. Their use declined after the 17th century.\n\nExamples of breviographs:\n\n\n"}
{"id": "744504", "url": "https://en.wikipedia.org/wiki?curid=744504", "title": "Circular reference", "text": "Circular reference\n\nA circular reference is a series of references where the last object references the first, resulting in a closed loop.\nA circular reference is not to be confused with the logical fallacy of a circular argument. Although a circular reference will often be unhelpful and reveal no information, such as two entries in a book index referring to each other, it is not necessarily so that a circular reference is of no use. Dictionaries, for instance, must always ultimately be a circular reference since all words in a dictionary are defined in terms of other words, but a dictionary nevertheless remains a useful reference. Sentences containing circular references can still be meaningful;\n\nis circular but not without meaning. Indeed, it can be argued that self-reference is a necessary consequence of Aristotle's Law of non-contradiction, a fundamental philosophical axiom. In this view, without self-reference, logic and mathematics become impossible, or at least, lack usefulness.\n\nCircular references can appear in computer programming when one piece of code requires the result from another, but that code needs the result from the first. For example:\n\nFunction A will show the time the sun last set based on the current date, which it can obtain by calling Function B. Function B will calculate the date based on the number of times the moon has orbited the earth since the last time Function B was called. So, Function B asks Function C just how many times that is. Function C doesn't know, but can figure it out by calling Function A to get the time the sun last set.\n\nThe entire set of functions is now worthless because none of them can return any useful information whatsoever. This leads to what is technically known as a livelock. It also appears in spreadsheets when two cells require each other's result. For example, if the value in Cell A1 is to be obtained by adding 5 to the value in Cell B1, and the value in Cell B1 is to be obtained by adding 3 to the value in Cell A1, no values can be computed. (Even if the specifications are A1:=B1+5 and B1:=A1-5, there is still a circular reference. It doesn't help that, for instance, A1=3 and B1=-2 would satisfy both formulae, as there are infinitely many other possible values of A1 and B1 that can satisfy both instances.)\n\nA circular reference represents a big problem in computing.\n\nIn ISO Standard SQL circular integrity constraints are implicitly supported within a single table. Between multiple tables circular constraints (e.g. foreign keys) are permitted by defining the constraints as deferrable (See CREATE TABLE for PostgreSQL and DEFERRABLE Constraint Examples for Oracle). In that case the constraint is checked at the end of the transaction not at the time the DML statement is executed. To update a circular reference two statements can be issued in a single transaction that will satisfy both references once the transaction is committed.\n\nA distinction should be made with processes containing a circular reference between those that are incomputable and those that are an iterative calculation with a final output. The latter may fail in spreadsheets not equipped to handle them but are nevertheless still logically valid.\n\nCircular reference in worksheets can be a very useful technique for solving implicit equations such as the Colebrook equation and many others, which might otherwise require tedious Newton-Raphson algorithms in VBA or use of macros.\n\n"}
{"id": "46902218", "url": "https://en.wikipedia.org/wiki?curid=46902218", "title": "Citation network", "text": "Citation network\n\nCitation Network is a social network which contains paper sources and linked by co-citation relationships. Egghe & Rousseau once (1990, p. 228) explain \"when a document \"d\" cites a document \"d\", we can show this by an arrow going from the node representing \"d\" to the document representing \"d\". In this way the documents from a collection D form a directed graph, which is called a 'citation graph' or 'citation network' \".\n\nCitation is a reference to a published or unpublished source (not always the original source). More precisely, a citation is an abbreviated alphanumeric expression embedded in the body of an intellectual work that denotes an entry in the bibliographic references section of the work for the purpose of acknowledging the relevance of the works of others to the topic of discussion at the spot where the citation appears. Generally the combination of both the in-body citation and the bibliographic entry constitutes what is commonly thought of as a citation (whereas bibliographic entries by themselves are not). References to single, machine-readable assertions in electronic scientific articles are known as nanopublications, a form of microattribution.\nCitation networks, the principal focus of this study, are one kind of social networks that have been studied quantitatively almost from the moment citation databases first became available. In 1965, Derek J. de Solla Price described the inherent linking characteristic of the SCI in his seminal paper titled \"Networks of Scientific Papers\". The links between citing and cited papers became dynamic when the SCI began to be published online. In 1973, Henry Small published his work on co-citation analysis which became a self-organizing classification system that led to document clustering experiments and eventually what is called \"Research Reviews\".\n\n"}
{"id": "17077434", "url": "https://en.wikipedia.org/wiki?curid=17077434", "title": "Comparative Toxicogenomics Database", "text": "Comparative Toxicogenomics Database\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool launched in November 2004 that curates scientific data describing relationships between chemicals/drugs, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules.\nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool that curates scientific data describing relationships between chemicals, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules, launched on November 12, 2004. \nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nOne of the primary goals of CTD is to advance the understanding of the effects of environmental chemicals on human health on the genetic level, a field called toxicogenomics.\n\nThe etiology of many chronic diseases involves interactions between environmental factors and genes that modulate important physiological processes. Chemicals are an important component of the environment. Conditions such as asthma, cancer, diabetes, hypertension, immunodeficiency, and Parkinson's disease are known to be influenced by the environment; however, the molecular mechanisms underlying these correlations are not well understood. CTD may help resolve these mechanisms. The most up-to-date extensive list of peer-reviewed scientific articles about CTD is available at their publications page\n\nCTD is a unique resource where biocurators read the scientific literature and manually curate four types of core data:\n\n\nBy integrating the above four data sets, CTD automatically constructs putative chemical-gene-phenotype-disease networks to illuminate molecular mechanisms underlying environmentally-influenced diseases.\n\nThese inferred relationships are statistically scored and ranked and can be used by scientists and computational biologists to generate and verify testable hypotheses about toxicogenomic mechanisms and how they relate to human health.\n\nUsers can search CTD to explore scientific data for chemicals, genes, diseases, or interactions between any of these three concepts. Currently, CTD integrates toxicogenomic data for vertebrates and invertebrates.\n\nCTD integrates data from or hyperlinks to these databases:\n\n"}
{"id": "1809113", "url": "https://en.wikipedia.org/wiki?curid=1809113", "title": "Comparative biology", "text": "Comparative biology\n\nComparative biology uses natural variation and disparity to understand the patterns of life at all levels—from genes to communities—and the critical role of organisms in ecosystems. Comparative biology is a cross-lineage approach to understanding the phylogenetic history of individuals or higher taxa and the mechanisms and patterns that drives it. Comparative biology encompasses Evolutionary Biology, Systematics, Neontology, Paleontology, Ethology, Anthropology, and Biogeography as well as historical approaches to Developmental biology, Genomics, Physiology, Ecology and many other areas of the biological sciences.The comparative approach also has numerous applications in human health, genetics, biomedicine, and conservation biology. The biological relationships (phylogenies, pedigree) are important for comparative analyses and usually represented by a phylogenetic tree or cladogram to differentiate those features with single origins (Homology) from those with multiple origins (Homoplasy).\n\n"}
{"id": "14308145", "url": "https://en.wikipedia.org/wiki?curid=14308145", "title": "Comparative bullet-lead analysis", "text": "Comparative bullet-lead analysis\n\nComparative bullet-lead analysis (CBLA), also known as compositional bullet-lead analysis, is a now discredited and abandoned forensic technique which used chemistry to link crime scene bullets to ones possessed by suspects on the theory that each batch of lead had a unique elemental makeup.\n\nThe technique was first used after U.S. President John F. Kennedy's assassination in 1963. From the early 1980s through 2004 the US Federal Bureau of Investigation conducted about 2,500 analyses on cases submitted by law-enforcement groups. The results of these analyses had often been questioned by defence lawyers and the press, so the FBI finally asked the United States National Academy of Science's Board on Science, Technology, and Economic Policy to research the scientific merit of the process.\n\nIn 2004 the Board's study was summarized in \"Forensic Analysis: Weighing Bullet Lead Evidence.\" The Board determined that the chemical analyses were being performed correctly and were probably sufficient to determine correlation between two bullets from separate sources (the analysis used plasma-optical emission spectroscopy to identify trace elements in the bullets). The report also concluded that the seven trace elements selected for the analyses (arsenic, antimony, tin, copper, bismuth, silver and cadmium) are acceptable for sample correlation. The report finally concluded that the procedure is the best available method for such correlations. The greatest caveat in the report was that the statistical tests as applied by the FBI could cause confusion and misinterpretation when transmitted to prosecutors or when explained to a trial jury. Because of the significance of this weakness, the report concluded that the analysis should be used with caution. This report helped the FBI decide in 2004 to voluntarily cease offering the analysis to law-enforcement entities. The National Academy of Sciences never required that the FBI stop using the test.\n\n\"CNN PRESENTS Encore Presentation: Reasonable Doubt\" examined the unreliability of this technique. It has been discontinued as of September 1, 2005.\n\nThe U.S. government has fought releasing the list of the estimated 2,500 cases over three decades in which it performed the analysis, which may have led to false convictions. According to the FBI, only 20% of the 2,500 tests performed introduced the CBLA results into evidence at trial.\n\nOn 17 December 2008, Jimmy Ates was released from a Florida prison after serving ten years on the conviction of having murdered his wife, a conviction obtained largely on the strength of a bullet-lead analysis. His conviction was overturned as a consequence of the 2004 report.\n\n"}
{"id": "12185843", "url": "https://en.wikipedia.org/wiki?curid=12185843", "title": "Comparative cognition", "text": "Comparative cognition\n\nComparative cognition is the comparative study of the mechanisms and origins of cognition in various species, and is sometimes seen as more general than, or similar to, comparative psychology.\nFrom a biological point of view, work is being done on the brains of fruit flies that should yield techniques precise enough to allow an understanding of the workings of the human brain on a scale appreciative of individual groups of neurons rather than the more regional scale previously used. Similarly, gene activity in the human brain is better understood through examination of the brains of mice by the Seattle-based Allen Institute for Brain Science (see link below), yielding the freely available Allen Brain Atlas. This type of study is related to comparative cognition, but better classified as one of comparative genomics. Increasing emphasis in psychology and ethology on the biological aspects of perception and behavior is bridging the gap between genomics and behavioral analysis.\n\nIn order for scientists to better understand cognitive function across a broad range of species they can systematically compare cognitive abilities between closely and distantly related species Through this process they can determine what kinds of selection pressure has led to different cognitive abilities across a broad range of animals. For example, it has been hypothesized that there is convergent evolution of the higher cognitive functions of corvids and apes, possibly due to both being omnivorous, visual animals that live in social groups.\n\n\n"}
{"id": "2466507", "url": "https://en.wikipedia.org/wiki?curid=2466507", "title": "Comparative sociology", "text": "Comparative sociology\n\nComparative sociology involves comparison of the social processes between nation states, or across different types of society (for example capitalist and socialist). There are two main approaches to comparative sociology: some seek similarity across different countries and cultures whereas others seek variance. For example, structural Marxists have attempted to use comparative methods to discover the general processes that underlie apparently different social orderings in different societies. The danger of this approach is that the different social contexts are overlooked in the search for supposed universal structures.\n\nOne sociologist who employed comparative methods to understand variance was Max Weber, whose studies attempted to show how differences between cultures explained the different social orderings that had emerged (see for example \"The Protestant Ethic and the Spirit of Capitalism\" and Sociology of religion).\n\nThere is some debate within sociology regarding whether the label of 'comparative' is suitable. Emile Durkheim argued in \"The Rules of Sociological Method\" (1895) that all sociological research was in fact comparative since social phenomenon are always held to be typical, representative or unique, all of which imply some sort of comparison. In this sense, all sociological analysis is comparative and it has been suggested that what is normally referred to as comparative research, may be more appropriately called cross-national research.\n\n"}
{"id": "1266596", "url": "https://en.wikipedia.org/wiki?curid=1266596", "title": "Comparison of Dewey and Library of Congress subject classification", "text": "Comparison of Dewey and Library of Congress subject classification\n\nThis is a conversion chart showing how the Dewey Decimal and Library of Congress Classification systems organize resources by concept, in part for the purpose of assigning . These two systems account for over 95% of the classification in United States libraries, and are used widely around the world.\n\nThe chart includes all ninety-nine second level (two-digit) DDC classes (040 is not assigned), and should include all second level (two-digit) LCC classes. Where a class in one system maps to several classes in other system, it will be listed multiple times (e.g. DDC class 551).\n\nAdditional information on these classification plans is available at:\n\n\n"}
{"id": "32639223", "url": "https://en.wikipedia.org/wiki?curid=32639223", "title": "Comparison of online charity donation services in the United Kingdom", "text": "Comparison of online charity donation services in the United Kingdom\n\nThe page is a comparison of notable online charity donation services in the UK.\n\nThe table below gives examples of the various transaction fees for a £10 donation using each organisation, assuming they claim back the tax for the charity using gift aid. (Charities may also be charged set-up fees and monthly fees as detailed above.)\n\n\n"}
{"id": "39275628", "url": "https://en.wikipedia.org/wiki?curid=39275628", "title": "Comparison of professional wrestling and mixed martial arts", "text": "Comparison of professional wrestling and mixed martial arts\n\nProfessional wrestling and mixed martial arts (Also known as MMA) both combine grappling and strikes. In MMA fights are competed over, in contrast to professional wrestling where the outcomes and moves performed are often scripted or predetermined. Despite the differences in competition, there have been people that have competed in both professional wrestling and MMA. \nProfessional wrestling could be considered a performance art which combines athletics with theatrical performance. Matches are contested on a pre-determined basis, where the fight on display is merely for entertainment purposes. Viewers are integral to a professional wrestling match, as the audience is who the action is for. Professional wrestling mixes many styles of amateur wrestling, striking and showmanship to display a fight, whilst the two performers work together to achieve a \"worked\" fight. Professional wrestling was first popularized in the 19th century Europe as a carnival attraction.\n\nMixed Martial Arts, is a hybrid of many types of physical full-contact sports; including wrestling, boxing and Martial Arts, such as Brazilian Jiu-Jitsu. The early 1990s saw the Ultimate Fighting Championship popularize the term \"Mixed Martial Arts\", for such a bout. Competitors in MMA are generally skilled in many different styles, however, it is possible to be only proficient in one type of combat. As some professional wrestling moves are simply worked versions of legitimate holds, there can be a crossover between the two.\n\nPossibly the most successful pro wrestler and MMA star is Brock Lesnar, who has won the world championships in both the WWE and UFC. Other wrestlers to have won a championship in both MMA and Pro Wrestling include Bobby Lashley (Shark Fights Heavyweight Championship as well as the Impact Global Championship), and Dan Severn who won the UFC Superfight Championship, and the NWA World Heavyweight Championship.\n\nSince MMA's rise to prominence in the 1990s, some pro wrestlers or MMA fighters have been cynical against the other's profession. This can be an attack on the sport as a whole, or acting as a fan dismissing certain actions within that sport.\n\nBelow is a list of professional wrestlers to have appeared in an official Mixed Martial Arts match; as well as their career win/loss record. All records are written as W-L-D. \n"}
{"id": "16759434", "url": "https://en.wikipedia.org/wiki?curid=16759434", "title": "Comparison of the Amundsen and Scott Expeditions", "text": "Comparison of the Amundsen and Scott Expeditions\n\nBetween December 1911 and January 1912, both Roald Amundsen (leading his South Pole expedition) and Robert Falcon Scott (leading the Terra Nova Expedition) reached the South Pole within a month of each other. But while Scott and his four companions died on the return journey, Amundsen's party managed to reach the geographic south pole first and subsequently return to their base camp at Framheim without loss of life, suggesting that they were better prepared for the expedition. The contrasting fates of the two teams seeking the same prize at the same time invites comparison.\n\nThe outcomes of the two expeditions were as follows.\n\nHistorically, several factors have been discussed and many contributing factors claimed, including:\n\nSullivan states that it was the last factor that probably was decisive. he states \"Man is a poor beast of burden, as was shown in the terrible experience of Scott, Shackleton, and Wilson in their thrust to the south of 1902–3. However, Scott relied chiefly on man-hauling in 1911–12 because ponies could not ascend the glacier midway to the Pole. The Norwegians correctly estimated that dog teams could go all the way. Furthermore, they used a simple plan, based on their native skill with skis and on dog-driving methods that were tried and true. In a similar fashion to the way the moon was reached by expending a succession of rocket stages and then casting each aside; the Norwegians used the same strategy, sacrificing the weaker animals along the journey to feed the other animals and the men themselves.\"\n\nScott and his financial backers saw the expedition as having a scientific basis, while also wishing to reach the pole. However, it was recognised by all involved that the South Pole was the primary objective (\"The Southern Journey involves the most important object of the Expedition\" – Scott), and had priority in terms of resources, such as the best ponies and all the dogs and motor sledges as well as involvement of the vast majority of the expedition personnel. Scott and his team knew the expedition would be judged on his attainment of the pole (\"The ... public will gauge the result of the scientific work of the expedition largely in accordance with the success or failure of the main object\" – Scott). He was prepared to make a second attempt the following year (1912–13) if this attempt failed and had Indian Army mules and additional dogs delivered in anticipation. In fact the mules were used by the team that discovered the dead bodies of Scott, Henry Robertson Bowers, and Edward Adrian Wilson in November 1912, but proved even less useful than the ponies, according to Cherry-Garrard.\n\nAmundsen's expedition was planned to reach the South Pole. This was a plan he conceived in 1909. Amundsen's expedition did conduct geographical work under Kristian Prestrud who conducted an expedition to King Edward VII Land while Amundsen was undertaking his attempt at the pole.\n\nAmundsen camped on the Ross Ice Shelf at the Bay of Whales which is 60 miles (96 km) closer to the pole than Scott's camp (which was 350 miles west of Amundsen). Amundsen had deduced that, as the Trans-Antarctic Mountains ran northwest to southeast then if he were to meet a mountain range on his route then the time spent at the high altitude of the Antarctic plateau would be less than Scott's.\nScott's base was at Cape Evans on Ross Island, with access to the Trans-Antarctic mountain range to the west, and was a better base for geological exploration. He had based his previous expedition in the same area. However, he knew it to be poor as a route to the pole as he had to start before sea ice melted and had suffered delay in returning while waiting for the sea ice to freeze. They also had to make detours around Ross Island and its known crevassed areas which meant a longer journey. The crossing of the Ross Ice Shelf was an onerous task for the ponies. Scott had advanced considerable stores across the ice shelf the year before to allow the ponies to carry lighter loads over the early passage across the ice. Even so, he had to delay the departure of the ponies until 1 November rather than 24 October when the dogs and motor sledges set off.\nConsequently, the Motor Party spent 6 days at the Mount Hooper Depot waiting for Scott to arrive.\n\nThe major comparison between Scott and Amundsen has focused on the choice of draft transport —dog versus pony/man-hauling. In fact Scott took dogs, ponies and three \"motor sledges\". Scott spent nearly seven times the amount of money on his motor sledges than on the dogs and horses combined. They were therefore a vital part of the expedition. Unfortunately, Scott decided to leave behind the engineer, Lieutenant Commander Reginald William Skelton who had created and trialled the motor sledges. This was due to the selection of Lieutenant E.R.G.R. \"Teddy\" Evans as the expedition's second in command. As Evans was junior in rank to Skelton, he insisted that Skelton could not come on the expedition. Scott agreed to this request and Skelton's experience and knowledge was lost. One of the original three motor sledges was a failure even before the expedition set out; the heavy sledge was lost through thin ice on unloading it from the ship. The two remaining motor sledges failed relatively early in the main expedition because of repeated faults. Skelton's experience might have been valuable in overcoming the failures.\n\nScott had used dogs on his first (Discovery) expedition and felt they had failed. On that journey, Scott, Shackleton, and Wilson started with three sledges and 13 dogs. But on that expedition, the men had not properly understood how to travel on snow with the use of dogs. The party had skis but were too inexperienced to make good use of them. As a result, the dogs travelled so fast that the men could not keep up with them. The Discovery expedition had to increase their loads to slow the dogs down. Additionally, the dogs were fed Norwegian dried fish, which did not agree with them and soon they began to deteriorate. The whole team of dogs eventually died (and were eaten), and the men took over hauling the sleds.\n\nScott's opinion was reinforced by Shackleton's experience on his Nimrod expedition that got to within of the pole. Shackleton used ponies. Scott planned to use ponies only to the base of the Beardmore Glacier (one-quarter of the total journey) and man-haul the rest of the journey. Scott's team had developed snow shoes for his ponies, and trials showed they could significantly increase daily progress. However, Lawrence Oates, whom Scott had made responsible for the ponies, was reluctant to use the snow shoes and Scott failed to insist on their use.\n\nThere was plenty of evidence that dogs could succeed in the achievements of William Speirs Bruce in his Arctic, Antarctic, and Scottish National Antarctic Expedition, Amundsen in the \"Gjøa\" North West passage expedition, Fridtjof Nansen's crossing of Greenland, Robert Peary's three attempts at the North Pole, Eivind Astrup's work supporting Peary, Frederick Cook's discredited North Pole expedition, and Otto Sverdrup's explorations of Ellesmere Island. Moreover, Scott ignored the direct advice he received (while attending trials of the motor sledges in Norway) from Nansen, the most famous explorer of the day, who told Scott to take \"dogs, dogs and more dogs\".\n\nAt the time of the events, the expert view in England had been that dogs were of dubious value as a means of Antarctic transport. Broadly speaking, Scott saw two ways in which dogs may be used—they may be taken with the idea of bringing them all back safe and sound, or they may be treated as pawns in the game, from which the best value is to be got regardless of their lives. He stated that if, and only if, the comparison was made with a dog sledge journey which aimed to preserve the dogs' lives, 'I am inclined to state my belief that in the polar regions properly organised parties of men will perform as extended journeys as teams of dogs.' On the other hand, if the lives of the dogs were to be sacrificed, then 'the dog-team is invested with a capacity for work which is beyond the emulation of men. To appreciate this is a matter of simple arithmetic'. But efficiency notwithstanding, he expressed \"reluctance\" to use dogs in this way: \"One cannot calmly contemplate the murder of animals which possess such intelligence and individuality, which have frequently such endearing qualities, and which very possibly one has learnt to regard as friends and companions.\"\n\nAmundsen, by contrast, took an entirely utilitarian approach. Amundsen planned from the start to have weaker animals killed to feed the other animals and the men themselves. He expressed the opinion that it was less cruel to feed and work dogs correctly before shooting them, than it would be to starve and overwork them to the point of collapse. Amundsen and his team had similar affection for their dogs as those expressed above by the English, but they \"also had agreed to shrink from nothing in order to achieve our goal\". The British thought such a procedure was distasteful, though they were willing to eat their ponies.\n\nAmundsen had used the opportunity of learning from the Inuit while on his \"Gjøa\" North West passage expedition of 1905. He recruited experienced dog drivers. To make the most of the dogs he paced them and deliberately kept daily mileages shorter than he need have for 75 percent of the journey, and his team spent up to 16 hours a day resting. His dogs could eat seals and penguins hunted in the Antarctic while Scott's pony fodder had to be brought all the way from England in their ship. It has been later shown that seal meat with the blubber attached is the ideal food for a sledge dog. Amundsen went with 52 dogs, and came back with 11.\n\nWhat Scott did not realise is a sledge dog, if it is to do the same work as a man, will require the same amount of food. Furthermore, when sledge dogs are given insufficient food they become difficult to handle. The advantage of the sledge dog is its greater mobility. Not only were the Norwegians accustomed to skiing, which enabled them to keep up with their dogs, but they also understood how to feed them and not overwork them.\n\nScott took the Norwegian pilot and skier Tryggve Gran to the Antarctic on the recommendation of Nansen to train his expedition to ski, but although a few of his party began to learn, he made no arrangements for compulsory training for the full party. Gran (possibly because he was Norwegian) was not included in the South Pole party, which could have made a difference. Gran was, one year later, the first to locate the deceased Scott and his remaining companions in their tent just some 18 km (11 miles) short of One Ton depot, that might have saved their lives had they reached it.\n\nScott would subsequently complain in his diary, while well into his journey and therefore too late to take any corrective action and after over 10 years since the Discovery expedition, that \"Skis are the thing, and here are my tiresome fellow countrymen too prejudiced to have prepared themselves for the event\".\n\nAmundsen on his side recruited a team of well experienced skiers, all Norwegians who had skied from an early age. He also recruited a champion skier, Olav Bjaaland, as the front runner. The Amundsen party gained weight on their return travel from the South Pole.\n\nScott and Shackleton's experience in 1903 and 1907 gave them first-hand experience of average conditions in Antarctica. Simpson, Scott's meteorologist 1910–1912, charted the weather during their expedition, often taking two readings a day. On their return to the Ross Ice Shelf, Scott's group experienced prolonged low temperatures from 27 February until 10 March which have only been matched once in 15 years of current records. The exceptional severity of the weather meant they failed to make the daily distances they needed to get to the next depot. This was a serious position as they were short of fuel and food. When Scott, Wilson, and Bowers died (Petty Officer Edgar Evans and Lawrence Oates had died earlier during the return from the South Pole) they were short of One-Ton Depot, which was from Corner Camp, where they would have been safe.\n\nOn the other hand, Cherry-Garrard had travelled nearly in the same area, during the same time period and same temperatures, using a dog team. Scott also blamed \"a prolonged blizzard\". But while there is evidence to support the low temperatures, there is only evidence for a \"normal\" two- to four-day blizzard, and not the ten days that Scott claims.\n\nDuring depot laying in February 1911, Roald Amundsen had his first (and last) of his route marked like a Norwegian ski course using marker flags initially every eight miles. He added to this by using food containers painted black, resulting in a marker every mile. From 82 degrees on, Amundsen built a cairn every three miles with a note inside recording the cairn's position, the distance to the next depot, and direction to the next cairn. In order not to miss a depot considering the snow and great distances, Amundsen took precautions. Each depot laid out up to 85 degrees (laid out every degree of latitude) had a line of bamboo flags laid out transversely every half-mile for five miles on either side of the depot, ensuring that the returning party could locate the designated depot.\n\nScott relied on depots much less frequently laid out. For one distance where Amundsen laid seven depots, Scott laid only two. Routes were marked by the walls made at lunch and evening stops to protect the ponies. Depots had a single flag. As a result, Scott has much concern recorded in his diaries over route finding, and experienced close calls about finding depots. It is also clear that Scott's team did not travel on several days, because the swirling snow hid their three-month-old outward tracks. With better depot and route marking they would have been able to travel on more days with a following wind which would have filled the sail attached to their sledge, and so travel further, and might have reached safety.\n\nBy the time they arrived at the pole, the health of Scott's team had significantly deteriorated, whereas Amundsen's team actually gained weight during the expedition. While Scott's team managed to maintain the scheduled pace for most of the return leg, and hence was virtually always on full rations, their condition continued to worsen rapidly. (The only delay occurred when they were held for four days by a blizzard, and had to open their summit rations early as a consequence.)\n\nApsley Cherry-Garrard in his analysis of the expedition estimated that even under optimistic assumptions the summit rations contained only a little more than half the calories actually required for the man-hauling of sledges. A carefully planned 2006 re-enactment of both Amundsen's and Scott's travels, sponsored by the BBC, confirmed Cherry-Garrard's theory. The British team had to abort their tour due to the severe weight loss of all members. The experts hinted that Scott's reports of unusually bad surfaces and weather conditions might in part have been due to their exhausted state which made them feel the sledge weights and the chill more severely.\n\nScott's calculations for the supply requirements were based on a number of expeditions, both by members of his team (e.g., Wilson's trip with Cherry-Garrard and Bowers to the Emperor penguin colony which had each man on a different type of experimental ration), and by Shackleton. Apparently, Scott didn't take the strain of prolonged man-hauling at high altitudes sufficiently into account.\n\nSince the rations contained no B and C vitamins, the only source of these vitamins during the trek was from the slaughter of ponies or dogs. This made the men progressively malnourished, manifested most clearly in the form of scurvy.\n\nScott also had to fight with a shortage of fuel due to leakage from stored fuel cans which used leather washers. This was a phenomenon that had been noticed previously by other expeditions, but Scott took no measures to prevent it. Amundsen, in contrast, had learned the lesson and had his fuel cans soldered closed. A fuel depot he left on Betty's Knoll was found 50 years later still full.\n\nDehydration may also have been a factor. Amundsen's team had plenty of fuel due to better planning and soldered fuel cans. Scott had a shortage of fuel and was unable to melt as much water as Amundsen. At the same time Scott's team were more physically active in man-hauling the sledges.\n\nIt has been said (by the present-day explorer Ranulph Fiennes amongst others) that Scott's team was appropriately dressed for man-hauling in their woolen and wind-proof clothing, and as Amundsen was skiing it was appropriate he wore furs. Skiing at the pace of a dog team is a strenuous activity. Yet Amundsen never complained about the clothing being too hot. That is because the furs are worn loosely so air circulates and sweat evaporates. Scott's team, on the other hand, made regular complaints about the cold.\n\nAmundsen's team did initially have problems with their boots. However, the depot-laying trips of January and February 1911 and an abortive departure to the South Pole on 8 September 1911 allowed changes to be made before it was too late.\n\nScott's team suffered regularly from snow blindness and sometimes this affected over half the team at any one time. By contrast, there was no recorded case of snow blindness during the whole of Amundsen's expedition. On the return journey, Amundsen's team rested during the \"day\" (when the sun was in front of them) and travelled during the \"night\" (when the sun was behind them) to minimise the effects of snow blindness.\n\nIn 1921, 'Teddy' Evans wrote in his book \"South with Scott\" that Scott had left the following written orders at Cape Evans.\n\nHe did however place a lesser importance upon this journey than that of replenishing the food rations at One Ton Depot.\n\nHe continued his instructions in the next paragraph \"You will of course understand that whilst the object of your third journey is important, that of the second is vital. At all hazards three X.S. units of provision must be got to One Ton Camp by the date named (19th January), and if the dogs are unable to perform this task, a man party must be organised.\" with that qualification he closed his notes regarding his instructions for the dogs.\n\nExpedition member Apsley Cherry-Garrard did not mention Scott's order in his 1922 book \"The Worst Journey in the World\". However, in the 1948 preface to his book, he discusses Scott's order. Cherry-Garrard writes that he and Edward Atkinson reached Cape Evans on 28 January. Scott had estimated Atkinson would reach camp by 13 January. Atkinson, now the senior officer discovered that the dog handler Cecil Meares had resigned from the expedition and that neither Meares nor anyone else had resupplied dog food to the depots. Cherry-Garrard also wrote \"In my opinion he [Atkinson] would not have been fit to take out the dogs in the first week of February\".\n\nOn 13 February, Atkinson set off on the first lap southwards to Hut Point with the dog assistant, Dimitri Gerov, and the dogs to avoid being cut off by disintegrating sea ice. Atkinson and Gerov were still at Hut Point when, on 19 February, Tom Crean arrived on foot from the Barrier and reported that Lt Edward Evans was lying seriously ill in a tent some to the south, and in urgent need of rescue. Atkinson decided that this mission was his priority, and set out with the dogs to bring Evans back. This was achieved; the party was back at Hut Point on 22 February.\n\nAtkinson sent a note back to the Cape Evans base camp requesting either the meteorologist Wright or Cherry-Garrard to take over the task of meeting Scott with the dogs. Chief meteorologist Simpson was unwilling to release Wright from his scientific work, and Atkinson therefore selected Apsley Cherry-Garrard. It was still not in Atkinson's mind that Cherry-Garrard's was a relief mission, and according to Cherry-Garrard's account, told him to \"use his judgement\" as to what to do in the event of not meeting the polar party by One Ton, and that Scott's orders were that the dogs must not be risked. Cherry-Garrard left with Gerov and the dogs on 26 February, carrying extra rations for the polar party to be added to the depot and 24 days' of dog food. They arrived at One Ton Depot on 4 March and did not proceed further south. Instead, he and Gerov, after waiting there for Scott for several days, apparently mostly in blizzard conditions (although no blizzard was recorded by Scott some 100 miles further south until 10 March), they returned to Hut Point on 16 March, in poor physical condition and without news of the polar party.\n\nOn the return journey from the pole, Scott reached the 82.30°S meeting point for the dog teams three days ahead of schedule, around 27 February 1912. Scott's diary for that day notes \"We are naturally always discussing possibility of meeting dogs, where and when, etc. It is a critical position. We may find ourselves in safety at the next depot, but there is a horrid element of doubt.\" By 10 March it became clear that the dog teams were not coming: \"The dogs which would have been our salvation have evidently failed. Meares [the dog-driver] had a bad trip home I suppose. It's a miserable jumble.\"\n\nAround 25 March, awaiting death in his tent at latitude 79.30°S, Scott speculated, in a farewell letter to his expedition treasurer Sir Edgar Speyer, that he had overshot the meeting point with the dog relief teams, writing \"We very nearly came through, and it's a pity to have missed it, but lately I have felt that we have overshot our mark. No-one is to blame and I hope no attempt will be made to suggest that we had lacked support.\" (Farewell letter to Sir Edgar Speyer, cited from Karen May 2012.)\n\n"}
{"id": "590473", "url": "https://en.wikipedia.org/wiki?curid=590473", "title": "Contraction (grammar)", "text": "Contraction (grammar)\n\nA contraction is a shortened version of the written and spoken forms of a word, syllable, or word group, created by omission of internal letters and sounds.\n\nIn linguistic analysis, contractions should not be confused with crasis, abbreviations nor acronyms (including initialisms), with which they share some semantic and phonetic functions, though all three are connoted by the term \"abbreviation\" in loose parlance. Contraction is also distinguished from clipping, where beginnings and endings are omitted.\n\nThe definition overlaps with the term portmanteau (a linguistic \"blend\"), but a distinction can be made between a portmanteau and a contraction by noting that contractions are formed from words that would otherwise appear together in sequence, such as \"do\" and \"not\", whereas a portmanteau word is formed by combining two or more existing words that all relate to a singular concept which the portmanteau describes.\n\nEnglish has a number of contractions, mostly involving the elision of a vowel (which is replaced by an apostrophe in writing), as in \"I'm\" for \"I am\", and sometimes other changes as well, as in \"won't\" for \"will not\" or \"ain't\" for \"am not\". These contractions are commonly used in speech and in informal writing, though tend to be avoided in more formal writing (with limited exceptions, such as the mandatory form of \"o'clock\").\n\nThe main contractions are listed in the following table (for more explanation see English auxiliaries and contractions).\nSome other simplified pronunciations of common word groups, which can often equally be described as cases of elision, may also be considered (non-standard) contractions (not enshrined into the written standard language, but frequently expressed in written form anyway), such as \"wanna\" for \"want to\", \"gonna\" for \"going to\", \"y'all\" for \"you all\", \"ya'll\" for \"ya all\" in the Southern United States and others common forms in colloquial speech.\n\nIn subject–auxiliary inversion, the contracted negative forms behave as if they were auxiliaries themselves, changing place with the subject. For example, the interrogative form of \"He won't go\" is \"Won't he go\", whereas the uncontracted equivalent is \"Will he not go?\", with \"not\" following the subject.\n\nContractions exist in Classical Chinese, some of which are used in modern Chinese.\nContractions also appear in Cantonese, for example, 乜嘢 and 咩.\n\nThe French language has a variety of contractions, similar to English but mandatory, as in \"C'est la vie\" (\"That's life\"), where \"c'est\" stands for \"ce\" + \"est\" (\"that is\"). The formation of these contractions is called elision.\n\nIn general, any monosyllabic word ending in \"e caduc\" (schwa) will contract if the following word begins with a vowel, \"h\" or \"y\" (as \"h\" is silent and absorbed by the sound of the succeeding vowel; \"y\" sounds like \"i\"). In addition to \"ce\" → \"c'-\" (demonstrative pronoun \"that\"), these words are \"que\" → \"qu'-\" (conjunction, relative pronoun, or interrogative pronoun \"that\"), \"ne\" → \"n'-\" (\"not\"), \"se\" → \"s'-\" (\"himself\", \"herself\", \"itself\", \"oneself\" before a verb), \"je\" → \"j'-\" (\"I\"), \"me\" → \"m'-\" (\"me\" before a verb), \"te\" → \"t'- \" (informal singular \"you\" before a verb), \"le\" or \"la\" → \"l'-\" (\"the\"; or \"he/she\", \"it\" before a verb or after an imperative verb and before the word \"y\" or \"en\"), and \"de\" → \"d'-\" (\"of\"). Unlike with English contractions, however, these contractions are mandatory: one would never say (or write) \"*ce est\" or \"*que elle\".\n\n\"Moi\" (\"myself\") and \"toi\" (informal \"yourself\") mandatorily contract to \"m'-\" and \"t'-\" respectively after an imperative verb and before the word \"y\" or \"en\".\n\nIt is also mandatory to avoid the repetition of a sound when the conjunction \"si\" (\"if\") is followed by \"il\" (\"he\", \"it\") or \"ils\" (\"they\"), which begin with the same vowel sound \"i\": \"*si il\" → \"s'il\" (\"if it\", if he\"); \"*si ils\" → \"s'ils\" (\"if they\").\n\nCertain prepositions are also mandatorily merged with masculine and plural direct articles: \"au\" for \"à le\", \"aux\" for \"à les\", \"du\" for \"de le\", and \"des\" for \"de les\". However, the contraction of \"cela\" (demonstrative pronoun \"that\") to \"ça\" is optional and informal.\n\nIn informal speech, a personal pronoun may sometimes be contracted onto a following verb. For example, \"je ne sais pas\" (, \"I don't know\") may be pronounced roughly \"chais pas\" (), with the \"ne\" being completely elided and the of \"je\" being mixed with the of \"sais\". It is also common in informal contexts to contract \"tu\" to \"t'-\" before a vowel, e.g., \"t'as mangé\" for \"tu as mangé\".\n\nIn Modern Hebrew, the prepositional prefixes -בְּ /bə-/ 'in' and -לְ /lə-/ 'to' contract with the definite article prefix -ה (/ha-/) to form the prefixes -ב /ba/ 'in the' and -ל /la/ 'to the'. In colloquial Israeli Hebrew, the preposition את (/ʔet/), which indicates a definite direct object, and the definite article prefix -ה (/ha-/) are often contracted to 'ת (/ta-/) when the former immediately precedes the latter. Thus ראיתי את הכלב (/ʁaˈʔiti ʔet haˈkelev/, \"I saw the dog\") may become ראיתי ת'כלב (/ʁaˈʔiti taˈkelev/).\n\nIn Italian, prepositions merge with direct articles in predictable ways. The prepositions \"a\", \"da\", \"di\", \"in\", \"su\", \"con\" and \"per\" combine with the various forms of the definite article, namely \"il\", \"lo\", \"la\", \"l',\" \"i\", \"gli\", \"gl',\" and \"le\".\n\n\nThe words \"ci\" and \"è\" (form of \"essere\", to be) and the words \"vi\" and \"è\" are contracted into \"c'è\" and \"v'è\" (both meaning \"there is\").\n\nThe words \"dove\" and any word that begins with \"e\" are contracted into one single, deleting the e of the principal word, dove (dov'). Equally \"come\" does be made so.\nAs well other words may be contracted the same these two, like \"quale\", and other ones, etcetera.\n\nSpanish has two mandatory phonetic contractions between prepositions and articles: \"al\" (to the) for \"a el\", and \"del\" (of the) for \"de el\" (not to be confused with \"a él\", meaning \"to him\", and \"de él\", meaning \"his\" or, more literally, \"of him\").\n\nOther contractions were common in writing until the 17th century, the most usual being \"de\" + personal and demonstrative pronouns: \"destas\" for \"de estas\" (of these, fem.), \"daquel\" for \"de aquel\" (of that, masc.), \"dél\" for \"de él\" (of him) etc.; and the feminine article before words beginning with \"a-\": \"l'alma\" for \"la alma\", now \"el alma\" (the soul). Several sets of demonstrative pronouns originated as contractions of \"aquí\" (here) + pronoun, or pronoun + \"otro/a\" (other): \"aqueste\", \"aqueso\", \"estotro\" etc. The modern \"aquel\" (that, masc.) is the only survivor of the first pattern; the personal pronouns \"nosotros\" (we) and \"vosotros\" (pl. you) are remnants of the second. In medieval texts unstressed words very often appear contracted: \"todol\" for \"todo el\" (all the, masc.), \"ques\" for \"que es\" (which is); etc. including with common words, like d'ome (d'home/d'homme) instead de ome (home/homme), and so on.\n\nThough not strictly a contraction, a special form is used when combining con with mí, ti or sí which is written as \"conmigo\" for *\"con mí\" (with me), \"contigo\" for *\"con ti\" (with you sing.), \"consigo\" for *\"con sí\" (with himself/herself/itself/themselves (themself).\n\nFinally, one can hear \"pa\"' for \"para\", deriving as \"pa'l\" for \"para el\", but these forms are only considered appropriate in informal speech.\n\nIn Portuguese, contractions are common and much more numerous than those in Spanish. Several prepositions regularly contract with certain articles and pronouns. For instance, \"de\" (of) and \"por\" (by; formerly \"per\") combine with the definite articles \"o\" and \"a\" (masculine and feminine forms of \"the\" respectively), producing \"do\", \"da\" (of the), \"pelo\", \"pela\" (by the). The preposition \"de\" contracts with the pronouns \"ele\" and \"ela\" (he, she), producing \"dele\", \"dela\" (his, her). In addition, some verb forms contract with enclitic object pronouns: e.g., the verb \"amar\" (to love) combines with the pronoun \"a\" (her), giving \"amá-la\" (to love her).\n\nAnother contraction in portuguese which is similar to English ones is the combination of the pronoun \"da\" with words starting in \"a\", resulting in changing the first letter \"a\" for an apostrophe and joining both words. Examples: \"Estrela d'alva\" (A popular phrase to refer to Venus that means \"Alb star\", as a reference to its brightness) ; \"Caixa d'água\" (water tank).\n\nIn informal, spoken German prepositional phrases, one can often merge the preposition and the article; for example, \"von dem\" becomes \"vom\", \"zu dem\" becomes \"zum\", or \"an das\" becomes \"ans\". Some of these are so common that they are mandatory. In informal speech, \"aufm\" for \"auf dem\", \"unterm\" for \"unter dem\", etc. are also used, but would be considered to be incorrect if written, except maybe in quoted direct speech, in appropriate context and style.\n\nThe pronoun \"es\" often contracts to \"s\" (usually written with the apostrophe) in certain contexts. For example, the greeting \"Wie geht es?\" is usually encountered in the contracted form \"Wie geht's?\".\n\nRegional dialects of German, and various local languages which usually were already used long before today's Standard German was created, do use contractions usually more frequently than German, but varying widely between different local languages. The informally spoken German contractions are observed almost everywhere, most often accompanied by additional ones, such as \"in den\" becoming \"in'n\" (sometimes \"im\") or \"haben wir\" becoming \"hamwer\", \"hammor\", \"hemmer\", or \"hamma\" depending on local intonation preferences. Bavarian German features several more contractions such as \"gesund sind wir\" becoming \"xund samma\" which are schematically applied to all word or combinations of similar sound. (One must remember, however, that German \"wir\" exists alongside Bavarian \"mir\", or \"mia\", with the same meaning.) The Munich-born footballer Franz Beckenbauer has as his catchphrase \"Schau mer mal\" (\"Schauen wir einmal\" - in English \"let's have a look\"). A book about his career had as its title the slightly longer version of the phrase, \"Schau'n Mer Mal\".\n\nSuch features are found in all central and southern language regions. A sample from Berlin: \"Sag einmal, Meister, kann man hier einmal hinein?\" is spoken as \"Samma, Meesta, kamma hier ma rin?\"\n\nSeveral West Central German dialects along the Rhine River have built contraction patterns involving long phrases and entire sentences. In speech, words are often concatenated, and frequently the process of \"liaison\" is used. So, \"[Dat] kriegst Du nicht\" may become \"Kressenit\", or \"Lass mich gehen, habe ich gesagt\" may become \"Lomejon haschjesaat\".\n\nMostly, there are no binding orthographies for local dialects of German, hence writing is left to a great extent to authors and their publishers. Outside quotations, at least, they usually pay little attention to print more than the most commonly spoken contractions, so as not to degrade their readability. The use of apostrophes to indicate omissions is a varying and considerably less frequent process than in English-language publications.\n\nThe use of contractions is not allowed in any form of standard Norwegian spelling, however, it is fairly common to shorten or contract words in spoken language. Yet, the commonness varies from dialect to dialect and from sociolect to sociolect—it depends on the formality etc. of the setting. Some common, and quite drastic, contractions found in Norwegian speech are \"jakke\" for \"jeg har ikke\", meaning \"I do not have\" and \"dække\" for \"det er ikke\", meaning \"there is not\". The most frequently used of these contractions—usually consisting of two or three words contracted into one word, contain short, common and often monosyllabic words like , , , , or . The use of the apostrophe (') is much less common than in English, but is sometimes used in contractions to show where letters have been dropped.\n\nIn extreme cases, long, entire sentences may be written as one word. An example of this is \"Det ordner seg av seg selv\" in standard written Bokmål, meaning \"It will sort itself out\" could become \"dånesæsæsjæl\" (note the letters Å and Æ, and the word \"sjæl\", as an eye dialect spelling of ). R-dropping, being present in the example, is especially common in speech in many areas of Norway , but plays out in different ways, as does elision of word-final phonemes like .\n\nBecause of the many dialects of Norwegian and their widespread use it is often difficult to distinguish between non-standard writing of standard Norwegian and eye dialect spelling. It is almost universally true that these spellings try to convey the way each word is pronounced, but it is rare to see language written that does not adhere to at least some of the rules of the official orthography. Reasons for this include words spelled unphonemically, ignorance of conventional spelling rules, or adaptation for better transcription of that dialect's phonemes.\n\nLatin contains several examples of contractions. One such case is preserved in the verb \"nolo\" (I am unwilling/do not want) which was formed by a contraction of \"non volo\" (\"volo\" meaning “I want”). Similarly this is observed in the first person plural and third person plural forms (nolumus and nolunt respectively).\n\nSome contractions in rapid speech include ～っす (\"-ssu\") for です (\"desu\") and すいません (\"suimasen\") for すみません (\"sumimasen\"). では (\"dewa\") is often contracted to じゃ (\"ja\"). In certain grammatical contexts the particle の (\"no\") is contracted to simply ん (\"n\").\n\nWhen used after verbs ending in the conjunctive form ～て (\"-te\"), certain auxiliary verbs and their derivations are often abbreviated. Examples:\n<nowiki>*</nowiki> this abbreviation is never used in the polite conjugation, to avoid the resultant ambiguity between an abbreviated \"ikimasu\" (go) and the verb \"kimasu\" (come).\n\nThe ending ～なければ (\"-nakereba\") can be contracted to ～なきゃ (\"-nakya\") when it is used to indicate obligation. It is often used without an auxiliary, e.g., 行かなきゃ（いけない） (\"ikanakya (ikenai)\") \"I have to go.\"\n\nOther times, contractions are made to create new words or to give added or altered meaning:\n\nVarious dialects of Japanese also use their own specific contractions which are often unintelligible to speakers of other dialects.\n\nIn the Polish language pronouns have contracted forms which are more prevalent in their colloquial usage. Examples are \"go\" and \"mu\". The non-contracted forms are \"jego\" (unless it is used as a possessive pronoun) and \"jemu\", respectively. The clitic \"-ń\" which stands for \"niego\" (him) as in \"dlań\" (\"dla niego\") is more common in literature. The non-contracted forms are generally used as a means to accentuate.\n\nUyghur, a Turkic language spoken in Central Asia, includes some verbal suffixes that are actually contracted forms of compound verbs (serial verbs). For instance, \"sëtip alidu\" (sell-manage, \"manage to sell\") is usually written and pronounced \"sëtivaldu\", with the two words forming a contraction and the [p] leniting into a [v] or [w].\n\nIn Filipino, most contractions need other words to be contracted correctly. Only words that end with vowels can make a contraction with words like \"at\" and \"ay.\" In this chart, the \"@\" represents any vowel.\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "1833848", "url": "https://en.wikipedia.org/wiki?curid=1833848", "title": "Cross-reference", "text": "Cross-reference\n\nThe term cross-reference can refer to either:\n\nIn a document, especially those authored in a Content management system,\na cross-reference has two major aspects:\n\nThe visible form contains text, graphics, and other indications that:\n\nThe technical mechanism that resides within the system:\n\nIf the cross reference mechanism is well designed, the reader will be able to follow each cross reference to the referenced content whether the content is presented in print or electronically.\n\nAn author working in a content management system is responsible for identifying subjects of interest that cross documents, and creating appropriate systems of cross references to support readers who seek to understand those subjects. For an individual cross reference, an author should ensure that location and content of the target of the cross reference are clearly identified, and the reader can easily determine how to follow the cross reference in each medium in which publication is supported.\n\nContent strategy practitioners (known as content strategists) specialize in planning content to meet business needs, taking into account the processes for creating and maintaining the content, and the systems that support the content.\n\n"}
{"id": "358999", "url": "https://en.wikipedia.org/wiki?curid=358999", "title": "Difference feminism", "text": "Difference feminism\n\nTaking for granted an equal moral status as persons, difference feminism asserts that there are differences between men and women but that no value judgment can be placed upon them.\n\nThe term \"difference feminism\" developed during the \"equality-versus-difference debate\" in American feminism in the 1980s and 1990s, but subsequently fell out of favor and use. In the 1990s feminists addressed the binary logic of \"difference\" versus \"equality\" and moved on from it, notably with postmodern and/or deconstructionist approaches that either dismantled or did not depend on that dichotomy.\n\nDifference feminism did not require a commitment to essentialism. Most strains of difference feminism did not argue that there was a biological, inherent, ahistorical, or otherwise \"essential\" link between womanhood and traditionally feminine values, habits of mind (often called \"ways of knowing\"), or personality traits. These feminists simply sought to recognize that, in the present, women and men are significantly different and to explore the devalued \"feminine\" characteristics.\n\nSome strains of difference feminism, for example Mary Daly's, argue not just that women and men were different, and had different values or different ways of knowing, but that women and their values were superior to men's. This viewpoint does not require essentialism, although there is ongoing debate about whether Daly's feminism is essentialist.\n\nDifference feminism was developed by feminists in the 1980s, in part as a reaction to popular liberal feminism (also known as \"equality feminism\"), which emphasized the similarities between women and men in order to argue for equal treatment for women. Difference feminism, although it still aimed at equality between men and women, emphasized the differences between men and women and argued that identicality or sameness are not necessary in order for men and women, and masculine and feminine values, to be treated equally. Liberal feminism aimed to make society and law gender-neutral, since it saw recognition of gender difference as a barrier to rights and participation within liberal democracy, while difference feminism held that gender-neutrality harmed women \"whether by impelling them to imitate men, by depriving society of their distinctive contributions, or by letting them participate in society only on terms that favor men\".\n\nDifference feminism drew on earlier nineteenth-century strains of thought, for example the work of German writer Elise Oelsner, which held that not only should women be allowed into formerly male-only spheres and institutions (e.g. public life, science) but that those institutions should also be expected to change in a way that recognizes the value of traditionally devalued feminine ethics (like care [see ethics of care]). On the latter point, many feminists have re-read the phrase \"difference feminism\" in a way that asks \"what difference does feminism make?\" (e.g. to the practice of science) rather than \"what differences are there between men and women\"?\n\nSome have argued that the thought of certain prominent second-wave feminists, like psychologist Carol Gilligan and radical feminist theologian Mary Daly, is \"essentialist.\" In philosophy essentialism is the belief that \"(at least some) objects have (at least some) essential properties.\" In the case of sexual politics essentialism is taken to mean that \"women\" and \"men\" have fixed essences or essential properties (e.g. behavioral or personality traits) that cannot be changed. However, essentialist interpretations of Daly and Gilligan have been questioned by some feminist scholars, who argue that charges of \"essentialism\" are often used more as terms of abuse than as theoretical critiques based on evidence, and do not accurately reflect Gilligan or Daly's views.\n\n"}
{"id": "58632079", "url": "https://en.wikipedia.org/wiki?curid=58632079", "title": "Encyclopedia of Forensic and Legal Medicine 2nd Edition", "text": "Encyclopedia of Forensic and Legal Medicine 2nd Edition\n\nThe Encyclopedia of Forensic and Legal Medicine 2nd Edition is a reference source and pioneering 4 set encyclopedia of forensics and medico-legal knowledge published by Academic Press, Elsevier in 2016. This has been edited by the renowned British forensic specialist Jason Payne-James and Australian forensic pathologist Roger W. Byard and an international editorial board. \nThis reference work includes more than 300 articles contributed by forensic medicine and forensic science experts from all over the world. The encyclopedia is a complete reference source of articles covering from forensics, criminal investigations, health-care, legal, judicial, ballistics, toxicology,fingerprinting, DNA typing, disaster victim identification to autopsy and postmortem examination.\n\nThe encyclopedia is especially meant for forensic, medical, chemistry, physics, laboratory technologists and anthropology students and specialists such as forensic experts, lawyers, judicial officers, judges, police and investigating offices, nurses, medical officers etc. All the articles of the encyclopedia are available through Science direct and Scopus.\n"}
{"id": "20819040", "url": "https://en.wikipedia.org/wiki?curid=20819040", "title": "Hashtag", "text": "Hashtag\n\nA hashtag is a type of metadata tag used on social networks such as Twitter and other microblogging services, allowing users to apply dynamic, user-generated tagging which makes it possible for others to easily find messages with a specific theme or content. Users create and use hashtags by placing the number sign or pound sign codice_1 usually in front of a word or unspaced phrase in a message. The hashtag may contain letters, digits, and underscores. Searching for that hashtag will yield each message that has been tagged with it. A hashtag archive is consequently collected into a single stream under the same hashtag. For example, on the photo-sharing service Instagram, the hashtag \"#bluesky\" allows users to find all the posts that have been tagged using that hashtag. \n\nThe use of hashtags was first proposed by Chris Messina in a 2007 tweet that, although initially decried by Twitter as a \"thing for nerds\", eventually led to their use spreading like wild-fire through the platform. Messina, who made no attempt to copyright the use because he felt \"they were born of the internet, and owned by no one\", has subsequently been credited as the godfather of the hashtag. By the end of the decade hashtags could be seen in most emerging as well as established social media platforms including Instagram, Facebook, Reddit, and YouTube. So much so that Instagram had to officially place a \"30 hashtags\" limit on its posts to prevent people from abusing their use, a limit which Instagrammers eventually circumvented by posting hashtags in the comments section of their posts. As of 2018 more than 85% of the top 50 websites by traffic on the Internet use hashtags and their use is highly common with millennials, Gen Z, politicians, influencers, and celebrities worldwide. Because of its widespread use, \"hashtag\" was added to the \"Oxford English Dictionary\" in June 2014. The term \"hashtag\" is also sometimes erroneously used to refer to the hash symbol itself when used in the context of a hashtag. Formal taxonomies can be developed from the folk taxonomy rendered machine-readable by the markup that hashtags provide; this process is called folksonomy.\nThe US pound sign, number sign or hash symbol \"#\" is often used in information technology to highlight a special meaning. (\"Pound sign\" in the UK means \"£\"; \"#\" is called hash, gate, and occasionally octothorpe.) In 1970, for example, the number sign was used to denote \"immediate\" address mode in the assembly language of the PDP-11 when placed next to a symbol or a number. In 1978, Brian Kernighan and Dennis Ritchie used \"#\" in the C programming language for special keywords that had to be processed first by the C preprocessor. In the 1986 SGML standard, ISO 8879:1986 (q.v.), # is a reserved name indicator (rni) which precedes keyword syntactic literals, --e.g., the primitive content token #PCDATA, used for parsed character data.\n\nThe International Telecommunication Union approved in November 1988 recommendation E.161 that put the hash sign on the right side of the 0 in the 4 x 3 button arrangement for push buttons on telephones. This same arrangement is still used today in most software phones (see Android dialer for example). The ITU recommendation had 2 design options for the hash: a European version where the hash sign was built with a 90-degree angle and a North-American version with an 80-degree angle. The North-American version seems to have prevailed as most hash signs in Europe now follow the 80-degree inclination.\n\nThe pound sign was adopted for use within IRC networks circa 1988 to label groups and topics. Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an ampersand '&').\n\nThe use of the pound sign in IRC inspired Chris Messina to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network. He posted the first hashtag on Twitter:\n\nMessina’s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the 2007 San Diego forest fires in Southern California.\n\nAccording to Messina, he suggested use of the hashtag to make it easy for \"lay\" users to search for content and find specific relevant updates; they were for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag \"was created organically by Twitter users as a way to categorize messages.\" Today they are for anyone, either with or without technical knowledge, to easily impose enough annotation to be useful without needing a more formal system or adhering to many technical details.\n\nInternationally, the hashtag became a practice of writing style for Twitter posts during the 2009–2010 Iranian election protests; Twitter users inside and outside Iran used both English- and Persian-language hashtags in communications during the events.\n\nThe first published use of the term \"hash tag\" was in a blog post by Stowe Boyd, \"Hash Tags = Twitter Groupings,\" on August 26, 2007, according to lexicographer Ben Zimmer, chair of the American Dialect Society's New Words Committee.\n\nBeginning July 2, 2009, Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced \"Trending Topics\" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to spam the trending list and ensure that hashtags trend naturally.\n\nAlthough the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.\n\nA hashtag must begin with a hash character followed by other characters, and is terminated by a space, or end of message. It is always safe to precede the “#” with a space, and to include letters without diacritics, digits, and underscores. In many cases other characters are also allowed, in particular accented characters used in many languages, but handling may vary from one client to another, and from time to time as standards evolve. A discussion of hashtag standards suggests that if #Romeo&Juliet is used, different Twitter clients might link to #Romeo, #Romeo&, or #Romeo&Juliet. Hashtags are not case sensitive; a search for “#hashtag” will find “#HashTag”. The use of embedded capitals (CamelCase) increases readability and avoids confusion; a (real) pen shop would be advised to use #PenIsland rather than all lower-case. On microblogging and social networking sites hashtags can be inserted anywhere within a text, often at the beginning or the end, but also within the text, usually as a word (e.g. “It is #sunny today”).\n\nLanguages which do not use letters are handled slightly differently. In China, microblogs Sina Weibo and Tencent Weibo use a double-hashtag-delimited #HashName# format, since the lack of spacing between Chinese characters necessitates a closing tag. Twitter uses a different syntax for Chinese characters and orthographies with similar spacing conventions: the hashtag contains unspaced characters, separated from preceding and following text by spaces (e.g. '我 #爱 你' instead of '我#爱你') or by zero-width non-joiner characters before and after the hashtagged element, to retain a linguistically natural appearance (displaying as unspaced '我‌#爱‌你', but with invisible non-joiners delimiting the hashtag).\n\nIt is considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the \"absolute maximum\", and any contribution exceeding this risks \"raising the ire of the community.\"\n\nAs well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or suspended.\n\nJimmy Fallon and Justin Timberlake performed a sketch parodying the often incorrect and misunderstood use of hashtags on \"Late Night with Jimmy Fallon\" in September 2013.\n\nHashtags are mostly used in unmoderated, ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can \"trend\" and attract more individual users to discussion. On Twitter, when a hashtag becomes extremely popular, it will appear in the \"Trending Topics\" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users. They cannot be \"retired\" from public usage, meaning that any given hashtag can theoretically be used in perpetuity. They do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes, as chosen by the creators of them.\n\nHashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using #cakefestival rather than simply #cake. However, this can also make it difficult for topics to become \"trending topics\" because people often use different spelling or words to refer to the same topic. For topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.\n\nHashtags also function as beacons in order for users to find and \"follow\" (subscribe) or \"list\" (organize into public contact lists) other users of similar interest.\n\nTelevision broadcasters such as Channel 4 have employed the hashtag during the transmission of programmes such as First Dates and The Undateables. Research has shown that audience numbers go up when individuals can be interactive by tweeting while viewing a programme.\n\nHashtags can be used on the social network Instagram, by posting a picture and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic, such as #photography #iPhone #iphoneography, and therefore do not fulfill a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use. The ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagram's content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.\n\nFamous Youtube bloggers often use hashtags to promote their videos to a wide audience. Thus, by leaving various hashtags under the video, they are trying to increase their views and gain as many likes as possible. Usually, hashtags are left under the video itself in a special line. By clicking on the hashtag you go directly to the link to the video, which are similar in topic.\n\nHashtags are also used informally to express context around a given message, with no intent to categorize the message for later searching, sharing, or other reasons. One of the functions of the hashtag is to serve as a reflexive meta-commentary, which contributes to the idea of how written communication in new media can be paralleled to how pragmatic methodology is applied to speech.\n\nThis can help express contextual cues or offer more depth to the information or message that appears with the hashtag. \"My arms are getting darker by the minute. #toomuchfaketan\". Another function of the hashtag can be used to express personal feelings and emotions. For example, with \"It's Monday!! #excited #sarcasm\" in which the adjectives are directly indicating the emotions of the speaker. It can also be used as a disclaimer of the information that the hashtag accompanies, as in, \"BREAKING: US GDP growth is back! #kidding\". In this case, the hashtag provides an essential piece of information in which the meaning of the utterance is changed entirely by the disclaimer hashtag. This may also be conveyed with #sarcasm, as in the previous example. Self-mockery is another informal function of the hashtag used by writers, as in this tweet: \"Feeling great about myself till I met an old friend who now races at the Master's level. Yup, there's today's #lessoninhumility,\" where the informality of the hashtag provides commentary on the tweet itself.\n\nThe feature has been added to other, non-short-message-oriented services, such as the user comment systems on YouTube and Gawker Media. In the case of the latter, hashtags for blog comments and directly submitted comments were used to maintain a more constant rate of user activity even when paid employees were not logged into the website. Real-time search aggregators such as the former Google Real-Time Search also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the \"river\" of Twitter posts that can result from search terms or hashtags.\n\nThe use of hashtags has extended to televisiona concept that began rising in prominence in the early 2010s. Broadcasters may display a hashtag as an on-screen bug, encouraging viewers to participate in a backchannel of discussion via social media prior to, during, or after the program. Television commercials have sometimes contained hashtags for similar purposes. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement.\n\nWhile personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames to receive mentions and replies to posts, usage of related or \"branded\" hashtags alongside Twitter usernames (e.g., #edshow as well as @edshow) is increasingly encouraged as a microblogging style to \"trend\" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. #SunnyFX) and instantaneous, \"temporary\" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts. Some have speculated that hashtags might take the place of (or co-exist with) the Nielsen television ratings system.\n\nAn example of trending \"temporary\" hashtags garnering viewers during broadcasts is observed on \"The Tonight Show\" with Jimmy Fallon, a variety talk show on NBC. Every Wednesday, Fallon hosts a segment on his show called \"Tonight Show Hashtags,\" which engages viewers by inviting them via Twitter to post humorous stories based on a specific hashtag topic, such as #WhydidIsaythat, #Worstfirstdate, to #Onetimeinclass, reflecting on funny experiences in daily life. By using hashtags, Fallon creates a sense of community and solidarity among his viewers and draws a wider range of viewers through an online platform while they watch a classic, non-interactive television program. Because of its popularity, the \"Tonight Show Hashtags\" are usually the 'most tweeted hashtag' on Twitter, which promotes the show. By engaging viewers with a lighthearted subject and simple hashtags, Fallon can gauge topical responses from viewers during broadcasts and also use the hashtags to brand his show.\n\nThe increased usage of hashtags as brand promotion devices has been compared to the promotion of branded \"keywords\" by AOL in the late 1990s and early 2000s, as such keywords were also promoted at the end of television commercials and series episodes.\n\nThe late-night television comedy game show @midnight with Chris Hardwick on Comedy Central features a daily game entitled \"Hashtag Wars,\" in which three comedians compete against one another to come up with phrases based on a given hashtag theme.\n\nSome hashtags have become famous worldwide. For instance the slogan \"Je suis Charlie,\" which was first used on Twitter as the hashtag #jesuischarlie and #iamcharlie to indicate solidarity with \"Charlie Hebdo\" offices attacked in Paris, spread to the internet at large.\n\nSince February 2013 Twitter and American Express have collaborated to enable users to pay for discounted goods online by tweeting a special hashtag. American Express members can sync their card with Twitter and pay for offers by tweeting; American Express tweets a response to the member that confirms the purchase.\n\nOrganized real-world events have used hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants to find each other, both on Twitter and, in many cases, during actual physical events.\n\nCompanies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.\n\nPolitical protests and campaigns in the early 2010s, such as #OccupyWallStreet and #LibyaFeb17, have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion. Hashtags have also been used to promote official events; the Finnish Ministry of Foreign Affairs officially titled the 2018 Russia–United States summit as the \"#HELSINKI2018 Meeting\".\n\nHashtags are often used by consumers on social media platforms to complain about the customer service experience with large companies. The term \"bashtag\" has been created to describe situations in which a user refers to a corporate social media hashtag to criticise the company or to tell others about poor customer service. For example, in January 2012, McDonald's created the #McDStories hashtag so that customers could share positive experiences about the restaurant chain. But, the marketing effort was cancelled after two hours when McDonald's received numerous complaint tweets rather than the positive stories they were anticipating.\n\nThe use of hashtags also reveals what feelings or sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is sarcastic—a difficult AI problem.\n\nThe YouTuber Spencer FC used the hashtag for the name and crest of his YouTube-based association football team, Hashtag United F.C..\n\nSince the 2012–13 season, the National Basketball Association (NBA) has allowed fans to vote players in as All-Star Game starters on Twitter and Facebook using #NBAVOTE. The tweets and Facebook posts must include #NBAVOTE along with the player's first and last name or Twitter handle.\n\nDuring the April 2011 Canadian party leader debate, Jack Layton, then-leader of the New Democratic Party, referred to Conservative Prime Minister Stephen Harper's crime policies as \"a hashtag fail\" (presumably #fail).\n\nThe term \"hashtag rap\", coined by Kanye West, was developed in the 2010s to describe a style of rapping which, according to Rizoh of the \"Houston Press,\" uses \"three main ingredients: a metaphor, a pause, and a one-word punch line, often placed at the end of a rhyme\". Rappers Nicki Minaj, Big Sean, Drake, and Lil Wayne are credited with the popularization of hashtag rap, while the style has been criticized by Ludacris, The Lonely Island, and various music writers.\n\nOn September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a \"New York Times\" front-page article regarding Twitter's initial public offering.\n\nBird's Eye foods released in 2014 a shaped mashed potato food that included forms of @-symbols and hashtags, called \"Mashtags\".\n\nHashtags have been used verbally to make a humorous point in informal conversations, such as \"I’m hashtag confused!\" In August 2012, British journalist Tom Meltzer reported in \"The Guardian\" about a new hand gesture that mimicked the hashtag, sometimes called the \"finger hashtag\", in which both hands form a peace sign, and then the fingers are crossed to form the symbol of a hashtag. The emerging gesture was reported about in \"Wired\" by Nimrod Kamer, and during 2013, it was seen on TV as used by Jimmy Fallon, and on \"The Colbert Report,\" among other programs. Writing in 2015, Paola Maria Caleff considered this usage a fad, but noted that people talking the way that they write was a consequence of computer-mediated communication.\n\n\n\n"}
{"id": "23452847", "url": "https://en.wikipedia.org/wiki?curid=23452847", "title": "Hebrew abbreviations", "text": "Hebrew abbreviations\n\nAbbreviations () are a common part of the Hebrew language, with many organizations, places, people and concepts known by their abbreviations.\n\nAcronyms in Hebrew use a special punctuation mark called gershayim (״). This mark is placed between the last two letters of the non-inflected form of the acronym (e.g. \"report\" in singular is \",\" hence the plural \"\"). Acronyms can be formed from strings of single initial letters, e.g. \"\" pazátsta (for ), or multiple initial letters, e.g. (for , the Holy Land) or ráshlats (for , Rishon LeZion).\n\nIf the acronym is read as is, then the spelling should be with a final form letter. If, on the other hand, the acronym is read as the complete phrase or read as the individual letters, then it should be spelled with a medial form letter. In practice, this rule is more often than not ignored, and the acronyms spelled either way.\n\nAbbreviations that are truncations of a single word, consisting of the first letter or first several letters of that word (as opposed to acronyms formed from initials or truncations of more than one word) are denoted using the punctuation mark geresh () by placing the sign after the last letter of the abbreviation (e.g. \"Ms.\": \"\"). However, in practice, single and double quotes are often used instead of the special punctuation marks (for which most keyboards do not have keys), with the single quote used both in acronyms and abbreviations.\n\nOften (and especially when they describe a noun), Hebrew acronyms are pronounced by the insertion of a vowel sound (usually ) between the letters. These vowels often appear in transliterations to other scripts. Examples include Shas (), Tanakh () and Shabak (). There are exceptions to the use of \"a\", such as Etzel ().\n\nWhen one of the letters is vav or yud, these may be read as vowels (\"u\"/“o” and \"i\") instead: (\"duakh\"/\"dokh\" = , judgement and account); (\"admor\" = , hasidic rebbe; (\"shut\" = , questions and answers); (\"sakum\" = , knife spoon and fork); (\"tapuz\" = , orange, lit. golden apple); (\"um\" = , the United Nations); Bilu; Lehi. (An exception is , Beitar, pronounced \"beytar\".)\n\nHebrew numbers (e.g. year numbers in the Hebrew calendar) are written the same way as acronyms, with gershayim before the last character, but pronounced as separate letter names. For example,  (5775 AM, or 2014-2015 CE) is pronounced hei-tav-shin-ayin-hei.\n\nAcronyms have been widely used in Hebrew since at least the Middle Ages. Several important rabbis are referred to with acronyms of their names. For example, Rabbi Shlomo ben Yitzchak is known as Rashi, Rav Moshe ben Maimon (Maimonides) is commonly known as \"Rambam\" (Hebrew: ), Rabbi Moshe ben Nahman (Nahmanides) is likewise known as the \"Ramban\" (Hebrew: ), and Baal Shem Tov is called the \"Besht\" (Hebrew: ).\n\nA number of such acronyms differ only in their last letter. They all begin with \"Mahara-\", as an acronym of the words ... (\"Morenu Ha-Rav rabi ...\", Our teacher the Rabbi ...).\" \n\nThe usage of Hebrew acronyms extends to liturgical groupings: the word \"Tanakh\" (Hebrew: תנ״ך) is an acronym for Torah (Five Books of Moses), Nevi'im (Book of Prophets), and Ketuvim (Hagiographa).\n\nMost often, though, one will find use of acronyms as acrostics, in both prayer, poetry (see Piyyut), and kabbalistic works. Because each Hebrew letter also has a numeric value, embedding an acrostic may give an additional layer of meaning to these works.\n\nOne purpose of acrostics was as a mnemonic or a way for an author to weave his name as a signature, or some other spiritual thought, into his work, at a time when much was memorized. Examples of prayers which contain acrostics include:\n\n\nישי נוימן, גורמים פגרמטיים, סמנטיים וגרפופונמיים במילוּן קיצורי הכתב, החוג הישראלי לבלשנו ת 18, 2011\n"}
{"id": "1865442", "url": "https://en.wikipedia.org/wiki?curid=1865442", "title": "Hofstadter's law", "text": "Hofstadter's law\n\nHofstadter's law is a self-referential time-related adage, coined by Douglas Hofstadter and named after him.\n\nHofstadter's law was a part of Douglas Hofstadter's 1979 book \"Gödel, Escher, Bach: An Eternal Golden Braid\". The \"law\" is a statement regarding the difficulty of accurately estimating the time it will take to complete tasks of substantial complexity. It is often cited by programmers, especially in discussions of techniques to improve productivity, such as \"The Mythical Man-Month\" or extreme programming. The recursive nature of the law is a reflection of the widely experienced difficulty of estimating complex tasks despite all best efforts, including knowing that the task is complex.\n\nThe law was initially introduced in connection with a discussion of chess-playing computers, where top-level players were continually beating machines, even though the machines outweighed the players in recursive analysis. The intuition was that the players were able to focus on particular positions instead of following every possible line of play to its conclusion. Hofstadter wrote in 1979, \"In the early days of computer chess, people used to estimate that it would be ten years until a computer (or program) was world champion. But after ten years had passed, it seemed that the day a computer would become world champion was still more than ten years away ... This is just one more piece of evidence for the rather recursive Hofstadter's Law:\" Notably, that day did indeed come, when Deep Blue defeated Garry Kasparov in 1997, which indicates that the Law of Accelerating Returns may take effect when the task is repeated, thus counteracting -- and [in some cases] overpowering -- Hofstadter's law.\n\n"}
{"id": "10260905", "url": "https://en.wikipedia.org/wiki?curid=10260905", "title": "I (pronoun)", "text": "I (pronoun)\n\nThe pronoun I is the first-person singular nominative case personal pronoun in Modern English. It is used to refer to one's self and is capitalized, although other pronouns, such as \"he\" or \"she\", are not capitalized.\n\nThe grammatical variants of \"I\" are \"me\", \"my\", \"mine\", and \"myself\".\n\nEnglish \"I\" originates from Old English (OE) \"ic\". Its predecessor \"ic\" had in turn originated from the continuation of Proto-Germanic *\"ik\", and \"ek\"; the asterisk denotes an unattested form, \"ek\" was attested in the Elder Futhark inscriptions (in some cases notably showing the variant \"eka\"; see also ek erilaz). Linguists assume \"ik\" to have developed from the unstressed variant of \"ek\". Variants of \"ic\" were used in various English dialects up until the 1600s.\n\nGermanic cognates are: Old Frisian \"ik\", Old Norse \"ek\" (Danish, Norwegian \"jeg\", Swedish \"jag\", Icelandic ég), Old High German \"ih\" (German \"ich\") and Gothic \"ik\" and in Dutch also \"ik\".\n\nThe Proto-Germanic root came, in turn, from the Proto Indo-European language (PIE). The reconstructed PIE pronoun is *\"egō, egóm\", with cognates including\nSanskrit \"aham\", Hittite \"uk\", Latin \"ego\", Greek \"egō\", Old Slavonic \"azъ\" and Alviri-Vidari (an Iranian language) \"az\".\n\nThe oblique forms are formed from a stem \"*me-\" (English \"me\"), the plural from \"*wei-\" (English \"we\"), the oblique plurals from \"*ns-\" (English \"us\") and from Proto-Germanic \"*unseraz\", PIE \"*no-s-ero-\" (\"our, ours\").\n\n\"I\" (and only this form of the pronoun) is the only pronoun that is always capitalized in English. This practice became established in the late 15th century, though lowercase \"i\" was sometimes found as late as the 17th century.\n\nLike the other English personal pronouns \"we\" (\"us\"), \"he\" (\"him\"), \"she\" (\"her\"), and \"they\" (\"them\"), the pronoun \"I\" has several singular case forms.\nThese are: \n\nThere are some situations in which only the nominative form (\"I\") is grammatically correct and others in which only the accusative form (\"me\") is correct. There are also situations in which one form is used in informal style (and was often considered ungrammatical by older prescriptive grammars) and the other form is preferred in formal style.\n\nIn all varieties of standard English, the nominative form \"I\" is used exclusively when it is the whole subject of an \"explicit\" verb, e.g. \nnot \nWith other pronouns, such as \"we\" (strictly speaking when used as a personal determiner), there may be exceptions to this in some varieties of English.\n\nIn all varieties of standard English, the accusative form \"me\" is used exclusively when it is the whole direct or indirect object of a verb or preposition. The accusative \"me\" is also required in a number of constructions such as \"Silly me!\"\n\nIn many situations, both the nominative \"I\" and the accusative \"me\" are encountered.\n\nWhen the pronoun is used as a subjective predicative complement, the nominative \"I\" is sometimes encountered in (very) formal style:\nBut this is often seen as hypercorrect and may be unacceptable, as in:\n\"Me\" is usually preferred as a subjective predicate, especially in informal style:\nThe nominative \"I\" is more common in this role when it is followed by a relative clause:\nthough even here \"me\" is more common in non-formal style:\n\nFollowing \"as\" or \"than\" (without a following explicit verb), the accusative form is common:\nHowever, where it is possible to think of the pronoun as the subject of an implicit verb and \"than\" or \"as\" as a conjunction, the nominative \"I\" is found in formal style:\n\nIn Australian English, British English and Irish English, many speakers have an unstressed form of \"my\" that is identical to \"me\" (see archaic and non-standard forms of English personal pronouns).\n\nThe above applies when the pronoun stands alone as the subject or object.\nIn some varieties English (particularly formal English), those rules also apply in coordinative constructions such as \"you and I\". So the correct form is \n\nIn some varieties of non-standard informal English, the accusative is sometimes used when the pronoun is part of a coordinative \"subject\" construction, as in\nThis is highly stigmatized.\n\nOn the other hand, the use of the nominative \"I\" in coordinative constructions like \"you and I\"where \"me\" would be used in a non-coordinative object is less stigmatized – and in some cases so widespread as to be considered a variety of standard English: \n\n\n\n\n"}
{"id": "3270420", "url": "https://en.wikipedia.org/wiki?curid=3270420", "title": "Idem", "text": "Idem\n\nidem. is a Latin term meaning \"the same\". It is commonly abbreviated as id.,\nwhich is particularly used in legal citations to denote the previously cited source (compare \"ibid.\"). It is also used in academic citations to replace the name of a repeated author. \n\n\"Id.\" is employed extensively in Canadian legislation and in legal documents of the United States to apply a short description to a section with the same focus as the previous.\n\n\"Id\" is masculine and neuter; ead. (feminine), is the abbreviation for eadem, which also translates to \"the same\". \n\nAs an abbreviation, \"Id.\" always takes a period (or full stop) in both British and American usage (see usage of the full stop in abbreviations). Its first known use dates back to the 14th century.\n\nHere, the first citation refers to the case of \"United States v. Martinez-Fuerte.\" The volume number cited is 428 and the page on which the case begins is 543, and the page number cited to is 545. The \"U.S.\" between the numerical portions of the citation refers to the \"United States Reports\". 1976 refers to the year that the case was published. The second citation references the first citation and automatically incorporates the same reporter and volume number; however, the page number cited is now 547. \"Id.\" refers to the immediately preceding citation, so if the previous citation includes more than one reference, or it is unclear which reference \"Id.\" refers to, its usage is inappropriate.\n\n\nHere, Id. refers to the Executive Order that was mentioned in the previous sentence.\n\n\nIn this example, \"Id\" in the second citation indicates that the author is identical to that of the previous citation. That is, the author of the second citation is also Macgillivray, J. A.\n\n"}
{"id": "1930406", "url": "https://en.wikipedia.org/wiki?curid=1930406", "title": "Impredicativity", "text": "Impredicativity\n\nSomething that is impredicative, in mathematics, logic and philosophy of mathematics, is a self-referencing definition. Roughly speaking, a definition is impredicative if it invokes (mentions or quantifies over) the set being defined, or (more commonly) another set that contains the thing being defined. There is no generally accepted precise definition of what it means to be predicative or impredicative. Authors have given different but related definitions.\n\nThe opposite of impredicativity is predicativity, which essentially entails building stratified (or ramified) theories where quantification over lower levels results in variables of some new type, distinguished from the lower types that the variable ranges over. A prototypical example is intuitionistic type theory, which retains ramification so as to discard impredicativity.\n\nRussell's paradox is a famous example of an impredicative construction—namely the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be asked whether it contains itself or not — if it does then by definition it should not, and if it does not then by definition it should.\n\nThe greatest lower bound of a set , , also has an impredicative definition: if and only if for all elements of , is less than or equal to , and any less than or equal to all elements of is less than or equal to . This definition quantifies over the set (potentially infinite, depending on the order in question) whose members are the lower bounds of , one of which being the glb itself. Hence predicativism would reject this definition.\n\nThe terms \"predicative\" and \"impredicative\" were introduced by , though the meaning has changed a little since then. \n\nSolomon Feferman provides a historical review of predicativity, connecting it to current outstanding research problems.\n\nThe vicious circle principle was suggested by Henri Poincaré (1905-6, 1908) and Bertrand Russell in the wake of the paradoxes as a requirement on legitimate set specifications. Sets that do not meet the requirement are called \"impredicative\".\n\nThe first modern paradox appeared with Cesare Burali-Forti's 1897 \"A question on transfinite numbers\" and would become known as the Burali-Forti paradox. Cantor had apparently discovered the same paradox in his (Cantor's) \"naive\" set theory and this become known as Cantor's paradox. Russell's awareness of the problem originated in June 1901 with his reading of Frege's treatise of mathematical logic, his 1879 \"Begriffsschrift\"; the offending sentence in Frege is the following:\nIn other words, given the function is the variable and is the invariant part. So why not substitute the value for itself? Russell promptly wrote Frege a letter pointing out that:\nFrege promptly wrote back to Russell acknowledging the problem:\nWhile the problem had adverse personal consequences for both men (both had works at the printers that had to be emended), van Heijenoort observes that \"The paradox shook the logicians' world, and the rumbles are still felt today. ... Russell's paradox, which uses the bare notions of set and element, falls squarely in the field of logic. The paradox was first published by Russell in \"The principles of mathematics\" (1903) and is discussed there in great detail ...\". Russell, after six years of false starts, would eventually answer the matter with his 1908 theory of types by \"propounding his \"axiom of reducibility\". It says that any function is coextensive with what he calls a \"predicative\" function: a function in which the types of apparent variables run no higher than the types of the arguments\". But this \"axiom\" was met with resistance from all quarters.\n\nThe rejection of impredicatively defined mathematical objects (while accepting the natural numbers as classically understood) leads to the position in the philosophy of mathematics known as predicativism, advocated by Henri Poincaré and Hermann Weyl in his \"Das Kontinuum\". Poincaré and Weyl argued that impredicative definitions are problematic only when one or more underlying sets are infinite.\n\nErnst Zermelo in his 1908 \"A new proof of the possibility of a well-ordering\" presents an entire section \"b. \"Objection concerning nonpredicative definition\"\" where he argued against \"Poincaré (1906, p. 307) [who states that] a definition is 'predicative' and logically admissible only if it \"excludes\" all objects that are dependent upon the notion defined, that is, that can in any way be determined by it\". He gives two examples of impredicative definitions – (i) the notion of Dedekind chains and (ii) \"in analysis wherever the maximum or minimum of a previously defined \"completed\" set of numbers is used for further inferences. This happens, for example, in the well-known Cauchy proof of the fundamental theorem of algebra, and up to now it has not occurred to anyone to regard this as something illogical\". He ends his section with the following observation: \"A definition may very well rely upon notions that are equivalent to the one being defined; indeed, in every definition \"definiens\" and \"definiendum\" are equivalent notions, and the strict observance of Poincaré's demand would make every definition, hence all of science, impossible\".\n\nZermelo's example of minimum and maximum of a previously defined \"completed\" set of numbers reappears in Kleene 1952:42-42 where Kleene uses the example of Least upper bound in his discussion of impredicative definitions; Kleene does not resolve this problem. In the next paragraphs he discusses Weyl's attempt in his 1918 \"Das Kontinuum\" (\"The Continuum\") to eliminate impredicative definitions and his failure to retain the \"theorem that an arbitrary non-empty set of real numbers having an upper bound has a least upper bound (cf. also Weyl 1919)\".\n\nRamsey argued that \"impredicative\" definitions can be harmless: for instance, the definition of \"tallest person in the room\" is impredicative, since it depends on a set of things of which it is an element, namely the set of all persons in the room. Concerning mathematics, an example of an impredicative definition is the smallest number in a set, which is formally defined as: if and only if for all elements of , is less than or equal to , and is in .\n\nBurgess (2005) discusses predicative and impredicative theories at some length, in the context of Frege's logic, Peano arithmetic, second order arithmetic, and axiomatic set theory.\n\n\n"}
{"id": "36983", "url": "https://en.wikipedia.org/wiki?curid=36983", "title": "Recursive acronym", "text": "Recursive acronym\n\nA recursive acronym is an acronym that refers to itself. The term was first used in print in 1979 in Douglas Hofstadter's book \"Gödel, Escher, Bach: An Eternal Golden Braid\", in which Hofstadter invents the acronym GOD, meaning \"GOD Over Djinn\", to help explain infinite series, and describes it as a recursive acronym. Other references followed, however the concept was used as early as 1968 in John Brunner's science fiction novel \"Stand on Zanzibar\". In the story, the acronym EPT (Education for Particular Task) later morphed into \"Eptification for Particular Task\".\n\nRecursive acronyms typically form backwardly: either an existing ordinary acronym is given a new explanation of what the letters stand for, or a name is turned into an acronym by giving the letters an explanation of what they stand for, in each case with the first letter standing recursively for the whole acronym.\n\nIn computing, an early tradition in the hacker community (especially at MIT) was to choose acronyms and abbreviations that referred humorously to themselves or to other abbreviations. Perhaps the earliest example in this context, from 1960 the backronym \"Mash Until No Good\" was created to describe Mung, and a while after it was revised to \"Mung Until No Good\". It lived on as a recursive command in the editing language TECO. In 1977 or 1978 came TINT (\"TINT Is Not TECO\"), an editor for MagicSix written (and named) by Ted Anderson. This inspired the two MIT Lisp Machine editors called EINE (\"EINE Is Not Emacs\", German for \"one\") and ZWEI (\"ZWEI Was EINE Initially\", German for \"two\"). These were followed by Richard Stallman's GNU (GNU's Not Unix). Many others also include negatives, such as denials that the thing defined is or resembles something else (which the thing defined does in fact resemble or is even derived from), to indicate that, despite the similarities, it was distinct from the program on which it was based.\n\nAn earlier example appears in a 1976 textbook on data structures, in which the pseudo-language SPARKS is used to define the algorithms discussed in the text. \"SPARKS\" is claimed to be a non-acronymic name, but \"several cute ideas have been suggested\" as expansions of the name. One of the suggestions is \"Smart Programmers Are Required to Know SPARKS\". (this example is tail recursive)\n\n\n\nSome organizations have been named or renamed in this way:\n\n\n"}
{"id": "20110874", "url": "https://en.wikipedia.org/wiki?curid=20110874", "title": "Reference", "text": "Reference\n\nReference is a relation between objects in which one object designates, or acts as a means by which to connect to or link to, another object. The first object in this relation is said to \"refer to\" the second object. It is called a \"name\" for the second object. The second object, the one to which the first object refers, is called the \"referent\" of the first object. A name is usually a phrase or expression, or some other symbolic representation. Its referent may be anything – a material object, a person, an event, an activity, or an abstract concept.\n\nReferences can take on many forms, including: a thought, a sensory perception that is audible (onomatopoeia), visual (text), olfactory, or tactile, emotional state, relationship with other, spacetime coordinate, symbolic or alpha-numeric, a physical object or an energy projection. In some cases, methods are used that intentionally hide the reference from some observers, as in cryptography.\n\nReferences feature in many spheres of human activity and knowledge, and the term adopts shades of meaning particular to the contexts in which it is used. Some of them are described in the sections below.\n\nThe word \"reference\" is derived from Middle English \"referren\", from Middle French \"référer\", from Latin \"referre\", \"to carry back\", formed from the prefix \"re\"- and \"ferre\", \"to bear\". A number of words derive from the same root, including \"refer\", \"referee\", \"referential\", \"referent\", \"referendum\".\n\nThe verb \"refer (to)\" and its derivatives may carry the sense of \"link to\" or \"connect to\", as in the meanings of \"reference\" described in this article. Another sense is \"consult\"; this is reflected in such expressions as reference work, reference desk, job reference, etc.\n\nIn semantics, reference is generally construed as the relationships between nouns or pronouns and objects that are named by them. Hence, the word \"John\" refers to the person John. The word \"it\" refers to some previously specified object. The object referred to is called the \"referent\" of the word. Sometimes the word-object relation is called \"denotation\"; the word denotes the object. The converse relation, the relation from object to word, is called \"exemplification\"; the object exemplifies what the word denotes. In syntactic analysis, if a word refers to a previous word, the previous word is called the \"antecedent\".\n\nGottlob Frege argued that reference cannot be treated as identical with meaning: \"Hesperus\" (an ancient Greek name for the evening star) and \"Phosphorus\" (an ancient Greek name for the morning star) both refer to Venus, but the astronomical fact that '\"Hesperus\" is \"Phosphorus\"' can still be informative, even if the \"meanings\" of \"Hesperus\" and \"Phosphorus\" are already known. This problem led Frege to distinguish between the sense and reference of a word. Some cases seem to be too complicated to be classified within this framework; the acceptance of the notion of secondary reference may be necessary to fill the gap. See also Opaque context.\n\nThe very concept of the linguistic sign is the combination of content and expression, the former of which may refer entities in the world or refer more abstract concepts, e.g. thought.\nCertain parts of speech exist only to express reference, namely anaphora such as pronouns. The subset of reflexives expresses co-reference of two participants in a sentence. These could be the agent (actor) and patient (acted on), as in \"The man washed himself\", the theme and recipient, as in \"I showed Mary to herself\", or various other possible combinations.\n\nIn computer science, references are data types that refer to an object elsewhere in memory and are used to construct a wide variety of data structures, such as linked lists. Generally, a reference is a value that enables a program to directly access the particular data item. Most programming languages support some form of reference. For the specific type of reference used in the C++ language, see reference (C++).\n\nThe notion of reference is also important in relational database theory; see referential integrity.\n\nReferences to many types of printed matter may come in an electronic or machine-readable form. For books, there exists the ISBN and for journal articles, the Digital object identifier (DOI) is gaining relevance. Information on the Internet may be referred to by a Uniform Resource Identifier (URI).\n\nIn terms of mental processing, a self-reference is used in psychology to establish identification with a mental state during self-analysis. This seeks to allow the individual to develop own frames of reference in a greater state of immediate awareness. However, it can also lead to circular reasoning, preventing evolution of thought.\n\nAccording to Perceptual Control Theory (PCT), a reference condition is the state toward which a control system's output tends to alter a controlled quantity. The main proposition is that \"All behavior is oriented all of the time around the control of certain quantities with respect to specific reference conditions.\"\n\nIn academics and scholarship, an author-title-date information in bibliographies and footnotes, specifying complete works of other people. Copying of material by another author without proper citation or without required permissions is plagiarism.\n\nKeeping a diary allows an individual to use references for personal organization, whether or not anyone else understands the systems of reference used. However, scholars have studied methods of reference because of their key role in communication and co-operation between \"different\" people, and also because of misunderstandings that can arise. Modern academic study of reference has been developing since the 19th century.\n\nIn scholarship, a reference may be a citation of a text that has been used in the creation of a piece of work such as an essay, report, or oration. Its primary purpose is to allow people who read such work to examine the author's sources, either for validity or to learn more about the subject. Such items are often listed at the end of an article or book in a section marked \"Bibliography\" or \"References\". A bibliographical section often contains works not cited by the author, but used as background reading or listed as potentially useful to the reader. A reference section contains only those works cited by the author(s) in the main text.\n\nIn patent law, a reference is a document that can be used to show the state of knowledge at a given time and that therefore may make a claimed invention obvious or anticipated. Examples of references are patents of any country, magazine articles, Ph.D. theses that are indexed and thus accessible to those interested in finding information about the subject matter, and to some extent Internet material that is similarly accessible.\n\nIn art, a reference is an item from which a work is based. This may include:\nAnother example of reference is samples of various musical works being incorporated into a new one.\n\n\n"}
{"id": "49632241", "url": "https://en.wikipedia.org/wiki?curid=49632241", "title": "Self-perpetuation", "text": "Self-perpetuation\n\nSelf-perpetuation, the capability of something to cause itself to continue to exist, is one of the main characteristics of life. Organisms' capability of reproduction leads to self-perpetuation of the species, if not to the individual. Populations self-perpetuate and grow. Entire ecosystems show homeostasis, and thus perpetuate themselves. The slow modifying effect of succession and similar shifts in the composition of the system can, however, not be neglected in the long run. Overall, life's object's capabilities of self-perpetuation are always accompanied by evolution, a perfect steady state of the biological system is never reached. Sexual reproduction is also a form of imperfect self-replication and thus imperfect self-perpetuation because of recombination and mutation. Organisms are not like self-replicating machine but amass random modifications from generation to generation. The property of self-perpetuation in the strict sense thus only applies to life itself.\n\nIn a social context, self-perpetuation is tied to reflexivity and (usually) positive feedback loops:\nDepending on the time scope or the context, self-perpetuation either depends on self-sustainability, or is equivalent to it. While we may talk about the self-sustainability of an ecosystem, this depends amongst other factor on the self-perpetuation of its constituting species.\n\nIn computer science, self-reproducing programs constitute an incomplete metaphor for self-perpetuation. A better analogue can be seen in computer viruses which are actually able to self-reproduce - given a suitable computing environment.\n\n"}
{"id": "4106285", "url": "https://en.wikipedia.org/wiki?curid=4106285", "title": "Self-referential encoding", "text": "Self-referential encoding\n\nEvery day, people are presented with endless amounts of information, and in an effort to help keep track and organize this information, people must be able to recognize, differentiate and store information. One way to do that is to organize information as it pertains to the self. The overall concept of self-reference suggests that people interpret incoming information in relation to themselves, using their self-concept as a background for new information. Examples include being able to attribute personality traits to oneself or to identify recollected episodes as being personal memories of the past. The implications of self-referential processing are evident in many psychological phenomena. For example, the \"cocktail party effect\" notes that people attend to the sound of their names even during other conversation or more prominent, distracting noise. Also, people tend to evaluate things related to themselves more positively (This is thought to be an aspect of implicit self-esteem). For example, people tend to prefer their own initials over other letters. The self-reference effect (SRE) has received the most attention through investigations into memory. The concepts of self-referential encoding and the SRE rely on the notion that relating information to the self during the process of encoding it in memory facilitates recall, hence the effect of self-reference on memory. In essence, researchers have investigated the potential mnemonic properties of self-reference.\n\nResearch includes investigations into self-schema, self-concept and self-awareness as providing the foundation for self-reference's role in memory. Multiple explanations for the self-reference effect in memory exist, leading to a debate about the underlying processes involved in the self-reference effect. In addition, through the exploration of the self-reference effect, other psychological concepts have been discovered or supported, including simulation theory and the effect.\nAfter researchers developed a concrete understanding of the self-reference effect, many expanded their investigations to consider the self-reference effect in particular groups like those with autism spectrum disorders or those experiencing depression.\n\nSelf-knowledge can be categorized by structures in memory or schemata. A self-schema is a set of facts or beliefs that one has about themselves. For any given trait, an individual may or may not be \"schematic\"; that is, the individual may or may not think about themselves as to where they stand on that trait. For example, people who think of themselves as very overweight or who identify themselves to a greater extent based on their body weight would be considered \"schematic\" on the attribute of body weight. Thus, many everyday events, such as going out for a meal or discussing a friend's eating habits, could induce thoughts about the self. When people relate information to something that has to do with the self, it facilitates memory. Self-descriptive adjectives that fit into one's self-schema are easier to remember than adjectives not viewed as related to the self. Thus, the self-schema is an aspect of oneself that is used as an encoding structure that brings upon memory of information consistent with one's self-schema. Memories that are elaborate and well encoded are usually the result of self-referent correlations during the process of remembering. During the process of encoding, trait representations are encoded in long term memory either directly or indirectly. When they are directly encoded, it is in terms of relating to the self, and when it is indirectly encoded it is done through spouts of episodic information instead of information about the self.\n\nSelf-schema is often used as somewhat of a database for encoding personal data. The self-schema is also used by paying selective attention to outside information and internalizing that information more deeply in one's memory depending on how much that information relates to their schema. When self-schema is engaged, traits that go along with one's view of themselves are better remembered and recalled. These traits are also often recalled much better when processed with respect to the self. Similarly, items that are encoded with the self are based on one's self-schema. Processing the information should balance out when recalled for individuals who have a self-schema that goes along with the information.\n\nSelf-schemas do not necessarily only involve individual traits. People self-categorize at different levels that range from more personal to more social. Self-schemas have three main categories which play a role: the personal self, the relational self, and the collective self. The personal self deals with individual level characteristics, the relational self deals with intimate relationship partners, and the collective self deals with group identities, relating to self-important social groups to which one belongs (e.g., one's family or university). Information that is related to any type of self-schema, including group-related knowledge structures facilitates memory.\n\nIn order for the self to be an effective encoding mechanism, it must be a uniform, consistent, well-developed schema. It has been shown that identity exploration leads to the development of self-knowledge which facilitates self-judgments. Identity exploration led to shorter decision times, higher confidence ratings and more intrusions in memory tasks. Previous researchers hypothesized that words compatible with a person's self-schema are easily accessible in memory and are more likely than incompatible words to intrude on a schema-irrelevant memory task. In one experiment, when participants were asked to decide if certain adjectives were \"like me\" or \"not like me,\" they made the decisions faster when the words were compatible with their self-schema.\n\nHowever, despite the existence of the self-reference effect when considering schemata consistent adjectives, the connection between the self and memory can lead to a larger number of mistakes in recognition, commonly referred to as false alarms. Rogers et al. (1979) found that people are more likely to falsely recognize adjectives they had previously designated to be self-descriptive. Expanding on this, Strube et al. (1986) found that false alarms occurred more for self-schema consistent content, presumably because the presence of such words in the schema makes them more accessible in memory.\n\nIn addition to investigating the self-reference effect in regards to schemata consistent information, Strube et al. discussed how counter schemata information relates to this framework. They noted that the pattern of making correct decisions more rapidly did not hold when considering words that countered a person's self-schema, presumably because they were difficult to integrate into memory due to lack of a preexisting structure. That is, they lacked the organizational structure of encoding because they did not fall into the \"like me\" category, and elaboration would not work because prior connections to the adjective did not exist.\n\nTwo of the most common functions of the self receiving significant attention in research are the self-acting to organize the individual's understanding of the social environment, and the self functioning to regulate behavior through self-evaluation. The concept of self-awareness is considered to be the foundational principle for both functions of the self. Some research presents self-awareness in terms of self-focused attention whereas Hull and Levy suggest that self-awareness refers to the encoding of information based on its relevance to the self. Based on the latter interpretation of self-awareness, individuals must identify the aspects of situations that are relevant to themselves and their behavior will be shaped accordingly. Hull and Levy suggest that self-awareness corresponds to the encoding of information cued by self-symbolic stimuli, and examine the idea of self-awareness as a method of encoding. They structured an investigation that examined self-referent encoding in individuals with different levels of self-awareness, predicting that individuals with higher levels of self-consciousness would encode self-relevant information more deeply than other information, and that they would encode it more deeply than individuals with low levels of self-consciousness. The results of their investigation supported their hypothesis that self-focused attention is not enough to explain the role of self-awareness on attribution. Their results suggest that self-awareness leads to increased sensitivity to the situationally defined meanings of behavior, and therefore organizes the individual's understanding of the social environment. The research presented by Hull and Levy led to future research on the encoding of information associated with self-awareness.\n\nIn later research, Hull and colleagues examined the associations between self-referential encoding, self-consciousness and the extent to which a stimulus is consistent with self-knowledge. They first assumed that the encoding of a stimulus is facilitated if an individual's working memory already contains information consistent with the stimulus, and suggested that self-consciousness as an encoding mechanism relies on an individual's self-knowledge. It is known that situational and dispositional factors may activate certain pools of knowledge, moving them into working memory, and guiding the processing of certain stimulus information.\n\nIn order to better understand the idea of activating information in memory, Hull et al. presented an example of how information is activated. They referred to the sentence \"The robber took the money from the bank\". In English, the word bank has two applicable meanings in the context of this sentence (monetary institution and river shore). However, the monetary institution meaning of the word is more highly activated in this context due to the addition of the words robber and money to the sentence, because they are associatively relevant and therefore pull the monetary institution definition for bank into working memory. Once information is added to working memory, meanings and associations are more easily drawn. Therefore, the meaning of this example sentence is almost universally understood.\n\nIn reference to self-consciousness and self-reference, the connection between self-consciousness and self-referent encoding relies on such information activation. Research suggests that self-consciousness activates knowledge relating to the self, thereby guiding the processing of self-relevant information. Three experiments conducted by Hull and colleagues provided evidence that a manipulation of accessible self-knowledge impacts self-referent encoding based on the self-relevance of such information, individual differences in the accessibility of self-knowledge (self-consciousness) impacts perception, and a mediation relationship exists between self-consciousness and individual differences in self-referential encoding.\n\nSimilar to how self-awareness impacts the availability of self-knowledge and the encoding of self-relevant information, through the development of the self-schema, people develop and maintain certain personality characteristics leading to a variety of behavior patterns. Research has been done on the differences between Type A and Type B behavior patterns, focusing on how people in each group respond to environmental information and their interpretation of the performance of others and themselves. It has been found that Type A behavior is characterized by competitive achievement striving, time urgency and hostility, whereas Type B is usually defined as an absence of Type A characteristics. When investigating causal attributions for hypothetical positive and negative outcomes, Strube et al. found that Type A individuals were more self-serving, in that they took greater responsibility for positive than negative effects. Strube and colleagues argued that this could be a result of the fact that schema-consistent information is more easily remembered and the ease with which past successes and failures are recalled, determined by self-schema, would impact attributions. It is reasonable to believe that Type A's might recall successes more easily and hence be more self-serving.\n\nInfluential psychologists Craik and Lockhart laid the groundwork for research focused on self-referential encoding and memory. In 1972 they proposed their Depth of Processing framework which suggests that memory retention depends on how the stimulus material was encoded in memory. Their original research considered structural, phonemic, and semantic encoding tasks, and showed that semantic encoding is the best method to aid in recall. They asked participants to rate 40 descriptive adjectives on one of four tasks; Structural (Big font or small font?), Phonemic (Rhymes with xxx?), Semantic (Means same as xxx?), or Self-reference (Describes you?). This was then followed by an \"incidental recall task\". This is where participants are asked, without prior warning, to recall as many of the words they had seen as possible within a given time limit. Craik and Tulving's original experiment showed that structural and phonemic tasks lead only to \"shallow\" encoding, while the semantic tasks lead to \"deep\" encoding and resulted in better recall.\n\nHowever, in 1977, it was shown that self-relevant or self-descriptive encoding leads to even better recall than semantic tasks. Experts suggest that the call on associative memory required by semantic tasks is what provides the advantage over structural or phonemic tasks, but is not enough to surpass the benefit provided by self-referential encoding. The fact that self-reference was shown to be a stronger memory encoding method than semantic tasks is what led to more significant interest in the field One early and significant experiment aimed to place self-reference on Craik and Lockhart's depth of processing hierarchy, and suggested that self-reference was a more beneficial encoding method than semantic tasks. In this experiment, participants filled out self-ratings on 84 adjectives. Months later, these participants were revisited and were randomly shown 42 of those words. They then had to select the group of 42 \"revisited\" words out of the total original list. The researchers argued that if the \"self\" was involved in memory retrieval, participants would incorrectly recognize words that were more self-descriptive In another experiment, subjects answered yes or no to cue questions about 40 adjective in 4 tasks (structural, phonemic, semantic and self-referential) and later had to recall the adjectives. This experiment validated the strength of self-reference as an encoding method, and indicated it developed a stronger memory trace than the semantic task.\n\nResearchers are implementing a new strategy by developing different encoding tasks that enhance memory very similarly to self-referential encoding. Symons (1990) had findings that went against the norm when he was unable to find evidence of self-schematicity in the self-reference effect. Another finding was that when referencing gender and religion, there was a low memory recall when compared with referencing the self. A meta-analysis by Symons and Johnson (1997) showed self-reference resulting in better memory in comparison to tasks relying on semantic encoding or other-referent encoding. According to Symons and Johnson, self-referencing questions elicit elaboration and organization in memory, both of which creating a deeper encoding and thus facilitate memory.\n\nTheorists that favor the view that the self has a special role believe that the self leads to more in depth processing, leading to easier recall during self-reference tasks. Theorists also promote the self-schema as being one of the sole inhibitors that allow for recall from deep memory. Thorndyke and Hayes-Roth had the goal of focusing on the process made by the active memory schemata. Sex-typed individuals recall trait adjectives that go along with their sex role more quickly than trait adjectives that are not. During the process of free recall, these individuals also showed more patterns for gender clustering than other sexually typed individuals.\n\nAs research on self-referential encoding became more prolific, some psychologists took an opportunity to delineate specific self-referential encoding tasks. It is noted that descriptive tasks are those that require participants to determine if a stimulus word can be classified as \"self-descriptive.\" Autobiographical tasks are those that require participants to use the stimulus word as a cue to recall an autobiographical memory. Results from experiments that differentiated between these types of self-referential encoding found that they both produced better recall than semantic tasks, and neither was more advantageous than the other. However, research does suggest that the two types of self-referential encoding do rely on different processes to facilitate memory. In most experiments discussed, these types of self- referential encoding were not differentiated.\n\nIn a typical self-reference task, adjectives are presented and classified as either self-descriptive or not. For example, in a study by Dobson and Shaw, adjectives about the self that were preselected were given to the participants and they decide whether or not the adjectives are self-descriptive. The basis for making certain judgments, decisions, inferences and decisions is a self-referent encoding task. If two items are classified as self-descriptive there is no reason one trait would not be equally as easy to retrieve as the other on a self-reference task.\n\nWhile a significant amount of research supports the existence of the self-reference effect, the processes behind it are not well understood. However, multiple hypotheses have been introduced, and two main arguments have been developed: the elaborative processing hypothesis and the organizational processing hypothesis. Encodings in reference to the self are so elaborate because of the information one has about the self. Information encoded with the self is better remembered than information encoded with reference to something else.\n\nElaboration refers to the encoding of a single word by forming connections between it and other material already stored in memory. By creating these connections between the stimulus word and other material already in memory, multiple routes for retrieval of the stimulus word are formed. Based on the depth of processing framework, memory retention increases as elaboration during encoding increases. The Elaborative Processing Hypothesis would suggest that any encoding task that leads to the development of the most trace elaboration or associations is the best for memory retention. Additional research on the depth of processing hierarchy suggests that self-reference is the superior method of information encoding. The elaborative hypothesis would suggest this is because self-reference creates the most elaborate trace, due to the many links that can be made between the stimulus and information about the self already in memory.\n\nThe organizational processing hypothesis was proposed by Klein and Kihlstrom. This hypothesis suggests that encoding is best prompted by considering stimulus words in relation to one another. This thought process and relational thinking creates word to word associations. These inter-item associations are paths in memory that can be used during retrieval. Also, the category labels that define the relations between stimulus items can be used as item cues. Evidence of the organizational component of encoding is demonstrated through the clustering of words during recall. Word clustering during recall indicates that relational information was used to store the words in memory. Rogers, Kuiper and Kirker showed that self-referential judgments were more likely to encourage organization than semantic ones. Therefore, they suggested the self-reference effect was likely due to the organizational processing endured by self-referential encoding.\n\nStructural, phonemic and semantic tasks within the depth of processing paradigm require words to be considered individually, and lend themselves to an elaborative approach. As such, it can be argued that self-referential encoding is superior because it leads to an indirect division of words into categories: words that describe me versus words that do not. Due to this connection between self-reference and organizational processing, further research has been done on this area. Klein and Kihlstrom's research suggests first that, like previous research, self-reference led to better recall than semantic and structural encoding. Second, they found that self-referentially encoded words were more clustered in recall than words from other tasks, suggesting higher levels of organizational processing. From this they concluded that the organization, not encoding task, is what makes self-referential encoding superior \n\nPsychologists Einstein and Hunt showed that both elaborative processing and organizational processing facilitate recall. However, their research argues that the effectiveness of either approach depends on how related the stimulus words are to one another. A list of highly related stimulus words would be better encoded using the elaborative method. The relations between the words would be evident to subjects; therefore, they would not gain any additional pathways for retrieval by encoding the words based on their categorical membership. Instead, the other information gained through elaborative processing would be more beneficial. On the other hand, a list of stimulus words with little relation would be better stored to memory through the organizational method. Since the words have no obvious connection to one another, subjects would likely encode them individually, using an elaborative approach. Since relational information wouldn't be readily detected, focusing on it would add to memory by creating new traces for retrieval. Superior recall was better explained by a combination of elaboration and organization.\nUltimately, the exact processes behind self-referential encoding that makes it superior to other encoding tasks are still under debate. Research suggests that if elaborative processing is behind self-referential encoding, a self-referential task should have the same effect as an elaborative task, whereas if organizational processing underlies the self-reference effect self-referential encoding tasks should function like organizational tasks. To test this, Klein and Loftus ran a 3x2 study testing organizational, elaborative and self-referential encoding with lists of 30 related or unrelated words. When participants were asked to memorize the unrelated list, recall and clustering were higher for the organizational task, which produced almost equal results to the self-referential task, suggesting that has an organizational basis. For the list of related words, the elaborative task led to better recall and had matched results to the self-reference task, suggesting an elaborative basis. This research, then, suggests that the self-reference effect cannot be explained by a single type of processing. Instead, self-referential encoding must lead to information in memory that incorporates item specific and relational information.\n\nOverall, the SRE relies on the unique mnemonic aspects of the self. Ultimately, if the research is suggesting that the self has superior elaborative or organizational properties, information related to the self should be more easily remembered and recalled. The research presented suggests that self-referential encoding is superior because it promotes organization and elaboration simultaneously, and provides self-relevant categories that promote recall.\n\nThe field of social brain science is aimed at examining the neural foundations of social behavior. Neuroimaging and neuropsychology have led to the examination of neuroanatomy and its connection to psychological topics. Through this research, neuropsychologists have found a connection between social cognitive functioning and the medial prefrontal cortex (mPFC). In addition, the mPFC has been connected to reflection and introspection about personal mental states. Supporting these findings, it has been shown that damage to the mPFC is connected to impairments with self-reflection, introspection and daydreaming, as well as social competence, but not other areas of functioning. As such, the mPFC has been connected to self-referential processing.\n\nThe research discussed by those focusing on the neuroanatomy of self-referential processing included similar tasks to the memory and depth of processing research discussed previously. When participants were asked to judge adjectives based in whether or not they were self-descriptive, it was noted that the more self-relevant the trait, the stronger the activation of the mPFC. In addition, it was shown that the mPFC was activated during the appraisal of one's own personality traits, as well as during trait retrieval. One study showed that the more activity in the mPFC during self-referential judgments, the more likely the word was to be remembered on a subsequent surprise memory test. These results suggest that the mPFC is involved in both self-referential processing and in creating self-relevant memories.\n\nMedial prefrontal cortex (mPFC) activation occurs during processing of self-relevant information. When self-referent judgment is more relatable and less negative, the mFPC is activated. Finding support clear cut circuits that have high levels of activation when cognitive and emotional aspects of self-reflection are present. The caudate nucleus has not been associated with self-reference before, however, Fossati and colleagues found activity while participants were retrieving self-relevant trait adjectives. The ventral anterior cingulate cortex (vACC) is also a part of the brain that becomes activated when there are signs of self-referencing and processing. The vACC is activated when self-descriptive information is negative. There is also pCC (posterior cingulate cortex) activity seen in neuroimaging studies during self-referential processing.\n\nGiven all of the neurological support for the effect of self-reference on encoding and memory, there is still a debate in the psychological community about whether or not the self-reference effect signifies a special functional role played by the self in cognition. Generally, this question is met by people that have two opposing views on the processes behind self-reference. On one side of the debate, people believe that the self has special mnemonic abilities because it is a unique cognitive structure. On the other side, people support the arguments described above that suggest there is no special structure, but instead, the self-reference effect is simply a part of the standard depth of processing hierarchy. Since the overall hypothesis is the same for both sides of the debate, that self-relevant material leads to enhanced memory, it is difficult to test them using strictly behavioral measures. Therefore, PET and fMRI scans have been used to see the neural marker of self-referential mental activity.\n\nPrevious studies have shown that areas of the left prefrontal cortex are activated during semantic encoding. Therefore, if the self-reference effect works the same way, as part of the depth of processing hierarchy, the same brain region should be activated when judging traits related to the self. However, if the self has unique mnemonic properties, then self-referential tasks should activate brain regions distinct from those activated during semantic tasks. The field is still at is infancy, but future work on this hypothesis might help to settle the debate about the underlying processes of self-referential encoding.\n\nWhile not able to completely settle the debate over the foundation of self-referential processing, studies on the neurological aspect of personality trait judgments did lead to a related, significant result. It has been shown that judging personality traits about oneself and a close friend activated overlapping brain regions, and the activated regions have all been implicated in self-reference. Noting the similarity between making self-judgments and judgments about close others led to the introduction of the simulation theory of empathy. Simulation theory rests on the idea that one can make inferences about others by using the knowledge they have about themselves. In essence, the theory suggests that people use self-reflection to understand or predict the mental state of others. The more similar a person perceives another to be, the more active the mPFC has shown to be, suggesting more deep or intricate self-reference. However, this effect can cause people to make inaccurate judgments about others or to believe that their own opinions are representative of others in general. This misrepresentation is referred to as the false-consensus effect.\n\nIn addition to simulation theory, other expansions of the self-reference effect have been examined. Through studying the self, researchers have found that the self consists of many independent cognitive representations. For example, the personal self composed of individual characteristics is separate from the relational self which is based on relationships with significant others. These two forms of self are again separate from the collective self which corresponds to a particular group identity. Noting the existence of the collective self and the different group identities that combine to form such a self-representation led researchers to question if information stored in reference to a social group identity has the same effects in memory as information stored in reference to the individual self. In essence, researchers questioned if the self-reference effect can be extended to include situations where the self is more socially defined, producing a group-reference effect.\n\nPrevious research supports the idea that the group-reference effect should exist from a theoretical standpoint. First, the self-expansion model argues that individuals incorporate characteristics of their significant others (or other in-group members into the development of their self-concept. From this model, it is reasonable to conclude that characteristics that are common to both oneself and their significant others (or in-group members) would be more accessible. Second, the previous research discussed suggests that the self-reference effect is due to some combination of organizational, elaborative, mental cueing or evaluative properties of self-referential encoding tasks. Given that we have significant stores of knowledge about our social identities, and such collective identities provide an organizational framework, it is reasonable to assume that a group-reference task would operate similar to that of a self-reference task.\n\nIn order to test these claims, Johnson and colleagues aimed to test whether the self-reference effect generalized to group level identities. Their first study was structured to simply assess if group-reference influenced subsequent memory. In their experiment, they used membership at a particular university as the group of reference. They included group-reference, self-reference and semantic tasks. The experiment replicated the self-reference effect, consistent with previous research. In addition, evidence for a group-reference effect was found. Group-referenced encoding produced better recall than the semantic tasks, and the level of recall from the group-referenced task was not significantly different from the self-referenced task.\n\nDespite finding evidence of a group-reference effect, Johnson and colleagues pointed out that people identify with numerous groups, each with unique characteristics. Therefore, in order to reach conclusive evidence of a group-reference effect, alternative group targets need to be considered. In a second experiment by Johnson et al., the group of reference was modified to be the family of the individual. This group has fewer exemplars than the pool of university students, and affective considerations of the family as a group should be strong. No specific instructions or definitions were provided for family, allowing individuals to consider either the group as a whole (prototype) or specific exemplars (group). When the experiment was repeated using family as the group of reference, group-reference produced recall as much as self-reference. The mean number of recall for the group-reference was higher than self-reference. Participants indicated that they considered both the prototype and individual exemplars when responding to the questions, suggesting that the magnitude of the group-reference effect might not be dependent on the number of exemplars in the target group.\nBoth experiments presented by Johnson et al. found evidence for the group-reference effect. However, these conclusions are limited to the target groups of university students and family. Other research included gender (males and females) and religion (Jewish) as the reference groups and the group-reference effect on memory was not as evident. The group-reference recall for these two groups was not significantly more advantageous than the semantic task. Questioning what characteristics of reference groups that lead to the group-reference effect, a meta-analysis of all four group-reference conditions was performed. This analysis found that self-reference emerged as the most powerful encoding device; however, evidence was found to support the existence of a group-reference effect. The size of the reference groups and number of specific, individual exemplars was hypothesized to influence the existence of the group-reference effect. In addition, accessibility and level of knowledge about group members may also impact such an effect. So, while university students is a much larger group than family, individual exemplars may be more readily accessible than those in a religious group. Similarly, different cognitive representations were hypothesized to influence the group-reference effect. When a larger group is considered, people may be more likely to consider a prototype which may lead to fewer elaborations and cues later on. Smaller groups may lead to relying on the prototype and specific exemplars. Finally, desirability judgments that influence later processing may be influenced by self-reference and certain group-reference tasks. Individuals may be more sensitive to evaluative implications for the personal self and some group identities, but not others.\n\nGroups are also a major part of the self; therefore we attribute the role that different groups play in our self-concept also play a role in the self-reference effect. We process information about group members similarly to how we process for ourselves. Recall of remarks referencing our home and our self and group to familiarity of those aspects of our self. Reference to the self and social group and the identity that comes along with being a part of a social group are equally affective for memory. This is especially true when the groups are small, rather than large.\n\nUltimately, the group-reference effect provides evidence to explain the tendency to notice or pay attention to and remember statements made in regard to our home when traveling in a foreign place. Considering the proposal that groups form part of the self, this phenomenon can be considered an extension of the self-reference effect. Similar to the memorable nature of references to a person's individual self, references to social identities are seemed to be privileged in memory as well.\n\nOnce the foundation of research on self-referential encoding was established, psychologists began to explore how the concept applied to different groups of people, and connected to different phenomena.\n\nIndividuals diagnosed with autism spectrum disorders (ASDs) can display a wide range of symptoms. Some of the most common characteristics of individuals with ASDs include impairments with social functioning, language and communication difficulties, repetitive behaviors and restricted interests. In addition, it is often noted that these individuals are more \"self-focused.\" That is, they have difficulty seeing things from another's perspective. Despite being self-focused, though, research has shown that individuals with ASD's often have difficulty identifying or describing their emotions or the emotions of others. When asked to describe their daily experiences, responses from individuals on the autism spectrum tended to focus more on physical descriptions rather than mental and emotional states. In regards to their social interactions and behavior differences, it is thought that these individuals lack top down control, and therefore, their bottom up decisions remain unchecked. This simply suggests that these individuals cannot use their prior knowledge and memory to make sense of new input, but instead react to each new input individually, compiling them to make a whole picture \n\nNoting the difficulty individuals with ASDs experience with self-awareness, it was thought that they might have difficulty with self-related memory processes. Psychologists questioned if these individuals would show the typical self-reference effect in memory. In one Depth of Processing Study, participants were asked questions about the descriptiveness of certain stimulus words. However, unlike previous DOP studies that focused on phonemic, structural, semantic and self-referential tasks, the tasks were altered for this experiment. To test the referential abilities of individuals with ASD's, the encoding tasks were divided into: \"the self,\" asking to what extent a stimulus word described oneself, \"similar close other,\" asking to what extent a stimulus word was descriptive of one's best friend, \"dissimilar non-close other,\" asking to what extent a stimulus word was descriptive of Harry Potter, and a control group that was asked to determine the number of syllables in each word. Following these encoding tasks, participants were given thirty minutes before a surprise memory task. It was found that individuals with ASD's had no impairment in memory for words encoded in the syllable or dissimilar non-close other condition. However, they had decreased memory for words related to the self.\n\nTherefore, while research suggests that self-referentially encoded information is encoded more deeply than other information, the research on individuals with ASD's showed no advantage for memory recognition with self-reference tasks over semantic encoding tasks. This suggests that individuals with ASD's don't preferentially encode self-relevant information. Psychologists have investigated the biological basis for the decreased self-reference effect among individuals with Autism Spectrum Disorders and have suggested that it may be due to less specialized neural activity in the mPFC for those individuals. However, while individuals with ASD's showed smaller self-reference effects than the control group, some evidence of a self-reference effect was evident in some cases. This indicates that self-referent impairments are a matter of degree, not total absence.\n\nLombardo and his colleagues measured empathy among individuals with ASD's, and showed that these individuals scored lower than the control group on all empathy measures. This may be a result of the difficulty for these individuals to understand or take the perspective of others, in conjunction with their difficulty identifying emotions. This has implications for simulation theory, because these individuals are unable to use their self-knowledge to make conclusions about similar others.\n\nUltimately, the research suggests that people with ASD's might benefit from being more self-focused. The better their ability to reflect on themselves, the better the can mentalize with others.\n\nThere are three possible relations between cognitive processes and anxiety and depression. The first is whether cognitive processes are actually caused by the onset of clinically diagnosed symptoms of major depression or just generalized sadness or anxiousness. The second is whether emotional disorders such as depression and anxiety are able to be considered as caused by cognitions. And the third is whether different specific cognitive processes are able to be considered associates of different disorders. Kovacs and Beck (1977) posited a schematic model of depression where an already depressed self was primed by outside prompts that negatively impacted cognitive illusions of the world in the eye of oneself. These prompts only led participants to a more depressive series of emotions and behavior. The results from the study done by Derry and Kuiper supported Beck's theory that a negative self-schema is present in people, especially those with depressive disorder. Depressed individuals attribute depressive adjectives to themselves more than nondepressive adjectives. Those suffering from a more mild case of depression have trouble deciphering between the traits of themselves and others which results in a loss of their self-esteem and their negative self-evaluation. A depressive schema is what causes the negativity reported by those suffering from depression. Kuiper and Derry found that self-referent recall enhancement was limited only to nondepressed content.\n\nGenerally, self-focus is association with negative emotions. In particular private self-focus is more strongly associated with depression than public self-focus. Results from brain-imaging studies shows\nthat during self-referential processing, those with major depressive disorder show greater activation in the medial prefrontal cortex, suggesting that depressed individuals may be exhibiting greater cognitive control than\nnon-depressed individuals when processing self-relevant information.\n"}
{"id": "12153317", "url": "https://en.wikipedia.org/wiki?curid=12153317", "title": "Sixpenny Library", "text": "Sixpenny Library\n\nErnest Benn Limited’s Sixpenny Library is a complete series of reference books published in the late 1920s and early 1930s. The library included over one hundred and eighty volumes. The series was edited by William Rose, who solicited current authorities in such areas as history, literature, religion, psychology, science, and economics. Some contributing authors were Hilaire Belloc, Maurice Baring, J.B. Priestley, Sir (later Lord) Robert Baden-Powell, Sir Oliver Lodge, S.V Keeling and Sir Ernest Benn himself. \"The Spectator\", in November 1927, after announcing some the latest additions to \"Messrs Benn's excellent Sixpenny Library\" devoted a further paragraph to his contribution on Trade (both of which are free to read online). Partial lists of the books published in the series can be found here and here.\n\nThe books were praised by critics for their excellence, brevity, and inexpensive price.\n"}
{"id": "40849944", "url": "https://en.wikipedia.org/wiki?curid=40849944", "title": "Sources for the historicity of Jesus", "text": "Sources for the historicity of Jesus\n\nChristian sources, such as the New Testament books in the Christian Bible, include detailed stories about Jesus but scholars differ on the historicity of specific episodes described in the Biblical accounts of Jesus. The only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by the order of the Roman Prefect Pontius Pilate.\n\nNon-Christian sources that are used to study and establish the historicity of Jesus include Jewish sources such as Josephus, and Roman sources such as Tacitus. These sources are compared to Christian sources such as the Pauline Epistles and the Synoptic Gospels. These sources are usually independent of each other (e.g. Jewish sources do not draw upon Roman sources), and similarities and differences between them are used in the authentication process.\n\nIn a review of the state of research, the Jewish scholar Amy-Jill Levine stated that \"no single picture of Jesus has convinced all, or even most scholars\" and that all portraits of Jesus are subject to criticism by some group of scholars.\n\nThe writings of the 1st century Romano-Jewish historian Flavius Josephus include references to Jesus and the origins of Christianity. Josephus' \"Antiquities of the Jews\", written around 93–94 CE, includes two references to Jesus in Books and .\n\nOf the two passages, the James passage in Book 20 is used by scholars to support the existence of Jesus, the \"Testimonium Flavianum\" in Book 18 his crucifixion. Josephus' James passage attests to the existence of Jesus as a historical person and that some of his contemporaries considered him the Messiah. According to Bart Ehrman, Josephus' passage about Jesus was altered by a Christian scribe, including the reference to Jesus as the Messiah.\n\nA textual argument against the authenticity of the James passage is that the use of the term \"Christos\" there seems unusual for Josephus. An argument based on the flow of the text in the document is that, given that the mention of Jesus appears in the \"Antiquities\" before that of the John the Baptist, a Christian interpolator may have inserted it to place Jesus in the text before John. A further argument against the authenticity of the James passage is that it would have read well even without a reference to Jesus.\n\nThe passage deals with the death of \"James the brother of Jesus\" in Jerusalem. Whereas the works of Josephus refer to at least twenty different people with the name Jesus, this passage specifies that this Jesus was the one \"who was called Christ\". Louis Feldman states that this passage, above others, indicates that Josephus did say something about Jesus.\n\nModern scholarship has almost universally acknowledged the authenticity of the reference in of the \"Antiquities\" to \"the brother of Jesus, who was called Christ, whose name was James\", and considers it as having the highest level of authenticity among the references of Josephus to Christianity.\n\nThe \"Testimonium Flavianum\" (meaning the testimony of Flavius [Josephus]) is the name given to the passage found in of the \"Antiquities\" in which Josephus describes the condemnation and crucifixion of Jesus at the hands of the Roman authorities. Scholars have differing opinions on the total or partial authenticity of the reference in the passage to the execution of Jesus by Pontius Pilate. The general scholarly view is that while the \"Testimonium Flavianum\" is most likely not authentic in its entirety, it is broadly agreed upon that it originally consisted of an authentic nucleus with a reference to the execution of Jesus by Pilate which was then subject to Christian interpolation. Although the exact nature and extent of the Christian redaction remains unclear, there is broad consensus as to what the original text of the \"Testimonium\" by Josephus would have looked like.\n\nThe references found in \"Antiquities\" have no parallel texts in the other work by Josephus such as the \"Jewish War\", written twenty years earlier, but some scholars have provided explanations for their absence, such as that the \"Antiquities\" covers a longer time period and that during the twenty-year gap between the writing of the \"Jewish Wars\" (c. 70 CE) and \"Antiquities\" (after 90 CE) Christians had become more important in Rome and were hence given attention in the \"Antiquities\".\n\nA number of variations exist between the statements by Josephus regarding the deaths of James and the New Testament accounts. Scholars generally view these variations as indications that the Josephus passages are not interpolations, because a Christian interpolator would more likely have made them correspond to the Christian traditions. Robert Eisenman provides numerous early Christian sources that confirm the Josephus testament, that James was the brother of Jesus.\n\nThe Roman historian and senator Tacitus referred to Christ, his execution by Pontius Pilate and the existence of early Christians in Rome in his final work, \"Annals\" (c. AD 116), . The relevant passage reads: \"called Christians by the populace. Christus, from whom the name had its origin, suffered the extreme penalty during the reign of Tiberius at the hands of one of our procurators, Pontius Pilatus.\"\n\nScholars generally consider Tacitus's reference to the execution of Jesus by Pontius Pilate to be both authentic, and of historical value as an independent Roman source about early Christianity that is in unison with other historical records. William L. Portier has stated that the consistency in the references by Tacitus, Josephus and the letters to Emperor Trajan by Pliny the Younger reaffirm the validity of all three accounts.\n\nTacitus was a patriotic Roman senator and his writings shows no sympathy towards Christians. Andreas Köstenberger and separately Robert E. Van Voorst state that the tone of the passage towards Christians is far too negative to have been authored by a Christian scribe – a conclusion shared by John P. Meier Robert E. Van Voorst states that \"of all Roman writers, Tacitus gives us the most precise information about Christ\".\n\nJohn Dominic Crossan considers the passage important in establishing that Jesus existed and was crucified, and states: \"That he was crucified is as sure as anything historical can ever be, since both Josephus and Tacitus... agree with the Christian accounts on at least that basic fact.\" Bart D. Ehrman states: \"Tacitus's report confirms what we know from other sources, that Jesus was executed by order of the Roman governor of Judea, Pontius Pilate, sometime during Tiberius's reign.\" Eddy and Boyd state that it is now \"firmly established\" that Tacitus provides a non-Christian confirmation of the crucifixion of Jesus.\n\nAlthough the majority of scholars consider it to be genuine, a few scholars question the authenticity of the passage given that Tacitus was born 25 years after Jesus' death.\n\nSome scholars have debated the historical value of the passage given that Tacitus does not reveal the source of his information. Gerd Theissen and Annette Merz argue that Tacitus at times had drawn on earlier historical works now lost to us, and he may have used official sources from a Roman archive in this case; however, if Tacitus had been copying from an official source, some scholars would expect him to have labeled Pilate correctly as a \"prefect\" rather than a \"procurator\". Theissen and Merz state that Tacitus gives us a description of widespread prejudices about Christianity and a few precise details about \"Christus\" and Christianity, the source of which remains unclear. However, Paul R. Eddy has stated that given his position as a senator Tacitus was also likely to have had access to official Roman documents of the time and did not need other sources.\n\nMichael Martin notes that the authenticity of this passage of the Annals has also been disputed on the grounds that Tacitus would not have used the word “messiah” in an authentic Roman document.\n\nWeaver notes that Tacitus spoke of the persecution of Christians, but no other Christian author wrote of this persecution for a hundred years.\n\nHotema notes that this passage was not quoted by any Church father up to the 15th century, although the passage would have been very useful to them in their work; and that the passage refers to the Christians in Rome being a multitude, while at that time the Christian congregation in Rome would actually have been very small.\n\nRichard Carrier has put forward the ideas that the 'Christ, the author of this name, was executed by the procurator Pontius Pilate in the reign of Tiberius' line is a Christian interpolation and that Tacitus wrote about Chrestians not Christians.\n\nScholars have also debated the issue of hearsay in the reference by Tacitus. Charles Guignebert argued that \"So long as there is that possibility [that Tacitus is merely echoing what Christians themselves were saying], the passage remains quite worthless\". R. T. France states that the Tacitus passage is at best just Tacitus repeating what he had heard through Christians. However, Paul R. Eddy has stated that as Rome's preeminent historian, Tacitus was generally known for checking his sources and was not in the habit of reporting gossip. Tacitus was a member of the Quindecimviri sacris faciundis, a council of priests whose duty it was to supervise foreign religious cults in Rome, which as Van Voorst points out, makes it reasonable to suppose that he would have acquired knowledge of Christian origins through his work with that body.\n\nMara (son of Sarapion) was a Stoic philosopher from the Roman province of Syria. Sometime between 73 AD and the 3rd century, Mara wrote a letter to his son (also called Sarapion) which may contain an early non-Christian reference to the crucifixion of Jesus.\n\nThe letter refers to the unjust treatment of \"three wise men\": the murder of Socrates, the burning of Pythagoras, and the execution of \"the wise king\" of the Jews. The author explains that in all three cases the wrongdoing resulted in the future punishment of those responsible by God and that when the wise are oppressed, not only does their wisdom triumph in the end, but God punishes their oppressors.\n\nThe letter includes no Christian themes and the author is presumed to be a pagan. Some scholars see the reference to the execution of the \"wise king\" of the Jews as an early non-Christian reference to Jesus. Criteria that support the non-Christian origin of the letter include the observation that \"king of the Jews\" was not a Christian title, and that the letter's premise that Jesus lives on based on the wisdom of his teachings is in contrast to the Christian concept that Jesus continues to live through his resurrection.\n\nScholars such as Robert Van Voorst see little doubt that the reference to the execution of the \"king of the Jews\" is about the death of Jesus. Others such as Craig A. Evans see less value in the letter, given its uncertain date, and the possible ambiguity in the reference.\n\nThe Roman historian Suetonius (c. 69 – after 122 CE) made references to early Christians and their leader in his work \"Lives of the Twelve Caesars\" (written 121 CE). The references appear in and which describe the lives of Roman Emperors Claudius and Nero. The Nero 16 passage refers to the abuses by Nero and mentions how he inflicted punishment on Christians – which is generally dated to around AD 64. This passage shows the clear contempt of Suetonius for Christians - the same contempt expressed by Tacitus and Pliny the younger in their writings, but does not refer to Jesus himself.\n\nThe earlier passage in Claudius, may include a reference to Jesus, but is subject to debate among scholars. In Suetonius refers to the expulsion of Jews by Claudius and states:\n\nThe reference in Claudius 25 involves the agitations in the Jewish community which led to the expulsion of some Jews from Rome by Claudius, and is likely the same event mentioned in the Acts of the Apostles (). Most historians date this expulsion to around AD 49–50. Suetonius refers to the leader of the Christians as \"Chrestus\", a term also used by used by Tacitus, referred in Latin dictionaries as a (amongst other things) version of 'Christus'. However, the wording used by Suetonius implies that Chrestus was alive at the time of the disturbance and was agitating the Jews in Rome. This weakens the historical value of his reference as a whole, and there is no overall scholarly agreement about its value as a reference to Jesus. However, the confusion of Suetonius also points to the lack of Christian interpolation, for a Christian scribe would not have confused the Jews with Christians.\n\nMost scholars assume that in the reference Jesus is meant and that the disturbances mentioned were due to the spread of Christianity in Rome. However, scholars are divided on the value of the Suetonius' reference. Some scholars such as Craig A. Evans, John Meier and Craig S. Keener see it as a likely reference to Jesus. Others such as Stephen Benko and H. Dixon Slingerland see it as having little or no historical value.\n\nMenahem Stern states Suetonius definitely was referring to Jesus; because he would have added \"a certain\" to Chrestus if he had meant some unknown agitator.\n\nThe Babylonian Talmud in a few cases includes possible references to Jesus using the terms \"Yeshu\", \"Yeshu ha-Notzri\", \"ben Stada\", and \"ben Pandera\". Some of these references probably date back to the Tannaitic period (70–200 CE). In some cases, it is not clear if the references are to Jesus, or other people, and scholars continue to debate their historical value, and exactly which references, if any, may be to Jesus.\n\nRobert Van Voorst states that the scarcity of Jewish references to Jesus is not surprising, given that Jesus was not a prominent issue for the Jews during the first century, and after the devastation caused by the Siege of Jerusalem in the year 70, Jewish scholars were focusing on preserving Judaism itself, rather than paying much attention to Christianity.\n\nRobert Eisenman argues that the derivation of Jesus of Nazareth from \"ha-Notzri\" is impossible on etymological grounds, as it would suggest rather \"the Nazirite\" rather than \"the Nazarene\".\n\nVan Voorst states that although the question of who was referred to in various points in the Talmud remains subject to debate among scholars, in the case of \"Sanhedrin 43a\" (generally considered the most important reference to Jesus in rabbinic literature), Jesus can be confirmed as the subject of the passage, not only from the reference itself, but from the context that surrounds it, and there is little doubt that it refers to the death of Jesus of Nazareth. Christopher M. Tuckett states that if it is accepted that death narrative of Sanhedrin 43a refers to Jesus of Nazareth then it provides evidence of Jesus' existence and execution.\n\nAndreas Kostenberger states that the passage is a Tannaitic reference to the trial and death of Jesus at Passover and is most likely earlier than other references to Jesus in the Talmud. The passage reflects hostility toward Jesus among the rabbis and includes this text:\n\nIt is taught: On the eve of Passover they hung Yeshu and the crier went forth for forty days beforehand declaring that \"[Yeshu] is going to be stoned for practicing witchcraft, for enticing and leading Israel astray. Anyone who knows something to clear him should come forth and exonerate him.\" But no one had anything exonerating for him and they hung him on the eve of Passover. \n\nPeter Schäfer states that there can be no doubt that the narrative of the execution of Jesus in the Talmud refers to Jesus of Nazareth, but states that the rabbinic literature in question are not Tannaitic but from a later Amoraic period and may have drawn on the Christian gospels, and may have been written as responses to them. Bart Ehrman and separately Mark Allan Powell state that given that the Talmud references are quite late, they can give no historically reliable information about the teachings or actions of Jesus during his life.\n\nAnother reference in early second century Rabbinic literature (Tosefta Hullin II 22) refers to Rabbi Eleazar ben Dama who was bitten by a snake, but was denied healing in the name of Jesus by another Rabbi for it was against the law, and thus died. This passage reflects the attitude of Jesus' early Jewish opponents, i.e. that his miracles were based on evil powers.\n\nEddy and Boyd, who question the value of several of the Talmudic references state that the significance of the Talmud to historical Jesus research is that it never denies the existence of Jesus, but accuses him of sorcery, thus indirectly confirming his existence. R. T. France and separately Edgar V. McKnight state that the divergence of the Talmud statements from the Christian accounts and their negative nature indicate that they are about a person who existed. Craig Blomberg states that the denial of the existence of Jesus was never part of the Jewish tradition, which instead accused him of being a sorcerer and magician, as also reflected in other sources such as Celsus. Andreas Kostenberger states that the overall conclusion that can be drawn from the references in the Talmud is that Jesus was a historical person whose existence was never denied by the Jewish tradition, which instead focused on discrediting him.\n\nPliny the Younger (c. 61 – c. 112), the provincial governor of Pontus and Bithynia, wrote to Emperor Trajan \"c\". 112 concerning how to deal with Christians, who refused to worship the emperor, and instead worshiped \"Christus\". Charles Guignebert, who does not doubt that Jesus of the Gospels lived in Gallilee in the 1st century, nevertheless dismisses this letter as acceptable evidence for a historical Jesus.\n\nThallus, of whom very little is known, and none of whose writings survive, wrote a history allegedly around the middle to late first century CE, to which Eusebius referred. Julius Africanus, writing \"c\" 221, links a reference in the third book of the \"History\" to the period of darkness described in the crucifixion accounts in three of the Gospels . It is not known whether Thallus made any mention to the crucifixion accounts; if he did, it would be the earliest noncanonical reference to a gospel episode, but its usefulness in determining the historicity of Jesus is uncertain. The dating of Thallus is dependent on him writing about an event during the 207th Olympiad (49–52 AD), which means he wrote after that date, not near that date. This depends on the text being corrupt, which would mean Thallus could have been writing after the 217th Olympiad (89–92 AD), or even the 167th Olympiad (112–109 BC). He is first referenced by Theophilus, writing around 180 AD, which means Thallus could have written any time between 109 BC and 180 AD. All we know is Thallus mentioned a solar eclipse, and as solar eclipses are not possible at Passover, that would mean Thallus was not talking about the crucifixion of Jesus at all.\n\nPhlegon of Tralles, AD 80–140, similar to Thallus, Julius Africanus mentions a historian named Phlegon who wrote a chronicle of history around AD 140, where he records:\n“Phlegon records that, in the time of Tiberius Caesar, at full moon, there was a full eclipse of the sun from the sixth to the ninth hour.” (Africanus, Chronography, 18:1) Phlegon is also mentioned by Origen (an early church theologian and scholar, born in Alexandria):\n“Now Phlegon, in the thirteenth or fourteenth book, I think, of his Chronicles, not only ascribed to Jesus a knowledge of future events . . . but also testified that the result corresponded to His predictions.” (Origen Against Celsus, Book 2, Chapter 14)\n“And with regard to the eclipse in the time of Tiberius Caesar, in whose reign Jesus appears to have been crucified, and the great earthquakes which then took place … ” (Origen Against Celsus, Book 2, Chapter 33)\n“Jesus, while alive, was of no assistance to himself, but that he arose after death, and exhibited the marks of his punishment, and showed how his hands had been pierced by nails.” (Origen Against Celsus, Book 2, Chapter 59). However, Eusebius in The Chronicon (written in the 4th century AD) records what Phlegon said verbatim. \"Now, in the fourth year of the 202nd Olympiad [32 AD], a great eclipse of the sun occurred at the sixth hour [noon] that excelled every other before it, turning the day into such darkness of night that the stars could be seen in heaven, and the earth moved in Bithynia, toppling many buildings in the city of Nicaea.\" Phlegon never mentions Jesus or the 3 hour darkness. He also mentions a solar eclipse, which can not occur at Passover. Apart from the year (which may be a corruption), this description fits an earthquake and eclipse that occurred in North West Turkey on November, 29 AD.\n\nCelsus writing late in the second century produced the first full-scale attack on Christianity. Celsus' document has not survived but in the third century Origen replied to it, and what is known of Celsus' writing is through the responses of Origen. According to Origen, Celsus accused Jesus of being a magician and a sorcerer. While the statements of Celsus may be seen as valuable, they have little historical value, given that the wording of the original writings can not be examined.\n\nThe Dead Sea Scrolls are first century or older writings that show the language and customs of some Jews of Jesus' time. Scholars such as Henry Chadwick see the similar uses of languages and viewpoints recorded in the New Testament and the Dead Sea Scrolls as valuable in showing that the New Testament portrays the first century period that it reports and is not a product of a later period. However, the relationship between the Dead Sea scrolls and the historicity of Jesus has been the subject of highly controversial theories, and although new theories continue to appear, there is no overall scholarly agreement about their impact on the historicity of Jesus, despite the usefulness of the scrolls in shedding light on first-century Jewish traditions.\n\nThe following sources are disputed, and of limited historical value, but they are at least proof of Christians existing and being known and talked about in the first and second centuries.\n\nThere is a limestone burial box from the 1st century known as the James Ossuary with the Aramaic inscription, \"James, son of Joseph, brother of Jesus.\" The authenticity of the inscription was challenged by the Israel Antiquities Authority, who filed a complaint with the Israeli police. In 2012, the owner of the ossuary was found not guilty, with the judge ruling that the authenticity of the ossuary inscription had not been proven either way. It has been suggested it was a forgery.\n\nVarious books, memoirs and stories were written about Jesus by the early Christians. The most famous are the gospels of Matthew, Mark, Luke and John. All but one of these are believed to have been written within 50–70 years of the death of Jesus, with the Gospel of Mark believed to be the earliest, and the last the Gospel of John. Blainey writes that the oldest surviving record written by an early Christian is a short letter by St Paul: the First Epistle to the Thessalonians, which appeared about 25 years after the death of Jesus. This letter, while important in describing issues for the development of Gentilic Christianity, contains little of significance for understanding the life of the historic Jesus.\n\nBart Ehrman, Robert Eisenman and others critical of traditional Christian views, in assessing the problems involved in conducting historical Jesus research, say the Gospels are full of discrepancies, were written decades after Jesus' death, by authors who had not witnessed any events in Jesus' life. They go on to say the Gospels were authored not by eyewitnesses who were contemporary with the events that they narrate but rather by people who did not know Jesus, see anything he did, or hear anything he taught, and that the authors did not even share a language with Jesus. The accounts they produced are not disinterested; they are narratives produced by Christians who actually believed in Jesus, and were not immune from slanting the stories in light of their biases. Ehrman points out that the texts are widely inconsistent, full of discrepancies and contradictions in both details and larger portraits of who Jesus was.\n\nIn the context of Christian sources, even if all other texts are ignored, the Pauline epistles can provide some information regarding Jesus. This information does not include a narrative of the life of Jesus, and refers to his existence as a person, but adds few specific items apart from his death by crucifixion. This information comes from those letters of Paul whose authenticity is not disputed. Paul was not a companion of Jesus and claims his information comes from the holy spirit acquired after Jesus' death.\n\nOf the thirteen letters that bear Paul's name, seven are considered authentic by almost all scholars, and the others are generally considered pseudepigraphic. The 7 undisputed letters (and their approximate dates) are: 1 Thessalonians (c. 51 CE), Philippians (c. 52–54 CE), Philemon (c. 52–54 CE), 1 Corinthians (c. 53–54 CE), Galatians (c. 55 CE), 2 Corinthians (c. 55–56 CE) and Romans (c. 55–58 CE). The authenticity of these letters is accepted by almost all scholars, and they have been referenced and interpreted by early authors such as Origen and Eusebius.\n\nGiven that the Pauline epistles are generally dated to AD 50 to AD 60, they are the earliest surviving Christian texts that include information about Jesus. These letters were written approximately twenty to thirty years after the generally accepted time period for the death of Jesus, around AD 30–36. The letters were written during a time when Paul recorded encounters with the disciples of Jesus, e.g. states that several years after his conversion Paul went to Jerusalem and stayed with Apostle Peter for fifteen days. During this time, Paul disputed the nature of Jesus' message with Jesus's brother James, concerning the importance of adhering to kosher food restrictions and circumcision, important features of determining Jewish identity.\n\nThe Pauline letters were not intended to provide a narrative of the life of Jesus, but were written as expositions of Christian teachings. In Paul's view, the earthly life of Jesus was of a lower importance than the theology of his death and resurrection,a theme that permeates Pauline writings. However, the Pauline letters clearly indicate that for Paul Jesus was a real person (born of a woman as in Gal 4.4), a Jew (\"born under the law\", Romans 1.3) who had disciples (1 Corinthians 15.5), who was crucified (as in 1 Corinthians 2.2 and Galatians 3.1) and who resurrected from the dead (1 Corinthians 15.20, Romans 1.4 and 6.5, Philippians 3:10–11). And the letters reflect the general concept within the early Gentillic Christian Church that Jesus existed, was crucified and was raised from the dead.\n\nThe references by Paul to Jesus do not in themselves prove the existence of Jesus, but they do establish that the existence of Jesus was the accepted norm within the early Christians (including the Christian community in Jerusalem, given the references to collections there) twenty to thirty years after the death of Jesus, at a time when those who could have been acquainted with him could still be alive.\n\nThe seven Pauline epistles that are widely regarded as authentic include the following information that along with other historical elements are used to study the historicity of Jesus:\n\nThe existence of only these references to Jesus in the Pauline epistles has given rise to criticism of them by G. A. Wells, who is generally accepted as a leader of the movement to deny the historicity of Jesus. When Wells was still denying the existence of Jesus, he criticized the Pauline epistles for not mentioning items such as John the Baptist or Judas or the trial of Jesus and used that argument to conclude that Jesus was not a historical figure.\n\nJames D. G. Dunn addressed Wells' statement and stated that he knew of no other scholar that shared that view, and most other scholars had other and more plausible explanations for the fact that Paul did not include a narrative of the life of Jesus in his letters, which were primarily written as religious documents rather than historical chronicles at a time when the life story of Jesus could have been well known within the early Church. Dunn states that despite Wells' arguments, the theories of the non-existence of Jesus are a \"thoroughly dead thesis\".\n\nWhile Wells no longer denies the existence of Jesus, he has responded to Dunn, stating that his arguments from silence not only apply to Paul but all early Christian authors, and that he still has a low opinion of early Christian texts, maintaining that for Paul Jesus may have existed a good number of decades before.\n\nThe Pauline letters sometimes refer to creeds, or confessions of faith, that predate their writings. For instance reads: \"For what I received I passed on to you as of first importance: that Christ died for our sins according to the Scriptures, that he was buried, that he was raised on the third day according to the Scriptures.\" refers to Romans 1:2 just before it which mentions an existing gospel, and in effect may be treating it as an earlier creed.\n\nOne of the keys to identifying a pre-Pauline tradition is given in \n\nHere Paul refers to others before him who preached the creed. James Dunn states that indicates that in the 30s Paul was taught about the death of Jesus a few years earlier.\n\nThe Pauline letters thus contain Christian creed elements of pre-Pauline origin. The antiquity of the creed has been located by many Biblical scholars to less than a decade after Jesus' death, originating from the Jerusalem apostolic community. Concerning this creed, Campenhausen wrote, \"This account meets all the demands of historical reliability that could possibly be made of such a text,\" whilst A. M. Hunter said, \"The passage therefore preserves uniquely early and verifiable testimony. It meets every reasonable demand of historical reliability.\"\n\nThese creeds date to within a few years of Jesus' death, and developed within the Christian community in Jerusalem. Although embedded within the texts of the New Testament, these creeds are a distinct source for Early Christianity. This indicates that existence and death of Jesus was part of Christian belief a few years after his death and over a decade before the writing of the Pauline epistles.\n\nThe four canonical gospels, Matthew, Mark, Luke, and John, are the main sources for the biography of Jesus' life, the teachings and actions attributed to him. Three of these (Matthew, Mark, and Luke) are known as the synoptic Gospels, from the Greek σύν (syn \"together\") and ὄψις (opsis \"view\"), given that they display a high degree of similarity in content, narrative arrangement, language and paragraph structure. The presentation in the fourth canonical gospel, i.e. John, differs from these three in that it has more of a thematic nature rather than a narrative format. Scholars generally agree that it is impossible to find any direct literary relationship between the synoptic gospels and the Gospel of John.\n\nThe authors of the New Testament generally showed little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life with the secular history of the age. The gospels were primarily written as theological documents in the context of early Christianity with the chronological timelines as a secondary consideration. One manifestation of the gospels being theological documents rather than historical chronicles is that they devote about one third of their text to just seven days, namely the last week of the life of Jesus in Jerusalem. Although the gospels do not provide enough details to satisfy the demands of modern historians regarding exact dates, scholars have used them to reconstruct a number of portraits of Jesus. However, as stated in the gospels do not claim to provide an exhaustive list of the events in the life of Jesus.\n\nScholars have varying degrees of certainty about the historical reliability of the accounts in the gospels, and the only two events whose historicity is the subject of almost universal agreement among scholars are the baptism and crucifixion of Jesus. Scholars such as E.P. Sanders and separately Craig A. Evans go further and assume that two other events in the gospels are historically certain, namely that Jesus called disciples, and caused a controversy at the Temple.\n\nEver since the Augustinian hypothesis, scholars continue to debate the order in which the gospels were written, and how they may have influenced each other, and several hypothesis exist in that regard, e.g. the Markan priority hypothesis holds that the Gospel of Mark was written first c. 70 CE. In this approach, Matthew is placed at being sometime after this date and Luke is thought to have been written between 70 and 100 CE. However, according to the competing, and more popular, Q source hypothesis, the gospels were not independently written, but were derived from a common source called Q. The two-source hypothesis then proposes that the authors of Matthew and Luke drew on the Gospel of Mark as well as on Q.\n\nThe gospels can be seen as having three separate lines: A literary line which looks at it from a textual perspective, secondly a historical line which observes how Christianity started as a renewal movement within Judaism and eventually separated from it, and finally a theological line which analyzes Christian teachings. Within the historical perspective, the gospels are not simply used to establish the existence of Jesus as sources in their own right alone, but their content is compared and contrasted to non-Christian sources, and the historical context, to draw conclusions about the historicity of Jesus.\n\nTwo possible patristic sources that may refer to eye witness encounters with Jesus are the early references of Papias and Quadratus, reported by Eusebius of Caesarea in the 4th century.\n\nThe works of Papias have not survived, but Eusebius quotes him as saying:\n\nRichard Bauckham states that while Papias was collecting his information (c. 90), Aristion and the elder John (who were Jesus' disciples) were still alive and teaching in Asia minor, and Papias gathered information from people who had known them. However, the exact identity of the \"elder John\" is wound up in the debate on the authorship of the Gospel of John, and scholars have differing opinions on that, e.g. Jack Finegan states that Eusebius may have misunderstood what Papias wrote, and the elder John may be a different person from the author of the fourth gospel, yet still a disciple of Jesus. Gary Burge, on the other hand sees confusion on the part of Eusebius and holds the elder John to be different person from the apostle John.\n\nThe letter of Quadratus (possibly the first Christian apologist) to emperor Hadrian (who reigned 117 – 138) is likely to have an early date and is reported by Eusebius in his \"Ecclesiastical History\" 4.3.2 to have stated:\n\nBy \"our Savior\" Quadratus means Jesus and the letter is most likely written before AD 124. Bauckham states that by \"our times\" he may refer to his early life, rather than when he wrote (117–124), which would be a reference contemporary with Papias. Bauckham states that the importance of the statement attributed to Quadratus is that he emphasizes the \"eye witness\" nature of the testimonies to interaction with Jesus. Such \"eye witness statements\" abound in early Christian writings, particularly the pseudonymous Christian Apocrypha, Gospels and Letters, in order to give them credibility.\n\nA number of later Christian texts, usually dating to the second century or later, exist as New Testament apocrypha, among which the gnostic gospels have been of major recent interest among scholars. The 1945 discovery of the Nag Hammadi library created a significant amount of scholarly interest and many modern scholars have since studied the gnostic gospels and written about them. However, the trend among the 21st century scholars has been to accept that while the gnostic gospels may shed light on the progression of early Christian beliefs, they offer very little to contribute to the study of the historicity of Jesus, in that they are rather late writings, usually consisting of sayings (rather than narrative, similar to the hypothesised Q documents), their authenticity and authorship remain questionable, and various parts of them rely on components of the New Testament. The focus of modern research into the historical Jesus has been away from gnostic writings and towards the comparison of Jewish, Greco-Roman and canonical Christian sources.\n\nAs an example, Bart Ehrman states that gnostic writings of the Gospel of Thomas (part of the Nag Hammadi library) have very little value in historical Jesus research, because the author of that gospel placed no importance on the physical experiences of Jesus (e.g. his crucifixion) or the physical existence of believers, and was only interested in the secret teachings of Jesus rather than any physical events. Similarly, the Apocryphon of John (also part of the Nag Hammadi library) has been useful in studying the prevailing attitudes in the second century, and questions of authorship regarding the Book of revelation, given that it refers to , but is mostly about the post ascension teachings of Jesus in a vision, not a narrative of his life. Some scholars such as Edward Arnal contend that the Gospel of Thomas continues to remain useful for understanding how the teachings of Jesus were transmitted among early Christians, and sheds light on the development of early Christianity.\n\nThere is overlap between the sayings of Jesus in the apocryphal texts and canonical Christian writings, and those not present in the canonical texts are called agrapha. There are at least 225 agrapha but most scholars who have studied them have drawn negative conclusions about the authenticity of most of them and see little value in using them for historical Jesus research. Robert Van Voorst states that the vast majority of the agrapha are certainly inauthentic. Scholars differ on the number of authentic agrapha, some estimating as low as seven as authentic, others as high as 18 among the more than 200, rendering them of little value altogether. While research on apocryphal texts continues, the general scholarly opinion holds that they have little to offer to the study of the historicity of Jesus given that they are often of uncertain origin, and almost always later documents of lower value.\n\n\n"}
{"id": "1707086", "url": "https://en.wikipedia.org/wiki?curid=1707086", "title": "Tag (metadata)", "text": "Tag (metadata)\n\nIn information systems, a tag is a keyword or term assigned to a piece of information (such as an Internet bookmark, digital image, database record, or computer file). This kind of metadata helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system, although they may also be chosen from a controlled vocabulary.\n\nTagging was popularized by websites associated with Web 2.0 and is an important feature of many Web 2.0 services. It is now also part of other database systems, desktop applications, and operating systems.\n\nPeople use tags to aid classification, mark ownership, note boundaries, and indicate online identity. Tags may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. People were using textual keywords to classify information and objects long before computers. Computer based search algorithms made the use of such keywords a rapid way of exploring records.\n\nTagging gained popularity due to the growth of social bookmarking, image sharing, and social networking websites. These sites allow users to create and manage labels (or \"tags\") that categorize content using simple keywords. Websites that include tags often display collections of tags as tag clouds, as do some desktop applications. On websites that aggregate the tags of all users, an individual user's tags can be useful both to them and to the larger community of the website's users.\n\nTagging systems have sometimes been classified into two kinds: \"top-down\" and \"bottom-up\". Top-down taxonomies are created by an authorized group of designers (sometimes in the form of a controlled vocabulary), whereas bottom-up taxonomies (called folksonomies) are created by all users. This definition of \"top down\" and \"bottom up\" should not be confused with the distinction between a \"single hierarchical\" tree structure (in which there is one correct way to classify each item) versus \"multiple non-hierarchical\" sets (in which there are multiple ways to classify an item); the structure of both top-down and bottom-up taxonomies may be either hierarchical, non-hierarchical, or a combination of both. Some researchers and applications have experimented with combining hierarchical and non-hierarchical tagging to aid in information retrieval. Others are combining top-down and bottom-up tagging, including in some large library catalogs (OPACs) such as WorldCat.\n\nWhen tags or other taxonomies have further properties (or semantics) such as relationships and attributes, they constitute an ontology.\n\nMetadata tags as described in this article should not be confused with the use of the word \"tag\" in some software to refer to an automatically generated cross-reference; examples of the latter are \"tags tables\" in Emacs and \"smart tags\" in Microsoft Office.\n\nThe use of keywords as part of an identification and classification system long predates computers. Paper data storage devices, notably edge-notched cards, that permitted classification and sorting by multiple criteria were already in use prior to the twentieth century, and faceted classification has been used by libraries since the 1930s.\n\nIn the late 1970s and early 1980s, the Unix text editor Emacs offered a companion software program called \"Tags\" that could automatically build a table of cross-references called a \"tags table\" that Emacs could use to jump between a function call and that function's definition. This use of the word \"tag\" did not refer to metadata tags, but was an early use of the word \"tag\" in software to refer to a word index.\n\nOnline databases and early websites deployed keyword tags as a way for publishers to help users find content. In the early days of the World Wide Web, the codice_1 meta element was used by web designers to tell web search engines what the web page was about, but these keywords were only visible in a web page's source code and were not modifiable by users.\n\nIn 2003, the social bookmarking website Delicious provided a way for its users to add \"tags\" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag. Within a couple of years, the photo sharing website Flickr allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable. The success of Flickr and the influence of Delicious popularized the concept, and other social software websites—such as YouTube, Technorati, and Last.fm—also implemented tagging. In 2005, the Atom web syndication standard provided a \"category\" element for inserting subject categories into web feeds, and in 2007 Tim Bray proposed a \"tag\" URN.\n\nMany blog systems (and other web content management systems) allow authors to add free-form tags to a post, along with (or instead of) placing the post into a predetermined category. For example, a post may display that it has been tagged with codice_2 and codice_3. Each of those tags is usually a web link leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.\n\nSome desktop applications and web applications feature their own tagging systems, such as email tagging in Gmail and Mozilla Thunderbird, bookmark tagging in Firefox, audio tagging in iTunes or Winamp, and photo tagging in various applications. Some of these applications display collections of tags as tag clouds.\n\nThere are various systems for applying tags to the files in a computer's file system. In Apple's macOS, the operating system has allowed users to assign multiple arbitrary tags as extended file attributes to any file or folder ever since OS X 10.9 was released in 2013, and before that time the open-source OpenMeta standard provided similar tagging functionality in macOS. Several semantic file systems that implement tags are available for the Linux kernel, including Tagsistant. Microsoft Windows allows users to set tags only on Microsoft Office documents and some kinds of picture files.\n\nCross-platform file tagging standards include Extensible Metadata Platform (XMP), an ISO standard for embedding metadata into popular image, video and document file formats, such as JPEG and PDF, without breaking their readability by applications that do not support XMP. XMP largely supersedes the earlier IPTC Information Interchange Model. Exif is a standard that specifies the image and audio file formats used by digital cameras, including some metadata tags. TagSpaces is an open-source cross-platform application for tagging files; it inserts tags into the filename.\n\nAn \"official tag\" is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a controlled vocabulary.\n\nA researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, freeform classification allows the author to manage what would otherwise be unwieldy amounts of information.\n\nA triple tag or machine tag uses a special syntax to define extra semantic information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a namespace, a predicate, and a value. For example, codice_4 is a tag for the geographical longitude coordinate whose value is 50.123456. This triple structure is similar to the Resource Description Framework model for information.\n\nThe triple tag format was first devised for geolicious in November 2004, to map Delicious bookmarks, and gained wider acceptance after its adoption by Mappr and GeoBloggers to map Flickr photos. In January 2007, Aaron Straup Cope at Flickr introduced the term \"machine tag\" as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.\n\nSpecialized metadata for geographical identification is known as \"geotagging\"; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using binomial nomenclature.\n\nA hashtag is a kind of metadata tag marked by the prefix codice_5, sometimes known as a \"hash\" symbol. This form of tagging is used on microblogging and social networking services such as Twitter, Facebook, Google+, VK and Instagram.\n\nA knowledge tag is a type of meta-information that describes or defines some aspect of a piece of information (such as a document, digital image, database table, or web page). Knowledge tags are more than traditional non-hierarchical keywords or terms; they are a type of metadata that captures knowledge in the form of descriptions, categorizations, classifications, semantics, comments, notes, annotations, hyperdata, hyperlinks, or references that are collected in tag profiles (a kind of ontology). These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository.\n\nKnowledge tags are part of a knowledge management discipline that leverages Enterprise 2.0 methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. Different kinds of knowledge can be captured in knowledge tags, including factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies). These forms of knowledge often exist outside the data itself and are derived from personal experience, insight, or expertise. Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information. Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turnover, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.\n\nIn a typical tagging system, there is no explicit information about the meaning or semantics of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them; in contrast, the flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.\n\nWhen users can freely choose tags (creating a folksonomy, as opposed to selecting terms from a controlled vocabulary), the resulting metadata can include homonyms (the same tags used with different meanings) and synonyms (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject. For example, the tag \"orange\" may refer to the fruit or the color, and items related to a version of the Linux kernel may be tagged \"Linux\", \"kernel\", \"Penguin\", \"software\", or a variety of other terms. Users can also choose tags that are different inflections of words (such as singular and plural), which can contribute to navigation difficulties if the system does not include stemming of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of \"tag terms\" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies may collectively develop a partial set of tagging conventions.\n\nDespite the apparent lack of control, research has shown that a simple form of shared vocabulary emerges in social bookmarking systems. Collaborative tagging exhibits a form of complex systems dynamics (or self-organizing dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags converges over time to stable power law distributions. Once such stable distributions form, simple folksonomic vocabularies can be extracted by examining the correlations that form between different tags. In addition, research has suggested that it is easier for machine learning algorithms to learn tag semantics when users tag \"verbosely\"—when they annotate resources with a wealth of freely associated, descriptive keywords.\n\nTagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a YouTube video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items. The number of tags allowed may also be limited to reduce spam.\n\nSome tagging systems provide a single text box to enter tags, so to be able to tokenize the string, a must be used. Two popular separators are the space character and the comma. To enable the use of separators in the tags, a system may allow for higher-level separators (such as quotation marks) or escape characters. Systems can avoid the use of separators by allowing only one tag to be added to each input widget at a time, although this makes adding multiple tags more time-consuming.\n\nA syntax for use within HTML is to use the rel-tag microformat which uses the \"rel\" attribute with value \"tag\" (i.e., codice_6) to indicate that the linked-to page acts as a tag for the current context.\n"}
{"id": "54226143", "url": "https://en.wikipedia.org/wiki?curid=54226143", "title": "The Crime Book", "text": "The Crime Book\n\nThe Crime Book (Big Ideas Simply Explained) is a non-fiction volume co-authored by American crime writers Cathy Scott, Shanna Hogan, Rebecca Morris, Canadian author and historian Lee Mellor, and United Kingdom author Michael Kerrigan, with a foreword for the U.S. edition by Scott and the U.K. edition by crime-fiction author Peter James. It was released by DK Books under its Big Ideas Learning imprint in May 2017.\n\nThe publisher describes \"The Crime Book\" as a guide to criminology that explores the most infamous cases of all time, from serial killers to mob hits to war crimes and more.\n\nIt includes a variety of crimes committed by more than 100 of the world's most notorious criminals. From Jack the Ripper to Jeffrey Dahmer, the book is a study of international true-crime history that covers shocking stories through infographics and research that lays out key facts and details. It examines the science, psychology and sociology of criminal behavior. It profiles of villains, victims and detectives. Each clue is listed for readers to follow investigations from start to finish, and studies the police and detective work for each case.\n\nIn a Q&A article for CrimeCon's blog with Scott, the author described the crimes detailed in the book as having \"such diversity that there is something for everyone. ... I can’t think of one crime that’s not represented in The Crime Book. It runs the gamut—from nonviolent cons to gangland-style criminals, to white-collar offenders—with a complete representation starting with the first known homicide committed against a Neanderthal man. Simply put, you can’t make this stuff up.\"\n\n\"Rolling Stone\" magazine's description, in an August 2017 interview with co-author Scott about the book, wrote that it is \"an encyclopedic treatment of the topic (that) makes for excellent companion reading. A compelling compilation of human trickery and awfulness, it covers crimes from arson, art forgery and kidnapping to bank robbery, drug trafficking and, of course, murder, with many of the entries accompanied by helpful illustrations.\"\n\n\"Reader's Digest\" listed it as one of its \"Best New Books You Should Read This April,\" describing it as \"everything you ever wanted to know about some of the most audacious, hideous, hilarious and mysterious acts of crime in one explosive book, filled with graphs, illustrations, quotes and timelines. This highly addictive encyclopedia of crime ... is a trivia goldmine and a helpful guide allowing you to put events into context.\"\n\n\"Culture Magazine\" in Germany had this to say: \"The level of expertise is quite high,\" noting that the book \"is lushly illustrated, readable and entertaining.\"\n\nIn its review, \"Crime Fiction Lover\" wrote that \"a crack team of true-crime experts helped put it together.\"\n\n\"Crimespree Magazine\" wrote, \"The crimes covered are all over from serial killers to gangsters and outlaws to kidnappers and elderly Brit bank robbers. This is a great book.\"\n\n"}
{"id": "5928902", "url": "https://en.wikipedia.org/wiki?curid=5928902", "title": "The Map Library", "text": "The Map Library\n\nThe Map Library is a project of The Map Maker Trust charity, and supported by Map Maker Ltd., for the supplying of free GIS data. The project website also hosts free conversion software for raster and vector files. As of November 2008, the only data sets available were for the continents of Africa and Central America.\n\nFrom the website...\n\nThe project data is managed in part by two pieces of software, each supporting different file formats and conversions.\n\n\nThe project uses data from NASA mapping projects, Famine Early Warning Systems Network, and the National Geospatial-Intelligence Agency.\n\n"}
{"id": "270906", "url": "https://en.wikipedia.org/wiki?curid=270906", "title": "Three-letter acronym", "text": "Three-letter acronym\n\nA three-letter acronym (TLA), or three-letter abbreviation, is an abbreviation, specifically an acronym, alphabetism, or initialism, consisting of three letters. These are usually the initial letters of the words of the phrase abbreviated, and are written in capital letters (upper case); three-letter abbreviations such as \"etc.\" and \"Mrs.\" are not three-letter acronyms, but \"TLA\" is a TLA (an example of an autological abbreviation).\n\nMost three-letter abbreviations are \"initialisms\": all the letters are pronounced as the names of letters, as in \"APA\" . Some are acronyms pronounced as a word; computed axial tomography, CAT, is almost always pronounced as the animal's name in \"CAT scan\".\n\n\nThe exact phrase \"three-letter acronym\" appeared in the sociology literature in 1975. Three-letter acronyms were used as mnemonics in biological sciences, from 1977 and their practical advantage was promoted by Weber in 1982. They are used in many other fields, but the term TLA is particularly associated with computing. In 1980, the manual for the Sinclair ZX81 home computer used and explained TLA. The specific generation of three-letter acronyms in computing was mentioned in a JPL report of 1982. In 1988, in a paper titled \"On the cruelty of really teaching computer science\", eminent computer scientist Edsger W. Dijkstra wrote \n\"Because no endeavour is respectable these days without a TLA ...\" By 1992 it was in a Microsoft handbook.\n\nThe number of possible three-letter abbreviations (or permutations) using the 26 letters of the alphabet from A to Z (AAA, AAB ... to ZZY, ZZZ) is 26 × 26 × 26 = 17,576. Another 26 × 26 × 10 = 6760 can be produced if the third element is allowed to be a digit 0-9, giving a total of 24,336.\n\nIn English, WWW is the longest possible TLA to pronounce, typically requiring nine syllables. The usefulness of TLAs typically comes from how it is quicker to say the acronym instead of than the phrase they represent, however saying 'WWW' in English requires three times as many syllables than the phrase it is meant to abbreviate (World Wide Web). Consequently, \"www\" is sometimes abbreviated as \"dubdubdub\" in speech.\n\n\n"}
{"id": "9032406", "url": "https://en.wikipedia.org/wiki?curid=9032406", "title": "Tupper's self-referential formula", "text": "Tupper's self-referential formula\n\nTupper's self-referential formula is a formula that visually represents itself when graphed at a specific location in the (\"x\", \"y\") plane.\n\nThe formula was defined by Jeff Tupper and appears as an example in Tupper's 2001 SIGGRAPH paper on reliable two-dimensional computer graphing algorithms.\n\nAlthough the formula is called \"self-referential\", Tupper did not name it as such.\n\nThe formula is an inequality defined as:\n\nor, as plaintext,\nwhere ⌊ ⌋ denotes the floor function, and mod is the modulo operation.\n\nLet \"k\" equal the following 543-digit integer:\n\nIf one graphs the set of points (\"x\", \"y\") in 0 ≤ \"x\" < 106 and \"k\" ≤ \"y\" < \"k\" + 17 satisfying the inequality given above, the resulting graph looks like this (the axes in this plot have been reversed, otherwise the picture would be upside-down and mirrored):\n\nThe formula is a general-purpose method of decoding a bitmap stored in the constant \"k\", and it could actually be used to draw any other image. When applied to the unbounded positive range 0 ≤ \"y\", the formula tiles a vertical swath of the plane with a pattern that contains all possible 17-pixel-tall bitmaps. One horizontal slice of that infinite bitmap depicts the drawing formula itself, but this is not remarkable, since other slices depict all other possible formulae that might fit in a 17-pixel-tall bitmap. Tupper has created extended versions of his original formula that rule out all but one slice.\n\nThe constant \"k\" is a simple monochrome bitmap image of the formula treated as a binary number and multiplied by 17. If \"k\" is divided by 17, the least significant bit encodes the upper-right corner (\"k\", 0); the 17 least significant bits encode the rightmost column of pixels; the next 17 least significant bits encode the 2nd-rightmost column, and so on.\n\n\n"}
