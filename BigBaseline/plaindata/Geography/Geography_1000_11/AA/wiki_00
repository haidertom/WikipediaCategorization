{"id": "987107", "url": "https://en.wikipedia.org/wiki?curid=987107", "title": "Buenaventura River (legend)", "text": "Buenaventura River (legend)\n\nThe non-existent Buenaventura River, alternatively San Buenaventura River, Río Buenaventura, etc. was once believed to run from the Rocky Mountains to the Pacific Ocean through the Great Basin region of what is now the western United States. The river was chronologically the last of several imagined incarnations of an imagined Great River of the West which would be for North America west of the Rockies what the Mississippi River was east of the Rockies. The hopes were to find a waterway from coast to coast, sparing the traveling around Cape Horn at the tip of South America.\n\nIn 1776, two Franciscan missionaries Atanasio Domínguez and Silvestre Vélez de Escalante sought to find a land route between Santa Fe in Nuevo México to Monterey in Alta California. They were part of what has become known as the Dominguez–Escalante Expedition, a ten-man expedition including Bernardo de Miera y Pacheco (Meira) acting as the cartographer. On September 13, they encountered what is now called the Green River, a southward-flowing tributary of the Colorado and named it \"San Buenaventura\" after the catholic saint Bonaventure.\n\nAt that point in time, there was nothing mythical about the Buenaventura River. Dominguez and Vélez de Escalante' journal correctly notes that above their crossing, the river flowed toward the west. It flowed generally southwest where they crossed it and continued southwest as they traveled in its vicinity. Escalante also correctly recorded that after its junction with the Rio San Clemente (today's White River? – which he also named) the Buenaventura River turned to the south. So, the original Buenaventura River is real and exists today under a different name.\n\nAfter establishing contact with a branch of the Ute (Yutahs) Tribe on the south shore of what they called Lake Timpanogos (various spellings), now called \"Utah Lake,\" the expedition turned south-southwest. On September 29, they were surprised to come upon a river (the Sevier River) flowing from the south-southeast and turning toward the west at the point they encountered it. Dominguez and Vélez de Escalante noted in their journal that the Native American name for this river suggested it was the same river they had named Buenaventura. They expressed doubts it was the same river because had it been so, it was substantially smaller downstream than it had been upstream-the opposite of the normal pattern. They named this river the Rio San Ysabel. The Native Americans told them it flowed west from there into a lake (Sevier Lake) and beyond. The Sevier Lake has no outlet, the indigenes may have been referencing the west-flowing Humboldt River which originates over 150 miles northwest, and were misinterpreted by the explorers.\n\nDespite Dominguez and Vélez de Escalante's doubts that the Green and Sevier Rivers were one and the same, the maps Miera produced do not include the Rio San Ysabel and depicts the Buenaventura flowing southwest from where they encountered it in northeastern Utah, to the Sevier Lake in west-central Utah. In an accompanying note to king Charles III of Spain, Miera recommended building several missions in the area and mentioned the possibility of a water way to the Pacific Ocean, via the Buenaventura or the Timpanogos River; the river Miera depicts on his map as flowing west from the Great Salt Lake (GSL). Although Miera documented a correct description of the GSL given to them by the indigenes the Spanish assumed that what they thought had been described was incorrect and interpreted their description of the \"extremely salty\" lake as the ocean and assumed the description of the river flowing from \"Lake Timpanogos\" which is the Jordan River flowing between Utah Lake and the Great Salt Lake, as a waterway to the Pacific.\n\nThe error of depicting the Buenaventura as flowing southwest to a lake was perpetuated by early explorers and cartographers such as Alexander von Humboldt, who used a map from the Dominguez and Escalanté expedition to prepare his maps in 1804 and 1809. and Zebulon Pike used Humboldts maps to prepare his map for his book from 1810 Aaron Arrowsmith in 1814 published a map depicting the Buenaventura flowing to \"Lac Sale\". These cartographers conservatively did not try to chart the area west of that explored by Dominguez and Escalante.\n\nThere had long been a hope that a river flowing west from the Rocky Mountains to the Pacific Ocean would provide an easy route for travel and trade. This dream was the descendant of the long sought Northwest Passage. When Francisco Garcés drew his maps of Alta California, he did not understand the nature of the Sierra Nevada, and he drew the \"San Felipe\" or San Joaquin River heading (originating) beyond the Sierras in the Great Basin to the Pacific Ocean, in or near San Francisco Bay. Then, when Manuel Augustin Mascaro and Miguel Constanso made the first map of the whole Viceroyalty of New Spain (1784), they extended the \"San Felipe\" almost to the Sevier Lake. A similar map was published in 1820 by Sidney E. Morse showing the Rio de San Buenaventura flowing into a lake, the western limits of which are unknown. This map shows the \"Supposed river between the Buenaventura and the Bay of Francisco, which will probably be the communication between the Atlantic and the Pacific\" toward this lake, but quite not connecting with it. A map by J. Finlayson (1822) shows a Rio de S. Buenaventura originating near the \" source of the Rio Colorado\" and emptying into a salt lake the western limits of which \"are unknown\". This same map indicates that an uncharted Rio de San Filipe crosses a range of mountains at 122 degrees west longitude.\n\nOther cartographers began to boldly assert that rivers flowed from Lake Timpanogos and \"Lac Salado\" to the Pacific Ocean. Henry S. Tanner's influential map of 1822 shows the Buenaventura River flowing from the north central Rockies through the Sevier Lake to the Pacific Ocean south of Monterey Bay. This map also shows two rivers flowing from Lake Timpanogos, (Utah Lake) one to San Francisco Bay (the R. Timpanogos), the other to Port Orford, Oregon (the Los Mongos R.) where the Rogue River enters the Pacific. A similar map by Anthony Finley was published in 1826 A map by Thomas Bradford (1835) shows a river flowing to San Francisco Bay (mislabeled Sir Francis Drake) from the south end of Lake Timpanogos. There is no reference to Rio de San Buenaventura. An 1844 map by James Bowden shows a landlocked Buenaventura wrapping around the southeast side of a \"doubtful\" Lake Timpanogos.\n\nThe Great Salt Lake (GSL) was first seen and reported as very saline by white North Americans in 1824, apparently independently by Jim Bridger and Etienne Provost. Upon learning of the GSL, explorers began equating Lake Timpanogos on the maps with it, and apparently unaware of the fact that saline lakes such as the Great Salt and Sevier Lakes have no outlets; began efforts to find the rivers flowing from them west to the Pacific as promised by maps such as Tanner's.\n\nIn 1825, William H. Ashley attempted to float the Green (Buenaventura) River to either what was called \"Salt Lake\" (actually Sevier Lake) on Tanner's map, or the Colorado River, to which the fur traders suspected it flowed. He started in present-day Wyoming, and after passing through the treacherous Gates of Lodore, aborted the trip prior to entering Desolation Canyon. Despite not having gotten close to the Colorado River, he concluded that the Green did empty into it, and continued his exploration by engaging Provost to lead him on an overland excursion to observe the GSL. The next spring his partner, Jedediah Smith, explored areas the north and west of the GSL, but found no rivers flowing from it. He sent more men to float around the shoreline to the west and south, and they were also unsuccessful in finding the \"Los Mongos R.\" and \"R. Timpanogos\". However, this information was delayed in reaching cartographers, and in 1836 Tanner repeated the depictions of the three rivers, except he identified the \"R. Timpanogos\" as \"R. S. Sacramento ou (or) Timpanogos\". So, when, in 1841, John Bidwell embarked over the Rocky Mountains to California, he was advised to take carpenters tools with them, to build canoes and sail from the GSL to the Pacific.\n\nAfter Ashley retired from the fur trade his partnership with Smith was bought out by Smith and two new partners, Smith focused on finding the Buenaventura. In 1826, he led an expedition south from southern Idaho, and upon reaching the mouth of the Jordan River on the southeast end of the GSL, traveled south along to the east side of Utah Lake. Since he identified the Jordan River as the outlet of Utah Lake, he did not explore the lake further. He encountered the Sevier River on its northeast-flowing stretch, and assuming it continued to flow north to Utah Lake, dismissed it as the possible Buenaventura and continued southwest to Southern California. In 1827 and 1828 he tracked the western flank of the Sierra Nevada in its full length, without registering a river that passed through the range, but he heard the Sacramento River referenced as the \"Buenaventura\" by Luis Antonio Argüello. A map by Albert Gallatin (1836), based on information from Smith's travels, labels the Sacramento River as the Buenaventura and equated Lake Timpanogos with the GSL, but did not try to connect the two.\n\nIn the spring of 1827, Daniel Potts, a fur trader in the employ of Smith, encountered the Sevier River downstream from the point Smith had the previous summer. He continued to follow the river downstream to the Sevier Lake, which he confirmed was a saline lake, but did not try to circumvent it to find an outlet. He instead followed the entire course of the river upstream to its head. \n\nThe Buenaventura River's existence or non-existence was a matter of controversy until 1843, when John Charles Frémont, with Thomas Fitzpatrick and Kit Carson as scouts, led a perilous expedition from the Columbia River to Sacramento, California via the Sierra Nevada. By that time the fact that the Buenaventura River did not flow from the headwaters of the Colorado/Green River was well established among the trappers and guides, but maps continued to show it and other rivers flowing from the region of Great Salt Lake to the sea. On January 27, 1844 at Walker River, he briefly believed himself to have found the mythical river, but it was the result of a faulty measurement. Two days later he discovered his mistake and definitively proved that the Buenaventura did not exist. Upon later finding the Salinas River flowed into the Pacific at about the same point the maps depicted the mouth of the Buenaventura, Frémont concluded that it was the source of the legend and applied the name \"Rio San Buenaventura\" to it.\n\nAfter Frémont established that no rivers flowed across the Great Basin region to the Pacific, President Polk was reluctant to accept his conclusion. However, after the premise was accepted that no east to west waterway flowed across the interior of the western United States, Frémont and his father-in-law and political sponsor, Senator Thomas Hart Benton, directed their ambitions to a transcontinental railway, which was completed in 1869, after the Mexican–American War of 1846–48 and the American Civil War.\n\n"}
{"id": "18005052", "url": "https://en.wikipedia.org/wiki?curid=18005052", "title": "Center for Southeastern Tropical Advanced Remote Sensing", "text": "Center for Southeastern Tropical Advanced Remote Sensing\n\nThe Center for Southeastern Tropical Advanced Remote Sensing (CSTARS) is a ground station that receives imagery data from a variety of remote sensing satellites. CSTARS is owned and operated by the Rosenstiel School of Marine and Atmospheric Science, a college within the University of Miami.\n\nCSTARS is a state-of-the-art real-time satellite reception and analysis facility located in southern Miami-Dade county, Florida. Its mission is collect satellite imagery for environmental monitoring of hurricanes, volcanoes, landslides and other natural or man made disasters.\n\nIn 2000, the University of Miami purchased the United States Naval Observatory Secondary National Time Standard Facility. The purchase included of land with several buildings and a 20-meter antenna once used for Very Long Baseline Interferometry. This large antenna is currently used to support scientific communications with the Antarctic (on behalf of the National Science Foundation). Two 11 meter X-band antennas were added to create a high bandwidth data reception capability for the downlink of satellite image data. Scientists and staff perform research and analysis activities on-site as well.\n\nThe station mask covers a very large area stretching from Hudson Bay in northeastern Canada down to northern South America in the south. The mask includes Central America, the eastern Pacific Ocean, the Caribbean Basin, the Gulf of Mexico and much of the Eastern US including Eastern Seaboard. CSTARS provides a VoIP communication system for the Amundsen–Scott South Pole Station.\n\nCSTARS is located on the Richmond campus of the University of Miami at .\n\nCSTARS played a vital role in the damage assessment and relief efforts of New Orleans and the Gulf Coast in the aftermath of Hurricane Katrina. The first remote sensing images illustrating the extent of the flooding in New Orleans were collected at CSTARS.\n\nThe United States Department of Homeland Security has included CSTARS as one of its \"Centers of Excellence\" so that CSTARS will work with the Stevens Institute of Technology to research port security and maritime monitoring issues.\n\nThe Office of Naval Research awarded CSTARS a grant to support satellite based research studies on internal waves and typhoons in the western Pacific Ocean.\n\nCSTARS participated in a Pew Charitable Trusts study to examine how to protect one of the largest Marine Protected (MPA) areas in the world through the utilization of commercially available earth imaging satellites.\n\nAdmiral James Stavridis, former commanding officer of United States Southern Command, writes about CSTARS in his book \"Partnership for the Americas\". In chapter 7 entitled \"Innovation\", he describes how CSTARS aided SOUTHCOM in humanitarian assistance and disaster relief (HA/DR) efforts during his tenure in the SOUTHCOM AOR (i.e., in Latin America and the Caribbean):\n\n"}
{"id": "1251637", "url": "https://en.wikipedia.org/wiki?curid=1251637", "title": "Community Identification Number", "text": "Community Identification Number\n\nThe Official Municipality Key, formerly also known as the Official Municipality Characteristic Number or Municipality Code Number, is a number sequence for the identification of politically independent municipalities or unincorporated areas. Other classifications for the identification of areas include postal codes, NUTS codes or FIPS codes.\n00923105059929\n\nThe municipality identifier consists of five digits in Austria, which are generated as follows: The designates the number of the Austrian state, the designate the district, and the designate the municipality.\n\n : Rappottenstein\n\n\n\n\n\nSee Code officiel géographique.\n\nIn Germany the Official Municipality Key serves statistical purposes and is issued by the statistics offices of individual German states. The municipality key is to be indicated in instances such as changing residence on the notice of departure or registration documents. This is done at the registration office in every town's city hall.\n\nThe municipality key consists of eight digits, which are generated as follows: The designate the individual German state. The designates the government district (in areas without government districts a zero is used instead). The designate the number of the urban area (in a district-free city) or the district (in a city with districts). The indicate the municipality or the number of the unincorporated area.\n\n : Stuttgart\n\n\n\n\n\nUsually known as the municipal code or , the number comprises nine digits rrppmmbbb, for which rr = region code, pp = province code, code, bbb = barangay code.\n\nFor example, the municipality of Ubay, Bohol has a code of 071246000 meaning region 07 (Central Visayas), province 12 (Bohol), municipality 46 (Ubay) with barangay code of zero signifying \"not at this level.\" Bongbong, one of its constituent barangays, has a code of 071246007.\n\nThe province code is unique and is independent of the region code. All PSGCs, are therefore, unique.\n\nThe Swiss Federal Statistical Office generates code numbers with up to four digits, which are sequentially assigned in accordance with the official order of the cantons, districts, and municipalities.\n\nA kommunkod (municipal code) is a numerical code given to all Swedish municipalities by the Swedish tax authorities. The code consists of four digits, the first two indicating which county the municipality is situated in, and the last two specific for the municipality.\n\nThe code system was introduced with the municipal reform of 1952. There were three different categories of municipalities at the time, which affected the number that they were allocated. Those with stad-status (cities) were given codes ending in 80 to 99, smaller towns (köping) 60 to 79 and rural municipalities 01 to 59. The county seats were allocated codes ending in 80.\n\nAs part of the reform in the early 1970s, the categorization of municipalities was abolished, but the code was in most cases preserved. When several municipalities were merged, the code for the biggest municipality was kept.\n\n"}
{"id": "10178401", "url": "https://en.wikipedia.org/wiki?curid=10178401", "title": "Distances Between Ports", "text": "Distances Between Ports\n\nDistances Between Ports (PUB 151) is a publication that lists the distances between major ports. Reciprocal distances between two ports may differ due to the different routes of currents and climatic conditions chosen. To reduce the number of listings\nneeded, junction points along major routes are used to consolidate routes converging from different directions.\n\nThis book can be most effectively used for voyage planning in conjunction with the proper volume(s) of the Sailing Directions (Planning Guide). It is corrected via the Notice to Mariners.\n\nThe positions listed for ports are central positions that most represent each port. The distances are between positions shown for each port and are generally over routes that afford the safest passage. Most of the distances represent the shortest navigable routes, but in some cases, longer routes, that take advantage of favorable currents, have been used. In other cases, increased distances result from routes selected to avoid ice or other dangers to navigation, or to follow required separation schemes.\n\nThe text of this article originated from section 414 of The American Practical Navigator, a document produced by the government of the United States of America.\n\n\n"}
{"id": "4876592", "url": "https://en.wikipedia.org/wiki?curid=4876592", "title": "Diver navigation", "text": "Diver navigation\n\nDiver navigation, termed \"underwater navigation\" by scuba divers, is a set of techniques—including observing natural features, the use of a compass, and surface observations—that divers use to navigate underwater. Free-divers do not spend enough time underwater for navigation to be important, and surface supplied divers are limited in the distance they can travel by the length of their umbilicals and are usually directed from the surface control point. On those occasions when they need to navigate they can use the same methods used by scuba divers.\n\nAlthough it is considered a basic skill, it is normally only taught to a limited degree as part of basic Open Water certification. Most North American diver training agencies only teach significant elements of underwater navigation as part of the Advanced Open Water Diver certification program.\n\nUnderwater navigation is usually a core component of most, if not all, advanced recreational diver training. In the PADI Advanced Open Water Diver course, it is one of the two mandatory skills (together with Deep diving) which must be taken alongside three elective skills.\n\nTraining agencies promote underwater navigation as a skill (despite the fact that it is less popular than other recreational diving specialties) on the basis that it:\n\nUnderwater compass navigation is a component of the scuba-based underwater sport, underwater orienteering.\n\nUnderwater navigation in recreational diving is broadly split into two categories. \"Natural navigation\" techniques, and \"orienteering\", which is navigation focused upon the use of an underwater magnetic compass.\n\nNatural navigation, sometimes known as pilotage, involves orienting by naturally observable phenomena, such as sunlight, water movement, bottom composition (for example, sand ripples run parallel to the direction of the wave front, which tends to run parallel to the shore), bottom contour and noise. Although natural navigation is taught on courses, developing the skills is generally more a matter of experience.\n\nOrienteering, or compass navigation, is a matter of training, practice and familiarity with the use of underwater compasses, combined with various techniques for reckoning distance underwater, including kick cycles (one complete upward and downward sweep of a kick), time, air consumption and occasionally by actual measurement. Kick cycles depend on the diver's finning technique and equipment, but are generally more reliable than time, which is critically dependent on speed, or air consumption, which is critically dependent on depth, work rate, diver fitness, and equipment drag. Techniques for direct measurement also vary, from the use of calibrated distance lines or surveyor's tape measures, to a mechanism like an impeller log, to pacing off the distance along the bottom with the arms.\n\nMany skilled underwater navigators use techniques from both of these categories in a seamless combination, using the compass to navigate between landmarks over longer distances and in poor visibility, while making use of the generic oceanographic indicators to help stay on course and as a check that there is no mistake with the bearing, and then recognising landmarks and using them with the remembered topography of a familiar site to confirm position.\n\nRecognisable topographical features may be remembered or noted and used identify position and direction. This is particularly useful if the visibility is sufficient to see the next landmark on the route before leaving the last. Landmarks are ordinarily considered permanent or semi-permanent features, such as ridges, boulders, wrecks or clumps of weed, but use can also be made of temporary marks such as anchor cables, shot lines, jackstays and guide lines.\n\nThe slope of the bottom is often a reliable indicator of the direction toward the shore, particularly when the bottom is of soft or loose material, and is not broken up greatly by rocky outcrops. This information can be checked for reliability on a sufficiently detailed chart of the area. Contours of depth running roughly parallel to the coastline indicate a slope dipping directly away from the shore, and can be used to maintain a sense of distance and orientation relative to the shore. In some places where the bottom is composed of predominantly rocky outcrops the slope may be in any direction and is not a reliable indicator of direction.\n\nIf circumstances of depth and water clarity allow the position of the sun to produce sufficient variation in brightness, this may indicate the direction of the sun, and be used as a cue to orientation. The effect is greater if the sun is relatively low in the sky, the water is clean, the depth fairly shallow and the surface fairly smooth.\n\nIn some circumstances the diver can look up at the surface, to see in which direction the land lies. These cues will not give any precise information about position, but will allow the diver to keep a mental picture of where he or she is and is going.\n\nCurrent direction can be useful as an orientation cue as long as the direction of the current is known. In rivers it tends to be fairly consistent and reliable, though localised eddies may occur. In the sea it may depend on weather conditions and local topography, as well as the state of the tide. In estuaries and harbours the currents will usually be predominantly tidal, so the state of the tide must be known, as the difference in direction between ebb and flow is usually about 180°.\n\nWave surge direction is essentially the same as wave direction, but may be felt at depths where the wave direction is no longer visible. It is useful if the offshore wave direction relative to the shore is known and does not change appreciably during the dive. In shallow water the wave crests will often be parallel to the shore. The important difference is that waves can be seen to travel in a definite direction, whereas surge is a back and forth motion, allowing a possible 180° error.\n\nA regular and distinct ripple pattern on a sand, mud or gravel bottom is an indication that it has been affected by wave action. The surge of the wave at depth causes the particles to be moved backwards and forwards in the direction the waves are travelling. This movement produces a ripple pattern on the bottom which is an indication of the wave direction on the surface. The ripple crests will be approximately parallel to the crests of the waves that formed them. It is however possible for the surface waves to change direction, and due to shorter wavelength, not reach the bottom to change the ripple pattern. When this is the case there will be no surge at the bottom. If there is a surge at the bottom, and the ripple crests are perpendicular the direction of the surge, then the wave crests will be parallel to the ripple crests. Ripple crests, like surge, may be interpreted with a 180° error.\n\nMany rock formations have characteristic angles known as dip and strike. Dip is the slope of the strata from the horizontal, and strike is the general direction of the strata in the horizontal plane (very roughly). These characteristics will usually be similar in the rocks above and below the water in a locality, so they can be used to estimate direction. Ridges above and below water are often parallel, and gullies and valleys may well extend under water for considerable distances.\n\nDifferent areas may for a wide variety of reasons have different ecologies. A diver who is familiar with an area can use the diversity variations and patterns to provide orientation cues.\n\nThere is often variation of ecological zoning with depth, but a diver is expected to be aware of the depth all the time anyway. In some places the seaward side of big rocks may have different species from the shoreward face because of the greater exposure to wave action.\n\nSea fans and sponges are filter feeders, and may grow into a fan shape at right angles to the usual current or surge direction, to get the maximum volume of water flowing past them.\n\nThe magnetic compass indicates the local direction of the ambient magnetic field, which is usually that of the Earth. This is usually a reliable and consistent feature and is very useful as a navigational aid as it is not affected by visibility, pressure, or the presence of water.\n\nAn important concept is that the compass card should not turn, even though it appears to always “swing” to magnetic north. The housing that holds the compass card turns around the card, which remains pointing in the same direction (Magnetic North) all the time. There are occasions when the card does turn, but this is when it has been stuck or the compass is turned over, and the card is unable to remain aligned with the magnetic field.\n\nTrue north is the geometrically accurate direction along the surface of the earth toward the North pole of the planet’s axis of rotation.\nThe lines of longitude on maps are in true North/South directions.\n\nThe Earth has a magnetic field which is not quite in line with the geographic directions. The difference between the magnetic and true directions is known as Variation. It differs from place to place and changes with time. Large scale charts and maps will usually include a compass rose showing variation.\n\nThe compass will indicate the magnetic field direction at the place where it happens to be at the time. If there are influences other than the Earth’s magnetic field, these may change the direction indicated by the compass. These effects are called deviation, and can be caused by a whole range of things. Any magnetic object or electrical current will have an influence, some more than others. The current in a dive computer is too small to affect the compass, even when quite near, but the hull of a ship or overhead power lines may make a difference even several meters away. It is difficult and often impossible to correct for all possible deviations, but it is worth checking a dive compass for deviation caused by dive equipment. It has been known for regulators to cause deviation, steel cylinders can cause deviation, and powerful lights may be a problem. A diver propulsion vehicle with an electric motor is also a potential problem for those who use them, though divers have been known to navogate adequately using compasses mounted on the handgrip of a DPV. A magnetic clip used to secure equipment to the diver's harness has a powerful magnet in both parts, and should not be used to hold the compass, as the part attached to the compass will produce a serious error.\n\nDeviation may be checked by comparing the compass bearing as measured with a known magnetic bearing measured by a compass with no deviation. Deviation may vary with different directions and for accurate work it is necessary to make up a table of deviations. This is done for ships, but for diving it is generally not worth the trouble. Bearings of one diver's compass may vary from those of another diver even if they have both been read correctly. The difference should not be large, but it can result in being off course and not finding something. A compass is a magnet, and will affect another compass nearby, so they can not be checked by putting them together.\n\nThe magnetic field of the earth is tilted from the horizontal. The angle is called dip and varies with place, so compasses can be corrected for different zones. This is a factory process. A compass made for the northern parts of the northern hemisphere will tilt badly in the southern hemisphere, in some cases to the extent that it will jam if held horizontal.\n\nAlso known as cave lines, distance lines, penetration lines and jackstays. These are permanent or temporary lines laid by divers to mark a route, particularly in caves, wrecks and other areas where the way out from an overhead environment may not be obvious. Guidelines are also useful in the event of silt out.\n\nDistance lines are wound on to a spool or a reel. The length of the distance line used is dependent on the plan for the dive. An open water diver using the distance line only for a surface marker buoy may only need 50 metres / 165 feet, whereas a cave diver may use multiple reels of lengths from 50 ft (15 m) to 1000+ ft (300 m).\n\nReels for distance lines may have a locking mechanism, ratchet or adjustable drag to control deployment of the line and a winding handle to help keep slack line under control and rewind line. Lines are used in open water to deploy surface marker buoys and decompression buoys and link the buoy on the surface to the submerged diver, or may be used to allow easy return navigation to a point such as a shotline or boat anchor.\n\nThe material used for any given distance line will vary based on intended use, nylon being the material of choice for cave diving. A common line used is 2 mm (0.08 inch) polypropylene line when it does not matter if the line is buoyant.\n\nThe use of guideline for navigation requires careful attention to laying and securing the line, line following, marking, referencing, positioning, teamwork, and communication.\n\nIn cave (and occasionally wreck) diving, line markers are used for orientation as a visual and tactile reference on a permanent guideline. Directional markers (commonly arrows), are also known as line arrows or Dorff arrows, and point the way to an exit. Line arrows may mark the location of a \"jump\" location in a cave when two are placed adjacent to each other. Two adjacent arrows facing away from each other, mark a point in the cave where the diver is equidistant from two exits. Arrow direction can be identified by feel in low visibility. Non-directional markers (\"cookies\") are purely personal markers that mark specific spots, or the direction of one's chosen exit at line intersections where there are options. Their shape does not provide a tactile indication of direction as this could cause confusion in low visibility. One important reason to be adequately trained before cave diving is that incorrect marking can confuse and fatally endanger not only oneself, but also other divers.\n\nIn some circumstances divers may be directed by their surface control personnel. This requires a method of communication between the surface team and the diver.\nBoth voice communications and line signals may be used to direct the movement of the diver and to provide other information.\nSurface direction may be used in scuba diving when diving under ice or conducting an underwater search, and in surface supplied diving for both these purposes and at any other time that it is useful or convenient for the dive controller to direct the movement of the diver. Surface direction is most useful when the surface personnel have a better idea of where the diver is relative to where he or she needs to be than the diver has, which can happen when the visibility is poor, or when the diver is following a search pattern controlled by the surface controller.\n\nSurface applications for compass navigation include marking a position and finding the position using compass bearings. At least two position lines are required to fix a position, as only direction can be found using a compass. When two bearings are used a large angle between the bearings will minimize error. The angle should preferably be between 60 and 120 degrees, and near 90 degrees would be ideal. Three bearings are better as they will also give an indication of probable accuracy when plotted on a chart. The \"cocked hat\" or triangle where the lines intersect, shows the probable location of the position measured, and a small triangle indicates a small probable error. The angle between the three bearings should preferably be in the order of 60 or 120 degrees where available landmarks allow. In all cases landmarks should be as close to the diver as possible and spread over a large arc for best accuracy.\n\nVarious pieces of equipment are available to assist divers navigating underwater.\nPeriodically reports are issued suggesting the development of underwater GPS technology, but no system is currently available on market. It is generally thought that the difficulty of locating satellite by signals from underwater at present is not capable of being overcome by existing technology.\n\nThe typical diving compass is made from a card with graduation in degrees, mounted on a pivot in a transparent housing filled with fluid which damps the movement and prevents pressure collapse of the housing. It may be wrist mounted, console mounted or carried some other way. It is desirable that the compass can operate accurately at significant tilt angles without sticking.\n\nOn the card there is a magnet which will interact with the ambient magnetic field so as to align itself and the card with the field provided it is free to rotate. There will be other marks on the housing which are intended to be aligned with the direction of travel of the user, so the offset of the card to the housing will indicate the direction of the magnetic field and the orientation of the user.\n\nImportant features of a diving compass are that it can easily be read in dim light, the card or needle does not easily jam if the housing is tilted slightly, and that it can be securely attached to the diver's arm or equipment and does not get lost. It is useful for any straps to be adjustable while wearing gloves, and any clips that may attached should be non-magnetic.\n\nThe strap should be long enough to go round the diver's wrist over the diving suit glove, and if it is slightly elastic it will stay in place when the suit compresses.\n\nThere may be a movable bezel which can be set to record a course and to help set a reciprocal course.\n\nThere are also electronic compasses which can provide a digital or analogue display These are based on magnetometer technology. Several models of dive computer incorporate a compass function, but this may not be accessible at the same time as the primary decompression information, and may be limited in their precision of display information.\n\nThere are two ways in which a compass may be marked, which influence the way you would read them. These are known as direct reading compasses and indirect reading compasses. Both provide the same information to the same level of accuracy. Both types may have graduations on the card which can be read through a side window to give the bearing directly.\n\nThe direct reading compass has graduations on the housing which read anti-clockwise round the face, with zero on the far side. \nThe effect of this configuration is that if the housing is aligned with a direction, the north point of the card or needle will point directly towards the number representing the bearing. No further effort is needed on the part of the operator, you just find the number the arrow points at and read off the bearing. \nThe bezel has no graduations, it is just a marker to align the card.\n\nThe indirect reading compass has graduations on the bezel. These graduations are clockwise round the face, and the zero mark coincides with the notch. To take a bearing the compass must first be aligned with the direction, then the bezel must be turned so that the notch aligns with the north point of the card or needle, and the bearing can then be read at the far side of the compass.\n\nFlux-gate compasses are built into several models of dive computer as an extra function. They may require calibration when powered up, but calibration usually lasts as long as the processor is running. They are usually insensitive to tilt as there are no moving parts to jam. The display varies, and may not be as intuitive as for a mechanical compass needle or card arrangement. They can often be calibrated to account for local deviation and give true direction. The nearby presence of a magnetic compass can cause large error, but they are not greatly affected by other electronic compasses, as can be seen from the images.\n\nSome digital cameras for underwater use also have a built in flux-gate compass (such as the Olympus TG series) which can be used for navigation as well as for recording the direction of a photograph.\n\n\n"}
{"id": "2884986", "url": "https://en.wikipedia.org/wiki?curid=2884986", "title": "Ex-meridian", "text": "Ex-meridian\n\nEx-meridian is a celestial navigation method of calculating an observer’s position on Earth. The method gives the observer a position line on which the observer is situated. It is usually used when the Sun is obscured at noon, and as a result, a meridian altitude is not possible. The navigator measures the altitude of the Sun as close to noon as possible and then calculates where the position line lies.\n\nThis method uses an assumed longitude and calculates the latitude that a position line crosses it. The position line obtained is actually part of a small circle, as opposed to great circle, where any observer can stand and the heavenly object would have the same altitude in the sky. When plotting the small segment of this circle on a chart it is drawn as a straight line, the resulting tiny errors are too small to be significant.\n\nThe assumed longitude is usually obtained from the DR or Dead Reckoning position run up from a morning sight taken at around 9.00 am. This is worked out by applying the distance from that position either by log or by the estimated speed over time with the course steered. A sight is taken, that is the distance above the horizon of a heavenly object, in this case nearly always the sun, is measured with a sextant and the exact time noted in UTC. The sextant angle obtained is corrected for dip (the error caused by the observers height above the sea) and refraction to obtain the true altitude of the object above the horizon. This is then subtracted from 90° to obtain the angular distance from the position directly above, the zenith. This is referred to as the True Zenith Distance. The true zenith distance of the object is also the distance (in arc) on the Earth's surface from the observer to where that object is overhead, the geographical position of the object.\n\nUsing a nautical almanac, the declination (celestial latitude), and the Greenwich hour angle (celestial longitude) are obtained of the observed object for the time of observation. The assumed longitude is now added or subtracted to the Greenwich Hour Angle of the object to obtain the local hour angle, that is the difference in longitude between the DR position and the geographical position of the object.\n\nWith this information it is possible using the haversine formula to calculate the latitude where the position line crosses the assumed longitude. The formula is:\n\nformula_1\n\nWhere\n\nformula_2 = Meridian Zenith Distance\n\nformula_3 = True Zenith Distance\n\nformula_4 = Local Hour Angle\n\nformula_5 = DR Latitude\n\nformula_6 = Declination\n\nOnce the value of the Meridian Zenith Distance is obtained the algebraic sum of it with the declination of the object gives the latitude of a point where the position line crosses the meridian of DR longitude.\n\nTo draw the position line on a chart the azimuth or bearing of the heavenly object must be known. It is usually calculated but could have been observed. A line at right angles to the azimuth is drawn through the calculated position which is where the calculated latitude and the DR longitude cross. The observer is somewhere on this line.\n\nTo obtain a fix (a position) this line must be crossed with another position line either from another sight or from elsewhere. In the case of ex-meridian the position line is usually crossed with the position line obtained earlier which has been run up.\n\nThe first of these tables applies corrections to the altitude taken with the argument of \"Change of Altitude in one minute from Meridian Passage\". Two other tables apply more corrections until the correct latitude is arrived at. \n\nThe Ex-Meridian method of calculating sights is at its most accurate when the azimuth of the object is near to south or north. As the azimuth changes towards the east or west the cross of the position line with the assumed longitude becomes more and more oblique and the position obtained is therefore less accurate. For this reason it is a less versatile method of calculating sights than the intercept method which can be used for all azimuths. The tables are a quick and easy way to correct the altitude when the object is fairly low in the sky and the observer has only missed noon by a few minutes but if noon has been missed by more than that or the sun is high in the sky it is better to work out a sight by the intercept method.\n\n\n"}
{"id": "16953152", "url": "https://en.wikipedia.org/wiki?curid=16953152", "title": "Extreme environment", "text": "Extreme environment\n\nAn extreme environment contains conditions that are hard to survive for most known life forms. These conditions may be extremely high or low temperature or pressure; high or low content of oxygen or carbon dioxide in the atmosphere; high levels of radiation, acidity, or alkalinity; absence of water; water containing a high concentration of salt or sugar; presence of sulphur, petroleum, and other toxic substances.\n\nExamples of extreme environments include the geographical poles, very arid deserts, volcanoes, deep ocean trenches, upper atmosphere, Mt Everest, outer space, and the environments of every planet in the Solar System except the Earth. Any organisms living in these conditions are often very well adapted to their living circumstances, which is usually a result of long-term evolution. Physiologists have long known that organisms living in extreme environments are especially likely to exhibit clear examples of evolutionary adaptation because of the presumably intense past natural selection they have experienced.\n\nThe distribution of extreme environments on Earth has varied through geological time. Humans generally do not inhabit extreme environments. There are organisms referred to as extremophiles that do live in such conditions and are so well-adapted that they readily grow and multiply.\n\nMost of the moons and planets in the Solar System are also extreme environments. Astrobiologists have not yet found life in any environments beyond Earth, though experiments have shown that tardigrades can survive the harsh vacuum and intense radiation of outer space. The conceptual modification of conditions in locations beyond Earth, to make them more habitable by humans and other terrestrial organisms, is known as terraforming.\n\nAmong extreme environments are alkaline, acidic, extremely cold, extremely hot, hyper-saline, places without water or oxygen, and places altered by humans, such as mine tailings or oil impacted habitats.\n\n\n\n"}
{"id": "904807", "url": "https://en.wikipedia.org/wiki?curid=904807", "title": "Frederick Cook", "text": "Frederick Cook\n\nFrederick Albert Cook (June 10, 1865 – August 5, 1940) was an American explorer, physician, and ethnographer, noted for his claim of having reached the North Pole on April 21, 1908. This was nearly a year before Robert Peary, who reached the North Pole on April 6, 1909. Both men's accounts were disputed for several years. His expedition did discover Meighen Island, the only discovery of an island in the American Arctic by a United States expedition.\n\nIn December 1909, after reviewing Cook's limited records, a commission of the University of Copenhagen ruled his claim unproven. In 1911, Cook published a memoir of his expedition, continuing his claim. His account of reaching Mount McKinley's summit has also been discredited.\n\nCook's birthplace is often listed as Callicoon, New York, but he was actually born in Hortonville, New York. His parents were recent German immigrants who anglicized their name by adopting the phonetic version of their surname. He attended Columbia University and received his doctorate from New York University Medical School in 1890. Cook married Libby Forbes in 1889. She died two years later. In 1902, on his 37th birthday, he married Marie Fidele Hunt. They had two daughters and divorced in 1923.\n\nCook was the surgeon on Robert Peary's Arctic expedition of 1891–1892, and on the Belgian Antarctic Expedition of 1897–1899. He contributed to saving the lives of its crew members when their ship—the —was ice-bound during the winter, as they had not prepared for such an event. It became the first expedition to winter in the Antarctic region. To prevent scurvy, Cook went hunting to keep the crew supplied with fresh meat.\n\nIn 1897, Cook twice visited Tierra del Fuego, where he met Anglican missionary Thomas Bridges. They studied the Ona and Yahgan, with whom Bridges had worked for two decades, during which time he had prepared a manuscript on their language's grammar and a dictionary of more than 30,000 words. Cook borrowed the manuscript for reference but failed to return it before Bridges' death in 1898. Several years later, he tried to publish the dictionary as his own.\n\nIn 1903, he led an expedition to Mount McKinley during which he circumnavigated the Denali range. He made a second journey in 1906, after which he claimed to have achieved the first summit of its peak with one other expedition crew member. Other members, including Belmore Browne, whom Cook had left on the lower mountain, immediately but privately expressed doubt. Cook's claims were not publicly challenged until 1909 when the dispute with Peary over the North Pole claim erupted, with Peary's supporters claiming Cook's McKinley ascent was also fraudulent.\n\nContrary to Hudson Stuck in 1913, Cook had not taken photographs from atop McKinley. His alleged photo of the summit was found to have been taken on a small outcrop on a ridge beside the Ruth Glacier, 19 miles away.\n\nIn late 1909, Ed Barrill, Cook's sole companion during the 1906 climb, signed an affidavit denying that they had reached the summit. In the late 20th century, historians found that he was paid by Peary supporters to do so. (Henderson writes that this fact was covered up at the time, but Bryce says that it was never a secret.) Up until a month before, Barrill had consistently asserted that he and Cook had reached the summit. His 1909 affidavit included a map correctly locating what became called Fake Peak, featured in Cook's \"summit\" photo, and showing that he and Cook had turned back at the Gateway.\n\nClimber Bradford Washburn has gathered data, repeated the climbs, and taken new photos to evaluate Cook's 1906 claim. Between 1956 and 1995, Washburn and Brian Okonek identified the locations of most of the photographs Cook took during his 1906 Denali foray and took new photos at the same spots. In 1997 Bryce identified the locations of the remaining photographs, including Cook's \"summit\" photograph; none were taken anywhere near the summit. Washburn showed that none of Cook's 1906 photos were taken past the \"Gateway\" (north end of the Great Gorge), 12 horizontal bee-line miles from Denali and 3 miles below its top.\n\nAn expedition by the Mazama Club in 1910 reported that Cook's map departed abruptly from the landscape at a point when the summit was still 10 miles distant. Critics of Cook's claims have compared Cook's map of his alleged 1906 route with the landscape of the last 10 miles. Cook's descriptions of the summit ridge are variously claimed to bear no resemblance to the mountain and to have been verified by many subsequent climbers. However, in the 1970s, climber Hans Waale found a route which fit Cook's narrative and descriptions. Three decades later, in 2005 and 2006, this route was successfully climbed by a group of Russian mountaineers.\n\nNo evidence of Cook's purported journey between the \"Gateway\" and the summit has been found. His claim to have reached the summit is not supported by his photos' vistas, his two sketch maps' markers, and peak-numberings for points attained, nor by his compass bearings, barometer readings, route-map or camp trash. However, samples of all such evidence have been found short of the Gateway.\n\nAfter the Mount Denali expedition, Cook returned to the Arctic in 1907. He planned to attempt to reach the North Pole, although he did not announce his intention until August 1907, when he was already in the Arctic. He left Annoatok, a small settlement in the north of Greenland, in February 1908. Cook claimed that he reached the pole on April 22, 1908, after traveling north from Axel Heiberg Island, taking with him only two Inuit men, Ahpellah and Etukishook. On the journey south, he claimed to have been cut off from his intended route to Annoatok by open water. Living off local game, his party was forced to push south to Jones Sound, spending the open water season and part of the winter on Devon Island. From there they traveled north, eventually crossing Nares Strait to Annoatok on the Greenland side in the spring of 1909. They said they almost died of starvation during the journey.\n\nCook and his two companions were gone from Annoatok for 14 months, and their whereabouts in that period is a matter of intense controversy. In the view of Canadian historian Pierre Berton (Berton, 2001), Cook's story of his trek around the Arctic islands is probably legitimate. Other writers have relied on later accounts told by Cook's companions to investigators, who seemed to present another view.\n\nThere are striking similarities between Ahpellah and Etukishook's sketched route of their journey south, and the route taken by the fictional shipwrecked explorers in Jules Verne's novel \"The Adventures of Captain Hatteras\". For example, the route the two Inuit traced on a map goes over both the Pole of Cold and the wintering site of the fictional expedition. Both expeditions went to the same area of Jones Sound in hopes of finding a whaling ship to take them to civilization. For details, see Osczevski (2003).\n\nCook's claim was initially widely believed, but it was disputed by Cook's rival polar explorer Robert Peary, who claimed to have reached the North Pole in April 1909. Cook initially congratulated Peary for his achievement, but Peary and his supporters launched a campaign to discredit Cook. They enlisted the aid of socially prominent persons outside the field of science, such as football coach Fielding H. Yost (as related in Fred Russell's 1943 book, \"I'll Go Quietly\").\nCook never produced detailed original navigational records to substantiate his claim to have reached the North Pole. He said that his detailed records were part of his belongings, contained in three boxes, which he left at Annoatok in April 1909. He had left them with Harry Whitney, an American hunter who had traveled to Greenland with Peary the previous year due to the lack of manpower for a second sledge-journey 700 miles south to Upernavik. When Whitney tried to bring Cook's boxes with him on his return to the USA on Peary's ship \"Roosevelt\" in 1909, Peary refused to allow them on board. As a result, Whitney left Cook's boxes in a cache in Greenland. They were never found.\n\nOn December 21, 1909, a commission at the University of Copenhagen, after having examined evidence submitted by Cook, ruled that his records did not contain proof that the explorer reached the Pole. (Peary refused to submit his records for review by such a third party, and for decades the National Geographic Society, which held his papers, refused researchers access to them.)\n\nCook intermittently claimed he had kept copies of his sextant navigational data, and in 1911 published some. These have an incorrect solar diameter. Ahwelah and Etukishook, Cook's Inuit companions, gave seemingly conflicting details about where they had gone with him. The major conflicts have been resolved in the light of improved geographical knowledge. Whitney was convinced that they had reached the North Pole with Cook, but was reluctant to be drawn into the controversy.\n\nThe Peary expedition's people (primarily Matthew Henson, who had a working knowledge of Inuit, and George Borup, who did not) claimed that Ahwelah and Etukishook told them they had only traveled a few days from land. A map allegedly was drawn by Ahwelaw and Etukishook correctly located and accurately depicted then-unknown Meighen Island, which strongly suggests that they visited it as they claimed. A genuine Cook discovery, Meighen Island, is the only island discovered by a United States expedition in the American arctic. For more detail see Bryce (1997) and Henderson (2005).\n\nThe conflicting claims of Cook and Peary prompted Roald Amundsen to take extensive precautions in navigation during his South Pole expedition so there could be no doubt concerning attainment of the pole if successful. Amundsen also had the advantage of traveling over a continent. He left unmistakable evidence of his presence at the South Pole, whereas any ice on which Cook might or might not have camped would have drifted many miles in the year between the competing claims.\n\nAt the end of his 1911 memoir, Cook wrote: \"I have stated my case, presented my proofs. As to the relative merits of my claim, and Mr. Peary's, place the two records side by side. Compare them. I shall be satisfied with your decision.\"\n\nCook's reputation never recovered. While Peary's North Pole claim was widely accepted for most of the 20th century, it has been discredited by a variety of reviewers, including the National Geographic Society, which long supported him. Cook spent the next few years defending his claim and threatening to sue writers who said that he had faked the trip. Researching the complicated story of the conflicting claims, the writer Robert Bryce began to see how the men's personalities and goals were in contrast, and set them against the period of the Gilded Age. He believes that Cook, as a physician and ethnographer, cared about the people on his expedition and admired the Inuit. Bryce writes that Cook \"genuinely loved and hungered for the real meat of exploration-mapping new routes and shorelines, learning and adapting to the survival techniques of the Eskimos, advancing his own knowledge-and that of the world-for its own sake.\" However, he could not find supporters to help finance the expeditions without a goal that was more flashy. There was tremendous pressure on each man to be the first to reach the Pole, in order to gain support for continued expeditions.\n\nIn 1919, Cook started promoting startup oil companies in Fort Worth. In April 1923, Cook and 24 other Fort Worth oil promoters were indicted in a federal crackdown on fraudulent oil company promotions. Three of Cook's employees pleaded guilty, but Cook insisted on his innocence and was put on trial. Joining him in the trial was his head advertising copywriter, S. E. J. Cox, who had been previously convicted of mail fraud in connection with his own oil company promotions.\n\nAmong other deceptive practices, Cook was charged with paying dividends from stock sales, rather than from profits. Cook's attorney was former politician Joseph Weldon Bailey, who clashed frequently with the judge. The jury found Cook guilty on 14 counts of fraud, and, in November 1923, Judge Killits sentenced Cook and 13 other oil company promoters to prison terms. Cook drew the longest sentence, 14 years 9 months. His attorney appealed the verdict, but the conviction was upheld.\n\nCook was imprisoned until 1930. Roald Amundsen, who believed he owed his life to Cook's extrication of the \"Belgica\", visited him several times. Cook was pardoned by President Franklin D. Roosevelt in 1940, ten years after his release and shortly before his death of a cerebral hemorrhage on August 5. He was interred at the Chapel of Forest Lawn Cemetery, Buffalo.\n\n\n\n"}
{"id": "48987289", "url": "https://en.wikipedia.org/wiki?curid=48987289", "title": "Free stationing", "text": "Free stationing\n\nIn surveying, free stationing (also known as resection) is a method of determining a location of one unknown point in relation to known points. There is a zero point of reference called a Total station. The instrument can be freely positioned so that all survey points are at a suitable sight from the instrument. Setting up the Total station on a known point, often it is not possible to see all survey points. With the Total Station, bearings and distances are measured to at least two known points of a control network. This with a handheld computer recorded data is related to local polar coordinates, defined by the Horizontal Circle of the total station. By a geometric transformation, these polar coordinates are transformed to the coordinate system of the control network. Errors are distributed by least squares adjustment. The position and orientation of the Total Station in relation to where the Control network is established. \n\n\nBecause bearings and distances in a Resection (Free Stationing) are measured, the result may have a different mathematical solution. This method of a \"Total station set up\" has different names in other languages, e.g. German: (Free Stationing).\n\nNaming is also regulated by the German Institute for Standardization DIN 18 709.\n\nBy measuring bearings and distances, local polar coordinates are recorded. The orientation of this local polar coordinate system is defined by the 0° Horizontal Circle of the Total station (polar axis L). The pole of this local polar coordinate system is the Vertical Axis (pole O) of the Total Stations. The polar coordinates (r,f) with the pole are transformed with a Surveying Software in a data collector to the Cartesian coordinates (x,y) of the known points and the coordinates for the position of the Total station are calculated.\n\nIn a resection (triangulation) measuring bearings only, there can be a problem with an infinite number of solutions called: \"danger circle\" or \"inscribed angle theorem\".\n\nThe back-sight points of the control network should cover and surround the stationing site. The position of the total station is not part of the area. This is the area where you want to measure with this station setup. Topographic points or stakeout points should not be measured outside this area. If measured outside this area, the errors in orientation will be extrapolated instead of being interpolated.\n\nWhile it is possible to use only two known control points in a resection (free stationing), it is recommended to use three control points. There is no redundancy for orientation, using two points only.\n\nUsing five or more points of the control network, there is only a slight improvement in the accuracy.\n\n\nBecause of the range and accuracy of total stations, the method of a resection (free stationing) permits a great freedom of positioning the total station. For this reason, this method is one of the most used station set ups.\n\nWith the calculated coordinates and orientation of the total station, it can be used to set out points in construction surveying, machine guidance, site plan or other types of surveys.\n\n"}
{"id": "57425862", "url": "https://en.wikipedia.org/wiki?curid=57425862", "title": "Geographic contiguity", "text": "Geographic contiguity\n\nGeographic contiguity is the characteristic in geography of political or geographical land divisions, as a group, not being interrupted by other land or water. Such divisions are referred to as being \"contiguous.\" In the United States, for example, the \"48 contiguous states\" excludes Hawaii and Alaska, which do not share borders with other U.S. states.\n\nOther examples of geographical contiguity might include the \"contiguous European Union\" excluding member states such as the United Kingdom, Ireland, Sweden, Finland, Malta and Cyprus (these being non-contiguous), or the \"contiguous United Kingdom\" referring to all parts of the country excepting Northern Ireland (it being geographically non-contiguous).\n\nTwo or more contiguous municipalities can be consolidated into one, or one municipality can consist of many noncontiguous elements. For example, the Financially Distressed Municipalities Act allows the commonwealth of Pennsylvania to merge contiguous municipalities to reduce financial distress.\n\nGeographic contiguity is important in biology, especially animal ranges. For a particular species, its habitat may be a 'contiguous range', or it might be broken, requiring periodic, typically seasonal migrations (see: Disjunct distribution). The same concept of contiguous range is true for human transportation studies in an attempt to understand census geography. It also comes into play with electoral geography and politics.\n"}
{"id": "31346116", "url": "https://en.wikipedia.org/wiki?curid=31346116", "title": "Geographical cluster", "text": "Geographical cluster\n\nA geographical cluster is a localised anomaly, usually an excess of something given the distribution or variation of something else. Often it is considered as an incidence rate that is unusual in that there is more of some variable than might be expected. Examples would include: a local excess disease rate, a crime hot spot, areas of high unemployment, accident blackspots, unusually high positive residuals from a model, high concentrations of flora or fauna, physical features or events like earthquake epicenters etc... \n\nIdentifying these extreme regions may be useful in that there could be implicit geographical associations with other variables that can be identified and would be of interest. Pattern detection via the identification of such geographical clusters is a very simple and generic form of geographical analysis that has many applications in many different contexts. The emphasis is on localized clustering or patterning because this may well contain the most useful information. \n\nA geographical cluster is different from a high concentration as it is generally second order, involving the factoring in of the distribution of something else. \n\nIdentifying geographical clusters can be an important stage in a geographical analysis. Mapping the locations of unusual concentrations may help identify causes of these. Some techniques include the Geographical Analysis Machine and Besag and Newell's cluster detection method.\n"}
{"id": "1009410", "url": "https://en.wikipedia.org/wiki?curid=1009410", "title": "Geographical segregation", "text": "Geographical segregation\n\nGeographical segregation exists whenever the proportions of population rates of two or more populations are not homogenous throughout a defined space. Populations can be considered any plant or animal species, human genders, followers of a certain religion, people of different nationalities, ethnic groups, etc.\n\nIn social geography segregation of ethnic groups, social classes and genders is often measured by the calculation of indices such as the index of dissimilarity. Different dimensions of segregation (or its contrary) are recognised: exposure, evenness, clustering, concentration, centralisation, etc. More recent studies also highlight new local indices of segregation.\n\nSegregation, as a broad concept, has appeared in all parts of the world where people exist—in different contexts and times it takes on different forms, shaped by the physical and human environments. The spatial concentration of population groups is not a new phenomenon. Since societies began to form there have been segregated inhabitants. Either segregated purposefully by force, or gradually over time, segregation was based on socio-economic, religious, educational, linguistic or ethnic grounds. Some groups choose to be segregated to strengthen social identity.\n\nSegregation can be caused by legal frameworks, such as in the extreme example of apartheid in South Africa, and even Jewish ghettoization in Germany in the 20th century. Segregation can also happen slowly, stimulated by increased land and housing prices in certain neighborhoods, resulting in segregation of rich and poor in many urban cities. Segregation can also be assigned arbitrarily. This can occur on a global scale, such as is seen in the Partition of India, instances in Ireland, and many other situations. Geographical boundaries were often put in place without much consideration for native peoples and natural geographic terrain and cultural limits that had long been in place.\n\nIn apartheid South Africa, segregation was very much a legal concept. Enforced by the government, Africans were discriminated against, and forced to comply with apartheid. Some of the legislation passed dealt with physical segregation in schools, land tenure, geographic segregation and state repression. These were very clearly legislative, but also in the case of most white South Africans, a social construct as well.\n\nSegregation can also be encouraged, using geographical boundaries, while not explicitly enforced. Public housing projects, especially in the United States, have been criticized for this. Putting cheap housing in poor black neighborhoods encouraged local African-Americans to stay in the area, keeping other richer areas white by not building public housing there. This has been changing in the last ten years.\n\nSegregation can also be caused by social factors that become evident as they happen, but are not necessarily government sanctioned. This could be things like informal ghettos, or simply rich neighborhoods. In terms of land capital, over time in a given area, humans will settle down and buy or take land. Some privileged people will acquire better land (that is, more arable, proximate to potential capital, more pleasing views). Demand for these nicer habitats drives up prices, and areas deemed “better” based solely on geography become inherently exclusionary in their population makeup.\n\nWest Point Grey, an area of Vancouver Canada, is in part rich because of the views offered of Downtown Vancouver, the Gulf Islands, and its location near the water and University of British Columbia. Wealthy people had the resources to pay for advantages, and subsequently drove up prices. Examples of this can be seen all over the world. Geographical segregation is not defined by the sightline of places, though. It also occurs around certain structures, or simply in areas that are specifically developed with an income bracket in mind.\n\nAnother segregation term, the ghetto, has been used in many different contexts over time, generally meaning any physical part of a city predominantly occupied by any particular group of people. It implies that the group may be looked down upon and segregated purposefully. This does not mean that all ghettos are built up communities and buildings specifically for a segregation purpose, although many are. In the case of the United States, segregation of the African-American community was to a degree due to white flight out of the cities, than forcing African-Americans to live in the downtown cores.\n\nGated communities could be seen as a combination of both legal frameworks and social conventions regarding segregation. A gated community today is a controlled neighborhood, inhabited by people with common interests, such as safety, or class separation, but not necessarily of the same ethnicity or religion—it is distinct from an international community (in most cases). Gated communities are very controversial, as they can be seen as encouraging distinction and separation, and therefore superiority from those who do not live with the gates community.\n\nVoluntary segregation is almost as common an occurrence as involuntary segregation is. Often, immigrants coming to a new and foreign country will band together for mutual benefit, and to keep a sense of community in the new country. These can be called ethnic enclaves and can be formed by any community or people group. Some well-known groups are Chinatowns, Little Italys and \"barrios\". These localized phenomena also come in the form of ethnoburbs, which are essentially the same concept as an ethnic enclave, but specifically located in suburbs, rather than the traditional downtowns, where Chinatowns and Little Italys are usually based.\n\n"}
{"id": "2853761", "url": "https://en.wikipedia.org/wiki?curid=2853761", "title": "Intercept method", "text": "Intercept method\n\nThe intercept method, also known as Marcq St. Hilaire method, is an astronomical navigation method of calculating an observer's position on earth. It was originally called the \"azimuth intercept\" method because the process involves drawing a line which intercepts the azimuth line. This name was shortened to \"intercept\" method and the \"intercept distance\" was shortened to 'intercept'.\n\nThe method yields a line of position (LOP) on which the observer is situated. The intersection of two or more such lines will define the observer's position, called a \"fix\". Sights may be taken at short intervals, usually during hours of twilight, or they may be taken at an interval of an hour or more (as in observing the Sun during the day). In either case, the lines of position, if taken at different times, must be advanced or retired to correct for the movement of the ship during the interval between observations. If observations are taken at short intervals, a few minutes at most, the corrected lines of position by convention yield a \"fix\". If the lines of position must be advanced or retired by an hour or more, convention dictates that the result is referred to as a \"running fix\".\n\nThe intercept method is based on the following principle.\nThe actual distance from the observer to the geographical position (GP) of a celestial body (that is, the point where it is directly overhead) is \"measured\" using a sextant. The observer has already estimated his position by dead reckoning and calculated the distance from the estimated position to the body's GP; the difference between the \"measured\" and calculated distances is called the intercept.\n\nThe diagram on the right shows why the zenith distance of a celestial body is equal to the angular distance of its GP from the observer's position.\n\nThe rays of light from a celestial body are assumed to be parallel (unless the observer is looking at the moon, which is too close for such a simplification). The angle at the centre of the Earth that the ray of light passing through the body's GP makes with the line running from the observer's zenith is the same as the zenith distance. This is because they are corresponding angles. In practice it is not necessary to use zenith distances, which are 90° minus altitude, as the calculations can be done using observed altitude and calculated altitude.\n\nTaking a sight using the intercept method consists of the following process:\n\nSuitable bodies for celestial sights are selected, often using a Rude Star Finder. Using a sextant, an altitude is obtained of the sun, the moon, a star or a planet. The name of the body and the precise time of the sight in UTC is recorded. Then the sextant is read and the altitude (\"Hs\") of the body is recorded. Once all sights are taken and recorded, the navigator is ready to start the process of sight reduction and plotting.\n\nThe first step in sight reduction is to correct the sextant altitude for various errors and corrections. The instrument may have an error, IC or index correction (See article on adjusting a sextant). Refraction by the atmosphere is corrected for with the aid of a table or calculation and the observer's height of eye above sea level results in a \"dip\" correction, (as the observer's eye is raised the horizon dips below the horizontal). If the Sun or Moon was observed, a semidiameter correction is also applied to find the centre of the object. The resulting value is \"observed altitude\" (\"Ho\").\n\nNext, using an accurate clock, the observed celestial object's geographic position (\"GP\") is looked up in an almanac. That's the point on the Earth's surface directly below it (where the object is in the zenith). The latitude of the geographic position is called declination, and the longitude is usually called the hour angle.\n\nNext, the altitude and azimuth of the celestial body are computed for a selected position (assumed position or AP). This involves resolving a spherical triangle. Given the three magnitudes: local hour angle (\"LHA\"), observed body's declination (\"dec\"), and assumed latitude (\"lat\"), the altitude \"Hc\" and azimuth \"Zn\" must be computed. The local hour angle, \"LHA\", is the difference between the AP longitude and the hour angle of the observed object. It is always measured in a westerly direction from the assumed position.\n\nThe relevant formulas (derived using the spherical trigonometric identities) are:\n\nor, alternatively,\n\nWhere\n\nThese computations can be done easily using electronic calculators or computers but traditionally there were methods which used logarithm or haversine tables. Some of these methods were H.O. 211 (Ageton), Davies, haversine, etc. The relevant haversine formula for \"Hc\" is\n\nWhere \"\" is the zenith distance, or complement of \"Hc\".\n\n\"\" = 90° - \"Hc\".\n\nThe relevant formula for Zn is\n\nWhen using such tables or a computer or scientific calculator, the navigation triangle is solved directly, so any assumed position can be used. Often the dead reckoning DR position is used. This simplifies plotting and also reduces any slight error caused by plotting a segment of a circle as a straight line.\n\nWith the use of astral navigation for air navigation, faster methods needed to be developed and tables of precomputed triangles were developed. When using precomputed sight reduction tables, selection of the assumed position is one of the trickier steps for the fledgling navigator to master. Sight reduction tables provide solutions for navigation triangles of integral degree values. When using precomputed sight reduction tables, such as H.O. 229, the assumed position must be selected to yield integer degree values for \"LHA\" (local hour angle) and latitude. West longitudes are subtracted and east longitudes are added to \"GHA\" to derive \"LHA\", so AP's must be selected accordingly. When using precomputed sight reduction tables each observation and each body will require a different assumed position.\n\nProfessional navigators are divided in usage between sight reduction tables on the one hand, and handheld computers or scientific calculators on the other. The methods are equally accurate. It is simply a matter of personal preference which method is used. An experienced navigator can reduce a sight from start to finish in about 5 minutes using nautical tables or a scientific calculator.\n\nThe precise location of the assumed position has no great impact on the result, as long as it is reasonably close to the observer's actual position. An assumed position within 1 degree of arc of the observer's actual position is usually considered acceptable.\n\nThe calculated altitude (\"Hc\") is compared to the observed altitude (\"Ho\", sextant altitude (\"Hs\") corrected for various errors). The difference between \"Hc\" and \"Ho\" is called \"intercept\" and is the observer's distance from the assumed position. The resulting line of position (\"LOP\") is a small segment of the circle of equal altitude, and is represented by a straight line perpendicular to the azimuth of the celestial body. When plotting the small segment of this circle on a chart it is drawn as a straight line, the resulting tiny errors are too small to be significant.\n\nNavigators use the memory aid \"computed greater away\" to determine whether the observer is farther from the body's geographic position (measure intercept from \"Hc\" away from the azimuth). If the \"Hc\" is less than \"Ho\", then the observer is closer to the body's geographic position, and intercept is measured from the AP toward the azimuth direction.\n\nThe last step in the process is to plot the lines of position \"LOP\" and determine the vessel's location. Each assumed position is plotted first. Best practise is to then advance or retire the assumed positions to correct for vessel motion during the interval between sights. Each LOP is then constructed from its associated AP by striking off the azimuth to the body, measuring intercept toward or away from the azimuth, and constructing the perpendicular line of position.\n\nTo obtain a fix (a position) this \"LOP\" must be crossed with another \"LOP\" either from another sight or from elsewhere e.g. a bearing of a point of land or crossing a depth contour such as the 200 metre depth line on a chart.\n\nUntil the age of satellite navigation ships usually took sights at dawn, during the forenoon, at noon (meridian transit of the Sun) and dusk. The morning and evening sights were taken during twilight while the horizon was visible and the stars, planets and/or moon were visible, at least through the telescope of a sextant. Two observations are always required to give a position accurate to within a mile under favourable conditions. Three are always sufficient.\n\nA fix is called a \"running fix\" when one or more of the LOPs used to obtain it is an LOP advanced or retrieved over time. In order to get a fix the LOP must cross at an angle, the closer to 90° the better. This means the observations must have different azimuths. During the day, if only the Sun is visible, it is possible to get an LOP from the observation but not a fix as another LOP is needed. What may be done is take a first sight which yields one LOP and, some hours later, when the Sun's azimuth has changed substantially, take a second sight which yields a second LOP. Knowing the distance and course sailed in the interval, the first LOP can be advanced to its new position and the intersection with the second LOP yields a \"running fix\".\n\nAny sight can be advanced and used to obtain a \"running fix\". It may be that the navigator due to weather conditions could only obtain a single sight at dawn. The resulting LOP can then be advanced when, later in the morning, a Sun observation becomes possible. The precision of a running fix depends on the error in distance and course so, naturally, a running fix tends to be less precise than an unqualified fix and the navigator must take into account his confidence in the exactitude of distance and course to estimate the resulting error in the running fix.\n\nDetermining a fix by crossing LOPs and advancing LOPs to get running fixes are not specific to the intercept method and can be used with any sight reduction method or with LOPs obtained by any other method (bearings, etc.).\n\n\n\n"}
{"id": "2286680", "url": "https://en.wikipedia.org/wiki?curid=2286680", "title": "International Ice Patrol", "text": "International Ice Patrol\n\nThe International Ice Patrol is an organization with the purpose of monitoring the presence of icebergs in the Atlantic and Arctic Oceans and reporting their movements for safety purposes. It is operated by United States Coast Guard but is funded by the 13 nations interested in trans-Atlantic navigation. As of 2011 the governments contributing to the International Ice Patrol include Belgium, Canada, Denmark, Finland, France, Germany, Greece, Italy, Japan, the Netherlands, Norway, Panama, Poland, Spain, Sweden, the United Kingdom, and the United States of America.\n\nThe organization was established in 1914 in response to the sinking of the RMS \"Titanic\". The primary mission of the Ice Patrol is to alert any seacraft traveling the great circle shipping lanes between Europe and the major ports of the United States and Canada of the presence of any icebergs there. \n\nFrom the earliest journeys into the North Atlantic, icebergs have threatened vessels. A review of the history of navigation prior to the turn of the 20th century shows an impressive number of casualties occurred in the vicinity of the Grand Banks of Newfoundland. For example, \"Lady of the Lake\" sank in 1833 with a loss of 215 people. Between 1882 and 1890, 14 vessels were lost and 40 seriously damaged due to ice. This does not include the large number of whaling and fishing vessels lost or damaged by ice. It took one of the greatest marine disasters of all time to arouse public demand for international cooperative action to deal with this marine hazard. This disaster, the sinking of the on 15 April 1912, was the prime impetus for the establishment of the International Ice Patrol.\n\nOn her maiden voyage from Southampton, England bound for New York, \"Titanic\" collided with an iceberg just south of the tail of the Grand Banks and sank in less than three hours. The loss of life was enormous with more than 1,500 of the 2,224 passengers and crew perishing. \"Titanic\", the brand new ship of the White Star Line, was the largest passenger liner of her time displacing 45,000 tons and capable of sustained speed in excess of . Loss of \"Titanic\" gripped the world with a sobering awareness of an iceberg's potential for tragedy. The sheer dimensions of the \"Titanic\" disaster created sufficient public reaction on both sides of the Atlantic to prod reluctant governments into action, producing the first Safety of Life at Sea (SOLAS) convention in 1914. \n\nAfter the \"Titanic\" disaster, the U.S. Navy assigned the cruisers and to patrol the Grand Banks of Newfoundland for the remainder of 1912. In 1913, the Navy could not spare ships for this purpose, so the Revenue Cutter Service (forerunner of the United States Coast Guard) assumed responsibility, assigning the USRC \"Seneca\" and USRC \"Miami\" to conduct the patrol.\n\nAt the first International Conference on the Safety of Life at Sea, which was convened in London on 12 November 1913, the subject of patrolling the ice regions was thoroughly discussed. The convention signed on 30 January 1914, by the representatives of the world's various maritime powers, provided for the inauguration of an international derelict-destruction, ice observation, and ice patrol service, consisting of vessels, which should patrol the ice regions during the season of iceberg danger and attempt to keep the trans-Atlantic lanes clear of derelicts during the remainder of the year. Due primarily to the experience gained in 1912 and 1913, the United States Government was invited to undertake the management of the triple service, the expense to be defrayed by the 13 nations interested in trans-Atlantic navigation.\n\nThe second International Conference on Safety of Life at Sea was convened in London on 16 April 1929. Eighteen nations participated, all of which signed the final act on 31 May 1929. Because of the fear in the United States Senate as a result of ambiguities in Article 54 dealing with control, the 1929 convention was not ratified by the United States until 7 August 1936, and even then the ratification was accompanied by three reservations. At the same time, Congress enacted legislation on 25 June 1936, formally requiring the Commandant of the Coast Guard to administer the International Ice Observation and Ice Patrol Service (Chap. 807, para. 2 49 USC 1922) and describing in general fashion the manner in which this service was to be performed. With only minor changes, this remains today as the basic Coast Guard authority to operate the International Ice Patrol. Since 1929, there have been three SOLAS conventions (1948, 1960 & 1974). None of these have recommended any basic change affecting the Ice Patrol.\n\nEvery year since 1914, the United States Coast Guard and the International Ice Patrol lay a wreath from a ship or an aircraft at the site of the \"Titanic\" disaster on 15 April. The solemn ceremony is attended by the craft's crew and a dedication statement to the \"Titanic\" and her lost passengers is read.\n\nFrom its inception until the beginning of World War II, the Ice Patrol was conducted from two surface patrol cutters alternating surveillance patrols of the southern ice limits. In 1931 and thereafter a third ship was assigned to Ice Patrol to perform oceanographic observations in the vicinity of the Grand Banks. After World War II, aerial surveillance became the primary ice reconnaissance method with surface patrols phased out except during unusually heavy ice years or extended periods of reduced visibility. Use of the oceanographic vessel continued until 1982, when the Coast Guard's sole remaining oceanographic ship, , was converted to a medium endurance cutter. The aircraft has distinct advantages for ice reconnaissance providing much greater coverage in a relatively short period of time.\n\nFrom 1946 until 1966, the Ice Patrol offices, operations center and reconnaissance aircraft were based at the Coast Guard Air Detachment Argentia, Newfoundland during the ice season.\n\nDue to changing operational commitments and financial constraints the Coast Guard Argentia Air Detachment closed in 1966. Ice Patrol headquarters and operations center moved to Governors Island, New York where they remained until October 1983. \n\nToday the International Ice Patrol is located at the Coast Guard Research and Development Center in New London, Connecticut. The ice reconnaissance detachment, usually composed of eleven aircrew and four ice observers flying in a HC-130 aircraft, continues to work out of Newfoundland.\n\nThe Ice Patrol disseminates information on icebergs and the limit of all known sea ice via radio broadcast from the U.S. Coast Guard Communications Command (COMMCOM) located in Chesapeake, Virginia via Inmarsat Safetynet, and radio facsimile chart. Ice Patrol information is also available via http Internet access. 2002 changes to SOLAS requires ships transiting the region guarded by the Ice Patrol to use the services provided during the ice season.\n\n\n"}
{"id": "625551", "url": "https://en.wikipedia.org/wiki?curid=625551", "title": "Land claim", "text": "Land claim\n\nA land claim is a legal declaration of desired control over areas of property, including bodies of water. The phrase is usually only used with respect to disputed or unresolved land claims. Some types of land claims include aboriginal land claims, Antarctic land claims, and post-colonial land claims.\n\nLand claims is sometimes used as a term when referring to disputed territories like Western Sahara or to refer to the claims of displaced persons. \n\nIn the colonial times of the United States American men could claim a piece of land for themselves and the claim has different level of merit according to the de facto conditions:\n\nToday, only small areas of unclaimed land remain, yet large plots of land with little economical value (e.g., in Alaska) can still be bought for very low prices. Also, in certain parts of the world, land can still be obtained by making productive use of it.\n\nA mining claim is the claim of the right to extract minerals from a tract of public land. In the United States, the practice began with the California gold rush of 1849. In the absence of effective government, the miners in each new mining camp made up their own rules, and chose to essentially adopt Mexican mining law then in effect in California. The Mexican law gave the right to mine to the first one to discover the mineral deposit and begin mining it. The area that could be claimed by one person was limited to that which could be mined by a single individual or a small group.\n\nThe US system of mining claims is an application of the legal theory of \"prior appropriation\", by which public property is granted to the first one to put it to beneficial use. Other applications of appropriation theory were the Homestead Act, which granted public land to farmers, and water rights in the west.\n\nThe California miners spread the concept of mining claims to other mining districts all over the western United States. The US Congress legalized the practice in 1866, and amended it in the Mining Act of 1872. All land in the public domain, that is, federal land whose use has not been restricted by the government to some specific purpose, was subject to being claimed. The mining law has been changed numerous times, but still retains some features similar to those settled on by the California 49ers.\n\nThe concept was also used in other countries, for example during the Australian gold rushes which occurred at a similar time starting from the 1850s, and included similar groups of people including miners that migrated from the American gold rushes. The Oriental Claims in Victoria are one example of this.\n\nStaking a claim involves first the discovery of a valuable mineral in quantities that a \"prudent man\" (the Prudent Man Rule) would invest time and expenses to recover. Next, marking the claim boundaries, typically with wooden posts or capped steel posts, which must be four feet tall, or stone cairns, which must be three feet tall. Finally, filing a claim with both the land management agency (USFS or BLM), and the local county registrar.\n\nThere are four main types of mining claims: \n\nA mining claim always starts out as an unpatented claim. The owner of an unpatented claim must continue mining or exploration activities on an unpatented claim, or he may pay a fee to the land management agency by September 1 of each year, or it is considered abandoned and becomes null. Activities on unpatented claims must be restricted to those necessary to mining. A patented claim is one for which the federal government has issued a \"patent\" (deed). To obtain a patent, the owner of a mining claim must prove to the federal government that the claim contains locatable minerals that can be extracted at a profit. A patented claim can be used for any purpose desired by the owner, just like any other real estate. However, Congress has ceased funding for the patenting process, so at this time a claim cannot be patented.\n\n"}
{"id": "26054581", "url": "https://en.wikipedia.org/wiki?curid=26054581", "title": "Land surveying in Kentucky", "text": "Land surveying in Kentucky\n\nLand Surveying in Kentucky is the practice of Land Surveying in the Commonwealth of Kentucky.\n\nIt is a good idea to have a property survey:\n\nOften a residential customer will request a Boundary Survey. A mortgage inspection is less expensive, but offers much less protection and value. ALTA/ACSM surveys in Kentucky are typically requested only for commercial activity.\n\nMost land surveying work is acquired through the personal recommendation of satisfied clients, lending institutions, attorneys, real estate companies and title companies who frequently handle real estate transactions.\n\nEach of Kentucky's 120 counties elects a County Surveyor. However, a consumer can seek services from any licensed Kentucky Professional Land Surveyor. The status of a licensee or firm can be verified online, including checking for Disciplinary Actions in the past 5 years. Formal complaints against a Professional Land Surveyor should be made in writing to the Kentucky Board of Engineers and Land Surveyors.\n\nResidents should be aware of the right of entry on land of others by a land surveyor, which include persons such as employees that are under the supervision of a Professional Land Surveyor. A professional land surveyor shall not be liable to arrest or a civil action by reason of this entry. No owner or occupant of the land shall be liable for any injury or damage sustained by any person entering upon his or her land.\n\nLand Surveying in Kentucky is regulated by the Commonwealth in KRS 322. The Standards of Practice are defined in 201 KAR 18:150. Compliance is maintained by the Kentucky Board of Engineers and Land Surveyors, which was established by an Act of the Kentucky General Assembly in 1938.\n\nAll chain of title in the Commonwealth traces back to Virginia land patents and Kentucky land patents. The Secretary of State maintains the security and preservation of these historical documents. More recent Deed and Plat records are maintained by Kentucky's County Clerks.\n\nMost of the state utilizes the Metes and bounds land system, which is based on English Common Law and reflects the Commonwealth's original status as a part of Virginia until statehood in 1792. Parts of Western Kentucky, since it was acquired under the Jackson Purchase in 1818, utilizes a rectangular system based on the Public Land Survey System created by the Land Ordinance of 1785.\n\nCommon surveying measures in Kentucky include acre and the survey foot, which are both now referenced in decimal and historically in fraction. For example, a modern survey should list a distance of one-foot and six-inches as 1.50 feet. Historically lengths were also measured as chain (length) and rod (length). A rod is also known as a pole, both being 16.5 feet. A chain is most commonly 66 feet (4 poles) but can also be 33 feet (2 poles).\n\nNotable Surveyors in Kentucky History include Daniel Boone, Abraham Lincoln, Thomas Bullitt, George Rogers Clark, and Isaac Shelby.\n\nModern technology used in surveying in Kentucky includes GPS, the Total station, and CAD. A popular CAD system with Kentucky Surveyors is Carlson Software, which is headquartered in Maysville, Kentucky. Surveying is also aided by the development of Geographic information systems, such as LOJIC.\n\nThe Kentucky Association of Professional Surveys (KAPS) maintains and perpetuates an organization for members having common professional problems and interests. The organization provides effective forums for discussion and united action on the part of its members for the enhancement and betterment of professional recognition, status and conditions of employment. KAPS also addresses other matters which contribute to the welfare of the people of the Commonwealth of Kentucky, KAPS membership and State Government.\n\nJohnson, E. Polk (1912). A history of Kentucky and Kentuckians, Lewis Publishing Company\n\n"}
{"id": "23515723", "url": "https://en.wikipedia.org/wiki?curid=23515723", "title": "Landscape limnology", "text": "Landscape limnology\n\nLandscape limnology is the spatially explicit study of lakes, streams, and wetlands as they interact with freshwater, terrestrial, and human landscapes to determine the effects of pattern on ecosystem processes across temporal and spatial scales. Limnology is the study of inland water bodies inclusive of rivers, lakes, and wetlands; landscape limnology seeks to integrate all of these ecosystem types.\n\nThe terrestrial component represents spatial hierarchies of landscape features that influence which materials, whether solutes or organisms, are transported to aquatic systems; aquatic connections represent how these materials are transported; and human activities reflect features that influence how these materials are transported as well as their quantity and temporal dynamics.\n\nThe core principles or themes of landscape ecology provide the foundation for landscape limnology. These ideas can be synthesized into a set of four landscape ecology themes that are broadly applicable to any aquatic ecosystem type, and that consider the unique features of such ecosystems. \n\nA landscape limnology framework begins with the premise of Thienemann (1925). Wiens (2002): freshwater ecosystems can be considered patches. As such, the location of these patches and their placement relative to other elements of the landscape is important to the ecosystems and their processes. Therefore, the four main themes of landscape limnology are:\n\n(1) Patch characteristics: The characteristics of a freshwater ecosystem include its physical morphometry, chemical, and biological features, as well as its boundaries. These boundaries are often more easily defined for aquatic ecosystems than for terrestrial ecosystems (e.g., shoreline, riparian zones, and emergent vegetation zone) and are often a focal-point for important ecosystem processes linking terrestrial and aquatic components.\n\n(2) Patch context: The freshwater ecosystem is embedded in a complex terrestrial mosaic (e.g., soils, geology, and land use/cover) that has been shown to drive many within-ecosystem features and processes such as water chemistry, species richness, and primary and secondary productivity. \n\n(3) Patch connectivity and directionality: The complex freshwater mosaic is connected to the particular patch of interest and defines the degree to which materials and organisms move across the landscape through freshwater connections. For freshwater ecosystems, these connections often display a strong directionality component that must be explicitly considered. For example, a specific wetland can be connected through groundwater to other wetlands or lakes, or through surface water connections directly to lakes and rivers, or both, and the directionality of those connections will strongly impact the movement of nutrients and biota.\n\n(4) Spatial scale and hierarchy: Interactions among terrestrial and freshwater elements occur at multiple spatial scales that must be considered hierarchically. The explicit integration of hierarchy into landscape limnology is important because (a) many freshwater ecosystems are hierarchically organized and controlled by processes that are hierarchically organized, (b) most freshwater ecosystems are managed at multiple spatial scales, from policy set at the national level, to land management conducted at local scales, and (c) the degree of homogeneity among freshwater ecosystems can change in relation to the scale of observation.\n\nFindings from landscape limnology research are contributing to many facets of aquatic ecosystem research, management, and conservation. Landscape limnology is especially relevant for geographical areas with thousands of ecosystems (i.e. lake-rich regions of the world), in situations with a range of human disturbances, or when considering lakes, streams, and wetlands that are connected to other such ecosystems. For example, landscape limnology perspectives have contributed to the development of nutrient criteria for lakes, formation of classification systems that can be used to monitor the health of aquatic ecosystems, understanding ecosystem responses to environmental stressors, or explaining biogeographic patterns of community composition.\n\n"}
{"id": "19058442", "url": "https://en.wikipedia.org/wiki?curid=19058442", "title": "Landscape mythology", "text": "Landscape mythology\n\nLandscape mythology and anthropology of landscape (\"Landschaftsmythologie\", \"Landschaftsethnologie\") are terms for a field of study advocated since about 1990 by Kurt Derungs (born 1962 in St. Gallen, Switzerland). Derungs describes the field as an interdisciplinary approach to landscape combining archaeology, ethnology and mythology. \n\nDerungs interprets landscape features in terms of \"totemism, shamanism and matriarchal mythology\", claiming that his approach qualifies as neither esotericism nor as positivism but as a \"sound alternative\" to both. His interpretations are strongly influenced by the hypothesis of a matriarchal structure of society and a cult of the Great Goddess in Neolithic Europe, and he associates megalithic monuments and elements of traditional fairy tales with these ideas.\n\nSince 1994, Derungs manages the \"edition amalia\" publishing house, where his books appear besides publications on related topics (matriarchy, Great Goddess) by other authors. Derungs is popular in German Neopagan circles, but has received little attention in academic literature.\n\n\n\n"}
{"id": "221054", "url": "https://en.wikipedia.org/wiki?curid=221054", "title": "Legal education", "text": "Legal education\n\nLegal education is the education of individuals in the principles, practices, and theory of law. It may be undertaken for several reasons, including to provide the knowledge and skills necessary for admission to legal practice in a particular jurisdiction, to provide a greater breadth of knowledge to those working in other professions such as politics or business, to provide current lawyers with advanced training or greater specialisation, or to update lawyers on recent developments in the law.\n\nLegal education can take the form of a variety of programs, including: \n\nEarly Western legal education emerged in Republican Rome. Initially those desiring to be advocates would train in schools of rhetoric. Around the third century BC Tiberius Coruncanius began teaching law as a separate discipline. His public legal instruction had the effect of creating a class of legally skilled non-priests (\"jurisprudentes\"), a sort of consultancy. After Coruncanius' death, instruction gradually became more formal, with the introduction of books on law beyond the then scant official Roman legal texts. It is possible that Coruncanius allowed members of the public and students to attend consultations with citizens in which he provided legal advice. These consultations were probably held outside the College of Pontiffs, and thus accessible to all those interested.\n\nCanon and ecclesiastical law were studied in universities in medieval Europe. However, institutions providing education in the domestic law of each country emerged later in the eighteen century.\n\nIn England, legal education emerged in the late thirteenth century through apprenticeships. The Inns of Court controlled admission to practice and also provided some legal training. English universities had taught Roman and canon law for some time, but formal degrees focused on the native common law did not emerge until the 1800s.\n\nIn many countries, including most of those in the Commonwealth of Nations, the principal law degree is an undergraduate degree, usually known as a Bachelor of Laws (LLB). Graduates of such a program are eligible to become lawyers by passing the country's equivalent of a bar exam. In these countries, graduate law programs are advanced degrees which allow for more in-depth study or specialisation.\n\nIn the United States, the primary law degree is a graduate degree known as the Juris Doctor (JD). Students may pursue such a degree only after completing an undergraduate degree, usually a bachelor's degree. The undergraduate degree can be in any field, though most American lawyers hold bachelor's degrees in the humanities and social sciences. American law schools are usually an autonomous entity within a larger university.\n\nPrimary degrees in law are offered by law schools, known in some countries as faculties of law. Law schools may have varying degrees of autonomy within a particular university or, in some countries, can be entirely independent of any other post-secondary educational institution.\n\nHigher degrees allow for more advanced academic study. These include the Masters of Law (LLM) by coursework or research, and doctoral degrees such as the PhD or SJD.\n\nPractitioners may undertake a Masters of Law by coursework to obtain greater specialisation in an area in which they practice. In many common law countries, a higher degree in law is common and expected for legal academics. In addition, incorporating practical skills is beneficial for practitioners seeking higher degrees to better prepare them in their respective legal area of practice.\n\nIn contrast, higher degrees in law are uncommon in the United States, even within the academy.\n\nIn some countries, including the United Kingdom, Italy, Germany, Canada and some states of Australia, the final stages of vocational legal education required to qualify to practice law are carried out outside the university system. The requirements for qualification as a barrister or as a solicitor are covered in those articles.\n\nLegal education providers in some countries offer courses which lead to a certificate or accreditation in applied legal practice or a particular specialisation.\n\nContinuing legal education (also known as continuing professional development) programs are informal seminars or short courses which provide legal practitioners with an opportunity to update their knowledge and skills throughout their legal career. In some jurisdictions, it is mandatory to undertake a certain amount of continuing legal education each year.\n\nIn Australia most universities offer law as an undergraduate-entry course (LLB, 4 years), or combined degree course (e.g., BSc/LLB, BCom/LLB, BA/LLB, BE/LLB, 5–6 years). Some of these also offer a three-year postgraduate Juris Doctor (JD) program. Bond University in Queensland runs three full semesters each year, teaching from mid-January to late December. This enables the Bond University Law Faculty to offer the LLB in the usual 8 semesters, but only 2 years. They also offer a JD in two years. The University of Technology, Sydney will from 2010 offer a 2-year accelerated JD program.\n\nIn 2008, the University of Melbourne introduced the Melbourne Model, whereby Law is only available as a graduate degree, with students having to have completed a three-year bachelor's degree (usually an Arts degree) before being eligible. Students in combined degree programs would spend the first 3 years completing their first bachelor's degree together with some preliminary law subjects, and then spend the last 2–3 years completing the law degree (JD). Alternatively, one can finish any bachelor's degree, and providing their academic results are high, apply for graduate-entry into a 3-year LLB program. Australian Law Schools include those at the University of New England, Australian Catholic University, Australian National University, La Trobe University, Flinders University, Bond University, Macquarie, Monash, Deakin, UNSW, University of Tasmania, Adelaide, Victoria University, Sydney, Melbourne, Queensland University of Technology, the University of Queensland, the University of Western Australia and the University of Canberra.\n\nThe professional law degree in Canada is the Bachelor of Laws (LL.B.) or Juris Doctor (J.D.), for common law jurisdictions, and the Bachelor of Laws, Licentiate of Law or Bachelor of Civil Law for Quebec, a civil law jurisdiction. Admittance to an LL.B. or J.D. program requires at least two years of undergraduate education, although, a completed undergraduate degree is usually required. In practice, the vast majority of those who are admitted have already earned at least an undergraduate (bachelor's) degree. The change in academic nomenclature redesignating the common law degree as a J.D. rather than an LL.B., currently completed or under consideration at a number of Canadian schools, has not affected the level of instruction—because it is the same degree. In the case of Quebec civil law degrees and the transsystemic LL.B/B.C.L. program at McGill University, students can be admitted after college.\n\nList of law schools in China\n\nIn Hong Kong law can be studied as a four-year undergraduate degree Bachelor of Laws (LLB), a two-year postgraduate degree (Juris Doctor), or the Common Professional Examination conversion course for non-law graduates. One must then pass the one-year Postgraduate Certificate in Laws (PCLL) currently offered at the University of Hong Kong (HKU), Chinese University of Hong Kong and City University of Hong Kong, before starting vocational training: a year's pupillage for barristers or a two-year training contract for solicitors.\n\nThe move to a four-year LLB was recent and, in the case of HKU, was aimed at shifting some of the more theoretical aspects of the HKU PCLL into the LLB, leaving more room for practical instruction.\n\nThe Bar Council of India prescribes and supervises standard of legal education in India. Law degrees in India are granted and conferred in terms of the Advocates Act, 1961, which is a law passed by the Parliament both on the aspect of legal education and also regulation of conduct of legal profession. Various regional universities or specialised national law universities offer Law graduate degrees through various law schools.\n\nIn India law can be studied, as LL.B. (Bachelor of Laws) or B.L. (Bachelor of Law), a three-year graduate degree after completion of Bachelor's degree. Alternatively after standard 12 one can join an integrated five-year law course which provides option to avail B.A. LL.B. or B.B.A. LLB. or B.Sc. LL.B. In India applied legal education for specific branches of law is also offered such as, Business law, Human resource and Labour laws, Property laws, Family laws, Human rights & Legal awareness, Taxation law and many more.\n\nLaw in Italy and France is studied in a jurisprudence school which is an entity within a larger university. Legal education can be started immediately after obtained a Diploma. Italian and French law schools are affiliated with public universities, and are thus public institutions. As a consequence, law schools are required to admit anyone holding the baccalaureate. However, the failure rate is extremely high (up to 70%) during the first two years of the \"licenza in diritto\". There are no vast disparities in the quality of Southern European law schools. Many schools focus on their respective city and region.\n\nThe law school program is divided following the European standards for university studies (Bologna process):\n\nThe first year of the master program (M1) is specialized : public law, private law, business law, European and international law, etc.\n\nThe second year of the master of law program (M2) can be work-oriented or research oriented (the students write a substantial thesis and can apply to doctoral programs, e.g., a PhD in Law).\n\nThe second year is competitive (entry is based on the student's grades and overall score and on extracurricular activities) and generally more specialized (IP law, contract law, civil liberties, etc.).\n\nStudents must pass a specific examination to enter bar school (CRFPA, école du barreau). They must successfully finish the first year of a Master of law (M1 or maitrise de droit) to be able to attend.\n\nIf they succeed, then after 18 months (school, practical aspects, ethics and internship) they then take the CAPA exam and diploma(Certificat d'Aptitude à la Profession d'Avocat). Successful students also take the Oath in order to practice law.\n\nThe Japanese Ministry of Justice opened the University of Tokyo Faculty of Law in 1877 (changed to Imperial University in 1886). To matriculate to the University of Tokyo, students had to finish ten to fifteen years of compulsory education; acceptance was therefore available to only a small elite. The law program produced politically-dependable graduates to fill fast-track administrative positions in government, also known as high civil servants (koto bunkan), and to serve as judges and prosecutors.\n\nPrivate law schools opened around 1880. These lacked the government funding given to the University of Tokyo, so the quality of education there lagged behind. Students only had to pass an examination to matriculate to private law schools, so many of them had not completed middle school. The private law schools produced a large portion of private attorneys because their graduates were often ineligible to apply for government positions.\n\nThe Imperial University Faculty of Law was given supervisory authority over many private law schools in 1887; by the 1920s, it promulgated a legal curriculum comprising six basic codes: Constitutional Law, Civil Law, Commercial Law, Civil Procedure, Criminal Law, and Criminal Procedure. The same basic structure survived in Japanese legal education to the end of the twentieth century.\n\nPrior to the implementation of the \"law school system\" in 2004, the legal education system was driven more by examinations than by formal schooling. The passage rate for the bar exam was historically around three percent, and nearly all those who sat for the exam took it several times. A number of specialized \"cram schools\" trained prospective lawyers for the exam, and these schools remain prevalent today. After passing the bar exam, prospective barristers were required to train for 16 months at the Legal Research and Training Institute of the Supreme Court of Japan. The training period has traditionally been devoted to litigation practice and virtually no training is given for other aspects of legal practice, e.g., contract drafting, legal research. During this period, the most \"capable trainees\" are \"selected out\" to become career judges; others may become prosecutors or private practitioners.\n\nIn 2004, the Japanese Diet passed a law allowing for the creation of graduate level law school that offer a J.D., or Hōmu Hakushi (法務博士). The 2006 bar examination was first in Japanese history to require a law school degree as a prerequisite. In the past, although there has been no educational requirement, most of those who passed the examination had earned undergraduate degrees from \"elite\" Japanese universities such as the University of Tokyo, Kyoto University or Hitotsubashi University. With this new law school system came a new bar exam, with a 40–50% passage rate which is capped by a numerical quota. Applicants are now limited to taking the exam three times in a five-year period. Despite the much higher bar passage rate with the new exam, due to the quotas, approximately half of Japanese law school graduates will never be admitted to practice. The new system also reduced the apprenticeship period at the Legal Research and Training Institute to one year.\n\nA number of other law-related professions exist in Japan, such as patent agents (\"benrishi\"), tax accountants (\"zeirishi\"), scriveners, etc., entry to each of which is governed by a separate examination. Attorneys (\"bengoshi\"), being qualified to practice any law, can automatically be qualified as patent agents and tax accountants with no additional examination, but not vice versa.\n\nLegal education in Korea is driven by examination. The profession of barristers, is highly regulated, and the pass rate for the bar exam is around five percent. Prospective attorneys who do pass the exam usually take it two or three times before passing it, and a number of specialized \"private educational institutes\" exist for prospective lawyers. After passing the bar exam, prospective barristers undergo a two-year training period at the Judicial Research and Training Institute of the Supreme Court of Korea. During this period, the most capable trainees are \"selected out\" to become career judges; others may become prosecutors or private practitioners.\n\nIn 2007, the Korean government passed a law allowing for the creation of three-year law schools (\"법학전문대학원\"). According to the new law, the old system of selecting lawyers by examination will be phased out by 2013 and the U.S.-style law schools will be the sole route to become a lawyer.\n\nIn February 2008, the Ministry of Education of Korea selected 25 universities to open law schools. The total enrollment for all law schools is capped at 2,000, which is a source of contention between the powerful Korea Bar Association, and citizen groups and school administrators. There is an uproar among the schools which failed to get the government's approval and even among the schools that did get the approval, there is dissatisfaction due to an extremely low enrollment number. Several law schools are permitted to enroll 40 students per year, which is far below the financially sustainable number. Beginning in 2012, passage of the Lawyer Admission Test (which is distinct from the old bar exam) will be required for qualification to practice.\n\nA number of other legal professions exist in Korea, such as patent attorneys (\"변리사\"), tax attorneys (\"세무사\"), solicitors(\"법무사\"), etc., entry to each of which is governed by a separate examination.\n\nAs a Commonwealth country, the Malaysian legal education system is rooted from the United Kingdom. Legal qualifications offered by the local law faculties require students to have a pre-university qualification such as the Malaysian Higher School Certificate, A-Level, International Baccalaureate, Foundation Course or a Diploma. Generally, the law degree programmes in Malaysia consist of civil law subjects, but there are institutions such as The National University of Malaysia, International Islamic University Malaysia and Universiti Sultan Zainal Abidin that include Sharia or Islamic law courses as requirements for admission and graduation.\n\nMalaysian law graduates from universities in the UK, Australia or New Zealand are allowed to practice law in Malaysia. However, they are required to obtain a Certificate of Legal Practice in Laws of Malaysia.\n\nLaw degree programs are considered graduate programs in the Philippines. As such, admission to law schools requires the completion of a bachelor's degree, with a sufficient number of credits or units in certain subject areas.\nLegal education in the Philippines is regulated and supervised by the LEGAL EDUCATION BOARD, a statutorily created independent Body chaired by a retired member of the Supreme Court or of the Court of Appeals. Its first chairman is Justice Hilarion Aquino. Sitting as members of the Board are a representative of the law professors, a representative of the law deans and a representative of the Commission on Higher Education. The membership of a student representative has been subject to continuing debate and resistance on the part of law schools.\nGraduation from a Philippine law school constitutes the primary eligibility requirement for the Philippine Bar Examinations, administered by the Supreme Court during the month of September every year.\n\nIn order to be eligible to take the bar examinations, one must complete either of the two professional degrees: The Bachelor of Laws (LL.B.) program or the Juris Doctor (J.D.) program. Advanced degrees are offered by some law schools, but are not requirements for admission to the practice of law in the Philippines. The degrees Master of Laws (LL.M.), Master of Legal Studies are available in only a handful of Philippine universities and colleges, among these San Beda College Graduate School of Law, the University of Santo Tomas and Ateneo de Manila University. The Doctor of Civil Law degree (DCL) is offered only by the University of Santo Tomas and the Doctor of Juridical Science (JSD) degree is offered by the San Beda College Graduate School of Law. Graduate programs in law are also regulated by the Legal Education Board\n\nLegal education in the Philippines normally proceeds along the following route:\n\nLaw degree – jurist (often compared to an LL.M., but in fact equivalent to the degree of Specialist specific to the Soviet educational system) is awarded in Russia and Ukraine after 5 years of study at a university. Jurist degree may also be awarded in a shorter period of time if a law student has already completed Bachelor or Specialist degree in another field of studies or has previously earned a basic law degree (comparable to Paralegal, an associate degree in U.S.) from a specialized law college. Bachelor jurist degree (equivalent to Bachelor of Laws (LL.B.)) may be earned concurrently with another bachelor's or master's degree in some universities (comparable to a double-major). Note that this fused, one-degree (Specialist) educational scheme has coexisted with the two-degree (bachelor's – master's) scheme since Russia and Ukraine launched their higher education reforms to bring the domestic educational systems in closer compliance with the Bologna accords. See also academic degree. The latest educational reforms created new system where a four-year law program is offered at the universities for earning bachelor's degree, and a five-year law program is offered for master's degree. The degree of Specialist is no longer awarded and is renamed into master's degree.\n\nTo become a lawyer in Serbia, students must graduate from an accredited faculty of law. Studies last for five years (ten semesters) in accordance to the Bologna Convention. To become a student of the faculty of law, a candidate must pass the admission test. Students are divided into full-time students and part-time students. The practical training for students is organized at courts of law, and local and international moot court competitions. A lawyer must pass the national bar examination to become an attorney, a judge, or a prosecutor.\n\nIn South Africa, the LL.B. is the universal legal qualification for admission and enrollment as an Advocate or Attorney. Since 1998, LL.B. programmes may be entered directly at the undergraduate level; at the same time, the LLB. continues to be offered postgraduate and may then be accelerated dependent on the bachelor's degree. The programme lasts between two and four years correspondingly (compare Australia, above). See Bachelor of Laws #South Africa.\n\nAlthough not formally required for specialised practice, further training, e.g. in tax, is usually via postgraduate diplomas or focused, coursework-based LL.M. programmes. Research degrees are the LL.M. and LL.D., or PhD depending on university. The Master's dissertation reflects an ability to conduct independent research, whereas the Doctoral thesis will, in addition, constitute an original contribution to the field of law in question. A doctorate, generally, is required for positions in legal academia. See Master of Laws #South Africa; Doctor of Law #South Africa.\n\nHistorically, the B.Proc. and B.Juris were the legal degrees offered at the undergraduate level. The four-year BProc qualified one to practice as an attorney, or become a prosecutor or magistrate in the lower courts, but did not allow for admission as an advocate. The three-year B.Juris was the basic requirement for prosecutors and magistrates in the lower courts, but on its own, did not qualify one to practice as an attorney. Both offered admission to the LLB.\n\nFor admission as an attorney, one serves \"articles\" as a candidate attorney with a practicing attorney for two years, and then writes a \"board exam\" set by the relevant provincial Law Society. See Attorneys in South Africa. The length of articles may be reduced by attending a practical legal training course or performing community service.\n\nAttorneys may additionally qualify as Notaries and Conveyancers, via the Conveyancing and Notarial Practice Examinations; those with technical or scientific training may further qualify as patent attorneys – see Patent attorney #South Africa.\n\nThe requirements to enter private practice as advocates (Junior Counsel) are to become members of a Bar Association by undergoing a period of training (pupilage) for one year with a practicing Advocate, and to sit an admission examination. On the recommendation of the Bar Councils, an advocate \"of proven experience and skill\" with at least ten years experience, may be appointed by the President of South Africa as a Senior Counsel (SC; also referred to as a \"silk\"). See Advocates in South Africa.\n\nThe Act regulating admission to practice law (\"The Qualifications of Legal Practitioners Amendment Act of 1997\") is being revised.\n\nThe law of South America is one of the most unified in the world. All countries can be said to follow civil law systems, although recent developments in the law of Brazil suggest a move towards the \"stare decisis\" doctrine. Moreover, all countries have recently signed up to the Union of South American Nations agreement, which aims to establish a system of supra-national law along the lines of the European Union.\n\nIn order to practice law in Sri Lanka, a lawyer must be 'admitted and enrolled as an Attorney-at-Law of the Supreme Court of the Democratic Socialist Republic of Sri Lanka. To be admitted to the bar a law student must complete law exams held by the Sri Lanka Law College and undergo a six-month period of apprenticeship under a senior practicing lawyer. There are two routes taken by students:\n\n\nBoth groups of students must undergo a period of apprenticeship under a senior practicing lawyer who has at least 8 years of practicing experience. To become a judge one must be admitted as an Attorney-at-Law.\n\nIn England and Wales, law can be studied as an undergraduate degree or in a Graduate Diploma in Law where students complete the Common Professional Examination. After obtaining the degree which is necessary to complete certain vocational courses and to serve a period of on the job training before one is able to qualify to practice as a barrister, legal executive, or solicitor. Bar Professional Training Course is regarded as one of the hardest degrees and presently it is the most expensive law-related degree.\n\nThe education of lawyers in the United States is generally undertaken through a law school program, although in some states (such as California and Virginia) applicants who have not attended law school may qualify to take the bar exam.\n\nLegal education in the United States normally proceeds along the following route:\n\nIn the United States,in most cases, the degree awarded by American law schools is the Doctor of Jurisprudence or Juris Doctor (J.D.), a Doctoral degree, the pursuit of which students undertake only after having completed an undergraduate degree in some other field (usually a bachelor's degree). The law school program is considered to be a professional school program and upon graduation you receive the distinct title of Doctor (although most states strictly regulate the ability of attorneys to style themselves \"doctor\").\n\nResearch degrees that are awarded include the Master of Laws (LL.M.), Doctor of Juridical Science degrees (J.S.D. or S.J.D.) and Doctor of Comparative Law (D.C.L.), are post-undergraduate and research and academic-based level degrees. In the U.S. the Legum Doctor (LL.D.) is only awarded as an honorary degree.\n\nA number of law students apply for an optional judicial clerkship (less than 10% end up in such position), to be taken after law school and before legal practice. Clerkships usually last one year with appellate courts, but trial level courts (including federal district court) are increasingly moving towards two-year clerkships.\n\nOnce a student has graduated from law school, he or she is expected to pursue admission to the bar in order to practice. Requirements for membership in the bar vary across the United States. In almost every state, the only way to be admitted to the bar is to pass a (usually multi-day) written examination. Once admitted, most States require attorneys to must meet certain Continuing Legal Education (CLE) requirements.\n\nAcademic degrees for non-lawyers are available at the baccalaureate and master's level. A common baccalaureate level degree is a Bachelor of Science in Legal Studies (B.S.). Academic master's degrees in legal studies are available, such as the Master of Studies (M.S.), and the Master of Professional Studies (M.P.S.). Such a degree is not required to enter a J.D. program.\n\nForeign lawyers seeking to practice in the U.S., who do not have a J.D., often seek to obtain a Master of Laws (LL.M.) (or other degrees similar to the LL.M., such as the Juris Master (J.M.), Master of Comparative Law (M.C.L.) and Master of Jurisprudence (M.J.)).\n"}
{"id": "493833", "url": "https://en.wikipedia.org/wiki?curid=493833", "title": "Light characteristic", "text": "Light characteristic\n\nA light characteristic is a graphic and text description of a navigational light sequence or colour displayed on a nautical chart or in a Light List with the chart symbol for a lighthouse, lightvessel, buoy or sea mark with a light on it. The graphic indicates how the real light may be identified when looking at its actual light output type or sequence. Different lights use different colours, frequencies and light patterns, so mariners can identify which light they are seeing.\n\nWhile light characteristics can be described in prose, e.g. \"Flashing white every three seconds\", lists of lights and navigation chart annotations use abbreviations. The abbreviation notation is slightly different from one light list to another, with dots added or removed, but it usually follows a pattern similar to the following (see the chart to the right for examples).\n\nAn example of a complete light characteristic is \"Gp Oc(3) W 10s 15m 10M\". This indicates that the light is a \"group occulting light\" in which a group of three eclipses repeat every 10 seconds; the light is white; the light is 15 metres above the chart datum and is visible for .\n\nA fixed light, abbreviated \"F\", is a continuous and steady light.\n\nA flashing light is a rhythmic light in which the total duration of the light in each period is clearly shorter than the total duration of the darkness and in which the flashes of light are all of equal duration. It is most commonly used for a single-flashing light which exhibits only single flashes which are repeated at regular intervals, in which case it is abbreviated simply as \"Fl\". It can also be used with a group of flashes which are regularly repeated, in which case the abbreviation is \"Fl.(2)\" or \"Gr Fl.(2)\", for a group of two flashes. Another possibility is a composite group, in which successive groups in the period have different numbers of flashes, e.g. \"Fl. (2+1)\" indicates a group of two flashes, followed by one flash.\n\nA specific case sometimes used is when the flashes are longer than two seconds. Such a light is sometimes denoted \"long flashing\" with the abbreviation \"L.Fl\".\n\nIf the frequency of flashes is large (more than 30 or 50 per minute) the light is denoted as a \"quick light\", see below.\n\nAn occulting light is a rhythmic light in which the duration of light in each period is longer than the total duration of darkness. In other words, it is the opposite to a flashing light where the total duration of darkness is longer than the duration of light. It has the appearance of flashing off, rather than flashing on. Like a flashing light, it can be used for a single occulting light that exhibits only a single period of darkness or the periods of darkness can be grouped and repeated at regular intervals (abbreviated \"Oc\"), a group (Oc (3)) or a composite group (Oc (2+1)).\n\nAn isophase light, abbreviated \"Iso\", is a light which has dark and light periods of equal length. The prefix derives from the Greek \"iso-\" meaning \"same\".\n\nA quick light, abbreviated \"Q\", is a special case of a flashing light with a large frequency (more than 30 or 50 per minute). If the sequence of flashes is interrupted by regularly repeated eclipses of constant and long duration, the light is denoted \"interrupted quick\", abbreviated \"I.Q\". \n\nGroup notation similar to flashing and occulting lights is also sometimes used (e.g. Q.(9)). \n\nAnother distinction sometimes made is between quick (more than 50 and less than 80 flashes per minute), very quick (more than 80 and less than 160 flashes per minutes, abbreviated \"V.Q\") and ultra quick (no less than 160 flashes per minute, abbreviate \"U.Q\"). This can be combined with notations for interruptions, e.g. I.U.Q. for interrupted ultra quick, or grouping, e.g. V.Q.(9) for a very quick group of nine flashes. Quick characteristics can also be followed by other characteristics, e.g. V.Q.(6)+L.Fl. for a very quick group of six flashes, followed by a long flash.\n\nA Morse code light is light in which appearances of light of two clearly different durations (dots and dashes) are grouped to represent a character or characters in the Morse Code. For example, \"Mo. (A)\" is a light in which in each period light is shown for a short period (dot) followed by a long period (dash), the Morse Code for \"A\".\n\nA fixed and flashing light, abbreviated \"F. Fl\", is a light in which a fixed low intensity light is combined with a flashing high intensity light.\n\nAn alternating light, abbreviated \"Al\", is a light which shows alternating colors. For example, \"Al WG\" shows white and green lights alternately.\n\n\n"}
{"id": "18410438", "url": "https://en.wikipedia.org/wiki?curid=18410438", "title": "List of countries by forest area", "text": "List of countries by forest area\n\nThis article is a list of places by forest area. Types of places listed include the entire planet, continents, regions, countries, provinces, states, and territories. Percentage data was calculated using information from the CIA's World Factbook 2011.\n\n\n\n"}
{"id": "27440422", "url": "https://en.wikipedia.org/wiki?curid=27440422", "title": "List of geographical societies", "text": "List of geographical societies\n\nThis is a list of geographical societies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "35865686", "url": "https://en.wikipedia.org/wiki?curid=35865686", "title": "List of international presidential trips made by François Hollande", "text": "List of international presidential trips made by François Hollande\n\nThis is a list of international presidential trips made by François Hollande, the 24th President of France. During his presidency, which began with his inauguration on 15 May 2012 and ended with the inauguration of Emmanuel Macron on 14 May 2017, François Hollande made 183 presidential trips to 83 states internationally. The number of visits per country where he travelled are:\n\nIn his first year in office, President François Hollande made 25 international trips to 18 different countries. The following were the international trips made by the President in 2012:\nIn 2013, President François Hollande made 34 international trips to 28 different countries. The following were the international trips made by President Hollande during the year:\n\nIn 2014, President François Hollande made 30 international trips to 27 different countries. The following were the international trips made by President Hollande during the year:\n\nIn 2015, President François Hollande made 45 international trips to 34 different countries. The following were the international trips made by President Hollande during the year:\n\nIn 2016, President François Hollande made 37 international trips to 30 different countries. The following were the international trips already made by President Hollande during the year:\nIn 2017, President François Hollande made thirteen international trips to thirteen different countries. The following are the trips made by President Hollande in 2017:\n\n"}
{"id": "43807462", "url": "https://en.wikipedia.org/wiki?curid=43807462", "title": "List of international prime ministerial trips made by Narendra Modi", "text": "List of international prime ministerial trips made by Narendra Modi\n\nThe following is a list of international prime ministerial trips made by Narendra Modi since he became the Prime Minister of India following the Indian general election, 2014.\nAs of , Narendra Modi has made 41 foreign trips on six continents, visiting 59 countries including the visits to USA to attend UN general assembly, to Asian countries, following his \"neighbourhood first\" and \"act east\" policies.\n\nPotential upcoming foreign visits for Prime Minister Narendra Modi.\n\n\n"}
{"id": "51104499", "url": "https://en.wikipedia.org/wiki?curid=51104499", "title": "List of international prime ministerial trips made by Theresa May", "text": "List of international prime ministerial trips made by Theresa May\n\nThis is a list of international prime ministerial trips made by Theresa May, the current Prime Minister of the United Kingdom. As of , Theresa May has made 60 trips to 32 countries since her premiership began on 13 July 2016.\n\nThe number of visits per country: \n\nThe following international trips are scheduled to be made by Theresa May during 2018:\n\nThe following international trips are scheduled to be made by Theresa May during 2019:\nTheresa May participated in the following summits during her premiership:\n\n"}
{"id": "18596887", "url": "https://en.wikipedia.org/wiki?curid=18596887", "title": "List of political and geographic subdivisions by total area from 200,000 to 500,000 square kilometers", "text": "List of political and geographic subdivisions by total area from 200,000 to 500,000 square kilometers\n"}
{"id": "11485939", "url": "https://en.wikipedia.org/wiki?curid=11485939", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: G", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: G\n\n\n"}
{"id": "11486064", "url": "https://en.wikipedia.org/wiki?curid=11486064", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: L", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: L\n\n\n"}
{"id": "9895080", "url": "https://en.wikipedia.org/wiki?curid=9895080", "title": "Local Notice to Mariners", "text": "Local Notice to Mariners\n\nA Local Notice to Mariners is an authoritative instruction issued by a designated official, typically the harbormaster.\n\nIn the United States, notices are issued by each U.S. Coast Guard District to disseminate important information affecting navigational safety within that District. This Notice reports changes and deficiencies in aids to navigation maintained by the Coast Guard. Other marine information such as new charts, channel depths, naval operations, and regattas is included. Since temporary information of short duration is not included in the weekly Notice to Mariners, the \"Local Notice to Mariners\" may be the only source of such information. Small craft using the Intracoastal Waterway and small harbors not normally used by oceangoing vessels need it to keep charts and publications up-to-date.\n\nSince correcting information for U.S. charts in the \"Notice to Mariners\" is obtained from the Coast Guard Local Notices, it is normal to expect a lag of 1 or 2 weeks for the \"Notice to Mariners\" to publish a correction from this source.\n\nThe \"Local Notice to Mariners\" may be obtained free of charge by contacting the appropriate Coast Guard District Commander. Vessels operating in ports and waterways in several districts must obtain the \"Local Notice to Mariners\" from each district. \n\nThe text of the US material originated from section 422 of The American Practical Navigator, a document produced by the government of the United States of America.\n\nLNTMs are, by definition, concerned with local issues. Each issuing authority has its own series of LNTMs – there is no international standard numbering or indexing scheme. Individual LNTMs may concern short or long term situations. At Portsmouth, mariners are instructed by LNTM 28/07 to \"keep clear of warship berths\". At Holyhead, LNTM 5/2008 concerns \"Bunkering or transferring oil in port\". At Chichester, No 16 of 2008 \"Sea defence works at Hayling Island\" gives timely information about dredging operations.\n\n\n"}
{"id": "23077283", "url": "https://en.wikipedia.org/wiki?curid=23077283", "title": "Long baseline acoustic positioning system", "text": "Long baseline acoustic positioning system\n\nA long baseline (LBL) acoustic positioning system is one of three broad classes of underwater acoustic positioning systems that are used to track underwater vehicles and divers. The other two classes are ultra short baseline systems (USBL) and short baseline systems (SBL). LBL systems are unique in that they use networks of sea-floor mounted baseline transponders as reference points for navigation. These are generally deployed around the perimeter of a work site. The LBL technique results in very high positioning accuracy and position stability that is independent of water depth. It is generally better than 1-meter and can reach a few centimeters accuracy. LBL systems are generally employed for precision underwater survey work where the accuracy or position stability of ship-based (SBL, USBL) positioning systems does not suffice.\n\nLong baseline systems determine the position of a vehicle or diver by acoustically measuring the distance from a vehicle or diver interrogator to three or more seafloor deployed baseline transponders. These range measurements, which are often supplemented by depth data from pressure sensors on the devices, are then used to triangulate the position of the vehicle or diver. In figure 1, a diver mounted interrogator (A) sends a signal, which is received by the baseline transponders (B, C, D). The transponders reply, and the replies are received again by the diver station (A). Signal run time measurements now yield the distances A-B, A-C and A-D, which are used to compute the diver position by triangulation or position search algorithms. The resulting positions are relative to the location of the baseline transducers. These can be readily converted to a geo-referenced coordinate system such as latitude/longitude or UTM if the geo-positions of the baseline stations are first established.\n\nLong baseline systems get their name from the fact that the spacing of the baseline transponders is long or similar to the distance between the diver or vehicle and the transponders. That is, the baseline transponders are typically mounted in the corners of an underwater work site within which the vehicle or diver operates. This method yields an ideal geometry for positioning, in which any given error in acoustic range measurements produce only about an equivalent position error. This compares to SBL and USBL systems with shorter baselines where ranging disturbances of a given amount can result in much larger position errors. Further, the mounting of the baseline transponders on the sea floor eliminates the need for converting between reference frames, as is the case for USBL or SBL positioning systems mounted on moving vessels. Finally, sea floor mounting makes the positioning accuracy independent of water depth. For these reasons LBL systems are generally applied to tasks where the required standard of positioning accuracy or reliability exceeds the capabilities of USBL and SBL systems.\n\nThe search and inspection of the lost nuclear submarine USS Thresher by the U.S. Navy oceanographic vessel USNS Mizar in 1963 is frequently credited as the origin of modern underwater acoustic navigation systems. Mizar primarily used a short baseline (SBL) system to track the bathyscaphe Trieste 1. However, its capability also included seafloor transponders, which in conjunction with early navigation satellites supported station-keeping with a precision of about 300 feet, considered remarkable at the time.\n\nBy the mid-1960s and possibly earlier, the Soviets were developing underwater navigation systems including seafloor transponders to allow nuclear submarines to operate precisely while staying submerged. Besides navigating through canyons and other difficult underwater terrain, there was also a need to establish the position of the submarine prior to the launch of a nuclear missile (ICBM). In 1981, acoustic positioning was proposed as part of the U.S. military's MX missile system. A network of 150 covert transponder fields was envisioned. Submarines typically are guided by inertial navigation systems, but these dead reckoning systems develop position drift which must be corrected by occasional position fixes from a GNSS system. If the enemy were to knock out the GNSS satellites, the submarine could rely on the covert transponder network to establish its position and program the missile's own inertial navigation system for launch.\n"}
{"id": "58642133", "url": "https://en.wikipedia.org/wiki?curid=58642133", "title": "Museum of Lands, Mapping and Surveying", "text": "Museum of Lands, Mapping and Surveying\n\nThe Museum of Lands, Mapping and Surveying is a museum at 317 Edward Street, Brisbane, Queensland, Australia. It collects and exhibits material relating to the surveying of Queensland and the maps created. It is a sub-branch of the Queensland Museum. It actively digitises and makes available historic maps and aerial imagery under open licences.\n\nLand exploration and surveying has played a crucial role in the development of Queensland. Surveyors and cartographers played an active part in establishing Queensland's borders, physically surveying the land as well as creating maps, survey plans and registers.\n\nThe historical input of the surveying and mapping industry to the development of Queensland is an important factor in the role the department plays in providing a valuable resource for a wide range of community interests and government.\n\nIn its short history the museum has established a comprehensive collection, housing many important maps, survey plans and artefacts, significant to the history of mapping, surveying and land development particularly in Queensland.\n\nThe museum acquires, preserves, catalogues and displays information and artefacts pertaining to historical land settlement, surveying and cartographic activities in Queensland.\n\nThe museum aims to identify records, artefacts, maps and plans that are useful in filling gaps in the knowledge about the history of mapping and surveying in Queensland, and make such knowledge accessible. New material is constantly being identified as important for research purposes.\n\nOur physical and digital collections include:\n\n\nIn 1982, the Queensland Government through the Surveyor General of Queensland, JM Serisier ordered that a mapping and surveying museum be set up in order to preserve the mapping and surveying history of Queensland. The original museum was established in the Lands Administration Building, (now the Conrad Treasury Hotel), and Bill Kitson was tasked with establishing and operating the museum.\n\nWith the support of the department and the mapping and surveying industry Kitson set about collecting artefacts, which were rapidly disappearing as technology overtook the traditional means of creating maps and surveys. The museum’s collection grew from simply holding artefacts to include diaries, artwork, photographs and biographies which captured the personal and social history of those involved in surveying and mapping. Many historical maps of Queensland were also acquired.\n\nIn 1987, the museum moved to the newly constructed Landcentre building in Woolloongabba.\n\nIn 1988, in order to preserve the artefacts that had been collected an agreement was signed with the Queensland Museum to become custodian of the collections whilst the government provides staff to manage and run the museum.\n\nIn 1991, the focus of the museum was broadened and it was renamed the Museum of Lands, Mapping and Surveying to include history of land development in Queensland.\n\nIn 2018, the museum relocated to 317 Edward Street in the Brisbane CBD.\n\n\nThis article was based on material from Collections and Services and Museum history published by The State of Queensland 2018 under CC-BY-4.0 license, accessed on 3 October 2018.\n\n"}
{"id": "7693731", "url": "https://en.wikipedia.org/wiki?curid=7693731", "title": "NavPix", "text": "NavPix\n\nNavPix is the proprietary name applied by Navman to its technology that combines an image with geographical data.\n\nThe \"NavPix\" name is used for both the software and the geo-referenced image that results from that software.\n\nThe NavPix technology enables users to take a JPEG image using the integrated digital camera on the N Series (\"N\" for NavPix), iCN 720 or iCN 750 portable Navman GPS navigation devices.\n\nThe Navman's GPS (Global Positioning System) receiver determines the latitude and longitude of where that image was taken. That information is then written into the image's Exif (Exchangeable image file format) meta data by the NavPix software. The NavPix, therefore, effectively provides a Georeference of the location where the image was taken, which is not necessarily the same georeference as the object being \"NavPix-ed\".\n\nThe NavPix image can then be used to define a destination or point of interest on compatible Navman devices.\n\nFurthermore, as the geographical information is written to the meta data, the image itself can be shared between compatible devices or uploaded to Navman's NavPix Library which offers a wide range of NavPix images that have been taken by both Navman users and sourced from professional photo providers, including Lonely Planet.\n\nThe NavPix Library also enables people to upload non-NavPix images (including other formats such as GIF) and convert them to NavPix images by using entering either the latitude and longitude they want to associate with the image or by entering the address and using the Library's software to generate the latitude and longitude values based on a Postal code look-up.\n\nUnlike some geo-referencing applications, the NavPix Library writes the georeference values to the image itself via the Exif meta data.\n\nThe photo taking abilities do not help navigation.\n\n\n"}
{"id": "1918784", "url": "https://en.wikipedia.org/wiki?curid=1918784", "title": "Navigational instrument", "text": "Navigational instrument\n\nNavigational instruments refers to the instruments used by nautical navigators and pilots as tools of their trade. The purpose of navigation is to ascertain the present position and to determine the speed, direction etc. to arrive at the port or point of destination.\n\n\n\nThese instruments are used primarily to measure the elevation or altitude of a celestial object:\n\n\nThese instruments are also used to measure the angular distance between objects:\n\n\n\n\n\nAll those mentioned were the traditional instruments used until well into the second half of the 20th century. After World War II electronic aids to navigation developed very rapidly and, to a great extent, replaced more traditional tools. Electronic speed and depth finders have totally replaced their older counterparts. Radar has become widespread even in small boats. Some Electronic aids to navigation like LORAN have already become obsolete themselves and have been replaced by GPS.\n\n"}
{"id": "28252313", "url": "https://en.wikipedia.org/wiki?curid=28252313", "title": "Newbridge, County Londonderry", "text": "Newbridge, County Londonderry\n\nNewbridge is a small townland in south County Londonderry, Northern Ireland. Stretching from Toomebridge in County Antrim, to Bellaghy, Castledawson, Magherafelt, and Ballymaguigan, it latter which shares the parish of Ardtrea North, St. Trea's.\n\nNewbridge is the home to Anahorish Primary School and to Sean O'Leary's Newbridge GAC.\n"}
{"id": "6755999", "url": "https://en.wikipedia.org/wiki?curid=6755999", "title": "Normalnull", "text": "Normalnull\n\nNormalnull (\"standard zero\") or Normal-Null (short N. N. or NN ) is an outdated official vertical datum used in Germany. Elevations using this reference system were to be marked \"Meter über Normal-Null\" (“meters above standard zero”). Normalnull has been replaced by Normalhöhennull (short NHN).\n\nIn 1878 reference heights were taken from the Amsterdam Ordnance Datum and transferred to the New Berlin Observatory in order to define the Normalhöhenpunkt 1879. Normalnull has been defined as a level going through an imaginary point 37.000 m below \"Normalhöhenpunkt 1879\". When the New Berlin Observatory was demolished in 1912 the reference point was moved east to the village of Hoppegarten (now part of the town of Müncheberg, Brandenburg, Germany).\n"}
{"id": "23266725", "url": "https://en.wikipedia.org/wiki?curid=23266725", "title": "North Atlantic Track Agreement", "text": "North Atlantic Track Agreement\n\nThe North Atlantic Track Agreement was an agreement in November 1898 among thirteen passenger steamship companies to use a set series of trans-Atlantic routes that stretched from the northeast of North America to western Europe for the Atlantic crossing. Following the tracks was recommended but not compulsory.\n\nThere were seven routes: three to Canada and four to New York and Boston. The two main routes are apart to prevent collisions.\n\nThe agreement was given government recognition in the 1948 Safety-at-Sea-Convention.\n"}
{"id": "2554504", "url": "https://en.wikipedia.org/wiki?curid=2554504", "title": "Ordnance datum", "text": "Ordnance datum\n\nIn the British Isles, an ordnance datum or OD is a vertical datum used by an ordnance survey as the basis for deriving altitudes on maps. A spot height may be expressed as AOD for \"above ordnance datum\". Usually mean sea level (MSL) is used for the datum. In particular:\n\n\nTunnel datum is a datum based on an ordnance datum and used in designing tunnels which pass below sea level.\n\n"}
{"id": "5886597", "url": "https://en.wikipedia.org/wiki?curid=5886597", "title": "Philosophy of geography", "text": "Philosophy of geography\n\nPhilosophy of geography is the subfield of philosophy which deals with epistemological, metaphysical, and axiological issues in geography, with geographic methodology in general, and with more broadly related issues such as the perception and representation of space and place.\n\nThough methodological issues concerning geographical knowledge have been debated for centuries, Richard Hartshorne (1899–1992) is often credited with its first major systematic treatment in English, \"The Nature of Geography: A Critical Survey of Current Thought in the Light of the Past\", which appeared in 1939, and which prompted several volumes of critical essays in subsequent decades. John Kirtland Wright (1891–1969), an American geographer notable for his cartography and study of the history of geographical thought, coined the related term geosophy in 1947, for this kind of broad study of geographical knowledge. Other books oft-cited as key works in the field include David Harvey's 1969 \"Explanation in Geography\" and Henri Lefebvre's 1974 The Production of Space. It was a discussion of issues raised by the latter which in part inspired the founding of a \"Society for Philosophy and Geography\" in the 1990s.\n\nThe \"Society for Philosophy and Geography\" was founded in 1997 by Andrew Light, a philosopher currently at George Mason University, and Jonathan Smith a geographer at Texas A&M University. Three volumes of an annual peer-reviewed journal, \"Philosophy and Geography,\" were published by Rowman & Littlefield Press which later became a bi-annual journal published by Carfax publishers. This journal merged with another journal started by geographers, \"Ethics, Place, and Environment,\" in 2005 to become \"Ethics, Place, and Environment: A journal of philosophy and geography\" published by Routledge. The journal was edited by Light and Smith up to 2009, and has published work by philosophers, geographers, and others in allied fields, on questions of space, place, and the environment broadly construed. It has come to be recognized as instrumental in expanding the scope of the field of environmental ethics to include work on urban environments.\n\nIn 2009 Smith retired from the journal and Benjamin Hale from the University of Colorado came on as the new co-editor. Hale and Light relaunched the journal in January 2011 as Ethics, Policy, and Environment. While the journal has since focused more on the relationship between environmental ethics and policy, it still welcomes submissions on relevant work from geographers.\n\nA book series, also initially published by Rowman & Littlefield, and later by Cambridge Scholars Press, began in 2002 to publish the transactions of the \"Society for Philosophy and Geography\"'s annual meetings, organized by Gary Backhaus and John Murungi of Towson University. In 2005 the society sponsoring these annual meetings became the \"International Association for the Study of Environment, Space, and Place\", and in 2009 the book series gave way to a peer-reviewed journal, \"Environment, Space, Place\", published semiannually and currently edited by C. Patrick Heidkamp, Troy Paddock, and Christine Petto of Southern Connecticut State University.\n\n\n"}
{"id": "57879883", "url": "https://en.wikipedia.org/wiki?curid=57879883", "title": "Physical Geography (journal)", "text": "Physical Geography (journal)\n\nPhysical Geography is a bimonthly peer-reviewed scientific journal covering all aspects of physical geography. It was established in 1980 and is published by Taylor & Francis. It was originally published by Bellwether Publishing until the start of the 34th volume in 2013, when it moved to Taylor & Francis. The editor-in-chief is Carol Harden (University of Tennessee). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.086.\n"}
{"id": "10943769", "url": "https://en.wikipedia.org/wiki?curid=10943769", "title": "Place identity", "text": "Place identity\n\nPlace identity or place-based identity refers to a cluster of ideas about place and identity in the fields of geography, urban planning, urban design, landscape architecture, environmental psychology, ecocriticism and urban sociology/ecological sociology. Place identity is sometimes called urban character, neighbourhood character or local character. Place identity has become a significant issue in the last 25 years in urban planning and design. Place identity concerns the meaning and significance of places for their inhabitants and users, and how these meanings contribute to individuals' conceptualizations of self. Place identity also relates to the context of modernity, history and the politics of representation. In other words, historical determinism, which intersects historical events, social spaces and groups by gender, class, ethnicity. In this way, it explores how spaces have evolved over time by exploring the social constructs through time and the development of space, place and power. To the same extent, the politics of representation is brought into context, as the making of place identity in a community also relates to the exclusion or inclusion in a community. Through this, some have argued that place identity has become an area for social change because it gives marginalized communities agency over their own spaces. In the same respect, it is argued that place identity has also been used to intervene social change and perpetuate oppression from a top-down approach by creating segregated spaces for marginalized communities.\n\nIn some ways it is related to the concepts of place attachment and sense of place. Place identity is largely related to the concepts of community formation because it recognizes that geographical spaces do not solely bond a community together but rather there are social bonds that account for community formation. Those social forces often are feelings of belonging and security, which involve theoretical formations of community. Theoretical formations of community, which were identified in \"Community: Seeking Safety in an Insecure World\" (Bauman, 2001) as bonds formed by similar locality, culture, language, kinship and/or experiences. In addition, identity also conceives feelings of security and freedom as one is able to self-identify and especially, when it comes to being able to foster agency over community formation. In addition, the similar and shared experiences of culture, language and locality foster the sense of community. This fostering of community is largely seen as an extension of agency because when a community is able to achieve a sense of place and place attachment, this allows for individuals to reinforce their own identities and strengthen their bonds within their community.\n\nMethodologies for understanding place identity primarily involve qualitative techniques, such as interviewing, participant observation, discourse analysis and mapping a range of physical elements. Some urban planners, urban designers and landscape architects use forms of deliberative planning, design charettes and participatory design with local communities as a way of working with place identity to transform existing places as well as create new ones. This kind of planning and design process is sometimes referred to as placemaking.\n\nThe following case studies are examples of how place identity is researched on the field.\n\nIn a study by Lee Cuba and David M. Hummon (1993), they focus on Cape Cod, Massachusetts residents and how social and environmental factors are associated with place identity. Place identity in regards to \"at-homeness\" was defined by existence, affiliations, and locus. Community members were asked if they feel at home in Cape Cod to measure the positive responses for existence. The open-ended responses to why community members feel at home were used to measure place affiliation. A close-ended question, \"Do you associate feeling at home with living in this particular house or apartment, with living in this community, or with living on the Cape, in general?\" was used to measure locus. Most respondents reported they did feel \"at home\".\n\nMichigan and the Great Lakes are analyzed to see the values and connections shared within the residents of Michigan. A questionnaire was given to Michigan residents to see how attached the residents are. The questionnaire consisted of statements and the statements were evaluated through the five-point Likert scale. As a result, the data revealed \"Michigan's voters have developed a strong sense of place regarding the state\".\n\nThese two case studies shows that place has a lot more to offer than just a physical location. Understanding how to measure a sense of place assist policy makers in decision making and creating potential policy implementation. They will take the community's issues into consideration during the planning process once they understand the values of a community.\n\nCuba, L. & Hummon, D.M. (1993). A place to call home: Identification with dwelling, community, and region. The Sociological Quarterly, 34 (1), 111-131.\n\nHague, C. and Jenkins, P. (Eds) (2005). Place identity, planning and participation, London ; New York : Routledge, 2005. (hard cover) 0415262429 (soft cover) 0203646754 (ebook)\n\nProshansky, H. M. (1978). 'The city and self-identity', Journal of Environment and Behaviour, Vol. 10, pp. 57–83\n\nNanzer, B. (2004). Measuring sense of place: A scale of Michigan. Administrative Theory & Praxis, 26 (3), 362-382.\n\nProshansky, H. M., Fabian, A. K. and Kaminoff, R. (1983). 'Place-identity: Physical world socialization of the self', Journal of Environmental Psychology, Vol. 3, pp. 57–83\n\nRelph, E (1976) Place and placelessness. London: Pion, 1976 ()\n\nRoudavski, Stanislav (2008). \"Staging Places as Performances: Creative Strategies for Architecture\" (PhD, University of Cambridge)\n"}
{"id": "949357", "url": "https://en.wikipedia.org/wiki?curid=949357", "title": "Polynya", "text": "Polynya\n\nA polynya is an area of open water surrounded by sea ice. It is now used as geographical term for an area of unfrozen sea within the ice pack. It is a loanword from (polynya) , which refers to a natural ice hole, and was adopted in the 19th century by polar explorers to describe navigable portions of the sea. In past decades, for example, some polynyas, such as the Weddell Polynya, have lasted over multiple winters (1974–1976).\n\nPolynyas are formed through two main processes:\n\n\nLatent heat polynyas are regions of high ice production and therefore are possible sites of dense water production in both polar regions. The high ice production rates within these polynyas leads to a large amount of brine rejection into the surface waters. This salty water then sinks. It is an open question as to whether the polynyas of the Arctic can produce enough dense water to form a major portion of the dense water required to drive the thermohaline circulation.\n\nSome polynyas, such as the North Water Polynya between Canada and Greenland occur seasonally at the same time and place each year. Because animals can adapt their life strategies to this regularity, these types of polynyas are of special ecological research significance. In winter, marine mammals such as walruses, narwhals and belugas that do not migrate south, remain there. In spring, the thin or absent ice cover allows light in, through the surface layer as soon as the winter night ends, which triggers the early blooming of microalgae that are at the basis of the marine food chain. So, polynyas are suspected to be places where intense and early production of the planktonic herbivores ensure the transfer of solar energy (food chain) fixed by planktonic microalgae to Arctic cod, seals, whales, and polar bears. Polar bears are known to be able to swim as far as 65 kilometres across open waters of a polynya.\n\nThe presence of polynyas in McMurdo Sound in the Antarctic provides an ice-free area where penguins can feed, thus they are important for the survival of the Cape Royds penguin colony.\n\nAntarctic Bottom Water is the dense water with high salinity that exists in the abyssal layer of the Southern Ocean and it plays a major role in the global overturning circulation. Coastal Polynyas (Latent heat polynyas) are a source of AABW as brine rejection during the formation of sea ice at these polynyas increase the salinity of the sea water which then sinks down to the ocean bottom as AABW. Antarctic polynyas form when ice masses diverge from the coast and move away in the direction of the wind; creating an exposed area of sea water which subsequently freezes over, with brine rejection, to form another mass of ice.\n\nWhen submarines of the U.S. Navy made expeditions to the North Pole in the 1950s and 60s, there was a significant concern about surfacing through the thick pack ice of the Arctic Ocean. In 1962, both the USS \"Skate\" and USS \"Seadragon\" surfaced within the same, large polynya near the North Pole, for the first polar rendezvous of the U.S. Atlantic Fleet and the U.S. Pacific Fleet.\n\n\n"}
{"id": "48134895", "url": "https://en.wikipedia.org/wiki?curid=48134895", "title": "Primary direction", "text": "Primary direction\n\nPrimary direction is a term in astronomy for the reference meridian used in a celestial coordinate system for that system's longitude.\n\n"}
{"id": "17660060", "url": "https://en.wikipedia.org/wiki?curid=17660060", "title": "RINEX", "text": "RINEX\n\nIn the field of geodesy, Receiver Independent Exchange Format (RINEX) is a data interchange format for raw satellite navigation system data. This allows the user to post-process the received data to produce a more accurate result — usually with other data unknown to the original receiver, such as better models of the atmospheric conditions at time of measurement.\n\nThe final output of a navigation receiver is usually its position, speed or other related physical quantities. However, the calculation of these quantities are based on a series of measurements from one or more satellite constellations. Although receivers calculate positions in real time, in many cases it is interesting to store intermediate measures for later use. RINEX is the standard format that allows the management and disposal of the measures generated by a receiver, as well as their off-line processing by a multitude of applications, whatever the manufacturer of both the receiver and the computer application.\n\nThe RINEX format is designed to evolve over time, adapting to new types of measurements and new satellite navigation systems. The first RINEX version was published by W. Gurtner and G. Mader in the CSTG GPS Bulletin of September/October 1990. Since 1993 the RINEX 2 is available, which has been revised and adopted several times. RINEX enables storage of measurements of pseudorange, carrier-phase, Doppler and signal-to-noise from GPS (including GPS modernization signals e.g. L5 and L2C), GLONASS, Galileo, Beidou, along with data from EGNOS and WAAS satellite based augmentation systems (SBAS), QZSS, simultaneously. RINEX version 3.02 was submitted in April 2013 and is capable of new measurements from GPS or Galileo systems. The most recent version is RINEX 3.03 from July 2015 with update 1 to 3.03 published in 2017.\n\nAlthough not part of the RINEX format, the \"Hatanaka compression scheme \" is commonly used to reduced the size of RINEX files, resulting in an ASCII-based CompactRINEX format.. It uses higher-order time differences to reduce the number of characters needed to store time data.\n\n"}
{"id": "711230", "url": "https://en.wikipedia.org/wiki?curid=711230", "title": "Regional geography", "text": "Regional geography\n\nRegional geography is a major branch of geography. It focuses on the interaction of different cultural and natural geofactors in a specific land or landscape, while its counterpart, systematic geography, concentrates on a specific geofactor at the global level. \n\nAttention is paid to unique characteristics of a particular region such as natural elements, human elements, and regionalization which covers the techniques of delineating space into regions. Rooted in the tradition of the German speaking countries, the two pillars of regional geography are the idiographic study of \"Länder\" or spatial individuals (specific places, countries, continents) and the typological study of \"Landschaften\" or spatial types (landscapes such as coastal regions, mountain regions, border regions, etc.).\n\nRegional geography is also a certain approach to geographical study, comparable to quantitative geography or critical geography. This approach prevailed during the second half of the 19th century and the first half of the 20th century, a period when then regional geography paradigm was central within the geographical sciences. It was later criticised for its descriptiveness and the lack of theory. Strong criticism was leveled against it in particular during the 1950s and the quantitative revolution. Main critics were G. H. T. Kimble and Fred K. Schaefer. The regional geography paradigm has influenced many other geographical sciences, including economic geography and geomorphology. Regional geography is still taught in some universities as a study of the major regions of the world, such as Northern and Latin America, Europe, and Asia and their countries. In addition, the notion of a city-region approach to the study of geography, underlining urban-rural interactions, gained credence since the mid-1980s. Some geographers have also attempted to reintroduce a certain amount of regionalism since the 1980s. This involves a complex definition of regions and their interactions with other scales.\n\nNotable figures in regional geography were Alfred Hettner in Germany, with his concept of chorology; Paul Vidal de la Blache in France, with the possibilism approach (possibilism being a softer notion than environmental determinism); and, in the United States, Richard Hartshorne with his concept of areal differentiation. The school of Carl O. Sauer, strongly influenced by Alfred Hettner and Paul Vidal de la Blache, is also seen as regional geography in its broadest sense.\n\n"}
{"id": "5284746", "url": "https://en.wikipedia.org/wiki?curid=5284746", "title": "Sustainable city", "text": "Sustainable city\n\nSustainable cities, urban sustainability, or eco-city (also \"ecocity\") is a city designed with consideration for social, economic, environmental impact , and resilient habitat for existing populations, without compromising the ability of future generations to experience the same. These cities are inhabited by people whom are dedicated towards minimization of required inputs of energy, water, food, waste, output of heat, air pollution - CO, methane, and water pollution. Richard Register first coined the term \"ecocity\" in his 1987 book, \"Eco city Berkeley: Building Cities for a Healthy Future\". Other leading figures who envisioned the sustainable city are architect Paul F Downton, who later founded the company Ecopolis Pty Ltd, as well as authors Timothy Beatley and Steffen Lehmann, who have written extensively on the subject. The field of industrial ecology is sometimes used in planning these cities.\n\nThere remains no completely agreed upon definition for what a sustainable city should be or completely agreed upon paradigm for what components should be included. Generally, developmental experts agree that a sustainable city should meet the needs of the present without sacrificing the ability of future generations to meet their own needs. The ambiguity within this idea leads to a great deal of variation in terms of how cities carry out their attempts to become sustainable.\n\nIdeally, a sustainable city creates an enduring way of life across the four domains of ecology, economics, politics and culture. However, minimally a sustainable city should firstly be able to feed itself with a sustainable reliance on the surrounding countryside. Secondly, it should be able to power itself with renewable sources of energy. The core of this is to create the smallest conceivable ecological footprint, while producing the lowest quantity of pollution achievable. All while efficiently using the land; composting used materials, and recycling or converting waste-to-energy. All of these contributions will lead to the city's overall impacts on climate change to be minimal and with as little impact. The Adelaide City Council states that socially sustainable cities should be equitable, diverse, connected, and democratic and provide a good quality of life. \n\nA sustainable city can feed itself with minimal reliance on the surrounding countryside, and power itself with renewable sources of energy. The crux of this is to create the smallest possible ecological footprint, and to produce the lowest quantity of pollution possible, to efficiently use land; compost used materials, recycle it or convert waste-to-energy, and thus the city's overall contribution to climate change will be minimal, if such practices are adhered to. \n\nIt is estimated that over 50% of the world’s population now lives in cities and urban areas. These large communities provide both challenges and opportunities for environmentally-conscious developers. There are distinct advantages to further defining and working towards the goals of sustainable cities. Humans are social creatures and thrive in urban spaces that foster social connections. Richard Florida, an urban studies theorist, focuses on the social impact of sustainable cities and states that cities need to be more than a competitive business climate; they need to be a great people climate that appeals to individuals and families of all types. Because of this, a shift to more dense, urban living would provide an outlet for social interaction and conditions under which humans can prosper. These types of urban areas would also promote the use of public transit, walkability and biking which would benefit citizens health wise but also be environmentally beneficial.\n\nContrary to common belief, urban systems can be more environmentally sustainable than rural or suburban living. With people and resource located so close to one another it is possible to save energy for transportation and mass transit systems, and resources such as food. Cities benefit the economy by locating human capital in one relatively small geographic area where ideas can be generated. Having a more dense, urban space would also increase people's efficiency since they wouldn't have to spend as much time commuting to places if resources are located close together, which in turn would benefit the economy since people can use this extra time on other matters; like work.\n\nThese ecological cities are achieved through various means, such as:\n\nBuildings provide the infrastructure for a functioning city and allow for many opportunities to demonstrate a commitment to sustainability. A commitment to sustainable architecture encompasses all phases of building including the planning, building, and restructuring. Sustainable Site Initiatives is used by landscape architects, designers, engineers, architects, developers, policy-makers and others to align land development and management with innovative sustainable design.\n\nThe purpose of an eco-industrial park is to connect a number of firms and organizations to work together to decrease their environmental impact while simultaneously improving their economic performance. The community of businesses accomplishes this goal through collaboration in managing environmental and resource issues, such as energy, water, and materials. The components for building an eco-industrial park include natural systems, more efficient use of energy, and more efficient material and water flows Industrial parks should be built to fit into their natural settings in order to reduce environmental impacts, which can be accomplished through plant design, landscaping, and choice of materials. For instance, there is an industrial park in Michigan built by Phoenix Designs that is made almost entirely from recycled materials. The landscaping of the building will include native trees, grasses, and flowers, and the landscaping design will also act as climate shelter for the facility. In choosing the materials for building an eco-industrial park, designers must consider the life-cycle analysis of each medium that goes into the building to assess their true impact on the environment and to ensure that they are using it from one plant to another, steam connections from firms to provide heating for homes in the area, and using renewable energy such as wind and solar power. In terms of material flows, the companies in an eco-industrial park may have common waste treatment facilities, a means for transporting by-products from one plant to another, or anchoring the park around resource recovery companies that are recruited to the location or started from scratch. To create more efficient water flows in industrial parks, the processed water from one plant can be reused by another plant and the parks infrastructure can include a way to collect and reuse storm water runoff.\n\nSee also: Urban Agriculture\n\nUrban farming is the process of growing and distributing food, as well as raising animals, in and around a city or in urban area. According to the RUAF Foundation, urban farming is different from rural agriculture because \"it is integrated into the urban economic and ecological system: urban agriculture is embedded in -and interacting with- the urban ecosystem. Such linkages include the use of urban residents as labourers, use of typical urban resources (like organic waste as compost and urban wastewater for irrigation), direct links with urban consumers, direct impacts on urban ecology (positive and negative), being part of the urban food system, competing for land with other urban functions, being influenced by urban policies and plans, etc.\" There are many motivations behind urban agriculture, but in the context of creating a sustainable city, this method of food cultivation saves energy in food transportation and saves costs. In order for urban farming to be a successful method of sustainable food growth, cities must allot a common area for community gardens or farms, as well as a common area for a farmers market in which the foodstuffs grown within the city can be sold to the residents of the urban system.\nBerms of fava beans have been planted at Hayes Valley Farm, a community-built farm on the former Central freeway ramps of San Francisco.\n\nMany cities are currently in a shift from the suburban sprawl model of development to a return to urban dense living. This shift in geographic distribution of population leads to a denser core of city residents. These residents provide a growing demand in many sectors that is reflected in the architectural fabric of the city. This new demand can be supplied by new construction or historic rehabilitation. Sustainable cities will opt for historical rehabilitation wherever possible. Having people live in higher densities not only gives economies of scale but also allows for infrastructure to be more efficient.\n\nWalkable urbanism is a development strategy in opposition to suburban sprawl. It advocates housing for a diverse population, a full mix of uses, walkable streets, positive public space, integrated civic and commercial centers, transit orientation and accessible open space. It also advocates for density and accessibility of commercial and government activity.\n\nThe most clearly defined form of walkable urbanism is known as the Charter of New Urbanism. It is an approach for successfully reducing environmental impacts by altering the built environment to create and preserve smart cities which support sustainable transport. Residents in compact urban neighborhoods drive fewer miles, and have significantly lower environmental impacts across a range of measures, compared with those living in sprawling suburbs. The concept of circular flow land use management has also been introduced in Europe to promote sustainable land use patterns that strive for compact cities and a reduction of greenfield land taken by urban sprawl.\n\nIn sustainable architecture the recent movement of New Classical Architecture promotes a sustainable approach towards construction, that appreciates and develops smart growth, walkability, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as opposing solitary housing estates and suburban sprawl. Both trends started in the 1980s.\n\nMain article: Leadership in Energy and Environmental Design\n\nThe Leadership in Energy and Environmental Design (LEED) Green Building Rating System® encourages and accelerates global adoption of sustainable green building and development practices through the creation and implementation of universally understood and accepted tools and performance criteria.\n\nLEED, or Leadership in Energy and Environmental Design, is an internationally recognized green building certification system. LEED recognizes whole building sustainable design by identifying key areas of excellence including: Sustainable Sites, Water Efficiency, Energy and Atmosphere, Materials and Resources, Indoor Environmental Quality, Locations & Linkages, Awareness and Education, Innovation in Design, Regional Priority. In order for a building to become LEED certified sustainability needs to be prioritized in design, construction, and use. One example of sustainable design would be including a certified wood like bamboo. Bamboo is fast growing and has an incredible replacement rate after being harvested. By far the most credits are rewarded for optimizing energy performance. This promotes innovative thinking about alternative forms of energy and encourages increased efficiency.\n\nSustainable Sites Initiative, a combined effort of the American Society of Landscape Architects, The Lady Bird Johnson Wildflower Center at The University of Texas at Austin, and the United States Botanic Garden, is a voluntary national guideline and performance benchmark for sustainable land design, construction and maintenance practices. The building principles of SSI are to design with nature and culture, use a decision-making hierarchy of preservation, conservation, and regeneration, use a system thinking approach, provide regenerative systems, support a living process, use a collaborative and ethical approach, maintain integrity in leadership and research, and finally foster environmental stewardship. All of these help promote solutions to common environmental issues such as greenhouse gases, urban climate issues, water pollution and waste, energy consumption, and health and wellbeing of site users. The main focus is hydrology, soils, vegetation, materials, and human health and well being.\n\nIn SSI, the main goal for hydrology in sites is to protect and restore existing hydrologic functions. To design storm water features to be accessible to site users, and manage and clean water on site. For site design of soil and vegetationmany steps can be done during the construction process to help minimize the urban heat island effects, to and minimize the building heating requirements by using plants.\n\nAs major focus of the sustainable cities, sustainable transportation attempts to reduce a city’s reliance and use of greenhouse emitting gases by utilizing eco friendly urban planning, low environmental impact vehicles, and residential proximity to create an urban center that has greater environmental responsibility and social equity.\n\nDue to the significant impact that transportation services have on a city’s energy consumption, the last decade has seen an increasing emphasis on sustainable transportation by developmental experts. Currently, transportation systems account for nearly a quarter of the world’s energy consumption and carbon dioxide emission. In order to reduce the environmental impact caused by transportation in metropolitan areas, sustainable transportation has three widely agreed upon pillars that it utilizes to create more healthy and productive urban centers.\n\nThe Carbon Trust states that there are three main ways cities can innovate to make transport more sustainable without increasing journey times - better land use planning, modal shift to encourage people to choose more efficient forms of transport, and making existing transport modes more efficient.\n\nThe concept of car free cities or a city with large pedestrian areas is often part of the design of a sustainable city. A large part of the carbon footprint of a city is generated by cars so the car free concept is often considered an integral part of the design of a sustainable city.\n\nCreated by eco friendly urban planning, the concept of urban proximity is an essential element of current and future sustainable transportation systems. This requires that cities be built and added onto with appropriate population and landmark density so that destinations are reached with reduced time in transit. This reduced time in transit allows for reduced fuel expenditure and also opens the door to alternative means of transportation such as bike riding and walking.\nTransportation in downtown Chicago\nFurthermore, close proximity of residents and major landmarks allows for the creation of efficient public transportation by eliminating long sprawled out routes and reducing commute time. This in turn decreases the social cost to residents who choose to live in these cities by allowing them more time with families and friends instead by eliminating part of their commute time.\n\nSee also: Compact city and Pocket neighborhood\n\nSustainable transportation emphasizes the use of a diversity of fuel-efficient transportation vehicles in order to reduce greenhouse emissions and diversity fuel demand. Due to the increasingly expensive and volatile cost of energy, this strategy has become very important because it allows a way for city residents to be less susceptible to varying highs and lows in various energy prices.\n\nAmong the different modes of transportation, the use alternative energy cars and widespread installation of refueling stations has gained increasing importance, while the creation of centralized bike and walking paths remains a staple of the sustainable transportation movement.\n\nIn order to maintain the aspect of social responsibility inherent within the concept of sustainable cities, implementing sustainable transportation must include access to transportation by all levels of society. Due to the fact that car and fuel cost are often too expensive for lower income urban residents, completing this aspect often revolves around efficient and accessible public transportation.\n\nIn order to make public transportation more accessible, the cost of rides must be affordable and stations must be located no more than walking distance in each part of the city. As studies have shown, this accessibility creates a great increase in social and productive opportunity for city residents. By allowing lower income residents cheap and available transportation, it allows for individuals to seek employment opportunities all over the urban center rather than simply the area in which they live. This in turn reduces unemployment and a number of associated social problems such as crime, drug use, and violence.\n\nAlthough there is not an international policy regarding sustainable cities and there are not established international standards, there is an organization, the United Cities and Local Governments (UCLG) that is working to establish universal urban strategic guidelines. The UCLG a democratic and decentralized structure that operates in Africa, Asia, Eurasia, Europe, Latin America, North America, Middle East, West Asian and a Metropolitan section work to promote a more sustainable society. The 60 members of the UCLG committee evaluate urban development strategies and debate theses experiences to make the best recommendations. Additionally, the UCLG accounts for differences in regional and national context. All the organizations are making a great effort to promote this concept by media and internet, and in conferences and workshops. An International conference was held in Italy at Università del Salento and Università degli Studi della Basilicata, called 'Green Urbanism', from 12–14 October 2016.\n\nRecently, local and national governments and regional bodies such as the European Union have recognized the need for a holistic understanding of urban planning. This is instrumental to establishing an international policy that focuses on cities challenges and the role of the local authorities responses. Generally, in terms of urban planning, the responsibility of local governments are limited to land use and infrastructure provision excluding inclusive urban development strategies. The advantages of urban strategic planning include an increase in governance and cooperation that aids local governments in establishing performance based-management, clearly identifying the challenges facing local community and more effectively responding on a local level rather than national level, and improves institutional responses and local decision making. Additionally, it increases dialogue between stakeholders and develops consensus-based solutions, establishing continuity between sustainability plans and change in local government; it places environmental issues as the priority for the sustainable development of cities and serves as a platform to develop concepts and new models of housing, energy and mobility.\n\nThe City Development Strategies (CDS) addresses new challenges and provides space for innovative policies that involves all stakeholders. The inequality in spatial development and socio-economic classes paired with concerns of poverty reduction and climate change are factors in achieving global sustainable cities. According to the UCLG there are differences between regional and national conditions, framework and practice that are overcome in the international commitment to communication and negotiation with other governments, communities and the private sector to continual to develop through innovative and participatory approaches in strategic decisions, building consensus and monitoring performance management and raising investment.\n\nAccording to UN Habitat, around half of the world's population is concentrated in cities, which is set to rise to 60% within a couple decades. The UCLG has specifically identified 13 global challenges to establishing sustainable cities: demographic change and migration, globalisation of the job market, poverty and unmet Millennium Development Goals, segregation, spatial patterns and urban growth, metropolisation and the rise of urban regions, more political power for local authories, new actors for developing a city and providing services, decline in public funding for development, the environment and climate change, new and accessible building technologies, preparing for uncertainty and limits of growth and global communications and partnerships.\n\nUrban forests\n\nIn Adelaide, South Australia (a city of 1.3 million people) Premier Mike Rann (2002 to 2011) launched an urban forest initiative in 2003 to plant 3 million native trees and shrubs by 2014 on 300 project sites across the metro area. The projects range from large habitat restoration projects to local biodiversity projects. Thousands of Adelaide citizens have participated in community planting days. Sites include parks, reserves, transport corridors, schools, water courses and coastline. Only trees native to the local area are planted to ensure genetic integrity. Premier Rann said the project aimed to beautify and cool the city and make it more liveable; improve air and water quality and reduce Adelaide's greenhouse gas emissions by 600,000 tonnes of C02 a year. He said it was also about creating and conserving habitat for wildlife and preventing species loss.\n\nSolar power\n\nThe Rann government also launched an initiative for Adelaide to lead Australia in the take-up of solar power. In addition to Australia's first 'feed-in' tariff to stimulate the purchase of solar panels for domestic roofs, the government committed millions of dollars to place arrays of solar panels on the roofs of public buildings such as the museum, art gallery, Parliament, Adelaide Airport, 200 schools and Australia's biggest rooftop array on the roof of Adelaide Showgrounds' convention hall which was registered as a power station.\n\nWind power\n\nSouth Australia went from zero wind power in 2002 to wind power, making up 26% of its electricity generation by October 2011. In the five years preceding 2011 there was a 15% drop in emissions, despite strong economic growth.\n\nWaste recycling\n\nFor Adelaide the South Australian government also embraced a Zero Waste recycling strategy, achieving a recycling rate of nearly 80% by 2011 with 4.3 million tonnes of materials diverted from landfill to recycling. On a per capita basis this was the best result in Australia, the equivalent of preventing more than a million tonnes of C02 entering the atmosphere. In the 1970s container deposit legislation was introduced. Consumers are paid a 10 cent rebate on each bottle, can, or container they return to recycling. In 2009 non-reusable plastic bags used in supermarket checkouts were banned by the Rann Government, preventing 400 million plastic bags per year entering the litter stream. In 2010 Zero Waste SA was commended by a UN Habitat Report entitled 'Solid Waste Management in the World Cities'.\n\nMelbourne\n\n\nThe City of Greater Taree north of Sydney has developed a masterplan for Australia's first low-to-no carbon urban development.\n\nBelo Horizonte, Brazil was created in 1897 and is the third largest metropolis in Brazil, with 2.4 million inhabitants. The Strategic Plan for Belo Horizonte (2010–2030) is being prepared by external consultants based on similar cities' infrastructure, incorporating the role of local government, state government, city leaders and encouraging citizen participation. The need for environmental sustainable development is led by the initiative of new government following planning processes from the state government. Overall, the development of the metropolis is dependent on the land regularization and infrastructure improvement that will better support the cultural technology and economic landscape.Southern cities of Porto Alegre and Curitiba are often cited as examples of urban sustainability.\n\n\nThe GreenScore City Index studies the ecological footprints of Canadian cities and splits them into three population categories: large, medium, and small. The index studies 50 cities in Canada.\n\n\nMost cities in Canada have sustainability action plans which are easily searched and downloaded from city websites.\n\nIn 2010, Calgary ranked as the top eco-city in the planet for it's, \"excellent level of service on waste removal, sewage systems, and water drinkability and availability, coupled with relatively low air pollution.” The survey was performed in conjunction with the reputable Mercer Quality of Living Survey.\n\n\nTwo comprehensive studies were carried out for the whole of Denmark in 2010 (The IDA Climate Plan 2050) and 2011 (The Danish Commission on Climate Change Policy). The studies analysed the benefits and obstacles of running Denmark on 100% renewable energy from the year 2050. There is also a larger, ambitious plan in action: the Copenhagen 2025 Climate Plan.\n\nOn a more local level, the industrial park in Kalundborg is often cited as a model for industrial ecology. However, projects have been carried out in several Danish cities promoting 100% renewable energy. Examples include Aalborg, Ballerup and Frederikshavn. Aalborg University has launched a master education program on sustainable cities (Sustainable Cities @ Aalborg University Copenhagen). See also the Danish Wikipedia.\n\n\nLoja, Ecuador won three international prizes for the sustainability efforts begun by its mayor Dr. Jose Bolivar Castillo.\n\nOxford Residences for four seasons in Estonia, winning a prize for Sustainable Company of the Year, is arguably one of the most advanced sustainable developments, not only trying to be carbon neutral, but already carbon negativeGermany \n\nFreiburg im Breisgau is often referred to as green city. It is known for its strong solar economy. Vauban, Freiburg is a sustainable model district. All houses are built to a low energy consumption standard and the whole district is designed to be carfree. \n\nThe Finnish city of Turku has adopted a \"Carbon Neutral Turku by 2040\" strategy to achieve carbon neutrality via combining the goal with circular economy.\n\nNo other country has built more eco-city projects than Germany. Freiburg im Breisgau is often referred to as a green city. It is one of the few cities with a Green mayor and is known for its strong solar energy industry. Vauban, Freiburg is a sustainable model district. All houses are built to a low energy consumption standard and the whole district is designed to be carfree. Another green district in Freiburg is Rieselfeld, where houses generate more energy than they consume. There are several other green sustainable city projects such as Kronsberg in Hannover and current developments around Munich, Hamburg and Frankfurt.\n\n\nThe government portrays the proposed Hung Shui Kiu new town as an eco-city. The same happened with the urban development plan on the site of the former Kai Tak Airport.\n\nSouth Dublin County Council announced plans in late 2007 to develop Clonburris, a new suburb of Dublin to include up to 15,000 new homes, to be designed to achieve the highest of international standards. The plans for Clonburris include countless green innovations such as high levels of energy efficiency, mandatory renewable energy for heating and electricity, the use of recycled and sustainable building materials, a district heating system for distributing heat, the provision of allotments for growing food, and even the banning of tumble driers, with natural drying areas being provided instead.\n\nIn 2012 a energy plan was carried out by the Danish Aalborg University for the municipalities of Limerick and Clare. The project was a short-term 2020 renewable energy strategy giving a 20% reduction in CO emissions, while ensuring that short-term actions are beneficial to the long-term goal of 100% renewable energy.\n\nIndia is working on Gujarat International Finance Tec-City or GIFT which is an under-construction world-class city in the Indian state of Gujarat. It will come up on 500 acres (2.0 km) land. It will also be first of its kind fully Sustainable City.\nAuroville was founded in 1968 with the intention of realizing human unity, and is now home to approximately 2,000 individuals from over 45 nations around the world. Its focus is its vibrant community culture and its expertise in renewable energy systems, habitat restoration, ecology skills, mindfulness practices, and holistic education.\nAndhra Pradesh state New capital also coming up with a future sustainable city.\n\nHacienda - Mombasa, Kenya. It is the largest development of eco-friendly residential properties in East Africa; construction is currently ongoing, and it will eventually be one of Africa’s first self-sustaining estates.\n\nSongdo IBD is a planned city in Incheon which has incorporated a number of eco-friendly features. These include a central park irrigated with seawater, a subway line, bicycle lanes, rainwater catchment systems, and pneumatic waste collection system. 75% of the waste generated by the construction of the city will be recycled.\n\nGwanggyo City Centre is another planned sustainable city.\n\nAs of 2014 a Low Carbon Cities programme is being piloted in Malaysia by KeTTHA, the Malaysian Ministry of Energy, Green Technology and Water, Malaysian Green Technology Corporation (GreenTech Malaysia) and the Carbon Trust.\n\nMalacca has a stated ambition to become a carbon-free city, taking steps towards creating a smart electricity grid. This is being done as part of an initiative to create a Green Special Economic Zone, where it is intended that as many as 20 research and development centers will be built focusing on renewable energy and clean technology, creating up to 300,000 new green jobs. \n\nThe Federal Department of Town and Country Planning (FDTCP) in peninsular Malaysia is a focal point for the implementation of the Malaysian Urban Rural National Indicators Network for Sustainable Development (MURNInets)MURNInets includes 36 sets of compulsory indicators grouped under 21 themes under six dimensions. Most of the targets and standards for the selected indicators were adjusted according to hierarchy of local authorities. In MURNInets at least three main new features are introduced. These include the Happiness Index, an indicator under the quality of life theme to meet the current development trend that emphasizes on the well-being of the community. Another feature introduced is the customer or people satisfaction level towards local authorities' services. Through the introduction of these indicators the bottom-up approach in measuring sustainability is adopted.\n\nThe city of Waitakere, the Western part of the greater Auckland urban region, was New Zealand's first eco-city, working from the Greenprint, a guiding document that the City Council developed in the early 1990s.\n\nClark Freeport Zone is a former United States Air Force base in the Philippines. It is located on the northwest side of Angeles City and on the west side of Mabalacat City in the province of Pampanga, about 40 miles (60 km) northwest of Metro Manila. A multi-billion project will convert the 36,000 hectare former Clark Air Force Base into a mix of industrial, commercial and institutional areas of green environment. The heart of the project is a 9,450-hectare metropolis dubbed as the \"Clark Green City\". Builders will use the green building system for environmentally-friendly structures. Its facilities will tap renewable energy such as solar and hydro power.\n\nThe organization Living PlanIT is currently constructing a city from scratch near Porto, Portugal. Buildings will be electronically connected to vehicles giving the user a sense of personal eco-friendliness.\n\n\n\n\n\n\nSee also the Sustainability navigational box at the bottom of the page.\n\n\n\n"}
{"id": "27919989", "url": "https://en.wikipedia.org/wiki?curid=27919989", "title": "Timeline of European exploration", "text": "Timeline of European exploration\n\nThe following timeline covers European exploration from 1418 to 1957.\n\nThe 15th century witnessed the rounding of the feared Cape Bojador and Portuguese exploration of the west coast of Africa, while in the last decade of the century the Spanish sent expeditions to the New World, focusing on exploring the Caribbean Sea, and the Portuguese discovered the sea route to India. In the 16th century, various countries sent exploring parties into the interior of the Americas, as well as to their respective west and east coasts north to California and Labrador and south to Chile and Tierra del Fuego. In the 17th century, the Russians explored and conquered Siberia in search of sables, while the Dutch roughly worked on the chart for Australia. The 18th century saw the first extensive exploration of the South Pacific and the discovery of Alaska, while the nineteenth was dominated by exploration of the polar regions (not to mention excursions into the heart of Africa). By the 20th century, the poles themselves had been reached.\n\n\n\n\n \n\n\n\n"}
{"id": "8331945", "url": "https://en.wikipedia.org/wiki?curid=8331945", "title": "Ultra-short baseline", "text": "Ultra-short baseline\n\nUSBL (ultra-short baseline, also sometimes known as SSBL for super short base line) is a method of underwater acoustic positioning. A complete USBL system consists of a transceiver, which is mounted on a pole under a ship, and a transponder or responder on the seafloor, on a towfish, or on an ROV. A computer, or \"topside unit\", is used to calculate a position from the ranges and bearings measured by the transceiver.\n\nAn acoustic pulse is transmitted by the transceiver and detected by the subsea transponder, which replies with its own acoustic pulse. This return pulse is detected by the shipboard transceiver. The time from the transmission of the initial acoustic pulse until the reply is detected is measured by the USBL system and is converted into a range.\n\nTo calculate a subsea position, the USBL calculates both a range and an angle from the transceiver to the subsea beacon. Angles are measured by the transceiver, which contains an array of transducers. The transceiver head normally contains three or more transducers separated by a baseline of 10 cm or less. A method called “phase-differencing” within this transducer array is used to calculate the direction to the subsea transponder.\n\nUSBLs have also begun to find use in \"inverted\" (iUSBL) configurations, with the transceiver mounted on an autonomous underwater vehicle, and the transponder on the target. In this case, the \"topside\" processing happens inside the vehicle to allow it to locate the transponder for applications such as automatic docking and target tracking.\n\n\n"}
{"id": "44318410", "url": "https://en.wikipedia.org/wiki?curid=44318410", "title": "What3words", "text": "What3words\n\nwhat3words is a geocoding system for the communication of locations with a resolution of three metres. What3words encodes geographic coordinates into three dictionary words. For example, the torch of the Statue of Liberty is located at \"toned.melt.ship\". This differs from most other location encoding systems in that it displays three words rather than long strings of numbers or letters. What3words has a website, apps for iOS and Android, and an API that enables bidirectional conversion of what3words address and latitude/longitude coordinates. As the system relies on a fixed algorithm, not a large database of every location on earth, it works on devices with limited storage and no internet connection, and the encoding is permanently fixed and unchangeable.\n\nFounded by Chris Sheldrick, Jack Waley-Cohen, Mohan Ganesalingam, and Michael Dent, what3words launched in July 2013. Sheldrick and Ganesalingam originally conceived the idea after Sheldrick struggled to get equipment and bands to event locations on time due to inadequate addressing while working as a concert organizer. The company was incorporated on March 5, 2013 and a patent application for the core technology filed on April 19, 2013.\n\nIn November 2013, what3words raised $500,000 of seed funding, and in March 2014 the company raised a second seed round of $1,000,000.\nOn November 3, 2015, what3words closed a $3.5 million Series A funding round led by Intel Capital, with Li Ka-shing's Horizons Ventures participating.\nOn June 29, 2016, what3words closed a $8.5 million Series B round led by Aramex.\nOn January 10, 2018, Mercedes-Benz bought approximately 10% of the company and announced what3words support in future versions of the Mercedes-Benz User Experience (MBUX) infotainment and navigation system. The A-Class, launched in May 2018, became the first vehicle in the world with what3words on board.\n\nIn March 2016, the company announced that Colorado-based Steve Coast, founder of OpenStreetMap, joined the team as Chief Evangelist, charged with developing and strengthening partnerships in North America. Coast has since moved on.\n\nwhat3words uses a grid of the world made up of 57 trillion squares of 3 metres by 3 metres. Each square has been given a three-word English address. What3words has named the world's landmass with three words in various other languages. , what3words addresses (as well as web and iOS app user interface) are available in Arabic, English, Finnish, French, German, Italian, Mongolian, Polish, Portuguese, Russian, Spanish, Swedish, and Turkish; the iOS app also supports Swahili. What3words launched 12 more languages at the start of 2018: Indonesian, Zulu, Japanese, Korean, and Hindi. The company has also mentioned Chinese and various languages of Pakistan, including Urdu and Farsi.\n\nEach what3words language uses a wordlist of 25,000 words (40,000 in English, as it covers the sea as well as land). The lists go through multiple automated and human processes before being sorted by an algorithm that takes into account word length, distinctiveness, frequency, and ease of spelling and pronunciation. Homophones and variant spellings are treated to minimize any potential for confusion, and offensive words are removed.\n\nThe what3words algorithm actively shuffles similar-sounding three-word combinations around the world to enable both human and automated error-checking. The result is that if a three-word combination is entered slightly incorrectly and the result is still a valid what3words reference, the location will usually be so far away from the user's intended area that it will be immediately obvious to both a user and an intelligent error-checking system.\n\nThe what3words system uses a proprietary algorithm in combination with a limited database, meaning that the core technology is contained within a file around 10 MB in size. The database is used to assign more memorable words to locations in urban areas. what3words originally sold \"OneWord\" addresses, which were stored in a database for a yearly fee, but this feature has been canceled.\n\nThe main claimed advantages of what3words are memorability, error-detection, unambiguous nature of words for most everyday and non-technical uses and voice input.\n\nSupporters of open standards denounce the what3words system for being controlled by a private business and the software for being copyrighted and thus not freely usable. The fact that similar addresses are purposely far away from each other is also seen by some as a disadvantage.\n\nAutomotive\n\nAid and humanitarian\n\nMobility\n\nDelivery & logistics\n\nFestivals and events\n\nAsset management\n\n\nA number of parody sites have been created to satirize what3word's proprietary nature, including:\n\n\nAlphanumeric competitors include:\n\n"}
