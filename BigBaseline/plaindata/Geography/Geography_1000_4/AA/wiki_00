{"id": "1256241", "url": "https://en.wikipedia.org/wiki?curid=1256241", "title": "American Cordillera", "text": "American Cordillera\n\nThe American Cordillera is a chain of mountain ranges (cordilleras) that consists of an almost continuous sequence of mountain ranges that form the western \"backbone\" of North America, South America and Antarctica. It is also the backbone of the volcanic arc that forms the eastern half of the Pacific Ring of Fire.\n\nFrom north to south, this sequence of overlapping and parallel ranges begins with the Alaska Range and the Brooks Range in Alaska and runs through the Yukon into British Columbia. The main belt of the Rocky Mountains along with the parallel Columbia Mountains and Coast Ranges of mountains and islands continue through British Columbia and Vancouver Island. In the United States, the Cordillera branches include the Rockies, the Sierra Nevada, the Cascades, and various small Pacific coastal ranges. In Mexico, the Cordillera continues through the Sierra Madre Occidental and Sierra Madre Oriental, as well as the backbone mountains of the Baja California peninsula.\n\nThe ranges of the Cordillera from Mexico northwards are collectively called the North American Cordillera or Western Cordillera in the United States and Canada, and also named as the Canadian Cordillera or Pacific Cordillera in Canada.\n\nThe Cordillera continues on through the mountain ranges of Central America in Guatemala, Honduras, Nicaragua, Costa Rica, and Panama, and becomes the Andes Mountains of South America. The Andes with their parallel chains and the island chains off the coast of Chile continue through Colombia, Venezuela, Ecuador, Peru, Bolivia, Argentina, and Chile to the very tip of South America at Tierra del Fuego. The Cordillera continues along the Scotia Arc before reaching the mountains of the Antarctic Peninsula.\n"}
{"id": "39294118", "url": "https://en.wikipedia.org/wiki?curid=39294118", "title": "British wildwood", "text": "British wildwood\n\nBritish wildwood, or simply 'the wildwood', is the wholly natural landscape which developed across major parts of England after the last ice age. This woodland was not yet affected by human intervention, and was home to many species which are not now found in England, such as elk and brown bears. Over centuries, starting in the Neolithic period, this wildwood gradually gave way to open plains and fields as human populations increased and began to exploit and develop the land to their advantage. Much of the areas of woodland that remain in England descend from the original wildwood but are now in a semi-natural state due to being managed and controlled, for example as a source of timber. These are known as ancient woodland. True wildwood is thought to be no longer extant in the UK.\n\n"}
{"id": "7403047", "url": "https://en.wikipedia.org/wiki?curid=7403047", "title": "C-squares", "text": "C-squares\n\nC-squares (acronym for the concise spatial query and representation system) is a system of geocodes (actually a type of global grid) that provides a basis for simple spatial indexing of geographic features or data. It was devised by Tony Rees of CSIRO Marine and Atmospheric Research (then \"CSIRO Marine Research\") in 2001-2, and described in the literature in 2003. The notation system of c-squares incorporates a compact encoding of latitude and longitude coordinates into a machine- and human-readable \"c-squares code\", which can then be used either for spatial search or display via a suitable mapping application. The c-squares codes also provide an application- and vendor-independent, interoperable notation system for any gridded data whose units of organization correspond with steps of the c-squares hierarchy (e.g. 5-, 1-, 0.5 degree cells, etc.).\n\nAccording to the initial system description, c-squares was devised as an improved (more precise) method for expressing dataset geographic extents in searchable metadata catalogues, in place of (or in addition to) conventional bounding rectangle representations. The method is also useful as a generic, interoperable notation for gridded data (see above), for example a variety of datasets that describe environmental characteristics of global half-degree cells (see AquaMaps site). C-squares can also simply be used for mapping (example: CSIRO Marine and Atmospheric Research's \"CAAB\" application), although the most value is obtained when the system is used for spatial search as well (e.g. OBIS database, CMAR's \"MarLIN\" metadatabase, etc.).\n\nC-squares provides a hierarchical nomenclature for dividing 10°x10° World Meteorological Organization (WMO) squares into smaller units (each an individual \"c-square\") of 5°x5°, 1°x1°, 0.5°x0.5°, 0.1°x0.1°, etc., using an alternating base 2, base 5 linear division, as fine as may be required. Each cell of the resulting subdivision is allocated a unique alphanumeric identifier (c-squares code), such that the position of an object or objects on the surface of the Globe can be represented by a set of one or more such codes that define the cell(s) within which the object occurs. Storing these codes as text identifiers, for example in a database, repository of spatial metadata, searchable text file or web page, then offers the functionality for a simple, text-based spatial search, without the requirement for any more complex geographic information system (GIS). Once stored (or if desired, generated on-the-fly using a c-squares encoder), a code or set of codes can also be rendered on a map by a utility (for example, the web-accessible c-squares mapper) that incorporates the relevant decoding routines.\n\nThe actual assignment of the individual cell identifiers to vector objects whose position is expressed in latitude, longitude coordinates follows rules described in reference (1) and the c-squares website and can be automated via simple routines, and the reverse is also possible (decoding) in a straightforward manner. In addition, since the codes are hierarchical and interleaved (each \"cycle\" containing identifiers for both longitude and latitude, at progressively finer levels of resolution), only the relevant \"leading\" portion of a (for example) high resolution code need be interrogated to match a lower resolution spatial query.\n\nConceptually, the c-squares encoding method belongs to a class of algorithms which can be considered derivations of the Z-order principle.\n\nLondon, UK, occupies (for example) portions of four 0.5°x0.5° cells:\n\nIn c-squares notation these are cells \"7500:110:3\", \"7500:110:1\", \"1500:110:3\" and \"1500:110:1\" (at 0.5°x0.5° resolution). Alternatively at 1°x1° they are portions of two cells, \"7500:110\" and \"1500:110\"; at 5°x5° resolution, portions of two cells \"7500:1\" and \"1500:1\"; and at 10°x10° resolution, portions of 2 cells \"7500\" and \"1500\" (equivalent to the same identifiers as in WMO squares).\n\nTo visualize the position of these squares on a map, the current syntax to address an installation of the \"c-squares mapper\" is (e.g.):\n\nhttp://www.obis.org.au/cgi-bin/cs_map.pl?csq=7500:110:3|7500:110:1|1500:110:3|1500:110:1.\n\nIn a system that uses c-squares codes as units of spatial indexing, a text-based search on any of these square identifiers will retrieve data associated with the relevant square. If a wildcard search is supported (for example suppose that the wildcard character is a percent sign), a search on \"7500%\" will retrieve all data items in that ten degree square, a search on \"7500:1%\" will retrieve all data items in that five degree square, etc.\n\nThe asterisk character \"*\" has a special (reserved) meaning in c-squares notation, being a \"compact\" notation indicating that all finer cells within a higher level cell are included, to the level of resolution indicated by the number of asterisks. In the example above, \"7500:*\" would indicate that all 4 five-degree cells within parent ten-degree cell \"7500\" are filled, \"7500:***\" would indicate that all 100 one-degree cells within parent ten-degree cell \"7500\" are filled, etc. This approach enables the filling of contiguous blocks of cells with an economy of characters in many cases (a form of data compression), that is useful for efficient storage and transfer of c-squares codes as required.\n\nC-squares spatial indexing, search, and/or mapping routines are currently incorporated into OBIS, the Ocean Biogeographic Information System; FishBase; AquaMaps; and a variety of web-accessible applications at CSIRO Marine and Atmospheric Research. For a full list, see the link Sample c-squares enabled systems on the c-squares website.\n\n\n"}
{"id": "143335", "url": "https://en.wikipedia.org/wiki?curid=143335", "title": "Celestial navigation", "text": "Celestial navigation\n\nCelestial navigation, also known as astronavigation, is the ancient and modern practice of position fixing that enables a navigator to transition through a space without having to rely on estimated calculations, or dead reckoning, to know their position. Celestial navigation uses \"sights\", or angular measurements taken between a celestial body (e.g. the Sun, the Moon, a planet, or a star) and the visible horizon. The Sun is most commonly used, but navigators can also use the Moon, a planet, Polaris, or one of 57 other navigational stars whose coordinates are tabulated in the nautical almanac and air almanacs.\n\nCelestial navigation is the use of angular measurements (sights) between celestial bodies and the visible horizon to locate one's position in the world, on land as well as at sea. At a given time, any celestial body is located directly over one point on the Earth's surface. The latitude and longitude of that point is known as the celestial body's geographic position (GP), the location of which can be determined from tables in the nautical or air almanac for that year.\n\nThe measured angle between the celestial body and the visible horizon is directly related to the distance between the celestial body's GP and the observer's position. After some computations, referred to as sight reduction, this measurement is used to plot a line of position (LOP) on a navigational chart or plotting work sheet, the observer's position being somewhere on that line. (The LOP is actually a short segment of a very large circle on Earth that surrounds the GP of the observed celestial body. An observer located anywhere on the circumference of this circle on Earth, measuring the angle of the same celestial body above the horizon at that instant of time, would observe that body to be at the same angle above the horizon.) Sights on two celestial bodies give two such lines on the chart, intersecting at the observer's position (actually, the two circles would result in two points of intersection arising from sights on two stars described above, but one can be discarded since it will be far from the estimated position—see the figure at example below). Most navigators will use sights of three to five stars, if available, since that will result in only one common intersection and minimizes the chance of error. That premise is the basis for the most commonly used method of celestial navigation, referred to as the 'altitude-intercept method'.\n\nThere are several other methods of celestial navigation that will also provide position-finding using sextant observations, such as the noon sight, and the more archaic lunar distance method. Joshua Slocum used the lunar distance method during the first recorded single-handed circumnavigation of the world. Unlike the altitude-intercept method, the noon sight and lunar distance methods do not require accurate knowledge of time. The altitude-intercept method of celestial navigation requires that the observer know exact Greenwich Mean Time (GMT) at the moment of his observation of the celestial body, to the second—since for every four seconds that the time source (commonly a chronometer or, in aircraft, an accurate \"hack watch\") is in error, the position will be off by approximately one nautical mile.\n\nAn example illustrating the concept behind the intercept method for determining one’s position is shown to the right. (Two other common methods for determining one’s position using celestial navigation are the longitude by chronometer and ex-meridian methods.) In the adjacent image, the two circles on the map represent lines of position for the Sun and Moon at 1200 GMT on October 29, 2005. At this time, a navigator on a ship at sea measured the Moon to be 56 degrees above the horizon using a sextant. Ten minutes later, the Sun was observed to be 40 degrees above the horizon. Lines of position were then calculated and plotted for each of these observations. Since both the Sun and Moon were observed at their respective angles from the same location, the navigator would have to be located at one of the two locations where the circles cross.\n\nIn this case the navigator is either located on the Atlantic Ocean, about west of Madeira, or in South America, about southwest of Asunción, Paraguay. In most cases, determining which of the two intersections is the correct one is obvious to the observer because they are often thousands of miles apart. As it is unlikely that the ship is sailing across South America, the position in the Atlantic is the correct one. Note that the lines of position in the figure are distorted because of the map’s projection; they would be circular if plotted on a globe.\n\nAn observer in the Gran Chaco point would see the Moon at the left of the Sun, and an observer in the Madeira point would see the Moon at the right of the Sun.\n\nAccurate angle measurement evolved over the years. One simple method is to hold the hand above the horizon with one's arm stretched out. The width of the little finger is an angle just over 1.5 degrees elevation at extended arm's length and can be used to estimate the elevation of the sun from the horizon plane and therefore estimate the time until sunset. The need for more accurate measurements led to the development of a number of increasingly accurate instruments, including the kamal, astrolabe, octant and sextant. The sextant and octant are most accurate because they measure angles from the horizon, eliminating errors caused by the placement of an instrument's pointers, and because their dual mirror system cancels relative motions of the instrument, showing a steady view of the object and horizon.\n\nNavigators measure distance on the globe in degrees, arcminutes and arcseconds. A nautical mile is defined as 1852 meters, but is also (not accidentally) one minute of angle along a meridian on the Earth. Sextants can be read accurately to within 0.2 arcminutes, so the observer's position can be determined within (theoretically) 0.2 miles, about 400 yards (370 m). Most ocean navigators, shooting from a moving platform, can achieve a practical accuracy of 1.5 miles (2.8 km), enough to navigate safely when out of sight of land.\n\nPractical celestial navigation usually requires a marine chronometer to measure time, a sextant to measure the angles, an almanac giving schedules of the coordinates of celestial objects, a set of sight reduction tables to help perform the height and azimuth computations, and a chart of the region. </small> With sight reduction tables, the only calculations required are addition and subtraction. Small handheld computers, laptops and even scientific calculators enable modern navigators to \"reduce\" sextant sights in minutes, by automating all the calculation and/or data lookup steps. Most people can master simpler celestial navigation procedures after a day or two of instruction and practice, even using manual calculation methods.\n\nModern practical navigators usually use celestial navigation in combination with satellite navigation to correct a dead reckoning track, that is, a course estimated from a vessel's position, course and speed. Using multiple methods helps the navigator detect errors, and simplifies procedures. When used this way, a navigator will from time to time measure the sun's altitude with a sextant, then compare that with a precalculated altitude based on the exact time and estimated position of the observation. On the chart, one will use the straight edge of a plotter to mark each position line. If the position line indicates a location more than a few miles from the estimated position, more observations can be taken to restart the dead-reckoning track.\n\nIn the event of equipment or electrical failure, taking sun lines a few times a day and advancing them by dead reckoning allows a vessel to get a crude running fix sufficient to return to port. One can also use the Moon, a planet, Polaris, or one of 57 other navigational stars to track celestial positioning.\n\nLatitude was measured in the past either by measuring the altitude of the Sun at noon (the \"noon sight\"), or by measuring the altitudes of any other celestial body when crossing the meridian (reaching its maximum altitude when due north or south), and frequently by measuring the altitude of Polaris, the north star (assuming it is sufficiently visible above the horizon, which it is not in the Southern Hemisphere). Polaris always stays within 1 degree of the celestial north pole. If a navigator measures the angle to Polaris and finds it to be 10 degrees from the horizon, then he is about 10 degrees north of the equator. This approximate latitude is then corrected using simple tables or almanac corrections to determine a latitude theoretically accurate to within a fraction of a mile. Angles are measured from the horizon because locating the point directly overhead, the zenith, is not normally possible. When haze obscures the horizon, navigators use artificial horizons, which are horizontal mirrors of pans of reflective fluid, especially mercury historically. In the latter case, the angle between the reflected image in the mirror and the actual image of the object in the sky is exactly twice the required altitude.\n\nLongitude can be measured in the same way. If the angle to Polaris can be accurately measured, a similar measurement to a star near the eastern or western horizons will provide the longitude. The problem is that the Earth turns 15 degrees per hour, making such measurements dependent on time. A measure a few minutes before or after the same measure the day before creates serious navigation errors. Before good chronometers were available, longitude measurements were based on the transit of the moon, or the positions of the moons of Jupiter. For the most part, these were too difficult to be used by anyone except professional astronomers. The invention of the modern chronometer by John Harrison in 1761 vastly simplified longitudinal calculation.\n\nThe longitude problem took centuries to solve and was dependent on the construction of a non-pendulum clock (as pendulum clocks cannot function accurately on a tilting ship, or indeed a moving vehicle of any kind). Two useful methods evolved during the 18th century and are still practised today: lunar distance, which does not involve the use of a chronometer, and use of an accurate timepiece or chronometer.\n\nPresently, lay-person calculations of longitude can be made by noting the exact local time (leaving out any reference for Daylight Saving Time) when the sun is at its highest point in the sky. The calculation of noon can be made more easily and accurately with a small, exactly vertical rod driven into level ground—take the time reading when the shadow is pointing due north (in the northern hemisphere). Then take your local time reading and subtract it from GMT (Greenwich Mean Time) or the time in London, England. For example, a noon reading (1200 hours) near central Canada or the US would occur at approximately 6 pm (1800 hours) in London. The six-hour differential is one quarter of a 24-hour day, or 90 degrees of a 360-degree circle (the Earth). The calculation can also be made by taking the number of hours (use decimals for fractions of an hour) multiplied by 15, the number of degrees in one hour. Either way, it can be demonstrated that much of central North America is at or near 90 degrees west longitude. Eastern longitudes can be determined by adding the local time to GMT, with similar calculations.\n\nThe older method, called \"lunar distances\", was refined in the 18th century and employed with decreasing regularity at sea through the middle of the 19th century. It is only used today by sextant hobbyists and historians, but the method is theoretically sound, and can be used when a timepiece is not available or its accuracy is suspect during a long sea voyage. The navigator precisely measures the angle between the moon and the sun, or between the moon and one of several stars near the ecliptic. The observed angle must be corrected for the effects of refraction and parallax, like any celestial sight. To make this correction the navigator would measure the altitudes of the moon and sun (or star) at about the same time as the lunar distance angle. Only rough values for the altitudes were required. Then a calculation with logarithms or graphical tables requiring ten to fifteen minutes' work would convert the observed angle to a geocentric lunar distance. The navigator would compare the corrected angle against those listed in the almanac for every three hours of Greenwich time, and interpolate between those values to get the actual Greenwich time aboard ship. Knowing Greenwich time and comparing against local time from a common altitude sight, the navigator can work out his longitude.\n\nThe considerably more popular method was (and still is) to use an accurate timepiece to directly measure the time of a sextant sight. The need for accurate navigation led to the development of progressively more accurate chronometers in the 18th century (see John Harrison). Today, time is measured with a chronometer, a quartz watch, a shortwave radio time signal broadcast from an atomic clock, or the time displayed on a GPS. A quartz wristwatch normally keeps time within a half-second per day. If it is worn constantly, keeping it near body heat, its rate of drift can be measured with the radio and, by compensating for this drift, a navigator can keep time to better than a second per month. Traditionally, a navigator checked his chronometer from his sextant, at a geographic marker surveyed by a professional astronomer. This is now a rare skill, and most harbourmasters cannot locate their harbour's marker.\n\nTraditionally, three chronometers were kept in gimbals in a dry room near the centre of the ship. They were used to set a hack watch for the actual sight, so that no chronometers were ever exposed to the wind and salt water on deck. Winding and comparing the chronometers was a crucial duty of the navigator. Even today, it is still logged daily in the ship's deck log and reported to the Captain before eight bells on the forenoon watch (shipboard noon). Navigators also set the ship's clocks and calendar.\n\nThe celestial line of position concept was discovered in 1837 by Thomas Hubbard Sumner when, after one observation, he computed and plotted his longitude at more than one trial latitude in his vicinity – and noticed that the positions lay along a line. Using this method with two bodies, navigators were finally able to cross two position lines and obtain their position – in effect determining both latitude and longitude. Later in the 19th century came the development of the modern (Marcq St. Hilaire) intercept method; with this method the body height and azimuth are calculated for a convenient trial position, and compared with the observed height. The difference in arcminutes is the nautical mile \"intercept\" distance that the position line needs to be shifted toward or away from the direction of the body's subpoint. (The intercept method uses the concept illustrated in the example in the “How it works” section above.) Two other methods of reducing sights are the longitude by chronometer and the ex-meridian method.\n\nWhile celestial navigation is becoming increasingly redundant with the advent of inexpensive and highly accurate satellite navigation receivers (GPS), it was used extensively in aviation until the 1960s, and marine navigation until quite recently. However; since a prudent mariner never relies on any sole means of fixing his position, many national maritime authorities still require deck officers to show knowledge of celestial navigation in examinations, primarily as a backup for electronic/satellite navigation. One of the most common current usages of celestial navigation aboard large merchant vessels is for compass calibration and error checking at sea when no terrestrial references are available.\n\nThe U.S. Air Force and U.S. Navy continued instructing military aviators on celestial navigation use until 1997, because:\n\nThe United States Naval Academy announced that it was discontinuing its course on celestial navigation (considered to be one of its most demanding non-engineering courses) from the formal curriculum in the spring of 1998. In October 2015, citing concerns about the reliability of GPS systems in the face of potential hostile hacking, the USNA reinstated instruction in celestial navigation in the 2015–16 academic year.\n\nAt another federal service academy, the US Merchant Marine Academy, there was no break in instruction in celestial navigation as it is required to pass the US Coast Guard License Exam to enter the Merchant Marine. It is also taught at Harvard, most recently as Astronomy 2.\n\nCelestial navigation continues to be used by private yachtsmen, and particularly by long-distance cruising yachts around the world. For small cruising boat crews, celestial navigation is generally considered an essential skill when venturing beyond visual range of land. Although GPS (Global Positioning System) technology is reliable, offshore yachtsmen use celestial navigation as either a primary navigational tool or as a backup.\n\nCelestial navigation was used in commercial aviation up until the early part of the jet age; early Boeing 747s had a \"sextant port\" in the roof of the cockpit. It was only phased out in the 1960s with the advent of inertial navigation and doppler navigation systems, and today's satellite-based systems which can locate the aircraft's position accurate to a 3-meter sphere with several updates per second.\n\nA variation on terrestrial celestial navigation was used to help orient the Apollo spacecraft en route to and from the Moon. To this day, space missions such as the Mars Exploration Rover use star trackers to determine the attitude of the spacecraft.\n\nAs early as the mid-1960s, advanced electronic and computer systems had evolved enabling navigators to obtain automated celestial sight fixes. These systems were used aboard both ships and US Air Force aircraft, and were highly accurate, able to lock onto up to 11 stars (even in daytime) and resolve the craft's position to less than . The SR-71 high-speed reconnaissance aircraft was one example of an aircraft that used a combination of automated celestial and inertial navigation. These rare systems were expensive, however, and the few that remain in use today are regarded as backups to more reliable satellite positioning systems.\n\nIntercontinental ballistic missiles use celestial navigation to check and correct their course (initially set using internal gyroscopes) while flying outside the Earth's atmosphere. The immunity to jamming signals is the main driver behind this seemingly archaic technique.\n\nX-ray pulsar-based navigation and timing (XNAV) is an experimental navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to triangulate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter. On 9 November 2016 the Chinese Academy of Sciences launched an experimental pulsar navigation satellite called XPNAV 1. SEXTANT (Station Explorer for X-ray Timing and Navigation Technology) is a NASA-funded project developed at the Goddard Space Flight Center that is testing XNAV on-orbit on board the International Space Station in connection with the NICER project, launched on 3 June 2017 on the SpaceX CRS-11 ISS resupply mission.\n\nCelestial navigation trainers for aircraft crews combine a simple flight simulator with a planetarium.\n\nAn early example is the Link Celestial Navigation Trainer, used in the Second World War. Housed in a high building, it featured a cockpit accommodating a whole bomber crew (pilot, navigator and bombardier). The cockpit offered a full array of instruments which the pilot used to fly the simulated aeroplane. Fixed to a dome above the cockpit was an arrangement of lights, some collimated, simulating constellations from which the navigator determined the plane's position. The dome's movement simulated the changing positions of the stars with the passage of time and the movement of the plane around the earth. The navigator also received simulated radio signals from various positions on the ground. Below the cockpit moved \"terrain plates\" – large, movable aerial photographs of the land below – which gave the crew the impression of flight and enabled the bomber to practise lining up bombing targets. A team of operators sat at a control booth on the ground below the machine, from which they could simulate weather conditions such as wind or cloud. This team also tracked the aeroplane's position by moving a \"crab\" (a marker) on a paper map.\n\nThe Link Celestial Navigation Trainer was developed in response to a request made by the Royal Air Force (RAF) in 1939. The RAF ordered 60 of these machines, and the first one was built in 1941. The RAF used only a few of these, leasing the rest back to the US, where eventually hundreds were in use.\n\n"}
{"id": "4254743", "url": "https://en.wikipedia.org/wiki?curid=4254743", "title": "Chorography", "text": "Chorography\n\nChorography (from χῶρος \"khōros\", \"place\" and γράφειν \"graphein\", \"to write\") is the art of describing or mapping a region or district, and by extension such a description or map. This term derives from the writings of the ancient geographer Pomponius Mela and Ptolemy, where it meant the geographical description of regions. However, its resonances of meaning have varied at different times. Richard Helgerson states that \"chorography defines itself by opposition to chronicle. It is the genre devoted to place, and chronicle is the genre devoted to time\". Darrell Rohl prefers a broad definition of \"the representation of space or place\".\n\nIn his text of the \"Geographia\" (2nd century CE), Ptolemy defined geography as the study of the entire world, but chorography as the study of its smaller parts—provinces, regions, cities, or ports. Its goal was \"an impression of a part, as when one makes an image of just an ear or an eye\"; and it dealt with \"the qualities rather than the quantities of the things that it sets down\". Ptolemy implied that it was a \"graphic\" technique, comprising the making of views (not simply maps), since he claimed that it required the skills of a draftsman or landscape artist, rather than the more technical skills of recording \"proportional placements\". Ptolemy's most recent English translators, however, render the term as \"regional cartography\".\n\nPtolemy's text was rediscovered in the west at the beginning of the fifteenth century, and the term \"chorography\" was revived by humanist scholars. An early instance is a small-scale map of Britain in an early fifteenth-century manuscript, which is labelled a \"tabula chorographica\". John Dee in 1570 regarded the practice as \"an underling, and a twig of \"Geographie\"\", by which the \"plat\" [plan or drawing] of a particular place would be exhibited to the eye.\nThe term also came to be used, however, for \"written\" descriptions of regions. These regions were extensively visited by the writer, who then combined local topographical description, summaries of the historical sources, and local knowledge and stories, into a text. The most influential example (at least in Britain) was probably William Camden's \"Britannia\" (first edition 1586), which described itself on its title page as a \"Chorographica descriptio\". William Harrison in 1587 similarly described his own \"Description of Britaine\" as an exercise in chorography, distinguishing it from the historical/chronological text of Holinshed's \"Chronicles\" (to which the \"Description\" formed an introductory section). Peter Heylin in 1652 defined chorography as \"the exact description of some Kingdom, Countrey, or particular Province of the same\", and gave as examples Pausanias's \"Description of Greece\" (2nd century AD); Camden's \"Britannia\" (1586); Lodovico Guicciardini's \"Descrittione di tutti i Paesi Bassi\" (1567) (on the Low Countries); and Leandro Alberti's \"Descrizione d'Italia\" (1550).\n\nCamden's \"Britannia\" was predominantly concerned with the history and antiquities of Britain, and, probably as a result, the term chorography in English came to be particularly associated with antiquarian texts. William Lambarde, John Stow, John Hooker, Michael Drayton, Tristram Risdon, John Aubrey and many others used it in this way, arising from a gentlemanly topophilia and a sense of service to one's county or city, until it was eventually often applied to the genre of county history. A late example was William Grey's \"Chorographia\" (1649), a survey of the antiquities of the city of Newcastle upon Tyne. Even before Camden's work appeared, Andrew Melville in 1574 had referred to chorography and chronology as the \"twa lights\" [two lights] of history.\nHowever, the term also continued to be used for maps and map-making, particularly of sub-national or county areas. William Camden praised the county mapmakers Christopher Saxton and John Norden as \"most skilfull Chorographers\"; and Robert Plot in 1677 and Christopher Packe in 1743 both referred to their county maps as chorographies.\n\nBy the beginning of the eighteenth century the term had largely fallen out of use in all these contexts, being superseded for most purposes by either \"topography\" or \"cartography\". Samuel Johnson in his \"Dictionary\" (1755) made a distinction between geography, chorography and topography, arguing that geography dealt with large areas, topography with small areas, but chorography with intermediary areas, being \"less in its object than geography, and greater than topography\". In practice, however, the term is only rarely found in English by this date.\n\nIn more technical geographical literature, the term had been abandoned as city views and city maps became more and more sophisticated and demanded a set of skills that required not only skilled draftsmanship but also some knowledge of scientific surveying. However, its use was revived for a second time in the late nineteenth century by the geographer Ferdinand von Richthofen. He regarded chorography as a specialization within geography, comprising the description through field observation of the particular traits of a given area.\n\nThe term is also now widely used by historians and literary scholars to refer to the early modern genre of topographical and antiquarian literature.\n\n\n"}
{"id": "12348808", "url": "https://en.wikipedia.org/wiki?curid=12348808", "title": "Claim club", "text": "Claim club\n\nClaim clubs, also called actual settlers' associations or squatters' clubs, were a nineteenth-century phenomenon in the American West. Usually operating within a confined local jurisdiction, these pseudo-governmental entities sought to regulate land sales in places where there was little or no legal apparatus to deal with land-related quarrels of any size. Some claim clubs sought to protect squatters, while others defended early land owners. In the twentieth century, sociologists suggested that claim clubs were a pioneer adaptation of democratic bodies on the East Coast, including town halls.\n\nClaim clubs were essentially designed to \"do what politicians refused to do: make land available to needy settlers.\" Their general purpose was to protect the first settlers to arrive on unclaimed lands, particularly in their rights to speculate and cultivate. With the continuous availability of frontier lands from the 1830s through the 1890s, settlers kept moving west. Each claim club established its own rules of governance and enforcement; however, these were almost always vigilante actions. Period accounts report that in some areas, claim clubs were regarded with \"the same majesty of the law of the Supreme Court of the United States.\"\n\nEast Coast land speculators were prone to roam the recently opened Western United States and select the most desirable spots with the intent to outbid the settler and real claimant when the lands were offered for sale at the Land Office. Claim jumpers were also a problem. Generally they sought to be present at a land sale when the first claimant was not there. In many cases, when people who claimed land and then did not live on it and had not developed it with a shelter, fencing or other structures, \"claim jumpers\" would move in.\n\nThis was one scenario where claim clubs would enter. The absentee-owned land would be exploited directly and indirectly, or just simply seized with the title held \"by claim club.\" Members might vote expensive local improvements for the land, including roads and schoolhouses, and assign the heavy costs of development as a tax burden on the land held by absentees. This became the regular policy of some claim clubs, designed to force the sale by absentee owners to actual residents, or at least to local speculators.\n\nClaim clubs did not always protect the honest settler against the scheming speculator. Although claim club law sometimes shielded of the simple homesteader, it was also a tool and a weapon of the speculator. Claim clubs acted not only to protect a squatter's title to land he lived on and was cultivating, but also to help the same squatter defend unoccupied second and third tracts against the claim of later arrivals.\n\nThe institution of the claim club is said to have \"reached perfection\" in Iowa, where more than a hundred such groups carefully regulated land commerce until the United States government intervened.\n\nThe first claim club in the United States was established by settlers around Burlington, Iowa, where claims were formed soon after the American Revolutionary War. These clubs were established in direct violation of federal law, in what J. Sterling Morton described as a claim meeting According to one report, \"Such clubs sprang up 'as readily as did the sunflowers wherever the prairie sod was broken' in Illinois, Wisconsin, Iowa, Minnesota, Nebraska...\" Other reports corroborate the spread of claim clubs, with their presence felt in the aforementioned states, as well as New Mexico, North Dakota, Wyoming, Montana, Colorado and Washington.\n\nIn Colorado City, Colorado, the El Paso Claim Club was formed by members of the Colorado City Town Company in 1859. The Club reportedly \"settled land disputes and recorded real property transactions until federal government regulations provided for an official land office in 1862.\"\n\nThe Cañon City (Colorado) Claim Club first platted the town of Cañon City in 1860. The Club had six members, each of whom mined coal, iron, gypsum, marble and granite mining in the area, and Denver also had a claim club.\n\nThe Omaha Claim Club was founded in 1854, the year the city was founded. Initially designed to protect the interests of 20 men, it grew to include almost two hundred settlers. The group used violent means to impose \"frontier justice\", including dunking in the frozen Missouri River, running off legitimate settlers, and other forms of vigilantism. The club imposed their will on the Nebraska Territorial Legislature as well, and with the passage of a territorial law granting per settler, they doubled the federally imposed limit of . The club ran Omaha until the Supreme Court ruled against their violent measures in \"Baker v. Morton\", a hallmark in contract law cases.\n\nIn the 1854 the Bellevue (Nebraska) Claim Club was organized. The original aim of the club was \"to secure the peaceful adjustment of all cases in which claims in this then un-surveyed country overlapped each other.\" The club was renowned for visiting \"claim jumpers\" with beatings to convince them to leave their claims. Its last act, reportedly in 1858, was to attempt to tar and feather an old man and his three sons reportedly squatting in the area. The Platte Valley Claim Club was established in Fremont in August, 1856 to settle land disputes, and folded by late 1857.\n\nFort Saint Vrain, Nebraska Territory also had a claim club in the late 1850s that was designed to keep the town from failing. It did not succeed.\n\nOne story of claim club \"justice\" comes from Montgomery County, Kansas town in 1867. An early settler had tilled his land and improved on it, according to the provisions of the Homestead Act. However, he had not lived on it for five years. After he sold it to another man, this same settler reportedly went to the United States Land Office to preempt the man to whom he sold the land. After doing so this settler attempted to displace the man he sold the land to and claim it as his own. When the local claim club ordered the town sheriff to \"put the man away\", the original settler was never seen again.\n\nClaim clubs also secured members' stakes on land they speculated to become important to the federal government, for the purpose of selling it back to the government at a later date. Members of one Iowa claim club purchased 15,000 acres (61 km²) in central Iowa, which eventually was sold in order to develop both the state capitol in Des Moines and Iowa City, where the state university is today. \nIowa had several other claim clubs, as well. In Fort Des Moines, Fort Dodge, Iowa and Iowa City active clubs abounded. In Iowa City the club's mission was to \"...protect all persons who do or may hold claims, against the interference of any person who shall attempt to deprive such claim holders of their claims and improvements, by preemption or otherwise.\"\n\nFrom 1832-1843, Dupage County, Illinois created a number of claim clubs until the federal land surveyors arrived. In 1835 settlers in Elkhorn Creek, Wisconsin formed a claim club. Other settlers did the same, including the town of Yankton, South Dakota. There is also a report of a claim club in Alabama in the 1850s.\n\nIn the latter part of the 1850s claim clubs came under pressure from the federal government, and lost public support in many communities. In an 1858 ruling, the United States Department of the Interior addressed claim clubs directly, stating that, \"A member of a claim club, organized for the purpose of illegally appropriating and selling public lands, will be held to the strictest proof of honest intent, when asserting an individual claim.\"\n\nThe violent actions of the Omaha Claim Club may have brought about the demise of claim clubs across the country. In 1860, in \"Baker v. Morton\", the Supreme Court ordered that city's club to disband. Other sources say that with the arrival of several United States Land Offices across the West, the claim clubs simply were not needed.\n\nThe Omaha Claim Club, along with many claim clubs around Nebraska, disbanded by 1860.\n\n"}
{"id": "5232329", "url": "https://en.wikipedia.org/wiki?curid=5232329", "title": "Collective landscape", "text": "Collective landscape\n\nThe term collective landscape was introduced to landscape design and landscape planning by Sir Geoffrey Jellicoe. He wrote, on the dust jacket of his book:\n\nIt appears that the term was inspired by Carl Jung's use of the term collective unconscious. Jellicoe admired Jung, but the use of 'collective landscape' in the above quotation has something in common with its use in collectivism. Unlike the terms public park and national park, the term collective landscape is a psychological construct and an abstract concept.\n\n'Collective landscape' is therefore best understood as a landscape which:\n\nIn American contemporary landscape architecture the term 'collective landscape' is used to mean 'landscape which matters to the community' as in the following quotation from the \"Smokey Mountain News\" for the week of 10/12/05: \" \"The struggle to preserve a collective landscape heritage has been fought and won by the Cherokee once in the past, but is now at risk again.\" \" \n\nGeoffrey and Susan Jellicoe, \"The landscape of man: shaping the environment from prehistory to the present day\" (London:Thames and Hudson, 1975)\n\n"}
{"id": "55601685", "url": "https://en.wikipedia.org/wiki?curid=55601685", "title": "Common Operational Datasets", "text": "Common Operational Datasets\n\nCommon Operational Datasets or CODs, are authoritative reference datasets needed to support operations and decision-making for all actors in a humanitarian response. CODs are 'best available' datasets that ensure consistency and simplify the discovery and exchange of key data. The data is typically geo-spatially linked using a coordinate system (especially administrative boundaries) and have unique geographic identification codes (P-codes).\n\nCommon operation datasets are commonly used and referenced by all operations, they provide consistency among all actors working on humanitarian preparedness and response and enable a common operational picture (COP) of the crisis. P-codes facilitate the exchange and harmonization of data and information, they provide a geographic framework for data collection, analysis, and visualization (see P-code) and allow for a better interoperability and exchange between different actors. Therefore, common cperation cdatasets (CODs) reduce duplication of work on baseline data by partnering organizations and facilitate informed decision making both pre- and post-crisis.\n\nThe Information Management Network (IM Network) is an important component of the common operational datatset (COD) process. The IM Network can exist at country, regional and global levels and is composed of the humanitarian information management actors active in the country, region or at the global level. Potential IM Network actors include governments, United NAtions agencies and programmes, humanitarian cluster information management staff, international and national NGOs.\n\nCore CODs are required in all disaster-prone countries as a preparedness measure, including administrative boundaries (COD-AB), sex and age-disaggregated population data (COD-PS), and humanitarian profile (caseload or COD-HP). They are critical for information and data products and to underpin effective coordination. Core CODs enable effective risk analysis, needs assessment, decision-making, and reporting on all aspects of the response.\n\nAdministrative Boundary CODs are baseline geographical datasets that are used by humanitarian agencies during preparedness and response activities. They are preferably sourced from official government boundaries but when these are unavailable the information management network must develop and agree to a process to develop or adopt an alternate dataset. Administrative boundaries provide an essential data standard and are used directly and indirectly in almost every information product.\n\nThe administrative boundary dataset is key in preparedness and undergoes a review process to keep the data up to date.\n\nPopulation Statistics CODs are the baseline population figures of a country pre-crisis situation. They are preferably developed by a government during a census but can also be derived from estimated figures. When neither of these options are available the information management network must develop and agree to a process to develop a dataset that can be used by humanitarian agencies during preparedness and response activities.\n\nPopulation statistics are required to inform programming in humanitarian response. Specifically, they are used to estimate the potential number of affected people or as a reference/resource in the development of needs assessments and in analysis.\n\nHumanitarian profile (affected people, people in need)\n\nCountry-specific Common Operational Datasets are a subset of the CODs that are specific to each country’s risk profile. They are datasets for which it is essential that the humanitarian community uses the same version of the data as a reference. The purpose of a country-specific COD is to provide a common reference for the humanitarian community to create a common operational picture (spatial datasets); to allow for further understanding of the situation (statistical data, tabular data etc); or to aid with assessments. Ideally, Country-specific CODs are identified and agreed to as a preparedness activity but the list should be reviewed and revised (if required) at the onset of a crisis based on the situation at that time and the humanitarian needs).\n\n\n\n\n\n\n\nThe life cycle of a Common Operational Dataset (COD) is planning, collecting, processing, endorsing, communicating, maintaining.\n\nCommon Operational Datasets (CODs) are identified by the information management network (IM network) in a country or regionally. If there is no IM network in place all efforts are made at higher levels (e.g. global) to identify desired datasets. The IM network is responsible for collecting and maintaining the CODs and revising the datasets as required. CODs are made available online, if possible, prior to an emergency or shortly after disaster strikes.\n\nIM Network partners work together to develop a plan for all CODs including prioritization of datasets. The goal of the planning phase is to have an agreed upon list of commonly used datasets by partners. Once clear goals and objectives are identified, work can begin on the identification of sources and datasets.\n\nCollecting CODs is the process of locally acquiring datasets, and includes collection of metadata. The second part of the collection process is the evaluation phase that includes an examination of potential sources. The evaluation involves a quick in-country quality assurance check to ensure (potential) compliance with the minimum standard of data characteristics (spatial and attribute) and metadata. The process identifies potential problems or opportunities with datasets that should be considered when deciding what dataset should become the COD.\n\nIn the process phase the quality of the dataset is improved to the best standard possible. The processing phase cleans and standardizes the datasets and can be the most resource and time intensive stage of the COD cycle if the quality of the data is low.\n\nThe endorsement phase has 2 steps: validation and endorsement (in the country and at the global level).\n\nThe validation phase is a technical review on the Candidate COD and is done to ensure that the corrections made in the processing phase have created the best available COD.\n\nThe endorsement phase of CODs is the defining moment of the COD cycle as it is at this stage that operational partners agree that the candidate COD is going to be the referential dataset for humanitarian preparedness and response activities.\n\nThe Communication phase includes agreed upon means of sharing the dataset and also transparency about changes of the data in the process, available metadata and advocation for the use of common operational datatsets.\n\nMaintenance is an important step in the COD cycle as it ensures that the datasets are still relevant and accurate for humanitarian use. This is done at least once a year or in the timespan agreed upon.\n\nPlace Codes (P-Codes) are found in Administrative Boundary CODs. They are unique geographic (geo) identification codes, represented by combinations of letters and/or numbers to identify a specific location or feature on a map or within a database. For a specific place, point, or positional locations, the geo-codes have come into common usages as P-codes (abbreviated for Place-code). These terms can be essentially interchanged as long as one recognizes the focus on “position or place” for P-codes. They are also used to provide unique reference codes to refer to settlements or administrative boundaries in other datasets.\n\nWhilst some data-sets remain relatively constant (e.g. geographical features and administrative boundaries) others change (assessed disaster impacts and needs) and require to be regularly updated.\n\nCODs are intended to be used universally to improve coordination in humanitarian action; to build a common operational picture enabling more consistent activity and reduce duplication of data collection. The main source of curated CODs is the Humanitarian Data Exchange, though CODs may also be found on various governmental and independent websites.\n\nEssential Reading for OCHA IMOs\n\n\nTutorial\n\n\nAdvocacy Resources\n\n"}
{"id": "4706497", "url": "https://en.wikipedia.org/wiki?curid=4706497", "title": "County surveyor", "text": "County surveyor\n\nA county surveyor is a public official in many counties of the United States. At the bottom of this page are working \"External Links\" as at 4 November 2011 to websites of a selection of such County Surveyor's departments. Most of these officials are elected on the partisan ballot to four-year terms. They administer the county land survey records, re-establish and maintain the official government survey monuments, and review property boundaries surveys and subdivision plans. Other duties vary from state to state. Those marked with an asterisk (*) are nominated by the National Association of County Surveyors (NACS).\n\nNACS is part of the National Association of Counties of the USA (NACo). The NACo website sets out its perception of the history of county government in the USA, tracing it to Anglo-Saxon England (initial division of land into holdings for government purposes called 'shires', hence 'shire-reeve', the origin of 'sheriff'), Anglo-Norman feudalism (renaming shires conquered by William I as 'counties' and establishing his allodial title to them via the Domesday Book survey), and the increasingly \"plural executive structure\" commissioned by his successors to the royal throne of England to defend the peace and enforce the complex of chivalric, common, and statutory laws of England (and of Wales from the reign of Edward I) up to the time of the first county government established in America (County of James City, Virginia). This triad of origins is fundamental to understanding the organisation role that county surveying plays in the administration and development of the real estate of many states and nations around the world, even though sometimes it goes by other names. It was the framework that the King of England applied to his colonies in America and sufficiently successful as to have since been adopted by many other states.\n\nChapter V 'The Development of an Extra-Legal Constitution', of 'English Local Government from the Revolution to the Municipal Corporations Act: The Parish and The County' by Sidney Webb and Beatrice Potter Web, describes the increasing chaos that began to prevail within this same period on the 'county surveying' front in England and Wales. Eventually, the military defence component of county surveying in the UK began to separate from the civil in 1791, with the Crown's 'Board of Ordnance' being commissioned to carry out a comprehensive survey of the South Coast of England which, as a result of 'the last invasion of Britain 1797', at Fishguard in South West Wales ultimately extended to all of the UK. With that shift in emphasis, county surveying began to concentrate more on its civil engineering and civic architecture roles, producing the historically famous British county surveyors such as Thomas Telford, John Loudon McAdam and John Nash; the expression, \"County Surveyor\", became a UK statutory title (Bridges Act 1803); and, in England and Wales, its incumbents were appointed by elected councils as of the coming into effect of the Local Government Act 1888 rather than being Crown-appointed by Justices of the Peace.\n\nThe UK equivalent of NACS, namely, the County Surveyors Society (CSS) founded in 1885, was subsumed into the pluralistic Association of Directors of Environment, Economy, Planning and Transport (ADEPT) in 2010.\n\nCalifornia\n\nColorado\n\nFlorida\n\nIdaho\n\nIndiana\n\nMichigan\n\nMinnesota\n\nNebraska\n\nOregon\n\nUtah\n\nWashington\n\nWisconsin\n"}
{"id": "39533411", "url": "https://en.wikipedia.org/wiki?curid=39533411", "title": "European green infrastructure", "text": "European green infrastructure\n\nThe European green infrastructure is an important part of the new (post-2010) EU strategy for biodiversity and biodiversity policy.\n\nIt is one of the main tools to tackle threats on biodiversity resulting from habitat fragmentation, land use change and loss of habitats.\nGreen Infrastructure will play a decisive role in integrating biodiversity into other policies, such as agriculture, forestry, water, marine and fisheries, regional and cohesion policy, climate change mitigation and adaptation, transport, energy and land use policy. It is also an important tool for existing Directives such as the Water Framework Directive, the Marine Framework Directive, Environmental Impact Assessment and Strategic Environment Assessment Directives.\n\nIn addition, particular attention will be given to strengthening the integration of green infrastructure aspects in the EU’s various funding programmes (e.g. structural and cohesion funds, CAP, LIFE) over the current and future financial programming period starting in 2013 and to improving the ecological coherence of the Natura 2000 Network.\n\nThe continued development of Geographic Information Systems (GIS) and their increasing level of use is particularly important in the development of Green Infrastructure plans. The plans frequently are based on GIS analysis of many layers of geographic information.\n\n\n"}
{"id": "13348166", "url": "https://en.wikipedia.org/wiki?curid=13348166", "title": "Federal Geographic Data Committee", "text": "Federal Geographic Data Committee\n\nThe Federal Geographic Data Committee (FGDC) is a United States government committee which promotes the coordinated development, use, sharing, and dissemination of geospatial data on a national basis. Its 32 members are representatives from the Executive Office of the President, and Cabinet level and independent Federal agencies. The Secretary of the Department of the Interior chairs the FGDC, with the Deputy Director for Management, Office of Management and Budget (OMB) as Vice-Chair.\n\nThe FGDC coordinates the sharing of geographic data, maps, and online services through an online portal, \"geodata.gov\", that searches metadata held within the NSDI Clearinghouse Network.\n\nOn July 23, 2009, the Energy and Mineral Resources Subcommittee of the House Natural Resources Committee held an oversight hearing on federal geospatial data management. Rep. John Sarbanes of Maryland quoted a U.S. General Accounting Office (GAO) report from his briefing material saying that only 4 of the FGDC member agencies were in compliance.\n"}
{"id": "1508112", "url": "https://en.wikipedia.org/wiki?curid=1508112", "title": "Four continents", "text": "Four continents\n\nEuropeans in the 16th century divided the world into four continents: Africa, America, Asia and Europe. Each of the four continents was seen to represent its quadrant of the world—Europe in the north, Asia in the east, Africa in the south, and America in the west. This division fit the Renaissance sensibilities of the time, which also divided the world into four seasons, four classical elements, four cardinal directions, four classical virtues, etc.\n\nThe four parts of the world or the four corners of the world refers to the Americas (the \"west\"), Europe (the \"north\"), Asia (the \"east\"), and Africa (the \"south\"). \n\nBefore the discovery of the New World a commonplace of classical and medieval geography had been the \"three parts\" in which, from Mediterranean and European perspectives, the world was divided: Europe, Asia and Africa. As Laurent de Premierfait, the pre-eminent French translator of Latin literature in the early fifteenth century, informed his readers:\nAsia is one of the three parts of the world, which the authors divide in Asia, Africa and Europe. Asia extends towards the Orient as far as the rising sun (\"devers le souleil levant\"), towards the south (\"midi\") it ends at the great sea, towards the occident it ends at our sea, and towards the north (\"septentrion\") it ends in the Maeotian marshes and the river named \"Thanaus\".\n\nFor Laurent's French readers, Asia ended at \"our sea\", the Mediterranean; Europeans were only dimly aware of the Ural Mountains, which divide Europe from Asia in the eyes of the modern geographer, and which represent the geological suture between two fragmentary continents, or cratons. Instead, the division between these continents in the European-centered picture was the Hellespont, which neatly separated Europe from Asia. From the European perspective, into the Age of Discovery, Asia began beyond the Hellespont with Asia Minor, where the Roman province of Asia had lain, and stretched away to unimaginably exotic and distant places— \"the Orient\".\n\nIn the sixteenth century America too was full of exotic promise: the \"New World\".\n\nIn 1603, Cesare Ripa published a book of emblems for the use of artists and artisans who might be called upon to depict allegorical figures. He covered an astonishingly wide variety of fields, and his work was reprinted many times. It was still being brought up-to-date in the 18th century. The illustrations reveal fixed Eurocentric perceptions of the nature of the \"four corners of the world.\" Ripa's \"Europe\" (\"illustration, left\") is the land of abundance (cornucopia) of kings and the pope, whose crowns and the papal tiara lie at her feet, and of cities.\n\n\"Africa\", by contrast (\"illustration, below right\") wears the elephant headdress (worn by rulers depicted on Hellenistic Bactrian coins) and is accompanied by a lion, the scorpion of the desert sands and Cleopatra's asps. \"Asia\" (\"illustration, right\"), the seat of Religion, carries a smoking censer as a camel takes its ease.\n\nAnd the iconic image of \"America\" (\"illustration, below left\") shows a Native American maiden in a feathered headdress, with bow and arrow. Perhaps she represents a fabled Amazon from the river that already carried the name.\n\nThe American millionaire philanthropist James Hazen Hyde, who inherited a majority share in Equitable Life Assurance Society, formed a collection of allegorical prints illustrating the Four Continents that are now at the New-York Historical Society; Hyde's drawings and a supporting collection of sets of porcelain table ornaments and other decorative arts illustrating the Four Continents were shared by various New York City museums.\nThe Renaissance associated one major river to each of the continents.\nThe Four Rivers theme appears for example in the Fontana dei Quattro Fiumi in the Piazza Navona in Rome.\n\nWith the confirmed discovery that Australia was an island continent, the theme of the \"Four Continents\" lost much of its drive, long before a sixth continent, Antarctica, was discovered. The iconography survived as the Four Corners of the World, however, generally in self-consciously classicizing contexts: for instance, in New York, in front of the Beaux-Arts Alexander Hamilton U.S. Custom House (1907), four sculptural groups by Daniel Chester French symbolize the \"Four Corners of the World.\"\n\n\n\n"}
{"id": "8556113", "url": "https://en.wikipedia.org/wiki?curid=8556113", "title": "Geography Cup", "text": "Geography Cup\n\nThe Geography Cup is an online, international competition between the United States and the United Kingdom, with the aim of determining which nation collectively knows more about geography. It was also intended to raise awareness of the importance of world geography in the modern world. Any resident of the U.S. or UK could participate to test their own geographic knowledge and to support their nation. The first (and so far only) competition took place between 12 November and 31 December 2006 to coincide with Geography Awareness Week, the third week in November.\n\nThe Geography Cup was created in late 2006 by two geography enthusiasts: Roger Andresen from Atlanta, Georgia, and Daniel Raven-Ellison from Reading, Berkshire. Both became committed to expanding public awareness of the importance of geography after seeing the results of polls illustrating Americans' and Britons' startling lack of geographical knowledge. A poll of young Americans conducted in the United States by National Geographic in 2006 found that only 37% of those polled could find Iraq on a map, and half could not even find the state of New York.\nAndresen and Raven-Ellison created the competition for two main reasons. One reason was to determine whether people who live in the United States or people who live in the United Kingdom knew more about world geography. However, the main reason was to encourage people to learn more about world geography and to understand the importance of geography. The two creators plan to hold the competition yearly beginning in the third week in November to coincide with Geography Awareness Week, established in the United States by Ronald Reagan in 1987.\n\nAny citizen of the USA or the UK was eligible to participate in the contest. Upon signup, the website determined which country a user lived in based on their IP address and automatically put the user into the correct team. The quiz consisted of thirteen random, geography-related questions that had to be answered in a total of two minutes. The first ten questions consisted of placing randomly selected countries onto a blank political map. The final three questions consisted of geography-related trivia questions that were answered by selecting the correct country on the blank map. Each user could practice, affect their own score, and learn about geography by playing as many times as they wanted to, but only three plays per 24-hour period could affect their nation's score.\n\nThe randomly selected countries could be any country (or territory) in the world. This included the large and well-known regions (Russia, Canada, United States, etc.) as well as smaller and more obscure nations (Palau, Djibouti, Togo, etc.). Most participants had little trouble recognizing the major European and Asian countries but many people on both teams had trouble locating small Pacific Islands and some African countries.\n\nThe trivia questions were sometimes related directly to geography, e. g., \"Made up of 33 atolls, which country straddles both the equator and the International Date Line?\". However, some were about current events to test if participants knew where, geographically, news stories were taking place, e. g., \"Which Asian country recently made its first test of a nuclear weapon?\".\n\nDue to the large difference in population between the United States and the United Kingdom, the score of each team was determined by an average percentage based on the percentage scores of each individual player. After each game, a player could see how he or she affected their nation's score and their own personal percentage. Because there were 13 questions, each question was worth approximately 7.7%.\n\nThe USA won with a final accuracy score of 62.368% after trailing for a few days, while the UK lagged behind slightly at the end with an accuracy of 57.947%. The USA had 41,885 participants while the UK team had 10,820, indicating about 30% greater per capita participation in the UK.\n\nUsers in Britain using Internet Explorer reported some problems with the website. Also, some US and UK residents were rejected or put into the incorrect team because their internet service provider used overseas servers. Finally, after the surge of media attention following the Reuters and Christian Science Monitor articles, the number of users exceeded the capacity of the servers, causing a temporary suspension of the competition.\n\nThe Geography Cup was sponsored by A Broader View, founded by Roger Andresen to decrease geographical ignorance with geography puzzles and games. It is also sponsored by Give Geography Its Place, founded by Daniel Raven-Ellison and David Rayner to raise awareness of the importance geography in UK media. The third sponsor is Nelson Thornes, a UK-based educational publisher owned by Wolters Kluwer.\n\nThe Geographic Alliance in Nevada (GAIN), part of National Geographic EdNet, encouraged its members to participate in the competition in the belief that \"It is only fitting that these two countries compete in a geography competition, as they both share the same widely-publicized affliction.\" Polls in the United States, including the National Geographic-Roper poll, and similar polls in the UK, which include a magazine poll cited by the BBC, illustrate a glaring lack of geographic knowledge by young people on both sides of the Atlantic Ocean. In addition, a group of geography teachers and enthusiasts from the UK on a Staffordshire Learning Net geography forum supported the competition and encouraged students to try to improve their nation's score.\n\n"}
{"id": "33867932", "url": "https://en.wikipedia.org/wiki?curid=33867932", "title": "Geospatial topology", "text": "Geospatial topology\n\nGeospatial topology studies the rules concerning the relationships between the points, lines, and polygons that represent the features of a geographic region. For example, where two polygons represent adjacent counties, typical topological rules would require that the counties share a common boundary with no gaps and no overlaps. Similarly, it would be nonsense to allow two polygons representing lakes to overlap.\n\nIn \"spatial analysis\" the topological spatial relations are derived from the DE-9IM model, as spatial predicates about relations between points, lines, and/or areas: Equals, Contains, Covers, CoveredBy, Crosses, Disjoint, Intersects, Overlaps, Touches and Within. In network and graph representations the topology analysis is about topological objects such as faces, edges and nodes.\n\nThe ESRI White Paper \"GIS Topology\" explains that topology operations are used to manage shared geometry, define and enforce data integrity rules, support topological relationship queries and navigation, and build more complex shapes such as polygons, from primitive ones such as lines. A \"GIS for Educators\" worksheet at Linfiniti adds the detection and correction of digitising errors and carrying out network analysis. Topological error correction is explained in more detail in a paper by Ubeda and Egenhofer.\n\nUnlike GML, topologies are not directly represented in ESRI shapefiles which store individual geometric objects in isolation. Topological processing can, however, be undertaken in GIS software such as GRASS GIS or QGIS or could in principle be enforced using integrity constraints in a GIS-enabled DBMS such as PostGIS. However, as Riedemann (2004) explains, topological operators are inherently complex and their implementation requires care to be taken with usability and conformance to standards.\n\nOracle and PostGIS provide fundamental topological operators allowing applications to test for \"such relationships as contains, inside, covers, covered by, touch, and overlap with boundaries intersecting.\" Unlike the PostGIS documentation, the Oracle documentation draws a distinction between \"topological relationships [which] remain constant when the coordinate space is deformed, such as by twisting or stretching\" and \"relationships that are not topological [which] include length of, distance between, and area of.\" These operators are leveraged by applications to ensure that data sets are stored and processed in a topologically correct fashion.\n"}
{"id": "20290597", "url": "https://en.wikipedia.org/wiki?curid=20290597", "title": "Grassed waterway", "text": "Grassed waterway\n\nA grassed waterway consists in a to 48-metre-wide (157 ft) native grassland strip of green belt. It is generally installed in the thalweg, the deepest continuous line along a valley or watercourse, of a cultivated dry valley in order to control erosion. A study carried out on a grassed waterway during 8 years in Bavaria showed that it can lead to several other types of positive impacts, e.g. on biodiversity.\n\nConfusion between \"grassed waterway\" and \"vegetative filter strips\" should be avoided. The latter are generally narrower (only a few metres wide) and rather installed along rivers as well as along or within cultivated fields. However, buffer strip can be a synonym, with shrubs and trees added to the plant component, as does a riparian zone.\n\nRunoff generated on cropland during storms or long winter rains concentrates in the thalweg where it can lead to rill or gully erosion. \n\nRills and gullies further concentrate runoff and speed up its transfer, which can worsen damage occurring downstream. This can result in a muddy flood. \n\nIn this context, a grassed waterway allows increasing soil cohesion and roughness. It also prevents the formation of rills and gullies. Furthermore, it can slow down runoff and allow its re-infiltration during long winter rains. In contrast, its infiltration capacity is generally not sufficient to reinfiltrate runoff produced by heavy spring and summer storms. It can therefore be useful to combine it with extra measures, like the installation of earthen dams across the grassed waterway, in order to buffer runoff temporarily.\n\n"}
{"id": "1064904", "url": "https://en.wikipedia.org/wiki?curid=1064904", "title": "Grid north", "text": "Grid north\n\nGrid north is a navigational term referring to the direction northwards along the grid lines of a map projection. It is contrasted with true north (the direction of the North Pole) and magnetic north (the direction in which a compass needle points). Many topographic maps, including those of the United States Geological Survey and Great Britain's Ordnance Survey, indicate the difference between grid north, true north, and magnetic north.\n\nThe grid lines on Ordnance Survey maps divide the UK into one-kilometre squares, east of an imaginary zero point in the Atlantic Ocean, west of Cornwall. The grid lines point to a Grid North, varying slightly from True North. This variation is zero on the central meridian (north-south line) of the map, which is at two degrees West of the Prime Meridian, and greatest at the map edges. The difference between grid north and true north is very small and can be ignored for most navigation purposes. The difference exists because the correspondence between a flat map and the round Earth is necessarily imperfect.\n\nAt the South Pole, grid north conventionally points northwards along the Prime Meridian. Since the meridians converge at the poles, true east and west directions change rapidly in a condition similar to gimbal lock. Grid north solves this problem.\n"}
{"id": "1809289", "url": "https://en.wikipedia.org/wiki?curid=1809289", "title": "Hardscape", "text": "Hardscape\n\nHardscape refers to hard landscape materials in the built environment structures that are incorporated into a landscape. This can include paved areas, driveways, retaining walls, sleeper walls, stairs, walkways, and any other landscaping made up of hard wearing materials such as wood, stone, concrete etc. as opposed to softscape, the horticultural elements of a landscape.\n\nHard landscaping involves projects that cover the entirety of the yard and that are necessary before soft landscaping features come into play. Hard landscaping is very much altering the foundation of the yard, the \"bricks and mortar\", and so only once this is completed can the landscaper begin to focus on the more beautiful features of the yard, hard-soft landscaping such as the floral arrangements, trees and shrubs and perfecting color schemes. One key feature of hard landscaping is to do with the absorption of water – something that is of great importance given the climate. Hard landscaping ensures that worrying about water after heavy rain or snowfall is not an issue. The right water absorption and irrigation system installed through hard landscaping, coupled with hard materials that safely move water away from the property can ensure that soil movement is never a problem and that the yard stays a drier, enjoyable living space, rather than a wet and muddy bog. There are soft landscaping options that can help to achieve this, but the bulk of this is achieved through hard landscaping.\n\nFrom an urban planning perspective, hardscapes can include very large features, such as paved roads, driveways or fountains, and even small pools. Most water features are hardscapes because they require a barrier to retain the water, instead of letting it drain into the surrounding soil.\n\nHardscaping allows the erection of man-made landscaping features that would otherwise be impossible due to soil erosion, including some that compensate for large amounts of human traffic that would cause wear on bare earth or grass. For example, sheer vertical features are possible.\n\nWithout nearby bare soil, or natural drainage channels or culverts, hardscape with an impervious surface requires artificial methods of drainage or surface runoff to carry off the water that would normally be absorbed into the ground as groundwater and prevent premature wear to itself. Lack of capacity, or poorly planned or executed drainage or grading of the surface can cause problems after severe storms or heavy extended periods of rain fall, such as flooding, washout, mud flows, sink holes, accelerated erosion, wet rot to wood elements, drowning of plants trees and shrubs, and even foundation problems to an adjacent home such as cracking the foundation, basement flooding due to water infiltration, and pest infiltration, such as ants and other insects entering through damaged areas.\n\nHardscape landscaping in Queensland, Australia is a licensed qualification called Structural Landscaping, which is divided into two classes of licenses: Trade Contractor Structural Landscaper, and a Builder restricted to Structural Landscaping, referred to as the \"jack of all trades\" due to its large scope of works. These Structural Landscaping licenses include the erection and fabrication of decking, fences, carports, pergolas, paving and the construction of retaining walls.\n\n"}
{"id": "32038850", "url": "https://en.wikipedia.org/wiki?curid=32038850", "title": "Indices of deprivation 2010", "text": "Indices of deprivation 2010\n\nThe Indices of Deprivation 2010 (ID 2010) is a deprivation index at the small area level, created by the British Department for Communities and Local Government (DCLG) and released on 24 March 2011. It follows the ID2007 and because much of the datasets are the same or similar between indices allows a comparison of \"relative deprivation\" of an area between the two indices.\n\nWhile it is known as the ID2010, most of the data actually dates from 2008.\n\n\nAccording to the research, the most deprived area in the country is in the village of Jaywick on the Essex coast.\n"}
{"id": "186446", "url": "https://en.wikipedia.org/wiki?curid=186446", "title": "Intermontane", "text": "Intermontane\n\nIntermontane is a physiographic adjective formed from the prefix \"inter-\" (\"\"signifying among, between, amid, during, within, mutual, reciprocal\") and the adjective \"montane\" (\"inhabiting, or growing in mountainous regions, especially cool, moist upland slopes below the timberline.\") \n\nThe corresponding \"physiographic\" noun is intermountain, while the noun \"intermontane\" is an \"ecologic\" noun meaning \"among, between, amid, or within \"flora and fauna of a montane habitat.\"\" As an example, an alpine region would be an intermontane for a species that migrates between a glacial region and a subalpine region.\n\n\nIn palaeogeography, intermontane may refer to\n\n"}
{"id": "53091648", "url": "https://en.wikipedia.org/wiki?curid=53091648", "title": "José Antonio Sosa", "text": "José Antonio Sosa\n\nJosé Antonio Sosa Díaz-Saavedra was brought up in a family with an interest in art and history, especially in relation to the Canary Islands. As a result, he was encouraged to study architecture, graduating from the Superior Technical School of Architecture of Madrid in 1981. In 1994, he earned a Ph.D at the School of Architecture, University of Las Palmas de Gran Canaria.\n\nAfter completing his doctoral studies, he obtained the chair of Architectural Projects in the Department of Graphic Expression and Architectural Projects of the EALPGC, School of Architecture of Las Palmas de Gran Canaria, where he has been since 1983.\n\nIn the year 2000 he was \"Visiting Scholar\" and taught in the Department of Architecture, belonging to the Graduate School of Design, Harvard University, he has also participated as a jury in several universities among which is the Swiss Federal Institute of Technology, ETH (Studio Basel).\n\nIn 2015 he joined the University Institute of Intelligent Systems and Numerical Applications in Engineering of the ULPGC, University of Las Palmas de Gran Canaria.\n\nJosé Antonio Sosa has been the main proponent of modern architectural development in the early twentieth century in the Canary Islands and Spain, promoting its place in the history of urban and cultural heritage. As a result, he was appointed a member of the Royal Spanish Academy.\n\nIn 1996 Professor Sosa joined his studio to Magüi González creating the Nred Arquitectos Group, where several projects stand out: the City of Justice (New Law Courts Headquarters) and the rehabilitation of the Town Halls (Town Hall Houses) of Las Palmas de Gran Canaria.\n\nSince 2011, he has founded a new studio for architecture and urban planning together with Evelyn Alonso Rohner.\n\nIn 2015 José Antonio Sosa is the managing editor of the Arquiatesis editorial line for the publication and publication of theses and research works in architecture.\n\nSosa, has participated in the following exhibitions:\n\n\nProfessor Sosa has been awarded in the following competitions:\n\n\nSome of them are:\n\n\nSosa has published the following texts:\n\n\nAmong them are:\n\n"}
{"id": "160420", "url": "https://en.wikipedia.org/wiki?curid=160420", "title": "Juan Rodríguez Cabrillo", "text": "Juan Rodríguez Cabrillo\n\nJuan Rodríguez Cabrillo (1497 – January 3, 1543) was a Spanish explorer born in Palma del Rio, Córdoba, Spain, although he is also claimed by tradition as a native of Portugal. Among other things he was a maritime navigator known for exploring the West Coast of North America on behalf of the Spanish Empire. Cabrillo was the first European to navigate the coast of present-day California. He is best known for his exploration of the coast of California in 1542–1543. Juan Rodríguez Cabrillo served under the command of Pánfilo de Narváez and aided him in the conquest of Cuba about 1518.\n\nCabrillo's nationalityPortuguese or Spanishhas been debated for centuries. He was described as Portuguese by Spanish chronicler Antonio de Herrera y Tordesillas; in his \"Historia General de los hechos de los Castellanos en las Islas y tierra firme del Mar Oceano\", written 60 years after Cabrillo's death, Herrera referred to Cabrillo as \"Juan Rodriguez Cabrillo Português\". Several locations in Portugal claim to be his birthplace (e.g. Lapela (Cabril), Portugal). However, the source for Herrera's description is unknown.\n\nSome historians have long believed that Cabrillo was from Spain, and a set of documents discovered in 2015 gave strength to that opinion. A witness from a 1532 lawsuit, named Juan Rodriguez Cabrillo, testified under oath that he was born in Palma de Micergilio (now Palma del Río), a town in the province of Córdoba in Spain. Other details of the witness's biography match known facts about the explorer. A leader of San Diego's Portuguese community cautioned that the new evidence must be carefully evaluated, and requested that copies of the documents be turned over to the Portuguese government for study.\n\nLapela, in the parish of Cabril and a municipality of Montalegre (Portugal), is the region where allegedly the nickname \"Cabrilha\" originated. It became the surname Cabrilho and was pronounced at the time \"Cabrilhe\" in Galician and \"Cabrillo\" in Spanish, according to the historian João Soares Tavares, biographer of João Rodrigues Cabrilho. The name still exists in Portugal as a surname, and several localities named Cabril in Beira Alta and neighboring regions as Castro Daire, Viseu or Pampilhosa da Serra have been claimed as Cabrillo's birthplace. In Lapela there is an ancient house where local tradition claims he was born. Local people, and alleged local descendants of branches of his ancient family with the same surname (\"Rodrigues Cabrilho\"), call the house \"Casa do Galego\" (\"House of the Galician\") and \"Casa do Americano\" (\"House of the American\").\n\nCabrillo shipped for Havana as a young man and joined forces with Hernán Cortés in Mexico (then called New Spain). Later, his success in mining gold in Guatemala made him one of the richest of the conquistadores in Mexico. According to his biographer Harry Kelsey, he took an indigenous woman as his common-law wife and sired several children, including at least three daughters. Later he married Beatriz Sanchez de Ortega in Seville during a hiatus in Spain. She returned to Guatemala with him and bore him two sons.\n\nCabrillo benefited from the encomienda system that enslaved the Native peoples of the Americas. In Honduras, for example, he broke up families, sending the men to the mines for gold and to the forest to harvest materials he needed for ship building. The women and girls he gave over to his soldiers and sailors, presumably as slaves.\n\nHe accompanied Francisco de Orozco to subdue the indigenous Mixtec people at what would eventually become the city of Oaxaca, in Mexico. Little is known of what Cabrillo did there.\n\nIn 1539, Francisco de Ulloa, who had been commissioned by Cortés, discovered the Gulf of California and reached nearly as far north as the 30th parallel. Cabrillo was then commissioned by the new Viceroy of New Spain, Antonio de Mendoza, to lead an expedition up the Pacific coast in search of trade opportunities, perhaps to find a way to China (for the full extent of the northern Pacific was unknown) or to find the mythical Strait of Anián (or Northwest Passage) connecting the Pacific Ocean with Hudson Bay. Cabrillo built and owned the flagship of his venture (two or three ships), and stood to profit from any trade or treasure.\nIn 1540 the fleet sailed from Acajutla, El Salvador, and reached Navidad, Mexico on Christmas Day. While in Mexico, Pedro de Alvarado went to the assistance of the town of Nochistlán, which was under siege by hostile natives, and was killed when his horse fell on him, crushing his chest. Following Alvarado's death, the viceroy took possession of Alvarado's fleet. Part of the fleet was sent off to the Philippine Islands under Ruy Lopez de Villalobos and two of the ships were sent north under the command of Cabrillo.\n\nOn June 27, 1542, Cabrillo set out from Navidad with three ships: the 200-ton galleon and flagship \"San Salvador\", the smaller \"La Victoria\" (c. 100 tons), and the lateen-rigged, twenty-six oared \"fragata\" or \"bergantin\" \"San Miguel\". On August 1, Cabrillo anchored within sight of Cedros Island. Before the end of the month they had passed Baja Point (named \"Cabo del Engaño\" by de Ulloa in 1539) and entered \"uncharted waters, where no Spanish ships had been before\". On September 28, he landed in what is now San Diego Bay and named it \"San Miguel\". A little over a week later he reached Santa Catalina Island (October 7), which he named \"San Salvador\", after his flagship. On sending a boat to the island \"a great crowd of armed Indians appeared\" — whom, however, they later \"befriended\". Nearby San Clemente Island was named \"Victoria\", in honor of the third ship of the fleet. The next morning, October 8, Cabrillo came to San Pedro Bay, which was named \"Baya de los Fumos\" (English: Smoke Bay). The following day they anchored overnight in Santa Monica Bay. Going up the coast Cabrillo saw Anacapa Island, which they learned from the Indians was uninhabited. The fleet spent the next week in the islands, mostly anchored in Cuyler Harbor, a bay on the northeastern coast of San Miguel Island. On October 18 the expedition saw Point Conception, which they named \"Cabo de Galera\".\n\nCabrillo's expedition recorded the names of numerous Chumash villages on the California coast and adjacent islands in October 1542 — then located in the two warring provinces of \"Xexo\" (ruled by an \"old woman\", now Santa Barbara County, California) and \"Xucu\" (now Ventura County, California).\n\nOn November 13 they sighted and named \"Cabo de Piños\" (possibly either Point Pinos or Point Reyes), but missed the entrance to San Francisco Bay, a lapse that mariners would repeat for the next two centuries and more. The expedition reached as far north as the Russian River before autumn storms forced them to turn back. Coming back down the coast, Cabrillo entered Monterey Bay, naming it \"Bahia de Los Piños\".\n\nOn November 23, 1542, the little fleet arrived back in \"San Salvador\" (Santa Catalina Island) to overwinter and make repairs. There, around Christmas Eve, Cabrillo stepped out of his boat and splintered his shin when he stumbled onto a jagged rock while trying to rescue some of his men from attacking Tongva warriors. The injury became infected and developed gangrene, and he died on January 3, 1543 and was buried. A possible headstone was later found on San Miguel Island. His second-in-command brought the remainder of the party back to Navidad, where they arrived April 14, 1543.\n\nA notary's official report of Cabrillo's expedition was lost; all that survives is a summary of it made by another investigator, Andrés de Urdaneta, who also had access to ships' logs and charts. No printed account of Cabrillo's voyage appeared before historian Antonio de Herrera's account early in the 17th century.\n\nHis discoveries went largely unnoticed at the time, so none of his place names were permanently adopted. Despite this, Cabrillo is now remembered as the first European to travel the California coast, and many parks, schools, buildings and streets in California bear his name.\n\nMost notably, the National Park Service operates Cabrillo National Monument, overlooking the bay and ocean from Point Loma in San Diego, commemorating his first landing in California and offering views of San Diego and the Pacific Ocean. The monument features a larger-than-life statue of Cabrillo, donated by the government of Portugal, as well as a plaque honoring him donated in 1935 by the Portuguese ambassador to the United States. A museum in the park focuses on Cabrillo and his voyages of discovery. Every September Cabrillo Festival Inc. hosts the Cabrillo Festival, an annual three-day celebration of his discovery of San Diego Bay, including a re-enactment of his landing at Ballast Point.\n\nAnother Cabrillo Monument is located on San Miguel Island.\n\nIn the state of California, September 28 is officially \"Cabrillo Day\".\n\nA civic organization of Portuguese-Americans primarily in California is called the Cabrillo Club.\n\nIn northern California, the Point Cabrillo Light is named after him. San Pedro, part of the city of Los Angeles, has Cabrillo Beach and the Cabrillo Marine Aquarium.\n\nSchools named for him include Cabrillo College in Aptos, California, high schools in Lompoc and Long Beach, and several middle and elementary schools.\n\nThe portion of California State Route 1 that runs from Las Cruces in Santa Barbara County north to San Francisco is called the Cabrillo Highway. The Cabrillo Bridge and Cabrillo Freeway (California State Route 163) running through San Diego's Balboa Park are also named for him. There are streets named for him in many cities in California.\n\nThe SS \"Cabrillo\" was a wooden steamer launched in 1914 to serve as a ferry across the San Pedro Channel to Santa Catalina Island. It was later requisitioned by the United States Army and served as a troop transport in northern California during World War II.\n\nIn 1992, the United States Postal Service issued a 29¢ stamp in honor of Cabrillo.\n\nThe Maritime Museum of San Diego, in partnership with Cabrillo National Monument, has built a full-sized, fully functional, and historically accurate replica of Juan Rodriquez Cabrillo’s flagship, \"San Salvador\". The construction of the replica was based on historical and archeological research into early Spanish and Portuguese shipbuilding techniques.\n\nThe construction was carried out in full public view on the shores of San Diego Bay by professional boat builders, assisted by scores of volunteers. Her keel was laid in April 2011; her first official public unveiling was in September 2015 when she led a parade of tall ships. The replica ship now sails on regular tours in the waters of the Southern California coast as an educational historical resource.\n\n\n"}
{"id": "6974596", "url": "https://en.wikipedia.org/wiki?curid=6974596", "title": "Land cover", "text": "Land cover\n\nLand cover is the physical material at the surface of the earth. Land covers include grass, asphalt, trees, bare ground, water, etc. Earth cover is the expression used by ecologist Frederick Edward Clements that has its closest modern equivalent being vegetation. The expression continues to be used by the Bureau of Land Management.\n\nThere are two primary methods for capturing information on land cover: field survey and analysis of remotely sensed imagery. \n\nOne of the major land cover issues (as with all natural resource inventories) is that every survey defines similarly named categories in different ways. For instance, there are many definitions of \"forest\"—sometimes within the same organisation—that may or may not incorporate a number of different forest features (e.g., stand height, canopy cover, strip width, inclusion of grasses, and rates of growth for timber production). Areas without trees may be classified as forest cover \"if the intention is to re-plant\" (UK and Ireland), while areas with many trees may not be labelled as forest \"if the trees are not growing fast enough\" (Norway and Finland). \n\n\"Land cover\" is distinct from \"land use\", despite the two terms often being used interchangeably. Land use is a description of how people \"utilize\" the land and of socio-economic activity. Urban and agricultural land uses are two of the most commonly known land use classes. At any one point or place, there may be multiple and alternate land uses, the specification of which may have a political dimension. The origins of the \"land cover/land use\" couplet and the implications of their confusion are discussed in Fisher et al. (2005).\n\nFollowing table is Land Cover statistics by Food and Agriculture Organization (FAO) with 14 classes.\n\n"}
{"id": "221054", "url": "https://en.wikipedia.org/wiki?curid=221054", "title": "Legal education", "text": "Legal education\n\nLegal education is the education of individuals in the principles, practices, and theory of law. It may be undertaken for several reasons, including to provide the knowledge and skills necessary for admission to legal practice in a particular jurisdiction, to provide a greater breadth of knowledge to those working in other professions such as politics or business, to provide current lawyers with advanced training or greater specialisation, or to update lawyers on recent developments in the law.\n\nLegal education can take the form of a variety of programs, including: \n\nEarly Western legal education emerged in Republican Rome. Initially those desiring to be advocates would train in schools of rhetoric. Around the third century BC Tiberius Coruncanius began teaching law as a separate discipline. His public legal instruction had the effect of creating a class of legally skilled non-priests (\"jurisprudentes\"), a sort of consultancy. After Coruncanius' death, instruction gradually became more formal, with the introduction of books on law beyond the then scant official Roman legal texts. It is possible that Coruncanius allowed members of the public and students to attend consultations with citizens in which he provided legal advice. These consultations were probably held outside the College of Pontiffs, and thus accessible to all those interested.\n\nCanon and ecclesiastical law were studied in universities in medieval Europe. However, institutions providing education in the domestic law of each country emerged later in the eighteen century.\n\nIn England, legal education emerged in the late thirteenth century through apprenticeships. The Inns of Court controlled admission to practice and also provided some legal training. English universities had taught Roman and canon law for some time, but formal degrees focused on the native common law did not emerge until the 1800s.\n\nIn many countries, including most of those in the Commonwealth of Nations, the principal law degree is an undergraduate degree, usually known as a Bachelor of Laws (LLB). Graduates of such a program are eligible to become lawyers by passing the country's equivalent of a bar exam. In these countries, graduate law programs are advanced degrees which allow for more in-depth study or specialisation.\n\nIn the United States, the primary law degree is a graduate degree known as the Juris Doctor (JD). Students may pursue such a degree only after completing an undergraduate degree, usually a bachelor's degree. The undergraduate degree can be in any field, though most American lawyers hold bachelor's degrees in the humanities and social sciences. American law schools are usually an autonomous entity within a larger university.\n\nPrimary degrees in law are offered by law schools, known in some countries as faculties of law. Law schools may have varying degrees of autonomy within a particular university or, in some countries, can be entirely independent of any other post-secondary educational institution.\n\nHigher degrees allow for more advanced academic study. These include the Masters of Law (LLM) by coursework or research, and doctoral degrees such as the PhD or SJD.\n\nPractitioners may undertake a Masters of Law by coursework to obtain greater specialisation in an area in which they practice. In many common law countries, a higher degree in law is common and expected for legal academics. In addition, incorporating practical skills is beneficial for practitioners seeking higher degrees to better prepare them in their respective legal area of practice.\n\nIn contrast, higher degrees in law are uncommon in the United States, even within the academy.\n\nIn some countries, including the United Kingdom, Italy, Germany, Canada and some states of Australia, the final stages of vocational legal education required to qualify to practice law are carried out outside the university system. The requirements for qualification as a barrister or as a solicitor are covered in those articles.\n\nLegal education providers in some countries offer courses which lead to a certificate or accreditation in applied legal practice or a particular specialisation.\n\nContinuing legal education (also known as continuing professional development) programs are informal seminars or short courses which provide legal practitioners with an opportunity to update their knowledge and skills throughout their legal career. In some jurisdictions, it is mandatory to undertake a certain amount of continuing legal education each year.\n\nIn Australia most universities offer law as an undergraduate-entry course (LLB, 4 years), or combined degree course (e.g., BSc/LLB, BCom/LLB, BA/LLB, BE/LLB, 5–6 years). Some of these also offer a three-year postgraduate Juris Doctor (JD) program. Bond University in Queensland runs three full semesters each year, teaching from mid-January to late December. This enables the Bond University Law Faculty to offer the LLB in the usual 8 semesters, but only 2 years. They also offer a JD in two years. The University of Technology, Sydney will from 2010 offer a 2-year accelerated JD program.\n\nIn 2008, the University of Melbourne introduced the Melbourne Model, whereby Law is only available as a graduate degree, with students having to have completed a three-year bachelor's degree (usually an Arts degree) before being eligible. Students in combined degree programs would spend the first 3 years completing their first bachelor's degree together with some preliminary law subjects, and then spend the last 2–3 years completing the law degree (JD). Alternatively, one can finish any bachelor's degree, and providing their academic results are high, apply for graduate-entry into a 3-year LLB program. Australian Law Schools include those at the University of New England, Australian Catholic University, Australian National University, La Trobe University, Flinders University, Bond University, Macquarie, Monash, Deakin, UNSW, University of Tasmania, Adelaide, Victoria University, Sydney, Melbourne, Queensland University of Technology, the University of Queensland, the University of Western Australia and the University of Canberra.\n\nThe professional law degree in Canada is the Bachelor of Laws (LL.B.) or Juris Doctor (J.D.), for common law jurisdictions, and the Bachelor of Laws, Licentiate of Law or Bachelor of Civil Law for Quebec, a civil law jurisdiction. Admittance to an LL.B. or J.D. program requires at least two years of undergraduate education, although, a completed undergraduate degree is usually required. In practice, the vast majority of those who are admitted have already earned at least an undergraduate (bachelor's) degree. The change in academic nomenclature redesignating the common law degree as a J.D. rather than an LL.B., currently completed or under consideration at a number of Canadian schools, has not affected the level of instruction—because it is the same degree. In the case of Quebec civil law degrees and the transsystemic LL.B/B.C.L. program at McGill University, students can be admitted after college.\n\nList of law schools in China\n\nIn Hong Kong law can be studied as a four-year undergraduate degree Bachelor of Laws (LLB), a two-year postgraduate degree (Juris Doctor), or the Common Professional Examination conversion course for non-law graduates. One must then pass the one-year Postgraduate Certificate in Laws (PCLL) currently offered at the University of Hong Kong (HKU), Chinese University of Hong Kong and City University of Hong Kong, before starting vocational training: a year's pupillage for barristers or a two-year training contract for solicitors.\n\nThe move to a four-year LLB was recent and, in the case of HKU, was aimed at shifting some of the more theoretical aspects of the HKU PCLL into the LLB, leaving more room for practical instruction.\n\nThe Bar Council of India prescribes and supervises standard of legal education in India. Law degrees in India are granted and conferred in terms of the Advocates Act, 1961, which is a law passed by the Parliament both on the aspect of legal education and also regulation of conduct of legal profession. Various regional universities or specialised national law universities offer Law graduate degrees through various law schools.\n\nIn India law can be studied, as LL.B. (Bachelor of Laws) or B.L. (Bachelor of Law), a three-year graduate degree after completion of Bachelor's degree. Alternatively after standard 12 one can join an integrated five-year law course which provides option to avail B.A. LL.B. or B.B.A. LLB. or B.Sc. LL.B. In India applied legal education for specific branches of law is also offered such as, Business law, Human resource and Labour laws, Property laws, Family laws, Human rights & Legal awareness, Taxation law and many more.\n\nLaw in Italy and France is studied in a jurisprudence school which is an entity within a larger university. Legal education can be started immediately after obtained a Diploma. Italian and French law schools are affiliated with public universities, and are thus public institutions. As a consequence, law schools are required to admit anyone holding the baccalaureate. However, the failure rate is extremely high (up to 70%) during the first two years of the \"licenza in diritto\". There are no vast disparities in the quality of Southern European law schools. Many schools focus on their respective city and region.\n\nThe law school program is divided following the European standards for university studies (Bologna process):\n\nThe first year of the master program (M1) is specialized : public law, private law, business law, European and international law, etc.\n\nThe second year of the master of law program (M2) can be work-oriented or research oriented (the students write a substantial thesis and can apply to doctoral programs, e.g., a PhD in Law).\n\nThe second year is competitive (entry is based on the student's grades and overall score and on extracurricular activities) and generally more specialized (IP law, contract law, civil liberties, etc.).\n\nStudents must pass a specific examination to enter bar school (CRFPA, école du barreau). They must successfully finish the first year of a Master of law (M1 or maitrise de droit) to be able to attend.\n\nIf they succeed, then after 18 months (school, practical aspects, ethics and internship) they then take the CAPA exam and diploma(Certificat d'Aptitude à la Profession d'Avocat). Successful students also take the Oath in order to practice law.\n\nThe Japanese Ministry of Justice opened the University of Tokyo Faculty of Law in 1877 (changed to Imperial University in 1886). To matriculate to the University of Tokyo, students had to finish ten to fifteen years of compulsory education; acceptance was therefore available to only a small elite. The law program produced politically-dependable graduates to fill fast-track administrative positions in government, also known as high civil servants (koto bunkan), and to serve as judges and prosecutors.\n\nPrivate law schools opened around 1880. These lacked the government funding given to the University of Tokyo, so the quality of education there lagged behind. Students only had to pass an examination to matriculate to private law schools, so many of them had not completed middle school. The private law schools produced a large portion of private attorneys because their graduates were often ineligible to apply for government positions.\n\nThe Imperial University Faculty of Law was given supervisory authority over many private law schools in 1887; by the 1920s, it promulgated a legal curriculum comprising six basic codes: Constitutional Law, Civil Law, Commercial Law, Civil Procedure, Criminal Law, and Criminal Procedure. The same basic structure survived in Japanese legal education to the end of the twentieth century.\n\nPrior to the implementation of the \"law school system\" in 2004, the legal education system was driven more by examinations than by formal schooling. The passage rate for the bar exam was historically around three percent, and nearly all those who sat for the exam took it several times. A number of specialized \"cram schools\" trained prospective lawyers for the exam, and these schools remain prevalent today. After passing the bar exam, prospective barristers were required to train for 16 months at the Legal Research and Training Institute of the Supreme Court of Japan. The training period has traditionally been devoted to litigation practice and virtually no training is given for other aspects of legal practice, e.g., contract drafting, legal research. During this period, the most \"capable trainees\" are \"selected out\" to become career judges; others may become prosecutors or private practitioners.\n\nIn 2004, the Japanese Diet passed a law allowing for the creation of graduate level law school that offer a J.D., or Hōmu Hakushi (法務博士). The 2006 bar examination was first in Japanese history to require a law school degree as a prerequisite. In the past, although there has been no educational requirement, most of those who passed the examination had earned undergraduate degrees from \"elite\" Japanese universities such as the University of Tokyo, Kyoto University or Hitotsubashi University. With this new law school system came a new bar exam, with a 40–50% passage rate which is capped by a numerical quota. Applicants are now limited to taking the exam three times in a five-year period. Despite the much higher bar passage rate with the new exam, due to the quotas, approximately half of Japanese law school graduates will never be admitted to practice. The new system also reduced the apprenticeship period at the Legal Research and Training Institute to one year.\n\nA number of other law-related professions exist in Japan, such as patent agents (\"benrishi\"), tax accountants (\"zeirishi\"), scriveners, etc., entry to each of which is governed by a separate examination. Attorneys (\"bengoshi\"), being qualified to practice any law, can automatically be qualified as patent agents and tax accountants with no additional examination, but not vice versa.\n\nLegal education in Korea is driven by examination. The profession of barristers, is highly regulated, and the pass rate for the bar exam is around five percent. Prospective attorneys who do pass the exam usually take it two or three times before passing it, and a number of specialized \"private educational institutes\" exist for prospective lawyers. After passing the bar exam, prospective barristers undergo a two-year training period at the Judicial Research and Training Institute of the Supreme Court of Korea. During this period, the most capable trainees are \"selected out\" to become career judges; others may become prosecutors or private practitioners.\n\nIn 2007, the Korean government passed a law allowing for the creation of three-year law schools (\"법학전문대학원\"). According to the new law, the old system of selecting lawyers by examination will be phased out by 2013 and the U.S.-style law schools will be the sole route to become a lawyer.\n\nIn February 2008, the Ministry of Education of Korea selected 25 universities to open law schools. The total enrollment for all law schools is capped at 2,000, which is a source of contention between the powerful Korea Bar Association, and citizen groups and school administrators. There is an uproar among the schools which failed to get the government's approval and even among the schools that did get the approval, there is dissatisfaction due to an extremely low enrollment number. Several law schools are permitted to enroll 40 students per year, which is far below the financially sustainable number. Beginning in 2012, passage of the Lawyer Admission Test (which is distinct from the old bar exam) will be required for qualification to practice.\n\nA number of other legal professions exist in Korea, such as patent attorneys (\"변리사\"), tax attorneys (\"세무사\"), solicitors(\"법무사\"), etc., entry to each of which is governed by a separate examination.\n\nAs a Commonwealth country, the Malaysian legal education system is rooted from the United Kingdom. Legal qualifications offered by the local law faculties require students to have a pre-university qualification such as the Malaysian Higher School Certificate, A-Level, International Baccalaureate, Foundation Course or a Diploma. Generally, the law degree programmes in Malaysia consist of civil law subjects, but there are institutions such as The National University of Malaysia, International Islamic University Malaysia and Universiti Sultan Zainal Abidin that include Sharia or Islamic law courses as requirements for admission and graduation.\n\nMalaysian law graduates from universities in the UK, Australia or New Zealand are allowed to practice law in Malaysia. However, they are required to obtain a Certificate of Legal Practice in Laws of Malaysia.\n\nLaw degree programs are considered graduate programs in the Philippines. As such, admission to law schools requires the completion of a bachelor's degree, with a sufficient number of credits or units in certain subject areas.\nLegal education in the Philippines is regulated and supervised by the LEGAL EDUCATION BOARD, a statutorily created independent Body chaired by a retired member of the Supreme Court or of the Court of Appeals. Its first chairman is Justice Hilarion Aquino. Sitting as members of the Board are a representative of the law professors, a representative of the law deans and a representative of the Commission on Higher Education. The membership of a student representative has been subject to continuing debate and resistance on the part of law schools.\nGraduation from a Philippine law school constitutes the primary eligibility requirement for the Philippine Bar Examinations, administered by the Supreme Court during the month of September every year.\n\nIn order to be eligible to take the bar examinations, one must complete either of the two professional degrees: The Bachelor of Laws (LL.B.) program or the Juris Doctor (J.D.) program. Advanced degrees are offered by some law schools, but are not requirements for admission to the practice of law in the Philippines. The degrees Master of Laws (LL.M.), Master of Legal Studies are available in only a handful of Philippine universities and colleges, among these San Beda College Graduate School of Law, the University of Santo Tomas and Ateneo de Manila University. The Doctor of Civil Law degree (DCL) is offered only by the University of Santo Tomas and the Doctor of Juridical Science (JSD) degree is offered by the San Beda College Graduate School of Law. Graduate programs in law are also regulated by the Legal Education Board\n\nLegal education in the Philippines normally proceeds along the following route:\n\nLaw degree – jurist (often compared to an LL.M., but in fact equivalent to the degree of Specialist specific to the Soviet educational system) is awarded in Russia and Ukraine after 5 years of study at a university. Jurist degree may also be awarded in a shorter period of time if a law student has already completed Bachelor or Specialist degree in another field of studies or has previously earned a basic law degree (comparable to Paralegal, an associate degree in U.S.) from a specialized law college. Bachelor jurist degree (equivalent to Bachelor of Laws (LL.B.)) may be earned concurrently with another bachelor's or master's degree in some universities (comparable to a double-major). Note that this fused, one-degree (Specialist) educational scheme has coexisted with the two-degree (bachelor's – master's) scheme since Russia and Ukraine launched their higher education reforms to bring the domestic educational systems in closer compliance with the Bologna accords. See also academic degree. The latest educational reforms created new system where a four-year law program is offered at the universities for earning bachelor's degree, and a five-year law program is offered for master's degree. The degree of Specialist is no longer awarded and is renamed into master's degree.\n\nTo become a lawyer in Serbia, students must graduate from an accredited faculty of law. Studies last for five years (ten semesters) in accordance to the Bologna Convention. To become a student of the faculty of law, a candidate must pass the admission test. Students are divided into full-time students and part-time students. The practical training for students is organized at courts of law, and local and international moot court competitions. A lawyer must pass the national bar examination to become an attorney, a judge, or a prosecutor.\n\nIn South Africa, the LL.B. is the universal legal qualification for admission and enrollment as an Advocate or Attorney. Since 1998, LL.B. programmes may be entered directly at the undergraduate level; at the same time, the LLB. continues to be offered postgraduate and may then be accelerated dependent on the bachelor's degree. The programme lasts between two and four years correspondingly (compare Australia, above). See Bachelor of Laws #South Africa.\n\nAlthough not formally required for specialised practice, further training, e.g. in tax, is usually via postgraduate diplomas or focused, coursework-based LL.M. programmes. Research degrees are the LL.M. and LL.D., or PhD depending on university. The Master's dissertation reflects an ability to conduct independent research, whereas the Doctoral thesis will, in addition, constitute an original contribution to the field of law in question. A doctorate, generally, is required for positions in legal academia. See Master of Laws #South Africa; Doctor of Law #South Africa.\n\nHistorically, the B.Proc. and B.Juris were the legal degrees offered at the undergraduate level. The four-year BProc qualified one to practice as an attorney, or become a prosecutor or magistrate in the lower courts, but did not allow for admission as an advocate. The three-year B.Juris was the basic requirement for prosecutors and magistrates in the lower courts, but on its own, did not qualify one to practice as an attorney. Both offered admission to the LLB.\n\nFor admission as an attorney, one serves \"articles\" as a candidate attorney with a practicing attorney for two years, and then writes a \"board exam\" set by the relevant provincial Law Society. See Attorneys in South Africa. The length of articles may be reduced by attending a practical legal training course or performing community service.\n\nAttorneys may additionally qualify as Notaries and Conveyancers, via the Conveyancing and Notarial Practice Examinations; those with technical or scientific training may further qualify as patent attorneys – see Patent attorney #South Africa.\n\nThe requirements to enter private practice as advocates (Junior Counsel) are to become members of a Bar Association by undergoing a period of training (pupilage) for one year with a practicing Advocate, and to sit an admission examination. On the recommendation of the Bar Councils, an advocate \"of proven experience and skill\" with at least ten years experience, may be appointed by the President of South Africa as a Senior Counsel (SC; also referred to as a \"silk\"). See Advocates in South Africa.\n\nThe Act regulating admission to practice law (\"The Qualifications of Legal Practitioners Amendment Act of 1997\") is being revised.\n\nThe law of South America is one of the most unified in the world. All countries can be said to follow civil law systems, although recent developments in the law of Brazil suggest a move towards the \"stare decisis\" doctrine. Moreover, all countries have recently signed up to the Union of South American Nations agreement, which aims to establish a system of supra-national law along the lines of the European Union.\n\nIn order to practice law in Sri Lanka, a lawyer must be 'admitted and enrolled as an Attorney-at-Law of the Supreme Court of the Democratic Socialist Republic of Sri Lanka. To be admitted to the bar a law student must complete law exams held by the Sri Lanka Law College and undergo a six-month period of apprenticeship under a senior practicing lawyer. There are two routes taken by students:\n\n\nBoth groups of students must undergo a period of apprenticeship under a senior practicing lawyer who has at least 8 years of practicing experience. To become a judge one must be admitted as an Attorney-at-Law.\n\nIn England and Wales, law can be studied as an undergraduate degree or in a Graduate Diploma in Law where students complete the Common Professional Examination. After obtaining the degree which is necessary to complete certain vocational courses and to serve a period of on the job training before one is able to qualify to practice as a barrister, legal executive, or solicitor. Bar Professional Training Course is regarded as one of the hardest degrees and presently it is the most expensive law-related degree.\n\nThe education of lawyers in the United States is generally undertaken through a law school program, although in some states (such as California and Virginia) applicants who have not attended law school may qualify to take the bar exam.\n\nLegal education in the United States normally proceeds along the following route:\n\nIn the United States,in most cases, the degree awarded by American law schools is the Doctor of Jurisprudence or Juris Doctor (J.D.), a Doctoral degree, the pursuit of which students undertake only after having completed an undergraduate degree in some other field (usually a bachelor's degree). The law school program is considered to be a professional school program and upon graduation you receive the distinct title of Doctor (although most states strictly regulate the ability of attorneys to style themselves \"doctor\").\n\nResearch degrees that are awarded include the Master of Laws (LL.M.), Doctor of Juridical Science degrees (J.S.D. or S.J.D.) and Doctor of Comparative Law (D.C.L.), are post-undergraduate and research and academic-based level degrees. In the U.S. the Legum Doctor (LL.D.) is only awarded as an honorary degree.\n\nA number of law students apply for an optional judicial clerkship (less than 10% end up in such position), to be taken after law school and before legal practice. Clerkships usually last one year with appellate courts, but trial level courts (including federal district court) are increasingly moving towards two-year clerkships.\n\nOnce a student has graduated from law school, he or she is expected to pursue admission to the bar in order to practice. Requirements for membership in the bar vary across the United States. In almost every state, the only way to be admitted to the bar is to pass a (usually multi-day) written examination. Once admitted, most States require attorneys to must meet certain Continuing Legal Education (CLE) requirements.\n\nAcademic degrees for non-lawyers are available at the baccalaureate and master's level. A common baccalaureate level degree is a Bachelor of Science in Legal Studies (B.S.). Academic master's degrees in legal studies are available, such as the Master of Studies (M.S.), and the Master of Professional Studies (M.P.S.). Such a degree is not required to enter a J.D. program.\n\nForeign lawyers seeking to practice in the U.S., who do not have a J.D., often seek to obtain a Master of Laws (LL.M.) (or other degrees similar to the LL.M., such as the Juris Master (J.M.), Master of Comparative Law (M.C.L.) and Master of Jurisprudence (M.J.)).\n"}
{"id": "52203249", "url": "https://en.wikipedia.org/wiki?curid=52203249", "title": "List of international presidential trips made by Dmitry Medvedev", "text": "List of international presidential trips made by Dmitry Medvedev\n\nThis is a list of international trips made by Dmitry Medvedev, during his presidency, he made 102 international trips to different countries.\n\n\n\n\n\n\n\n\n\n"}
{"id": "51848502", "url": "https://en.wikipedia.org/wiki?curid=51848502", "title": "List of international presidential trips made by Ronald Reagan", "text": "List of international presidential trips made by Ronald Reagan\n\nThis is a list of international presidential trips made by Ronald Reagan, the 40th president of the United States. Ronald Reagan made 25 international trips to 26 different countries on four continents—Europe, Asia, North America, and South America—during his presidency, which began on January 20, 1981 and ended on January 20, 1989.\n\nThe number of visits per country where he travelled are:\n\n"}
{"id": "9875604", "url": "https://en.wikipedia.org/wiki?curid=9875604", "title": "List of marches", "text": "List of marches\n\nThis is a list of European medieval marches.\n\nAt the beginning of his rule as king of Germany, Otto I tried to reorganize his realm to prepare an expansion to the East. At the beginning of the year 937, he created two marches: the March of the Billungen, given to Hermann Billung, later Duke of Saxony; and the Eastern march, given to Gero. In 961, when Billung became Duke of Saxony, his March was merged with the duchy. In the case of Gero, Otto I, now emperor, decided the division of his territories, greatly expanded since 937.\n\n\nIn 861, Charles the Bald, king of France, created two marches to protect his realm from warriors coming from Brittany and Normandy. Both were named March of Neustria, but will be known as March of Brittany and March of Normandy. In 863, the king created the March of Flanders.\n\n\nThree marches belonging to the Holy Roman Empire were created in the Low Countries:\n\n\n\n\n\n"}
{"id": "18596887", "url": "https://en.wikipedia.org/wiki?curid=18596887", "title": "List of political and geographic subdivisions by total area from 200,000 to 500,000 square kilometers", "text": "List of political and geographic subdivisions by total area from 200,000 to 500,000 square kilometers\n"}
{"id": "53482266", "url": "https://en.wikipedia.org/wiki?curid=53482266", "title": "List of reportedly haunted locations in South Africa", "text": "List of reportedly haunted locations in South Africa\n\nThe following is a list of reportedly haunted locations in South Africa.\n\n"}
{"id": "17353952", "url": "https://en.wikipedia.org/wiki?curid=17353952", "title": "List of selected stars for navigation", "text": "List of selected stars for navigation\n\nFifty-eight selected navigational stars are given a special status in the field of celestial navigation. Of the approximately 6,000 stars visible to the naked eye under optimal conditions, the selected stars are among the brightest and span 38 constellations of the celestial sphere from the declination of −70° to +89°. Many of the selected stars were named in antiquity by the Babylonians, Greeks, Romans, and Arabs.\n\nThe star Polaris, often called the \"North Star\", is treated specially due to its proximity to the north celestial pole. When navigating in the Northern Hemisphere, special techniques can be used with Polaris to determine latitude or gyrocompass error. The other 57 selected stars have daily positions given in nautical almanacs, aiding the navigator in efficiently performing observations on them. A second group of 115 \"tabulated stars\" can also be used for celestial navigation, but are often less familiar to the navigator and require extra calculations.\n\nFor purposes of identification, the positions of navigational stars — expressed as declination and sidereal hour angle — are often rounded to the nearest degree. In addition to tables, star charts provide an aid to the navigator in identifying the navigational stars, showing constellations, relative positions, and brightness.\n\nUnder optimal conditions, approximately 6,000 stars are visible to the naked eye of an observer on Earth. Of these, 58 stars are known in the field of navigational astronomy as \"selected stars\", including 19 stars of the first magnitude, 38 stars of the second magnitude, and Polaris. The selection of the stars is made by Her Majesty's Nautical Almanac Office and the US Naval Observatory, in the production of the yearly \"Nautical Almanac\" which the two organizations have published jointly since 1958. Criteria in the choice of stars includes their distribution across the celestial sphere, brightness, and ease of identification. Information for another 115 stars, known as \"tabulated stars\", is also available to the navigator. This list provides information on the name, approximate position in the celestial sphere, and apparent magnitude of the 58 selected stars in tabular form and by star charts.\n\nThese stars are typically used in two ways by the navigator. The first is to obtain a line of position by use of a sextant observation and the techniques of celestial navigation. Multiple lines of position can be intersected to obtain a position known as a celestial fix. The second typical use of the navigational stars is to determine gyrocompass error by computing the azimuth of a star and comparing it to an azimuth measured using the ship's gyrocompass. Numerous other applications also exist.\n\nNavigators typically refer to stars using one of two naming systems for stars: common names and Bayer's designations. All of the selected stars have had a common name since 1953, and many were named in antiquity by the Arabs, Greeks, Romans, and Babylonians. Bayer's naming convention has been in use since 1603, and consists of a Greek letter combined with the possessive form of the star's constellation. Both names are shown for each star in the tables and charts below.\n\nEach star's approximate position on the celestial sphere is given using the equatorial coordinate system. The celestial sphere is an imaginary globe of infinite size with the Earth at its center. Positions on the celestial sphere are often expressed using two coordinates: declination and sidereal hour angle, which are similar to latitude and longitude on the surface of the Earth. To define declination, the Earth's equator is projected out to the celestial sphere to construct the celestial equator, and declination is measured in degrees north or south of this celestial equator. Sidereal hour angle is a measurement between 0° and 360°, indicating how far west a body is from an arbitrarily chosen point on the celestial sphere called the \"First Point of Aries\". Note that right ascension, as used by astronomers, is 360° minus the sidereal hour angle.\n\nThe final characteristic provided in the tables and star charts is the star's brightness, expressed in terms of apparent magnitude. Magnitude is a logarithmic scale of brightness, designed so that a body of one magnitude is approximately 2.512 times brighter than a body of the next magnitude. Thus, a body of magnitude 1 is 2.512, or 100 times brighter than a body of magnitude 6. The dimmest stars that can be seen through a 200 inch terrestrial telescope are of the 20th magnitude, and very bright objects like the Sun and a full Moon have magnitudes of −26.7 and −12.6 respectively.\n\nThe table of navigational stars provides several types of information. In the first column is the identifying index number, followed by the common name, the Bayer designation, and the etymology of the common name. Then the star's approximate position, suitable for identification purposes, is given in terms of declination and sidereal hour angle, followed by the star's magnitude. The final column presents citations to the sources of the data, \"The American Practical Navigator\" and the star's entry at the SIMBAD database, a project of the Strasbourg Astronomical Data Center or CDS.\n\nNavigators often use star charts to identify a star by its position relative to other stars. References like the \"Nautical Almanac\" and \"The American Practical Navigator\" provide four star charts, covering different portions of the celestial sphere. Two of these charts are azimuthal equidistant projections of the north and south poles. The other two cover the equatorial region of the celestial sphere, from the declination of 30° south to 30° north. The two equatorial charts are mercator projections, one for the eastern hemisphere of the celestial sphere and one for the western hemisphere. Note that unlike familiar maps, east is shown to the left and west is shown to the right. With this orientation, the navigator can hold the star chart overhead, and the arrangement of the stars on the chart will resemble the stars in the sky.\nIn the star charts, constellations are labelled with capital letters and indicated by dotted lines collecting their stars. The 58 selected stars for navigation are shown in blue and labelled with their common name, star number, and a Greek letter to indicate their Bayer designation. The additional 115 tabulated stars that can also be used for navigation are shown in red and labelled with a Greek letter to indicate their Bayer designation. Some additional stars not suitable for navigation are also included on the charts to indicate constellations, they are presented as unlabelled small red dots.\n\nThe equatorial region of the celestial sphere's eastern hemisphere includes 16 navigational stars from Alpheratz in the constellation Andromeda to Denebola in Leo. It also includes stars from the constellations Cetus, Aries, Taurus, Orion, Canis Major and Minor, Gemini, and Hydra. Of particular note among these stars are \"the dog star\" Sirius, the brightest star in the sky, and four stars of the easily identified constellation Orion. \n \nThe equatorial region of the celestial sphere's western hemisphere includes 13 navigational stars from Gienah in the constellation Corvus to Markab in Pegasus. It also includes stars from the constellations Virgo, Bootes, Libra, Corona Borealis, Scorpio, Ophiuchus, Sagittarius, and Aquila. The variable star Arcturus is the brightest star in this group. \n\nThe 11 northern stars are those with a declination between 30° north and 90° north. They are listed in order of decreasing sidereal hour angle, or from the vernal equinox westward across the sky. Starting with Schedar in the constellation Cassiopeia, the list includes stars from the constellations Auriga, the Great and Little Bears, Draco, Lyra and Cygnus. The two brightest northern stars are Vega and Capella.\n\nIn the star chart to the right, declination is shown by the radial coordinate, starting at 90° north in the center and decreasing to 30° north at the outer edge. Sidereal hour angle is shown as the angular coordinate, starting at 0° at the left of the chart, and increasing counter-clockwise.\n\nThe 18 southern stars are those with a declination between 30° south and 90° south. They are listed in order of decreasing sidereal hour angle, or from the vernal equinox westward across the sky. Starting with Ankaa in the constellation Phoenix, the list includes stars from the constellations Eridanus, Carina, Crux, Centaurus, Libra, Triangulum Australe, Scorpio, Sagittarius, Pavo, and Grus. Canopus, Rigil Kentaurus, Achernar, and Hadar are the brightest stars in the southern sky.\n\nIn the star chart to the right, declination is shown by the radial coordinate, starting at 90° south in the center and decreasing to 30° south at the outer edge. Sidereal hour angle is shown as the angular coordinate, starting at 0° at the right of the chart, and increasing clockwise.\n\n"}
{"id": "11486139", "url": "https://en.wikipedia.org/wiki?curid=11486139", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: P", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: P\n\n\n"}
{"id": "9895080", "url": "https://en.wikipedia.org/wiki?curid=9895080", "title": "Local Notice to Mariners", "text": "Local Notice to Mariners\n\nA Local Notice to Mariners is an authoritative instruction issued by a designated official, typically the harbormaster.\n\nIn the United States, notices are issued by each U.S. Coast Guard District to disseminate important information affecting navigational safety within that District. This Notice reports changes and deficiencies in aids to navigation maintained by the Coast Guard. Other marine information such as new charts, channel depths, naval operations, and regattas is included. Since temporary information of short duration is not included in the weekly Notice to Mariners, the \"Local Notice to Mariners\" may be the only source of such information. Small craft using the Intracoastal Waterway and small harbors not normally used by oceangoing vessels need it to keep charts and publications up-to-date.\n\nSince correcting information for U.S. charts in the \"Notice to Mariners\" is obtained from the Coast Guard Local Notices, it is normal to expect a lag of 1 or 2 weeks for the \"Notice to Mariners\" to publish a correction from this source.\n\nThe \"Local Notice to Mariners\" may be obtained free of charge by contacting the appropriate Coast Guard District Commander. Vessels operating in ports and waterways in several districts must obtain the \"Local Notice to Mariners\" from each district. \n\nThe text of the US material originated from section 422 of The American Practical Navigator, a document produced by the government of the United States of America.\n\nLNTMs are, by definition, concerned with local issues. Each issuing authority has its own series of LNTMs – there is no international standard numbering or indexing scheme. Individual LNTMs may concern short or long term situations. At Portsmouth, mariners are instructed by LNTM 28/07 to \"keep clear of warship berths\". At Holyhead, LNTM 5/2008 concerns \"Bunkering or transferring oil in port\". At Chichester, No 16 of 2008 \"Sea defence works at Hayling Island\" gives timely information about dredging operations.\n\n\n"}
{"id": "2882854", "url": "https://en.wikipedia.org/wiki?curid=2882854", "title": "Longitude by chronometer", "text": "Longitude by chronometer\n\nLongitude by chronometer is a method, in navigation, of determining longitude using a marine chronometer, which was developed by John Harrison during the first half of the eighteenth century. It is an astronomical method of calculating the longitude at which a position line, drawn from a sight by sextant of any celestial body, crosses the observer's assumed latitude. In order to calculate the position line, the time of the sight must be known so that the celestial position i.e. the Greenwich Hour Angle (Celestial Longitude - measured in a westerly direction from Greenwich) and Declination (Celestial Latitude - measured north or south of the equational or celestial equator), of the observed celestial body is known. All that can be derived from a single sight is a single position line, which can be achieved at any time during daylight when both the sea horizon and the sun are visible. To achieve a fix, more than one celestial body and the sea horizon must be visible. This is usually only possible at dawn and dusk.\n\nThe angle between the sea horizon and the celestial body is measured with a sextant and the time noted. The Sextant reading is known as the 'Sextant Altitude'. This is corrected by use of tables to a 'True Altitude'. The actual declination and hour angle of the celestial body are found from astronomical tables for the time of the measurement and together with the 'True Altitude' are put into a formula with the assumed latitude. This formula calculates the 'True Hour Angle' which is compared to the assumed longitude providing a correction to the assumed longitude. This correction is applied to the assumed position so that a position line can be drawn through the assumed latitude at the corrected longitude at 90° to the azimuth (bearing) on the celestial body. The observer's position is somewhere along the position line, not necessarily at the found longitude at the assumed latitude. If two or more sights or measurements are taken within a few minutes of each other a 'fix' can be obtained and the observer's position determined as the point where the position lines cross.\n\nThe azimuth (bearing) of the celestial body is also determined by use of astronomical tables and for which the time must also be known.\n\nFrom this, it can be seen that a navigator will need to know the time very accurately so that the position of the observed celestial body is known just as accurately. The position of the sun is given in degrees and minutes north or south of the equational or celestial equator and east or west of Greenwich, established by the English as the Prime Meridian.\n\nThe desperate need for an accurate chronometer was finally met in the mid 18th century when an Englishman, John Harrison, produced a series of chronometers that culminated in his celebrated model H-4 that satisfied the requirements for a shipboard standard time-keeper.\n\nMany nations, such as France, have proposed their own reference longitudes as a standard, although the world’s navigators have generally come to accept the reference longitudes tabulated by the British. The reference longitude adopted by the British became known as the Prime Meridian and is now accepted by most nations as the starting point for all longitude measurements. The Prime Meridian of zero degrees longitude runs along the meridian passing through the Royal Observatory at Greenwich, England. Longitude is measured east and west from the Prime Meridian. To determine \"longitude by chronometer,\" a navigator requires a chronometer set to the local time at the Prime Meridian. Local time at the Prime Meridian has historically been called Greenwich Mean Time (GMT), but now, due to international sensitivities, has been renamed as Coordinated Universal Time (UTC), and is known colloquially as \"zulu time\".\n\nNoon sights obtain the observer's Latitude. It is impossible to determine longitude with an accuracy better than 10nmi by means of a noon sight. A noon sight is called a Meridian Altitude. While it is very easy to determine the observer's latitude at noon without knowing the exact time, longitude cannot accurately be measured at noon. At noon the sun's change of altitude is very slow, so determining the exact time that the sun is at its zenith by direct observation is impossible, and therefore it is impossible to obtain an accurate longitude at the moment of Zenith. However, it is possible to determine the time of zenith for longitude with a useful accuracy by performing a mean time of observation when the sun is on its ascent and descent prior to and following its moment of Zenith. By taking a sextant reading within 15 to 30 minutes prior to local noon (zenith) and noting the time, then leaving the sextant set to the same angle and subsequently observing the moment in time at which the sun passes through the sight tube on its descent from Zenith between a half-hour and hour later, the two times can be averaged to obtain a longitude sufficiently accurate for navigation (within 2nmi).\n\nUnfortunately, the Earth does not make a perfect circular orbit around the Sun. Due to the elliptical nature of the Earth’s orbit around the Sun, the speed of the Sun’s apparent orbit around the Earth varies throughout the year and that causes it to appear to speed up and slow down very slightly. Consequently, noon at the Prime Meridian is rarely if ever exactly at 1200 UTC, but rather it occurs some minutes and seconds before or after that time each day. This slight daily variation has been calculated and is listed for each day of the year in the Nautical Almanac under the title of \"Equation of time\". This variation must be added to or subtracted from the UTC of local apparent noon to improve the accuracy of the calculation. Even with that, other factors, including the difficulty of determining the exact moment of local apparent noon due to the flattening of the Sun’s arc across the sky at its highest point, diminish the accuracy of determining longitude by chronometer as a method of celestial navigation. Accuracies of less than error in position are difficult to achieve using the \"longitude by chronometer\" method. Other celestial navigation methods involving more extensive use of both the Nautical Almanac and sight reduction tables are used by navigators to achieve accuracies of one nautical mile (1.9 km) or less.\n\n This only calculates a longitude at the assumed latitude though a position line can be drawn. The observer is somewhere along the position line.\nTime sight is a general method for determining longitude by celestial observations using a chronometer; these observations are reduced by solving the navigational triangle for meridian angle and require known values for altitude, latitude, and declination; the meridian angle is converted to local hour angle and compared with Greenwich hour angle.\n\nIf \"Dec\" is the declination of the observed celestial body and Ho is its observed altitude, the local hour angle, \"LHA\", is obtained for a known latitude \"B\" by:\n\nformula_1\n\nThe time sight was a complement to the noon sight or latitude by Polaris in order to obtain a fix.\n\n\n\n"}
{"id": "286960", "url": "https://en.wikipedia.org/wiki?curid=286960", "title": "Mainland", "text": "Mainland\n\nMainland is a contiguous landmass that is larger and often politically, economically and/or demographically more significant than politically associated remote territories, such as exclaves or oceanic islands situated outside the continental shelf. \n\nIn geography, \"mainland\" can denote the continental (i.e. non-insular) part of any polity or the main island within an island nation. In geopolitics, \"mainland\" is sometimes used interchangeably with terms like Metropole as an antonym to overseas territories. In the sense of \"heartland\", mainland is the opposite of periphery.\n\nThe term is relative- in Tasmania, continental Australia is the mainland, while to residents of Flinders Island, the main island of Tasmania is also \"the mainland\".\n\n\n"}
{"id": "32374980", "url": "https://en.wikipedia.org/wiki?curid=32374980", "title": "Nautical measured mile", "text": "Nautical measured mile\n\nA nautical measured mile is a nautical mile which is marked by two pairs of towers. A mile is measure by sailing on a given bearing and lining up the pairs of towers. The start of the mile is recorded when the first pair of towers line up and the end of the mile recorded when the second pair line up.\n\nTo accurately measure performance ships must make at least four to six runs in both directions to allow for the wind and tide. \nThere are several nautical measured miles around the British Isles:\n\n"}
{"id": "9997671", "url": "https://en.wikipedia.org/wiki?curid=9997671", "title": "Nautical stationkeeping", "text": "Nautical stationkeeping\n\nStation-keeping in a nautical situation is when a vessel is to:\n\n"}
{"id": "25437026", "url": "https://en.wikipedia.org/wiki?curid=25437026", "title": "Polar exploration", "text": "Polar exploration\n\nPolar exploration is the process of exploration of the polar regions of the Earth - the Arctic region and Antarctica - particularly with the goal of reaching the North Pole and South Pole, respectively. Historically, this was accomplished by explorers making often arduous travels on foot or by sled in these regions, known as a polar expedition. More recently, exploration has been accomplished with technology, particularly with satellite imagery.\n\nFrom 600 BC to 300 BC Greek Philosophers theorized that the planet was a Spherical Earth with North and South Polar regions. By 150 AD Ptolemy published Geographia, which notes a hypothetical Terra Australis Incognita. However, due to harsh weather conditions, the poles themselves would not be reached for centuries after that. When they finally were reached, the achievement was realized only a few years apart.\n\nThere are two claims, both disputed, about who was the first person to reach the geographic North Pole. Frederick Cook, accompanied by two Inuit men, Ahwelah and Etukishook claimed to have reached the Pole on April 21, 1908, although this claim is generally doubted. On April 6, 1909, Robert Peary claimed to be the first person in recorded history to reach the North Pole, accompanied by his employee Matthew Henson and four Inuit men Ootah, Seegloo, Egingway, and Ooqueah (although whether he actually reached the Pole is doubted by some).\n\nNorwegian explorer Roald Amundsen had planned to reach the North Pole by means of an extended drift in an icebound ship. He obtained the use of Fridtjof Nansen's polar exploration ship \"Fram\", and undertook extensive fundraising. Preparations for this expedition were disrupted when Cook and Peary each claimed to have reached the North Pole. Amundsen then changed his plan and began to prepare for a conquest of the geographic South Pole; uncertain of the extent to which the public and his backers would support him, he kept this revised objective secret. When he set out in June 1910, he led even his crew to believe they were embarking on an Arctic drift, and revealed their true Antarctic destination only when \"Fram\" was leaving their last port of call, Madeira.\n\nAmundsen's South Pole expedition, with Amundsen and four others, arrived at the pole on 14 December 1911, five weeks ahead of a British party led by Robert Falcon Scott as part of the Terra Nova Expedition. Amundsen and his team returned safely to their base, and later learned that Scott and his four companions had died on their return journey.\n\n"}
{"id": "50516307", "url": "https://en.wikipedia.org/wiki?curid=50516307", "title": "Primary care service area", "text": "Primary care service area\n\nPrimary Care Service Areas are geographic areas that are self-sufficient markets of primary care. These areas are designed in a manner such that the majority of patients living in these areas use primary care services form within the area. This ensures that any geographic targeting of policies and resources reach the patients they are meant for. These geographies have been created in Australia, United States and Switzerland using big data and Geographic information systems. In Australia, while they have been developed for the state of New South Wales, they have not found application among policymakers, where, as of 2016 much larger geographies called Primary Health Networks are used for primary care management. However, they have found an especially wide audience amongst policymakers and researchers in the United States, where they were first developed. Thus for example the Health Resources and Services Administration uses them to designate areas of workforce shortage. Primary Care Service Areas are thus for example an appropriate geography for measuring primary care physician supply or geographic access to General practitioners.\n\n"}
{"id": "5298959", "url": "https://en.wikipedia.org/wiki?curid=5298959", "title": "Public open space", "text": "Public open space\n\nPublic open space is often referred to by urban planners and landscape architects by the acronym 'POS'. Varied interpretations of the term are possible.\n\n'Public' can mean:\n\n'Open' can mean:\n\nDepending on which of these definitions are adopted, any of the following could be called Public Open Space:\n\n\n\n"}
{"id": "2519712", "url": "https://en.wikipedia.org/wiki?curid=2519712", "title": "Quantitative revolution", "text": "Quantitative revolution\n\nThe quantitative revolution (QR) was a paradigm shift that sought to develop a more rigorous and systematic methodology for the discipline of geography. It came as a response to the inadequacy of regional geography to explain general spatial dynamics. The main claim for the quantitative revolution is that it led to a shift from a descriptive (idiographic) geography to an empirical law-making (nomothetic) geography. The quantitative revolution occurred during the 1950s and 1960s and marked a rapid change in the method behind geographical research, from regional geography into a spatial science.\n\nIn the history of geography, the quantitative revolution was one of the four major turning-points of modern geography – the other three being environmental determinism, regional geography and critical geography).\n\nThe quantitative revolution had occurred earlier in economics and psychology and contemporaneously in political science and other social sciences and to a lesser extent in history.\n\nDuring the late 1940s and early 1950s:\n\nAll of these events presented a threat to geography's position as an academic subject, and thus geographers began seeking new methods to counter critique.\n\nThe quantitative revolution responded to the regional geography paradigm that was dominant at the time. Debates raged predominantly (although not exclusively) in the U.S., where regional geography was the major philosophical school. In the early 1950s, there was a growing sense that the existing paradigm for geographical research was not adequate in explaining how physical, economic, social, and political processes are spatially organized, ecologically related, or how outcomes generated by them are evidence for a given time and place. A growing number of geographers started to express their dissatisfaction with the traditional paradigm of the discipline and its focus on regional geography, deeming the work as too descriptive, fragmented, and non-generalizable. To address these concerns, early critics such as Ackerman suggested the systematization of the discipline. Soon thereafter, a series of debates regarding methodological approaches in geography took place. One of the first illustrations of this was the Schaefer vs. Hartshorne debate. In 1953 \"Exceptionalism in geography: A Methodological Examination\" was published. In this work, Schaefer rejected Hartshorne’s exceptionalist interpretations about the discipline of geography and having the region as its central object of study. Instead, Schaefer envisioned as the discipline’s main objective the establishment of morphological laws through scientific inquiry, i.e. incorporating laws and methods from other disciplines in the social sciences that place a greater emphasis on processes. Hartshorne, on the other hand, addressed Schaefer’s criticism in a series of publications, where he dismissed Schaefer’s views as subjective and contradictory. He also stressed the importance of describing and classifying places and phenomena, yet admitted that there was room for employing laws of generic relationships in order to maximize scientific understanding. In his view, however, there should be no hierarchy between these two approaches.\n\nWhile debates about methods carried on, the institutionalization of systematic geography was taking place in the U.S. academy. The geography programs at the University of Iowa, Wisconsin, and Washington were pioneering programs in that respect. At the University of Iowa, Harold McCarty led efforts to establish laws of association between geographical patterns. At the University of Wisconsin, Arthur H. Robinson led efforts to develop statistical methods for map comparison. And at the University of Washington, Edward Ullman and William Garrison worked on developing the field of economic and urban geography, and central place theory. These institutions engendered a generation of geographers that established spatial analysis as part of the research agenda at other institutions including University of Chicago, Northwestern University, Loyola University, The Ohio State University, the University of Michigan, among others.\n\nThe changes introduced during the 1950s and 1960s under the banner of bringing 'scientific thinking' to geography led to an increased use of technique-based practices, including an array of mathematical techniques and computerized statistics that improved precision, and theory-based practices to conceptualize location and space in geographical research.\n\nSome of the techniques that epitomize the quantitative revolution include:\n\nThe common factor, linking the above techniques, was a preference for numbers over words, plus a belief that numerical work had a superior scientific pedigree.\n\nThe new method of inquiry led to the development of generalizations about spatial aspects in a wide range of natural and cultural settings. Generalizations may take the form of tested hypotheses, models, or theories, and the research is judged on its scientific validity, turning geography into a nomothetic science.\n\nOne of the most significant works to provide a legitimate theoretical and philosophical foundation for the reorientation of geography into a spatial science was David Harvey’s book, \"Explanation in Geography\", published in 1969. In this work, Harvey laid out two possible methodologies to explain geographical phenomena: an inductive route where generalizations are made from observation; and a deductive one where, through empirical observation, testable models and hypothesis are formulated and later verified to become scientific laws. He placed preference on the latter method. This positivist approach was countered by critical rationalism, a philosophy advanced by Karl Popper who rejected the idea of verification and maintained that hypothesis can only be falsified. Both epistemological philosophies, however, sought to achieve the same objective: to produce scientific laws and theories.\n\nThe paradigm shift had its strongest repercussions in the sub-field of economic and urban geography, especially as it pertains to location theory. However, some geographers–such as Ian Burton–expressed their dissatisfaction with quantification while others – such as Emrys Jones, Peter Lewis, and Golledge and Amedeo – debated the feasibility of law-making. Others, such as F. Luckermann, criticized the scientific explanations offered in geography as conjectural and lacking empirical basis. As a result, even models that were tested failed to accurately depict reality.\n\nBy the mid-1960s the quantitative revolution had successfully displaced regional geography from its dominant position and the paradigm shift was evident by the myriad of publications in geographical academic journals and geography textbooks. The adoption of the new paradigm allowed the discipline to be more serviceable to the public and private sectors.\n\nThe quantitative revolution had enormous implications in shaping the discipline of geography into what it looks like today given that its effects led to the spread of positivist (post-positivist) thinking and counter-positivist responses.\n\nThe rising interest in the study of distance as a critical factor in understanding the spatial arrangement of phenomena during the revolution led to formulation of the first law of geography by Waldo Tobler. The development of spatial analysis in geography led to more applications in planning process and the further development of theoretical geography offered to geographical research a necessary theoretical background.\n\nThe greater use of computers in geography also led to many new developments in geomatics, such as the creation and application of GIS and remote sensing. These new developments allowed geographers for the first time to assess complex models on a full-scale model and over space and time and the relationship between spatial entities. To some extent, the development of geomatics helped obscure the binary between physical and human geography to some extent, as the complexities of the human and natural environments could be assessed on new computable models.\n\nThe overwhelming focus on statistical modelling would, eventually, be the undoing of the quantitative revolution. Many geographers became increasingly concerned that these techniques simply put a highly sophisticated technical gloss on an approach to study that was barren of fundamental theory. Other critics argued that it removed the 'human dimension' from a discipline that always prided itself on studying the human and natural world alike. As the 1970s dawned, the quantitative revolution came under direct challenge. The counter-positivist response came as geographers began to expose the inadequacy of quantitative methods to explain and address issues regarding race, gender, class and war. On that regard, David Harvey disregarded earlier works where he advocated for the quantitative revolution and adopted a Marxist theoretical framework. Soon new subfields would emerge in human geography to contribute a new vocabulary for addressing these issues, most notably critical geography and feminist geography.\n\n\n\n"}
{"id": "3659844", "url": "https://en.wikipedia.org/wiki?curid=3659844", "title": "Rankine's method", "text": "Rankine's method\n\nRankine's method is a technique for laying out circular curves by a combination of chaining and angles at circumference, fully exploiting the theodolite and making a substantial improvement in accuracy and productivity over existing methods. \n\nRankine's method is named for its discoverer William John Macquorn Rankine at an early stage of his career. He had been working on railways in Ireland, on the construction of the Dublin and Drogheda line.\n\n"}
{"id": "27812799", "url": "https://en.wikipedia.org/wiki?curid=27812799", "title": "Ribbon farm", "text": "Ribbon farm\n\nRibbon farms (also known as strip farms, long-lot farms, or just long lots) are long, narrow land divisions, usually lined up along a waterway. In some instances, they line a road.\n\nThe size of ribbon farms can vary from lot to lot and from place to place. In Illinois, these lots could be or more long and only wide. Near Detroit, the ribbon farms were about wide and up to long. In Texas, lots could be as small as in area, or as large as .\n\nFarmers of ribbon farms typically, although not universally, built houses on the farm along the river such that the houses on a series of ribbon farms were located near each other.\n\nRibbon or strip farms were prevalent in diverse areas of the world along rivers; locations where these farms appear include in parts of Ireland, Central Europe (particularly in Germany and Poland), West Africa, Hokkaido, Brazil, and Chile. In the United States, ribbon farms are found in various places settled by the French, particularly along the Saint Lawrence River, the Great Lakes, and the Detroit River and tributaries, and parts of Louisiana. Some sections of the American Southwest, particularly Texas, also had ribbon farms laid out.\n\nIt is likely that platting farms in ribbon lots arose independently in various parts of the world. However, the ribbon farms scattered through the United States probably derive from the European model. The origin of the ribbon farm in Europe is unclear, but the first recorded appearance of these types of farms was in Germany in the ninth to eleventh century. These early German long lots were cut through forests or marshes, rather than along rivers, allowing for clustering of houses along a central road. From Germany, the pattern spread, notably to western France, where forest, marsh, and river long lots were well-established by the time the French began colonizing the Americas. By the 1630s, the long-lot pattern had been imported to the New World and established along the St Lawrence Seaway as the French seigneurial system. From there, the ribbon farm plan situated along rivers was carried to other parts of the French colonies, and diffused into some parts of the Spanish colonies.\n\nIn areas where rivers provided the main form of transportation, the ribbon farm layout gave multiple landowners access to the waterway. In addition, the long lots increased variation in soil and drainage within one lot, and facilitated plowing by minimizing the number of times oxen teams needed to be turned. Where farmers lived on their lots (rather than in a central village), the ribbon farm fostered communication and socialization, with houses clustered at the ends of the lots. The ribbon farm also strikes an economic balance, where houses are relatively close together and can be easily and economically accessed, yet the farmers need not spend excessive travel time to reach their fields some distance from a central village. Finally, in those places in the New World where ribbon farms were platted, the division of land into long rectangles was relatively easy to survey and establish boundaries.\n\nOne disadvantage was that the agricultural land of a single farmer was awkwardly spread out, often over two or more linear miles, necessitating a long travel time to reach rear parts of the lot. However, this disadvantage was generally no more than would have been experienced by peasant farmers living in a central village and walking to their fields.\n\n"}
{"id": "1515653", "url": "https://en.wikipedia.org/wiki?curid=1515653", "title": "Satellite navigation", "text": "Satellite navigation\n\nA satellite navigation or satnav system is a system that uses satellites to provide autonomous geo-spatial positioning. It allows small electronic receivers to determine their location (longitude, latitude, and altitude/elevation) to high precision (within a few metres) using time signals transmitted along a line of sight by radio from satellites. The system can be used for providing position, navigation or for tracking the position of something fitted with a receiver (satellite tracking). The signals also allow the electronic receiver to calculate the current local time to high precision, which allows time synchronisation. Satnav systems operate independently of any telephonic or internet reception, though these technologies can enhance the usefulness of the positioning information generated.\n\nA satellite navigation system with global coverage may be termed a global navigation satellite system (GNSS). , the United States' Global Positioning System (GPS) and Russia's GLONASS are fully operational GNSSs, with China's BeiDou Navigation Satellite System (BDS) and the European Union's Galileo scheduled to be fully operational by 2020. India, France and Japan are in the process of developing regional navigation and augmentation systems as well.\n\nGlobal coverage for each system is generally achieved by a satellite constellation of 18–30 medium Earth orbit (MEO) satellites spread between several orbital planes. The actual systems vary, but use orbital inclinations of >50° and orbital periods of roughly twelve hours (at an altitude of about ).\n\nSatellite navigation systems that provide enhanced accuracy and integrity monitoring usable for civil navigation are classified as follows:\n\n\nGround based radio navigation has long been practiced. The DECCA, LORAN, GEE and Omega systems used terrestrial longwave radio transmitters which broadcast a radio pulse from a known \"master\" location, followed by a pulse repeated from a number of \"slave\" stations. The delay between the reception of the master signal and the slave signals allowed the receiver to deduce the distance to each of the slaves, providing a fix.\n\nThe first satellite navigation system was Transit, a system deployed by the US military in the 1960s. Transit's operation was based on the Doppler effect: the satellites travelled on well-known paths and broadcast their signals on a well-known radio frequency. The received frequency will differ slightly from the broadcast frequency because of the movement of the satellite with respect to the receiver. By monitoring this frequency shift over a short time interval, the receiver can determine its location to one side or the other of the satellite, and several such measurements combined with a precise knowledge of the satellite's orbit can fix a particular position. Satellite orbital position errors are induced by variations in the gravity field and radar refraction, among others. These were resolved by a team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970-1973. Using real-time data assimilation and recursive estimation, the systematic and residual errors were narrowed down to a manageable level to permit accurate navigation. \n\nPart of an orbiting satellite's broadcast included its precise orbital data. In order to ensure accuracy, the US Naval Observatory (USNO) continuously observed the precise orbits of these satellites. As a satellite's orbit deviated, the USNO would send the updated information to the satellite. Subsequent broadcasts from an updated satellite would contain its most recent ephemeris.\n\nModern systems are more direct. The satellite broadcasts a signal that contains orbital data (from which the position of the satellite can be calculated) and the precise time the signal was transmitted. The orbital ephemeris is transmitted in a data message that is superimposed on a code that serves as a timing reference. The satellite uses an atomic clock to maintain synchronization of all the satellites in the constellation. The receiver compares the time of broadcast encoded in the transmission of three (at sea level) or four different satellites, thereby measuring the time-of-flight to each satellite. Several such measurements can be made at the same time to different satellites, allowing a continual fix to be generated in real time using an adapted version of trilateration: see GNSS positioning calculation for details.\n\nEach distance measurement, regardless of the system being used, places the receiver on a spherical shell at the measured distance from the broadcaster. By taking several such measurements and then looking for a point where they meet, a fix is generated. However, in the case of fast-moving receivers, the position of the signal moves as signals are received from several satellites. In addition, the radio signals slow slightly as they pass through the ionosphere, and this slowing varies with the receiver's angle to the satellite, because that changes the distance through the ionosphere. The basic computation thus attempts to find the shortest directed line tangent to four oblate spherical shells centred on four satellites. Satellite navigation receivers reduce errors by using combinations of signals from multiple satellites and multiple correlators, and then using techniques such as Kalman filtering to combine the noisy, partial, and constantly changing data into a single estimate for position, time, and velocity.\n\nThe original motivation for satellite navigation was for military applications. Satellite navigation allows precision in the delivery of weapons to targets, greatly increasing their lethality whilst reducing inadvertent casualties from mis-directed weapons. (See Guided bomb). Satellite navigation also allows forces to be directed and to locate themselves more easily, reducing the fog of war.\n\nThe ability to supply satellite navigation signals is also the ability to deny their availability. The operator of a satellite navigation system potentially has the ability to degrade or eliminate satellite navigation services over any territory it desires.\n\nThe United States' Global Positioning System (GPS) consists of up to 32 medium Earth orbit satellites in six different orbital planes, with the exact number of satellites varying as older satellites are retired and replaced. Operational since 1978 and globally available since 1994, GPS is the world's most utilized satellite navigation system.\n\nThe formerly Soviet, and now Russian, \"Global'naya Navigatsionnaya Sputnikovaya Sistema\", (GLObal NAvigation Satellite System or GLONASS), is a space-based satellite navigation system that provides a civilian radionavigation-satellite service and is also used by the Russian Aerospace Defence Forces. GLONASS has full global coverage with 24 satellites.\n\nThe European Union and European Space Agency agreed in March 2002 to introduce their own alternative to GPS, called the Galileo positioning system. Galileo became operational on 15 December 2016 (global Early Operational Capability (EOC)) At an estimated cost of €3 billion, the system of 30 MEO satellites was originally scheduled to be operational in 2010. The original year to become operational was 2014. The first experimental satellite was launched on 28 December 2005. Galileo is expected to be compatible with the modernized GPS system. The receivers will be able to combine the signals from both Galileo and GPS satellites to greatly increase the accuracy. Galileo is expected to be in full service in 2020 and at a substantially higher cost.\nThe main modulation used in Galileo Open Service signal is the Composite Binary Offset Carrier (CBOC) modulation.\n\nChina has indicated their plan to complete the entire second generation Beidou Navigation Satellite System (BDS or BeiDou-2, formerly known as COMPASS), by expanding current regional (Asia-Pacific) service into global coverage by 2020. The BeiDou-2 system is proposed to consist of 30 MEO satellites and five geostationary satellites. A 16-satellite regional version (covering Asia and Pacific area) was completed by December 2012.\n\nChinese regional (Asia-Pacific, 16 satellites) network to be expanded into the whole BeiDou-2 global system which consists of all 35 satellites by 2020.\n\nThe NAVIC or NAVigation with Indian Constellation is an autonomous regional satellite navigation system developed by Indian Space Research Organisation (ISRO) which would be under the total control of Indian government. The government approved the project in May 2006, with the intention of the system completed and implemented on 28 April 2016. It will consist of a constellation of 7 navigational satellites. 3 of the satellites will be placed in the Geostationary orbit (GEO) and the remaining 4 in the Geosynchronous orbit(GSO) to have a larger signal footprint and lower number of satellites to map the region. It is intended to provide an all-weather absolute position accuracy of better than 7.6 meters throughout India and within a region extending approximately 1,500 km around it. A goal of complete Indian control has been stated, with the space segment, ground segment and user receivers all being built in India. All seven satellites, IRNSS-1A, IRNSS-1B, IRNSS-1C, IRNSS-1D, IRNSS-1E, IRNSS-1F, and IRNSS-1G, of the proposed constellation were precisely launched on 1 July 2013, 4 April 2014, 16 October 2014, 28 March 2015, 20 January 2016, 10 March 2016 and 28 April 2016 respectively from Satish Dhawan Space Centre. The system is expected to be fully operational by August 2016.\n\nThe Quasi-Zenith Satellite System (QZSS) is a proposed four-satellite regional time transfer system and enhancement for GPS covering Japan and the Asia-Oceania regions. QZSS services are available on a trial basis as of January 12, 2018, and are scheduled to be launched in November 2018. The first satellite was launched in September 2010.\n\nSources: \n\nGNSS augmentation is a method of improving a navigation system's attributes, such as accuracy, reliability, and availability, through the integration of external information into the calculation process, for example, the Wide Area Augmentation System, the European Geostationary Navigation Overlay Service, the Multi-functional Satellite Augmentation System, Differential GPS, GPS Aided GEO Augmented Navigation (GAGAN) and inertial navigation systems.\n\nDoppler Orbitography and Radio-positioning Integrated by Satellite (DORIS) is a French precision navigation system. Unlike other GNSS systems, it is based on static emitting stations around the world, the receivers being on satellites, in order to precisely determine their orbital position. The system may be used also for mobile receivers on land with more limited usage and coverage. Used with traditional GNSS systems, it pushes the accuracy of positions to centimetric precision (and to millimetric precision for altimetric application and also allows monitoring very tiny seasonal changes of Earth rotation and deformations), in order to build a much more precise geodesic reference system.\n\nThe two current operational low Earth orbit satellite phone networks are able to track transceiver units with accuracy of a few kilometers using doppler shift calculations from the satellite. The coordinates are sent back to the transceiver unit where they can be read using AT commands or a graphical user interface. This can also be used by the gateway to enforce restrictions on geographically bound calling plans.\n\n\n\n\n"}
{"id": "13251530", "url": "https://en.wikipedia.org/wiki?curid=13251530", "title": "Schleberoda", "text": "Schleberoda\n\nSchleberoda is a village and a former municipality in the Burgenlandkreis district, in Saxony-Anhalt, Germany. Since 1 July 2009, it is part of the town Freyburg.\n\nIt has been proposed by Germany for inscription in the List of World Heritage. The World Heritage nomination Naumburg Cathedral and the High Medieval Cultural Landscape of the Rivers Saale and Unstrut is representative for the processes that shaped the continent during the High Middle Ages between 1000 and 1300: Christianization, the so-called Landesausbau and the dynamics of cultural exchange and transfer characteristic for this very period.\n\nSchleberoda is located north of the Unstrut. It was founded as a result of intensive land development by clearance in the High Middle Ages such as other villages with ″root″ (Roda) in the name (e.g. Albersroda, Baumersorda, Schnellroda, Ebersroda and Schleberoda). \nThe closely bordering local nobles in the area separated their territories using stone landmarks, as is still visible at the historic edges of the nearby woods, authentic landmarks which have been unchanged since the High Middle Ages: The clearance work and founding of new villages ran southwards from the original parish of Mücheln and ended at the Neue Göhle forest, which belonged to the territory of the Landgraves of Thuringia and could therefore not be cleared.<br>\nThe forest Neue Göhle (Old Sorbian name gola = heath forest) has survived as a continuous, large forest complex. It was used for the production of pole wood in the context of winegrowing at a very early point in time, which is why it had been afforested in the form of coppice-with-standards. Stone landmarks, ditches, banks and paths are still visible at the historic edges of this woods separated the territories of the closely bordering local nobles in the area.\n\nSchleberoda has retained the structures and authentic form dating from the time of its foundation. It is an examples of the land development in the contact area between Germans and Slavs in the High Middle Ages. It has been able to retain its original ground plan as a radial round village dating from the High Middle Ages. Around the original village pond, serving for fire-fighting, and the bake house, replaced at the end of the 18th century, forward-gabled and side-gabled buildings are grouped, some made with packed loam, with high-quality portals dating from the 16th to 19th century. \nThe position of the former gate to the compact village settlement is still identifiable today. Including the outer boundary of the official village area is not only identifiable by a circumferential path and has survived in its original form to the north-west in the shape of an impenetrable hedge. The Romanesque choir tower of the village church with coupled abat sons has survived.\n\n\n"}
{"id": "40676951", "url": "https://en.wikipedia.org/wiki?curid=40676951", "title": "Standard port", "text": "Standard port\n\nA Standard port is a port whose tidal predictions are directly given in the Tide tables.\n\nTide predictions for standard ports are based on continuous observation of tide over a period of at least one year. These predictions are given in feet or meters, with respect to the chart datum for average meteorological conditions. Some examples of primary ports are Bhavnagar, Bombay, Cochin, Manila, Karachi, Singapore, Suez, Port Dickson, etc. Standard ports are marked in bold letters in the Index pages of Admiralty Tide Tables (or Tidal almanac). Tidal predictions are usually made by the Bureau of Meteorology of that country.\n\nSecondary ports (also called \"Secondary place\" in some countries like Australia), on the other hand, are ports for which tides have to be calculated, based on a primary port with a similar tidal curve. Some examples of Secondary ports are Rabigh, Porto Novo, Porbandar, Port Cornwalis, Sharjah, The Sandheads (Hugli), etc.\n"}
{"id": "36374608", "url": "https://en.wikipedia.org/wiki?curid=36374608", "title": "TVMDC", "text": "TVMDC\n\nTVMDC is a method for converting true, magnetic and compass headings. TVMDC is expressed as: True, variation, magnetic, deviation, compass. The most common use of the TVMDC method is correcting courses during nautical navigation.\n\nThe Geographic North Pole and the Magnetic North Pole are not exactly the same position on the earth. From any position on the globe, a direction can be determined to either the geographic north pole or to the magnetic north pole. These directions are expressed in degrees from 0-360, and also fractions of a degree. The difference between these two directions is known as magnetic variation. While a compass is installed in a vehicle or vessel, local anomalies of the vessel can introduce error into the direction that the compass points. The difference between the direction to Magnetic North Pole and the direction that the compass is pointing is known as magnetic deviation.\n\nMagnetic variation is different depending on the geographic position on the globe. The Magnetic North Pole is currently in Northern Canada and is moving generally South. A straight line can be drawn from the Geographic North Pole, down to the Magnetic North Pole and then drawn straight down to the equator. This line is known as the agonic line, and the line is also moving. In the year 1900, the Agonic line passed roughly through Detroit and then was East of Florida. It currently passes roughly west of Chicago, IL, and through New Orleans, LA. If a navigator is located on the agonic line, then Variation is zero. Magnetic North Pole and the Geographic North Pole appear to be directly in line with each other, while the navigator is located on the Agonic line. If a navigator is East of the agonic line, then the variation is westward. Magnetic North appears slightly west of the Geographic North Pole. If a navigator is West of the Agonic Line, then the Variation is Eastward. The magnetic North Pole appears to the East of the geographic North Pole. The further the navigator is from the agonic line, the greater the variation. The local magnetic variation is indicated on NOAA Nautical Charts at the center of the compass rose. The Magnetic Variation is indicated along with the year of that Variation. The annual increase or decrease of the Variation is also usually indicated. So that the Variation for the current year can be calculated.\n\nA compass installed in a vehicle or vessel has a certain amount of error caused by the magnetic properties of the vessel. This error is known as compass deviation. The magnitude of the compass deviation varies greatly depending upon the local anomalies created by the vessel. A fiberglass recreational vessel will generally have much less compass deviation then a steel hulled vessel. Electrical wires carrying current have a small magnetic field around them and can cause deviation. Any type of magnet, such as found in a speaker can also cause large magnitudes of compass deviation. The error can be corrected using a Deviation Table. Deviation Tables are very difficult to create. Once a deviation table is established, it is only good for that particular vessel, with that particular configuration. If electrical wires are moved or anything else magnetic (speakers, electric motors, etc.) are moved, the deviation table will change. All Deviations on the Deviation Table are indicated West or East. If the compass is pointing West of the Magnetic North Pole, then the Deviation is Westward. If the compass is pointing East of the Magnetic North Pole, then the Deviation is Eastward.\n\nCalculating TVMDC is done with simple arithmetic. First arrange the values vertically: \nThe formula is always added moving down, and subtracted when moving up. The most complicated part is determining if the values are positive or negative. The True, Magnetic, and Compass Values are directions on the compass, they must always be a positive number between 0 - 360. Deviation and Variation can be positive or negative. If either Variation or Deviation is Westward, then the values are entered into the equation as positive. If the Variation or Deviation is Eastward, then the values are entered into the formula as a negative.\n\nTrue course is 120°, the Variation is 5° West, and the Deviation is 1° West.\n\nTrue course is 120°, the Variation is 5° East and the Deviation is 1° East.\n\nTrue course is 035°, the Variation is 4° West and the Deviation is 1° East.\n\nTrue course is 306°, the Variation is 4° East and the Deviation is 11° West.\n\nThe formula can also be calculated in reverse. The formula is subtracted when moving up.\nCompass course is 093°, the Deviation is 4° West and the Variation is 3° West.\n"}
{"id": "10419768", "url": "https://en.wikipedia.org/wiki?curid=10419768", "title": "The Professional Geographer", "text": "The Professional Geographer\n\nThe Professional Geographer is a quarterly peer-reviewed academic journal publishing short articles on all aspects of geography. The journal is published by Taylor and Francis on behalf of the American Association of Geographers. According to the Journal Citation Reports, the journal has a 2015 impact factor of 1.407, ranking it 29th out of 77 journals in the category \"Geography\".\n\nEvery year, the journal publishes a special section with papers that were finalists for the J. Warren Nystrom Award, given to the best paper based upon a recent dissertation in geography. \n\nThe journal is abstracted and indexed in:\n"}
{"id": "2857072", "url": "https://en.wikipedia.org/wiki?curid=2857072", "title": "Triangulated irregular network", "text": "Triangulated irregular network\n\nA triangulated irregular network (TIN) is a representation of a continuous surface consisting entirely of triangular facets, used mainly as Discrete Global Grid in primary elevation modeling. \n\nThe vertices of these triangles are created from field recorded spot elevations through a variety of means including surveying through conventional, Global Positioning System Real-Time Kinematic (GPS RTK), photogrammetry, or some other means. Associated with three-dimensional data (\"x\", \"y\", and \"z\") and topography, TINs are useful for the description and analysis of general horizontal (\"x\" and \"y\") distributions and relationships.\n\nDigital TIN data structures are used in a variety of applications, including geographic information systems (GIS), and computer aided drafting (CAD) for the visual representation of a topographical surface. A TIN is an vector-based representation of the physical land surface or sea bottom, made up of irregularly distributed nodes and lines with three-dimensional coordinates (\"x\", \"y\", and \"z\") that are arranged in a network of non-overlapping triangles.\n\nA TIN comprises a triangular network of vertices, known as mass points, with associated coordinates in three dimensions connected by edges to form a triangular tessellation. Three-dimensional visualizations are readily created by rendering of the triangular facets. In regions where there is little variation in surface height, the points may be widely spaced whereas in areas of more intense variation in height the point density is increased.\n\nA TIN used to represent terrain is often called a digital elevation model (DEM), which can be further used to produce digital surface models (DSM) or digital terrain models (DTM). An advantage of using a TIN over a rasterized digital elevation model (DEM) in mapping and analysis is that the points of a TIN are distributed variably based on an algorithm that determines which points are most necessary to create an accurate representation of the terrain. Data input is therefore flexible and fewer points need to be stored than in a raster DEM, with regularly distributed points. While a TIN may be considered less suited than a raster DEM for certain kinds of GIS applications, such as analysis of a surface's slope and aspect, it is often used in CAD to create contour lines. A DTM and DSM can be formed from a DEM. A DEM can be interpolated from a TIN.\n\nTIN are based on a Delaunay triangulation or constrained Delaunay. Delaunay conforming triangulations are recommended over constrained triangulations. This is because the resulting TINs are likely to contain fewer long, skinny triangles, which are undesirable for surface analysis. Additionally, natural neighbor interpolation and Thiessen (Voronoi) polygon generation can only be performed on Delaunay conforming triangulations. A constrained Delaunay triangulation can be considered when you need to explicitly define certain edges that are guaranteed not to be modified (that is, split into multiple edges) by the triangulator. Constrained Delaunay triangulations are also useful for minimizing the size of a TIN, since they have fewer nodes and triangles where breaklines are not densified.\n\nThe TIN model was developed in the early 1970s as a simple way to build a surface from a set of irregularly spaced points. The first triangulated irregular network program for GIS was written by W. Randolph Franklin, under the direction of David Douglas and Thomas Peucker (Poiker), at Simon Fraser University in 1973.\n\n"}
{"id": "8331945", "url": "https://en.wikipedia.org/wiki?curid=8331945", "title": "Ultra-short baseline", "text": "Ultra-short baseline\n\nUSBL (ultra-short baseline, also sometimes known as SSBL for super short base line) is a method of underwater acoustic positioning. A complete USBL system consists of a transceiver, which is mounted on a pole under a ship, and a transponder or responder on the seafloor, on a towfish, or on an ROV. A computer, or \"topside unit\", is used to calculate a position from the ranges and bearings measured by the transceiver.\n\nAn acoustic pulse is transmitted by the transceiver and detected by the subsea transponder, which replies with its own acoustic pulse. This return pulse is detected by the shipboard transceiver. The time from the transmission of the initial acoustic pulse until the reply is detected is measured by the USBL system and is converted into a range.\n\nTo calculate a subsea position, the USBL calculates both a range and an angle from the transceiver to the subsea beacon. Angles are measured by the transceiver, which contains an array of transducers. The transceiver head normally contains three or more transducers separated by a baseline of 10 cm or less. A method called “phase-differencing” within this transducer array is used to calculate the direction to the subsea transponder.\n\nUSBLs have also begun to find use in \"inverted\" (iUSBL) configurations, with the transceiver mounted on an autonomous underwater vehicle, and the transponder on the target. In this case, the \"topside\" processing happens inside the vehicle to allow it to locate the transponder for applications such as automatic docking and target tracking.\n\n\n"}
{"id": "600470", "url": "https://en.wikipedia.org/wiki?curid=600470", "title": "Upstate", "text": "Upstate\n\nThe term upstate may refer to the northerly portions of several U.S. states. On the east coast, upstate generally refers to places away from the Atlantic Ocean. It also can refer to parts of states that have a higher elevation, away from sea level. These regions tend to be rural; an exception being Delaware.\n\n\n"}
