{"id": "7304484", "url": "https://en.wikipedia.org/wiki?curid=7304484", "title": "Alexander Rosenberg", "text": "Alexander Rosenberg\n\nAlexander Rosenberg (born 1946) is an American philosopher, and the R. Taylor Cole Professor of Philosophy at Duke University. He is also a novelist.\n\nRosenberg attended the City College of New York where he graduated with a B.A. in 1967. He received his Ph.D. from the Johns Hopkins University in 1971. He won the Lakatos Award in 1993 and was the National Phi Beta Kappa Romanell Lecturer in 2006.\n\nRosenberg is an atheist, and a metaphysical naturalist.\n\nRosenberg's early work focused on the philosophy of social science and especially the philosophy of economics. His doctoral dissertation, published as \"Microeconomic Laws\" in 1976, was the first treatment of the nature of economics by a contemporary philosopher of science. Over the period of the next decade he became increasingly skeptical about neoclassical economics as an empirical theory.\n\nHe later shifted to work on issues in the philosophy of science that are raised by biology. He became especially interested in the relationship between molecular biology and other parts of biology. Rosenberg introduced the concept of supervenience to the treatment of intertheoretical relations in biology, soon after Donald Davidson began to exploit Richard Hare's notion in the philosophy of psychology. Rosenberg is among the few biologists and fewer philosophers of science who reject the consensus view that combines physicalism with antireductionism (see his 2010 on-line debate with John Dupré at Philosophy TV).\n\nRosenberg also coauthored an influential book on David Hume with Tom Beauchamp, \"Hume and the Problem of Causation\", arguing that Hume was not a skeptic about induction but an opponent of rationalist theories of inductive inference.\n\nRosenberg’s treatment of fitness as a supervenient property, which is an undefined concept in the theory of natural selection, is criticized by Brandon and Beatty. His original development of how the supervenience of Mendelian concepts blocks traditional derivational reduction was examined critically by C. Kenneth Waters. His later account of reduction in developmental biology was criticized by Günter Wagner. Elliott Sober's \"Multiple realization arguments against reductionism\" reflects a shift towards Rosenberg's critique of anti-reductionist arguments of Putnam's and Fodor's.\n\nSober has also challenged Rosenberg’s view that the principle of natural selection is the only biological law.\n\nThe explanatory role of the principle of natural selection and the nature of evolutionary probabilities defended by Rosenberg were subject to counter arguments by Brandon and later by Denis Walsh. Rosenberg's account of the nature of genetic drift and the role of probability in the theory of natural selection draws on significant parallels between the principle of natural selection and the second law of thermodynamics.\n\nIn the philosophy of social science, Rosenberg’s more skeptical views about microeconomics were challenged first by Wade Hands and later by Daniel Hausman in several books and articles. The financial crisis of 2008–09 resulted in renewed attention to Rosenberg's skeptical views about microeconomics. Biologist Richard Lewontin and historian Joseph Fracchia express skepticism about Rosenberg’s claim that functional explanations in social science require Darwinian underlying mechanisms.\n\nIn 2011 Rosenberg published a defense of what he called \"Scientism\"—the claim that \"the persistent questions\" people ask about the nature of reality, the purpose of things, the foundations of value and morality, the way the mind works, the basis of personal identity, and the course of human history, could all be answered by the resources of science. This book was attacked on the front cover of The New Republic by Leon Wieseltier as \"The worst book of the year\". Leon Wiseltier's claim, in turn, was critiqued as exaggeration by Philip Kitcher in The New York Times Book Review. On February 1, 2013, Rosenberg debated Christian philosopher William Lane Craig over the topics discussed in \"The Atheist's Guide to Reality.\"\n\nRosenberg has contributed articles to \"The New York Times\" Op/Ed series The Stone, on naturalism, science and the humanities, and meta-ethics, and the mind's powers to understand itself by introspection that arise from the views he advanced in \"The Atheist's Guide to Reality.\"\n\nRosenberg’s 2015 novel, \"The Girl From Krakow\", Lake Union Publishing, is a narrative about a young woman named Rita Feuerstahl from 1935 to 1947, mainly focusing on her struggles to survive in Nazi-occupied Poland and later in Germany, under a false identity. A secondary plot involves her lover’s experiences in France and Spain during its Civil War in the 1930s and then in Moscow during the war. Rosenberg has acknowledged that the novel is based on the wartime experiences of people he knew. He has also admitted the incongruity of writing a narrative, given his attack on the form in \"The Atheist’s Guide to Reality\". He has said that \"The Girl from Krakow\" began as an attempt to put some of the difficult arguments of \"The Atheist’s Guide to Reality\" into a form easier to grasp\". \"The Girl From Krakow\" has been translated into Italian, Hungarian, Polish and Hebrew.\n\nIn 2016 Rosenberg’s second novel, \"Autumn in Oxford\", appeared, also published by Lake Union Publishing. An afterword identifies the large number of real persons—academics, civil rights advocates, military officers, politicians and intelligence agents from the 1940s and '50s who figure in the narrative.\n\nIn 2018 Rosenberg published ’‘How History Gets Things Wrong: The Neuroscience of our Addiction to Stories’‘ This work develops the eliminative materialism of ‘‘The Atheist’s guide to Reality‘‘ applying it to the role ‘the theory of mind’ plays in history and other forms of story telling. Rosenberg argues that the work of Nobel Prize winners, Eric Kandel, John O'Keefe and May-Britt Moser along with Edvard Moser reveals that the ‘‘theory of mind‘‘ employed in every day life and narrative history has no basis in the organization of the brain. Evidence from evolutionary anthropology, child psychology, medical diagnosis and neural imaging reveals it is an innate or almost innate tool that arose in Hominini evolution to foster collaboration among small numbers of individuals in immediate contact over the near future, but whose predictive weakness beyond this domain reveals its explanatory emptiness. \n\n\n"}
{"id": "55257518", "url": "https://en.wikipedia.org/wiki?curid=55257518", "title": "Amity-enmity complex", "text": "Amity-enmity complex\n\nThe amity-enmity complex was a term introduced by Sir Arthur Keith. His work, \"A New Theory of Human Evolution\" (1948), posited that humans evolved as differing races, tribes, and cultures, exhibiting patriotism, morality, leadership and nationalism. Those who belong are part of the in-group, and tolerated; all others are classed as out-group, and subject to hostility; 'The code of enmity is a necessary part of the machinery of evolution. He who feels generous towards his enemy... has given up his place in the turmoil of evolutionary competition.' Conscience in humans evolved a duality; to protect and save friends,\nand also to hate and fight enemies. \nKeith's work summarized earlier opinions on human tribalism by Charles Darwin, Alfred Russel Wallace, and Herbert Spencer.\n\n\nThe amity-enmity complex maintains 'tribal spirit' and thus unity, of the community, 'as long as personal contact between its members is possible.' If the community grows beyond this limitation, then disruption, swarming and disintegration occur. Modern mass communication enables communities 'of 100 million' to remain intact.\n\nKeith expressed regret that this phenomenon, which explains so much, had not become common knowledge: \"[W]e eternally experience the misery... of each new manifestation of the complex, then invent some new 'ism' to categorise this behavior as an evil, dealing with a common behavioural trait piecemeal [instead of] finally grasping and understanding the phenomenon.\"\n\nColleges, sports teams, churches, trades unions, female fashions and political parties enable people to exhibit tribal loyalty within large, mass-communicating nations. 'In politics we have to take sides.' But all these 'petty manifestations' are cast aside in time of war.\nBismarck, Abraham Lincoln and Lloyd George are cited as statesmen who knew how to exploit the tribal spirit for political ends.\n\nRobert Ardrey pointed out that similar behavior can be observed in most primates, especially baboons and chimps. \"Nationalism as such is no more than a human expression of the animal drive to maintain and defend a territory... the mentality of the single Germanic tribe under Hitler differed in no way from that of early man or late baboon.\"\n\nThe amity-enmity complex is a serious obstacle to world peace and world government, and may even lead to nuclear holocaust: \"How can we get along without war?... if we fail to get along without war, the future will be as lacking in human problems as it will be remarkably lacking in men.\"\n\nDesmond Morris makes a prescriptive point: \"We must try to step outside our groups and look down on human battlefields with the unbiased eye of a hovering Martian.\" And he warns that \"the truly violent species all appear to have exterminated themselves, a lesson we should not overlook.\" The inherited aggression of the amity-enmity rivalry between communities is rationalized under a \"persistent cloak of ideology... a matter of ideals, moral principles, social philosophies or religious beliefs... [O]nly an immense amount of intellectual restraint will save the situation.\"\n\nAfter World War Two, a debate about the place of instinct and learning (the nature-versus-nurture debate) has occurred. According to Steven Pinker, the \"bitter lessons of lynchings, world wars, and the Holocaust\" have caused \"prevailing theories of mind\" to be \"refashioned to make racism and sexism as untenable as possible. The doctrine of the blank slate became entrenched in intellectual life.\"\n\nPinker makes the point that \"conflicts of interest are inherent to the human condition.\" Man is a product of nature, as much as malarial mosquitoes; both \"are doing exactly what evolution designed them to do, even if the outcome makes people suffer... [We] cannot call their behavior pathological... [T]he belief that violence is an aberration is dangerous.\"\n\n"}
{"id": "31167381", "url": "https://en.wikipedia.org/wiki?curid=31167381", "title": "Asperity (geotechnical engineering)", "text": "Asperity (geotechnical engineering)\n\nIn Geotechnical engineering the term asperity is mostly used for unevenness (\"roughness\") of the surface of a discontinuity, grain, or particle with heights in the range from approximately 0.1 mm to many decimetre. Smaller unevenness is normally considered to be a \"material\" property (often denoted by \"material friction\" or \"basic material friction\").\n\nAn often used definition for \"asperities\" in geotechnical engineering:\nUnevenness of a surface are \"asperities\" if these cause dilation if two blocks with in between a discontinuity with matching \"asperities\" on the two opposing surfaces (i.e. a \"fitting discontinuity\") move relative to each other, under low stress levels that do not cause breaking of the \"asperities\".\n\nMaterials science recognizes asperities ranging from the sub-visual (normally less than 0.1 mm) to the atomic scale.\n\n"}
{"id": "25508508", "url": "https://en.wikipedia.org/wiki?curid=25508508", "title": "Building Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards", "text": "Building Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards\n\nBuilding Safer Communities. Risk Governance, Spatial Planning and Responses to Natural Hazards is a 2009 book edited by Urbano Fra Paleo, published by IOS Press.\n\nThis textbook examines the central principles of enhanced risk governance, whose implementation might help to mitigate the increasing losses caused by natural hazards. It promotes the adoption of proactive, preventive approaches in public policies, particularly through land use planning, by influencing on the occupation of hazard-prone areas.\nIt serves both as a comprehensive introduction to the formulation and implementation at the strategic level of policies that address risk, and as an advancement in the integration of current practices, including emergency management, environmental management, community development and spatial planning. \nThe authors study and construe solutions that review integrated strategies of the various levels of government considering:\n\nUrbano Fra Paleo is a geographer, and an Associate Professor of Human Geography at the University of Santiago de Compostela, Spain.\n\n"}
{"id": "4024", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "Butterfly effect\n\nIn chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nThe term, coined by Edward Lorenz, is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\n\nThough Lorenz gave a name to the phenomenon, the idea that small causes may have large effects in general and in weather specifically was earlier recognized by French mathematician and engineer Henri Poincaré and American mathematician and philosopher Norbert Wiener. Edward Lorenz's work placed the concept of \"instability\" of the earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\n\nThe butterfly effect can also be demonstrated by very simple systems.\n\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says that \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\n\nChaos theory and the sensitive dependence on initial conditions were described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\n\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\n\nThe idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury about time travel.\n\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\n\nLorenz wrote:\nIn 1963 Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated: Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\n\nThe phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different—but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\n\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\n\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.\n\nRecurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.\n\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any δ > 0, there are y in \"M\", with distance \"d\"(. , .) such that formula_3 and such that\n\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent.\n\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\n\nwhich, unlike most chaotic maps, has a closed-form solution:\n\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2 shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range [0, 1].\n\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat overstated.\"\n\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.\n\nOther authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\n\nThe journalist Peter Dizikes, writing in \"The Boston Globe\" in 2008, notes that popular culture likes the idea of the butterfly effect, but gets it wrong. Whereas Lorenz suggested correctly with his butterfly metaphor that predictability \"is inherently limited\", popular culture supposes that each event can be explained by finding the small reasons that caused it. Dizikes explains: \"It speaks to our larger expectation that the world should be comprehensible – that everything happens for a reason, and that we can pinpoint all those reasons, however small they may be. But nature itself defies this expectation.\"\n\n\n\n"}
{"id": "7807", "url": "https://en.wikipedia.org/wiki?curid=7807", "title": "Cavitation", "text": "Cavitation\n\nCavitation is the formation of vapour cavities in a liquid, small liquid-free zones (\"bubbles\" or \"voids\"), that are the consequence of forces acting upon the liquid. It usually occurs when a liquid is subjected to rapid changes of pressure that cause the formation of cavities in the liquid where the pressure is relatively low. When subjected to higher pressure, the voids implode and can generate an intense shock wave.\n\nCavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n\nInertial cavitation is the process where a void or bubble in a liquid rapidly collapses, producing a shock wave. Inertial cavitation occurs in nature in the strikes of mantis shrimps and pistol shrimps, as well as in the vascular tissues of plants. In man-made objects, it can occur in control valves, pumps, propellers and impellers.\n\nNon-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.\n\nSince the shock waves formed by collapse of the voids are strong enough to cause significant damage to moving parts, cavitation is usually an undesirable phenomenon. It is very often specifically avoided in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.\n\nInertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined \"cavitation inception\" and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.\n\nOther ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a perfect vacuum, but has a relatively low gas pressure. Such a low-pressure bubble in a liquid begins to collapse due to the higher pressure of the surrounding medium. As the bubble collapses, the pressure and temperature of the vapor within increases. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.\n\nInertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n\nThe physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local temperature of the liquid reaches the saturation temperature, and further heat is supplied to allow the liquid to sufficiently phase change into a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.\n\nIn order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold.\n\nThe vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n\nNon-inertial cavitation is the process in which small bubbles in a liquid are forced to oscillate in the presence of an acoustic field, when the intensity of the acoustic field is insufficient to cause total bubble collapse. This form of cavitation causes significantly less erosion than inertial cavitation, and is often used for the cleaning of delicate materials, such as silicon wafers.\n\nHydrodynamic cavitation describes the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n\nHydrodynamic cavitation can be produced by passing a liquid through a constricted channel at a specific flow velocity or by mechanical rotation of an object through a liquid. In the case of the constricted channel and based on the specific (or unique) geometry of the system, the combination of pressure and kinetic energy can create the hydrodynamic cavitation cavern downstream of the local constriction generating high energy cavitation bubbles.\n\nThe process of bubble generation, and the subsequent growth and collapse of the cavitation bubbles, results in very high energy densities and in very high local temperatures and local pressures at the surface of the bubbles for a very short time. The overall liquid medium environment, therefore, remains at ambient conditions. When uncontrolled, cavitation is damaging; by controlling the flow of the cavitation, however, the power can be harnessed and non-destructive. Controlled cavitation can be used to enhance chemical reactions or propagate certain unexpected reactions because free radicals are generated in the process due to disassociation of vapors trapped in the cavitating bubbles.\n\nOrifices and venturi are reported to be widely used for generating cavitation. A venturi has an inherent advantage over an orifice because of its smooth converging and diverging sections, such that it can generate a higher flow velocity at the throat for a given pressure drop across it. On the other hand, an orifice has an advantage that it can accommodate a greater number of holes (larger perimeter of holes) in a given cross sectional area of the pipe.\n\nThe cavitation phenomenon can be controlled to enhance the performance of high-speed marine vessels and projectiles, as well as in material processing technologies, in medicine, etc. Controlling the cavitating flows in liquids can be achieved only by advancing the mathematical foundation of the cavitation processes. These processes are manifested in different ways, the most common ones and promising for control being bubble cavitation and supercavitation. The first exact classical solution should perhaps be credited to the well- known solution by H. Helmholtz in 1868. The earliest distinguished studies of academic type on the theory of a cavitating flow with free boundaries and supercavitation were published in the book \"Jets, wakes and cavities\" followed by \"Theory of jets of ideal fluid\". Widely used in these books was the well-developed theory of conformal mappings of functions of a complex variable, allowing one to derive a large number of exact solutions of plane problems. Another venue combining the existing exact solutions with approximated and heuristic models was explored in the work \"Hydrodynamics of Flows with Free Boundaries\" that refined the applied calculation techniques based on the principle of cavity expansion independence, theory of pulsations and stability of elongated axisymmetric cavities, etc. and in \"Dimensionality and similarity methods in the problems of the hydromechanics of vessels\".\n\nA natural continuation of these studies was recently presented in \"The Hydrodynamics of Cavitating Flows\" – an encyclopedic work encompassing all the best advances in this domain for the last three decades, and blending the classical methods of mathematical research with the modern capabilities of computer technologies. These include elaboration of nonlinear numerical methods of solving 3D cavitation problems, refinement of the known plane linear theories, development of asymptotic theories of axisymmetric and nearly axisymmetric flows, etc. As compared to the classical approaches, the new trend is characterized by expansion of the theory into the 3D flows. It also reflects a certain correlation with current works of an applied character on the hydrodynamics of supercavitating bodies.\n\nHydrodynamic cavitation can also improve some industrial processes. For instance, cavitated corn slurry shows higher yields in ethanol production compared to uncavitated corn slurry in dry milling facilities.\n\nThis is also used in the mineralization of bio-refractory compounds which otherwise would need extremely high temperature and pressure conditions since free radicals are generated in the process due to the dissociation of vapors trapped in the cavitating bubbles, which results in either the intensification of the chemical reaction or may even result in the propagation of certain reactions not possible under otherwise ambient conditions.\n\nIn industry, cavitation is often used to homogenize, or mix and break down, suspended particles in a colloidal liquid compound such as paint mixtures or milk. Many industrial mixing machines are based upon this design principle. It is usually achieved through impeller design or by forcing the mixture through an annular opening that has a narrow entrance orifice with a much larger exit orifice. In the latter case, the drastic decrease in pressure as the liquid accelerates into a larger volume induces cavitation. This method can be controlled with hydraulic devices that control inlet orifice size, allowing for dynamic adjustment during the process, or modification for different substances. The surface of this type of mixing valve, against which surface the cavitation bubbles are driven causing their implosion, undergoes tremendous mechanical and thermal localized stress; they are therefore often constructed of super-hard or tough materials such as stainless steel, Stellite, or even polycrystalline diamond (PCD).\n\nCavitating water purification devices have also been designed, in which the extreme conditions of cavitation can break down pollutants and organic molecules. Spectral analysis of light emitted in sonochemical reactions reveal chemical and plasma-based mechanisms of energy transfer. The light emitted from cavitation bubbles is termed sonoluminescence.\n\nUse of this technology has been tried successfully in alkali refining of vegetable oils.\n\nHydrophobic chemicals are attracted underwater by cavitation as the pressure difference between the bubbles and the liquid water forces them to join together. This effect may assist in protein folding.\n\nCavitation plays an important role for the destruction of kidney stones in shock wave lithotripsy. Currently, tests are being conducted as to whether cavitation can be used to transfer large molecules into biological cells (sonoporation). Nitrogen cavitation is a method used in research to lyse cell membranes while leaving organelles intact.\n\nCavitation plays a key role in non-thermal, non-invasive fractionation of tissue for treatment of a variety of diseases and can be used to open the blood-brain barrier to increase uptake of neurological drugs in the brain.\n\nCavitation also plays a role in HIFU, a thermal non-invasive treatment methodology for cancer. \n\nUltrasound sometimes is used to increase bone formation, for instance in post-surgical applications.\nUltrasound treatments or exposure can create cavitation that potentially may \"result in a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness, and fatigue.\".\n\nIt has been suggested that the sound of \"cracking\" knuckles derives from the collapse of cavitation in the synovial fluid within the joint. Movements that cause cracking expand the joint space, thus reducing pressure to the point of cavitation. It remains controversial whether this is associated with clinically significant joint injury such as osteoarthritis. Some physicians say that osteoarthritis is caused by cracking knuckles regularly, as this causes wear and tear and may cause the bone to weaken. The implication being that, it is not the \"bubbles popping,\" but rather, the bones rubbing together, that causes osteoarthritis.\n\nIn industrial cleaning applications, cavitation has sufficient power to overcome the particle-to-substrate adhesion forces, loosening contaminants. The threshold pressure required to initiate cavitation is a strong function of the pulse width and the power input. This method works by generating controlled acoustic cavitation in the cleaning fluid, picking up and carrying contaminant particles away so that they do not reattach to the material being cleaned.\n\nCavitation has been applied to egg pasteurization. A hole-filled rotor produces cavitation bubbles, heating the liquid from within. Equipment surfaces stay cooler than the passing liquid, so eggs don't harden as they did on the hot surfaces of older equipment. The intensity of cavitation can be adjusted, making it possible to tune the process for minimum protein damage.\n\nCavitation is, in many cases, an undesirable occurrence. In devices such as propellers and pumps, cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. Cavitation has also become a concern in the renewable energy sector as it may occur on the blade surface of tidal stream turbines.\n\nWhen the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.\n\nAlthough the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller's or pump's lifetime.\n\nAfter a surface is initially affected by cavitation, it tends to erode at an accelerating pace. The cavitation pits increase the turbulence of the fluid flow and create crevices that act as nucleation sites for additional cavitation bubbles. The pits also increase the components' surface area and leave behind residual stresses. This makes the surface more prone to stress corrosion.\n\nMajor places where cavitation occurs are in pumps, on propellers, or at restrictions in a flowing liquid.\n\nAs an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure around it can become. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n\nCavitation in pumps may occur in two different forms:\n\nSuction cavitation occurs when the pump suction is under a low-pressure/high-vacuum condition where the liquid turns into a vapor at the eye of the pump impeller. This vapor is carried over to the discharge side of the pump, where it no longer sees vacuum and is compressed back into a liquid by the discharge pressure. This imploding action occurs violently and attacks the face of the impeller. An impeller that has been operating under a suction cavitation condition can have large chunks of material removed from its face or very small bits of material removed, causing the impeller to look spongelike. Both cases will cause premature failure of the pump, often due to bearing failure. Suction cavitation is often identified by a sound like gravel or marbles in the pump casing.\n\nCommon causes of suction cavitation can include clogged filters, pipe blockage on the suction side, poor piping design, pump running too far right on the pump curve, or conditions not meeting NPSH (net positive suction head) requirements.\n\nIn automotive applications, a clogged filter in a hydraulic system (power steering, power brakes) can cause suction cavitation making a noise that rises and falls in synch with engine RPM. It is fairly often a high pitched whine, like set of nylon gears not quite meshing correctly.\n\nDischarge cavitation occurs when the pump discharge pressure is extremely high, normally occurring in a pump that is running at less than 10% of its best efficiency point. The high discharge pressure causes the majority of the fluid to circulate inside the pump instead of being allowed to flow out the discharge. As the liquid flows around the impeller, it must pass through the small clearance between the impeller and the pump housing at extremely high flow velocity. This flow velocity causes a vacuum to develop at the housing wall (similar to what occurs in a venturi), which turns the liquid into a vapor. A pump that has been operating under these conditions shows premature wear of the impeller vane tips and the pump housing. In addition, due to the high pressure conditions, premature failure of the pump's mechanical seal and bearings can be expected. Under extreme conditions, this can break the impeller shaft.\n\nDischarge cavitation in joint fluid is thought to cause the popping sound produced by bone joint cracking, for example by deliberately cracking one's knuckles.\n\nSince all pumps require well-developed inlet flow to meet their potential, a pump may not perform or be as reliable as expected due to a faulty suction piping layout such as a close-coupled elbow on the inlet flange. When poorly developed flow enters the pump impeller, it strikes the vanes and is unable to follow the impeller passage. The liquid then separates from the vanes causing mechanical problems due to cavitation, vibration and performance problems due to turbulence and poor filling of the impeller. This results in premature seal, bearing and impeller failure, high maintenance costs, high power consumption, and less-than-specified head and/or flow.\n\nTo have a well-developed flow pattern, pump manufacturer's manuals recommend about (10 diameters?) of straight pipe run upstream of the pump inlet flange. Unfortunately, piping designers and plant personnel must contend with space and equipment layout constraints and usually cannot comply with this recommendation. Instead, it is common to use an elbow close-coupled to the pump suction which creates a poorly developed flow pattern at the pump suction.\n\nWith a double-suction pump tied to a close-coupled elbow, flow distribution to the impeller is poor and causes reliability and performance shortfalls. The elbow divides the flow unevenly with more channeled to the outside of the elbow. Consequently, one side of the double-suction impeller receives more flow at a higher flow velocity and pressure while the starved side receives a highly turbulent and potentially damaging flow. This degrades overall pump performance (delivered head, flow and power consumption) and causes axial imbalance which shortens seal, bearing and impeller life.\nTo overcome cavitation:\nIncrease suction pressure if possible.\nDecrease liquid temperature if possible.\nThrottle back on the discharge valve to decrease flow-rate.\nVent gases off the pump casing.\n\nCavitation can occur in control valves. If the actual pressure drop across the valve as defined by the upstream and downstream pressures in the system is greater than the sizing calculations allow, pressure drop flashing or cavitation may occur. The change from a liquid state to a vapor state results from the increase in flow velocity at or just downstream of the greatest flow restriction which is normally the valve port. To maintain a steady flow of liquid through a valve the flow velocity must be greatest at the vena contracta or the point where the cross sectional area is the smallest. This increase in flow velocity is accompanied by a substantial decrease in the fluid pressure which is partially recovered downstream as the area increases and flow velocity decreases. This pressure recovery is never completely to the level of the upstream pressure. If the pressure at the vena contracta drops below the vapor pressure of the fluid bubbles will form in the flow stream. If the pressure recovers after the valve to a pressure that is once again above the vapor pressure, then the vapor bubbles will collapse and cavitation will occur.\n\nWhen water flows over a dam spillway, the irregularities on the spillway surface will cause small areas of flow separation in a high-speed flow, and, in these regions, the pressure will be lowered. If the flow velocities are high enough the pressure may fall to below the local vapor pressure of the water and vapor bubbles will form. When these are carried downstream into a high pressure region the bubbles collapse giving rise to high pressures and possible cavitation damage.\n\nExperimental investigations show that the damage on concrete chute and tunnel spillways can start at clear water flow velocities of between 12 and 15 m/s, and, up to flow velocities of 20 m/s, it may be possible to protect the surface by streamlining the boundaries, improving the surface finishes or using resistant materials.\n\nWhen some air is present in the water the resulting mixture is compressible and this damps the high pressure caused\nby the bubble collapses. If the flow velocities near the spillway invert are sufficiently high, aerators (or aeration devices) must be introduced to prevent cavitation. Although these have been installed for some years, the mechanisms of air entrainment at the aerators and the slow movement of the air away from the spillway surface are still challenging.\n\nThe spillway aeration device design is based upon a small deflection of the spillway bed (or sidewall) such as a ramp and offset to deflect the high flow velocity flow away from the spillway surface. In the cavity formed below the nappe, a local subpressure beneath the nappe is produced by which air is sucked into the flow. The complete design includes the deflection device (ramp, offset) and the air supply system.\n\nSome larger diesel engines suffer from cavitation due to high compression and undersized cylinder walls. Vibrations of the cylinder wall induce alternating low and high pressure in the coolant against the cylinder wall. The result is pitting of the cylinder wall, which will eventually let cooling fluid leak into the cylinder and combustion gases to leak into the coolant.\n\nIt is possible to prevent this from happening with the use of chemical additives in the cooling fluid that form a protective layer on the cylinder wall. This layer will be exposed to the same cavitation, but rebuilds itself. Additionally a regulated overpressure in the cooling system (regulated and maintained by the coolant filler cap spring pressure) prevents the forming of cavitation.\n\nFrom about the 1980s, new designs of smaller gasoline engines also displayed cavitation phenomena. One answer to the need for smaller and lighter engines was a smaller coolant volume and a correspondingly higher coolant flow velocity. This gave rise to rapid changes in flow velocity and therefore rapid changes of static pressure in areas of high heat transfer. Where resulting vapor bubbles collapsed against a surface, they had the effect of first disrupting protective oxide layers (of cast aluminium materials) and then repeatedly damaging the newly formed surface, preventing the action of some types of corrosion inhibitor (such as silicate based inhibitors). A final problem was the effect that increased material temperature had on the relative electrochemical reactivity of the base metal and its alloying constituents. The result was deep pits that could form and penetrate the engine head in a matter of hours when the engine was running at high load and high speed. These effects could largely be avoided by the use of organic corrosion inhibitors or (preferably) by designing the engine head in such a way as to avoid certain cavitation inducing conditions.\n\nSome hypotheses relating to diamond formation posit a possible role for cavitation—namely cavitiation in the kimberlite pipes providing the extreme pressure needed to change pure carbon into the rare allotrope that is diamond.\n\nThe loudest three sounds ever recorded, during the 1883 eruption of Krakatoa, are now understood as the bursts of three huge cavitation bubbles, each larger than the last, formed in the volcano's throat. Rising magma, filled with dissolved gasses and under immense pressure, encountered a different magma that compressed easily, allowing bubbles to grow and combine. \n\nThere exist macroscopic white lamellae inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n\nCavitation occurs in the xylem of vascular plants when the tension of water within the xylem exceeds atmospheric pressure. The sap vaporizes locally so that either the vessel elements or tracheids are filled with water vapor. Plants are able to repair cavitated xylem in a number of ways. For plants less than 50 cm tall, root pressure can be sufficient to redissolve the vapor. Larger plants direct solutes into the xylem via \"ray cells\", or in tracheids, via osmosis through bordered pits. Solutes attract water, the pressure rises and vapor can redissolve. In some trees, the sound of the cavitation is audible, particularly in summer, when the rate of evapotranspiration is highest. Some deciduous trees have to shed leaves in the autumn partly because cavitation increases as temperatures decrease.\n\nJust as cavitation bubbles form on a fast-spinning boat propeller, they may also form on the tails and fins of aquatic animals. This primarily occurs near the surface of the ocean, where the ambient water pressure is low.\n\nCavitation may limit the maximum swimming speed of powerful swimming animals like dolphins and tuna. Dolphins may have to restrict their speed because collapsing cavitation bubbles on their tail are painful. Tuna have bony fins without nerve endings and do not feel pain from cavitation. They are slowed down when cavitation bubbles create a vapor film around their fins. Lesions have been found on tuna that are consistent with cavitation damage.\n\nSome sea animals have found ways to use cavitation to their advantage when hunting prey. The pistol shrimp snaps a specialized claw to create cavitation, which can kill small fish. The mantis shrimp (of the \"smasher\" variety) uses cavitation as well in order to stun, smash open, or kill the shellfish that it feasts upon.\n\nThresher sharks use 'tail slaps' to debilitate their small fish prey and cavitation bubbles have been seen rising from the apex of the tail arc.\n\nIn the last half-decade, coastal erosion in the form of inertial cavitation has been generally accepted. Bubbles in an incoming wave are forced into cracks in the cliff being eroded. Varying pressure decompresses some vapor pockets which subsequently implode. The resulting pressure peaks can blast apart fractions of the rock.\n\n\n\n"}
{"id": "36980", "url": "https://en.wikipedia.org/wiki?curid=36980", "title": "Clay", "text": "Clay\n\nClay is a finely-grained natural rock or soil material that combines one or more clay minerals with possible traces of quartz (SiO), metal oxides (AlO , MgO etc.) and organic matter. Geologic clay deposits are mostly composed of phyllosilicate minerals containing variable amounts of water trapped in the mineral structure. Clays are plastic due to particle size and geometry as well as water content, and become hard, brittle and non–plastic upon drying or firing. Depending on the soil's content in which it is found, clay can appear in various colours from white to dull grey or brown to deep orange-red.\n\nAlthough many naturally occurring deposits include both silts and clay, clays are distinguished from other fine-grained soils by differences in size and mineralogy. Silts, which are fine-grained soils that do not include clay minerals, tend to have larger particle sizes than clays. There is, however, some overlap in particle size and other physical properties. The distinction between silt and clay varies by discipline. Geologists and soil scientists usually consider the separation to occur at a particle size of 2 µm (clays being finer than silts), sedimentologists often use 4–5 μm, and colloid chemists use 1 μm. Geotechnical engineers distinguish between silts and clays based on the plasticity properties of the soil, as measured by the soils' Atterberg limits. ISO 14688 grades clay particles as being smaller than 2 μm and silt particles as being larger.\n\nMixtures of sand, silt and less than 40% clay are called loam. Loam makes good soil and is used as a building material.\n\nClay minerals typically form over long periods of time as a result of the gradual chemical weathering of rocks, usually silicate-bearing, by low concentrations of carbonic acid and other diluted solvents. These solvents, usually acidic, migrate through the weathering rock after leaching through upper weathered layers. In addition to the weathering process, some clay minerals are formed through hydrothermal activity. There are two types of clay deposits: primary and secondary. Primary clays form as residual deposits in soil and remain at the site of formation. Secondary clays are clays that have been transported from their original location by water erosion and deposited in a new sedimentary deposit. Clay deposits are typically associated with very low energy depositional environments such as large lakes and marine basins.\n\nDepending on the academic source, there are three or four main groups of clays: kaolinite, montmorillonite-smectite, illite, and chlorite. Chlorites are not always considered to be a clay, sometimes being classified as a separate group within the phyllosilicates. There are approximately 30 different types of \"pure\" clays in these categories, but most \"natural\" clay deposits are mixtures of these different types, along with other weathered minerals.\n\nVarve (or \"varved clay\") is clay with visible annual layers, which are formed by seasonal deposition of those layers and are marked by differences in erosion and organic content. This type of deposit is common in former glacial lakes. When fine sediments are delivered into the calm waters of these glacial lake basins away from the shoreline, they settle to the lake bed. The resulting seasonal layering is preserved in an even distribution of clay sediment banding.\n\nQuick clay is a unique type of marine clay indigenous to the glaciated terrains of Norway, Canada, Northern Ireland, and Sweden. It is a highly sensitive clay, prone to liquefaction, which has been involved in several deadly landslides.\n\nPowder X-ray diffraction can be used to identify clays.\n\nThe physical and reactive chemical properties can be used to help elucidate the composition of clays.\n\nClays exhibit plasticity when mixed with water in certain proportions. However, when dry, clay becomes firm and when fired in a kiln, permanent physical and chemical changes occur. These changes convert the clay into a ceramic material. Because of these properties, clay is used for making pottery, both utilitarian and decorative, and construction products, such as bricks, wall and floor tiles. Different types of clay, when used with different minerals and firing conditions, are used to produce earthenware, stoneware, and porcelain. Prehistoric humans discovered the useful properties of clay. Some of the earliest pottery shards recovered are from central Honshu, Japan. They are associated with the Jōmon culture and deposits they were recovered from have been dated to around 14,000 BC.\n\nClay tablets were the first known writing medium. Scribes wrote by inscribing them with cuneiform script using a blunt reed called a stylus. Purpose-made clay balls were also used as sling ammunition.\n\nClays sintered in fire were the first form of ceramic. Bricks, cooking pots, art objects, dishware, smoking pipes, and even musical instruments such as the ocarina can all be shaped from clay before being fired. Clay is also used in many industrial processes, such as paper making, cement production, and chemical filtering. Until the late 20th century, bentonite clay was widely used as a mold binder in the manufacture of sand castings.\n\nClay, being relatively impermeable to water, is also used where natural seals are needed, such as in the cores of dams, or as a barrier in landfills against toxic seepage (lining the landfill, preferably in combination with geotextiles). (See puddling.)\n\nStudies in the early 21st century have investigated clay's absorption capacities in various applications, such as the removal of heavy metals from waste water and air purification.\n\nTraditional uses of clay as medicine goes back to prehistoric times. An example is Armenian bole, which is used to soothe an upset stomach. Some animals such as parrots and pigs ingest clay for similar reasons. Kaolin clay and attapulgite have been used as anti-diarrheal medicines.\n\nClay as the defining ingredient of loam is one of the oldest building materials on Earth, among other ancient, naturally-occurring geologic materials such as stone and organic materials like wood. Between one-half and two-thirds of the world's population, in both traditional societies as well as developed countries, still live or work in buildings made with clay, often baked into brick, as an essential part of its load-bearing structure. Also a primary ingredient in many natural building techniques, clay is used to create adobe, cob, cordwood, and rammed earth structures and building elements such as wattle and daub, clay plaster, clay render case, clay floors and clay paints and ceramic building material. Clay was used as a mortar in brick chimneys and stone walls where protected from water.\n\n\n\n"}
{"id": "15938221", "url": "https://en.wikipedia.org/wiki?curid=15938221", "title": "Darwin among the Machines", "text": "Darwin among the Machines\n\n\"Darwin among the Machines\" is the name of an article published in \"The Press\" newspaper on 13 June 1863 in Christchurch, New Zealand, which references the work of Charles Darwin in the title. Written by Samuel Butler but signed \"Cellarius\" (q.v.), the article raised the possibility that machines were a kind of \"mechanical life\" undergoing constant evolution, and that eventually machines might supplant humans as the dominant species:\nThe article ends by urging that, \"War to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown; let us at once go back to the primeval condition of the race.\"\n\nButler developed this and subsequent articles into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. The Erewhonian society Butler envisioned had long ago undergone a revolution that destroyed most mechanical inventions. The narrator of the story finds a book that details the reasons for this revolution, which he translates for the reader. In \nchapter xxiii: the book of the machines, a number of quotes from this imaginary book discuss the possibility of machine consciousness:\n\nLater, in chapter xxiv: the machines—continued, the imaginary book also discusses the notion that machines can \"reproduce\" like living organisms:\n\nThis notion of machine \"reproduction\" anticipates the later notion of self-replicating machines, although in chapter xxv: the machines—concluded, the imaginary book supposes that while there is a danger that humans will become subservient to machines, the machines will still need humans to assist in their reproduction and maintenance:\n\nThe author of the imaginary book goes on to say that while life under machine rule might be materially comfortable for humans, the thought of the human race being superseded in the future is just as horrifying to him as the thought that his distant ancestors were anything other than fully human (apparently Butler imagines the author to be an Anti-evolutionist), so he urges that all machines which have been in use for less than 300 years be destroyed to prevent this future from coming to pass:\n\nErewhonian society came to the conclusion \"...that the machines were ultimately destined to supplant the race of man, and to become instinct with a vitality as different from, and superior to, that of animals, as animal to vegetable life. So... they made a clean sweep of all machinery that had not been in use for more than two hundred and seventy-one years...\" (from \nchapter ix: to the metropolis.)\n\nDespite the initial popularity of \"Erewhon\", Butler commented in the preface to the second edition that reviewers had \"in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin’s theory to an absurdity.\" He protested that \"few things would be more distasteful to me than any attempt to laugh at Mr. Darwin\", but also added \"I am surprised, however, that the book at which such an example of the specious misuse of analogy would seem most naturally levelled should have occurred to no reviewer; neither shall I mention the name of the book here, though I should fancy that the hint given will suffice\", which may suggest that the chapter on Machines was in fact a satire intended to illustrate the \"specious misuse of analogy\", even if the target was not Darwin; Butler, fearing that he had offended Darwin, wrote him a letter explaining that the actual target was Joseph Butler's 1736 \"The Analogy of Religion, Natural and Revealed, to the Constitution and Course of Nature\". The Victorian scholar Herbert Sussman has suggested that although Butler's exploration of machine evolution was intended to be whimsical, he may also have been genuinely interested in the notion that living organisms are a type of mechanism and was exploring this notion with his writings on machines, while the philosopher Louis Flaccus called it \"a mixture of fun, satire, and thoughtful speculation.\"\n\nGeorge Dyson applies Butler's original premise to the artificial life and intelligence of Alan Turing in Darwin Among the Machines: The Evolution of Global Intelligence (1998) , to suggest that the internet is a living, sentient being.\n\nDyson's main claim is that the evolution of a conscious mind from today's technology is inevitable. It is not clear whether this will be a single mind or multiple minds, how smart that mind would be, and even if we will be able to communicate with it. He also clearly suggests that there are forms of intelligence on Earth that we are currently unable to understand.\n\nFrom the book: \"What mind, if any, will become apprehensive of the great coiling of ideas now under way is not a meaningless question, but it is still too early in the game to expect an answer that is meaningful to us.\"\n\nThe theme of humanity at war or otherwise in conflict with machines is found in a number of later creative works:\n\n\n\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "322355", "url": "https://en.wikipedia.org/wiki?curid=322355", "title": "Deep time", "text": "Deep time\n\nDeep time is the concept of geologic time. The modern philosophical concept was developed in the 18th century by Scottish geologist James Hutton (1726–1797). The age of the Earth has been determined to be, after a long and complex history of developments, around 4.55 billion years.\n\nHutton based his view of deep time on a form of geochemistry that had developed in Scotland and Scandinavia from the 1750s onward. As mathematician John Playfair, one of Hutton's friends and colleagues in the Scottish Enlightenment, remarked upon seeing the strata of the angular unconformity at Siccar Point with Hutton and James Hall in June 1788, \"the mind seemed to grow giddy by looking so far into the abyss of time\".\n\nEarly geologists such as Nicolas Steno (1638-1686) and Horace-Bénédict de Saussure (1740-1799) had developed ideas of geological strata forming from water through chemical processes, which Abraham Gottlob Werner (1749–1817) developed into a theory known as Neptunism, envisaging the slow crystallisation of minerals in the ancient oceans of the Earth to form rock. Hutton's innovative 1785 theory, based on Plutonism, visualised an endless cyclical process of rocks forming under the sea, being uplifted and tilted, then eroded to form new strata under the sea. In 1788 the sight of Hutton's Unconformity at Siccar Point convinced Playfair and Hall of this extremely slow cycle, and in that same year Hutton memorably wrote \"we find no vestige of a beginning, no prospect of an end\".\n\nOther scientists such as Georges Cuvier (1769-1832) put forward ideas of past ages, and geologists such as Adam Sedgwick (1785-1873) incorporated Werner's ideas into concepts of catastrophism; Sedgwick inspired his university student Charles Darwin to exclaim \"What a capital hand is Sedgewick [sic] for drawing large cheques upon the Bank of Time!\". In a competing theory, Charles Lyell in his \"Principles of Geology\" (1830–1833) developed Hutton's comprehension of endless deep time as a crucial scientific concept into uniformitarianism. As a young naturalist and geological theorist, Darwin studied the successive volumes of Lyell's book exhaustively during the \"Beagle\" survey voyage in the 1830s, before beginning to theorise about evolution.\n\nPhysicist Gregory Benford addresses the concept in \"Deep Time: How Humanity Communicates Across Millennia\" (1999), as does paleontologist and \"Nature\" editor Henry Gee in \"In Search of Deep Time: Beyond the Fossil Record to a New History of Life\" (2001) Stephen Jay Gould's \"Time's Arrow, Time's Cycle\" (1987) also deals in large part with the evolution of the concept.\n\nJohn McPhee discussed \"deep time\" at length with the layperson in mind in \"Basin and Range\" (1981), parts of which originally appeared in the \"New Yorker\" magazine. In \"Time's Arrow, Time's Cycle\", Gould cited one of the metaphors McPhee used in explaining the concept of deep time:\nConsider the Earth's history as the old measure of the English yard, the distance from the King's nose to the tip of his outstretched hand. One stroke of a nail file on his middle finger erases human history.\nConcepts similar to geologic time were recognized in the 11th century by the Persian geologist and polymath Avicenna (Ibn Sina, 973–1037), and by the Chinese naturalist and polymath Shen Kuo (1031–1095).\n\nThe Roman Catholic theologian Thomas Berry (1914–2009) explored spiritual implications of the concept of deep time. Berry proposes that a deep understanding of the history and functioning of the evolving universe is a necessary inspiration and guide for our own effective functioning as individuals and as a species. This view has greatly influenced the development of deep ecology and ecophilosophy. The experiential nature of the experience of deep time has also greatly influenced the work of Joanna Macy and John Seed.\n\nH.G. Wells and Julian Huxley regarded the difficulties of coping with the concept of deep time as exaggerated:\n\"The use of different scales is simply a matter of practice\", they said in \"The Science of Life\" (1929). \"We very soon get used to maps, though they are constructed on scales down to a hundred-millionth of natural size. . .  to grasp geological time all that is needed is to stick tight to some magnitude which shall be the unit on the new and magnified scale—a million years is probably the most convenient—to grasp its meaning once and for all by an effort of imagination, and then to think of all passage of geological time in terms of this unit.\"\n\n\n"}
{"id": "19873073", "url": "https://en.wikipedia.org/wiki?curid=19873073", "title": "Defensible space (fire control)", "text": "Defensible space (fire control)\n\nA defensible space, in the context of fire control, is a natural and/or landscaped area around a structure that has been maintained and designed to reduce fire danger. The practice is sometimes called firescaping. \"Defensible space\" is also used in the context of wildfires, especially in the wildland-urban interface (WUI). This defensible space reduces the risk that fire will spread from one area to another, or to a structure, and provides firefighters access and a safer area from which to defend a threatened area. Firefighters sometimes do not attempt to protect structures without adequate defensible space, as it is less safe and less likely to succeed.\n\n\nThe term defensible space in landscape (\"firescape\") use refers to the zone surrounding a structure. Often the location is in the wildland–urban interface. This area need not be devoid of vegetation by using naturally fire resistive plants that are spaced, pruned and trimmed, and irrigated, to minimize the fuel mass available to ignite and also to hamper the spread of a fire.\n\n\nAn important component is ongoing maintenance of the fire-resistant landscaping for reduced fuel loads and fire fighting access. Fire resistive plants that are not maintained can desiccate, die, or amass deadwood debris, and become fire assistive. Irrigation systems and pruning can help maintain a plant's fire resistance. Maintaining access roads and driveways clear of side and low-hanging vegetation can allow large fire equipment to reach properties and structures.\nSome agencies recommend clearing combustible vegetation at minimum horizontal 10 ft from roads and driveways a vertical of 13 ft 6 inches above them. Considering the plant material involved is important to not create unintended consequences to habitat integrity and unnecessary aesthetic issues. Street signs, and homes clearly identified with the numerical address, assist access also.\n\nThe unintended negative consequences of erosion and native habitat loss can result from some unskillful defensible space applications. The disturbance of the soil surface, such as garden soil cultivation in and firebreaks beyond native landscape zones areas, destroys the native plant cover and exposes open soil, accelerating invasive species of plants (\"invasive exotics\") spreading and replacing native habitats.\n\nIn suburban and wildland–urban interface areas, the vegetation clearance and brush removal ordinances of municipalities for defensible space can result in mistaken excessive clearcutting of native and non-invasive introduced shrubs and perennials that exposes the soil to more light and less competition for invasive plant species, and also to erosion and landslides. Negative aesthetic consequences to natural and landscaped areas can be minimized with integrated and balanced defensible space practices.\n\n\n"}
{"id": "48211145", "url": "https://en.wikipedia.org/wiki?curid=48211145", "title": "Dorrite", "text": "Dorrite\n\nDorrite is a silicate mineral that is isostructural to the aenigmatite group. Although it is most chemically similar to the mineral rhönite [CaMgTi(AlSi)O], the lack of titanium (Ti) and presence of Fe influenced dorrite's independence. Dorrite is named for Dr. John (Jack) A. Dorr, a late professor at the University of Michigan that researched in outcrops where dorrite was found in 1982. This mineral is sub-metallic resembling colors of brownish-black, dark brown, to reddish brown.\n\nDorrite was first reported in 1982 by A. Havette in a basalt-limestone contact on Réunion Island off of the coast of Africa. The second report of dorrite was made by Franklin Foit and his associates while examining a paralava from the Powder River Basin, Wyoming in 1987. Analyses determined that this newly found mineral was surprisingly similar to the mineral rhönite, lacking Ti but presenting dominant Fe in its octahedral sites. Other minerals that coexist with this phase are plagioclase, gehlenite-akermanite, magnetite-magnesioferrite-spinel solid solutions, esseneite, nepheline, wollastonite, Ba-rich feldspar, apatite, ulvöspinel, ferroan sahamalite, and secondary barite, and calcite.\n\nDorrite can be found in mineral reactions that relate dorrite + magnetite + clinopyroxene, rhönite + magnetite + olivine + clinopyroxene, and aenigmatite + pyroxene + olivine assemblages in nature. These assemblages favor low pressures and high temperatures. Dorrite is stable in strongly oxidizing, high-temperature, low-pressure environments. It occurs in paralava, pyrometamorphic melt rock, formed from the burning of coal beds.\n\nResearchers conclusively determined that dorrite is triclinic-pseudomonoclinic and twinned by a twofold rotation about the pseudomonoclinic b axis. The parameters for dorrite are a=10.505, b=10.897, c=9.019 Å, α=106.26°, β=95.16°, γ=124.75°.\n\nCalcium 8.97%<br>Magnesium 5.44%<br>Aluminum 6.04%<br>Iron 37.48%<br>Silicon 6.28%<br>Oxygen 35.79%\n\nCaO 12.55%<br>MgO 9.02%<br>AlO 11.41%<br>FeO 53.59%<br>SiO 13.44%\n"}
{"id": "146689", "url": "https://en.wikipedia.org/wiki?curid=146689", "title": "Earth radius", "text": "Earth radius\n\nEarth radius is the distance from a selected center of Earth to a point on its surface, which is often chosen to be sea level, or more commonly, the surface of an idealized ellipsoid representing the shape of Earth. Because Earth is not a perfect sphere, the determination of Earth's radius can have several values, depending on how it is measured; from its equatorial radius of about to its polar radius of about . \n\nWhen only one radius is stated, the International Astronomical Union (IAU) prefers that it be Earth's equatorial radius. \n\nThe International Union of Geodesy and Geophysics (IUGG) gives three global average radii, the arithmetic mean of the radii of the ellipsoid (R), the radius of a sphere with the same surface area as the ellipsoid or authalic radius (R), and the radius of a sphere with the same volume as the ellipsoid (R). All three IUGG average radii are about . A fourth global average radius not mentioned by the IUGG is the rectifying radius, the radius of a sphere with a circumference equal to the perimeter of the polar cross section of the ellipsoid, about . The radius of curvature at any point on the surface of the ellipsoid depends on its coordinates and its azimuth, north-south (meridional), east-west (prime vertical), or somewhere in between. \n\nEarth's rotation, internal density variations, and external tidal forces cause its shape to deviate systematically from a perfect sphere. Local topography increases the variance, resulting in a surface of profound complexity. Our descriptions of Earth's surface must be simpler than reality in order to be tractable. Hence, we create models to approximate characteristics of Earth's surface, generally relying on the simplest model that suits the need.\n\nEach of the models in common use involve some notion of the geometric radius. Strictly speaking, spheres are the only solids to have radii, but broader uses of the term \"radius\" are common in many fields, including those dealing with models of Earth. The following is a partial list of models of Earth's surface, ordered from exact to more approximate:\n\nIn the case of the geoid and ellipsoids, the fixed distance from any point on the model to the specified center is called \"a radius of the Earth\" or \"the radius of the Earth at that point\". It is also common to refer to any \"mean radius\" of a spherical model as \"the radius of the earth\". When considering the Earth's real surface, on the other hand, it is uncommon to refer to a \"radius\", since there is generally no practical need. Rather, elevation above or below sea level is useful.\n\nRegardless of the model, any radius falls between the polar minimum of about 6,357 km and the equatorial maximum of about 6,378 km (3,950 to 3,963 mi). Hence, the Earth deviates from a perfect sphere by only a third of a percent, which supports the spherical model in many contexts and justifies the term \"radius of the Earth\". While specific values differ, the concepts in this article generalize to any major planet.\n\nRotation of a planet causes it to approximate an \"oblate ellipsoid/spheroid\" with a bulge at the equator and flattening at the North and South Poles, so that the \"equatorial radius\" is larger than the \"polar radius\" by approximately . The \"oblateness constant\" is given by\nwhere is the angular frequency, is the gravitational constant, and is the mass of the planet. For the Earth , which is close to the measured inverse flattening . Additionally, the bulge at the equator shows slow variations. The bulge had been decreasing, but since 1998 the bulge has increased, possibly due to redistribution of ocean mass via currents.\n\nThe variation in density and crustal thickness causes gravity to vary across the surface and in time, so that the mean sea level differs from the ellipsoid. This difference is the \"geoid height\", positive above or outside the ellipsoid, negative below or inside. The geoid height variation is under on Earth. The geoid height can change abruptly due to earthquakes (such as the Sumatra-Andaman earthquake) or reduction in ice masses (such as Greenland).\n\nNot all deformations originate within the Earth. The gravity of the Moon and Sun cause the Earth's surface at a given point to undulate by tenths of meters over a nearly 12-hour period (see Earth tide).\n\nGiven local and transient influences on surface height, the values defined below are based on a \"general purpose\" model, refined as globally precisely as possible within of reference ellipsoid height, and to within of mean sea level (neglecting geoid height).\n\nAdditionally, the radius can be estimated from the curvature of the Earth at a point. Like a torus, the curvature at a point will be greatest (tightest) in one direction (north–south on Earth) and smallest (flattest) perpendicularly (east–west). The corresponding radius of curvature depends on the location and direction of measurement from that point. A consequence is that a distance to the true horizon at the equator is slightly shorter in the north/south direction than in the east-west direction.\n\nIn summary, local variations in terrain prevent defining a single \"precise\" radius. One can only adopt an idealized model. Since the estimate by Eratosthenes, many models have been created. Historically, these models were based on regional topography, giving the best reference ellipsoid for the area under survey. As satellite remote sensing and especially the Global Positioning System gained importance, true global models were developed which, while not as accurate for regional work, best approximate the Earth as a whole.\n\nThe following radii are fixed and do not include a variable location dependence. They are derived from the World Geodetic System 1984 (WGS-84) standard ellipsoid.\n\nThe value for the equatorial radius is defined to the nearest 0.1 m in WGS-84. The value for the polar radius in this section has been rounded to the nearest 0.1 m, which is expected to be adequate for most uses. Refer to the WGS-84 ellipsoid if a more precise value for its polar radius is needed.\n\nThe radii in this section are for an idealized surface. Even the idealized radii have an uncertainty of ±2 m. The discrepancy between the ellipsoid radius and the radius to a physical location may be significant. When identifying the position of an observable location, the use of more precise values for WGS-84 radii may not yield a corresponding improvement in accuracy.\n\nThe symbol given for the named radius is used in the formulae found in this article.\n\nThe Earth's equatorial radius , or semi-major axis, is the distance from its center to the equator and equals . The equatorial radius is often used to compare Earth with other planets.\n\nThe Earth's polar radius , or semi-minor axis, is the distance from its center to the North and South Poles, and equals .\n\nThe distance from the Earth's center to a point on the spheroid surface at geodetic latitude is:\n\nwhere and are, respectively, the equatorial radius and the polar radius.\n\nThere are two principal radii of curvature: along the meridional and prime-vertical normal sections.\n\nIn particular, the Earth's \"radius of curvature in the (north–south) meridian\" at is:\nThis is the radius that Eratosthenes measured.\n\nIf one point had appeared due east of the other, one finds the approximate curvature in the east–west direction.\n\nThis \"radius of curvature in the prime vertical\" which is perpendicular (normal or orthogonal) to at geodetic latitude is:\nThis radius is also called the transverse radius of curvature. At the equator, .\n\nThe Earth's meridional radius of curvature at the equator equals the meridian's semi-latus rectum:\n\nThe Earth's polar radius of curvature is:\n\nThe Earth's radius of curvature along a course at an azimuth (measured clockwise from north) at is derived from Euler's curvature formula as follows:\n\nIt is possible to combine the principal radii of curvature above in a non-directional manner.\n\nThe Earth's Gaussian radius of curvature at latitude is:\n\nThe Earth's mean radius of curvature at latitude is:\n\n\nThe Earth can be modeled as a sphere in many ways. This section describes the common ways. The various radii derived here use the notation and dimensions noted above for the Earth as derived from the WGS-84 ellipsoid; namely,\n\nA sphere being a gross approximation of the spheroid, which itself is an approximation of the geoid, units are given here in kilometers rather than the millimeter resolution appropriate for geodesy.\n\nIn geophysics, the International Union of Geodesy and Geophysics (IUGG) defines the mean radius (denoted ) to be\nFor Earth, the mean radius is .\n\nIn astronomy, the International Astronomical Union denotes the \"nominal equatorial Earth radius\" as formula_9, which is defined to be . The \"nominal polar Earth radius\" is defined as formula_10 = . These values correspond to the zero tide radii. Equatorial radius is conventionally used as the nominal value unless the polar radius is explicitly required.\n\nEarth's authalic (\"equal area\") radius is the radius of a hypothetical perfect sphere that has the same surface area as the reference ellipsoid. The IUGG denotes the authalic radius as .\n\nA closed-form solution exists for a spheroid:\nwhere and is the surface area of the spheroid.\n\nFor the Earth, the authalic radius is .\n\nAnother spherical model is defined by the volumetric radius, which is the radius of a sphere of volume equal to the ellipsoid. The IUGG denotes the volumetric radius as .\nFor Earth, the volumetric radius equals .\n\nAnother mean radius is the \"rectifying radius\", giving a sphere with circumference equal to the perimeter of the ellipse described by any polar cross section of the ellipsoid. This requires an elliptic integral to find, given the polar and equatorial radii:\n\nFor integration limits of [0,], the integrals for rectifying radius and mean radius evaluate to the same result, which, for Earth, amounts to .\n\nThe meridional mean is well approximated by the semicubic mean of the two axes,\n\nwhich differs from the exact result by less than ; the mean of the two axes,\n\nabout , can also be used.\n\nMost global mean radii are based on the reference ellipsoid, which approximates the geoid. The geoid has no direct relationship with surface topography, however. An alternative calculation averages elevations everywhere, resulting in a mean radius larger than the IUGG mean radius, the authalic radius, or the volumetric radius. This average is with uncertainty of .\n\nThe best spherical approximation to the ellipsoid in the vicinity of a given point is the osculating sphere. Its radius equals the Gaussian radius of curvature as above, and its radial direction coincides with the ellipsoid normal direction. The center of the osculating sphere is offset from the center of the ellipsoid, but is at center of curvature for the ellipsoid surface point involved. This concept aids the interpretation of terrestrial and planetary radio occultation refraction measurements and in some navigation and surveillance applications.\n\nThis table summarizes the accepted values of the Earth's radius.\n\nThe first published reference to the Earth's size appeared around 350 BC, when Aristotle reported in his book \"On the Heavens\" that mathematicians had guessed the circumference of the Earth to be 400,000 stadia. Scholars have interpreted Aristotle's figure to be anywhere from highly accurate to almost double the true value. The first known scientific measurement and calculation of the circumference of the Earth was performed by Eratosthenes in about 240 BC. Estimates of the accuracy of Eratosthenes’s measurement range from 0.5% to 17%. For both Aristotle and Eratosthenes, uncertainty in the accuracy of their estimates is due to modern uncertainty over which stadion length they meant.\n\n"}
{"id": "212485", "url": "https://en.wikipedia.org/wiki?curid=212485", "title": "Earth religion", "text": "Earth religion\n\nEarth religion is a term used mostly in the context of neopaganism.\n\nEarth-centered religion or nature worship is a system of religion based on the veneration of natural phenomena. It covers any religion that worships the earth, nature, or fertility deity, such as the various forms of goddess worship or matriarchal religion. Some find a connection between earth-worship and the Gaia hypothesis. Earth religions are also formulated to allow one to utilize the knowledge of preserving the earth.\n\nAccording to Marija Gimbutas, pre-Indo-European societies lived in small-scale, family-based communities that practiced matrilineal succession and goddess-centered religion where creation comes from the woman. She is the Divine Mother who can give life and take it away. In Irish mythology she is Danu, in Slavic mythology she is Mat Zemlya, and in other cultures she is Pachamama, Ninsun, Terra Mater, Nüwa, Matres or Shakti.\n\nIn the late 1800s, James Weir wrote an article describing the beginnings and aspects of early religious feeling. According to Boyer, early man was forced to locate food and shelter in order to survive, while constantly being directed by his instincts and senses. Because man's existence depended on nature, men began to form their religion and beliefs on and around nature itself. It is evident that man's first religion would have had to develop from the material world, he argues, because man relied heavily on his senses and what he could see, touch, and feel. In this sense, the worship of nature formed, allowing man to further depend on nature for survival.\n\nNeopagans have tried to make claims that religion started in ways that correspond to earth religion. In one of their published works, \"The Urantia Book\", another reason for this worship of nature came from a fear of the world around primitive man. His mind lacked the complex function of processing and sifting through complex ideas. As a result, man worshiped the very entity that surrounded him every day. That entity was nature. Man experienced the different natural phenomena around him, such as storms, vast deserts, and immense mountains. Among the very first parts of nature to be worshiped were rocks and hills, plants and trees, animals, the elements, heavenly bodies, and even man himself. As primitive man worked his way through nature worship, he eventually moved on to incorporate spirits into his worship. Although these claims may have some merit, they are nonetheless presented from a biased position that cannot be authenticated by traditional and reliable sources. Therefore, their claims can not be relied upon.\n\nThe origins of religion can be looked at through the lens of the function and processing of the human mind. Pascal Boyer suggests that, for the longest period of time, the brain was thought of as a simple organ of the body. However, he claims that the more information collected about the brain indicates that the brain is indeed not a \"blank slate.\" Humans do not just learn any information from the environment and surroundings around them. They have acquired sophisticated cognitive equipment that prepares them to analyze information in their culture and determine which information is relevant and how to apply it. Boyer states that \"having a normal human brain does not imply that you have religion. All it implies is that people can acquire it, which is very different.\" He suggests that religions started for the reasons of providing answers to humans, giving comfort, providing social order to society, and satisfying the need of the illusion-prone nature of the human mind. Ultimately, religion came into existence because of our need to answer questions and hold together our societal order.\n\nAn additional idea on the origins of religion comes not from man's cognitive development, but from the ape. Barbara J. King argues that human beings have an emotional connection with those around them, and that that desire for a connection came from their evolution from apes. The closest relative to the human species is the African ape. At birth, the ape begins negotiating with its mother about what it wants and needs in order to survive. The world the ape is born into is saturated with close family and friends. Because of this, emotions and relationships play a huge role in the ape's life. Its reactions and responses to one another are rooted and grounded in a sense of belongingness, which is derived from its dependence on the ape's mother and family. Belongingness is defined as \"mattering to someone who matters to you ... getting positive feelings from our relationships.\" This sense and desire for belongingness, which started in apes, only grew as the hominid (a human ancestor) diverged from the lineage of the ape, which occurred roughly six to seven million years ago.\n\nAs severe changes in the environment, physical evolutions in the human body (especially in the development of the human brain), and changes in social actions occurred, humans went beyond trying to simply form bonds and relationships of empathy with others. As their culture and society became more complex, they began using practices and various symbols to make sense of the natural and spiritual world around them. Instead of simply trying to find belongingness and empathy from the relationships with others, humans created and evolved God and spirits in order to fulfil that need and exploration. King argued that \"an earthly need for belonging led to human religious imagination and thus to the otherworldly realm of relating to God, gods, and spirits.\"\n\nThe term \"earth religion\" encompasses any religion that worships the earth, nature or fertility gods or goddesses. There is an array of groups and beliefs that fall under earth religion, such as paganism, which is a polytheistic, nature based religion; animism, which is the worldview that all living entities (plants, animals, and humans) possess a spirit; Wicca, who hold the concept of an earth mother goddess as well as practice ritual magic; and druidism, which equates divinity with the natural world.\n\nAnother perspective of earth religion to consider is pantheism, which takes a varied approach to the importance and purpose of the earth, and man's relationship with the planet. Several of their core statements deal with the connectivity humans share with the planet, declaring that \"all matter, energy, and life are an interconnected unity of which we are an inseparable part\" and \"we are an integral part of Nature, which we should cherish, revere and preserve in all its magnificent beauty and diversity. We should strive to live in harmony with Nature locally and globally\".\n\nThe earth also plays a vital role to many Voltaic peoples, many of whom \"consider the Earth to be Heaven’s wife\", such as the Konkomba of northern Ghana, whose economic, social and religious life is heavily influenced by the earth. It is also important to consider various Native American religions, such as Peyote Religion, Longhouse Religion, and Earth Lodge Religion.\n\nApril 22 was established as International Mother Earth Day by the United Nations in 2009, but many cultures around the world have been celebrating the Earth for thousands of years. Winter solstice and Summer solstice are celebrated with holidays like Yule and Dongzhi in the winter and Tiregān and Kupala in the summer.\n\nAnimism is practiced among the Bantu peoples of Sub-Saharan Africa. The Dahomey mythology has deities like Nana Buluku, Gleti, Mawu, Asase Yaa, Naa Nyonmo and Xevioso.\n\nIn Baltic mythology, the sun is a female deity, Saule, a mother or a bride, and Mēness is the moon, father or husband, their children being the stars. In Slavic mythology Mokosh and Mat Zemlya together with Perun head up the pantheon. Celebrations and rituals are centered on nature and harvest seasons. Dragobete is a traditional Romanian spring holiday that celebrates \"the day when the birds are betrothed.\"\n\nIn Hindu philosophy, the yoni is the creative power of nature and the origin of life. In Shaktism, the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival which celebrates the Earth's menstruation.\n\nAlthough the idea of earth religion has been around for thousands of years, it did not fully show up in popular culture until the early 1990s. \"The X-Files\" was one of the first nationally broadcast television programs to air witchcraft and Wicca (types of earth religion) content. On average, Wiccans - those who practice Wicca - were more or less pleased with the way the show had portrayed their ideals and beliefs. However, they still found it to be a little \"sensationalistic\". That same year, the movie \"The Craft\" was released - also depicting the art of Wicca. Unfortunately, this cinematic feature was not as happily accepted as \"The X-Files\" had been.\n\nA few years later, programs showcasing the aforementioned religious practices - such as \"Charmed\" and \"Buffy the Vampire Slayer\" - became widely popular. Although \"Charmed\" focused mostly on witchcraft, the magic they practiced very closely resembled Wicca. Meanwhile, \"Buffy\" was one of the first shows to actually cast a Wiccan character. However, since the shows focus was primarily on vampires, the Wiccan was depicted as having supernatural powers, rather than being in-tuned with the Earth.\n\nOther movies and shows throughout the last few decades have also been placed under the genre of Earth Religion. Among them are two of director Hayao Miyazaki's most well known films - \"Princess Mononoke\" and \"My Neighbor Totoro\". Both movies present human interaction with land, animal, and other nature spirits. Speakers for Earth Religion have said that these interactions suggest overtones of Earth Religion themes.\n\nSome popular Disney movies have also been viewed as Earth Religion films. Among them are \"The Lion King\" and \"Brother Bear\". Those who practice Earth Religion view \"The Lion King\" as an Earth Religion film mainly for the \"interconnectedness\" and \"Circle of Life\" it shows between the animals, plants, and life in general. When that link is broken, viewers see chaos and despair spread throughout the once bountiful land. Congruently, \"Brother Bear\" portrays interactions and consequences when humans disobey or go against the animal and Earth spirits.\n\nOther earth religion movies include \"The 13th Warrior\", \"The Deceivers (film)\", \"Sorceress (1982 film)\", \"Anchoress (film)\", \"Eye of the Devil\", \"Agora (film)\", and \"The Wicker Man (1973 film)\". These movies all contain various aspects of earth religion and nature worship in general.\n\nMany religions have negative stereotypes of earth religion and neo-paganism in general. A common critique of the worship of nature and resources of \"Mother Earth\" is that the rights of nature and ecocide movements are inhibitors of human progress and development. This argument is fueled by the fact that those people socialized into 'western' world views believe the earth itself is not a living being. Wesley Smith believes this is “anti-humanism with the potential to do real harm to the human family.” According to Smith, earth worshipers are hindering large-scale development, and they are viewed as inhibitors of advancement.\n\nA lot of criticism of earth religion comes from the negative actions of a few people who have been chastised for their actions. One such negative representative of earth religion is Aleister Crowley. He is believed to be \"too preoccupied with awakening magical powers\" instead of putting the well-being of others in his coven. Crowley allegedly looked up to \"Old George\" Pickingill, who was another worshipper of nature who was viewed negatively. Critics regarded Pickingill as a Satanist and \"England’s most notorious Witch\".\n\nCrowley himself was \"allegedly expelled from the Craft because he was a pervert.\" He became aroused by torture and pain, and enjoyed being \"punished\" by women. This dramatically damaged Crowley’s public image, because of his lifestyle and actions. Many people regarded all followers of earth religion as perverted Satanists.\n\nFollowers of earth religion have suffered major opprobrium over the years for allegedly being Satanists. Some religious adherents can be prone to viewing religions other than their religion as being wrong sometimes because they perceive those religions as characteristic of their concept of Satan worship. To wit, Witchcraft, a common practice of Wiccans, is sometimes misinterpreted as Satan worship by members of these groups, as well as less-informed persons who may not be specifically religious but who may reside within the sphere-of-influence of pagan-critical religious adherents. From the Wiccan perspective, however, earth religion and Wicca lie outside of the phenomenological world that encompasses Satanism. An all-evil being does not exist within the religious perspective of western earth religions. Devotees worship and celebrate earth resources and earth-centric deities. Satanism and Wicca \"have entirely different beliefs about deity, different rules for ethical behavior, different expectations from their membership, different views of the universe, different seasonal days of celebration, etc.\"\n\nNeo-pagans, or earth religion followers, often claim to be unaffiliated with Satanism. Neo-pagans, Wiccans, and earth religion believers do not acknowledge the existence of a deity that conforms to the common Semitic sect religious concept of Satan. Satanism stems from Christianity, while earth religion stems from older religious concepts.\n\nSome earth religion adherents take issue with the religious harassment that is inherent in the social pressure that necessitates their having to distance themselves from the often non-uniform, Semitic sect religious concept of Satan worship. Having to define themselves as \"other\" from a religious concept that is not within their worldview implies a certain degree of outsider-facilitated, informal, but functional religious restriction that is based solely on the metaphysical and mythological religious beliefs of those outsiders. This is problematic because outsider initiated comparisons to Satanism with the intent of condemnation, even when easily refuted, can have the effect of social pressure on earth religion adherents to conform to outsider perception of acceptable customs, beliefs, and modes of religious behavior.\n\nTo illustrate, a problem could arise with the \"other\" than Satanism argument if an earth centered belief system adopted a holiday that a critic considered to be similar or identical to a holiday that Satanists celebrate. Satanists have historically been prone to adopting holidays that have origins in various pagan traditions, ostensibly because these traditional holidays are amongst the last known vestiges of traditional pre-Semitic religious practice in the west. Satanists are, perhaps irrationally, prone to interpreting non-Semitic holidays as anti-Christian and therefore as implicitly representative of their worldview. This is not surprising given the fact that this is, in fact, how many Christians interpret holidays such as Samhain. In spite of any flawed perceptions or rationale held by any other group, earth centered religion adherents do not recognize misinterpretation of their customs made by outside religious adherents or critics inclusive of Satan worshippers.\n\nOrganized Satan worship, as defined by and anchored in the Semitic worldview, is characterized by a relatively disorganized and often disparate series of movements and groups that mostly emerged in the mid-20th century. Thus, their adopted customs have varied, continue to vary, and therefore this moving target of beliefs and customs can not be justifiably nor continuously accounted for by earth centered religious adherents. Once a Satanist group adopts a holiday, social stigma may unjustifiably taint the holiday and anyone who observes it without discrimination as to whence and for what purpose it was originally celebrated. Given these facts, many earth centered religion devotees find comparisons to Satanism intrinsically oppressive in nature. This logic transfers to any and all religious customs to include prayer, magic, ceremony, and any unintentional similarity in deity characteristics (an example is the horned traditional entity Pan having similar physical characteristics to common horned depictions of Satan).\n\nThe issue is further complicated by the theory that the intra and extra-biblical mythology of Satan that is present throughout various Semitic sects may have originally evolved to figuratively demonize the heathen religions of other groups. Thus, the concept of Satan, or \"the adversary\", would have been representative of all non-Semitic religions and, by extension, the people who believed in them. Although, at times, the concept of the \"other\" as demonic has also been used to characterize competing Semitic sects. Amongst other purposes, such belief would have been extraordinarily useful during the psychological and physical process of cleansing Europe of traditional tribal beliefs in favor of Christianity. This possibility would account for the historical tendency of Christian authorities, for example, to deem most pagan customs carried out in the pagan religious context as demonic. By any modern standard, such current beliefs would violate western concepts of religious tolerance as well as be inimical to the preservation of what remains of the culture of long-persecuted religious groups.\n\nBecause of the vast diversity of religions that fall under the title of \"earth religion\" there is no consensus of beliefs. However, the ethical beliefs of most religions overlap. The most well-known ethical code is the Wiccan Rede. Many of those who practice an earth religion choose to be environmentally active. Some perform activities such as recycling or composting while others feel it to be more productive to try and support the earth spiritually. These six beliefs about ethics seem to be universal.\n\n\"An [if] it harm none, do what ye will.\" Commonly worded in modern English as \"if it doesn't harm anyone, do what you want.\" This maxim was first printed in 1964, after being spoken by the priestess Doreen Valiente in the mid-20th century, and governs most ethical belief of Wiccans and some Pagans. There is no consensus of beliefs but this rede provides a starting point for most people's interpretation of what is ethical. The rede clearly states to do no harm but what constitutes as harm and what level of self-interest is acceptable is negotiable. Many Wiccans reverse the phrase into \"Do what ye will an it harm none,\" meaning \"Do what you want if it doesn't harm anyone.\" The difference may not seem significant but it is. The first implies that it is good to do no harm but does not say that it is necessarily unethical to do so, the second implies that all forms of harm are unethical. The second phrase is nearly impossible to follow. This shift occurred when trying to better adapt the phrase into modern English as well as to stress the \"harmlessness\" of Wiccans. The true nature of the rede simply implies that there is personal responsibility for your actions. You may do as you wish but there is a karma reaction from every action. Even though this is the most well-known rede of practice, it does not mean that those that choose not to follow it are unethical. There are many other laws of practice that other groups follow.\n\nThe Threefold Law is the belief that for all actions there is always a cause and effect. For every action taken either the good or ill intention will be returned to the action taker threefold. This is why the Wiccan Rede is typically followed because of fear of the threefold return from that harmful action.\n\nThis term is what Emma Restall Orr calls reverence for the earth in her book \"Living with Honour: A Pagan Ethics\". She separates the term into three sections: courage, generosity and loyalty, or honesty, respect and responsibility. There is no evil force in Nature. Nothing exists beyond the natural, therefore it is up to the individual to choose to be ethical not because of divine judgment. All beings are connected by the earth and so all should be treated fairly. There is a responsibility toward the environment and a harmony should be found with nature.\n\nThe following was written by the Church of All Worlds in 1988 and was affirmed by the Pagan Ecumenical Conferences of Ancient Ways (California, May 27–30) and Pagan Spirit Gathering (Wisconsin, June 17). The Pagan Community Council of Ohio then presented it to the Northeast Council of W.I.C.C.A.\n\n\"We, the undersigned, as adherents of Pagan and Old and Neo-Pagan Earth Religions, including Wicca or Witchcraft, practice a variety of positive, life affirming faiths that are dedicated to healing, both of ourselves and of the Earth. As such, we do not advocate or condone any acts that victimize others, including those proscribed by law. As one of our most widely accepted precepts is the Wiccan Rede's injunction to \"harm none,\" we absolutely condemn the practices of child abuse, sexual abuse and any other form of abuse that does harm to the bodies, minds or spirits of the victims of such abuses. We recognize and revere the divinity of Nature in our Mother the Earth, and we conduct our rites of worship in a manner that is ethical, compassionate and constitutionally protected. We neither acknowledge or worship the Christian devil, \"Satan,\" who is not in our Pagan pantheons. We will not tolerate slander or libel against our Temples, clergy or Temple Assemblers and we are prepared to defend our civil rights with such legal action as we deem necessary and appropriate.\"\n"}
{"id": "52058583", "url": "https://en.wikipedia.org/wiki?curid=52058583", "title": "Energy system", "text": "Energy system\n\nAn energy system is a system primarily designed to supply energy-services to end-users. Taking a structural viewpoint, the IPCC Fifth Assessment Report defines an energy system as \"all components related to the production, conversion, delivery, and use of energy\". The field of energy economics includes energy markets and treats an energy system as the technical and economic systems that satisfy consumer demand for energy in the forms of heat, fuels, and electricity.\n\nThe first two definitions allow for demand-side measures, including daylighting, retrofitted building insulation, and passive solar building design, as well as socio-economic factors, such as aspects of energy demand management and even telecommuting, while the third does not. Neither does the third account for the informal economy in traditional biomass that is significant in many developing countries.\n\nThe analysis of energy systems thus spans the disciplines of engineering and economics. Merging ideas from both areas to form a coherent description, particularly where macroeconomic dynamics are involved, is challenging.\n\nThe concept of an energy system is evolving as new regulations, technologies, and practices enter into service – for example, emissions trading, the development of smart grids, and the greater use of energy demand management, respectively.\n\nFrom a structural perspective, an energy system is like any general system and is made up of a set of interacting component parts, located within an environment. These components derive from ideas found in engineering and economics. Taking a process view, an energy system \"consists of an integrated set of technical and economic activities operating within a complex societal framework\". The identification of the components and behaviors of an energy system depends on the circumstances, the purpose of the analysis, and the questions under investigation. The concept of an energy system is therefore an abstraction which usually precedes some form of computer-based investigation, such as the construction and use of a suitable energy model.\n\nViewed in engineering terms, an energy system lends itself to representation as a flow network: the vertices map to engineering components like power stations and pipelines and the edges map to the interfaces between these components. This approach allows collections of similar or adjacent components to be aggregated and treated as one to simplify the model. Once described thus, flow network algorithms, such as minimum cost flow, may be applied. The components themselves can be treated as simple dynamical systems in their own right.\n\nConversely, relatively pure economic modeling may adopt a sectorial approach with only limited engineering detail present. The sector and sub-sector categories published by the International Energy Agency are often used as a basis for this analysis. A 2009 study of the UK residential energy sector contrasts the use of the technology-rich Markal model with several UK sectoral housing stock models.\n\nInternational energy statistics are typically broken down by carrier, sector and sub-sector, and country. Energy carriers ( energy products) are further classified as primary energy and secondary (or intermediate) energy and sometimes final (or end-use) energy. Published energy datasets are normally adjusted so that they are internally consistent, meaning that all energy stocks and flows must balance. The IEA regularly publishes energy statistics and energy balances with varying levels of detail and cost and also offers mid-term projections based on this data. The notion of an energy carrier, as used in energy economics, is distinct and different from the definition of energy used in physics.\n\nEnergy systems can range in scope, from local, municipal, national, and regional, to global, depending on issues under investigation. Researchers may or may not include demand side measures within their definition of an energy system. The IPCC does so, for instance, but covers these measures in separate chapters on transport, buildings, industry, and agriculture.\n\nHousehold consumption and investment decisions may also be included within the ambit of an energy system. Such considerations are not common because consumer behavior is difficult to characterize, but the trend is to include human factors in models. Household decision-taking may be represented using techniques from bounded rationality and agent-based behavior. The American Association for the Advancement of Science (AAAS) specifically advocates that \"more attention should be paid to incorporating behavioral considerations other than price- and income-driven behavior into economic models [of the energy system]\".\n\nThe concept of an energy-service is central, particularly when defining the purpose of an energy system:\n\nEnergy-services can be defined as amenities that are either furnished through energy consumption or could have been thus supplied. More explicitly:\n\nA consideration of energy-services per capita and how such services contribute to human welfare and individual quality of life is paramount to the debate on sustainable energy. People living in poor regions with low levels of energy-services consumption would clearly benefit from greater consumption, but the same is not generally true for those with high levels of consumption.\n\nThe notion of energy-services has given rise to energy-service companies (ESCo) who contract to provide energy-services to a client for an extended period. The ESCo is then free to choose the best means to do so, including investments in the thermal performance and HVAC equipment of the buildings in question.\n\nISO13600, ISO13601, and ISO13602 form a set of international standards covering technical energy systems (TES). Although withdrawn prior to 2016, these documents provide useful definitions and a framework for formalizing such systems. The standards depict an energy system broken down into supply and demand sectors, linked by the flow of tradable energy commodities (or energywares). Each sector has a set of inputs and outputs, some intentional and some harmful byproducts. Sectors may be further divided into subsectors, each fulfilling a dedicated purpose. The demand sector is ultimately present to supply energyware-based services to consumers (see energy-services).\n\n"}
{"id": "9285", "url": "https://en.wikipedia.org/wiki?curid=9285", "title": "Ethical naturalism", "text": "Ethical naturalism\n\nEthical naturalism (also called moral naturalism or naturalistic cognitivistic definism) is the meta-ethical view which claims that:\n\n\nIt is important to distinguish the versions of ethical naturalism which have received the most sustained philosophical interest, for example, Cornell realism, from the position that \"the way things are is always the way they ought to be\", which few ethical naturalists hold. Ethical naturalism does, however, reject the fact-value distinction: it suggests that inquiry into the natural world can increase our moral knowledge in just the same way it increases our scientific knowledge. Indeed, proponents of ethical naturalism have argued that humanity needs to invest in the science of morality, a broad and loosely defined field that uses evidence from biology, primatology, anthropology, psychology, neuroscience, and other areas to classify and describe moral behavior.\n\nEthical naturalism encompasses any reduction of ethical properties, such as 'goodness', to non-ethical properties; there are many different examples of such reductions, and thus many different varieties of ethical naturalism. Hedonism, for example, is the view that goodness is ultimately just pleasure.\n\n\nEthical naturalism has been criticized most prominently by ethical non-naturalist G. E. Moore, who formulated the open-question argument. Garner and Rosen say that a common definition of \"natural property\" is one \"which can be discovered by sense observation or experience, experiment, or through any of the available means of science.\" They also say that a good definition of \"natural property\" is problematic but that \"it is only in criticism of naturalism, or in an attempt to distinguish between naturalistic and nonnaturalistic definist theories, that such a concept is needed.\" R. M. Hare also criticised ethical naturalism because of its fallacious definition of the terms 'good' or 'right' explaining how value-terms being part of our prescriptive moral language are not reducible to descriptive terms: \"Value-terms have a special function in language, that of commending; and so they plainly cannot be defined in terms of other words which themselves do not perform this function\".\n\nWhen it comes to the moral questions that we might ask, it can be difficult to argue that there is not necessarily some level of meta-ethical relativism – and failure to address this matter is criticized as ethnocentrism. \n\nAs a broad example of relativism, we would no doubt see very different moral systems in an alien race that can only survive by occasionally ingesting one another. As a narrow example, there would be further specific moral opinions for each individual of that species.\n\nSome forms of moral realism are compatible with some degree of meta-ethical relativism. This argument rests on the assumption that one can have a \"moral\" discussion on various scales; that is, what is \"good\" for: a certain part of your being (leaving open the possibility of conflicting motives), you as a single individual, your family, your society, your species, your type of species. For example, a moral universalist (and certainly an absolutist) might argue that, just as one can discuss what is 'good and evil' at an individual's level, so too can one make certain \"moral\" propositions with truth values relative at the level of the species. In other words, the moral relativist need not deem \"all\" moral propositions as necessarily subjective. The answer to \"is free speech normally good for human societies?\" is relative in a sense, but the moral realist would argue that an individual can be incorrect in this matter. This may be the philosophical equivalent of the more pragmatic arguments made by some scientists.\n\nMoral nihilists maintain that any talk of an objective morality is incoherent and better off using other terms. Proponents of moral science like Ronald A. Lindsay have counter-argued that their way of understanding \"morality\" as a practical enterprise is the way we ought to have understood it in the first place. He holds the position that the alternative seems to be the elaborate philosophical reduction of the word \"moral\" into a vacuous, useless term. Lindsay adds that it is important to reclaim the specific word \"Morality\" because of the connotations it holds with many individuals.\n\nAuthor Sam Harris has argued that we overestimate the relevance of many arguments against the science of morality, arguments he believes scientists happily and rightly disregard in other domains of science like physics. For example, scientists may find themselves attempting to argue against philosophical skeptics, when Harris says they should be practically asking – as they would in any other domain – \"why would we listen to a solipsist in the first place?\" This, Harris contends, is part of what it means to practice a science of morality.\n\nPhysicist Sean Carroll believes that conceiving of morality as a science could be a case of scientific imperialism and insists that what is \"good for conscious creatures\" is not an adequate working definition of \"moral\". In opposition, Vice President at the Center for Inquiry, John Shook, claims that this working definition is more than adequate for science at present, and that disagreement should not immobilize the scientific study of ethics.\n\nIn the collective \"The End of Christianity\", Richard Carrier's chapter \"Moral Facts Naturally Exist (and Science Could Find Them)\" sets out to propose a form of moral realism centered on human satisfaction..\nIn modern times, many thinkers discussing the fact–value distinction and the is–ought problem have settled on the idea that one cannot derive \"ought\" from \"is\". Conversely, Harris maintains that the fact-value distinction is a confusion, proposing that values are really a certain kind of fact. Specifically, Harris suggests that values amount to empirical statements about \"the flourishing of conscious creatures in a society\". He argues that there are objective answers to moral questions, even if some are difficult or impossible to possess in practice. In this way, he says, science can tell us what to value. Harris adds that we do not demand absolute certainty from predictions in physics so we should not demand that of a science studying morality (see \"The Moral Landscape\").\n\n\n"}
{"id": "500948", "url": "https://en.wikipedia.org/wiki?curid=500948", "title": "Field guide", "text": "Field guide\n\nA field guide is a book designed to help the reader identify wildlife (plants or animals) or other objects of natural occurrence (e.g. minerals). It is generally designed to be brought into the 'field' or local area where such objects exist to help distinguish between similar objects. Field guides are often designed to help users distinguish animals and plants that may be similar in appearance but are not necessarily closely related.\n\nIt will typically include a description of the objects covered, together with paintings or photographs and an index. More serious and scientific field identification books, including those intended for students, will probably include identification keys to assist with identification, but the publicly accessible field guide is more often a browsable picture guide organized by family, colour, shape, location or other descriptors.\n\nPopular interests in identifying things in nature probably were strongest in bird and plant guides. Perhaps the first popular field guide to plants in the United States was the 1893 \"How to Know the Wildflowers\" by \"Mrs. William Starr Dana\" (Frances Theodora Parsons). In 1890, Florence Merriam published \"Birds Through an Opera-Glass\", describing 70 common species. Focused on living birds observed in the field, the book is considered the first in the tradition of modern, illustrated bird guides. In 1902, now writing as Florence Merriam Bailey (having married the zoologist Vernon Bailey), she published \"Handbook of Birds of the Western United States\". By contrast, the \"Handbook\" is designed as a comprehensive reference for the lab rather a portable book for the field. It was arranged by taxonomic order and had clear descriptions of species size, distribution, feeding, and nesting habits.\n\nFrom this point into the 1930s, features of field guides were introduced by Chester A. Reed and others such as changing the size of the book to fit the pocket, including colour plates, and producing guides in uniform editions that covered subjects such as garden and woodland flowers, mushrooms, insects, and dogs.\n\nIn 1934, Roger Tory Peterson, using his fine skill as an artist, changed the way modern field guides approached identification. Using color plates with paintings of similar species together – and marked with arrows showing the differences – people could use his bird guide in the field to compare species quickly to make identification easier. This technique, the \"Peterson Identification System\", was used in most of Peterson's Field Guides from animal tracks to seashells and has been widely adopted by other publishers and authors as well.\n\nToday, each field guide has its own range, focus and organization. Specialist publishers such as Croom Helm, along with organisations like the Audubon Society, the RSPB, the Field Studies Council, National Geographic, HarperCollins, and many others all produce quality field guides.\n\nIt is somewhat difficult to generalise about how field guides are intended to be used, because this varies from one guide to another, partly depending on how expert the targeted reader is expected to be.\n\nFor general public use, the main function of a field guide is to help the reader identify a bird, plant, rock, butterfly or other natural object down to at least the popular naming level. To this end some field guides employ simple keys and other techniques: the reader is usually encouraged to scan illustrations looking for a match, and to compare similar-looking choices using information on their differences. Guides are often designed to first lead readers to the appropriate section of the book, where the choices are not so overwhelming in number.\n\nGuides for students often introduce the concept of identification keys. Plant field guides such as \"Newcomb's Wildflower Guide\" (which is limited in scope to the wildflowers of northeastern North America) frequently have an abbreviated key that helps limit the search. Insect guides tend to limit identification to Order or Family levels rather than individual species, due to their diversity.\n\nMany taxa show variability and it is often difficult to capture the constant features using a small number of photographs. Illustrations by artists or post processing of photographs help in emphasising specific features needed to for reliable identification. Peterson introduced the idea of lines to point to these key features. He also noted the advantages of illustrations over photographs:\n\nField guides aid in improving the state of knowledge of various taxa. By making the knowledge of experienced museum specialists available to amateurs, they increase the gathering of information by amateurs from a wider geographic area and increasing the communication of these findings to the specialists.\n\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "21873573", "url": "https://en.wikipedia.org/wiki?curid=21873573", "title": "Freddy Fox", "text": "Freddy Fox\n\nFreddy Fox () is a hardcover picture book written and illustrated by Ronald J. Meyer. It tells the story of a young fox and his family, following the daily activities of the fox kit, Freddy. Freddy Fox teaches the reader about red foxes, including that they are born shades of gray, how their den is built, what they eat, and family size. Then through a simple segue, Freddy's red fox mother teaches the young fox about other animals and traits attributed to them. By learning his lessons, Freddy will grow up to be a wise fox. All of the lessons, such as the bear eats a balanced diet or the raccoon washes his hands before eating, can easily be translated into lessons for young children.\n\nThe book was first published in 2004. It won the Silver Mom's Choice Award in the \"Animal Kingdom\" category in 2008.\n\nThe author is a wildlife photographer, and he used his wildlife photographs to illustrate the book. \n\n"}
{"id": "11603215", "url": "https://en.wikipedia.org/wiki?curid=11603215", "title": "Geological history of Earth", "text": "Geological history of Earth\n\nThe geological history of Earth follows the major events in Earth's past based on the geological time scale, a system of chronological measurement based on the study of the planet's rock layers (stratigraphy). Earth formed about 4.54 billion years ago by accretion from the solar nebula, a disk-shaped mass of dust and gas left over from the formation of the Sun, which also created the rest of the Solar System.\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a planetoid with the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nAs the surface continually reshaped itself over hundreds of millions of years, continents formed and broke apart. They migrated across the surface, occasionally combining to form a supercontinent. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, , then finally Pangaea, which broke apart .\n\nThe present pattern of ice ages began about , then intensified at the end of the Pliocene. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years. The last glacial period of the current ice age ended about 10,000 years ago.\n\nThe Precambrian includes approximately 90% of geologic time. It extends from 4.6 billion years ago to the beginning of the Cambrian Period (about 541 Ma). It includes three eons, the Hadean, Archean, and Proterozoic.\n\nMajor volcanic events altering the Earth's environment and causing extinctions may have occurred 10 times in the past 3 billion years.\n\nDuring Hadean time (4.6–4 Ga), the Solar System was forming, probably within a large cloud of gas and dust around the sun, called an accretion disc from which Earth formed .\nThe Hadean Eon is not formally recognized, but it essentially marks the era before we have adequate record of significant solid rocks. The oldest dated zircons date from about .\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a large planetoid with the Earth. Some of this object's mass merged with the Earth, significantly altering its internal composition, and a portion was ejected into space. Some of the material survived to form an orbiting moon. More recent potassium isotopic studies suggest that the Moon was formed by a smaller, high-energy, high-angular-momentum giant impact cleaving off a significant portion of the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nDuring the Hadean the Late Heavy Bombardment occurred (approximately ) during which a large number of impact craters are believed to have formed on the Moon, and by inference on Earth, Mercury, Venus and Mars as well.\n\nThe Earth of the early Archean () may have had a different tectonic style. During this time, the Earth's crust cooled enough that rocks and continental plates began to form. Some scientists think because the Earth was hotter, that plate tectonic activity was more vigorous than it is today, resulting in a much greater rate of recycling of crustal material. This may have prevented cratonisation and continent formation until the mantle cooled and convection slowed down. Others argue that the subcontinental lithospheric mantle is too buoyant to subduct and that the lack of Archean rocks is a function of erosion and subsequent tectonic events.\n\nIn contrast to the Proterozoic, Archean rocks are often heavily metamorphized deep-water sediments, such as graywackes, mudstones, volcanic sediments and banded iron formations. Greenstone belts are typical Archean formations, consisting of alternating high- and low-grade metamorphic rocks. The high-grade rocks were derived from volcanic island arcs, while the low-grade metamorphic rocks represent deep-sea sediments eroded from the neighboring island rocks and deposited in a forearc basin. In short, greenstone belts represent sutured protocontinents.\n\nThe Earth's magnetic field was established 3.5 billion years ago. The solar wind flux was about 100 times the value of the modern Sun, so the presence of the magnetic field helped prevent the planet's atmosphere from being stripped away, which is what probably happened to the atmosphere of Mars. However, the field strength was lower than at present and the magnetosphere was about half the modern radius.\n\nThe geologic record of the Proterozoic () is more complete than that for the preceding Archean. In contrast to the deep-water deposits of the Archean, the Proterozoic features many strata that were laid down in extensive shallow epicontinental seas; furthermore, many of these rocks are less metamorphosed than Archean-age ones, and plenty are unaltered. Study of these rocks show that the eon featured massive, rapid continental accretion (unique to the Proterozoic), supercontinent cycles, and wholly modern orogenic activity. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–540 Ma.\n\nThe first-known glaciations occurred during the Proterozoic, one began shortly after the beginning of the eon, while there were at least four during the Neoproterozoic, climaxing with the Snowball Earth of the Varangian glaciation.\n\nThe Phanerozoic Eon is the current eon in the geologic timescale. It covers roughly 541 million years. During this period continents drifted about, eventually collected into a single landmass known as Pangea and then split up into the current continental landmasses.\n\nThe Phanerozoic is divided into three eras – the Paleozoic, the Mesozoic and the Cenozoic.\n\nMost of biological evolution occurred during this time period.\n\nThe Paleozoic spanned from roughly (Ma) and is subdivided into six geologic periods; from oldest to youngest they are the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian. Geologically, the Paleozoic starts shortly after the breakup of a supercontinent called Pannotia and at the end of a global ice age. Throughout the early Paleozoic, the Earth's landmass was broken up into a substantial number of relatively small continents. Toward the end of the era the continents gathered together into a supercontinent called Pangaea, which included most of the Earth's land area.\n\nThe Cambrian is a major division of the geologic timescale that begins about 541.0 ± 1.0 Ma. Cambrian continents are thought to have resulted from the breakup of a Neoproterozoic supercontinent called Pannotia. The waters of the Cambrian period appear to have been widespread and shallow. Continental drift rates may have been anomalously high. Laurentia, Baltica and Siberia remained independent continents following the break-up of the supercontinent of Pannotia. Gondwana started to drift toward the South Pole. Panthalassa covered most of the southern hemisphere, and minor oceans included the Proto-Tethys Ocean, Iapetus Ocean and Khanty Ocean.\n\nThe Ordovician period started at a major extinction event called the Cambrian–Ordovician extinction event some time about 485.4 ± 1.9 Ma. During the Ordovician the southern continents were collected into a single continent called Gondwana. Gondwana started the period in the equatorial latitudes and, as the period progressed, drifted toward the South Pole. Early in the Ordovician the continents Laurentia, Siberia and Baltica were still independent continents (since the break-up of the supercontinent Pannotia earlier), but Baltica began to move toward Laurentia later in the period, causing the Iapetus Ocean to shrink between them. Also, Avalonia broke free from Gondwana and began to head north toward Laurentia. The Rheic Ocean was formed as a result of this. By the end of the period, Gondwana had neared or approached the pole and was largely glaciated.\n\nThe Ordovician came to a close in a series of extinction events that, taken together, comprise the second-largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. The only larger one was the Permian-Triassic extinction event. The extinctions occurred approximately and mark the boundary between the Ordovician and the following Silurian Period.\n\nThe most-commonly accepted theory is that these events were triggered by the onset of an ice age, in the Hirnantian faunal stage that ended the long, stable greenhouse conditions typical of the Ordovician. The ice age was probably not as long-lasting as once thought; study of oxygen isotopes in fossil brachiopods shows that it was probably no longer than 0.5 to 1.5 million years. The event was preceded by a fall in atmospheric carbon dioxide (from 7000ppm to 4400ppm) which selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it. Evidence of these ice caps have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.\n\nThe Silurian is a major division of the geologic timescale that started about 443.8 ± 1.5 Ma. During the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian ice caps were less extensive than those of the late Ordovician glaciation. The melting of ice caps and glaciers contributed to a rise in sea levels, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. Other cratons and continent fragments drifted together near the equator, starting the formation of a second supercontinent known as Euramerica. The vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include Proto-Tethys, Paleo-Tethys, Rheic Ocean, a seaway of Iapetus Ocean (now in between Avalonia and Laurentia), and newly formed Ural Ocean.\n\nThe Devonian spanned roughly from 419 to 359 Ma. The period was a time of great tectonic activity, as Laurasia and Gondwana drew closer together. The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions. Near the equator Pangaea began to consolidate from the plates containing North America and Europe, further raising the northern Appalachian Mountains and forming the Caledonian Mountains in Great Britain and Scandinavia. The southern continents remained tied together in the supercontinent of Gondwana. The remainder of modern Eurasia lay in the Northern Hemisphere. Sea levels were high worldwide, and much of the land lay submerged under shallow seas. The deep, enormous Panthalassa (the \"universal ocean\") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean and Ural Ocean (which was closed during the collision with Siberia and Baltica).\n\nThe Carboniferous extends from about 358.9 ± 0.4 to about 298.9 ± 0.15 Ma.\n\nA global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread epicontinental seas and carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwana was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush coal swamps flourished within 30 degrees of the northernmost glaciers. A mid-Carboniferous drop in sea-level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea-level drop and the associated unconformity in North America separate the Mississippian Period from the Pennsylvanian period.\n\nThe Carboniferous was a time of active mountain building, as the supercontinent Pangea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America-Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural mountains. There were two major oceans in the Carboniferous the Panthalassa and Paleo-Tethys. Other minor oceans were shrinking and eventually closed the Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica, and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean.\n\nThe Permian extends from about 298.9 ± 0.15 to 252.17 ± 0.06 Ma.\n\nDuring the Permian all the Earth's major land masses, except portions of East Asia, were collected into a single supercontinent known as Pangaea. Pangaea straddled the equator and extended toward the poles, with a corresponding effect on ocean currents in the single great ocean (\"Panthalassa\", the \"universal sea\"), and the Paleo-Tethys Ocean, a large ocean that was between Asia and Gondwana. The Cimmeria continent rifted away from Gondwana and drifted north to Laurasia, causing the Paleo-Tethys to shrink. A new ocean was growing on its southern end, the Tethys Ocean, an ocean that would dominate much of the Mesozoic Era. Large continental landmasses create climates with extreme variations of heat and cold (\"continental climate\") and monsoon conditions with highly seasonal rainfall patterns. Deserts seem to have been widespread on Pangaea.\n\nThe Mesozoic extended roughly from .\n\nAfter the vigorous convergent plate mountain-building of the late Paleozoic, Mesozoic tectonic deformation was comparatively mild. Nevertheless, the era featured the dramatic rifting of the supercontinent Pangaea. Pangaea gradually split into a northern continent, Laurasia, and a southern continent, Gondwana. This created the passive continental margin that characterizes most of the Atlantic coastline (such as along the U.S. East Coast) today.\n\nThe Triassic Period extends from about 252.17 ± 0.06 to 201.3 ± 0.2 Ma. During the Triassic, almost all the Earth's land mass was concentrated into a single supercontinent centered more or less on the equator, called Pangaea (\"all the land\"). This took the form of a giant \"Pac-Man\" with an east-facing \"mouth\" constituting the Tethys sea, a vast gulf that opened farther westward in the mid-Triassic, at the expense of the shrinking Paleo-Tethys Ocean, an ocean that existed during the Paleozoic.\n\nThe remainder was the world-ocean known as Panthalassa (\"all the sea\"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean. The supercontinent Pangaea was rifting during the Triassic—especially late in the period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangea—which separated New Jersey from Morocco—are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Supergroup.\nBecause of the limited shoreline of one super-continental mass, Triassic marine deposits are globally relatively rare; despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms living in lagoons and hypersaline environments, such as \"Estheria\" crustaceans and terrestrial vertebrates.\n\nThe Jurassic Period extends from about 201.3 ± 0.2 to 145.0 Ma.\nDuring the early Jurassic, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous Period, when Gondwana itself rifted apart.\nThe Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed. The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site and the renowned late Jurassic \"lagerstätten\" of Holzmaden and Solnhofen.\nIn contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation. The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.\n\nThe Cretaceous Period extends from circa to .\n\nDuring the Cretaceous, the late Paleozoic-early Mesozoic supercontinent of Pangaea completed its breakup into present day continents, although their positions were substantially different at the time. As the Atlantic Ocean widened, the convergent-margin orogenies that had begun during the Jurassic continued in the North American Cordillera, as the Nevadan orogeny was followed by the Sevier and Laramide orogenies. Though Gondwana was still intact in the beginning of the Cretaceous, Gondwana itself broke up as South America, Antarctica and Australia rifted away from Africa (though India and Madagascar remained attached to each other); thus, the South Atlantic and Indian Oceans were newly formed. Such active rifting lifted great undersea mountain chains along the welts, raising eustatic sea levels worldwide.\n\nTo the north of Africa the Tethys Sea continued to narrow. Broad shallow seas advanced across central North America (the Western Interior Seaway) and Europe, then receded late in the period, leaving thick marine deposits sandwiched between coal beds. At the peak of the Cretaceous transgression, one-third of Earth's present land area was submerged. The Cretaceous is justly famous for its chalk; indeed, more chalk formed in the Cretaceous than in any other period in the Phanerozoic. Mid-ocean ridge activity—or rather, the circulation of seawater through the enlarged ridges—enriched the oceans in calcium; this made the oceans more saturated, as well as increased the bioavailability of the element for calcareous nanoplankton. These widespread carbonates and other sedimentary deposits make the Cretaceous rock record especially fine. Famous formations from North America include the rich marine fossils of Kansas's Smoky Hill Chalk Member and the terrestrial fauna of the late Cretaceous Hell Creek Formation. Other important Cretaceous exposures occur in Europe and China. In the area that is now India, massive lava beds called the Deccan Traps were laid down in the very late Cretaceous and early Paleocene.\n\nThe Cenozoic Era covers the  million years since the Cretaceous–Paleogene extinction event up to and including the present day. By the end of the Mesozoic era, the continents had rifted into nearly their present form. Laurasia became North America and Eurasia, while Gondwana split into South America, Africa, Australia, Antarctica and the Indian subcontinent, which collided with the Asian plate. This impact gave rise to the Himalayas. The Tethys Sea, which had separated the northern continents from Africa and India, began to close up, forming the Mediterranean sea.\n\nThe Paleogene (alternatively Palaeogene) Period is a unit of geologic time that began and ended 23.03 Ma and comprises the first part of the Cenozoic Era. This period consists of the Paleocene, Eocene and Oligocene Epochs.\n\nThe Paleocene, lasted from to .\n\nIn many ways, the Paleocene continued processes that had begun during the late Cretaceous Period. During the Paleocene, the continents continued to drift toward their present positions. Supercontinent Laurasia had not yet separated into three continents. Europe and Greenland were still connected. North America and Asia were still intermittently joined by a land bridge, while Greenland and North America were beginning to separate. The Laramide orogeny of the late Cretaceous continued to uplift the Rocky Mountains in the American west, which ended in the succeeding epoch. South and North America remained separated by equatorial seas (they joined during the Neogene); the components of the former southern supercontinent Gondwana continued to split apart, with Africa, South America, Antarctica and Australia pulling away from each other. Africa was heading north toward Europe, slowly closing the Tethys Ocean, and India began its migration to Asia that would lead to a tectonic collision and the formation of the Himalayas.\n\nDuring the Eocene ( - ), the continents continued to drift toward their present positions. At the beginning of the period, Australia and Antarctica remained connected, and warm equatorial currents mixed with colder Antarctic waters, distributing the heat around the world and keeping global temperatures high. But when Australia split from the southern continent around 45 Ma, the warm equatorial currents were deflected away from Antarctica, and an isolated cold water channel developed between the two continents. The Antarctic region cooled down, and the ocean surrounding Antarctica began to freeze, sending cold water and ice floes north, reinforcing the cooling. The present pattern of ice ages began about .\n\nThe northern supercontinent of Laurasia began to break up, as Europe, Greenland and North America drifted apart. In western North America, mountain building started in the Eocene, and huge lakes formed in the high flat basins among uplifts. In Europe, the Tethys Sea finally vanished, while the uplift of the Alps isolated its final remnant, the Mediterranean, and created another shallow sea with island archipelagos to the north. Though the North Atlantic was opening, a land connection appears to have remained between North America and Europe since the faunas of the two regions are very similar. India continued its journey away from Africa and began its collision with Asia, creating the Himalayan orogeny.\n\nThe Oligocene Epoch extends from about to . During the Oligocene the continents continued to drift toward their present positions.\n\nAntarctica continued to become more isolated and finally developed a permanent ice cap. Mountain building in western North America continued, and the Alps started to rise in Europe as the African plate continued to push north into the Eurasian plate, isolating the remnants of Tethys Sea. A brief marine incursion marks the early Oligocene in Europe. There appears to have been a land bridge in the early Oligocene between North America and Europe since the faunas of the two regions are very similar. During the Oligocene, South America was finally detached from Antarctica and drifted north toward North America. It also allowed the Antarctic Circumpolar Current to flow, rapidly cooling the continent.\n\nThe Neogene Period is a unit of geologic time starting 23.03 Ma. and ends at 2.588 Ma. The Neogene Period follows the Paleogene Period. The Neogene consists of the Miocene and Pliocene and is followed by the Quaternary Period.\n\nThe Miocene extends from about 23.03 to 5.333 Ma.\n\nDuring the Miocene continents continued to drift toward their present positions. Of the modern geologic features, only the land bridge between South America and North America was absent, the subduction zone along the Pacific Ocean margin of South America caused the rise of the Andes and the southward extension of the Meso-American peninsula. India continued to collide with Asia. The Tethys Seaway continued to shrink and then disappeared as Africa collided with Eurasia in the Turkish-Arabian region between 19 and 12 Ma (ICS 2004). Subsequent uplift of mountains in the western Mediterranean region and a global fall in sea levels combined to cause a temporary drying up of the Mediterranean Sea resulting in the Messinian salinity crisis near the end of the Miocene.\n\nThe Pliocene extends from to . During the Pliocene continents continued to drift toward their present positions, moving from positions possibly as far as from their present locations to positions only 70 km from their current locations.\n\nSouth America became linked to North America through the Isthmus of Panama during the Pliocene, bringing a nearly complete end to South America's distinctive marsupial faunas. The formation of the Isthmus had major consequences on global temperatures, since warm equatorial ocean currents were cut off and an Atlantic cooling cycle began, with cold Arctic and Antarctic waters dropping temperatures in the now-isolated Atlantic Ocean. Africa's collision with Europe formed the Mediterranean Sea, cutting off the remnants of the Tethys Ocean. Sea level changes exposed the land-bridge between Alaska and Asia. Near the end of the Pliocene, about (the start of the Quaternary Period), the current ice age began. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years.\n\nThe Pleistocene extends from to 11,700 years before present. The modern continents were essentially at their present positions during the Pleistocene, the plates upon which they sit probably having moved no more than relative to each other since the beginning of the period.\n\nThe Holocene Epoch began approximately 11,700 calendar years before present and continues to the present. During the Holocene, continental motions have been less than a kilometer.\n\nThe last glacial period of the current ice age ended about 10,000 years ago. Ice melt caused world sea levels to rise about in the early part of the Holocene. In addition, many areas above about 40 degrees north latitude had been depressed by the weight of the Pleistocene glaciers and rose as much as over the late Pleistocene and Holocene, and are still rising today. The sea level rise and temporary land depression allowed temporary marine incursions into areas that are now far from the sea. Holocene marine fossils are known from Vermont, Quebec, Ontario and Michigan. Other than higher latitude temporary marine incursions associated with glacial depression, Holocene fossils are found primarily in lakebed, floodplain and cave deposits. Holocene marine deposits along low-latitude coastlines are rare because the rise in sea levels during the period exceeds any likely upthrusting of non-glacial origin. Post-glacial rebound in Scandinavia resulted in the emergence of coastal areas around the Baltic Sea, including much of Finland. The region continues to rise, still causing weak earthquakes across Northern Europe. The equivalent event in North America was the rebound of Hudson Bay, as it shrank from its larger, immediate post-glacial Tyrrell Sea phase, to near its present boundaries.\n\n\n"}
{"id": "23793839", "url": "https://en.wikipedia.org/wiki?curid=23793839", "title": "Global Earthquake Model", "text": "Global Earthquake Model\n\nThe Global Earthquake Model (GEM) is a public–private partnership initiated in 2006 by the Global Science Forum of the OECD to develop global, open-source risk assessment software and tools. With committed backing from academia, governments and industry, GEM contributes to achieving profound, lasting reductions in earthquake risk worldwide by following the priorities of the Hyogo Framework for Action. From 2009 to 2013 GEM is constructing its first working global earthquake model and will provide an authoritative standard for calculating and communicating earthquake risk worldwide.\n\nSince March 2009, GEM is a legal entity in the form of a non-profit foundation based in Pavia, Italy. The GEM Secretariat is hosted at the European Centre for Training and Research in Earthquake Engineering (EUCENTRE). The current secretary general is John Schneider.\n\nBetween 2000 and 2010 over half a million people died due to earthquakes and tsunamis, most of these in the developing world, where risks increase due to rapid population growth and urbanization. However, in many earthquake-prone regions no risk models exist, and even where models do exist, they are inaccessible. Better risk-awareness can reduce the toll that earthquakes take by leading to better construction, improved emergency response, and greater access to insurance.\n\nGEM will provide a basis for comparing earthquake risks across regions and across borders, and thereby take the necessary first step towards increased awareness and actions that reduce earthquake risk. GEM tools will be usable at the community, national and international level for uniform earthquake risk-evaluation and as a defensible basis for risk-mitigation plans. GEM results will be disseminated all over the world. GEM will build technical capacity and carry out awareness-raising activities.\n\nThe GEM scientific framework serves as the underlying basis for constructing the global earthquake model, and is organised in three principal integrated modules: seismic hazard, seismic risk and socio-economic impact.\n\n\nIt will take five years to build the first working global earthquake model – including corresponding tools, software and datasets. The work started in 2009 and will be finished at the end of 2013. Construction occurs in various stages that are partly overlapping in time. The pilot project GEM1 (January 2009 – March 2010) generates GEM’s first products and initial model building infrastructure, Global components will establish a common set of definitions, strategies, standards, quality criteria and formats for the compilation of databases that serve as an input to the global earthquake model. They are addressed by international consortia that respond to calls for proposals on hazard, risk and socio-economic impact. Global components will provide preliminary data on a global scale, but on a local scale, regional and national programmes will provide more detailed and reliable data. One Global component was the Global GMPEs project that proposed a set of ground motion prediction equations for use when calculating seismic hazard. Regional Programmes are projects with targeted funding taking place in various regions of the world; currently in the Middle East and Europe programs have already been completed. The data produced on a regional and national scale will be carefully quality-controlled and integrated into the global models. The actual development of the model will occur using a common, web-based platform for dynamic sharing of tools and resources, in order to create software and online tools as end-products. The global earthquake model will be tested and evaluated before its official release; the testing procedure will involve the establishment of scientific experiments that are reproducible, transparent, and set up within a controlled environment.\n\nGEM is however more than the creation and release of this first version of the model. GEM strives for continuous improvement of the model and will ensure that results are disseminated, technology is transferred through training and workshops and that awareness raising activities are deployed in order to contribute to risk reduction worldwide.\n\n\n"}
{"id": "180236", "url": "https://en.wikipedia.org/wiki?curid=180236", "title": "Greisen–Zatsepin–Kuzmin limit", "text": "Greisen–Zatsepin–Kuzmin limit\n\nThe Greisen–Zatsepin–Kuzmin limit (GZK limit) is a theoretical upper limit on the energy of cosmic ray protons traveling from other galaxies through the intergalactic medium to our galaxy. The limit is , or about 8 joules. The limit is set by slowing-interactions of the protons with the microwave background radiation over long distances (~160 million light-years). The limit is at the same order of magnitude as the upper limit for energy at which cosmic rays have experimentally been detected. For example, one extreme-energy cosmic ray has been detected which appeared to possess a record (50 joules) of energy (about the same as the kinetic energy of a 35 mph baseball).\n\nThe GZK limit is derived under the assumption that ultra-high energy cosmic rays are protons. Measurements by the largest cosmic-ray observatory, the Pierre Auger Observatory, suggest that most ultra-high energy cosmic rays are heavier elements. In this case, the argument behind the GZK limit does not apply in the originally simple form and there is no fundamental contradiction in observing cosmic rays with energies that violate the limit.\n\nIn the past, the apparent violation of the GZK limit has inspired cosmologists and theoretical physicists to suggest other ways that circumvent the limit. These theories propose that ultra-high energy cosmic rays are produced nearby our galaxy or that Lorentz covariance is violated in such a way that protons do not lose energy on their way to our galaxy.\n\nThe limit was independently computed in 1966 by Kenneth Greisen, Vadim Kuzmin, and Georgiy Zatsepin, based on interactions between cosmic rays and the photons of the cosmic microwave background radiation (CMB). They predicted that cosmic rays with energies over the threshold energy of would interact with cosmic microwave background photons formula_1, relatively blueshifted by the speed of the cosmic rays, to produce pions via the formula_2 resonance,\n\nor\n\nPions produced in this manner proceed to decay in the standard pion channels—ultimately to photons for neutral pions, and photons, positrons, and various neutrinos for positive pions. Neutrons decay also to similar products, so that ultimately the energy of any cosmic ray proton is drained off by production of high energy photons plus (in some cases) high energy electron/positron pairs and neutrino pairs.\n\nThe pion production process begins at a higher energy than ordinary electron-positron pair production (lepton production) from protons impacting the CMB, which starts at cosmic ray proton energies of only about . However, pion production events drain 20% of the energy of a cosmic ray proton as compared with only 0.1% of its energy for electron positron pair production. This factor of 200 is from two sources: the pion has only about ~130 times the mass of the leptons, but the extra energy appears as different kinetic energies of the pion or leptons, and results in relatively more kinetic energy transferred to a heavier product pion, in order to conserve momentum. The much larger total energy losses from pion production result in the pion production process becoming the limiting one to high energy cosmic ray travel, rather than the lower-energy light-lepton production process.\n\nThe pion production process continues until the cosmic ray energy falls below the pion production threshold. Due to the mean path associated with this interaction, extragalactic cosmic rays traveling over distances larger than () and with energies greater than this threshold should never be observed on Earth. This distance is also known as GZK horizon.\n\nA number of observations have been made by the largest cosmic ray experiments Akeno Giant Air Shower Array, High Resolution Fly's Eye Cosmic Ray Detector, the Pierre Auger Observatory and Telescope Array Project that appeared to show cosmic rays with energies above this limit (called extreme-energy cosmic rays, or EECRs). The observation of these particles was the so-called GZK paradox or cosmic ray paradox.\n\nThese observations appear to contradict the predictions of special relativity and particle physics as they are presently understood. However, there are a number of possible explanations for these observations that may resolve this inconsistency.\n\nAnother suggestion involves ultra-high energy weakly interacting particles (for instance, neutrinos) which might be created at great distances and later react locally to give rise to the particles observed. In the proposed Z-burst model, an ultra-high energy cosmic neutrino collides with a relic anti-neutrino in our galaxy and annihilates to hadrons. This process proceeds via a (virtual) Z-boson:\n\nformula_8\n\nThe cross section for this process becomes large if the center of mass energy of the neutrino antineutrino pair is equal to the Z-boson mass (such a peak in the cross section is called \"resonance\"). Assuming that the relic anti-neutrino is at rest, the energy of the incident cosmic neutrino has to be:\n\nformula_9\n\nwhere formula_10 is the mass of the Z-boson and formula_11 the mass of the neutrino.\n\nA number of exotic theories have been advanced to explain the AGASA observations, including doubly special relativity. However, it is now established that standard doubly special relativity does not predict any GZK suppression (or GZK cutoff), contrary to models of Lorentz symmetry violation involving an absolute rest frame. Other possible theories involve a relation with dark matter, decays of exotic super-heavy particles beyond those known in the Standard Model.\n\nA suppression of the cosmic ray flux which can be explained with the GZK limit has been confirmed by the latest generation of cosmic ray observatories. A former claim by the AGASA experiment that there is no suppression was overruled. It remains controversial, whether the suppression is due to the GZK effect. The GZK limit only applies if ultra-high energy cosmic rays are mostly protons.\n\nIn July 2007, during the 30th International Cosmic Ray Conference in Mérida, Yucatán, México, the High Resolution Fly's Eye Experiment (HiRes) and the Pierre Auger Observatory (Auger) presented their results on ultra-high-energy cosmic rays. HiRes observed a suppression in the UHECR spectrum at just the right energy, observing only 13 events with an energy above the threshold, while expecting 43 with no suppression. This was interpreted as the first observation of the GZK limit. Auger confirmed the flux suppression, but did not claim it to be the GZK limit: instead of the 30 events necessary to confirm the AGASA results, Auger saw only two, which are believed to be heavy nuclei events. The flux suppression was previously brought into question when the AGASA experiment found no suppression in their spectrum. According to Alan Watson, spokesperson for the Auger Collaboration, AGASA results have been shown to be incorrect, possibly due to the systematic shift in energy assignment.\n\nIn 2010 and the following years, both the Pierre Auger Observatory and HiRes confirmed again a flux suppression, in case of the Pierre Auger Observatory the effect is statistically significant at the level of 20 standard deviations.\n\nAfter the flux suppression was established, a heated debate ensued whether cosmic rays that violate the GZK limit are protons. The Pierre Auger Observatory, the world's largest observatory, found with high statistical significance that ultra-high energy cosmic rays are not purely protons, but a mixture of elements which is getting heavier with increasing energy.\nThe Telescope Array Project, a joint effort from members of the HiRes and AGASA collaborations, agrees with the former HiRes result that these cosmic rays look like protons. The claim is based on data with lower statistical significance, however. The area covered by Telescope Array is about one third of the area covered by the Pierre Auger Observatory, and the latter has been running for a longer time.\n\nThe controversy was partially resolved in 2017, when a joint working group formed by members of both experiments presented a report at the 35th International Cosmic Ray Conference. According to the report, the raw experimental results are not in contradiction with each other. The different interpretations are mainly based on the use of different theoretical models (Telescope Array uses an outdated model for its interpretation), and the fact that Telescope Array has not collected enough events yet to distinguish the pure proton hypothesis from the mixed-nuclei hypothesis.\n\nEUSO, which was scheduled to fly on the International Space Station (ISS) in 2009, was designed to use the atmospheric-fluorescence technique to monitor a huge area and boost the statistics of UHECRs considerably. EUSO is to make a deep survey of UHECR-induced extensive air showers (EASs) from space, extending the measured energy spectrum well beyond the GZK-cutoff. It is to search for the origin of UHECRs, determine the nature of the origin of UHECRs, make an all-sky survey of the arrival direction of UHECRs, and seek to open the astronomical window on the extreme-energy universe with neutrinos. The fate of the EUSO Observatory is still unclear since NASA is considering early retirement of the ISS.\n\nLaunched in June 2008, the Fermi Gamma-ray Space Telescope (formerly GLAST) will also provide data that will help resolve these inconsistencies.\n\nIn November 2007, researchers at the Pierre Auger Observatory announced that they had evidence that UHECRs appear to come from the active galactic nuclei (AGNs) of energetic galaxies powered by matter swirling onto a supermassive black hole. The cosmic rays were detected and traced back to the AGNs using the Véron-Cetty-Véron catalog. These results are reported in the journal \"Science\". Nevertheless, the strength of the correlation with AGNs from this particular catalog for the Auger data recorded after 2007 has been slowly diminishing.\n\n"}
{"id": "163901", "url": "https://en.wikipedia.org/wiki?curid=163901", "title": "Information society", "text": "Information society\n\nAn information society is a society where the creation, distribution, use, integration and manipulation of information is a significant economic, political, and cultural activity. Its main drivers are digital information and communication technologies, which have resulted in an information explosion and are profoundly changing all aspects of social organization, including the economy, education, health, warfare, government and democracy. The people who have the means to partake in this form of society are sometimes called digital citizens, defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen labels that have been identified to suggest that humans are entering a new phase of society.\n\nThe markers of this rapid change may be technological, economic, occupational, spatial, cultural, or some combination of all of these.\nInformation society is seen as the successor to industrial society. Closely related concepts are the post-industrial society (Daniel Bell), post-fordism, post-modern society, knowledge society, telematic society, Information Revolution, liquid modernity, and network society (Manuel Castells).\n\nThere is currently no universally accepted concept of what exactly can be termed information society and what shall rather not so be termed. Most theoreticians agree that a transformation can be seen that started somewhere between the 1970s and today and is changing the way societies work fundamentally. Information technology goes beyond the internet, and there are discussions about how big the influence of specific media or specific modes of production really is. Frank Webster notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information.\n\nKasiwulaya and Gomo (Makerere University) allude that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information\nSociety in the Tunis Commitment and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase.\n\nSome people, such as Antonio Negri, characterize the information society as one in which people do immaterial labour. By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function? For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It \"is\" now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development.\n\nMichael Buckland characterizes information in society in his book \"Information and Society.\" Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences.\n\nConsidering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the Japanese society) as an information society because we think of it as such.\nThe word information may be interpreted in many different ways. According to Buckland in \"Information and Society\", most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing.\n\nThe growth of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986, which is the informational equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of 60 CD-ROM per person in 2007 and represents a sustained annual growth rate of some 25%. The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, and 65 (optimally compressed) exabytes in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades.\n\nJames R. Beniger describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174)\n“Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175)\n\nOne of the first people to develop the concept of the information society was the economist Fritz Machlup. In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study \"The production and distribution of knowledge in the United States\" in 1962. This book was widely regarded and was eventually translated into Russian and Japanese. The Japanese have also studied the information society (or \"jōhōka shakai\", ).\n\nThe issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or information economy, post-industrial society, postmodern society, network society, the information revolution, informational capitalism, network capitalism, and the like, have been debated over the last several decades.\n\nFritz Machlup (1962) introduced the concept of the knowledge industry. He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries.\n\nPeter Drucker has argued that there is a transition from an economy based on material goods to one based on knowledge. Marc Porat distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy.\n\nPorat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The OECD has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy.\n\nFor Daniel Bell the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\".\n\nAlain Touraine already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning.\n\nJean-François Lyotard has argued that \"knowledge has become the force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society.\n\nSimilarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter. Radovan Richta (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55).\n\nNico Stehr (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a).\n\nAlso Alvin Toffler argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values – is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994).\n\nAt the end of the twentieth century, the concept of the network society gained importance in information society theory. For Manuel Castells, network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm.\n\nJan Van Dijk (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society. Darin Barney uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\".\n\nThe major critique of concepts such as information society, knowledge society, network society, postmodern society, postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as Frank Webster argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adopt to existing political realities (kasiwulaya 2002b: 267).\n\nThese critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and cultural capital. They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century, corporate capitalism in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006).\n\nFor describing contemporary society based on a dialectic of the old and the new, continuity and discontinuity, other critical scholars have suggested several terms like:\n\nOther scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism (Manuel Castells 2000, Christian Fuchs 2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500). Nicholas Garnham is critical of Castells and argues that the latter’s account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society.\n\nAntonio Negri and Michael Hardt argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and affective labour that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.).\n\nOverall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation (David Harvey 1989). They warn that new technologies are embedded into societal antagonisms that cause structural unemployment, rising poverty, social exclusion, the deregulation of the welfare state and of labour rights, the lowering of wages, welfare, etc.\n\nConcepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.\n\nInformation society is the means of getting information from one place to another. As technology has advanced so too has the way people have adapted in sharing this information with each other.\n\n\"Second nature\" refers a group of experiences that get made over by culture. They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted.\n\nHowever, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from else where. It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society.\n\nTherefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction.\n\nIn sociology, informational society refers to a post-modern type of society. Theoreticians like Ulrich Beck, Anthony Giddens and Manuel Castells argue that since the 1970s a transformation from industrial society to informational society has happened on a global scale.\n\nAs steam power was the technology standing behind industrial society, so information technology is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century.\n\nIn the book \"Future Shock\", Alvin Toffler used the phrase super-industrial society to describe this type of society. Other writers and thinkers have used terms like \"post-industrial society\" and \"post-modern industrial society\" with a similar meaning.\n\nA number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The Information Age is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the knowledge economy, rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The computer revolution, and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The Information Revolution relates with the well known terms agricultural revolution and industrial revolution.\n\nToday, It is important to selectively select the information. Due to information revolution, the amount of information is puzzling. Among these, we need to develop techniques that refine information. This is called \"data mining.\" It is an engineering term, but it is used in sociology. In other words, if the amount of information was competitive in the past, the quality of information is important today.\n\nOne of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to intellectual property. Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because copy protection is often easily circumvented and socially \"rejected\" because the users and citizens of the information society can prove to be unwilling to accept such absolute commodification of the facts and information that compose their environment.\n\nResponses to this concern range from the Digital Millennium Copyright Act in the United States (and similar legislation elsewhere) which make copy protection (see DRM) circumvention illegal, to the free software, open source and copyleft movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share).\n\nCaveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society.\n\n\n\n "}
{"id": "889726", "url": "https://en.wikipedia.org/wiki?curid=889726", "title": "Insect collecting", "text": "Insect collecting\n\nInsect collecting refers to the collection of insects and other arthropods for scientific study or as a hobby. Because most insects are small and the majority cannot be identified without the examination of minute morphological characters, entomologists often make and maintain insect collections. Very large collections are conserved in natural history museums or universities where they are maintained and studied by specialists. Many college courses require students to form small collections. There are also amateur entomologists and collectors who keep collections.\n\nHistorically, insect collecting has been widespread and was in the Victorian age a very popular educational hobby. Insect collecting has left traces in European cultural history, literature and songs (e.g., Georges Brassens's \"La chasse aux papillons\" (\"The Hunt for Butterflies\")). The practice is still widespread in many countries, and is particularly common among Japanese youths.\n\nInsects are passively caught using funnels, pitfall traps, bottle traps, malaise traps, flight interception traps and other passive types of insect traps, some of which are baited with small bits of sweet foods (such as honey). Different designs of ultraviolet light traps such as the Robinson trap are also used by entomologists for collecting nocturnal insects (especially moths) during faunistic survey studies. Aspirators or \"pooters\" suck up insects too small or delicate to handle with fingers.\nSeveral different types of nets are commonly used to actively collect insects. Aerial insect nets are used to collect flying insects. The bag of a butterfly net is generally constructed from a lightweight mesh to minimize damage to delicate butterfly wings. A sweep net is used to collect insects from grass and brush. It is similar to a butterfly net, except that the bag is generally constructed from more rugged material.The sweep net is swept back and forth through vegetation quickly turning the opening from side to side and following a shallow figure eight pattern. The collector walks forward while sweeping, and the net is moved through plants and grasses with force.This requires a heavy net fabric such as sailcloth to prevent tearing, although light nets can be used if swept less vigorously. Sweeping continues for some distance and then the net is flipped over, with the bag hanging over the rim, trapping the insects until they can be removed with a pooter. Other types of nets used for collecting insects include beating nets and aquatic nets. Leaf litter sieves are used by coleopterists and to collect larvae.\n\nOnce collected, a killing jar is used to kill required insects before they damage themselves trying to escape. However, killing jars are generally only used on hard-bodied insects. Soft-bodied insects, such as those in the larval stage, are generally fixed in a vial containing an ethanol and water solution. Another now mostly historical approach is Caterpillar inflation where the innards were removed and the skin dried.\n\nThe usual method of display is in a glass-covered box, with the insects mounted on specially made non corrosive insect pins stuck into suitable foam plastic or paper covered cork at the bottom of the box. Common pins are not used. Very small insects may be pinned on \"minuten\" (very tiny headless pins) stuck into a block of foam plastic on a standard insect pin. Alternatively they may be glued onto a small piece of card on the pin. There are specific procedures for proper mounting that are used to show off the insects' essential characteristics. Techniques and equipment may be varied to deal with various species or requirements. For example, one or both of the wings of a beetle or grasshopper can be pulled open and fanned out to show the wing structure that otherwise would be hidden. At least the date and place of capture should be written or computer printed onto a piece of paper or card transfixed by the pin. This is called a data label.\nRare insects, or those from distant parts of the world, may also be acquired from dealers or by trading. Some noted insect collections have been sold at auction.\n\n\"Pokémon\" creator Satoshi Tajiri's childhood hobby of insect collecting is the inspiration behind the popular video game series.\n\n\nPicture Guide series For college students. Out of date but very useful for beginners.\n\n"}
{"id": "41566158", "url": "https://en.wikipedia.org/wiki?curid=41566158", "title": "Janet Lembke", "text": "Janet Lembke\n\nJanet Lembke (2 March 1933 – 3 September 2013), \"née\" Janet Nutt, was an American author, essayist, naturalist, translator and scholar. She was born in Cleveland, Ohio during the Great Depression, graduated in 1953 from Middlebury College, Vermont, with a degree in Classics, and her knowledge of the classical Greek and Latin worldview, from Homer to Virgil, informed her life and work. A Certified Virginia Master Gardener, she lived in Virginia and North Carolina, drawing inspiration from both locales. She was recognized for her creative view of natural cycles, agriculture and of animals, both domestic and wild, with whom we share the natural environment. Referred to as an \"acclaimed Southern naturalist,\" she was equally (as The Chicago Tribune described her) a \"classicist, a noted Oxford University Press translator of the works of Sophocles, Euripides and Aeschylus\". She received a grant from the National Endowment for the Arts to translate Virgil's Georgics, having already translated Euripides' \"Electra\" and \"Hecuba\", and Aeschylus's \"Persians\" and \"Suppliants\".\n\nJanet Lembke's first book was \"Bronze and Iron: Old Latin Poetry from Its Beginnings to 100 B.C.\" (1973), but beyond translations and essays about classics, there were more than a dozen books on nature, works for which the author acquired a base of admirers. Her articles were printed in The New York Times, \"Sierra Magazine\" (The Sierra Club), Oxford American, Audubon, Raleigh News and Observer, Southern Review and other publications. The writing style was eclectic and personal, meditative and detailed, and though she was at least once accused of \"taking poetic license too far\" in her translation of \"Georgics\", readers were often charmed and seduced by her way of weaving scientific fact, history and culture, with personal anecdote, mythological allusion and poetic feeling. \"The author's ability to pull together disparate elements in her writing is impressive, and her passionate connection with the natural world is displayed in line after line,\" wrote The New York Times. Novelist Annie Proulx expressed a similar perception, observing that \"Lembke's writing tacks between three points: the stuff of her late-twentieth-century life; the tangle of creature and plant in every dimension of tide and river flow; and the haunting, connecting wires of mythos that still knot us to the ancient beginnings.\"\n\nAmong Janet Lembke's noted titles were \"Because the Cat Purrs: How We Relate to Other Species and Why It Matters\" (2008); \"Skinny Dipping: And Other Immersions in Water, Myth, and Being Human\" (2004); \"Dangerous Birds\" (1996); \"River Time\" (1997); \"Despicable Species: On Cowbirds, Kudzu, Hornworms, and Other Scourges\" (1999); and \"The Quality of Life: Living Well, Dying Well\" (2004)-- a sober and unflinching account of the death of the author's mother. At the time of her own death at age 80 in Staunton, Virginia, Janet Lembke was working on a memoir, \"I Married an Arsonist\". She had married twice, and had four children and six grandchildren.\n\nThere is a repository of archived materials (\"The Janet Lembke Papers, 1966 - 2008\"), including notes and correspondence by the author, at the Jackson Library of the University of North Carolina in Greensboro, NC.\n\n\n\n\n"}
{"id": "12804696", "url": "https://en.wikipedia.org/wiki?curid=12804696", "title": "Jaramillo reversal", "text": "Jaramillo reversal\n\nThe Jaramillo reversal was a reversal and excursion of the Earth's magnetic field that occurred approximately one million years ago. In the geological time scale it was a \"short-term\" positive reversal in the then-dominant Matuyama reversed magnetic chronozone; its beginning is widely dated to 990,000 years before the present (BP), and its end to 950,000 BP (though an alternative date of 1.07 million years ago to 990,000 is also found in the scientific literature).\n\nThe causes and mechanisms of short-term reversals and excursions like the Jaramillo, as well as the major field reversals like the Brunhes–Matuyama reversal, are subjects of study and dispute among researchers. One theory associates the Jaramillo with the Bosumtwi impact event, as evidenced by a tektite strewnfield in the Ivory Coast, though this hypothesis has been claimed as \"highly speculative\" and \"refuted\".\n\n"}
{"id": "8746727", "url": "https://en.wikipedia.org/wiki?curid=8746727", "title": "Level of support for evolution", "text": "Level of support for evolution\n\nThe level of support for evolution among scientists, the public and other groups is a topic that frequently arises in the creation-evolution controversy and touches on educational, religious, philosophical, scientific and political issues. The subject is especially contentious in countries where significant levels of non-acceptance of evolution by general society exist although evolution is taught at school and university.\n\nNearly all (around 97%) of the scientific community accepts evolution as the dominant scientific theory of biological diversity. Scientific associations have strongly rebutted and refuted the challenges to evolution proposed by intelligent design proponents.\n\nThere are religious sects and denominations in several countries for whom the theory of evolution is in conflict with creationism that is central to their beliefs, and who therefore reject it: in the United States, South Africa, India, South Korea, Singapore, the Philippines, and Brazil, with smaller followings in the United Kingdom, the Republic of Ireland, Japan, Italy, Germany, Israel, Australia, New Zealand, and Canada.\n\nSeveral publications discuss the subject of acceptance, including a document produced by the United States National Academy of Sciences.\n\nThe vast majority of the scientific community and academia supports evolutionary theory as the only explanation that can fully account for observations in the fields of biology, paleontology, molecular biology, genetics, anthropology, and others. A 1991 Gallup poll found that about 5% of American scientists (including those with training outside biology) identified themselves as creationists.\n\nAdditionally, the scientific community considers intelligent design, a neo-creationist offshoot, to be unscientific, pseudoscience, or junk science. The U.S. National Academy of Sciences has stated that intelligent design \"and other claims of supernatural intervention in the origin of life\" are not science because they cannot be tested by experiment, do not generate any predictions, and propose no new hypotheses of their own. In September 2005, 38 Nobel laureates issued a statement saying \"Intelligent design is fundamentally unscientific; it cannot be tested as scientific theory because its central conclusion is based on belief in the intervention of a supernatural agent.\" In October 2005, a coalition representing more than 70,000 Australian scientists and science teachers issued a statement saying \"intelligent design is not science\" and calling on \"all schools not to teach Intelligent Design (ID) as science, because it fails to qualify on every count as a scientific theory\".\n\nIn 1986, an \"amicus curiae\" brief, signed by 72 US Nobel Prize winners, 17 state academies of science and 7 other scientific societies, asked the US Supreme Court in \"Edwards v. Aguillard\", to reject a Louisiana state law requiring that where evolutionary science was taught in public schools, creation science must also be taught. The brief also stated that the term \"creation science\" as used by the law embodied religious dogma, and that \"teaching religious ideas mislabeled as science is detrimental to scientific education\". This was the largest collection of Nobel Prize winners to sign a petition up to that point. According to anthropologists Almquist and Cronin, the brief is the \"clearest statement by scientists in support of evolution yet produced.\"\n\nThere are many scientific and scholarly organizations from around the world that have issued statements in support of the theory of evolution. The American Association for the Advancement of Science, the world's largest general scientific society with more than 130,000 members and over 262 affiliated societies and academies of science including over 10 million individuals, has made several statements and issued several press releases in support of evolution. The prestigious United States National Academy of Sciences, which provides science advice to the nation, has published several books supporting evolution and criticising creationism and intelligent design.\n\nThere is a notable difference between the opinion of scientists and that of the general public in the United States. A 2009 poll by Pew Research Center found that \"Nearly all scientists (97%) say humans and other living things have evolved over time – 87% say evolution is due to natural processes, such as natural selection. The dominant position among scientists – that living things have evolved due to natural processes – is shared by only about a third (32%) of the public.\"\n\nOne of the earliest resolutions in support of evolution was issued by the American Association for the Advancement of Science in 1922, and readopted in 1929.\n\nAnother early effort to express support for evolution by scientists was organized by Nobel Prize–winning American biologist Hermann J. Muller in 1966. Muller circulated a petition entitled \"Is Biological Evolution a Principle of Nature that has been well established by Science?\" in May 1966:\n\nThis manifesto was signed by 177 of the leading American biologists, including George G. Simpson of Harvard University, Nobel Prize Winner Peter Agre of Duke University, Carl Sagan of Cornell, John Tyler Bonner of Princeton, Nobel Prize Winner George Beadle, President of the University of Chicago, and Donald F. Kennedy of Stanford University, formerly head of the United States Food and Drug Administration.\n\nThis was followed by the passing of a resolution by the American Association for the Advancement of Science (AAAS) in the fall of 1972 that stated, in part, \"the theory of creation ... is neither scientifically grounded nor capable of performing the rules required of science theories\". The United States National Academy of Sciences also passed a similar resolution in the fall of 1972. A statement on evolution called \"A Statement Affirming Evolution as a Principle of Science.\" was signed by Nobel Prize Winner Linus Pauling, Isaac Asimov, George G. Simpson, Caltech Biology Professor Norman H. Horowitz, Ernst Mayr, and others, and published in 1977. The governing board of the American Geological Institute issued a statement supporting resolution in November 1981.\nShortly thereafter, the AAAS passed another resolution supporting evolution and disparaging efforts to teach creationism in science classes.\n\nTo date, there are no scientifically peer-reviewed research articles that disclaim evolution listed in the scientific and medical journal search engine Pubmed.\n\nThe Discovery Institute announced that over 700 scientists had expressed support for intelligent design as of February 8, 2007. This prompted the National Center for Science Education to produce a \"light-hearted\" petition called \"Project Steve\" in support of evolution. Only scientists named \"Steve\" or some variation (such as Stephen, Stephanie, and Stefan) are eligible to sign the petition. It is intended to be a \"tongue-in-cheek parody\" of the lists of alleged \"scientists\" supposedly supporting creationist principles that creationist organizations produce. The petition demonstrates that there are more scientists who accept evolution with a name like \"Steve\" alone (over 1370) than there are in total who support intelligent design. This is, again, why the percentage of scientists who support evolution has been estimated by Brian Alters to be about 99.9 percent.\n\nMany creationists act as evangelists and their organizations are registered as tax-free religious organizations. Creationists have claimed that they represent the interests of true Christians, and evolution is associated only with atheism.\n\nHowever, not all religious organizations find support for evolution incompatible with their religious faith. For example, 12 of the plaintiffs opposing the teaching of creation science in the influential \"McLean v. Arkansas\" court case were clergy representing Methodist, Episcopal, African Methodist Episcopal, Catholic, Southern Baptist, Reform Jewish, and Presbyterian groups. There are several religious organizations that have issued statements advocating the teaching of evolution in public schools. In addition, the Archbishop of Canterbury, Dr. Rowan Williams, issued statements in support of evolution in 2006. The Clergy Letter Project is a signed statement by 12,808 (as of 28 May 2012) American Christian clergy of different denominations rejecting creationism organized in 2004. Molleen Matsumura of the National Center for Science Education found, of Americans in the twelve largest Christian denominations, at least 77% belong to churches that support evolution education (and that at one point, this figure was as high as 89.6%). These religious groups include the Catholic Church, as well as various denominations of Protestantism, including the United Methodist Church, National Baptist Convention, USA, Evangelical Lutheran Church in America, Presbyterian Church (USA), National Baptist Convention of America, African Methodist Episcopal Church, the Episcopal Church, and others. A figure closer to about 71% is presented by the analysis of Walter B. Murfin and David F. Beck.\n\nMichael Shermer argued in Scientific American in October 2006 that evolution supports concepts like family values, avoiding lies, fidelity, moral codes and the rule of law. Shermer also suggests that evolution gives more support to the notion of an omnipotent creator, rather than a tinkerer with limitations based on a human model.\n\nThe Ahmadiyya Movement universally accepts evolution and actively promotes it. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community. The Ahmadis do not believe Adam was the first human on earth, but merely the first prophet to receive a revelation of God.\n\nA fundamental part of `Abdul-Bahá's teachings on evolution is the belief that all life came from the same origin: \"the origin of all material life is one...\" He states that from this sole origin, the complete diversity of life was generated: \"Consider the world of created beings, how varied and diverse they are in species, yet with one sole origin\" He explains that a slow, gradual process led to the development of complex entities:\n\nThe 1950 encyclical \"Humani generis\" advocated scepticism towards evolution without explicitly rejecting it; this was substantially amended by Pope John-Paul II in 1996 in an address to the Pontifical Academy of Sciences in which he said, \"Today, almost half a century after publication of the encyclical, new knowledge has led to the recognition of the theory of evolution as more than a hypothesis.\" Between 2000 and 2002 the International Theological Commission found that \"Converging evidence from many studies in the physical and biological sciences furnishes mounting support for some theory of evolution to account for the development and diversification of life on earth, while controversy continues over the pace and mechanisms of evolution.\" This statement was published by the Vatican on July 2004 by the authority of Cardinal Ratzinger (who became Pope Benedict XVI) who was the president of the Commission at the time.\n\nThe Magisterium has not made an authoritative statement on intelligent design, and has permitted arguments on both sides of the issue. In 2005, Cardinal Christoph Schönborn of Vienna appeared to endorse intelligent design when he denounced philosophically materialist interpretations of evolution. In an op-ed in the New York Times he said \"Evolution in the sense of common ancestry might be true, but evolution in the neo-Darwinian sense - an unguided, unplanned process of random variation and natural selection - is not.\" This common line of reasoning among fundamentalist theologians is flawed, as evolution by natural selection is not random at all; only mutations occur in a stochastic manner, while natural selection establishes genes which aid survival in a particular environment.\n\nIn the January 16–17 2006 edition of the official Vatican newspaper \"L'Osservatore Romano\", University of Bologna evolutionary biology Professor Fiorenzo Facchini wrote an article agreeing with the judge's ruling in \"Kitzmiller v. Dover\" and stating that intelligent design was unscientific. Jesuit Father George Coyne, former director of the Vatican Observatory, has also denounced intelligent design.\n\nHindus believe in the concept of evolution of life on Earth. The concepts of Dashavatara—different incarnations of God starting from simple organisms and progressively becoming complex beings—and Day and Night of Brahma are generally cited as instances of Hindu acceptance of evolution.\n\nIn the United States, many Protestant denominations promote creationism, preach against evolution, and sponsor lectures and debates on the subject. Denominations that explicitly advocate creationism instead of evolution or \"Darwinism\" include the Assemblies of God, the Free Methodist Church, Lutheran Church–Missouri Synod, Pentecostal Churches, Seventh-day Adventist Churches, Wisconsin Evangelical Lutheran Synod, Christian Reformed Church, Southern Baptist Convention, and the Pentecostal Oneness churches. Jehovah's Witnesses produce gap creationism and day-age creationism literature to refute evolution but reject the \"creationist\" label, which they consider to apply only to Young Earth creationism.\n\nA common complaint of creationists is that evolution is of no value, has never been used for anything, and will never be of any use. According to many creationists, nothing would be lost by getting rid of evolution, and science and industry might even benefit.\n\nIn fact, evolution is being put to practical use in industry and widely used on a daily basis by researchers in medicine, biochemistry, molecular biology, and genetics to both formulate hypotheses about biological systems for the purposes of experimental design, as well as to rationalise observed data and prepare applications. As of August 2017 there are 487,558 scientific papers in PubMed that mention 'evolution'. Pharmaceutical companies utilize biological evolution in their development of new products, and also use these medicines to combat evolving bacteria and viruses.\n\nBecause of the perceived value of evolution in applications, there have been some expressions of support for evolution on the part of corporations. In Kansas, there has been some widespread concern in the corporate and academic communities that a move to weaken the teaching of evolution in schools will hurt the state's ability to recruit the best talent, particularly in the biotech industry. Paul Hanle of the Biotechnology Institute warned that the United States risks falling behind in the biotechnology race with other nations if it does not do a better job of teaching evolution. James McCarter of Divergence Incorporated stated that the work of 2001 Nobel Prize winner Leland Hartwell relied heavily on the use of evolutionary knowledge and predictions, both of which have significant implications for the treatment of cancers. Furthermore, McCarter concluded that 47 of the last 50 Nobel Prizes in medicine or physiology depended on an understanding of evolutionary theory (according to McCarter's unspecified personal criteria).\n\nThere are also many educational organizations that have issued statements in support of the theory of evolution.\n\nRepeatedly, creationists and intelligent design advocates have lost suits in US courts. Here is a list of important court cases in which creationists have suffered setbacks:\n\n\nThere does not appear to be significant correlation between believing in evolution and understanding evolutionary science. In some countries, creationist beliefs (or a lack of support for evolutionary theory) are relatively widespread, even garnering a majority of public opinion. A study published in \"Science\" compared attitudes about evolution in the United States, 32 European countries (including Turkey) and Japan. The only country where acceptance of evolution was lower than in the United States was Turkey (25%). Public acceptance of evolution was most widespread (at over 80% of the population) in Iceland, Denmark and Sweden.\nAccording to the PEW research center, Afghanistan has the lowest acceptance of evolution in the Muslim countries. Only 26% of people in Afghanistan accept evolution. 62% deny human evolution and believe that humans have always existed in their present form..\n\nAccording to a 2014 poll produced by the Pew Research Center, 71% of people in Argentina believe \"humans and other living things evolved over time\" while 23% believe they have \"always existed in the present form.\"\nAccording to the PEW research, 56 percent of Armenians deny human evolution & claim that humans have always existed in their present and only 34 percent of Armenians accept human evolution.\n\nA 2009 poll showed that almost a quarter of Australians believe \"the biblical account of human origins\" over the Darwinian account. 42 percent of Australians believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God\".\n\nA 2010 survey conducted by Auspoll and the Australian Academy of Science found that 79% of Australians believe in evolution (71% believe it is currently occurring, 8% believe in evolution but do not think it is currently occurring), 11% were not sure and 10% stated they do not believe in evolution.\n\nAccording to a 2014 poll by the Pew Research Center, 44% of people in Bolivia believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nIn a 2010 poll, 59% of respondents said they believe in theistic evolution, or evolution guided by God. A further 8% believe in evolution without divine intervention, while 25% were creationists. Support for creationism was stronger among the poor and the least educated. According to a 2014 poll produced by the Pew Research Center, 66% of Brazilians agree that humans evolved over time and 29% think they have always existed in the present form.\n\nIn a 2012 poll, 61% of Canadians believe that humans evolved from less advanced life forms, while 22% believe that God created human beings in their present form within the last 10,000 years.\n\nAccording to a 2014 poll by the Pew Research Center, 69% of people in Chile believe \"humans and other living things evolved over time\" while 26% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Colombia believe \"humans and other living things evolved over time\" while 35% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 56% of people in Costa Rica believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center,the Czech Republic has the highest acceptance of evolution in Eastern Europe. 83 percent people in the Czech Republic believe that humans evolved over time.\n\nAccording to a 2014 poll by the Pew Research Center, 41% of people in Dominican Republic believe \"humans and other living things evolved over time\" while 56% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 50% of people in Ecuador believe \"humans and other living things evolved over time\" while 44% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 46% of people in El Salvador believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 55% of people in Guatemala believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 49% of people in Honduras believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center, Kazakhstan has the highest acceptance of evolution in the Muslim countries. 79% of \npeople in Kazakhstan acceptance the theory of evolution.\n\nAmong those who had heard of Charles Darwin and knew something about the theory of evolution, 77% of people in India agree that enough scientific evidence exists to support Charles Darwin’s Theory of Evolution. Also, 85% of God believing Indians who know about evolution agree that life on earth evolved over time as a result of natural selection.\n\nIn a survey carried among 10 major nations, the highest proportion that agreed that evolutionary theories alone should be taught in schools was in India, at 49%.\n\nIn a recent survey conducted across 12 states in India, public acceptence of evolution stood at 65.5% across Indian population. Highest acceptence was found in Delhi, Maharashtra and Kerala (all above 78%) while the least was found to be in Haryana (41.3%). Males were marginally more likely to accept the evolution compared with females (72% vs. 69%), and non-religious people compared with religious people (74% vs. 67%). Surprisingly people who identified as ‘rightists’ accepted the evolution more than those who identified themselves as ‘leftists’ (66% vs. 61%) in political spectra. The study also identified teachers and students (over 73%) as most likely to accept evolution while employed adults (59%) least. While at the international level, the trend is quite clear that religiosity is inversely proportional to public acceptance of evolution, situation in India was strikingly opposite. Lead author, Dr. Felix Bast from Central University of Punjab conjectured possible reason for high public acceptance of evolution in India despite the fact of high religiosity is that Hinduism does not conflict Darwin’s theory of evolution to a large extent. According to 2011 census, Hindus encompass 80.3% of Indian population. Many concepts of Vedas and Hinduism support the scientific consensus of geology, climate science and evolution to a large extent. For example, according to Rigveda, the age of earth is 1.97 billion years, which is very old compared with that of creation myth propounded by Abrahamic religions (according to creationism-also called Intelligent Design, the age of earth is around 6000 years). Current scientific consensus of the age of earth is 4.543 billion years. A number of evolutionary biologists in the past as well were baffled about the surprising similarity between evolutionary theory and Hinduism. British evolutionary biologist JBS Haldane, for instance, suggested that Hindu concept of \"dashavatara\"- the ten incarnations of lord Vishnu- is a rough idea of vertebrate evolution (fish-the vertebrate to tortoise-reptile to boar-mammal to man). Vedic concepts of \"pralaya\" and \"mahapralaya\" too surprisingly capture the cyclic nature of global climate (glacial-interglacial cycles).\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 85% of Indonesian high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nThe theory of evolution is a 'hard sell' in schools in Israel. More than half of Israeli Jews accept the human evolution while more than 40% deny human evolution & claim that humans have always existed in their present form. \n\nAccording to a 2014 poll by the Pew Research Center, 64% of people in Mexico believe \"humans and other living things evolved over time\" while 32% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 47% of people in Nicaragua believe \"humans and other living things evolved over time\" while 48% believe they have \"always existed in the present form.\"\n\nAccording to a 2008 Norstat poll for NRK, 59% of the Norwegian population fully accept evolution, 24% somewhat agree with the theory, 4% somewhat disagree with the theory while 8% do not accept evolution. 4% did not know.\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 86% of Pakistani high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nAccording to a 2014 poll by the Pew Research Center, 61% of people in Panama believe \"humans and other living things evolved over time\" while 34% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Paraguay believe \"humans and other living things evolved over time\" while 30% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 51% of people in Peru believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nA 2006 UK poll on the \"origin and development of life\" asked participants to choose between three different explanations for the origin of life: 22% chose (Young Earth) creationism, 17% opted for intelligent design (\"certain features of living things are best explained by the intervention of a supernatural being, e.g. God\"), 48% selected evolution theory (with a divine role explicitly excluded) and the rest did not know. A 2009 poll found that only 38% of Britons believe God played no role in evolution. In a 2012 poll, 69% of Britons believe that humans evolved from less advanced life forms, while 17% believe that God created human beings in their present forms within the last 10,000 years.\n\nUS courts have ruled in favor of teaching evolution in science classrooms, and against teaching creationism, in numerous cases such as Edwards v. Aguillard, Hendren v. Campbell, McLean v. Arkansas and Kitzmiller v. Dover Area School District.\n\nA prominent organization in the United States behind the intelligent design movement is the Discovery Institute, which, through its Center for Science and Culture, conducts a number of public relations and lobbying campaigns aimed at influencing the public and policy makers in order to advance its position in academia. The Discovery Institute claims that because there is a significant lack of public support for evolution, that public schools should, as their campaign states, \"Teach the Controversy\", although there is no controversy over the validity of evolution within the scientific community.\n\nThe US has one of the highest levels of public belief in biblical or other religious accounts of the origins of life on earth among industrialized countries. However according to the PEW research center, 62 percent of adults in the United States accept human evolution and while 34 percent of adults believe that humans have always existed in their present form. The poll involved over 35,000 adults in the United States. However acceptance of evolution varies per state. For example the State of Vermont has the highest acceptance of evolution of any other State in the United States. 79% people in Vermont accept human evolution. Mississippi has the lowest acceptance of evolution of any other State in the United States.\nA 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years. 19% believed that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process\", despite 49% of respondents indicating they believed in evolution. Belief in creationism is inversely correlated to education; only 22% of those with post-graduate degrees believe in strict creationism. (The level of support for strict creationism could be even lower when poll results are adjusted after comparison with other polls with questions that more specifically account for uncertainty and ambivalence.A 2000 poll for People for the American Way found 70% of the American public felt that evolution was compatible with a belief in God.\n\nA 2005 Pew Research Center poll found that 70% of evangelical Christians believed that living organisms have not changed since their creation, but only 31% of Catholics and 32% of mainline Protestants shared this opinion. A 2005 Harris Poll estimated that 63% of liberals and 37% of conservatives agreed that humans and other primates have a common ancestry.\n\nAccording to a 2014 poll produced by the Pew Research Center, 74% of people in Uruguay believe \"humans and other living things evolved over time\" while 20% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 63% of people in Venezuela believe \"humans and other living things evolved over time\" while 33% believe they have \"always existed in the present form.\"\n\nThe level of assent that evolution garners has changed with time. The trends in acceptance of evolution can be estimated.\n\nThe level of support for evolution in different communities has varied with time. Darwin's theory had convinced almost every naturalist within 20 years of its publication in 1858, and was making serious inroads with the public and the more liberal clergy. It had reached such extremes, that by 1880, one\nAmerican religious weekly publication estimated that \"perhaps a quarter, perhaps a half of the educated ministers in our leading Evangelical denominations\" felt \"that the story of the creation and fall of man, told in Genesis, is no more the record of actual occurrences than is the parable of the Prodigal Son.\"\n\nBy the late 19th century, many of the most conservative Christians accepted an ancient earth, and life on earth before Eden. Victorian Era Creationists were more akin to people who subscribe to theistic evolution today. Even fervent anti-evolutionist Scopes Trial prosecutor William Jennings Bryan interpreted the \"days\" of Genesis as ages of the earth, and acknowledged that biochemical evolution took place, drawing the line only at the story of Adam and Eve's creation. Prominent pre-World War II creationist Harry Rimmer allowed an Old Earth by slipping millions of years into putative gaps in the Genesis account, and claimed that the Noachian Flood was only a local phenomenon.\n\nIn the decades of the 20th century, George McCready Price and a tiny group of Seventh-day Adventist followers were among the very few believers in a Young Earth and a worldwide flood, which Price championed in his \"new catastrophism\" theories. It was not until the publication of John C. Whitcomb, Jr., and Henry M. Morris’s book \"Genesis Flood\" in 1961 that Price's idea was revived. In the last few decades, many creationists have adopted Price's beliefs, becoming progressively more strict biblical literalists.\n\nIn a 1991 Gallup poll, 47% of the US population, and 25% of college graduates agreed with the statement, \"God created man pretty much in his present form at one time within the last 10,000 years.\"\n\nFourteen years\nlater, in 2005, Gallup found that 53% of Americans expressed the belief that \"God created human beings in their present form exactly the way the Bible describes it.\" About 2/3 (65.5%) of those surveyed thought that creationism was definitely or probably true. In 2005 a Newsweek poll discovered that 80 percent of the American public thought that \"God created the universe.\" and the Pew Research Center reported that \"nearly two-thirds of Americans say that creationism should be taught alongside evolution in public schools.\" Ronald Numbers commented on that with \"Most surprising of all was the discovery that large numbers of high-school biology teachers — from 30% in Illinois and 38% in Ohio to a whopping 69% in Kentucky — supported the teaching of creationism.\"\n\nThe National Center for Science Education reports that from 1985 to 2005, the number of Americans unsure about evolution increased from 7% to 21%, while the number rejecting evolution declined from 48% to 39%. Jon Miller of Michigan State University has found in his polls that the number of Americans who accept evolution has declined from 45% to 40% from 1985 to 2005.\n\nIn light of these somewhat contradictory results, it is difficult to know for sure what is happening to public opinion on evolution in the US. It does not appear that either side is making unequivocal progress. It does appear that uncertainty about the issue is increasing, however.\n\nAnecdotal evidence is that creationism is becoming more of an issue in the UK as well. One report in 2006 was that UK students are increasingly arriving ill-prepared to participate in medical studies or other advanced education.\n\nThe level of support for creationism among relevant scientists is minimal. In 2007 the Discovery Institute reported that about 600 scientists signed their \"A Scientific Dissent from Darwinism\" list, up from 100 in 2001. The actual statement of the Scientific Dissent from Darwinism is a relatively mild one that expresses skepticism about the absoluteness of 'Darwinism' (and is in line with the falsifiability required of scientific theories) to explain all features of life, and does not in any way represent an absolute denial or rejection of evolution. By contrast, a tongue-in-cheek response known as Project Steve, a list restricted to scientists named Steve, Stephanie etc. who agree that evolution is \"a vital, well-supported, unifying principle of the biological sciences,\" has 1,382 signatories . People with these names make up approximately 1% of the total U.S. population.\n\nThe United States National Science Foundation statistics on US yearly science graduates demonstrate that from 1987 to 2001, the number of biological science graduates increased by 59% while the number of geological science graduates decreased by 20.5%. However, the number of geology graduates in 2001 was only 5.4% of the number of graduates in the biological sciences, while it was 10.7% of the number of biological science graduates in 1987. The Science Resources Statistics Division of the National Science Foundation estimated that in 1999, there were 955,300 biological scientists in the US (about 1/3 of who hold graduate degrees). There were also 152,800 earth scientists in the US as well.\n\nA large fraction of the Darwin Dissenters have specialties unrelated to research on evolution; of the dissenters, three-quarters are not biologists. As of 2006, the dissenter list was expanded to include non-US scientists.\n\nSome researchers are attempting to understand the factors that affect people's acceptance of evolution. Studies have yielded inconsistent results, explains associate professor of education at Ohio State University, David Haury. He recently performed a study that found people are likely to reject evolution if they have feelings of uncertainty, regardless of how well they understand evolutionary theory. Haury believes that teachers need to show students that their intuitive feelings may be misleading (for example, using the Wason selection task), and thus to exercise caution when relying on them as they judge the rational merits of ideas.\n\n\n"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "24695021", "url": "https://en.wikipedia.org/wiki?curid=24695021", "title": "List of invasive species in North America", "text": "List of invasive species in North America\n\nThis is a list of invasive species in North America. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, directly threatening human industry, such as agriculture, or the local biodiversity.\n\nThe term \"invasive species\" refers to a subset of those species defined as introduced species. If a species has been introduced, but remains local, and is not problematic for human industry or the local biodiversity, then it is not considered invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2277747", "url": "https://en.wikipedia.org/wiki?curid=2277747", "title": "Metal clay", "text": "Metal clay\n\nMetal clay is a crafting medium consisting of very small particles of metal such as silver, gold, bronze, or copper mixed with an organic binder and water for use in making jewelry, beads and small sculptures. Originating in Japan in 1990, metal clay can be shaped just like any soft clay, by hand or using molds. After drying, the clay can be fired in a variety of ways such as in a kiln, with a handheld gas torch, or on a gas stove, depending on the type of clay and the metal in it. The binder burns away, leaving the pure sintered metal. Shrinkage of between 8% and 30% occurs (depending on the product used). Alloys such as bronze, sterling silver, and steel also are available.\n\nMetal clay first came out in Japan in 1990 to allow craft jewelry makers to make sophisticated looking jewelry without the years of study needed to make fine jewelry.\n\nFine silver metal clay results in objects containing 99.9% pure silver, which is suitable for enameling. Lump metal clay is sold in sealed packets to keep it moist and workable. The silver versions are also available as a softer paste in a pre-filled syringe which can be used to produce extruded forms, in small jars of slip and as paper-like sheets, from which most of the moisture has been removed. Common brands of silver metal clay include Precious Metal Clay (PMC) and Art Clay Silver (ACS).\n\nMetal clay artists looking for more strength in their silver creations can also mix PMC fine silver clay with an equal part of PMC Sterling clay. The firing of this alloy is found to be up to for two hours.\n\nAnother available alloy, EZ960 Sterling Silver Metal Clay was invented by Bill Struve from Metal Adventures, the inventor of BRONZclay™ and COPPRclay™. Because the clay is a sterling silver alloy, one of its best attributes is its post firing strength, in comparison to fine silver. This clay is fired open shelf on a raised hard ceramic kiln shelf at for 2 hours, full ramp. No carbon required. Its shrinkage rate is smaller than other clays, at 10–11%.\n\nPMC was developed in the early 1990s in Japan by metallurgist Masaki Morikawa. As a solid-phase sintered product of a precious metal powder used to form a precious metal article, the material consists of microscopic particles of pure silver or fine gold and a water-soluble, non-toxic, organic binder that burns off during firing. Success was first achieved with gold and later duplicated with silver. \nThe PMC brand includes the following products:\n\nACS was developed by AIDA Chemical Industries, also a Japanese company. ACS followed PMC Standard with their Art Clay Original clay (more like PMC+ than PMC Standard), which allows the user to fire with a handheld torch or on a gas hob. Owing to subtle differences in the binder and suggested firing times, this clay shrinks less than the PMC versions, approximately 8–10%.\n\nFurther developments introduced the Art Clay Slow Dry, a clay with a longer working time. Art Clay 650 and Art Clay 650 Slow Dry soon followed; both clays can be fired at , allowing the user to combine the clay with glass and sterling silver, which are affected negatively by the higher temperatures needed to fire the first generation clays. AIDA also manufacturers Oil Paste, a product used only on fired metal clay or milled fine silver, and Overlay Paste, which is designed for drawing designs on glass and porcelain.\n\nIn 2006 AIDA introduced the Art Clay Gold Paste, a more economical way to work with gold. The paste is painted onto the fired silver clay, then refired in a kiln, or with a torch or gas stove. When fired, it bonds with the silver, giving a 22-carat gold accent. The same year also saw Art Clay Slow Tarnish introduced, a clay that tarnishes less rapidly than the other metal clays.\n\nLump metal clay in bronze was introduced in 2008 by Metal Adventures Inc. and in 2009 by Prometheus. Lump metal clays in copper were introduced in 2009 by Metal Adventures Inc. and Aida. Because of the lower cost, the bronze and copper metal clays are used by artists more often than the gold and silver metal clays in the American market place. The actual creation time of a bronze or copper piece is also far greater than that of its silver counterpart. Base metal clays, such as bronze, copper, and steel metal clays are best fired in the absence of oxygen to eliminate the oxidation of the metal by atmospheric oxygen. A means to accomplish this –- to place the pieces in activated carbon inside a container – was developed by Bill Struve.\n\nMetal clays are also available as dry powders to which water is added to hydrate and kneaded to attain a clay consistency. One advantage to the powders are their unlimited shelf life. The first silver clay in powder form was released in 2006 as Silver Smiths' Metal Clay Powder. In the following years base metal clays by Hadar Jacobson and Goldie World released several variation containing copper, brass, and even steel.\n\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "14272151", "url": "https://en.wikipedia.org/wiki?curid=14272151", "title": "Nature religion", "text": "Nature religion\n\nA nature religion is a religious movement that believes nature and the natural world is an embodiment of divinity, sacredness or spiritual power. Nature religions include indigenous religions practiced in various parts of the world by cultures who consider the environment to be imbued with spirits and other sacred entities. It also includes contemporary Pagan faiths which are primarily concentrated in Europe and North America.\n\nThe term \"nature religion\" was first coined by the American religious studies scholar Catherine Albanese, who used it in her work \"Nature Religion in America: From the Algonkian Indians to the New Age\" (1991) and later went on to use it in other studies. Following on from Albanese's development of the term it has since been used by other academics working in the discipline.\n\nCatherine Albanese described nature religion as \"a symbolic center and the cluster of beliefs, behaviours, and values that encircles it\", deeming it to be useful for shining a light on aspects of history that are rarely viewed as religious.\nIn a paper of his on the subject, the Canadian religious studies scholar Peter Beyer described \"nature religion\" as a \"useful analytical abstraction\" to refer to \"any religious belief or practice in which devotees consider nature to be the embodiment of divinity, sacredness, transcendence, spiritual power, or whatever cognate term one wishes to use\". He went on to note that in this way nature religion was not an \"identifiable religious tradition\" such as Buddhism or Christianity are, but that it instead covers \"a range of religious and quasi-religious movements, groups and social networks whose participants may or may not identify with one of the many constructed religions of global society which referred to many other nature religion.\"\n\nPeter Beyer noted the existence of a series of common characteristics which he believed were shared by different nature religions. He remarked that although \"one must be careful not to overgeneralise\", he suspected that there were a series of features which \"occur sufficiently often\" in those nature religions known to recorded scholarship to constitute a pattern.\n\nThe first of these common characteristics was nature religion's \"comparative resistance to institutionalisation and legitimisation in terms of identifiable socio-religious authorities and organisations\", meaning that nature religionists rarely formed their religious beliefs into large, visible socio-political structures such as churches. Furthermore, Beyer noted, nature religionists often held a \"concomitant distrust of and even eschewing of politically orientated power\". Instead of this, he felt that among nature religious communities, there was \"a valuing of community as non-hierarchical\" and a \"conditional optimism with regard to human capacity and the future.\"\n\nIn the sphere of the environment, Beyer noted that nature religionists held to a \"holistic conception of reality\" and \"a valorisation of physical place as vital aspects of their spiritualities\". Similarly, Beyer noted the individualism which was favoured by nature religionists. He remarked that those adhering to such beliefs typically had respect for \"charismatic and hence purely individual authority\" and place a \"strong emphasis on individual paths\" which led them to believe in \"the equal value of individuals and groups\". Along similar lines, he also commented on the \"strong experiential basis\" to nature religionist beliefs \"where personal experience is a final arbiter of truth or validity\".\n\nIn April 1996, the University of Lancaster in North West England held a conference on contemporary Paganism entitled \"Nature Religion Today: Western Paganism, Shamanism and Esotericism in the 1990s\", and ultimately led to the publication of an academic anthology of the same name two years later. This book, \"Nature Religion Today: Paganism in the Modern World\", was edited by members of the University's Department of Religious Studies, a postgraduate named Joanne Pearson and two professors, Richard H. Roberts and Geoffrey Samuel.\n\nIn his study of Wicca, the Pagan studies scholar Ethan Doyle White expressed the view that the category of \"nature religion\" was problematic from a \"historical perspective\" because it solely emphasises the \"commonalities of belief and attitude to the natural world\" that are found between different religions and in doing so divorces these different belief systems from their distinctive socio-cultural and historical backgrounds.\n\n\n"}
{"id": "4368966", "url": "https://en.wikipedia.org/wiki?curid=4368966", "title": "Nature writing", "text": "Nature writing\n\nNature writing is nonfiction or fiction prose or poetry about the natural environment. Nature writing encompasses a wide variety of works, ranging from those that place primary emphasis on natural history facts (such as field guides) to those in which philosophical interpretation predominate. It includes natural history essays, poetry, essays of solitude or escape, as well as travel and adventure writing.\n\nNature writing often draws heavily on scientific information and facts about the natural world; at the same time, it is frequently written in the first person and incorporates personal observations of and philosophical reflections upon nature.\n\nModern nature writing traces its roots to the works of natural history that were popular in the second half of the 18th century and throughout the 19th. An important early figures was the \"parson-naturalist\" Gilbert White (1720 – 1793), a pioneering English naturalist and ornithologist. He is best known for his \"Natural History and Antiquities of Selborne\" (1789).\n\nWilliam Bartram (1739 – 1823) is a significant early American pioneer naturalist who first work was published in 1791.\n\nGilbert White is regarded by many as England's first ecologist, and one of those who shaped the modern attitude of respect for nature. He said of the earthworm: \"Earthworms, though in appearance a small and despicable link in the chain of nature, yet, if lost, would make a lamentable chasm. [...] worms seem to be the great promoters of vegetation, which would proceed but lamely without them\" White and William Markwick collected records of the dates of emergence of more than 400 plant and animal species in Hampshire and Sussex between 1768 and 1793, which was summarised in \"The Natural History and Antiquities of Selborne\", as the earliest and latest dates for each event over the 25-year period, are among the earliest examples of modern phenology.\n\nThe tradition of clerical naturalists predates White and can be traced back to some monastic writings of the Middle Ages, although some argue that their writings about animals and plants cannot be correctly classified as natural history. Notable early parson-naturalists were William Turner (1508–1568), John Ray (1627–1705), William Derham (1657–1735).\n\nWilliam Bertram, in 1773, embarked on a four year journey through eight southern American colonies. Bartram made many drawings and took notes on the native flora and fauna, and the native American Indians. In 1774, he explored the St. Johns River. William Bartram wrote of his experiences exploring the Southeast in his book known today as \"Bartram's Travels\", published in 1791. Ephraim George Squier and Edwin Hamilton Davis, in their book, \"Ancient Monuments of the Mississippi Valley\", name Bartram as \"the first naturalist who penetrated the dense tropical forests of Florida.\"\n\nAfter Gilbert White and William Bertram, other significant writers include American ornithologist John James Audubon (1785 – 1851), Charles Darwin( (1809 – 1882), Richard Jefferies (1848 – 1887), Susan Fenimore Cooper (1813 – 1894), mother of American nature writting, and Henry David Thoreau (1817 – 1862), who is often considered the father of modern American nature writing, Ralph Waldo Emerson (1803 – 1882) John Burroughs, John Muir, Aldo Leopold, Rachel Carson, M. Krishnan, and Edward Abbey (although he rejected the term for himself).\n\nAnother important early work is \"A History of British Birds\" by Thomas Bewick, published in two volumes. Volume 1, \"Land Birds\", appeared in 1797. Volume 2, \"Water Birds\", appeared in 1804. The book was effectively the first \"field guide\" for non-specialists. Bewick provides an accurate illustration of each species, from life if possible, or from skins. The common and scientific name(s) are listed, citing the naming authorities. The bird is described, with its distribution and behaviour, often with extensive quotations from printed sources or correspondents. Critics note Bewick's skill as a naturalist as well as an engraver.\n\nSome important contemporary figures in Britain include Richard Mabey, Roger Deakin, Mark Cocker, and Oliver Rackham. Rackham's books included \"Ancient Woodland\" (1980) and \"The History of the Countryside\" (1986). Richard Maybey has been involved with radio and television programmes on nature, and his book \"Nature Cure\", describes his experiences and recovery from depression in the context of man’s relationship with landscape and nature. He has also edited and introduced editions of Richard Jefferies, Gilbert White, Flora Thompson and Peter Matthiessen. Mark Crocker has written extensively for British newspapers and magazines and his books include Birds Britannica (with Richard Mabey) (2005). and \"Crow Country\" (2007). He frequently writes about modern responses to the wild, whether found in landscape, human societies or in other species. Roger Deakin was an English writer, documentary-maker and environmentalist. In 1999, Deakin's acclaimed book \"Waterlog\" was published. Inspired in part by the short story \"The Swimmer\" by John Cheever, it describes his experiences of 'wild swimming' in Britain's rivers and lakes and advocates open access to the countryside and waterways. Deakin's book \"Wildwood\" appeared posthumously in 2007. It describes a series of journeys across the globe that Deakin made to meet people whose lives are intimately connected to trees and wood.\n\nIn 2017 the German book publishing company Matthes & Seitz Berlin started to grant the German Award for Nature Writing, an annual literary award for writers in German language that excellently fulfil the criteria of the literary genre. It comes with a price money of 10.000 Euro and additionallly an artist in residency grant of six weeks at the International Academy for Nature Conservation of Germany on the German island Vilm. The British Council in 2018 is offering an education bursary and workshops to six young German authors dedicated to Nature writing.\n\n\n\n"}
{"id": "51506837", "url": "https://en.wikipedia.org/wiki?curid=51506837", "title": "Outline of Earth", "text": "Outline of Earth\n\nThe following outline is provided as an overview of and topical guide to the planet Earth:\n\nEarth – third planet from the Sun, the densest planet in the Solar System, the largest of the Solar System's four terrestrial planets, and the only astronomical object known to harbor life.\n\n\nEarth's location in the Universe\n\n\n\n\n\nThis sphere represents all water on Earth, wherever it is and in whatever form within the water cycle.\n\n\n\n\n\n\n\nHistory of Earth\n\nFuture of Earth\n\n"}
{"id": "44862806", "url": "https://en.wikipedia.org/wiki?curid=44862806", "title": "Outline of evolution", "text": "Outline of evolution\n\nThe following outline is provided as an overview of and topical guide to evolution:\n\nEvolution – change in heritable traits of biological organisms over generations due to natural selection, mutation, gene flow, and genetic drift. Also known as descent with modification. Over time these evolutionary processes lead to formation of new species (speciation), changes within lineages (anagenesis), and loss of species (extinction). \"Evolution\" is also another name for evolutionary biology, the subfield of biology concerned with studying evolutionary processes that produced the diversity of life on Earth.\n\n\n\n\n\n\"See also Basic principles (above)\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49516442", "url": "https://en.wikipedia.org/wiki?curid=49516442", "title": "Phylogenetic inertia", "text": "Phylogenetic inertia\n\nPhylogenetic inertia or phylogenetic constraint refers to the limitations on the future evolutionary pathways that have been imposed by previous adaptations.\n\nCharles Darwin first recognized this phenomenon, though the term was later coined by Huber in 1939. Darwin explained the idea of phylogenetic inertia based on his observations; he spoke about it when explaining the \"Law of Conditions of Existence\". Darwin also suggested that, after speciation, the organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa. This is the main concept of phylogenetic inertia.\n\nRichard Dawkins also explained these constraints by likening natural selection to a river in his 1982 book \"The Extended Phenotype\".\n\n\nBirds are the only speciose group of vertebrates that are exclusively oviparous, or egg laying. It has been suggested that birds are phylogenetically constrained, as being derived from reptiles, and likely have not overcome this constraint or diverged far enough away to develop viviparity, or live birth.\n\n\n\nThere have been several studies that have been able to effectively test for phylogenetic inertia when looking into shared traits; predominantly with a comparative methods approach. Some have used comparative methods and found evidence for certain traits attributed to adaptation, and some to phylogeny; there were also numerous traits that could be attributed to both. Another study developed a new method of comparative examination that showed to be a powerful predictor of phylogenetic inertia in a variety of situations. It was called Phylogenetic Eigenvector Regression (PVR), which runs principal component analyses between species on a pairwise phylogenetic distance matrix. In another, different study, the authors described methods for measuring phylogenetic inertia, looked at effectiveness of various comparative methods, and found that different methods can reveal different aspects of drivers. Autoregression and PVR showed good results with morphological traits.\n"}
{"id": "36104129", "url": "https://en.wikipedia.org/wiki?curid=36104129", "title": "Playa de Gulpiyuri", "text": "Playa de Gulpiyuri\n\nPlaya de Gulpiyuri is a flooded sinkhole with an inland beach located near Llanes, in Asturias Northern Spain, around 100 m from the Cantabrian Sea. Roughly 40 meters in length, it is fully tidal due to a series of underground tunnels carved by the salt water of the Cantabrian Sea which allows water from the Bay of Biscay to create small waves. \n\nIt is a popular tourist destination, natural monument, and part of Spain's Regional Network of Protected Natural Areas.\n"}
{"id": "41751262", "url": "https://en.wikipedia.org/wiki?curid=41751262", "title": "Prodromus", "text": "Prodromus\n\nA prodromus ('forerunner' or 'precursor') aka prodrome is a term used in the natural sciences to describe a preliminary publication intended as the basis for a later, more comprehensive work.\n\nIt is also a medical term used for a premonitory symptom, that is, a symptom indicating the onset of a disease.\n\nThe origin of the word is from the 19th century: via French from New Latin prodromus, from Greek prodromos forerunner.\n\nNotable prodromi were \"Prodromus Entomology\", \"Prodromus Florae Novae Hollandiae et Insulae Van Diemen\", \"Prodromus Systematis Naturalis Regni Vegetabilis\" and Nicolas Steno's \"De solido intra solidum naturaliter contento dissertationis prodromus\", one of the early treatises attempting to explain the occurrence of fossils in solid rock.\n"}
{"id": "3090379", "url": "https://en.wikipedia.org/wiki?curid=3090379", "title": "Rubble", "text": "Rubble\n\nRubble is broken stone, of irregular size, shape and texture; undressed especially as a filling-in. Rubble naturally found in the soil is known also as 'brash' (compare cornbrash). Where present, it becomes more noticeable when the land is ploughed or worked.\n\n\"Rubble-work\" is a name applied to several types of masonry. One kind, where the stones are loosely thrown together in a wall between boards and grouted with mortar almost like concrete, is called in Italian \"muraglia di getto\" and in French \"bocage\". In Pakistan, walls made of rubble and concrete, cast in a formwork, are called 'situ', which probably derives from Sanskrit (similar to the Latin 'in situ' meaning 'made on the spot').\n\nWork executed with more or less large stones put together without any attempt at courses is called rubble walling. Where similar work is laid in courses, it is known as coursed rubble. Dry-stone walling is somewhat similar work done without the use of mortar. It is bound together by the fit of the stones and the regular placement of stones which extend through the thickness of the wall. A rubble wall built with mortar will be stronger if assembled in this way.\n\nRubble walls () are found all over the island of Malta. Similar walls are also frequently found in Sicily and the Arab countries. The various shapes and sizes of the stones used to build these walls look like stones that were found in the area lying on the ground or in the soil. It is most probable that the practice of building these walls around the field was inspired by the Arabs during their rule in Malta, as in Sicily who were also ruled by the Arabs around the same period. The Maltese farmer found that the technique of these walls was very useful especially during an era where resources were limited. Rubble walls are used to serve as borders between the property of one farm from the other. A great advantage that rubble walls offered is that when heavy rain falls, their structure would allow excessive water to pass through and therefore, excess water will not ruin the products. Soil erosion is minimised as the wall structure allows the water to pass through but it traps the soil and prevents it from being carried away from the field. One can see many rubble walls on the side of the hills and in valleys where the land slopes down and consequently the soil is in greater danger of being carried away.\n\n\n"}
{"id": "58206657", "url": "https://en.wikipedia.org/wiki?curid=58206657", "title": "Self-sealing suction cup", "text": "Self-sealing suction cup\n\nThe self-sealing suction cup is a suction cup that exerts a suction force only when it is in physical contact with an object. Unlike most other suction cups, it does not exert any suction force when it is not in contact with an object. Its grasping ability is achieved entirely through passive means without the use of sensors, valves, or actuators.\n\nIt was designed so that, when used as part of a suction cup array, the suction cups that don’t come in contact with the object remain sealed. By having only the suction cups that are in direct contact of the object to exhibit suction force, the researchers were able to minimize leak points where air could enter and increase the pressure that each active cup receives, maximizing the suction force. As a result, an array of self-sealing suction cups can grasp and pick up a wide range of object sizes and shapes. This comes in contrast to conventional suction cups that are typically designed for one specific object size and geometry. In addition, suction cups of various sizes have been manufactured, ranging from the palm of a hand to the point of a fingertip.\n\nThe self-sealing suction cup was first developed in 2010 by a collaboration of researchers from the U.S. Army Research Laboratory (ARL), the Edgewood Chemical Biological Center at Aberdeen Proving Ground, and the University of Maryland.\n\nThe design of the self-sealing suction cup was initially inspired by the suckers of the octopus and its ability to pick up different sized items by individually actuating its suction cups based on the item’s size and physical features.\n\nThe internal geometry of the self-sealing suction cup was designed to the smallest possible size and features a minimum wall thickness of 1.02 mm, a tube diameter of 1.59 mm, and minimum part spacing of 0.13 mm. The suction cup incorporates a mix of rubber and plastic components, where the cup lip, base, tube, springs, and plug are made out of soft rubber while the cup side, collar, hinges, and flange are made out of plastic. As part of its design, a central vacuum pump can be used to maximize the suction force of the suction cup. A multi-material 3D printer was used to create the prototype of the self-sealing suction cup in about 20 minutes.\n\nInside the self-sealing suction cup, the plug is positioned close to the tube opening so that it can get sucked into the tube seal the hole when the central suction line is powered. A pair of springs connected to the suction cup’s base helps maintain the plug’s position, restoring the plug seal in the absence of object forces. If the cup makes contact with an object, a hinge action raises the plug away from the suction tube. The moment the cup’s lips are pushed against the object, the passive reaction forces from the cup lips are transferred to the rubber base of the cup, which stretches over the collar and allow the structure to compress. Acting as a pivot for the hinges, the collar causes the hinges to rotate and the edges of the hinges slide along the underside of the flange and raise the plug away from the suction tube opening. As a result, the suction cup self-seals when not in contact with an object and self-opens the cup’s lips makes contacts with an object.\n\nIn 2015, several improvements were made to the design of the self-sealing suction cup to improve its grasping capabilities. The previous design demonstrated the following flaws:\n\n\nTo address these flaws, researchers from ARL decreased the number of components by consolidating the functions of several parts, which reduced the uncompressed height of the suction cup by almost 50% to 0.72 cm. The cup diameter was also reduced to 1.07 cm. A lever system was added to the base of the cup, which pivots the collar to lift the plug. In addition, the tube doubles as a spring, which helps restore the levers and the plug to their closed position. A plastic restraint was added around the cup to aid with handling the hyper-extension, shear, and torsional forces.\n\nThe self-sealing suction cup has been subjected to a series of tests to determine the quality of its performance. A flexible test rig with four dime-sized suction cups and plastic ribs connected with rubber tubes was created for force-displacement and testing.\n\nA force-displacement test that compared the performance between the self-sealing suction cup, an identical suction cup, and a commercially available suction cup found that the internal structures of the self-sealing cup allowed more force to be exerted for the same displacement compared to the other cups. However, under identical conditions, the self-sealing cup achieved a maximum force of 12.5 N while the commercially available cup achieved a maximum force of 12.9 N.\n\nA seal quality test measured the pressure generated from each self-sealing suction cup. The results showed that an array of four cups maintained a pressure of 93.8% atmospheric. The test also demonstrated that not all the cups were equally efficient at sealing after object contact. However, this could be the result of variation in the cups’ prior usage.\n\nDuring object grasping testing where the grasping range was examined, the test rig successfully grasped about 80% of the objects attempted. These items consisted of the following: TV remote, pill bottle, glue stick, eyeglasses, fork, disposable bottle, toothpaste, coffee mug, bowl, plate, book, cell phone, bar of soap, paper money, mail, keys, show, table knife, medicine box, credit card, coin, pillow, hairbrush, non-disposable bottle, wallet, magazine, soda can, newspaper, scissors, wrist watch, purse, lighter, compact disc, telephone receiver, full wine bottle, full wine glass, light bulb, lock, padded volleyball, wooden block. (4) As a demonstration of the cups’ strength, the ARL researchers were able to pick up a full bottle of wine using only four of the dime-sized suction cups.\n\nThe self-sealing suction cups have been incorporated in robots to improve their passive grasping capabilities. Due to the design of the suction cups, a central vacuum source can be used to effectively generate suction force from the cups and reduce the number of actuators and sensors for the robot.\n\nResearchers from ARL designed and developed a three-finger hand actuator system using a 3D printer in order for the robot to properly utilize the self-sealing suction cups. Four suction cups run along the bottom of each finger, which contains a narrow vacuum channel running through the center. A central vacuum pump serves to power the suction cups and facilitate grasping. The fingers can also curl around the object to better grasp it and release any object in its hold by feeding back the output of the vacuum pump and emitting a burst of positive pressure.\n\nThe three-finger hand has been used by aerial systems and has demonstrated considerable success in grasping objects on the ground while maintaining flight. According to ARL researchers, the self-sealing suction cups may exhibit higher rates of success underwater due to the extra pressure from the sea depths surrounding and pressing against the object and grasper. However, they noted that an underwater environment would require different manufacturing materials that would allow the suction cups to perform well in salt water, such as a thermal plastic.\n"}
{"id": "4968799", "url": "https://en.wikipedia.org/wiki?curid=4968799", "title": "Sky brightness", "text": "Sky brightness\n\nSky brightness refers to the visual perception of the sky and how it scatters and diffuses light. The fact that the sky is not completely dark at night is easily visible. If light sources (e.g. the Moon and light pollution) were removed from the night sky, it would appear absolutely dark. Silhouettes of objects against the sky itself would not be visible.\n\nThe sky's brightness varies greatly over the day, and the primary cause differs as well. During daytime, when the Sun is above the horizon, the direct scattering of sunlight is the overwhelmingly dominant source of light. During twilight (the duration after sunset or before sunrise until or since, respectively, the full darkness of night), the situation is more complicated, and a further differentiation is required.\n\nTwilight (both dusk and dawn) is divided into three 6° segments that mark the Sun's position below the horizon. At civil twilight, the center of the Sun's disk appears to be between 1/4° and 6° below the horizon. At nautical twilight, the Sun's altitude is between –6° and –12°. At astronomical twilight, the Sun is between –12° and –18°. When the Sun's depth is more than 18°, the sky generally attains its maximum darkness.\n\nSources of the night sky's intrinsic brightness include airglow, indirect scattering of sunlight, scattering of starlight, and light pollution.\n\nWhen physicist Anders Ångström examined the spectrum of the aurora borealis, he discovered that even on nights when the aurora was absent, its characteristic green line was still present. It was not until the 1920s that scientists were beginning to identify and understand the emission lines in aurorae and of the sky itself, and what was causing them. The green line Angstrom observed is in fact an emission line with a wavelength of 557.7 nm, caused by the recombination of oxygen in the upper atmosphere.\n\nAirglow is the collective name of the various processes in the upper atmosphere that result in the emission of photons, with the driving force being primarily UV radiation from the Sun. Several emission lines are dominant: a green line from oxygen at 557.7 nm, a yellow doublet from sodium at 589.0 and 589.6 nm, and red lines from oxygen at 630.0 and 636.4 nm.\n\nThe sodium emissions come from a thin sodium layer approximately 10 km thick at an altitude of 90–100 km, above the mesopause and in the D-layer of the ionosphere. The red oxygen lines originate at altitudes of about 300 km, in the F-layer. The green oxygen emissions are more spatially distributed. How sodium gets to mesospheric heights is not yet well understood, but it is believed to be a combination of upward transport of sea salt and meteoritic dust.\n\nIn daytime, sodium and red oxygen emissions are dominant and roughly 1,000 times as bright as nighttime emissions because in daytime, the upper atmosphere is fully exposed to solar UV radiation. The effect is however not noticeable to the human eye, since the glare of directly scattered sunlight outshines and obscures it.\n\nIndirectly scattered sunlight comes from two directions. From the atmosphere itself, and from outer space. In the first case, the sun has just set but still illuminates the upper atmosphere directly. Because the amount of scattered sunlight is proportional to the number of scatterers (i.e. air molecules) in the line of sight, the intensity of this light decreases rapidly as the sun drops further below the horizon and illuminates less of the atmosphere.\n\nWhen the sun's altitude is < -6° 99% of the atmosphere in zenith is in the Earth's shadow and second order scattering takes over. At the horizon, however, 35% of the atmosphere along the line of sight is still directly illuminated, and continues to be until the sun reaches -12°. From -12° to -18° only the uppermost parts of the atmosphere along the horizon, directly above the spot where the sun is, is still illuminated. After that, all direct illumination ceases and astronomical darkness sets in.\n\nA second source sunlight is the zodiacal light, which is caused by reflection and scattering of sunlight on interplanetary dust. Zodiacal light varies quite a lot in intensity depending on the position of the earth, location of the observer, time of year, and composition and distribution of the reflecting dust.\n\nNot only sunlight is scattered by the molecules in the air. Starlight and the diffuse light of the milky way are also scattered by the air, and it is found that stars up to V magnitude 16 contribute to the diffuse scattered starlight.\n\nOther sources such as galaxies and nebulae don't contribute significantly.\n\nThe total brightness of all the stars was first measured by Burns in 1899, with a calculated result that the total brightness reaching earth was equivalent to that of 2,000 first-magnitude stars with subsequent measurements by others.\n\nLight pollution is an ever-increasing source of sky brightness in urbanized areas. In densely populated areas that do not have stringent light pollution control, the entire night sky is regularly 5 to 50 times brighter than it would be if all lights were switched off, and very often the influence of light pollution is far greater than natural sources (including moonlight). With urbanization and light pollution, one third of humanity, and the majority of those in developed countries, cannot see the Milky Way.\n\nWhen the sun has just set, the brightness of the sky decreases rapidly, thereby enabling us to see the airglow that is caused from such high altitudes that they are still fully sunlit until the sun drops more than about 12° below the horizon. During this time, yellow emissions from the sodium layer and red emissions from the 630 nm oxygen lines are dominant, and contribute to the purplish color sometimes seen during civil and nautical twilight.\n\nAfter the sun has also set for these altitudes at the end of nautical twilight, the intensity of light emanating from earlier mentioned lines decreases, until the oxygen-green remains as the dominant source.\n\nWhen astronomical darkness has set in, the green 557.7 nm oxygen line is dominant, and atmospheric scattering of starlight occurs.\n\nDifferential refraction causes different parts of the spectrum to dominate, producing a golden hour and a blue hour.\n\nThe following table gives the relative and absolute contributions to night sky brightness at zenith on a perfectly dark night at middle latitudes without moonlight and in the absence of any light pollution.\n\nThe total sky brightness in zenith is therefore ~220 S or 21.9 mag/arcsec² in the V-band. Note that the contributions from Airglow and Zodiacal light vary with the time of year, the solar cycle, and the observer's latitude roughly as follows:\n\nwhere \"S\" is the solar 10.7 cm flux in MJy, and various sinusoidally between 0.8 and 2.0 with the 11-year solar cycle, yielding an upper contribution of ~270 S at solar maximum.\n\nThe intensity of zodiacal light depends on the ecliptic latitude and longitude of the point in the sky being observed relative to that of the sun. At ecliptic longitudes differing from the sun's by > 90 degrees, the relation is \nwhere \"β\" is the ecliptic latitude and is smaller than 60°, when larger than 60 degrees the contribution is that given in the table. Along the ecliptic plane there are enhancements in the zodiacal light where it is much brighter near the sun and with a secondary maximum opposite the sun at 180 degrees longitude (the gegenschein).\n\nIn extreme cases natural zenith sky brightness can be as high as ~21.0 mag/arcsec², roughly twice as bright as nominal conditions.\n\n"}
{"id": "23083720", "url": "https://en.wikipedia.org/wiki?curid=23083720", "title": "The Evolution of God", "text": "The Evolution of God\n\nThe Evolution of God is a 2009 book by Robert Wright, in which the author explores the history of the concept of God in the three Abrahamic religions through a variety of means, including archeology, history, theology, and evolutionary psychology. The patterns which link Judaism, Christianity, and Islam and the ways in which they have changed their concepts over time are explored as one of the central themes.\n\nOne of the conclusions of the book that Wright tries to make is a reconciliation between science and religion. He also speculates on the future of the concept of God.\n\nAmong other things, Wright discusses the role of evolutionary biology in the development of religion. Geneticist Dean Hamer hypothesized that some people have a specific gene that makes them prone to religious belief, which he calls the God gene, and that over time natural selection has favored these people because their spirituality leads to optimism. Wright, however, thinks the tendency towards religious belief is not an adaptive trait influenced by natural selection, but rather a spandrel - a trait that happens to be supported by adaptations originally selected for other purposes. Wright states that the human brain approaches religious belief based on how it adapted to survive and reproduce in early hunter-gatherer societies.\n\nHe points out four key traits of religion that align with the human brain's survival adaptations:\nHumans have adapted to pay attention to surprising and confusing information, because it could make the difference between life and death. (For instance, if a person left the campsite and mysteriously never returned, it would be wise for the others to be on guard for a predator or some other danger.) Understanding and controlling cause and effect also takes top priority in the human brain, since humans live in complex social groups where predicting and influencing the actions and thoughts of others gains them allies, status, and access to resources. As human cognitive abilities and curiosity expanded over the centuries, their investigation of cause and effect expanded from the strictly social context out into the world at large, opening the doors for religions to explain things like weather and disease.\n\nThough some of these explanations were strange and perhaps dubious, the fact that they could not be completely disproven lent them credibility; it was better to be cautious than dead. Wright uses an example from the Haida people, indigenous to the northwest coast of North America, who would try to appease killer whale deities to calm storms out at sea; they would pour fresh water into the ocean or tie tobacco or deer tallow to the end of a paddle. While some people certainly died despite these offerings, those who survived were a testament to the ritual's possible efficacy.\n\nMysterious and unproven beliefs can also persist in a culture because human brains have adapted to agree with the group consensus even if it goes against one's better judgment or personal beliefs, since a person alienated from the group loses protection, food, and mates. Wright cites the Asch conformity experiments and even posits that Stockholm syndrome is not so much a syndrome as a natural product of evolution, the brain's way of ensuring that a person accepts and is accepted by his or her new social group. In addition, beliefs can persist because once a person publicly announces a belief, social psychologists have found that he or she is inclined to focus on evidence supporting that belief while conveniently ignoring evidence contradicting it, a logical fallacy known as cherry picking.\n\nJournalist and political commentator Andrew Sullivan gave the book a positive review in \"The Atlantic\", saying that the book \"...gave me hope that we can avoid both the barrenness of a world without God and the horrible fusion of fundamentalism and weapons of mass destruction.\" \n\n\"Newsweek\" religion editor, Lisa Miller, described \"The Evolution of God\" as a reframing of the faith vs. reason debate. Drawing a contrast to such authors as Sam Harris, Richard Dawkins and Christopher Hitchens, Miller gives an overall positive review of the book's approach to the examination of the concept of God.\n\nIn a review for \"The New York Times\", Yale professor of psychology Paul Bloom said, \"In his brilliant new book, “The Evolution of God,” Robert Wright tells the story of how God grew up.\" Bloom sums up Wright's controversial stance as, \"Wright’s tone is reasoned and careful, even hesitant, throughout, and it is nice to read about issues like the morality of Christ and the meaning of jihad without getting the feeling that you are being shouted at. His views, though, are provocative and controversial. There is something here to annoy almost everyone.\"\n\nHowever, in a \"New York Times\" review that included a reply from Wright, Nicholas Wade, a writer for the \"Science Times\" section, notes the book is \"a disappointment from the Darwinian perspective\", because evolution \"provides a simpler explanation for moral progression than the deity Wright half invokes.\" Wright replied to Wade's comments, saying Wade had misunderstood Wright's argument and that \"The deity (if there is one–and I’m agnostic on that point) would be realizing moral progress through evolution’s creation of the human moral sense (and through the subsequent development of that moral sense via cultural evolution, particularly technological evolution).\" Wade replied that \"evolution seems to me a sufficient explanation for the moral progress that Mr. Wright correctly discerns in the human condition, so there seemed no compelling need to invoke a deity.\"\n\nTo promote the book, Wright did a variety of interviews, including with the \"New York Times\", \"Publishers Weekly\", and \"Bill Moyers Journal\".\nHe also did a series of videos on Bloggingheads.tv, a website he co-founded with Mickey Kaus. Wright also appeared on \"The Colbert Report\" on August 18, 2009.\n\n\n"}
{"id": "31596828", "url": "https://en.wikipedia.org/wiki?curid=31596828", "title": "Unequal crossing over", "text": "Unequal crossing over\n\nUnequal crossing over is a type of gene duplication or deletion event that deletes a sequence in one strand and replaces it with a duplication from its sister chromatid in mitosis or from its homologous chromosome during meiosis. It is a type of chromosomal crossover between homologous sequences that are not paired precisely. Normally genes are responsible for occurrence of crossing over. It exchanges sequences of different links between chromosomes. Along with gene conversion, it is believed to be the main driver for the generation of gene duplications and is a source of mutation in the genome.\n\nDuring meiosis, the duplicated chromosomes (chromatids) in eukaryotic organisms are attached to each other in the centromere region and are thus paired. The maternal and paternal chromosomes then align alongside each other. During this time, recombination can take place via crossing over of sections of the paternal and maternal chromatids and leads to reciprocal recombination or non-reciprocal recombination. Unequal crossing over requires a measure of similarity between the sequences for misalignment to occur. The more similarity within the sequences, the more likely unequal crossing over will occur. One of the sequences is thus lost and replaced with the duplication of another sequence.\n\nWhen two sequences are misaligned, unequal crossing over may create a tandem repeat on one chromosome and a deletion on the other. The rate of unequal crossing over will increase with the number of repeated sequences around the duplication. This is because these repeated sequences will pair together, allowing for the mismatch in the cross over point to occur.\n\nUnequal crossing over is the process most responsible for creating regional gene duplications in the genome. Repeated rounds of unequal crossing over cause the homogenization of the two sequences. With the increase in the duplicates, unequal crossing over can lead to dosage imbalance in the genome and can be highly deleterious.\n\nIn unequal crossing over, there can be large sequence exchanges between the chromosomes. Compared with gene conversion, which can only transfer a maximum of 1,500 base pairs, unequal crossing over in yeast rDNA genes has been found to transfer about 20,000 base pairs in a single crossover event Unequal crossover can be followed by the concerted evolution of duplicated sequences.\n\nIt has been suggested that longer intron found between two beta-globin genes are a response to deleterious selection from unequal crossing over in the beta-globin genes. Comparisons between alpha-globin, which does not have long introns, and beta-globin genes show that alpha-globin have 50 times higher concerted evolution.\n\nWhen unequal crossing over creates a gene duplication, the duplicate has 4 evolutionary fates. This is due to the fact that purifying selection acting on a duplicated copy is not very strong. Now that there is a redundant copy, neutral mutations can act on the duplicate. Most commonly the neutral mutations will continue until the duplicate becomes a pseudogene. If the duplicate copy increases the dosage effect of the gene product, then the duplicate may be retained as a redundant copy. Neofunctionalization is also a possibility: the duplicated copy acquires a mutation that gives it a different function than its ancestor. If both copies acquire mutations, it is possible that a subfunctional event occurs. This happens when both of the duplicated sequences have a more specialized function than the ancestral copy\n\nGene duplications are the main reason for the increase of genome size, and as unequal crossing over is the main mechanism for gene duplication, unequal crossing over contributes to genome size evolution is the most common regional duplication event that increases the size of the genome.\n\nWhen viewing the genome of a eukaryote, a striking observation is the large amount of tandem, repetitive DNA sequences that make up a large portion of the genome. For example, over 50% of the \"Dipodmys ordii\" genome is made up of three specific repeats. \"Drosophila virilis\" has three sequences that make up 40% of the genome, and 35% of the \"Absidia glauca\" is repetitive DNA sequences. These short sequences have no selection pressure acting on them and the frequency of the repeats can be changed by unequal crossing over.\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "32496", "url": "https://en.wikipedia.org/wiki?curid=32496", "title": "Vacuum tube", "text": "Vacuum tube\n\nIn electronics, a vacuum tube, an electron tube, or valve (British usage) or, colloquially, a tube (North America), is a device that controls electric current flow in a high vacuum between electrodes to which an electric potential difference has been applied.\n\nThe type known as a thermionic tube or thermionic valve uses the phenomenon of thermionic emission of electrons from a heated cathode and is used for a number of fundamental electronic functions such as signal amplification and current rectification.\nNon-thermionic types, such as a vacuum phototube however, achieve electron emission through the photoelectric effect, and are used for such as the detection of light levels. In both types, the electrons are accelerated from the cathode to the anode by the electric field in the tube.\n\nThe simplest vacuum tube, the diode invented in 1904 by John Ambrose Fleming, contains only a heated electron-emitting cathode and an anode. Current can only flow in one direction through the device from the cathode to the anode. Adding one or more control grids within the tube allows the current between the cathode and anode to be controlled by the voltage on the grid or grids. These devices became a key component of electronic circuits for the first half of the twentieth century. They were crucial to the development of radio, television, radar, sound recording and reproduction, long distance telephone networks, and analogue and early digital computers. Although some applications had used earlier technologies such as the spark gap transmitter for radio or mechanical computers for computing, it was the invention of the thermionic vacuum tube that made these technologies widespread and practical, and created the discipline of electronics.\n\nIn the 1940s the invention of semiconductor devices made it possible to produce solid-state devices, which are smaller, more efficient, reliable and durable, and cheaper than thermionic tubes. From the mid-1960s, thermionic tubes were then being replaced by the transistor. However, the cathode-ray tube (CRT) remained the basis for television monitors and oscilloscopes until the early 21st century. Thermionic tubes still have some applications, such as the magnetron used in microwave ovens, and certain high-frequency amplifiers.\n\nNot all electronic circuit valves/electron tubes are vacuum tubes. Gas-filled tubes are similar devices, but containing a gas, typically at low pressure, which exploit phenomena related to electric discharge in gases, usually without a heater.\n\nOne classification of thermionic vacuum tubes is by the number of active electrodes. A device with two active elements is a diode, usually used for rectification. Devices with three elements are triodes used for amplification and switching. Additional electrodes create tetrodes, pentodes, and so forth, which have multiple additional functions made possible by the additional controllable electrodes.\n\nOther classifications are:\n\nTubes have different functions, such as cathode ray tubes which create a beam of electrons for display purposes (such as the television picture tube) in addition to more specialized functions such as electron microscopy and electron beam lithography. X-ray tubes are also vacuum tubes. Phototubes and photomultipliers rely on electron flow through a vacuum, though in those cases electron emission from the cathode depends on energy from photons rather than thermionic emission. Since these sorts of \"vacuum tubes\" have functions other than electronic amplification and rectification they are described in their own articles.\n\nA vacuum tube consists of two or more electrodes in a vacuum inside an airtight envelope. Most tubes have glass envelopes with a glass-to-metal seal based on kovar sealable borosilicate glasses, though ceramic and metal envelopes (atop insulating bases) have been used. The electrodes are attached to leads which pass through the envelope via an airtight seal. Most vacuum tubes have a limited lifetime, due to the filament or heater burning out or other failure modes, so they are made as replaceable units; the electrode leads connect to pins on the tube's base which plug into a tube socket. Tubes were a frequent cause of failure in electronic equipment, and consumers were expected to be able to replace tubes themselves. In addition to the base terminals, some tubes had an electrode terminating at a top cap. The principal reason for doing this was to avoid leakage resistance through the tube base, particularly for the high impedance grid input. The bases were commonly made with phenolic insulation which performs poorly as an insulator in humid conditions. Other reasons for using a top cap include improving stability by reducing grid-to-anode capacitance, improved high-frequency performance, keeping a very high plate voltage away from lower voltages, and accommodating one more electrode than allowed by the base. There was even an occasional design that had two top cap connections.\n\nThe earliest vacuum tubes evolved from incandescent light bulbs, containing a filament sealed in an evacuated glass envelope. When hot, the filament releases electrons into the vacuum, a process called thermionic emission, originally known as the \"Edison Effect\". A second electrode, the anode or \"plate\", will attract those electrons if it is at a more positive voltage. The result is a net flow of electrons from the filament to plate. However, electrons cannot flow in the reverse direction because the plate is not heated and does not emit electrons. The filament (\"cathode\") has a dual function: it emits electrons when heated; and, together with the plate, it creates an electric field due to the potential difference between them. Such a tube with only two electrodes is termed a diode, and is used for rectification. Since current can only pass in one direction, such a diode (or \"rectifier\") will convert alternating current (AC) to pulsating DC. Diodes can therefore be used in a DC power supply, as a demodulator of amplitude modulated (AM) radio signals and for similar functions.\n\nEarly tubes used the filament as the cathode; this is called a \"directly heated\" tube. Most modern tubes are \"indirectly heated\" by a \"heater\" element inside a metal tube that is the cathode. The heater is electrically isolated from the surrounding cathode and simply serves to heat the cathode sufficiently for thermionic emission of electrons. The electrical isolation allows all the tubes' heaters to be supplied from a common circuit (which can be AC without inducing hum) while allowing the cathodes in different tubes to operate at different voltages. H. J. Round invented the indirectly heated tube around 1913.\n\nThe filaments require constant and often considerable power, even when amplifying signals at the microwatt level. Power is also dissipated when the electrons from the cathode slam into the anode (plate) and heat it; this can occur even in an idle amplifier due to quiescent currents necessary to ensure linearity and low distortion. In a power amplifier, this heating can be considerable and can destroy the tube if driven beyond its safe limits. Since the tube contains a vacuum, the anodes in most small and medium power tubes are cooled by radiation through the glass envelope. In some special high power applications, the anode forms part of the vacuum envelope to conduct heat to an external heat sink, usually cooled by a blower, or water-jacket.\n\nKlystrons and magnetrons often operate their anodes (called collectors in klystrons) at ground potential to facilitate cooling, particularly with water, without high-voltage insulation. These tubes instead operate with high negative voltages on the filament and cathode.\n\nExcept for diodes, additional electrodes are positioned between the cathode and the plate (anode). These electrodes are referred to as grids as they are not solid electrodes but sparse elements through which electrons can pass on their way to the plate. The vacuum tube is then known as a triode, tetrode, pentode, etc., depending on the number of grids. A triode has three electrodes: the anode, cathode, and one grid, and so on. The first grid, known as the control grid, (and sometimes other grids) transforms the diode into a \"voltage-controlled device\": the voltage applied to the control grid affects the current between the cathode and the plate. When held negative with respect to the cathode, the control grid creates an electric field which repels electrons emitted by the cathode, thus reducing or even stopping the current between cathode and anode. As long as the control grid is negative relative to the cathode, essentially no current flows into it, yet a change of several volts on the control grid is sufficient to make a large difference in the plate current, possibly changing the output by hundreds of volts (depending on the circuit). The solid-state device which operates most like the pentode tube is the junction field-effect transistor (JFET), although vacuum tubes typically operate at over a hundred volts, unlike most semiconductors in most applications.\n\nThe 19th century saw increasing research with evacuated tubes, such as the Geissler and Crookes tubes. The many scientists and inventors who experimented with such tubes include Thomas Edison, Eugen Goldstein, Nikola Tesla, and Johann Wilhelm Hittorf. With the exception of early light bulbs, such tubes were only used in scientific research or as novelties. The groundwork laid by these scientists and inventors, however, was critical to the development of subsequent vacuum tube technology.\n\nAlthough thermionic emission was originally reported in 1873 by Frederick Guthrie, it was Thomas Edison's apparently independent discovery of the phenomenon in 1883 that became well known. Although Edison was aware of the unidirectional property of current flow between the filament and the anode, his interest (and patent) concentrated on the sensitivity of the anode current to the current through the filament (and thus filament temperature). Little practical use was ever made of this property (however early radios often implemented volume controls through varying the filament current of amplifying tubes). It was only years later that John Ambrose Fleming utilized the rectifying property of the diode tube to detect (demodulate) radio signals, a substantial improvement on the early cat's-whisker detector already used for rectification.\n\nHowever actual amplification by a vacuum tube only became practical with Lee De Forest's 1907 invention of the three-terminal \"audion\" tube, a crude form of what was to become the triode. Being essentially the first electronic amplifier, such tubes were instrumental in long-distance telephony (such as the first coast-to-coast telephone line in the US) and public address systems, and introduced a far superior and versatile technology for use in radio transmitters and receivers. The electronics revolution of the 20th century arguably began with the invention of the triode vacuum tube.\n\nThe English physicist John Ambrose Fleming worked as an engineering consultant for firms including Edison Swan, Edison Telephone and the Marconi Company. In 1904, as a result of experiments conducted on Edison effect bulbs imported from the United States, he developed a device he called an \"oscillation valve\" (because it passes current in only one direction). The heated filament, was capable of thermionic emission of electrons that would flow to the \"plate\" (or \"anode\") when it was at a positive voltage with respect to the heated cathode. Electrons, however, could not pass in the reverse direction because the plate was not heated and thus not capable of thermionic emission of electrons.\n\nLater known as the Fleming valve, it could be used as a rectifier of alternating current and as a radio wave detector. This greatly improved the crystal set which rectified the radio signal using an early solid-state diode based on a crystal and a so-called cat's whisker, an adjustable point contact. Unlike modern semiconductors, such a diode required painstaking adjustment of the contact to the crystal in order for it to rectify.\n\nThe tube was relatively immune to vibration, and thus vastly superior on shipboard duty, particularly for navy ships with the shock of weapon fire commonly knocking the sensitive but delicate galena off its sensitive point (the tube was in general no more sensitive as a radio detector, but was adjustment free). The diode tube was a reliable alternative for detecting radio signals.\n\nAs electronic engineering advanced, notably during World War II, this function of a diode came to be considered as one type of demodulation. While firmly established by history, the term \"detector\" is not of itself descriptive, and should be considered outdated.\n\nHigher power diode tubes or \"power rectifiers\" found their way into power supply applications until they were eventually replaced first by selenium, and later, by silicon rectifiers in the 1960s.\n\nOriginally, the only use for tubes in radio circuits was for rectification, not amplification. In 1906, Robert von Lieben filed for a patent for a cathode ray tube which included magnetic deflection. This could be used for amplifying audio signals and was intended for use in telephony equipment. He would later help refine the triode vacuum tube.\n\nHowever, Lee De Forest is credited with inventing the triode tube in 1907 while experimenting to improve his original (diode) Audion. By placing an additional electrode between the filament (cathode) and plate (anode), he discovered the ability of the resulting device to amplify signals. As the voltage applied to the control grid (or simply \"grid\") was lowered from the cathode's voltage to somewhat more negative voltages, the amount of current from the filament to the plate would be reduced.\n\nThe negative electrostatic field created by the grid in the vicinity of the cathode would inhibit passage of emitted electrons and reduce the current to the plate. Thus, a few volt difference at the grid would make a large change in the plate current and could lead to a much larger voltage change at the plate; the result was voltage and power amplification. In 1908, De Forest was granted a patent () for such a three-electrode version of his original Audion for use as an electronic amplifier in radio communications. This eventually became known as the triode.\nDe Forest's original device was made with conventional vacuum technology. The vacuum was not a \"hard vacuum\" but rather left a very small amount of residual gas. The physics behind the device's operation was also not settled. The residual gas would cause a blue glow (visible ionization) when the plate voltage was high (above about 60 volts). In 1912, De Forest brought the Audion to Harold Arnold in AT&T's engineering department. Arnold recommended that AT&T purchase the patent, and AT&T followed his recommendation. Arnold developed high-vacuum tubes which were tested in the summer of 1913 on AT&T's long distance network. The high-vacuum tubes could operate at high plate voltages without a blue glow.\n\nFinnish inventor Eric Tigerstedt significantly improved on the original triode design in 1914, while working on his sound-on-film process in Berlin, Germany. Tigerstedt's innovation was to make the electrodes concentric cylinders with the cathode at the centre, thus greatly increasing the collection of emitted electrons at the anode.\n\nIrving Langmuir at the General Electric research laboratory (Schenectady, New York) had improved Wolfgang Gaede's high-vacuum diffusion pump and used it to settle the question of thermionic emission and conduction in a vacuum. Consequently, General Electric started producing hard vacuum triodes (which were branded Pliotrons) in 1915. Langmuir patented the hard vacuum triode, but De Forest and AT&T successfully asserted priority and invalidated the patent.\n\nPliotrons were closely followed by the French type 'TM' and later the English type 'R' which were in widespread use by the allied military by 1916. Historically, vacuum levels in production vacuum tubes typically ranged from 10 µPa down to 10 nPa.\n\nThe triode and its derivatives (tetrodes and pentodes) are transconductance devices, in which the controlling signal applied to the grid is a \"voltage\", and the resulting amplified signal appearing at the anode is a \"current\". Compare this to the behavior of the bipolar junction transistor, in which the controlling signal is a current and the output is also a current.\n\nFor vacuum tubes, transconductance or mutual conductance () is defined as the change in the plate(anode)/cathode current divided by the corresponding change in the grid to cathode voltage, with a constant plate(anode) to cathode voltage. Typical values of for a small-signal vacuum tube are 1 to 10 millisiemens. It is one of the three 'constants' of a vacuum tube, the other two being its gain μ and plate resistance or . The Van der Bijl equation defines their relationship as follows: formula_1\n\nThe non-linear operating characteristic of the triode caused early tube audio amplifiers to exhibit harmonic distortion at low volumes. Plotting plate current as a function of applied grid voltage, it was seen that there was a range of grid voltages for which the transfer characteristics were approximately linear.\n\nTo use this range, a negative bias voltage had to be applied to the grid to position the DC operating point in the linear region. This was called the idle condition, and the plate current at this point the \"idle current\". The controlling voltage was superimposed onto the bias voltage, resulting in a linear variation of plate current in response to both positive and negative variation of the input voltage around that point.\n\nThis concept is called \"grid bias\". Many early radio sets had a third battery called the \"C battery\" (unrelated to the present-day C cell, for which the letter denotes its size and shape). The C battery's positive terminal was connected to the cathode of the tubes (or \"ground\" in most circuits) and whose negative terminal supplied this bias voltage to the grids of the tubes.\n\nLater circuits, after tubes were made with heaters isolated from their cathodes, used cathode biasing, avoiding the need for a separate negative power supply. For cathode biasing, a relatively low-value resistor is connected between the cathode and ground. This makes the cathode positive with respect to the grid, which is at ground potential for DC.\n\nHowever C batteries continued to be included in some equipment even when the \"A\" and \"B\" batteries had been replaced by power from the AC mains. That was possible because there was essentially no current draw on these batteries; they could thus last for many years (often longer than all the tubes) without requiring replacement.\n\nWhen triodes were first used in radio transmitters and receivers, it was found that tuned amplification stages had a tendency to oscillate unless their gain was very limited. This was due to the parasitic capacitance between the plate (the amplifier's output) and the control grid (the amplifier's input), known as the Miller capacitance.\n\nEventually the technique of \"neutralization\" was developed whereby the RF transformer connected to the plate (anode) would include an additional winding in the opposite phase. This winding would be connected back to the grid through a small capacitor, and when properly adjusted would cancel the Miller capacitance. This technique was employed and led to the success of the Neutrodyne radio during the 1920s.\nHowever, neutralization required careful adjustment and proved unsatisfactory when used over a wide range of frequencies.\n\nTo combat the stability problems and limited voltage gain due to the Miller effect, the physicist Walter H. Schottky invented the tetrode tube in 1919. He showed that the addition of a second grid, located between the control grid and the plate (anode), known as the \"screen grid\", could solve these problems. (\"Screen\" in this case refers to electrical \"screening\" or shielding, not physical construction: all \"grid\" electrodes in between the cathode and plate are \"screens\" of some sort rather than solid electrodes since they must allow for the passage of electrons directly from the cathode to the plate). A positive voltage slightly lower than the plate (anode) voltage was applied to it, and was bypassed (for high frequencies) to ground with a capacitor. This arrangement decoupled the anode and the control grid, essentially eliminating the Miller capacitance and its associated problems. Consequently, higher voltage gains from a single tube became possible, reducing the number of tubes required in many circuits. This two-grid tube is called a \"tetrode\", meaning four active electrodes, and was common by 1926.\nHowever, the tetrode had one new problem. In any tube, electrons strike the anode with sufficient energy to cause the emission of electrons from its surface. In a triode this so-called secondary emission of electrons is not important since they are simply re-captured by the more positive anode (plate). But in a tetrode they can be captured by the screen grid (thus also acting as an anode) since it is also at a high voltage, thus robbing them from the plate current and reducing the amplification of the device. Since secondary electrons can outnumber the primary electrons, in the worst case, particularly as the plate voltage dips below the screen voltage, the plate current can decrease with increasing plate voltage. This is the so-called \"tetrode kink\" and is an example of negative resistance which can itself cause instability. The otherwise undesirable negative resistance was exploited to produce a simple oscillator circuit only requiring connection of the plate to a resonant LC circuit to oscillate; this was effective over a wide frequency range. The so-called dynatron oscillator thus operated on the same principle of negative resistance as the tunnel diode oscillator many years later. Another undesirable consequence of secondary emission is that in extreme cases enough charge can flow to the screen grid to overheat and destroy it. Later tetrodes had anodes treated to reduce secondary emission; earlier ones such as the type 77 sharp-cutoff pentode connected as a tetrode made better dynatrons.\n\nThe solution was to add another grid between the screen grid and the main anode, called the suppressor grid (since it suppressed secondary emission current toward the screen grid). This grid was held at the cathode (or \"ground\") voltage and its negative voltage (relative to the anode) electrostatically repelled secondary electrons so that they would be collected by the anode after all. This three-grid tube is called a pentode, meaning five electrodes. The pentode was invented in 1926 by Bernard D. H. Tellegen and became generally favored over the simple tetrode. Pentodes are made in two classes: those with the suppressor grid wired internally to the cathode (e.g. EL84/6BQ5) and those with the suppressor grid wired to a separate pin for user access (e.g. 803, 837). An alternative solution for power applications is the beam tetrode or \"beam power tube\", discussed below.\n\nSuperheterodyne receivers require a local oscillator and mixer, combined in the function of a single pentagrid converter tube. Various alternatives such as using a combination of a triode with a hexode and even an octode have been used for this purpose. The additional grids include both control grids (at a low potential) and screen grids (at a high voltage). Many designs used such a screen grid as an additional anode to provide feedback for the oscillator function, whose current was added to that of the incoming radio frequency signal. The pentagrid converter thus became widely used in AM receivers, including the miniature tube version of the \"All American Five\". Octodes, such as the 7A8, were rarely used in the United States, but much more common in Europe, particularly in battery operated radios where the lower power consumption was an advantage.\n\nTo further reduce the cost and complexity of radio equipment, two separate structures (triode and pentode for instance) could be combined in the bulb of a single \"multisection tube\". An early example was the Loewe 3NF. This 1920s device had three triodes in a single glass envelope together with all the fixed capacitors and resistors required to make a complete radio receiver. As the Loewe set had only one tube socket, it was able to substantially undercut the competition, since, in Germany, state tax was levied by the number of sockets. However, reliability was compromised, and production costs for the tube were much greater. In a sense, these were akin to integrated circuits. In the United States, Cleartron briefly produced the \"Multivalve\" triple triode for use in the Emerson Baby Grand receiver. This Emerson set also had a single tube socket, but because it used a four-pin base, the additional element connections were made on a \"mezzanine\" platform at the top of the tube base.\n\nBy 1940 multisection tubes had become commonplace. There were constraints, however, due to patents and other licensing considerations (see British Valve Association). Constraints due to the number of external pins (leads) often forced the functions to share some of those external connections such as their cathode connections (in addition to the heater connection). The RCA Type 55 was a double diode triode used as a detector, automatic gain control rectifier and audio preamplifier in early AC powered radios. These sets often included the 53 Dual Triode Audio Output. Another early type of multi-section tube, the 6SN7, is a \"dual triode\" which performs the functions of two triode tubes, while taking up half as much space and costing less.\nThe 12AX7 is a dual \"high mu\" (high voltage gain) triode in a miniature enclosure, and became widely used in audio signal amplifiers, instruments, and guitar amplifiers.\n\nThe introduction of the miniature tube base (see below) which could have 9 pins, more than previously available, allowed other multi-section tubes to be introduced, such as the 6GH8/ECF82 triode-pentode, quite popular in television receivers. The desire to include even more functions in one envelope resulted in the General Electric Compactron which had 12 pins. A typical example, the 6AG11, contained two triodes and two diodes.\n\nSome otherwise conventional tubes do not fall into standard categories; the 6AR8, 6JH8 and 6ME8 had several common grids, followed by a pair of beam deflection electrodes which deflected the current towards either of two anodes. It was sometimes known as the 'sheet beam' tube, and was used in some color TV sets for color demodulation. The similar 7360 was popular as a balanced SSB (de)modulator.\n\nThe beam power tube is usually a tetrode with the addition of beam-forming electrodes, which take the place of the suppressor grid. These angled plates (not to be confused with the \"anode\") focus the electron stream onto certain spots on the anode which can withstand the heat generated by the impact of massive numbers of electrons, while also providing pentode behavior. The positioning of the elements in a beam power tube uses a design called \"critical-distance geometry\", which minimizes the \"tetrode kink\", plate to control grid capacitance, screen grid current, and secondary emission from the anode, thus increasing power conversion efficiency. The control grid and screen grid are also wound with the same pitch, or number of wires per inch. The two grids are positioned so that the control grid creates \"sheets\" of electrons which pass between the screen-grid wires. They're aligned to be equidistant from, say, the bottom of the tube.\n\nAligning the grid wires also helps to reduce screen current, which represents wasted energy. This design helps to overcome some of the practical barriers to designing high-power, high-efficiency power tubes. EMI engineers Cabot Bull and Sidney Rodda developed the design which became the 6L6, the first popular beam power tube, introduced by RCA in 1936 and later corresponding tubes in Europe the KT66, KT77 and KT88 made by the Marconi-Osram Valve subsidiary of GEC (the KT standing for \"Kinkless Tetrode\").\n\n\"Pentode operation\" of beam power tubes is often described in manufacturers' handbooks and data sheets, resulting in some confusion in terminology.\nThey are not pentodes, of course.\n\nVariations of the 6L6 design are still widely used in tube guitar amplifiers, making it one of the longest-lived electronic device families in history. Similar design strategies are used in the construction of large ceramic power tetrodes used in radio transmitters.\n\nBeam power tubes can be connected as triodes for improved audio tonal quality but in triode mode deliver significantly reduced power output.\n\nGas-filled tubes such as discharge tubes and cold cathode tubes are not \"hard\" vacuum tubes, though are always filled with gas at less than sea-level atmospheric pressure. Types such as the voltage-regulator tube and thyratron resemble hard vacuum tubes and fit in sockets designed for vacuum tubes. Their distinctive orange, red, or purple glow during operation indicates the presence of gas; electrons flowing in a vacuum do not produce light within that region. These types may still be referred to as \"electron tubes\" as they do perform electronic functions. High-power rectifiers use mercury vapor to achieve a lower forward voltage drop than high-vacuum tubes.\n\nEarly tubes used a metal or glass envelope atop an insulating bakelite base. In 1938 a technique was developed to use an all-glass construction with the pins fused in the glass base of the envelope. This was used in the design of a much smaller tube outline, known as the miniature tube, having 7 or 9 pins. Making tubes smaller reduced the voltage where they could safely operate, and also reduced the power dissipation of the filament. Miniature tubes became predominant in consumer applications such as radio receivers and hi-fi amplifiers. However the larger older styles continued to be used especially as higher power rectifiers, in higher power audio output stages and as transmitting tubes.Subminiature tubes with a size roughly that of half a cigarette were used in hearing-aid amplifiers. These tubes did not have pins plugging into a socket but were soldered in place. The \"acorn tube\" (named due to its shape) was also very small, as was the metal-cased RCA nuvistor from 1959, about the size of a thimble. The nuvistor was developed to compete with the early transistors and operated at higher frequencies than those early transistors could. The small size supported especially high-frequency operation; nuvistors were used in aircraft radio transceivers, UHF television tuners, and some HiFi FM radio tuners (Sansui 500A) until replaced by high-frequency capable transistors.\n\nThe earliest vacuum tubes strongly resembled incandescent light bulbs and were made by lamp manufacturers, who had the equipment needed to manufacture glass envelopes and the vacuum pumps required to evacuate the enclosures. De Forest used Heinrich Geissler's mercury displacement pump, which left behind a partial vacuum. The development of the diffusion pump in 1915 and improvement by Irving Langmuir led to the development of high-vacuum tubes. After World War I, specialized manufacturers using more economical construction methods were set up to fill the growing demand for broadcast receivers. Bare tungsten filaments operated at a temperature of around 2200 °C. The development of oxide-coated filaments in the mid-1920s reduced filament operating temperature to a dull red heat (around 700 °C), which in turn reduced thermal distortion of the tube structure and allowed closer spacing of tube elements. This in turn improved tube gain, since the gain of a triode is inversely proportional to the spacing between grid and cathode. Bare tungsten filaments remain in use in small transmitting tubes but are brittle and tend to fracture if handled roughly – e.g. in the postal services. These tubes are best suited to stationary equipment where impact and vibration is not present.\n\nThe desire to power electronic equipment using AC mains power faced a difficulty with respect to the powering of the tubes' filaments, as these were also the cathode of each tube. Powering the filaments directly from a power transformer introduced mains-frequency (50 or 60 Hz) hum into audio stages. The invention of the \"equipotential cathode\" reduced this problem, with the filaments being powered by a balanced AC power transformer winding having a grounded center tap.\n\nA superior solution, and one which allowed each cathode to \"float\" at a different voltage, was that of the indirectly heated cathode: a cylinder of oxide-coated nickel acted as electron-emitting cathode, and was electrically isolated from the filament inside it. Indirectly heated cathodes enable the cathode circuit to be separated from the heater circuit. The filament, no longer electrically connected to the tube's electrodes, became simply known as a \"heater\", and could as well be powered by AC without any introduction of hum. In the 1930s indirectly heated cathode tubes became widespread in equipment using AC power. Directly heated cathode tubes continued to be widely used in battery-powered equipment as their filaments required considerably less power than the heaters required with indirectly heated cathodes.\n\nTubes designed for high gain audio applications may have twisted heater wires to cancel out stray electric fields, fields that could induce objectionable hum into the program material.\n\nHeaters may be energized with either alternating current (AC) or direct current (DC). DC is often used where low hum is required.\n\nVacuum tubes used as switches made electronic computing possible for the first time, but the cost and relatively short mean time to failure of tubes were limiting factors. \"The common wisdom was that valves—which, like light bulbs, contained a hot glowing filament—could never be used satisfactorily in large numbers, for they were unreliable, and in a large installation too many would fail in too short a time\". Tommy Flowers, who later designed \"Colossus\", \"discovered that, so long as valves were switched on and left on, they could operate reliably for very long periods, especially if their 'heaters' were run on a reduced current\". In 1934 Flowers built a successful experimental installation using over 3,000 tubes in small independent modules; when a tube failed, it was possible to switch off one module and keep the others going, thereby reducing the risk of another tube failure being caused; this installation was accepted by the Post Office (who operated telephone exchanges). Flowers was also a pioneer of using tubes as very fast (compared to electromechanical devices) electronic switches. Later work confirmed that tube unreliability was not as serious an issue as generally believed; the 1946 ENIAC, with over 17,000 tubes, had a tube failure (which took 15 minutes to locate) on average every two days. The quality of the tubes was a factor, and the diversion of skilled people during the Second World War lowered the general quality of tubes. During the war Colossus was instrumental in breaking German codes. After the war, development continued with tube-based computers including, military computers ENIAC and Whirlwind, the Ferranti Mark 1 (the first commercially available electronic computer), and UNIVAC I, also available commercially.\n\nFlowers's Colossus and its successor Colossus Mk2 were built by the British during World War II to substantially speed up the task of breaking the German high level Lorenz encryption. Using about 1,500 vacuum tubes (2,400 for Mk2), Colossus replaced an earlier machine based on relay and switch logic (the Heath Robinson). Colossus was able to break in a matter of hours messages that had previously taken several weeks; it was also much more reliable. Colossus was the first use of vacuum tubes \"working in concert\" on such a large scale for a single machine.\n\nOnce Colossus was built and installed, it ran continuously, powered by dual redundant diesel generators, the wartime mains supply being considered too unreliable. The only time it was switched off was for conversion to Mk2, which added more tubes. Another nine Colossus Mk2s were built. Each Mk2 consumed 15 kilowatts; most of the power was for the tube heaters.\n\nA Colossus reconstruction was switched on in 1996; it was upgraded to Mk2 configuration in 2004; it found the key for a wartime German ciphertext in 2007.\n\nTo meet the reliability requirements of the 1951 US digital computer Whirlwind, \"special-quality\" tubes with extended life, and a long-lasting cathode in particular, were produced. The problem of short lifetime was traced to evaporation of silicon, used in the tungsten alloy to make the heater wire easier to draw. Elimination of silicon from the heater wire alloy (and more frequent replacement of the wire drawing dies) allowed production of tubes that were reliable enough for the Whirlwind project. The tubes developed for Whirlwind were later used in the giant SAGE air-defense computer system. SAGE computers were dual installations, with one operating, and the other in standby. To locate potential tube failures in the standby computer, heater voltages were reduced, which caused failures of tubes which would otherwise fail in service. These computers continued in service years after other tube computers had been superseded.\n\nHigh-purity nickel tubing and cathode coatings free of materials that can poison emission (such as silicates and aluminum) also contribute to long cathode life. The first such \"computer tube\" was Sylvania's 7AK7 of 1948. Computers were the first tube devices to run tubes at cutoff (enough negative grid voltage to make them cease conduction) for quite-extended periods of time. When their grids became less negative, they failed to conduct. While hot but non-conductive, an insulating layer (\"cathode interface\") developed between the nickel sleeve and the oxide coating. What was described above cured this problem.\n\nBy the late 1950s it was routine for special-quality small-signal tubes to last for hundreds of thousands of hours, if operated conservatively. This increased reliability also made mid-cable amplifiers in submarine cables possible.\n\nA considerable amount of heat is produced when tubes operate, both from the filament (heater) but also from the stream of electrons bombarding the plate. In power amplifiers this source of heat will exceed the power due to cathode heating.\nA few types of tube permit operation with the anodes at a dull red heat; in other types, red heat indicates severe overload.\n\nThe requirements for heat removal can significantly change the appearance of high-power vacuum tubes. High power audio amplifiers and rectifiers required larger envelopes to dissipate heat. Transmitting tubes could be much larger still.\n\nHeat escapes the device by black body radiation from the anode (plate) as infrared radiation, and by convection of air over the tube envelope. Convection is not possible inside most tubes since the anode is surrounded by vacuum.\n\nTubes which generate relatively little heat, such as the 1.4-volt filament directly heated tubes designed for use in battery-powered equipment, often have shiny metal anodes. 1T4, 1R5 and 1A7 are examples. Gas-filled tubes such as thyratrons may also use a shiny metal anode, since the gas present inside the tube allows for heat convection from the anode to the glass enclosure.\n\nThe anode is often treated to make its surface emit more infrared energy. High-power amplifier tubes are designed with external anodes which can be cooled by convection, forced air or circulating water. The water-cooled 80 kg, 1.25 MW 8974 is among the largest commercial tubes available today.\n\nIn a water-cooled tube, the anode voltage appears directly on the cooling water surface, thus requiring the water to be an electrical insulator to prevent high voltage leakage through the cooling water to the radiator system. Water as usually supplied has ions which conduct electricity; deionized water, a good insulator, is required. Such systems usually have a built-in water-conductance monitor which will shut down the high-tension supply if the conductance becomes too high.\n\nThe screen grid may also generate considerable heat. Limits to screen grid dissipation, in addition to plate dissipation, are listed for power devices. If these are exceeded then tube failure is likely.\n\nMost modern tubes have glass envelopes, but metal, fused quartz (silica) and ceramic have also been used. A first version of the 6L6 used a metal envelope sealed with glass beads, while a glass disk fused to the metal was used in later versions. Metal and ceramic are used almost exclusively for power tubes above 2 kW dissipation. The nuvistor was a modern receiving tube using a very small metal and ceramic package.\n\nThe internal elements of tubes have always been connected to external circuitry via pins at their base which plug into a socket. Subminiature tubes were produced using wire leads rather than sockets, however these were restricted to rather specialized applications. In addition to the connections at the base of the tube, many early triodes connected the grid using a metal cap at the top of the tube; this reduces stray capacitance between the grid and the plate leads. Tube caps were also used for the plate (anode) connection, particularly in transmitting tubes and tubes using a very high plate voltage.\n\nHigh-power tubes such as transmitting tubes have packages designed more to enhance heat transfer. In some tubes, the metal envelope is also the anode. The 4CX1000A is an external anode tube of this sort. Air is blown through an array of fins attached to the anode, thus cooling it. Power tubes using this cooling scheme are available up to 150 kW dissipation. Above that level, water or water-vapor cooling are used. The highest-power tube currently available is the Eimac , a forced water-cooled power tetrode capable of dissipating 2.5 megawatts. By comparison, the largest power transistor can only dissipate about 1 kilowatt.\n\nThe generic name \"[thermionic] valve\" used in the UK derives from the unidirectional current flow allowed by the earliest device, the thermionic diode emitting electrons from a heated filament, by analogy with a non-return valve in a water pipe. The US names \"vacuum tube\", \"electron tube\", and \"thermionic tube\" all simply describe a tubular envelope which has been evacuated (\"vacuum\"), has a heater, and controls electron flow.\n\nIn many cases manufacturers and the military gave tubes designations which said nothing about their purpose (e.g., 1614). In the early days some manufacturers used proprietary names which might convey some information, but only about their products; the KT66 and KT88 were \"Kinkless Tetrodes\". Later, consumer tubes were given names which conveyed some information, with the same name often used generically by several manufacturers. In the US, Radio Electronics Television Manufacturers' Association (RETMA) designations comprise a number, followed by one or two letters, and a number. The first number is the (rounded) heater voltage; the letters designate a particular tube but say nothing about its structure; and the final number is the total number of electrodes (without distinguishing between, say, a tube with many electrodes, or two sets of electrodes in a single envelope—a double triode, for example). For example, the 12AX7 is a double triode (two sets of three electrodes plus heater) with a 12.6V heater (which, as it happens, can also be connected to run from 6.3V). The \"AX\" has no meaning other than to designate this particular tube according to its characteristics. Similar, but not identical, tubes are the 12AD7, 12AE7...12AT7, 12AU7, 12AV7, 12AW7 (rare!), 12AY7, and the 12AZ7.\n\nA system widely used in Europe known as the Mullard–Philips tube designation, also extended to transistors, uses a letter, followed by one or more further letters, and a number. The type designator specifies the heater voltage or current (one letter), the functions of all sections of the tube (one letter per section), the socket type (first digit), and the particular tube (remaining digits). For example, the ECC83 (equivalent to the 12AX7) is a 6.3V (E) double triode (CC) with a miniature base (8). In this system special-quality tubes (e.g., for long-life computer use) are indicated by moving the number immediately after the first letter: the E83CC is a special-quality equivalent of the ECC83, the E55L a power pentode with no consumer equivalent.\n\nSome special-purpose tubes are constructed with particular gases in the envelope. For instance, voltage-regulator tubes contain various inert gases such as argon, helium or neon, which will ionize at predictable voltages. The thyratron is a special-purpose tube filled with low-pressure gas or mercury vapor. Like vacuum tubes, it contains a hot cathode and an anode, but also a control electrode which behaves somewhat like the grid of a triode. When the control electrode starts conduction, the gas ionizes, after which the control electrode can no longer stop the current; the tube \"latches\" into conduction. Removing anode (plate) voltage lets the gas de-ionize, restoring its non-conductive state.\n\nSome thyratrons can carry large currents for their physical size. One example is the miniature type 2D21, often seen in 1950s jukeboxes as control switches for relays. A cold-cathode version of the thyratron, which uses a pool of mercury for its cathode, is called an ignitron; some can switch thousands of amperes. Thyratrons containing hydrogen have a very consistent time delay between their turn-on pulse and full conduction; they behave much like modern silicon-controlled rectifiers, also called thyristors due to their functional similarity to thyratrons. Hydrogen thyratrons have long been used in radar transmitters.\n\nA specialized tube is the krytron, which is used for rapid high-voltage switching. Krytrons are used to initiate the detonations used to set off a nuclear weapon; krytrons are heavily controlled at an international level.\n\nX-ray tubes are used in medical imaging among other uses. X-ray tubes used for continuous-duty operation in fluoroscopy and CT imaging equipment may use a focused cathode and a rotating anode to dissipate the large amounts of heat thereby generated. These are housed in an oil-filled aluminium housing to provide cooling.\n\nThe photomultiplier tube is an extremely sensitive detector of light, which uses the photoelectric effect and secondary emission, rather than thermionic emission, to generate and amplify electrical signals. Nuclear medicine imaging equipment and liquid scintillation counters use photomultiplier tube arrays to detect low-intensity scintillation due to ionizing radiation.\n\nBatteries provided the voltages required by tubes in early radio sets. Three different voltages were generally required, using three different batteries designated as the A, B, and C battery. The \"A\" battery or LT (low-tension) battery provided the filament voltage. Tube heaters were designed for single, double or triple-cell lead-acid batteries, giving nominal heater voltages of 2 V, 4 V or 6 V. In portable radios, dry batteries were sometimes used with 1.5 or 1 V heaters. Reducing filament consumption improved the life span of batteries. By 1955 towards the end of the tube era, tubes using only 50 mA down to as little as 10 mA for the heaters had been developed.\n\nThe high voltage applied to the anode (plate) was provided by the \"B\" battery or the HT (high-tension) supply or battery. These were generally of dry cell construction and typically came in 22.5-, 45-, 67.5-, 90-, 120- or 135-volt versions.\nEarly sets used a grid bias battery or \"C\" battery which was connected to provide a \"negative\" voltage. Since virtually no current flows through a tube's grid connection, these batteries had very low drain and lasted the longest. Even after AC power supplies became commonplace, some radio sets continued to be built with C batteries, as they would almost never need replacing. However more modern circuits were designed using cathode biasing, eliminating the need for a third power supply voltage; this became practical with tubes using indirect heating of the cathode.\n\nThe \"C battery\" for bias is a designation having no relation to the \"C cell\" battery size.\n\nBattery replacement was a major operating cost for early radio receiver users. The development of the battery eliminator, and, in 1925, batteryless receivers operated by household power, reduced operating costs and contributed to the growing popularity of radio. A power supply using a transformer with several windings, one or more rectifiers (which may themselves be vacuum tubes), and large filter capacitors provided the required direct current voltages from the alternating current source.\n\nAs a cost reduction measure, especially in high-volume consumer receivers, all the tube heaters could be connected in series across the AC supply using heaters requiring the same current and with a similar warm-up time. In one such design, a tap on the tube heater string supplied the 6 volts needed for the dial light. By deriving the high voltage from a half-wave rectifier directly connected to the AC mains, the heavy and costly power transformer was eliminated. This also allowed such receivers to operate on direct current, a so-called AC/DC receiver design. Many different US consumer AM radio manufacturers of the era used a virtually identical circuit, given the nickname All American Five.\n\nWhere the mains voltage was in the 100-120V range, this limited voltage proved suitable only for low-power receivers. Television receivers either required a transformer or could use a voltage doubling circuit. Where 230 V nominal mains voltage was used, television receivers as well could dispense with a power transformer.\n\nTransformer-less power supplies required safety precautions in their design to limit the shock hazard to users, such as electrically insulated cabinets and an interlock tying the power cord to the cabinet back, so the line cord was necessarily disconnected if the user or service person opened the cabinet. A \"cheater cord\" was a power cord ending in the special socket used by the safety interlock; servicers could then power the device with the hazardous voltages exposed.\n\nTo avoid the warm-up delay, \"instant on\" television receivers passed a small heating current through their tubes even when the set was nominally off. At switch on, full heating current was provided and the set would play almost immediately.\n\nOne reliability problem of tubes with oxide cathodes is the possibility that the cathode may slowly become \"poisoned\" by gas molecules from other elements in the tube, which reduce its ability to emit electrons. Trapped gases or slow gas leaks can also damage the cathode or cause plate (anode) current runaway due to ionization of free gas molecules. Vacuum hardness and proper selection of construction materials are the major influences on tube lifetime. Depending on the material, temperature and construction, the surface material of the cathode may also diffuse onto other elements. The resistive heaters that heat the cathodes may break in a manner similar to incandescent lamp filaments, but rarely do, since they operate at much lower temperatures than lamps.\n\nThe heater's failure mode is typically a stress-related fracture of the tungsten wire or at a weld point and generally occurs after accruing many thermal (power on-off) cycles. Tungsten wire has a very low resistance when at room temperature. A negative temperature coefficient device, such as a thermistor, may be incorporated in the equipment's heater supply or a ramp-up circuit may be employed to allow the heater or filaments to reach operating temperature more gradually than if powered-up in a step-function. Low-cost radios had tubes with heaters connected in series, with a total voltage equal to that of the line (mains). Some receivers made before World War II had series-string heaters with total voltage less than that of the mains. Some had a resistance wire running the length of the power cord to drop the voltage to the tubes. Others had series resistors made like regular tubes; they were called ballast tubes.\n\nFollowing World War II, tubes intended to be used in series heater strings were redesigned to all have the same (\"controlled\") warm-up time. Earlier designs had quite-different thermal time constants. The audio output stage, for instance, had a larger cathode, and warmed up more slowly than lower-powered tubes. The result was that heaters that warmed up faster also temporarily had higher resistance, because of their positive temperature coefficient. This disproportionate resistance caused them to temporarily operate with heater voltages well above their ratings, and shortened their life.\n\nAnother important reliability problem is caused by air leakage into the tube. Usually oxygen in the air reacts chemically with the hot filament or cathode, quickly ruining it. Designers developed tube designs that sealed reliably. This was why most tubes were constructed of glass. Metal alloys (such as Cunife and Fernico) and glasses had been developed for light bulbs that expanded and contracted in similar amounts, as temperature changed. These made it easy to construct an insulating envelope of glass, while passing connection wires through the glass to the electrodes.\n\nWhen a vacuum tube is overloaded or operated past its design dissipation, its anode (plate) may glow red. In consumer equipment, a glowing plate is universally a sign of an overloaded tube. However, some large transmitting tubes are designed to operate with their anodes at red, orange, or in rare cases, white heat.\n\n\"Special quality\" versions of standard tubes were often made, designed for improved performance in some respect, such as a longer life cathode, low noise construction, mechanical ruggedness via ruggedized filaments, low microphony, for applications where the tube will spend much of its time cut off, etc. The only way to know the particular features of a special quality part is by reading the data sheet. Names may reflect the standard name (12AU7==>12AU7A, its equivalent ECC82==>E82CC, etc.), or be absolutely anything (standard and special-quality equivalents of the same tube include 12AU7, ECC82, B329, CV491, E2163, E812CC, M8136, CV4003, 6067, VX7058, 5814A and 12AU7A).\n\nThe longest recorded valve life was earned by a Mazda AC/P pentode valve (serial No. 4418) in operation at the BBC's main Northern Ireland transmitter at Lisnagarvey. The valve was in service from 1935 until 1961 and had a recorded life of 232,592 hours. The BBC maintained meticulous records of their valves' lives with periodic returns to their central valve stores.\n\nA vacuum tube needs an extremely good (\"hard\") vacuum to avoid the consequences of generating positive ions within the tube. With a small amount of residual gas, some of those atoms may ionize when struck by an electron and create fields that adversely affect the tube characteristics. Larger amounts of residual gas can create a self-sustaining visible glow discharge between the tube elements. To avoid these effects, the residual pressure within the tube must be low enough that the mean free path of an electron is much longer than the size of the tube (so an electron is unlikely to strike a residual atom and very few ionized atoms will be present). Commercial vacuum tubes are evacuated at manufacture to about .\n\nTo prevent gases from compromising the tube's vacuum, modern tubes are constructed with \"getters\", which are usually small, circular troughs filled with metals that oxidize quickly, barium being the most common. While the tube envelope is being evacuated, the internal parts except the getter are heated by RF induction heating to evolve any remaining gas from the metal parts. The tube is then sealed and the getter is heated to a high temperature, again by radio frequency induction heating, which causes the getter material to vaporize and react with any residual gas. The vapor is deposited on the inside of the glass envelope, leaving a silver-colored metallic patch which continues to absorb small amounts of gas that may leak into the tube during its working life. Great care is taken with the valve design to ensure this material is not deposited on any of the working electrodes. If a tube develops a serious leak in the envelope, this deposit turns a white color as it reacts with atmospheric oxygen. Large transmitting and specialized tubes often use more exotic getter materials, such as zirconium. Early gettered tubes used phosphorus-based getters, and these tubes are easily identifiable, as the phosphorus leaves a characteristic orange or rainbow deposit on the glass. The use of phosphorus was short-lived and was quickly replaced by the superior barium getters. Unlike the barium getters, the phosphorus did not absorb any further gases once it had fired.\n\nGetters act by chemically combining with residual or infiltrating gases, but are unable to counteract (non-reactive) inert gases. A known problem, mostly affecting valves with large envelopes such as cathode ray tubes and camera tubes such as iconoscopes, orthicons, and image orthicons, comes from helium infiltration. The effect appears as impaired or absent functioning, and as a diffuse glow along the electron stream inside the tube. This effect cannot be rectified (short of re-evacuation and resealing), and is responsible for working examples of such tubes becoming rarer and rarer. Unused (\"New Old Stock\") tubes can also exhibit inert gas infiltration, so there is no long-term guarantee of these tube types surviving into the future.\n\nLarge transmitting tubes have carbonized tungsten filaments containing a small trace (1% to 2%) of thorium. An extremely thin (molecular) layer of thorium atoms forms on the outside of the wire's carbonized layer and, when heated, serve as an efficient source of electrons. The thorium slowly evaporates from the wire surface, while new thorium atoms diffuse to the surface to replace them. Such thoriated tungsten cathodes usually deliver lifetimes in the tens of thousands of hours. The end-of-life scenario for a thoriated-tungsten filament is when the carbonized layer has mostly been converted back into another form of tungsten carbide and emission begins to drop off rapidly; a complete loss of thorium has never been found to be a factor in the end-of-life in a tube with this type of emitter.\nWAAY-TV in Huntsville, Alabama achieved 163,000 hours (18.6 years) of service from an Eimac external cavity klystron in the visual circuit of its transmitter; this is the highest documented service life for this type of tube. \nIt has been said that transmitters with vacuum tubes are better able to survive lightning strikes than transistor transmitters do. While it was commonly believed that at RF power levels above approximately 20 kilowatts, vacuum tubes were more efficient than solid-state circuits, this is no longer the case, especially in medium wave (AM broadcast) service where solid-state transmitters at nearly all power levels have measurably higher efficiency. FM broadcast transmitters with solid-state power amplifiers up to approximately 15 kW also show better overall power efficiency than tube-based power amplifiers.\n\nCathodes in small \"receiving\" tubes are coated with a mixture of barium oxide and strontium oxide, sometimes with addition of calcium oxide or aluminium oxide. An electric heater is inserted into the cathode sleeve, and insulated from it electrically by a coating of aluminium oxide. This complex construction causes barium and strontium atoms to diffuse to the surface of the cathode and emit electrons when heated to about 780 degrees Celsius.\n\nA catastrophic failure is one which suddenly makes the vacuum tube unusable. A crack in the glass envelope will allow air into the tube and destroy it. Cracks may result from stress in the glass, bent pins or impacts; tube sockets must allow for thermal expansion, to prevent stress in the glass at the pins. Stress may accumulate if a metal shield or other object presses on the tube envelope and causes differential heating of the glass. Glass may also be damaged by high-voltage arcing.\n\nTube heaters may also fail without warning, especially if exposed to over voltage or as a result of manufacturing defects. Tube heaters do not normally fail by evaporation like lamp filaments, since they operate at much lower temperature. The surge of inrush current when the heater is first energized causes stress in the heater, and can be avoided by slowly warming the heaters, gradually increasing current with a NTC thermistor included in the circuit. Tubes intended for series-string operation of the heaters across the supply have a specified controlled warm-up time to avoid excess voltage on some heaters as others warm up. Directly heated filament-type cathodes as used in battery-operated tubes or some rectifiers may fail if the filament sags, causing internal arcing. Excess heater-to-cathode voltage in indirectly heated cathodes can break down the insulation between elements and destroy the heater.\n\nArcing between tube elements can destroy the tube. An arc can be caused by applying voltage to the anode (plate) before the cathode has come up to operating temperature, or by drawing excess current through a rectifier, which damages the emission coating. Arcs can also be initiated by any loose material inside the tube, or by excess screen voltage. An arc inside the tube allows gas to evolve from the tube materials, and may deposit conductive material on internal insulating spacers.\n\nTube rectifiers have limited current capability and exceeding ratings will eventually destroy a tube.\n\nDegenerative failures are those caused by the slow deterioration of performance over time.\n\nOverheating of internal parts, such as control grids or mica spacer insulators, can result in trapped gas escaping into the tube; this can reduce performance. A getter is used to absorb gases evolved during tube operation, but has only a limited ability to combine with gas. Control of the envelope temperature prevents some types of gassing. A tube with an unusually high level of internal gas may exhibit a visible blue glow when plate voltage is applied. The getter (being a highly reactive metal) is effective against many atmospheric gases, but has no (or very limited) chemical reactivity to inert gases such as helium. One progressive type of failure, especially with physically large envelopes such as those used by camera tubes and cathode-ray tubes, comes from helium infiltration. The exact mechanism is not clear: the metal-to-glass lead-in seals are one possible infiltration site.\n\nGas and ions within the tube contribute to grid current which can disturb operation of a vacuum tube circuit. Another effect of overheating is the slow deposit of metallic vapors on internal spacers, resulting in inter-element leakage.\n\nTubes on standby for long periods, with heater voltage applied, may develop high cathode interface resistance and display poor emission characteristics. This effect occurred especially in pulse and digital circuits, where tubes had no plate current flowing for extended times. Tubes designed specifically for this mode of operation were made.\n\nCathode depletion is the loss of emission after thousands of hours of normal use. Sometimes emission can be restored for a time by raising heater voltage, either for a short time or a permanent increase of a few percent. Cathode depletion was uncommon in signal tubes but was a frequent cause of failure of monochrome television cathode-ray tubes. Usable life of this expensive component was sometimes extended by fitting a boost transformer to increase heater voltage.\n\nVacuum tubes may develop defects in operation that make an individual tube unsuitable in a given device, although it may perform satisfactorily in another application. \"Microphonics\" refers to internal vibrations of tube elements which modulate the tube's signal in an undesirable way; sound or vibration pick-up may affect the signals, or even cause uncontrolled howling if a feedback path develops between a microphonic tube and, for example, a loudspeaker. Leakage current between AC heaters and the cathode may couple into the circuit, or electrons emitted directly from the ends of the heater may also inject hum into the signal. Leakage current due to internal contamination may also inject noise. Some of these effects make tubes unsuitable for small-signal audio use, although unobjectionable for other purposes. Selecting the best of a batch of nominally identical tubes for critical applications can produce better results.\n\nTube pins can develop non-conducting or high resistance surface films due to heat or dirt. Pins can be cleaned to restore conductance.\n\nVacuum tubes can be tested outside of their circuitry using a vacuum tube tester.\n\nMost small signal vacuum tube devices have been superseded by semiconductors, but some vacuum tube electronic devices are still in common use. The magnetron is the type of tube used in all microwave ovens. In spite of the advancing state of the art in power semiconductor technology, the vacuum tube still has reliability and cost advantages for high-frequency RF power generation.\n\nSome tubes, such as magnetrons, traveling-wave tubes, carcinotrons, and klystrons, combine magnetic and electrostatic effects. These are efficient (usually narrow-band) RF generators and still find use in radar, microwave ovens and industrial heating. Traveling-wave tubes (TWTs) are very good amplifiers and are even used in some communications satellites. High-powered klystron amplifier tubes can provide hundreds of kilowatts in the UHF range.\n\nThe cathode ray tube (CRT) is a vacuum tube used particularly for display purposes. Although there are still many televisions and computer monitors using cathode ray tubes, they are rapidly being replaced by flat panel displays whose quality has greatly improved even as their prices drop. This is also true of digital oscilloscopes (based on internal computers and analog to digital converters), although traditional analog scopes (dependent upon CRTs) continue to be produced, are economical, and preferred by many technicians. At one time many radios used \"magic eye tubes\", a specialized sort of CRT used in place of a meter movement to indicate signal strength, or input level in a tape recorder. A modern indicator device, the vacuum fluorescent display (VFD) is also a sort of cathode ray tube.\n\nThe X-ray tube is a type of cathode ray tube that generates X-rays when high voltage electrons hit the anode.\n\nGyrotrons or vacuum masers, used to generate high-power millimeter band waves, are magnetic vacuum tubes in which a small relativistic effect, due to the high voltage, is used for bunching the electrons. Gyrotrons can generate very high powers (hundreds of kilowatts).\nFree-electron lasers, used to generate high-power coherent light and even X-rays, are highly relativistic vacuum tubes driven by high-energy particle accelerators. Thus, these are sorts of cathode ray tubes.\n\nA photomultiplier is a phototube whose sensitivity is greatly increased through the use of electron multiplication. This works on the principle of secondary emission, whereby a single electron emitted by the photocathode strikes a special sort of anode known as a dynode causing more electrons to be released from that dynode. Those electrons are accelerated toward another dynode at a higher voltage, releasing more secondary electrons; as many as 15 such stages provide a huge amplification. Despite great advances in solid-state photodetectors, the single-photon detection capability of photomultiplier tubes makes this vacuum tube device excel in certain applications. Such a tube can also be used for detection of ionizing radiation as an alternative to the Geiger–Müller tube (itself not an actual vacuum tube). Historically, the image orthicon TV camera tube widely used in television studios prior to the development of modern CCD arrays also used multistage electron multiplication.\n\nFor decades, electron-tube designers tried to augment amplifying tubes with electron multipliers in order to increase gain, but these suffered from short life because the material used for the dynodes \"poisoned\" the tube's hot cathode. (For instance, the interesting RCA 1630 secondary-emission tube was marketed, but did not last.) However, eventually, Philips of the Netherlands developed the EFP60 tube that had a satisfactory lifetime, and was used in at least one product, a laboratory pulse generator. By that time, however, transistors were rapidly improving, making such developments superfluous.\n\nOne variant called a \"channel electron multiplier\" does not use individual dynodes but consists of a curved tube, such as a helix, coated on the inside with material with good secondary emission. One type had a funnel of sorts to capture the secondary electrons. The continuous dynode was resistive, and its ends were connected to enough voltage to create repeated cascades of electrons. The microchannel plate consists of an array of single stage electron multipliers over an image plane; several of these can then be stacked. This can be used, for instance, as an image intensifier in which the discrete channels substitute for focussing.\n\nTektronix made a high-performance wideband oscilloscope CRT with a channel electron multiplier plate behind the phosphor layer. This plate was a bundled array of a huge number of short individual c.e.m. tubes that accepted a low-current beam and intensified it to provide a display of practical brightness. (The electron optics of the wideband electron gun could not provide enough current to directly excite the phosphor.)\n\nAlthough vacuum tubes have been largely replaced by solid-state devices in most amplifying, switching, and rectifying applications, there are certain exceptions. In addition to the special functions noted above, tubes have some niche applications.\n\nIn general, vacuum tubes are much less susceptible than corresponding solid-state components to transient overvoltages, such as mains voltage surges or lightning, the electromagnetic pulse effect of nuclear explosions, or geomagnetic storms produced by giant solar flares. This property kept them in use for certain military applications long after more practical and less expensive solid-state technology was available for the same applications, as for example with the MiG-25. In that aircraft, output power of the radar is about one kilowatt and it can burn through a channel under interference.\n\nVacuum tubes are still practical alternatives to solid-state devices in generating high power at radio frequencies in applications such as industrial radio frequency heating, particle accelerators, and broadcast transmitters. This is particularly true at microwave frequencies where such devices as the klystron and traveling-wave tube provide amplification at power levels unattainable using semiconductor devices. The household microwave oven uses a magnetron tube to efficiently generate hundreds of watts of microwave power.\n\nIn military applications, a high-power vacuum tube can generate a 10–100 megawatt signal that can burn out an unprotected receiver's frontend. Such devices are considered non-nuclear electromagnetic weapons; they were introduced in the late 1990s by both the U.S. and Russia.\n\nEnough people prefer tube sound to make tube amplifiers commercially viable in three areas: musical instrument (e.g., guitar) amplifiers, devices used in recording studios, and audiophile equipment.\n\nMany guitarists prefer using valve amplifiers to solid-state models, often due to the way they tend to distort when overdriven. Any amplifier can only accurately amplify a signal to a certain volume; past this limit, the amplifier will begin to distort the signal. Different circuits will distort the signal in different ways; some guitarists prefer the distortion characteristics of vacuum tubes. Most popular vintage models use vacuum tubes.\n\nA modern display technology using a variation of cathode ray tube is often used in videocassette recorders, DVD players and recorders, microwave oven control panels, and automotive dashboards. Rather than raster scanning, these vacuum fluorescent displays (VFD) switch control grids and anode voltages on and off, for instance, to display discrete characters. The VFD uses phosphor-coated anodes as in other display cathode ray tubes. Because the filaments are in view, they must be operated at temperatures where the filament does not glow visibly. This is possible using more recent cathode technology, and these tubes also operate with quite low anode voltages (often less than 50 volts) unlike cathode ray tubes. Their high brightness allows reading the display in bright daylight. VFD tubes are flat and rectangular, as well as relatively thin. Typical VFD phosphors emit a broad spectrum of greenish-white light, permitting use of color filters, though different phosphors can give other colors even within the same display. The design of these tubes provides a bright glow despite the low energy of the incident electrons. This is because the distance between the cathode and anode is relatively small. (This technology is distinct from fluorescent lighting, which uses a discharge tube.)\n\nIn the early years of the 21st century there has been renewed interest in vacuum tubes, this time with the electron emitter formed on a flat silicon substrate, as in integrated circuit technology. This subject is now called vacuum nanoelectronics. The most common design uses a cold cathode in the form of a large-area field electron source (for example a field emitter array). With these devices, electrons are field-emitted from a large number of closely spaced individual emission sites.\n\nSuch integrated microtubes may find application in microwave devices including mobile phones, for Bluetooth and Wi-Fi transmission, and in radar and satellite communication. , they were being studied for possible applications in field emission display technology, but there were significant production problems.\n\nAs of 2014, NASA's Ames Research Center was reported on working on vacuum-channel transistors produced using CMOS techniques.\n\n\n\n"}
