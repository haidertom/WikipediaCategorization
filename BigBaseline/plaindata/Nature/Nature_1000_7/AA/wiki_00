{"id": "71318", "url": "https://en.wikipedia.org/wiki?curid=71318", "title": "100-year flood", "text": "100-year flood\n\nA one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year.\n\nThe 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. In the UK The Environment Agency publishes a comprehensive map of all areas at risk of a 1 in 100 year flood. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.\n\nA common misunderstanding is that a 100-year flood is likely to occur only once in a 100-year period. In fact, there is approximately a 63.4% chance of one or more 100-year floods occurring in any 100-year period. On the Danube River at Passau, Germany, the actual intervals between 100-year floods during 1501 to 2013 ranged from 37 to 192 years. The probability P that one or more floods occurring during any period will exceed a given flood threshold can be expressed, using the binomial distribution, as\n\nformula_1\n\nwhere T is the threshold return period (e.g. 100-yr, 50-yr, 25-yr, and so forth), and n is the number of years in the period. The probability of exceedance P is also described as the natural, inherent, or hydrologic risk of failure. However, the expected value of the number of 100-year floods occurring in any 100-year period is 1.\n\nTen-year floods have a 10% chance of occurring in any given year (P =0.10); 500-year have a 0.2% chance of occurring in any given year (P =0.002); etc. The percent chance of an X-year flood occurring in a single year is 100/X. A similar analysis is commonly applied to coastal flooding or rainfall data. The recurrence interval of a storm is rarely identical to that of an associated riverine flood, because of rainfall timing and location variations among different drainage basins.\n\nThe field of extreme value theory was created to model rare events such as 100-year floods for the purposes of civil engineering. This theory is most commonly applied to the maximum or minimum observed stream flows of a given river. In desert areas where there are only ephemeral washes, this method is applied to the maximum observed rainfall over a given period of time (24-hours, 6-hours, or 3-hours). The extreme value analysis only considers the most extreme event observed in a given year. So, between the large spring runoff and a heavy summer rain storm, whichever resulted in more runoff would be considered the extreme event, while the smaller event would be ignored in the analysis (even though both may have been capable of causing terrible flooding in their own right).\n\nThere are a number of assumptions which are made to complete the analysis which determines the 100-year flood. First, the extreme events observed in each year must be independent from year-to-year. In other words, the maximum river flow rate from 1984 cannot be found to be significantly correlated with the observed flow rate in 1985. 1985 cannot be correlated with 1986, and so forth. The second assumption is that the observed extreme events must come from the same probability distribution function. The third assumption is that the probability distribution relates to the largest storm (rainfall or river flow rate measurement) that occurs in any one year. The fourth assumption is that the probability distribution function is stationary, meaning that the mean (average), standard deviation and max/min values are not increasing or decreasing over time. This concept is referred to as stationarity.\n\nThe first assumption is often but not always valid and should be tested on a case by case basis. The second assumption is often valid if the extreme events are observed under similar climate conditions. For example, if the extreme events on record all come from late summer thunder storms (as is the case in the southwest U.S.), or from snow pack melting (as is the case in north-central U.S.), then this assumption should be valid. If, however, there are some extreme events taken from thunder storms, others from snow pack melting, and others from hurricanes, then this assumption is most likely not valid. The third assumption is only a problem when trying to forecast a low, but maximum flow event (for example, an event smaller than a 2-year flood). Since this is not typically a goal in extreme analysis, or in civil engineering design, then the situation rarely presents itself. The final assumption about stationarity is difficult to test from data for a single site because of the large uncertainties in even the longest flood records (see next section). More broadly, substantial evidence of climate change strongly suggests that the probability distribution is also changing and that managing flood risks in the future will become even more difficult. The simplest implication of this is that not all of the historical data are, or can be, considered valid as input into the extreme event analysis.\n\nWhen these assumptions are violated there is an \"unknown\" amount of uncertainty introduced into the reported value of what the 100-year flood means in terms of rainfall intensity, or flood depth. When all of the inputs are known the uncertainty can be measured in the form of a confidence interval. For example, one might say there is a 95% chance that the 100-year flood is greater than X, but less than Y.\n\nDirect statistical analysis to estimate the 100-year riverine flood is possible only at the relatively few locations where an annual series of maximum instantaneous flood discharges has been recorded. In the United States as of 2014, taxpayers have supported such records for at least 60 years at fewer than 2,600 locations, for at least 90 years at fewer than 500, and for at least 120 years at only 11. For comparison, the total area of the nation is about , so there are perhaps 3,000 stream reaches that drain watersheds of and 300,000 reaches that drain . In urban areas, 100-year flood estimates are needed for watersheds as small as . For reaches without sufficient data for direct analysis, 100-year flood estimates are derived from indirect statistical analysis of flood records at other locations in a hydrologically similar region or from other hydrologic models. Similarly for coastal floods, tide gauge data exist for only about 1,450 sites worldwide, of which only about 950 added information to the global data center between January 2010 and March 2016.\nMuch longer records of flood elevations exist at a few locations around the world, such as the Danube River at Passau, Germany, but they must be evaluated carefully for accuracy and completeness before any statistical interpretation.\n\nFor an individual stream reach, the uncertainties in any analysis can be large, so 100-year flood estimates have large individual uncertainties for most stream reaches. For the largest recorded flood at any specific location, or any potentially larger event, the recurrence interval always is poorly known. Spatial variability adds more uncertainty, because a flood peak observed at different locations on the same stream during the same event commonly represents a different recurrence interval at each location. If an extreme storm drops enough rain on one branch of a river to cause a 100-year flood, but no rain falls over another branch, the flood wave downstream from their junction might have a recurrence interval of only 10 years. Conversely, a storm that produces a 25-year flood simultaneously in each branch might form a 100-year flood downstream. During a time of flooding, news accounts necessarily simplify the story by reporting the greatest damage and largest recurrence interval estimated at any location. The public can easily and incorrectly conclude that the recurrence interval applies to all stream reaches in the flood area.\n\nPeak elevations of 14 floods as early as 1501 on the Danube River at Passau, Germany, reveal great variability in the actual intervals between floods. Flood events greater than the 50-year flood occurred at intervals of 4 to 192 years since 1501, and the 50-year flood of 2002 was followed only 11 years later by a 500-year flood. Only half of the intervals between 50- and 100-year floods were within 50 percent of the nominal average interval. Similarly, the intervals between 5-year floods during 1955 to 2007 ranged from 5 months to 16 years, and only half were within 2.5 to 7.5 years.\nIn the United States, the 100-year flood provides the risk basis for flood insurance rates. Complete information on the National Flood Insurance Program (NFIP) is available here. A \"regulatory flood\" or \"base flood\" is routinely established for river reaches through a science-based rule-making process targeted to a 100-year flood at the historical average recurrence interval. In addition to historical flood data, the process accounts for previously established regulatory values, the effects of flood-control reservoirs, and changes in land use in the watershed. Coastal flood hazards have been mapped by a similar approach that includes the relevant physical processes. Most areas where serious floods can occur in the United States have been mapped consistently in this manner. On average nationwide, those 100-year flood estimates are well sufficient for the purposes of the NFIP and offer reasonable estimates of future flood risk, if the future is like the past. Approximately 3% of the U.S. population lives in areas subject to the 1% annual chance coastal flood hazard.\n\nIn theory, removing homes and businesses from areas that flood repeatedly can protect people and reduce insurance losses, but in practice it is difficult for people to retreat from established neighborhoods.\n\n\n"}
{"id": "4327451", "url": "https://en.wikipedia.org/wiki?curid=4327451", "title": "Adolf Dygasiński", "text": "Adolf Dygasiński\n\nAdolf Dygasiński (March 7, 1839, Niegosławice–June 3, 1902, Grodzisk Mazowiecki) was a Polish novelist, publicist and educator. In Polish literature, he was one of the leading representatives of Naturalism. \nDuring his literary career, Dygasiński wrote forty-two short stories and novels. \nSince 1884 his works were being published in book-form and enjoyed considerable success. \nThey were translated into Russian and German. \nIn 1891, Dygasiński went on a trip to Brazil on a trail of Polish emigrants from Partitioned Poland. \nHe produced a series of letters describing the tragic fate of Polish emigrees in South America. In the following years Dygasiński maintained a position of a tutor and coach for numerous wealthy landowning families. Late in life he settled in Warsaw, where he died on June 6, 1902, and was buried at the local Powązkowski Cemetery.\n\nIn his work Dygasiński often focused on topics of rural life and residents of small towns, highlighting the common fate of both, human and animal communities. Some of his most important work include: \n\n\n\n"}
{"id": "1260420", "url": "https://en.wikipedia.org/wiki?curid=1260420", "title": "Anti-predator adaptation", "text": "Anti-predator adaptation\n\nAnti-predator adaptations are mechanisms developed through evolution that assist prey organisms in their constant struggle against predators. Throughout the animal kingdom, adaptations have evolved for every stage of this struggle, namely by avoiding detection, warding off attack, fighting back, or escaping when caught.\n\nThe first line of defence consists in avoiding detection, through mechanisms such as camouflage, masquerade, apostatic selection, living underground, or nocturnality. \n\nAlternatively, prey animals may ward off attack, whether by advertising the presence of strong defences in aposematism, by mimicking animals which do possess such defences, by startling the attacker, by signalling to the predator that pursuit is not worthwhile, by distraction, by using defensive structures such as spines, and by living in a group. Members of groups are at reduced risk of predation, despite the increased conspicuousness of a group, through improved vigilance, predator confusion, and the likelihood that the predator will attack some other individual.\n\nSome prey species are capable of fighting back against predators, whether with chemicals, through communal defence, or by ejecting noxious materials. Many animals can escape by fleeing rapidly, outrunning or outmanoeuvring their attacker. \n\nFinally, some species are able to escape even when caught by sacrificing certain body parts: crabs can shed a claw, while lizards can shed their tails, often distracting predators long enough to permit the prey to escape.\n\nAnimals may avoid becoming prey by living out of sight of predators, whether in caves, underground, or by being nocturnal. Nocturnality is an animal behavior characterized by activity during the night and sleeping during the day. This is a behavioral form of detection avoidance called crypsis used by animals to either avoid predation or to enhance prey hunting. Predation risk has long been recognized as critical in shaping behavioral decisions. For example, this predation risk is of prime importance in determining the time of evening emergence in echolocating bats. Although early access during brighter times permits easier foraging, it also leads to a higher predation risk from bat hawks and bat falcons. This results in an optimum evening emergence time that is a compromise between the conflicting demands.\nAnother nocturnal adaptation can be seen in kangaroo rats, which avoid moonlight. They forage in relatively open habitats and reduce their activity outside their nest burrows in response to moonlight. During a full moon, they shift their activity towards areas of relatively dense cover to compensate for the extra brightness.\n\nCamouflage uses any combination of materials, coloration, or illumination for concealment to make the organism hard to detect by sight. It is common in both terrestrial and marine animals. Camouflage can be achieved in many different ways, such as through resemblance to surroundings, disruptive coloration, shadow elimination by countershading or counter-illumination, self-decoration, cryptic behavior, or changeable skin patterns and colour. Animals such as the flat-tail horned lizard of North America have evolved to eliminate their shadow and blend in with the ground. The bodies of these lizards are flattened, and their sides thin towards the edge. This body form, along with the white scales fringed along their sides, allows the lizards to effectively hide their shadows. In addition, these lizards hide any remaining shadows by pressing their bodies to the ground.\n\nAnimals can hide in plain sight by masquerading as inedible objects. For example, the potoo, a South American bird, habitually perches on a tree, convincingly resembling a broken stump of a branch, while a butterfly, \"Kallima\", looks just like a dead leaf.\n\nAnother way to remain unattacked in plain sight is to look different from other members of the same species. Predators such as tits selectively hunt for abundant types of insect, ignoring less common types that were present, forming search images of the desired prey. This creates a mechanism for negative frequency-dependent selection, apostatic selection.\n\nMany species make use of behavioral strategies to deter predators.\n\nMany weakly-defended animals, including moths, butterflies, mantises, phasmids, and cephalopods such as octopuses, make use of patterns of threatening or startling behaviour, such as suddenly displaying conspicuous eyespots, so as to scare off or momentarily distract a predator, thus giving the prey animal an opportunity to escape. In the absence of toxins or other defences, this is essentially bluffing, in contrast to aposematism which involves honest signals.\n\nPursuit-deterrent signals are behavioral signals used by prey that convince predators not to pursue them. For example, gazelles stot, jumping high with stiff legs and an arched back. This is thought to signal to predators that they have a high level of fitness and can outrun the predator. As a result, predators may choose to pursue a different prey that is less likely to outrun them.\nWhite-tailed deer and other prey mammals flag with conspicuous (often black and white) tail markings when alarmed, informing the predator that it has been detected.\nWarning calls given by birds such as the Eurasian jay are similarly honest signals, benefiting both predator and prey: the predator is informed that it has been detected and might as well save time and energy by giving up the chase, while the prey is protected from attack.\n\nAnother pursuit-deterrent signal is thanatosis or playing dead. Thanatosis is a form of bluff in which an animal mimics its own dead body, feigning death to avoid being attacked by predators seeking live prey. Thanatosis can also be used by the predator in order to lure prey into approaching.\nAn example of this is seen in white-tailed deer fawns, which experience a drop in heart rate in response to approaching predators. This response, referred to as \"alarm bradycardia\", causes the fawn's heart rate to drop from 155 to 38 beats per minute within one beat of the heart. This drop in heart rate can last up to two minutes, causing the fawn to experience a depressed breathing rate and decrease in movement, called tonic immobility. Tonic immobility is a reflex response that causes the fawn to enter a low body position that simulates the position of a dead corpse. Upon discovery of the fawn, the predator loses interest in the \"dead\" prey. Other symptoms of alarm bradycardia, such as salivation, urination, and defecation, can also cause the predator to lose interest.\n\nMarine molluscs such as sea hares, cuttlefish, squid and octopuses give themselves a last chance to escape by distracting their attackers. To do this, they eject a mixture of chemicals, which may mimic food or otherwise confuse predators. In response to a predator, animals in these groups release ink, creating a cloud, and opaline, affecting the predator's feeding senses, causing it to attack the cloud.\n\nDistraction displays attract the attention of predators away from an object, typically the nest or young, that is being protected. Distraction displays are performed by some species of birds, which may feign a broken wing while hopping about on the ground, and by some species of fish.\n\nMimicry occurs when an organism (the mimic) simulates signal properties of another organism (the model) to confuse a third organism. This results in the mimic gaining protection, food, and mating advantages. There are two classical types of defensive mimicry: Batesian and Müllerian. Both involve aposematic coloration, or warning signals, to avoid being attacked by a predator.\n\nIn Batesian mimicry, a palatable, harmless prey species mimics the appearance of another species that is noxious to predators, thus reducing the mimic's risk of attack. This form of mimicry is seen in many insects. The idea behind Batesian mimicry is that predators that have tried to eat the unpalatable species learn to associate its colors and markings with an unpleasant taste. This results in the predator learning to avoid species displaying similar colours and markings, including Batesian mimics, which are in effect parasitic on the chemical or other defences of the unprofitable models. Some species of octopus can mimic a selection of other animals by changing their skin color, skin pattern and body motion. When a damselfish attacks an octopus, the octopus mimics a banded sea-snake. The model chosen varies with the octopus's predator and habitat. Most of these octopuses use Batesian mimicry, selecting an organism repulsive to predators as a model.\n\nIn Müllerian mimicry, two or more aposematic forms share the same warning signals, as in viceroy and monarch butterflies. Birds avoid eating both species because their wing patterns honestly signal their unpleasant taste.\n\nMany animals are protected against predators with armour in the form of hard shells (such as most molluscs), leathery or scaly skin (as in reptiles), or tough chitinous exoskeletons (as in arthropods).\n\nA spine is a sharp, needle-like structure used to inflict pain on predators. An example of this seen in nature is in the Sohal surgeonfish. These fish have a sharp scalpel-like spine on the front of each of their tail fins, able to inflict deep wounds. The area around the spines is often brightly colored to advertise the defensive capability; predators often avoid the Sohal surgeonfish. Defensive spines may be detachable, barbed or poisonous. Porcupine spines are long, stiff, break at the tip, and are barbed to stick into a would-be predator. In contrast, the hedgehog's short spines, which are modified hairs, readily bend, and are barbed into the body, so they are not easily lost; they may be jabbed at an attacker.\n\nMany species of slug caterpillar, Limacodidae, have numerous protuberances and stinging spines along their dorsal surfaces. Species that possess these stinging spines suffer less predation than larvae that lack them, and a predator, the paper wasp, chooses larvae without spines when given a choice.\n\nGroup living can decrease the risk of predation to the individual in a variety of ways, as described below.\n\nA dilution effect is seen when animals living in a group \"dilute\" their risk of attack, each individual being just one of many in the group. George C. Williams and W.D. Hamilton proposed that group living evolved because it provides benefits to the individual rather than to the group as a whole, which becomes more conspicuous as it becomes larger. One common example is the shoaling of fish. Experiments provide direct evidence for the decrease in individual attack rate seen with group living, for example in Camargue horses in Southern France. The horse-fly often attacks these horses, sucking blood and carrying diseases. When the flies are most numerous, the horses gather in large groups, and individuals are indeed attacked less frequently. Water striders are insects that live on the surface of fresh water, and are attacked from beneath by predatory fish. Experiments varying the group size of the water striders showed that the attack rate per individual water strider decreases as group size increases.\n\nThe selfish herd theory was proposed by W.D. Hamilton to explain why animals seek central positions in a group. The theory's central idea is to reduce the individual's domain of danger. A domain of danger is the area within the group in which the individual is more likely to be attacked by a predator. The center of the group has the lowest domain of danger, so animals are predicted to strive constantly to gain this position. Testing Hamilton's selfish herd effect, Alta De Vos and Justin O'Rainn (2010) studied brown fur seal predation from great white sharks. Using decoy seals, the researchers varied the distance between the decoys to produce different domains of danger. The seals with a greater domain of danger had an increased risk of shark attack.\n\nA radical strategy for avoiding predators which may otherwise kill a large majority of the emerging young of a population is to emerge very rarely, at irregular intervals. This strategy is seen in dramatic form in the periodical cicadas, which emerge at intervals of 13 or 17 years. Predators with a life-cycle of one or a few years are unable to reproduce rapidly enough in response to such an emergence, so predator satiation is a likely evolutionary explanation for the cicadas' unusual life-cycle, though not the only one. Predators may still feast on the emerging cicadas, but are unable to consume more than a fraction of the brief surfeit of prey.\n\nAnimals that live in groups often give alarm calls that give warning of an attack. For example, vervet monkeys give different calls depending on the nature of the attack: for an eagle, a disyllabic cough; for a leopard or other cat, a loud bark; for a python or other snake, a \"chutter\". The monkeys hearing these calls respond defensively, but differently in each case: to the eagle call, they look up and run into cover; to the leopard call, they run up into the trees; to the snake call, they stand on two legs and look around for snakes, and on seeing the snake, they sometimes mob it. Similar calls are found in other species of monkey, while birds also give different calls that elicit different responses.\n\nIn the improved vigilance effect, groups are able to detect predators sooner than solitary individuals. For many predators, success depends on surprise. If the prey is alerted early in an attack, they have an improved chance of escape. For example, wood pigeon flocks are preyed upon by goshawks. Goshawks are less successful when attacking larger flocks of wood pigeons than they are when attacking smaller flocks. This is because the larger the flock size, the more likely it is that one bird will notice the hawk sooner and fly away. Once one pigeon flies off in alarm, the rest of the pigeons follow. Wild ostriches in Tsavo National Park in Kenya feed either alone or in groups of up to four birds. They are subject to predation by lions. As the ostrich group size increases, the frequency at which each individual raises its head to look for predators decreases. Because ostriches are able to run at speeds that exceed those of lions for great distances, lions try to attack an ostrich when its head is down. By grouping, the ostriches present the lions with greater difficulty in determining how long the ostriches' heads stay down. Thus, although individual vigilance decreases, the overall vigilance of the group increases.\n\nIndividuals living in large groups may be safer from attack because the predator may be confused by the large group size. As the group moves, the predator has greater difficulty targeting an individual prey animal. The zebra has been suggested by the zoologist Martin Stevens and his colleagues as an example of this. When stationary, a single zebra stands out because of its large size. To reduce the risk of attack, zebras often travel in herds. The striped patterns of all the zebras in the herd may confuse the predator, making it harder for the predator to focus in on an individual zebra. Furthermore, when moving rapidly, the zebra stripes create a confusing, flickering motion dazzle effect in the eye of the predator.\n\nDefensive structures such as spines may be used both to ward off attack as already mentioned, and if need be to fight back against a predator. Methods of fighting back include chemical defences, mobbing, defensive regurgitation, and suicidal altruism.\n\nMany prey animals, and to defend against seed predation also seeds of plants, make use of poisonous chemicals for self-defence. These may be concentrated in surface structures such as spines or glands, giving an attacker a taste of the chemicals before it actually bites or swallows the prey animal: many toxins are bitter-tasting. A last-ditch defence is for the animal's flesh itself to be toxic, as in the puffer fish, danaid butterflies and burnet moths. Many insects acquire toxins from their food plants; \"Danaus\" caterpillars accumulate toxic cardenolides from milkweeds (Asclepiadaceae). \n\nSome prey animals are able to eject noxious materials to deter predators actively. The bombardier beetle has specialized glands on the tip of its abdomen that allows it to direct a toxic spray towards predators. The spray is generated explosively through oxidation of hydroquinones and is sprayed at a temperature of 100 °C. Armoured crickets similarly release blood at their joints when threatened (autohaemorrhaging). Several species of grasshopper including \"Poecilocerus pictus\", \"Parasanaa donovani\", \"Aularches miliaris\", and \"Tegra novaehollandiae\" secrete noxious liquids when threatened, sometimes ejecting these forcefully. Spitting cobras accurately squirt venom from their fangs at the eyes of potential predators, striking their target eight times out of ten, and causing severe pain. Termite soldiers in the Nasutitermitinae have a fontanellar gun, a gland on the front of their head which can secrete and shoot an accurate jet of resinous terpenes \"many centimeters\". The material is sticky and toxic to other insects. One of the terpenes in the secretion, pinene, functions as an alarm pheromone. Seeds deter predation with combinations of toxic non-protein amino acids, cyanogenic glycosides, protease and amylase inhibitors, and phytohemaglutinins.\n\nA few vertebrate species such as the Texas horned lizard are able to shoot squirts of blood from their eyes, by rapidly increasing the blood pressure within the eye sockets, if threatened. Because an individual may lose up to 53% of blood in a single squirt, this is only used against persistent predators like foxes, wolves and coyotes (Canidae), as a last defence. Canids often drop horned lizards after being squirted, and attempt to wipe or shake the blood out of their mouths, suggesting that the fluid has a foul taste; they choose other lizards if given the choice, suggesting a learned aversion towards horned lizards as prey.\n\nThe slime glands along the body of the hagfish secrete enormous amounts of mucus when it is provoked or stressed. The gelatinous slime has dramatic effects on the flow and viscosity of water, rapidly clogging the gills of any fish that attempt to capture hagfish; predators typically release the hagfish within seconds \"(pictured above)\". Common predators of hagfish include seabirds, pinnipeds and cetaceans, but few fish, suggesting that predatory fish avoid hagfish as prey.\n\nIn communal defence, prey groups actively defend themselves by grouping together, and sometimes by attacking or mobbing a predator, rather than allowing themselves to be passive victims of predation. Mobbing is the harassing of a predator by many prey animals. Mobbing is usually done to protect the young in social colonies. For example, red colobus monkeys exhibit mobbing when threatened by chimpanzees, a common predator. The male red colobus monkeys group together and place themselves between predators and the group's females and juveniles. The males jump together and actively bite the chimpanzees. Fieldfares are birds which may nest either solitarily or in colonies. Within colonies, fieldfares mob and defecate on approaching predators, shown experimentally to reduce predation levels.\n\nSome birds and insects use defensive regurgitation to ward off predators. The northern fulmar vomits a bright orange, oily substance called stomach oil when threatened. The stomach oil is made from their aquatic diets. It causes the predator's feathers to mat, leading to the loss of flying ability and the loss of water repellency. This is especially dangerous for aquatic birds because their water repellent feathers protect them from hypothermia when diving for food.\n\nEuropean roller chicks vomit a bright orange, foul smelling liquid when they sense danger. This repels prospective predators and may alert their parents to danger: they respond by delaying their return.\n\nNumerous insects utilize defensive regurgitation. The eastern tent caterpillar regurgitates a droplet of digestive fluid to repel attacking ants. Similarly, larvae of the noctuid moth regurgitate when disturbed by ants. The vomit of noctuid moths has repellent and irritant properties that help to deter predator attacks.\n\nAn unusual type of predator deterrence is observed in the Malaysian exploding ant. Social hymenoptera rely on altruism to protect the entire colony, so the self-destructive acts benefit all individuals in the colony. When a worker ant's leg is grasped, it suicidally expels the contents of its hypertrophied submandibular glands, expelling corrosive irritant compounds and adhesives onto the predator. These prevent predation and serve as a signal to other enemy ants to stop predation of the rest of the colony.\n\nThe normal reaction of a prey animal to an attacking predator is to flee by any available means, whether flying, gliding, falling, swimming, running, jumping, burrowing or rolling, according to the animal's capabilities. Escape paths are often erratic, making it difficult for the predator to predict which way the prey will go next: for example, birds such as snipe, ptarmigan and black-headed gulls evade fast raptors such as peregrine falcons with zigzagging or jinking flight. In the tropical rain forests of Southeast Asia in particular, many vertebrates escape predators by falling and gliding. Among the insects, many moths turn sharply, fall, or perform a powered dive in response to the sonar clicks of bats. Among fish, the stickleback follows a zigzagging path, often doubling back erratically, when chased by a fish-eating merganser duck.\n\nSome animals are capable of autotomy (self-amputation), shedding one of their own appendages in a last-ditch attempt to elude a predator's grasp or to distract the predator and thereby allow escape. The lost body part may be regenerated later. Certain sea slugs discard stinging papillae; arthropods such as crabs can sacrifice a claw, which can be regrown over several successive moults; among vertebrates, many geckos and other lizards shed their tails when attacked: the tail goes on writhing for a while, distracting the predator, and giving the lizard time to escape; a smaller tail slowly regrows.\n\nAristotle recorded observations (around 350 BC) of the antipredator behaviour of cephalopods in his \"History of Animals\", including the use of ink as a distraction, camouflage, and signalling.\n\nIn 1940, Hugh Cott wrote a compendious study of camouflage, mimicry, and aposematism, \"Adaptive Coloration in Animals\".\n\n\n\n"}
{"id": "563239", "url": "https://en.wikipedia.org/wiki?curid=563239", "title": "Biogenic substance", "text": "Biogenic substance\n\nA biogenic substance is a product made by or of life forms. The term encompasses constituents, secretions, and metabolites of plants or animals. In context of molecular biology, biogenic substances are referred to as biomolecules. \n\n\nAn abiogenic substance or process does not result from the present or past activity of living organisms. Abiogenic products may, e.g., be minerals, other inorganic compounds, as well as simple organic compounds (e.g. extraterrestrial methane, see also abiogenesis).\n\n"}
{"id": "4290647", "url": "https://en.wikipedia.org/wiki?curid=4290647", "title": "Biological naturalism", "text": "Biological naturalism\n\nBiological naturalism is a theory about, among other things, the relationship between consciousness and body (i.e. brain), and hence an approach to the mind–body problem. It was first proposed by the philosopher John Searle in 1980 and is defined by two main theses: 1) all mental phenomena from pains, tickles, and itches to the most abstruse thoughts are caused by lower-level neurobiological processes in the brain; and 2) mental phenomena are higher-level features of the brain.\n\nThis entails that the brain has the right causal powers to produce intentionality. However, Searle's biological naturalism does not entail that brains and \"only\" brains can cause consciousness. Searle is careful to point out that while it appears to be the case that certain brain functions are sufficient for producing conscious states, our current state of neurobiological knowledge prevents us from concluding that they are necessary for producing consciousness. In his own words:\n\n\"The fact that brain processes cause consciousness does not imply that only brains can be conscious. The brain is a biological machine, and we might build an artificial machine that was conscious; just as the heart is a machine, and we have built artificial hearts. Because we do not know exactly how the brain does it we are not yet in a position to know how to do it artificially.\" (Biological Naturalism, 2004)\n\nSearle denies Cartesian dualism, the idea that the mind is a separate kind of substance to the body, as this contradicts our entire understanding of physics, and unlike Descartes, he does not bring God into the problem. Indeed, Searle denies any kind of dualism, the traditional alternative to monism, claiming the distinction is a mistake. He rejects the idea that because the mind is not objectively viewable, it does not fall under the rubric of physics.\n\nSearle believes that consciousness \"is a real part of the real world and it cannot be eliminated in favor of, or reduced to, something else\" whether that something else is a neurological state of the brain or a computer program. He contends, for example, that the software known as Deep Blue \"knows\" nothing about chess. He also believes that consciousness is both a cause of events in the body and a response to events in the body.\n\nOn the other hand, Searle doesn't treat consciousness as a ghost in the machine. He treats it, rather, as a state of the brain. The causal interaction of mind and brain can be described thus in naturalistic terms: Events at the micro-level (perhaps at that of individual neurons) cause consciousness. Changes at the macro-level (the whole brain) constitute consciousness. Micro-changes cause and then are impacted by holistic changes, in much the same way that individual football players cause a team (as a whole) to win games, causing the individuals to gain confidence from the knowledge that they are part of a winning team.\n\nHe articulates this distinction by pointing out that the common philosophical term 'reducible' is ambiguous. Searle contends that consciousness is \"causally reducible\" to brain processes without being \"ontologically reducible\". He hopes that making this distinction will allow him to escape the traditional dilemma between reductive materialism and substance dualism; he affirms the essentially physical nature of the universe by asserting that consciousness is completely caused by and realized in the brain, but also doesn't deny what he takes to be the obvious facts that humans really are conscious, and that conscious states have an essentially first-person nature.\n\nIt can be tempting to see the theory as a kind of property dualism, since, in Searle's view, a person's mental properties are categorically different from his or her micro-physical properties. The latter have \"third-person ontology\" whereas the former have \"first-person ontology.\" Micro-structure is accessible objectively by any number of people, as when several brain surgeons inspect a patient's cerebral hemispheres. But pain or desire or belief are accessible subjectively by the person who has the pain or desire or belief, and no one else has that mode of access. However, Searle holds mental properties to be a species of physical property—ones with first-person ontology. So this sets his view apart from a dualism of physical and non-physical properties. His mental properties are putatively physical.\n\nThere have been several criticisms of Searle's idea of biological naturalism.\n\nJerry Fodor suggests that Searle gives us no account at all of exactly \"why\" he believes that a biochemistry like, or similar to, that of the human brain is indispensable for intentionality. Fodor thinks that it seems much more plausible to suppose that it is the way in which an organism (or any other system for that matter) is connected to its environment that is indispensable in the explanation of intentionality. It is easier to see \"how the fact that my thought is causally connected to a tree might bear on its being a thought about a tree. But it's hard to imagine how the fact that (to put it crudely) my thought is made out of hydrocarbons could matter, except on the unlikely hypothesis that only hydrocarbons can be causally connected to trees in the way that brains are.\" \n\nJohn Haugeland takes on the central notion of some set of special \"right causal powers\" that Searle attributes to the biochemistry of the human brain. He asks us to imagine a concrete situation in which the \"right\" causal powers are those that our neurons have to reciprocally stimulate one another. In this case, silicon-based alien life forms can be intelligent just in case they have these \"right\" causal powers; i.e. they possess neurons with synaptics connections that have the power to reciprocally stimulate each other. Then we can take any speaker of the Chinese language and cover his neurons in some sort of wrapper which prevents them from being influenced by neurotransmitters and, hence, from having the right causal powers. At this point, \"Searle's demon\" (an English speaking nanobot, perhaps) sees what is happening and intervenes: he sees through the covering and determines which neurons would have been stimulated and which not and proceeds to stimulate the appropriate neurons and shut down the others himself. The experimental subject's behavior is unaffected. He continues to speak perfect Chinese as before the operation but now the causal powers of his neurotransmitters have been replaced by someone who does not understand the Chinese language. The point is generalizable: for any causal powers, it will always be possible to hypothetically replace them with some sort of Searlian demon which will carry out the operations mechanically. His conclusion is that Searle's is necessarily a dualistic view of the nature of causal powers, \"not intrinsically connected with the actual powers of physical objects.\" \n\nSearle himself actually does not rule out the possibility for alternate arrangements of matter bringing forth consciousness other than biological brains. He also disputes that Biological naturalism is dualistic in nature in a brief essay entitled \"Why I Am Not a Property Dualist\".\n\n\n\n"}
{"id": "8553751", "url": "https://en.wikipedia.org/wiki?curid=8553751", "title": "Biological organisation", "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organizational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organization scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.\n\nThe theoretical foundations are summarized by thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organization of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n"}
{"id": "665333", "url": "https://en.wikipedia.org/wiki?curid=665333", "title": "Cymdeithas Edward Llwyd", "text": "Cymdeithas Edward Llwyd\n\nCymdeithas Edward Llwyd (English: Edward Llwyd Society) is a Welsh natural history organization whose name commemorates the great Welsh natural historian, geographer and linguist Edward Llwyd.\n\nThe Cymdeithas Edward Llwyd organizes regular country walks throughout Wales in sites of interest of the Welsh environment, including SSI's & post-industrial landscapes. These are Welsh-language walking groups, although learners are just as welcome.\n\nThey also organize a variety of Nature & Environmental activities, including lectures, publications on Welsh Nature & Environment & conservation work.\n\n"}
{"id": "27233680", "url": "https://en.wikipedia.org/wiki?curid=27233680", "title": "Decomposed granite", "text": "Decomposed granite\n\nDecomposed granite is classification of rock that is derived from granite via its weathering to the point that the parent material readily fractures into smaller pieces of weaker rock. Further weathering yields material that easily crumbles into a mixtures of gravel-sized particles known as grus, that in turn may break down to produce a mixture of clay and silica sand or silt particles. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite. It has practical uses that include its incorporation into paving and driveway materials, residential gardening materials in arid environments, as well as various types of walkways and heavy-use paths in parks. Different colors of decomposed granite are available, deriving from the natural range of granite colors from different quarry sources, and admixture of other natural and synthetic materials can extend the range of decomposed granite properties.\n\nDecomposed granite is rock of granitic origin that has weathered to the point that it readily fractures into smaller pieces of weak rock. Further weathering produces rock that easily crumbles into mixtures of gravel-sized particles, sand, and silt-sized particles with some clay. Eventually, the gravel may break down to produce a mixture of silica sand, silt particles, and clay. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite.\n\nThe parent granite material is a common type of igneous rock that is granular, with its grains large enough to be distinguished with the unaided eye (i.e., it is phaneritic in texture); it is composed of plagioclase feldspar, orthoclase feldspar, quartz, mica, and possibly other minerals. The chemical transformation of feldspar, one of the primary constituents of granite, into the clay mineral kaolin is one of the important weathering processes. The presence of clay allows water to seep in and further weaken the rock allowing it to fracture or crumble into smaller particles, where, ultimately, the grains of silica produced from the granite are relatively resistant to weathering, and may remain almost unaltered.\n\nDecomposed granite, as a crushed stone form, is used as a pavement building material. It is used on driveways, garden walkways, bocce courts and pétanque terrains, and urban, regional, and national park walkways and heavy-use paths. DG can be installed and compacted to meet handicapped accessibility specifications and criteria, such as the ADA standards in the U.S. Different colors are available based on the various natural ranges available from different quarry sources, and polymeric stabilizers and other additives can be included to change the properties of the natural material. Decomposed granite is also sometimes used as a component of soil mixtures for cultivating bonsai.\n\n"}
{"id": "312308", "url": "https://en.wikipedia.org/wiki?curid=312308", "title": "Dirac sea", "text": "Dirac sea\n\nThe Dirac sea is a theoretical model of the vacuum as an infinite sea of particles with negative energy. It was first postulated by the British physicist Paul Dirac in 1930 to explain the anomalous negative-energy quantum states predicted by the Dirac equation for relativistic electrons. The positron, the antimatter counterpart of the electron, was originally conceived of as a hole in the Dirac sea, well before its experimental discovery in 1932.\n\nUpon solving the free Dirac equation,\n\none finds\n\nwhere\n\nfor plane wave solutions with -momentum . This is a direct consequence of the relativistic energy-momentum relation\n\nupon which the Dirac equation is built. The quantity is a constant column vector and is a normalization constant. The quantity is called the \"time evolution factor\", and its interpretation in similar roles in, for example, the plane wave solutions of the Schrödinger equation, is the energy of the wave (particle). This interpretation is not immediately available here since it may acquire negative values. A similar situation prevails for the Klein–Gordon equation. In that case, the \"absolute value\" of can be interpreted as the energy of the wave since in the canonical formalism, waves with negative actually have \"positive\" energy . But this is not the case with the Dirac equation. The energy in the canonical formalism associated with negative is .\n\nIn hole theory, the solutions with negative time evolution factors are reinterpreted as representing the positron, discovered by Carl Anderson. The interpretation of this result requires a Dirac sea, showing that the Dirac equation is not merely a combination of special relativity and quantum mechanics, but it also implies that the number of particles cannot be conserved.\n\nThe origins of the Dirac sea lie in the energy spectrum of the Dirac equation, an extension of the Schrödinger equation that is consistent with special relativity, that Dirac had formulated in 1928. Although the equation was extremely successful in describing electron dynamics, it possesses a rather peculiar feature: for each quantum state possessing a positive energy \"E\", there is a corresponding state with energy \"-E\". This is not a big difficulty when an isolated electron is considered, because its energy is conserved and negative-energy electrons may be left out. However, difficulties arise when effects of the electromagnetic field are considered, because a positive-energy electron would be able to shed energy by continuously emitting photons, a process that could continue without limit as the electron descends into lower and lower energy states. Real electrons clearly do not behave in this way.\n\nDirac's solution to this was to turn to the Pauli exclusion principle. Electrons are fermions, and obey the exclusion principle, which means that no two electrons can share a single energy state within an atom. Dirac hypothesized that what we think of as the \"vacuum\" is actually the state in which \"all\" the negative-energy states are filled, and none of the positive-energy states. Therefore, if we want to introduce a single electron we would have to put it in a positive-energy state, as all the negative-energy states are occupied. Furthermore, even if the electron loses energy by emitting photons it would be forbidden from dropping below zero energy.\n\nDirac also pointed out that a situation might exist in which all the negative-energy states are occupied except one. This \"hole\" in the sea of negative-energy electrons would respond to electric fields as though it were a positively charged particle. Initially, Dirac identified this hole as a proton. However, Robert Oppenheimer pointed out that an electron and its hole would be able to annihilate each other, releasing energy on the order of the electron's rest energy in the form of energetic photons; if holes were protons, stable atoms would not exist. Hermann Weyl also noted that a hole should act as though it has the same mass as an electron, whereas the proton is about two thousand times heavier. The issue was finally resolved in 1932 when the positron was discovered by Carl Anderson, with all the physical properties predicted for the Dirac hole.\n\nDespite its success, the idea of the Dirac sea tends not to strike people as very elegant. The existence of the sea implies an infinite positive electric charge filling all of space. In order to make any sense out of this, one must assume that the \"bare vacuum\" must have an infinite negative charge density which is exactly cancelled by the Dirac sea. Since the absolute energy density is unobservable—the cosmological constant aside—the infinite energy density of the vacuum does not represent a problem. Only changes in the energy density are observable. Geoffrey Landis (author of \"Ripples In The Dirac Sea\", a hard science fiction short story) also notes that Pauli exclusion does not definitively mean that a filled Dirac sea cannot accept more electrons, since, as Hilbert elucidated, a sea of infinite extent can accept new particles even if it is filled. This happens when we have a chiral anomaly and a gauge instanton.\n\nThe development of quantum field theory (QFT) in the 1930s made it possible to reformulate the Dirac equation in a way that treats the positron as a \"real\" particle rather than the absence of a particle, and makes the vacuum the state in which no particles exist instead of an infinite sea of particles. This picture is much more convincing, especially since it recaptures all the valid predictions of the Dirac sea, such as electron-positron annihilation. On the other hand, the field formulation does not eliminate all the difficulties raised by the Dirac sea; in particular the problem of the vacuum possessing infinite energy.\n\nThe Dirac sea interpretation and the modern QFT interpretation are related by what may be thought of as a very simple Bogoliubov transformation, an identification between the creation and annihilation operators of two different free field theories. In the modern interpretation, the field operator for a Dirac spinor is a sum of creation operators and annihilation operators, in a schematic notation:\n\nAn operator with negative frequency lowers the energy of any state by an amount proportional to the frequency, while operators with positive frequency raise the energy of any state.\n\nIn the modern interpretation, the positive frequency operators add a positive energy particle, adding to the energy, while the negative frequency operators annihilate a positive energy particle, and lower the energy. For a Fermionic field, the creation operator formula_6 gives zero when the state with momentum k is already filled, while the annihilation operator formula_7 gives zero when the state with momentum k is empty.\n\nBut then it is possible to reinterpret the annihilation operator as a \"creation\" operator for a negative energy particle. It still lowers the energy of the vacuum, but in this point of view it does so by creating a negative energy object. This reinterpretation only affects the philosophy. To reproduce the rules for when annihilation in the vacuum gives zero, the notion of \"empty\" and \"filled\" must be reversed for the negative energy states. Instead of being states with no antiparticle, these are states that are already filled with a negative energy particle.\n\nThe price is that there is a nonuniformity in certain expressions, because replacing annihilation with creation adds a constant to the negative energy particle number. The number operator for a Fermi field is:\n\nwhich means that if one replaces N by 1-N for negative energy states, there is a constant shift in quantities like the energy and the charge density, quantities that count the total number of particles. The infinite constant gives the Dirac sea an infinite energy and charge density. The vacuum charge density should be zero, since the vacuum is Lorentz invariant, but this is artificial to arrange in Dirac's picture. The way it is done is by passing to the modern interpretation.\n\nDirac's idea is more directly applicable to solid state physics, where the valence band in a solid can be regarded as a \"sea\" of electrons. Holes in this sea indeed occur, and are extremely important for understanding the effects of semiconductors, though they are never referred to as \"positrons\". Unlike in particle physics, there is an underlying positive charge — the charge of the ionic lattice — that cancels out the electric charge of the sea.\n\nDirac's original concept of a sea of particles was revived in the theory of causal fermion systems, a recent proposal for a unified physical theory. In this approach, the problems of the infinite vacuum energy and infinite charge density of the Dirac sea disappear because these divergences drop out of the physical equations formulated via the causal action principle. These equations do not require a preexisting space-time, making it possible to realize the concept that space-time and all structures therein arise as a result of the collective interaction of the sea states with each other and with the additional particles and \"holes\" in the sea.\n\n\n\n"}
{"id": "52303418", "url": "https://en.wikipedia.org/wiki?curid=52303418", "title": "Dynamic scaling", "text": "Dynamic scaling\n\nDynamic scaling (sometimes known as Family-Vicsek scaling) is the litmus test of showing that an evolving system exhibits self-similarity. In general a function is said to exhibit dynamic scaling if it satisfies:\n\nHere the exponent formula_2 is fixed by the dimensional requirement formula_3. \nNow, the numerical value of formula_4 should remain invariant despite the unit of measurement of formula_5 is changed by some factor since formula_6 is a dimensionless quantity. However, Tamás Vicsek and Fereydoon Family first proposed the idea of dynamic scaling in the context of diffusion-limited aggregation DLA of clusters in two dimensions. The form of their proposal for dynamic scaling was:\n\nMany phenomena which physicists often investigate are not static but rather evolve probabilistically with time. The universe is perhaps one of the best examples which is expanding ever since the Big Bang. Similarly, growth of networks like the Internet are also ever growing systems. Another example is polymer degradation where degradation does not occur in a blink of an eye but rather over quite a long time. Spread of biological and computer viruses too does not happen over night. Many of these evolves in a self-similar fashion in the sense that data obtained from the snapshot at any fixed time is similar to the respective data taken from the snapshot of any earlier or later time. That is, the system is similar to itself at different times. The litmus test of such self-similarity is provided by the dynamic scaling.\n\nIn such system we find certain stochastic variable formula_8 which assume values that depend on time. In such cases, we are often interested to know the distribution of formula_8 at various instants of time i.e. formula_10. Now the numerical value of formula_11 and the typical or mean value of formula_8 may well be very different at every different instant measurement. The question is: What happens to the corresponding dimensionless variables? If the numerical values of the dimensional quantities are different, however, corresponding dimensionless quantities remain invariant then we can argue that the snapshot of the system at different times are similar. When this happens we conclude that the system is self-similar. \n\nOne way of verifying the dynamic scaling is to plot dimensionless variables formula_13 as a function of formula_14 of the data extracted at various different time. Then if all the plots of formula_11 vs formula_8 obtained at different times collapse onto a single universal curve then it is said that the systems at different time are similar and it obeys dynamic scaling. The idea of data collapse is deeply rooted to the Buckingham formula_17 theorem. Essentially such systems can be termed as temporal self-similarity since the same system is similar at different times.\n\nThere have many seemingly disparate systems which are found to exhibit dynamic scaling e.g., kinetics of aggregation described by Smoluchowski coagulation equation, complex network described by Barabasi–Albert model, kinetic and stochastic Cantor set. The growth model within the Kardar–Parisi–Zhang (KPZ) class, one find that the width of the surface formula_18 exhibits dynamic scaling. The area size distribution of the blocks of weighted planar stochastic lattice (WPSL) too exhibits dynamic scaling.\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "38103101", "url": "https://en.wikipedia.org/wiki?curid=38103101", "title": "Earth pyramids of Platten", "text": "Earth pyramids of Platten\n\nThe earth pyramids of Platten (German: \"Erdpyramiden von Platten\" or \"Erdpyramiden bei Oberwielenbach\"; ) are earth pyramids located in Platten in the municipality of Percha, near Bruneck in South Tyrol, Italy.\n\nThe erosion area is located at an altitude of 1550 to 1750 meters. What is so impressive about the pyramids of Platten is their wildness which reminds at the same time also of their fragility.\n\nThe pyramids of Platten belong to the most beautiful natural monuments of South Tyrol such as the earth pyramids of Ritten and part of the earth pyramids of South Tyrol.\n\nThey were described in a scientific manner for the first time by Karl Meusburger in 1914 .\n\nFollowing a cloudburst, a few centuries ago, there came a landslide which cut off the roads connecting the villages in the surroundings of Aschbach.\n\nIn 1882, again after a heavy cloudburst, a new fault was formed. Following eluviations and erosions these earth\npyramids are constantly changing; that is due to the succession of severe cold spells in winter and hot summers which have the effect of continually forming new ones.\n\nFrom the main road we deter at Percha/Percia and drive less than 6 km up into Oberwielenbach and beyond it a bit more to the big parking place. That road continues to the hamlet of Platten, but before reaching it, another narrow mountain road deters left, heading towards the Gönner Alm (alpine pasture). On the crossroads there's the inscription that this mountain road leads also towards the pyramids, but it is closed for public traffic (2017). So, it's best to start the walk up to the pyramids before, on the big parking place, 1430 m, or eventually later, from Platten.\n\nFrom the big parking place, 1430 m, a good, marked path goes up into the woods. First it ascends a bit, then does a lot of crossing the slopes towards the right. When we reach out of the woods, hitting the before mentioned mountain road, we follow the road only to the big left turn. From there we go into the forest again and in a few minutes we reach the big ravine with the pyramids. Some 30 minutes till here.\n\nTo return, it is best to use the approach route, even if we descend along the ravine to the lower lookout point. From there a path goes further down, but it descends more than needed and makes the return a bit longer.\n\n"}
{"id": "38103099", "url": "https://en.wikipedia.org/wiki?curid=38103099", "title": "Earth pyramids of Ritten", "text": "Earth pyramids of Ritten\n\nThe earth pyramids of Ritten (German: \"Erdpyramiden am Ritten\"; ) are a natural monument that is located on the Ritten, a plateau not far from Bolzano in northern Italy. The earth pyramids of South Tyrol are a fairly widespread phenomenon which are existing in various locations.\nThe original name in this area for these earth pyramids is \"Lahntürme\", i.e. landslide towers. They are rather unusual formations of their kind which originate from morainic rocks of glacial origin. The columns of the pyramids may be more or less elongated, and the higher they are the thinner they get, ending usually with a stone cover. These earth pyramids are not static, they are constantly evolving, because their life cycle foresees a continuous erosion, or even a final collapse leaving room for new formations.\n\nIn South Tyrol there are other natural monuments like this such as the earth pyramids of Platten, but the ones of Ritten are considered the parents of them all.\n\n"}
{"id": "212485", "url": "https://en.wikipedia.org/wiki?curid=212485", "title": "Earth religion", "text": "Earth religion\n\nEarth religion is a term used mostly in the context of neopaganism.\n\nEarth-centered religion or nature worship is a system of religion based on the veneration of natural phenomena. It covers any religion that worships the earth, nature, or fertility deity, such as the various forms of goddess worship or matriarchal religion. Some find a connection between earth-worship and the Gaia hypothesis. Earth religions are also formulated to allow one to utilize the knowledge of preserving the earth.\n\nAccording to Marija Gimbutas, pre-Indo-European societies lived in small-scale, family-based communities that practiced matrilineal succession and goddess-centered religion where creation comes from the woman. She is the Divine Mother who can give life and take it away. In Irish mythology she is Danu, in Slavic mythology she is Mat Zemlya, and in other cultures she is Pachamama, Ninsun, Terra Mater, Nüwa, Matres or Shakti.\n\nIn the late 1800s, James Weir wrote an article describing the beginnings and aspects of early religious feeling. According to Boyer, early man was forced to locate food and shelter in order to survive, while constantly being directed by his instincts and senses. Because man's existence depended on nature, men began to form their religion and beliefs on and around nature itself. It is evident that man's first religion would have had to develop from the material world, he argues, because man relied heavily on his senses and what he could see, touch, and feel. In this sense, the worship of nature formed, allowing man to further depend on nature for survival.\n\nNeopagans have tried to make claims that religion started in ways that correspond to earth religion. In one of their published works, \"The Urantia Book\", another reason for this worship of nature came from a fear of the world around primitive man. His mind lacked the complex function of processing and sifting through complex ideas. As a result, man worshiped the very entity that surrounded him every day. That entity was nature. Man experienced the different natural phenomena around him, such as storms, vast deserts, and immense mountains. Among the very first parts of nature to be worshiped were rocks and hills, plants and trees, animals, the elements, heavenly bodies, and even man himself. As primitive man worked his way through nature worship, he eventually moved on to incorporate spirits into his worship. Although these claims may have some merit, they are nonetheless presented from a biased position that cannot be authenticated by traditional and reliable sources. Therefore, their claims can not be relied upon.\n\nThe origins of religion can be looked at through the lens of the function and processing of the human mind. Pascal Boyer suggests that, for the longest period of time, the brain was thought of as a simple organ of the body. However, he claims that the more information collected about the brain indicates that the brain is indeed not a \"blank slate.\" Humans do not just learn any information from the environment and surroundings around them. They have acquired sophisticated cognitive equipment that prepares them to analyze information in their culture and determine which information is relevant and how to apply it. Boyer states that \"having a normal human brain does not imply that you have religion. All it implies is that people can acquire it, which is very different.\" He suggests that religions started for the reasons of providing answers to humans, giving comfort, providing social order to society, and satisfying the need of the illusion-prone nature of the human mind. Ultimately, religion came into existence because of our need to answer questions and hold together our societal order.\n\nAn additional idea on the origins of religion comes not from man's cognitive development, but from the ape. Barbara J. King argues that human beings have an emotional connection with those around them, and that that desire for a connection came from their evolution from apes. The closest relative to the human species is the African ape. At birth, the ape begins negotiating with its mother about what it wants and needs in order to survive. The world the ape is born into is saturated with close family and friends. Because of this, emotions and relationships play a huge role in the ape's life. Its reactions and responses to one another are rooted and grounded in a sense of belongingness, which is derived from its dependence on the ape's mother and family. Belongingness is defined as \"mattering to someone who matters to you ... getting positive feelings from our relationships.\" This sense and desire for belongingness, which started in apes, only grew as the hominid (a human ancestor) diverged from the lineage of the ape, which occurred roughly six to seven million years ago.\n\nAs severe changes in the environment, physical evolutions in the human body (especially in the development of the human brain), and changes in social actions occurred, humans went beyond trying to simply form bonds and relationships of empathy with others. As their culture and society became more complex, they began using practices and various symbols to make sense of the natural and spiritual world around them. Instead of simply trying to find belongingness and empathy from the relationships with others, humans created and evolved God and spirits in order to fulfil that need and exploration. King argued that \"an earthly need for belonging led to human religious imagination and thus to the otherworldly realm of relating to God, gods, and spirits.\"\n\nThe term \"earth religion\" encompasses any religion that worships the earth, nature or fertility gods or goddesses. There is an array of groups and beliefs that fall under earth religion, such as paganism, which is a polytheistic, nature based religion; animism, which is the worldview that all living entities (plants, animals, and humans) possess a spirit; Wicca, who hold the concept of an earth mother goddess as well as practice ritual magic; and druidism, which equates divinity with the natural world.\n\nAnother perspective of earth religion to consider is pantheism, which takes a varied approach to the importance and purpose of the earth, and man's relationship with the planet. Several of their core statements deal with the connectivity humans share with the planet, declaring that \"all matter, energy, and life are an interconnected unity of which we are an inseparable part\" and \"we are an integral part of Nature, which we should cherish, revere and preserve in all its magnificent beauty and diversity. We should strive to live in harmony with Nature locally and globally\".\n\nThe earth also plays a vital role to many Voltaic peoples, many of whom \"consider the Earth to be Heaven’s wife\", such as the Konkomba of northern Ghana, whose economic, social and religious life is heavily influenced by the earth. It is also important to consider various Native American religions, such as Peyote Religion, Longhouse Religion, and Earth Lodge Religion.\n\nApril 22 was established as International Mother Earth Day by the United Nations in 2009, but many cultures around the world have been celebrating the Earth for thousands of years. Winter solstice and Summer solstice are celebrated with holidays like Yule and Dongzhi in the winter and Tiregān and Kupala in the summer.\n\nAnimism is practiced among the Bantu peoples of Sub-Saharan Africa. The Dahomey mythology has deities like Nana Buluku, Gleti, Mawu, Asase Yaa, Naa Nyonmo and Xevioso.\n\nIn Baltic mythology, the sun is a female deity, Saule, a mother or a bride, and Mēness is the moon, father or husband, their children being the stars. In Slavic mythology Mokosh and Mat Zemlya together with Perun head up the pantheon. Celebrations and rituals are centered on nature and harvest seasons. Dragobete is a traditional Romanian spring holiday that celebrates \"the day when the birds are betrothed.\"\n\nIn Hindu philosophy, the yoni is the creative power of nature and the origin of life. In Shaktism, the yoni is celebrated and worshipped during the Ambubachi Mela, an annual fertility festival which celebrates the Earth's menstruation.\n\nAlthough the idea of earth religion has been around for thousands of years, it did not fully show up in popular culture until the early 1990s. \"The X-Files\" was one of the first nationally broadcast television programs to air witchcraft and Wicca (types of earth religion) content. On average, Wiccans - those who practice Wicca - were more or less pleased with the way the show had portrayed their ideals and beliefs. However, they still found it to be a little \"sensationalistic\". That same year, the movie \"The Craft\" was released - also depicting the art of Wicca. Unfortunately, this cinematic feature was not as happily accepted as \"The X-Files\" had been.\n\nA few years later, programs showcasing the aforementioned religious practices - such as \"Charmed\" and \"Buffy the Vampire Slayer\" - became widely popular. Although \"Charmed\" focused mostly on witchcraft, the magic they practiced very closely resembled Wicca. Meanwhile, \"Buffy\" was one of the first shows to actually cast a Wiccan character. However, since the shows focus was primarily on vampires, the Wiccan was depicted as having supernatural powers, rather than being in-tuned with the Earth.\n\nOther movies and shows throughout the last few decades have also been placed under the genre of Earth Religion. Among them are two of director Hayao Miyazaki's most well known films - \"Princess Mononoke\" and \"My Neighbor Totoro\". Both movies present human interaction with land, animal, and other nature spirits. Speakers for Earth Religion have said that these interactions suggest overtones of Earth Religion themes.\n\nSome popular Disney movies have also been viewed as Earth Religion films. Among them are \"The Lion King\" and \"Brother Bear\". Those who practice Earth Religion view \"The Lion King\" as an Earth Religion film mainly for the \"interconnectedness\" and \"Circle of Life\" it shows between the animals, plants, and life in general. When that link is broken, viewers see chaos and despair spread throughout the once bountiful land. Congruently, \"Brother Bear\" portrays interactions and consequences when humans disobey or go against the animal and Earth spirits.\n\nOther earth religion movies include \"The 13th Warrior\", \"The Deceivers (film)\", \"Sorceress (1982 film)\", \"Anchoress (film)\", \"Eye of the Devil\", \"Agora (film)\", and \"The Wicker Man (1973 film)\". These movies all contain various aspects of earth religion and nature worship in general.\n\nMany religions have negative stereotypes of earth religion and neo-paganism in general. A common critique of the worship of nature and resources of \"Mother Earth\" is that the rights of nature and ecocide movements are inhibitors of human progress and development. This argument is fueled by the fact that those people socialized into 'western' world views believe the earth itself is not a living being. Wesley Smith believes this is “anti-humanism with the potential to do real harm to the human family.” According to Smith, earth worshipers are hindering large-scale development, and they are viewed as inhibitors of advancement.\n\nA lot of criticism of earth religion comes from the negative actions of a few people who have been chastised for their actions. One such negative representative of earth religion is Aleister Crowley. He is believed to be \"too preoccupied with awakening magical powers\" instead of putting the well-being of others in his coven. Crowley allegedly looked up to \"Old George\" Pickingill, who was another worshipper of nature who was viewed negatively. Critics regarded Pickingill as a Satanist and \"England’s most notorious Witch\".\n\nCrowley himself was \"allegedly expelled from the Craft because he was a pervert.\" He became aroused by torture and pain, and enjoyed being \"punished\" by women. This dramatically damaged Crowley’s public image, because of his lifestyle and actions. Many people regarded all followers of earth religion as perverted Satanists.\n\nFollowers of earth religion have suffered major opprobrium over the years for allegedly being Satanists. Some religious adherents can be prone to viewing religions other than their religion as being wrong sometimes because they perceive those religions as characteristic of their concept of Satan worship. To wit, Witchcraft, a common practice of Wiccans, is sometimes misinterpreted as Satan worship by members of these groups, as well as less-informed persons who may not be specifically religious but who may reside within the sphere-of-influence of pagan-critical religious adherents. From the Wiccan perspective, however, earth religion and Wicca lie outside of the phenomenological world that encompasses Satanism. An all-evil being does not exist within the religious perspective of western earth religions. Devotees worship and celebrate earth resources and earth-centric deities. Satanism and Wicca \"have entirely different beliefs about deity, different rules for ethical behavior, different expectations from their membership, different views of the universe, different seasonal days of celebration, etc.\"\n\nNeo-pagans, or earth religion followers, often claim to be unaffiliated with Satanism. Neo-pagans, Wiccans, and earth religion believers do not acknowledge the existence of a deity that conforms to the common Semitic sect religious concept of Satan. Satanism stems from Christianity, while earth religion stems from older religious concepts.\n\nSome earth religion adherents take issue with the religious harassment that is inherent in the social pressure that necessitates their having to distance themselves from the often non-uniform, Semitic sect religious concept of Satan worship. Having to define themselves as \"other\" from a religious concept that is not within their worldview implies a certain degree of outsider-facilitated, informal, but functional religious restriction that is based solely on the metaphysical and mythological religious beliefs of those outsiders. This is problematic because outsider initiated comparisons to Satanism with the intent of condemnation, even when easily refuted, can have the effect of social pressure on earth religion adherents to conform to outsider perception of acceptable customs, beliefs, and modes of religious behavior.\n\nTo illustrate, a problem could arise with the \"other\" than Satanism argument if an earth centered belief system adopted a holiday that a critic considered to be similar or identical to a holiday that Satanists celebrate. Satanists have historically been prone to adopting holidays that have origins in various pagan traditions, ostensibly because these traditional holidays are amongst the last known vestiges of traditional pre-Semitic religious practice in the west. Satanists are, perhaps irrationally, prone to interpreting non-Semitic holidays as anti-Christian and therefore as implicitly representative of their worldview. This is not surprising given the fact that this is, in fact, how many Christians interpret holidays such as Samhain. In spite of any flawed perceptions or rationale held by any other group, earth centered religion adherents do not recognize misinterpretation of their customs made by outside religious adherents or critics inclusive of Satan worshippers.\n\nOrganized Satan worship, as defined by and anchored in the Semitic worldview, is characterized by a relatively disorganized and often disparate series of movements and groups that mostly emerged in the mid-20th century. Thus, their adopted customs have varied, continue to vary, and therefore this moving target of beliefs and customs can not be justifiably nor continuously accounted for by earth centered religious adherents. Once a Satanist group adopts a holiday, social stigma may unjustifiably taint the holiday and anyone who observes it without discrimination as to whence and for what purpose it was originally celebrated. Given these facts, many earth centered religion devotees find comparisons to Satanism intrinsically oppressive in nature. This logic transfers to any and all religious customs to include prayer, magic, ceremony, and any unintentional similarity in deity characteristics (an example is the horned traditional entity Pan having similar physical characteristics to common horned depictions of Satan).\n\nThe issue is further complicated by the theory that the intra and extra-biblical mythology of Satan that is present throughout various Semitic sects may have originally evolved to figuratively demonize the heathen religions of other groups. Thus, the concept of Satan, or \"the adversary\", would have been representative of all non-Semitic religions and, by extension, the people who believed in them. Although, at times, the concept of the \"other\" as demonic has also been used to characterize competing Semitic sects. Amongst other purposes, such belief would have been extraordinarily useful during the psychological and physical process of cleansing Europe of traditional tribal beliefs in favor of Christianity. This possibility would account for the historical tendency of Christian authorities, for example, to deem most pagan customs carried out in the pagan religious context as demonic. By any modern standard, such current beliefs would violate western concepts of religious tolerance as well as be inimical to the preservation of what remains of the culture of long-persecuted religious groups.\n\nBecause of the vast diversity of religions that fall under the title of \"earth religion\" there is no consensus of beliefs. However, the ethical beliefs of most religions overlap. The most well-known ethical code is the Wiccan Rede. Many of those who practice an earth religion choose to be environmentally active. Some perform activities such as recycling or composting while others feel it to be more productive to try and support the earth spiritually. These six beliefs about ethics seem to be universal.\n\n\"An [if] it harm none, do what ye will.\" Commonly worded in modern English as \"if it doesn't harm anyone, do what you want.\" This maxim was first printed in 1964, after being spoken by the priestess Doreen Valiente in the mid-20th century, and governs most ethical belief of Wiccans and some Pagans. There is no consensus of beliefs but this rede provides a starting point for most people's interpretation of what is ethical. The rede clearly states to do no harm but what constitutes as harm and what level of self-interest is acceptable is negotiable. Many Wiccans reverse the phrase into \"Do what ye will an it harm none,\" meaning \"Do what you want if it doesn't harm anyone.\" The difference may not seem significant but it is. The first implies that it is good to do no harm but does not say that it is necessarily unethical to do so, the second implies that all forms of harm are unethical. The second phrase is nearly impossible to follow. This shift occurred when trying to better adapt the phrase into modern English as well as to stress the \"harmlessness\" of Wiccans. The true nature of the rede simply implies that there is personal responsibility for your actions. You may do as you wish but there is a karma reaction from every action. Even though this is the most well-known rede of practice, it does not mean that those that choose not to follow it are unethical. There are many other laws of practice that other groups follow.\n\nThe Threefold Law is the belief that for all actions there is always a cause and effect. For every action taken either the good or ill intention will be returned to the action taker threefold. This is why the Wiccan Rede is typically followed because of fear of the threefold return from that harmful action.\n\nThis term is what Emma Restall Orr calls reverence for the earth in her book \"Living with Honour: A Pagan Ethics\". She separates the term into three sections: courage, generosity and loyalty, or honesty, respect and responsibility. There is no evil force in Nature. Nothing exists beyond the natural, therefore it is up to the individual to choose to be ethical not because of divine judgment. All beings are connected by the earth and so all should be treated fairly. There is a responsibility toward the environment and a harmony should be found with nature.\n\nThe following was written by the Church of All Worlds in 1988 and was affirmed by the Pagan Ecumenical Conferences of Ancient Ways (California, May 27–30) and Pagan Spirit Gathering (Wisconsin, June 17). The Pagan Community Council of Ohio then presented it to the Northeast Council of W.I.C.C.A.\n\n\"We, the undersigned, as adherents of Pagan and Old and Neo-Pagan Earth Religions, including Wicca or Witchcraft, practice a variety of positive, life affirming faiths that are dedicated to healing, both of ourselves and of the Earth. As such, we do not advocate or condone any acts that victimize others, including those proscribed by law. As one of our most widely accepted precepts is the Wiccan Rede's injunction to \"harm none,\" we absolutely condemn the practices of child abuse, sexual abuse and any other form of abuse that does harm to the bodies, minds or spirits of the victims of such abuses. We recognize and revere the divinity of Nature in our Mother the Earth, and we conduct our rites of worship in a manner that is ethical, compassionate and constitutionally protected. We neither acknowledge or worship the Christian devil, \"Satan,\" who is not in our Pagan pantheons. We will not tolerate slander or libel against our Temples, clergy or Temple Assemblers and we are prepared to defend our civil rights with such legal action as we deem necessary and appropriate.\"\n"}
{"id": "43400705", "url": "https://en.wikipedia.org/wiki?curid=43400705", "title": "Ekeby oak tree", "text": "Ekeby oak tree\n\nThe Ekeby oak tree () is an oak tree in Ekerö outside Stockholm, Sweden, close to Ekebyhov Castle. It is the largest living deciduous tree in Sweden by volume. \n\nThe Ekeby oak is approximately 500 years old. It was declared a natural monument in 1956. There are many old trees around Ekebyhov Castle; the oak, sometimes called \"Ekeröjätten\" (the Ekerö giant) stands alone in a field south of the castle, where it had no competition for space from other trees. It was measured in 2008 as the largest tree by volume in Sweden.\n"}
{"id": "1800265", "url": "https://en.wikipedia.org/wiki?curid=1800265", "title": "Emergy", "text": "Emergy\n\nEmergy is the amount of energy that was consumed in direct and indirect transformations to make a product or service. Emergy is a measure of quality differences between different forms of energy. Emergy is an expression of all the energy used in the work processes that generate a product or service in units of one type of energy. Emergy is measured in units of \"emjoule\"s, a unit referring to the available energy consumed in transformations. Emergy accounts for different forms of energy and resources (e.g. sunlight, water, fossil fuels, minerals, etc.) Each form is generated by transformation processes in nature and each has a different ability to support work in natural and in human systems. The recognition of these quality differences is a key concept.\n\nThe theoretical and conceptual basis for the emergy methodology is grounded in thermodynamics, general system theory and systems ecology. Evolution of the theory by Howard T. Odum over the first thirty years is reviewed in \"Environmental Accounting\" and in the volume edited by C. A. S. Hall titled \"Maximum Power\".\n\nBeginning in the 1950s, Odum analyzed energy flow in ecosystems (\"e.g.\" Silver Springs, Florida; Enewetak atoll in the south Pacific; Galveston Bay, Texas and Puerto Rican rainforests, amongst others) where energies in various forms at various scales were observed. His analysis of energy flow in ecosystems, and the differences in the potential energy of sunlight, fresh water currents, wind and ocean currents led him to make the suggestion that when two or more different energy sources drive a system, they cannot be added without first converting them to a common measure that accounts for their differences in energy quality. This led him to introduce the concept of \"energy of one kind\" as a common denominator with the name \"energy cost\". He then expanded the analysis to model food production in the 1960s, and in the 1970s to fossil fuels.\n\nOdum's first formal statement of what would later be termed emergy was in 1973:\nEnergy is measured by calories, btu's, kilowatthours, and other intraconvertable units, but energy has a scale of quality which is not indicated by these measures. The ability to do work for man depends on the energy quality and quantity and this is measurable by the amount of energy of a lower quality grade required to develop the higher grade. The scale of energy goes from dilute sunlight up to plant matter, to coal, from coal to oil, to electricity and up to the high quality efforts of computer and human information processing.\n\nIn 1975, he introduced a table of \"Energy Quality Factors\", kilocalories of sunlight energy required to make a kilocalorie of a higher quality energy, the first mention of the energy hierarchy principle which states that \"energy quality is measured by the energy used in the transformations\" from one type of energy to the next.\n\nThese energy quality factors, were placed on a fossil-fuel basis and called \"Fossil Fuel Work Equivalents\" (FFWE), and the quality of energies were measured based on a fossil fuel standard with rough equivalents of 1 kilocalorie of fossil fuel equal to 2000 kilocalories of sunlight. \"Energy quality ratios\" were computed by evaluating the quantity of energy in a transformation process to make a new form and were then used to convert different forms of energy to a common form, in this case fossil fuel equivalents. FFWE's were replaced with coal equivalents (CE) and by 1977, the system of evaluating quality was placed on a solar basis and termed solar equivalents (SE).\n\nThe term \"embodied energy\" was used for a time in the early 1980s to refer to energy quality differences in terms of their costs of generation, and a ratio called a \"quality factor\" for the calories (or joules) of one kind of energy required to make those of another. However, since the term embodied energy was used by other groups who were evaluating the fossil fuel energy required to generate products and were not including all energies or using the concept to imply quality, embodied energy was dropped in favor of \"embodied solar calories\", and the quality factors became known as \"transformation ratios\".\n\nUse of the term \"embodied energy\" for this concept was modified in 1986 when David Scienceman, a visiting scholar at the University of Florida from Australia, suggested the term \"emergy\" and \"emjoule\" or \"emcalorie\" as the unit of measure to distinguish emergy units from units of available energy. The term transformation ratio was shortened to transformity in about the same time. It is important to note that throughout this twenty years the baseline or the basis for evaluating forms of energy and resources shifted from organic matter, to fossil fuels and finally to solar energy.\n\nAfter 1986, the emergy methodology continued to develop as the community of scientists expanded and as new applied research into combined systems of humans and nature presented new conceptual and theoretical questions. The maturing of the emergy methodology resulted in more rigorous definitions of terms and nomenclature and refinement of the methods of calculating transformities. The International Society for the Advancement of Emergy Research and a biennial International Conference at the University of Florida support this research.\n\nEmergy— amount of energy of one form that is used in transformations directly and indirectly to make a product or service. The unit of emergy is the emjoule or emergy joule. Using emergy, sunlight, fuel, electricity, and human service can be put on a common basis by expressing each of them in the emjoules of solar energy that is required to produce them. If solar emergy is the baseline, then the results are solar emjoules (abbreviated seJ). Although other baselines have been used, such as coal emjoules or electrical emjoules, in most cases emergy data are given in solar emjoules.\n\nUnit Emergy Values (UEVs) — the emergy required to generate one unit of output. Types of UEVs:\n\nEmergy accounting converts the thermodynamic basis of all forms of energy, resources and human services into equivalents of a single form of energy, usually solar. To evaluate a system, a system diagram organizes the evaluation and account for energy inputs and outflows. A table of the flows of resources, labor and energy is constructed from the diagram and all flows are evaluated. The final step involves interpreting the results.\n\nIn some cases, an evaluation is done to determine the fit of a development proposal within its environment. It also allows comparison of alternatives. Another purpose is to seek the best use of resources to maximize economic vitality.\n\nSystem diagrams show the inputs that are evaluated and summed to obtain the emergy of a flow. A diagram of a city and its regional support area is shown in Figure 1.\n\nA table (see example below) of resource flows, labor and energy is constructed from the diagram. Raw data on inflows that cross the boundary are converted into emergy units, and then summed to obtain total emergy supporting the system. Energy flows per unit time (usually per year) are presented in the table as separate line items.\n\nAll tables are followed by footnotes that show citations for data and calculations.\n\nThe table allows a unit emergy value to be calculated. The final, output row (row “O” in the example table above) is evaluated first in units of energy or mass. Then the input emergy is summed and the unit emergy value is calculated by dividing the emergy by the units of the output.\n\nFigure 2 shows non-renewable environmental contributions (N) as an emergy storage of materials, renewable environmental inputs (R), and inputs from the economy as purchased (F) goods and services. Purchased inputs are needed for the process to take place and include human service and purchased non-renewable energy and material brought in from elsewhere (fuels, minerals, electricity, machinery, fertilizer, etc.). Several ratios, or indices are given in Figure 2 that assess the global performance of a process.\n\nOther ratios are useful depending on the type and scale of the system under evaluation.\n\nThe recognition of the relevance of energy to the growth and dynamics of complex systems has resulted in increased emphasis on environmental evaluation methods that can account for and interpret the effects of matter and energy flows at all scales in systems of humanity and nature. The following table lists some general areas in which the emergy methodology has been employed.\n\nThe concept of emergy has been controversial within academe including ecology, thermodynamics and economy. Emergy theory has been criticized for allegedly offering an energy theory of value to replace other theories of value. The stated goal of emergy evaluations is to provide an \"ecocentric\" valuation of systems, processes. Thus it does not purport to replace economic values but to provide additional information, from a different point of view.\n\nThe idea that a calorie of sunlight is not equivalent to a calorie of fossil fuel or electricity strikes many as absurd, based on the 1st Law definition of energy units as measures of heat (i.e. Joule's mechanical equivalent of heat). Others have rejected the concept as impractical since from their perspective it is impossible to objectively quantify the amount of sunlight that is required to produce a quantity of oil. In combining systems of humanity and nature and evaluating environmental input to economies, mainstream economists criticize the emergy methodology for disregarding market values.\n\n"}
{"id": "7900498", "url": "https://en.wikipedia.org/wiki?curid=7900498", "title": "Energy being", "text": "Energy being\n\nAn energy being or astral being is a theoretical life form that is composed of energy rather than matter. They appear in myths/legends, paranormal/UFO accounts, and in various works of speculative fiction.\n\nEnergy beings are typically rendered as a translucent glowing fluid or as a collection of flames or electrical sparks or bolts; somewhat in common with the representations of ghosts.\n\nEnergy beings have a variety of capacities. The Taelons (from \"\") are barely more powerful than mortals, while others such as \"Star Trek\"s Q, \"Stargate SG-1\"s Ascended Ancients/Ori, \"\"s Anodites, or the Meekrob from \"Invader Zim\" possess god-like powers.\n\n\n"}
{"id": "44689684", "url": "https://en.wikipedia.org/wiki?curid=44689684", "title": "Energy informatics", "text": "Energy informatics\n\nEnergy Informatics is founded on flow networks that are the major suppliers and consumers of energy. Their efficiency can be improved by collecting and analyzing information.\nEnergy informatics is a research field covering the use of information and communication technology to address energy challenges. Methods used for \"smart\" implementations often combine sensors with artificial intelligence and machine learning.\n\nThe field among other consider application areas within:\n\n\n"}
{"id": "500948", "url": "https://en.wikipedia.org/wiki?curid=500948", "title": "Field guide", "text": "Field guide\n\nA field guide is a book designed to help the reader identify wildlife (plants or animals) or other objects of natural occurrence (e.g. minerals). It is generally designed to be brought into the 'field' or local area where such objects exist to help distinguish between similar objects. Field guides are often designed to help users distinguish animals and plants that may be similar in appearance but are not necessarily closely related.\n\nIt will typically include a description of the objects covered, together with paintings or photographs and an index. More serious and scientific field identification books, including those intended for students, will probably include identification keys to assist with identification, but the publicly accessible field guide is more often a browsable picture guide organized by family, colour, shape, location or other descriptors.\n\nPopular interests in identifying things in nature probably were strongest in bird and plant guides. Perhaps the first popular field guide to plants in the United States was the 1893 \"How to Know the Wildflowers\" by \"Mrs. William Starr Dana\" (Frances Theodora Parsons). In 1890, Florence Merriam published \"Birds Through an Opera-Glass\", describing 70 common species. Focused on living birds observed in the field, the book is considered the first in the tradition of modern, illustrated bird guides. In 1902, now writing as Florence Merriam Bailey (having married the zoologist Vernon Bailey), she published \"Handbook of Birds of the Western United States\". By contrast, the \"Handbook\" is designed as a comprehensive reference for the lab rather a portable book for the field. It was arranged by taxonomic order and had clear descriptions of species size, distribution, feeding, and nesting habits.\n\nFrom this point into the 1930s, features of field guides were introduced by Chester A. Reed and others such as changing the size of the book to fit the pocket, including colour plates, and producing guides in uniform editions that covered subjects such as garden and woodland flowers, mushrooms, insects, and dogs.\n\nIn 1934, Roger Tory Peterson, using his fine skill as an artist, changed the way modern field guides approached identification. Using color plates with paintings of similar species together – and marked with arrows showing the differences – people could use his bird guide in the field to compare species quickly to make identification easier. This technique, the \"Peterson Identification System\", was used in most of Peterson's Field Guides from animal tracks to seashells and has been widely adopted by other publishers and authors as well.\n\nToday, each field guide has its own range, focus and organization. Specialist publishers such as Croom Helm, along with organisations like the Audubon Society, the RSPB, the Field Studies Council, National Geographic, HarperCollins, and many others all produce quality field guides.\n\nIt is somewhat difficult to generalise about how field guides are intended to be used, because this varies from one guide to another, partly depending on how expert the targeted reader is expected to be.\n\nFor general public use, the main function of a field guide is to help the reader identify a bird, plant, rock, butterfly or other natural object down to at least the popular naming level. To this end some field guides employ simple keys and other techniques: the reader is usually encouraged to scan illustrations looking for a match, and to compare similar-looking choices using information on their differences. Guides are often designed to first lead readers to the appropriate section of the book, where the choices are not so overwhelming in number.\n\nGuides for students often introduce the concept of identification keys. Plant field guides such as \"Newcomb's Wildflower Guide\" (which is limited in scope to the wildflowers of northeastern North America) frequently have an abbreviated key that helps limit the search. Insect guides tend to limit identification to Order or Family levels rather than individual species, due to their diversity.\n\nMany taxa show variability and it is often difficult to capture the constant features using a small number of photographs. Illustrations by artists or post processing of photographs help in emphasising specific features needed to for reliable identification. Peterson introduced the idea of lines to point to these key features. He also noted the advantages of illustrations over photographs:\n\nField guides aid in improving the state of knowledge of various taxa. By making the knowledge of experienced museum specialists available to amateurs, they increase the gathering of information by amateurs from a wider geographic area and increasing the communication of these findings to the specialists.\n\n"}
{"id": "888727", "url": "https://en.wikipedia.org/wiki?curid=888727", "title": "Flag of Earth", "text": "Flag of Earth\n\nSome individuals and organizations have promoted designs for a flag representing the planet Earth, though none have been officially recognized as such by any governmental body. The most widely recognized flags associated with Earth are the flag of the United Nations and the Earth Day flag. Listed below are some of the unofficial contenders for a Flag of Earth:\n\nA flag designed by John McConnell in 1969 for the first Earth Day is a dark blue field charged with \"The Blue Marble\", a famous NASA photo of the Earth as seen from outer space. The first edition of McConnell's flag used screen-printing and used different colors: ocean and land were blue and the clouds were white. McConnell presented his flag to the United Nations as a symbol for consideration.\n\nBecause of the political views of its creator and its having become a symbol of Earth Day, the flag is associated with environmental awareness, and the celebration of the global community. It was offered for sale originally in the \"Whole Earth Catalog\", and is the only flag which was endorsed by McConnell. \n\n\"The Blue Marble\" image was placed in the public domain, and the public nature of this image was the basis of a legal battle that resulted in the invalidation of a trademark and copyright that was originally issued to the Earth Day flag through its original promotional entity, World Equity, Inc. This does not invalidate the official history of McConnell's flag, only the official documentation that was issued on it.\n\nThe One Flag in Space initiative is an offshoot of the Space Generation Congress (SGC), the Space Generation Advisory Council's yearly world meeting. It promotes usage of the \"Blue Marble\" flag for space exploration (it does not explicitly mention it being McConnell's design).\n\nAdopted in 1946, the flag of the United Nations has been used to indicate world unity, although it technically only represents the United Nations itself. It has a geographical representation of the planet, and its high visibility usage makes it a well-known contender for representing Earth. During the planning for NASA's moon landings of the 1960s, it was suggested that a UN flag be used in place of the flag of the US.\n\nJames William van Kirk, a minister from Youngstown, Ohio, designed in 1913 a peace flag with rainbow stripes, stars and a globe. With this flag, he twice made a peace tour through Europe. The Universal Peace Congress adopted this flag as its World Peace Flag.\n\nDesigned by Pierre de Coubertin in 1914 and adopted in Olympic games since 1920, the Olympic flag represents all mankind and has a white bar with 5 interlocking rings of 5 colors. The five rings represent the five continents. The six colors (including the white color of the background) represent all of the world's nations.\n\nThe World Citizen is a social movement for global citizenship under a proposed world government. In 1953, one of its activists, Garry Davis, founded the World Service Authority, which sells the World Passport (a fantasy travel document) with a proposed flag of the world.\n\nThe astrological (and astronomical) symbol of Earth is another candidate for Earth's flag, often depicted with a blue background, with the symbol at its center.\n\nAnother Earth flag was created around the same time in 1970 by a farmer from Homer, Illinois named James W. Cadle. Cadle's version of the Earth flag consists of a blue circle representing Earth in the center of the flag, a segment of a large yellow circle representing the sun and a small white circle for the moon, all on a black background. It is particularly popular amongst SETI researchers and is used by SETI worldwide. The flag flies at the Ohio State University Radio Observatory and was lowered to half mast when Carl Sagan died. Flag of Earth Co. International was also founded by Cadle which sold the flag. The Flag of Earth became public domain in 2003.\n\nThe World Flag is an international flag created in 1988 by Paul Carroll to act as a symbol to inspire \"positive global change while continuing to embrace and celebrate cultural diversity.\" The current 2008 version of the combined World Flag has a world map 216 flags; including the flags of every UN member state, the United Nations, and several territories of larger nations.\n\nThe World Flag has been flown at the UN Headquarters for the \"A Prayer for Peace\" event, The World Trade Center, Earth Day in Central Park, and at various other events around the world.\n\nIn 2015 a Swedish artist, Oskar Pernefeldt, proposed the \"International Flag of the Planet Earth\". It was conceived to be used in space expeditions and it has two main purposes: \n\nThe creators predict that it will be eventually used in Mars landing in 2025 or in a future colony on that planet. The flag is used by space research groups with intent to implanting a base on Mars. The design of the flag consists of seven rings intersecting each other and a deep-blue-sea in the background. The rings are centered on the flag forming a flower in the middle, representing life on Earth. The intersection of the rings represent that all things on Earth are linked directly or indirectly. The rings are organized in a Borromean rings–like fashion, representing not only the seven continents, but how no part of Earth can be removed without the whole structure collapsing. Finally, the deep-blue represents the ocean and the importance of water for life on Earth.\n\nThe One World Flag is a concept published in 2018 by the German activist and visual artist Thomas Mandl. The design of the flag consists of a blue circle on a translucent blank background. Through the usage of Sheer fabrics, the environment blends into the flag background and thus becomes an integral part of the flag design representing the ever-changing nature of culture, society, politics, and environment of the planet earth. \n\n\n"}
{"id": "13127033", "url": "https://en.wikipedia.org/wiki?curid=13127033", "title": "Flood risk assessment", "text": "Flood risk assessment\n\nA flood risk assessment (FRA) is an assessment of the risk of flooding from all flooding mechanisms, the identification of flood mitigation measures and should provide advice on actions to be taken before and during a flood.\n\nThe sources of water which produce floods include:\n\nFor each of the sources of water, different hydraulic intensities occur. Floods can occur because of a combination of sources of flooding, such as high groundwater and an inadequate surface water drainage system. The topography, hydrogeology and physical attributes of the existing or proposed development need to be considered. A flood risk assessment should be an evaluation of the flood risk and the consequences and impact and vulnerability.\n\n\"Non-professiona\"l flood risk assessments can be produced by members of the public, Architects, environment assessors, or others who are not specifically professionally qualified in this field. However, it is a complex evaluation and such assessments they can be rejected by Authorities as inadequate, or could be considered as negligent in the event of a flooding event, damage and a claim to insurers being made.\n\nIn the UK, the writing of professional flood risk assessments is undertaken by Civil Engineering Consultants. They will have membership of the Institution of Civil Engineers and are bound by their rules of professional conduct. A key requirement is to ensure such professional flood risk assessments are independent to all parties by carrying out their professional duties with complete objectivity and impartiality. Their professional advice should be supported by professional indemnity insurance for such specific professional advice ultimately held with a Lloyd's of London underwriter.\n\nProfessional flood risk assessments can cover single buildings, or whole regions. They can part of a due-diligence process for existing householders or businesses, or can be required in England and Wales to provide independent evidence to a planning application on the flood risk.\n\nIn England and Wales, the Environment Agency requires a professional Flood Risk Assessment (FRA) to be submitted alongside planning applications in areas that are known to be at risk of flooding (within flood zones 2 or 3) and/ or are greater than 1ha in area, planning permission is not usually granted until the FRA has been accepted by the Environment Agency.\n\nFlood Risk Assessments are required to be completed according to the National Planning Policy Framework, which replaces Planning Policy Statement PPS 25: Development and Flood Risk. The initial legislation (PPG25) was introduced in 2001 and subsequently revised.\n\nPPS 25 was designed to \"strengthen and clarify the key role of the planning system in managing flood risk and contributing to adapting to the impacts of climate change.\" and sets out policies for local authorities to ensure flood risk is taken into account during the planning process to prevent inappropriate development in high risk areas and to direct development away from areas at highest risk.\n\nIn its introduction, PPS25 states \"flooding threatens life and causes substantial damage to property [and that] although [it] cannot be wholly prevented, its impacts can be avoided and reduced through good planning and management\".\n\nFor a flood risk assessment to be written, information is needed concerning the existing and proposed developments, the Environment Agency modeled flood levels and topographic levels on site. At its most simple (and cheapest) level an FRA can provide an indication of whether a development will be allowed to take place at a site.\n\nAn initial idea of the risk of fluvial flooding to a local area can be found on the Environment Agency flood map website.\n\nFRAs consist of a detailed analysis of available data to inform the Environment Agency of flood risk at an individual site and also recommend to the developer any mitigation measures. More costly analysis of flood risk can be achieved through detailed flood modelling to challenge the agency's modelled levels and corresponding flood zones.\n\nThe FRA takes into account the risk and impact of flooding on the site, and takes into consideration how the development may affect flooding in the local area. It also includes provides recommendations as to how the risk of flooding to the development can be mitigated.\n\nFRAs should also consider flooding from all sources including fluvial, groundwater, surface water runoff and sewer flooding.\n\nIn 2006, the Planning Service, part of The Department of the Environment, published Planning Policy Statement 15 (PPS15): Planning and flood risk. The guidelines are precautionary and advise against development in flood plains and areas subject to historical flooding. In exceptional cases a FRA can be completed to justify development in flood risk areas. Advice on flood risk assessment is provided to the Planning Service by the Rivers Agency, which is the statutory drainage and flood defence authority for Northern Ireland.\n\nIn 2009, the Department of the Environment, Heritage and Local Government and Office of Public Works published planning guidelines requiring local authorities to apply a sequential approach to flood risk management. The guidelines require that proposed development in flood risk areas must undergo a justification test, consisting of a flood risk assessment.\n\n\n"}
{"id": "2898710", "url": "https://en.wikipedia.org/wiki?curid=2898710", "title": "Hot-filament ionization gauge", "text": "Hot-filament ionization gauge\n\nThe hot-filament ionization gauge, sometimes called a hot-filament gauge or hot-cathode gauge, is the most widely used low-pressure (vacuum) measuring device for the region from 10 to 10 Torr. It is a triode, with the filament being the cathode.\n\n\"Note: Principles are mostly the same for hot-cathode ion sources in particle accelerators to create electrons.\"\n\nA regulated electron current (typically 10 mA) is emitted from a heated filament. The electrons are attracted to the helical grid by a DC potential of about +150 V. Most of the electrons pass through the grid and collide with gas molecules in the enclosed volume, causing a fraction of them to be ionized. The gas ions formed by the electron collisions are attracted to the central ion collector wire by the negative voltage on the collector (typically −30 V). Ion currents are on the order of 1 mA/Pa. This current is amplified and displayed by a high-gain differential amplifier/electrometer.\n\nThis ion current differs for different gases at the same pressure; that is, a hot-filament ionization gauge is composition-dependent. Over a wide range of molecular density, however, the ion current from a gas of constant composition is directly proportional to the molecular density of the gas in the gauge.\n\nA hot-cathode ionization gauge is composed mainly of three electrodes, all acting as a triode, wherein the cathode is the filament. The three electrodes are a collector or plate, a filament, and a grid. The collector current is measured in picoamperes by an electrometer. The filament voltage to ground is usually at a potential of 30 volts, while the grid voltage at 180–210 volts DC, unless there is an optional electron bombardment feature, by heating the grid, which may have a high potential of approximately 565 volts.\nThe most common ion gauge is the hot-cathode Bayard–Alpert gauge, with a small collector inside the grid. A glass envelope with an opening to the vacuum can surround the electrodes, but usually the nude gauge is inserted in the vacuum chamber directly, the pins being fed through a ceramic plate in the wall of the chamber. Hot-cathode gauges can be damaged or lose their calibration if they are exposed to atmospheric pressure or even low vacuum while hot.\n\nElectrons emitted from the filament move several times in back-and-forth movements around the grid before finally entering the grid. During these movements, some electrons collide with a gas molecule to form a pair of an ion and an electron (electron ionization). The number of these ions is proportional to the gas molecule density multiplied by the electron current emitted from the filament, and these ions pour into the collector to form an ion current. Since the gas molecule density is proportional to the pressure, the pressure is estimated by measuring the ion current.\n\nThe low-pressure sensitivity of hot-cathode gauges is limited by the photoelectric effect. Electrons hitting the grid produce X-rays that produce photoelectric noise in the ion collector. This limits the range of older hot-cathode gauges to 10 Torr and the Bayard–Alpert gauges to about 10 Torr. Additional wires at cathode potential in the line of sight between the ion collector and the grid prevent this effect. In the extraction type the ions are not attracted by a wire but by an open cone. As the ions cannot decide which part of the cone to hit, they pass through the hole and form an ion beam. This ion beam can be passed on to a\n\n\n\n"}
{"id": "13680406", "url": "https://en.wikipedia.org/wiki?curid=13680406", "title": "Ion vibration current", "text": "Ion vibration current\n\nThe ion vibration current (IVI) and the associated ion vibration potential is an electric signal that arises when an acoustic wave propagates through a homogeneous fluid.\n\nHistorically, the IVI was the first known electroacoustic phenomenon. It was predicted by Peter Debye in 1933.\n\nWhen a longitudinal sound wave travels through a solvent, the associated pressure gradients push the fluid particles back and forth, and it is easy in practice to create such accelerations that measure thousands or millions of g's. If a solute molecule is more dense or less dense than the surrounding liquid, then in this accelerating environment, the molecule will move relative to the surrounding liquid. This relative motion is essentially the same phenomenon that occurs in a centrifuge, or more simply, it is essentially the same phenomenon that occurs when low-density objects float to the top of a glass of water, and high-density particles sink to the bottom (see the equivalence principle, which states that gravity is just like any other acceleration). The amount of relative motion depends on the balance between the molecule's effective mass (which includes both the mass of the molecule itself and any solvent molecules that are so tightly bound to the molecule that they follow along with the molecule's motion), its effective volume (related to buoyant force), and the viscous drag (friction) between the molecule and the surrounding fluid.\n\nIVI concerns the case where the particles in question are anions and cations. In general, they will have different amounts of motion relative to the fluid during the sound wave oscillations, and that discrepancy creates an alternating electric potential between various points in a sound wave.\n\nThis effect was extensively used in the 1950s and 1960s for characterizing ion solvation. These works are mostly associated with the names of Zana and Yaeger, who published a review of their studies in 1982.\n"}
{"id": "164610", "url": "https://en.wikipedia.org/wiki?curid=164610", "title": "Latent heat", "text": "Latent heat\n\nLatent heat is thermal energy released or absorbed, by a body or a thermodynamic system, during a constant-temperature process — usually a first-order phase transition.\n\nLatent heat can be understood as heat energy in hidden form which is supplied or extracted to change the state of a substance without changing its temperature. Examples are latent heat of fusion and latent heat of vaporization involved in phase changes, i.e. a substance condensing or vaporizing at a specified temperature and pressure. \n\nThe term was introduced around 1762 by British chemist Joseph Black. It is derived from the Latin \"latere\" (\"to lie hidden\"). Black used the term in the context of calorimetry where a heat transfer caused a volume change in a body while its temperature was constant.\n\nIn contrast to latent heat, sensible heat is a heat transfer that results in a temperature change in a body.\n\nThe terms ″sensible heat″ and ″latent heat″ refer to types of heat transfer between a body and its surroundings; they depend on the properties of the body. ″Sensible heat″ is ″sensed″ or felt in a process as a change in the body's temperature. ″Latent heat″ is heat transferred in a process without change of the body's temperature, for example, in a phase change ( solid / liquid / gas ). \n\nBoth sensible and latent heats are observed in many processes of transfer of energy in nature. Latent heat is associated with the change of phase of atmospheric or ocean water, vaporization, condensation, freezing or melting, whereas sensible heat is energy transferred that is evident in change of the temperature of the atmosphere or ocean, or ice, without those phase changes, though it is associated with changes of pressure and volume.\n\nThe original usage of the term, as introduced by Black, was applied to systems that were intentionally held at constant temperature. Such usage referred to \"latent heat of expansion\" and several other related latent heats. These latent heats are defined independently of the conceptual framework of thermodynamics.\n\nWhen a body is heated at constant temperature by thermal radiation in a microwave field for example, it may expand by an amount described by its \"latent heat with respect to volume\" or \"latent heat of expansion\", or increase its pressure by an amount described by its \"latent heat with respect to pressure\".\nLatent heat is energy released or absorbed, by a body or a thermodynamic system, during a constant-temperature process.\nTwo common forms of latent heat are latent heat of fusion (melting) and latent heat of vaporization (boiling). These names describe the direction of energy flow when changing from one phase to the next: from solid to liquid, and liquid to gas.\n\nIn both cases the change is endothermic, meaning that the system absorbs energy.\nFor example, when water evaporates, energy is required for the water molecules to overcome the forces of attraction between them, the transition from water to vapor requires an input of energy.\n\nIf the vapor then condenses to a liquid on a surface, then the vapor's latent energy absorbed during evaporation is released as the liquid's sensible heat onto the surface.\n\nThe large value of the enthalpy of condensation of water vapor is the reason that steam is a far more effective heating medium than boiling water, and is more hazardous.\n\nIn meteorology, latent heat flux is the flux of heat from the Earth's surface to the atmosphere that is associated with evaporation or transpiration of water at the surface and subsequent condensation of water vapor in the troposphere. It is an important component of Earth's surface energy budget. Latent heat flux has been commonly measured with the Bowen ratio technique, or more recently since the mid-1900s by the Jonathan Beaver method.\n\nThe English word \"latent\" comes from Latin \"latēns\", meaning \"lying hidden\". The term \"latent heat\" was introduced into calorimetry around 1750 when Joseph Black, commissioned by producers of Scotch whisky in search of ideal quantities of fuel and water for their distilling process, to studying system changes, such as of volume and pressure, when the thermodynamic system was held at constant temperature in a thermal bath. James Prescott Joule characterised latent energy as the energy of interaction in a given configuration of particles, i.e. a form of potential energy, and the sensible heat as an energy that was indicated by the thermometer, relating the latter to thermal energy.\n\nA \"specific\" latent heat (\"L\") expresses the amount of energy in the form of heat (\"Q\") required to completely effect a phase change of a unit of mass (\"m\"), usually , of a substance as an intensive property:\nIntensive properties are material characteristics and are not dependent on the size or extent of the sample. Commonly quoted and tabulated in the literature are the specific latent heat of fusion and the specific latent heat of vaporization for many substances.\n\nFrom this definition, the latent heat for a given mass of a substance is calculated by\nwhere:\n\nThe following table shows the specific latent heats and change of phase temperatures (at standard pressure) of some common fluids and gases.\n\nThe specific latent heat of condensation of water in the temperature range from −25 °C to 40 °C is approximated by the following empirical cubic function:\nwhere the temperature formula_4 is taken to be the numerical value in °C.\n\nFor sublimation and deposition from and into ice, the specific latent heat is almost constant in the temperature range from −40 °C to 0 °C and can be approximated by the following empirical quadratic function:\n\nAs the temperature (or pressure) rises to the critical point the LHOV falls to zero :\n\n"}
{"id": "183290", "url": "https://en.wikipedia.org/wiki?curid=183290", "title": "Life extension", "text": "Life extension\n\nLife extension is the idea of extending the human lifespan, either modestly – through improvements in medicine – or dramatically by increasing the maximum lifespan beyond its generally settled limit of 125 years. The ability to achieve such dramatic changes, however, does not currently exist.\n\nSome researchers in this area, and \"life extensionists\", \"immortalists\" or \"longevists\" (those who wish to achieve longer lives themselves), believe that future breakthroughs in tissue rejuvenation, stem cells, regenerative medicine, molecular repair, gene therapy, pharmaceuticals, and organ replacement (such as with artificial organs or xenotransplantations) will eventually enable humans to have indefinite lifespans (agerasia) through complete rejuvenation to a healthy youthful condition. The ethical ramifications, if life extension becomes a possibility, are debated by bioethicists.\n\nThe sale of purported anti-aging products such as supplements and hormone replacement is a lucrative global industry. For example, the industry that promotes the use of hormones as a treatment for consumers to slow or reverse the aging process in the US market generated about $50 billion of revenue a year in 2009. The use of such products has not been proven to be effective or safe.\n\nDuring the process of aging, an organism accumulates damage to its macromolecules, cells, tissues, and organs. Specifically, aging is characterized as and thought to be caused by \"genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication.\" Oxidation damage to cellular contents caused by free radicals is believed to contribute to aging as well.\n\nThe longest a human has ever been proven to live is 122 years, the case of Jeanne Calment who was born in 1875 and died in 1997, whereas the maximum lifespan of a wildtype mouse, commonly used as a model in research on aging, is about three years. Genetic differences between humans and mice that may account for these different aging rates include differences in efficiency of DNA repair, antioxidant defenses, energy metabolism, proteostasis maintenance, and recycling mechanisms such as autophagy.\n\nAverage lifespan in a population is lowered by infant and child mortality, which are frequently linked to infectious diseases or nutrition problems. Later in life, vulnerability to accidents and age-related chronic disease such as cancer or cardiovascular disease play an increasing role in mortality. Extension of expected lifespan can often be achieved by access to improved medical care, vaccinations, good diet, exercise and avoidance of hazards such as smoking.\n\nMaximum lifespan is determined by the rate of aging for a species inherent in its genes and by environmental factors. Widely recognized methods of extending maximum lifespan in model organisms such as nematodes, fruit flies, and mice include caloric restriction, gene manipulation, and administration of pharmaceuticals. Another technique uses evolutionary pressures such as breeding from only older members or altering levels of extrinsic mortality.\nSome animals such as hydra, planarian flatworms, and certain sponges, corals, and jellyfish do not die of old age and exhibit potential immortality.\n\nMuch life extension research focuses on nutrition—diets or supplements— although there is little evidence that they have an effect. The many diets promoted by anti-aging advocates are often contradictory. \n\nIn some studies calorie restriction has been shown to extend the life of mice, yeast, and rhesus monkeys. However, a more recent study did not find calorie restriction to improve survival in rhesus monkeys. In humans the long-term health effects of moderate caloric restriction with sufficient nutrients are unknown.\n\nThe free-radical theory of aging suggests that antioxidant supplements might extend human life. However, evidence suggest that β-carotene supplements and high doses of vitamin E increase mortality rates. Resveratrol is a sirtuin stimulant that has been shown to extend life in animal models, but the effect of resveratrol on lifespan in humans is unclear as of 2011.\n\nThe anti-aging industry offers several hormone therapies. Some of these have been criticized for possible dangers and a lack of proven effect. For example, the American Medical Association has been critical of some anti-aging hormone therapies.\n\nWhile growth hormone (GH) decreases with age, the evidence for use of growth hormone as an anti-aging therapy is mixed and based mostly on animal studies. There are mixed reports that GH or IGF-1 modulates the aging process in humans and about whether the direction of its effect is positive or negative.\n\nThe extension of life has been a desire of humanity and a mainstay motif in the history of scientific pursuits and ideas throughout history, from the Sumerian Epic of Gilgamesh and the Egyptian Smith medical papyrus, all the way through the Taoists, Ayurveda practitioners, alchemists, hygienists such as Luigi Cornaro, Johann Cohausen and Christoph Wilhelm Hufeland, and philosophers such as Francis Bacon, René Descartes, Benjamin Franklin and Nicolas Condorcet. However, the beginning of the modern period in this endeavor can be traced to the end of the 19th – beginning of the 20th century, to the so-called \"fin-de-siècle\" (end of the century) period, denoted as an \"end of an epoch\" and characterized by the rise of scientific optimism and therapeutic activism, entailing the pursuit of life extension (or life-extensionism). Among the foremost researchers of life extension at this period were the Nobel Prize winning biologist Elie Metchnikoff (1845-1916) -- the author of the cell theory of immunity and vice director of Institut Pasteur in Paris, and Charles-Édouard Brown-Séquard (1817-1894) -- the president of the French Biological Society and one of the founders of modern endocrinology.\n\nSociologist James Hughes claims that science has been tied to a cultural narrative of conquering death since the Age of Enlightenment. He cites Francis Bacon (1561–1626) as an advocate of using science and reason to extend human life, noting Bacon's novel \"New Atlantis\", wherein scientists worked toward delaying aging and prolonging life. Robert Boyle (1627–1691), founding member of the Royal Society, also hoped that science would make substantial progress with life extension, according to Hughes, and proposed such experiments as \"to replace the blood of the old with the blood of the young\". Biologist Alexis Carrel (1873–1944) was inspired by a belief in indefinite human lifespan that he developed after experimenting with cells, says Hughes.\n\nIn 1970, the American Aging Association was formed under the impetus of Denham Harman, originator of the free radical theory of aging. Harman wanted an organization of biogerontologists that was devoted to research and to the sharing of information among scientists interested in extending human lifespan.\n\nIn 1976, futurists Joel Kurtzman and Philip Gordon wrote \"No More Dying. The Conquest Of Aging And The Extension Of Human Life\", () the first popular book on research to extend human lifespan. Subsequently, Kurtzman was invited to testify before the House Select Committee on Aging, chaired by Claude Pepper of Florida, to discuss the impact of life extension on the Social Security system.\n\nSaul Kent published \"The Life Extension Revolution\" () in 1980 and created a nutraceutical firm called the Life Extension Foundation, a non-profit organization that promotes dietary supplements. The Life Extension Foundation publishes a periodical called \"Life Extension Magazine\". The 1982 bestselling book \"\" () by Durk Pearson and Sandy Shaw further popularized the phrase \"life extension\".\n\nRegulatory and legal struggles between the Food and Drug Administration (FDA) and the Life Extension Foundation included seizure of merchandise and court action. In 1991, Saul Kent and Bill Faloon, the principals of the Foundation, were jailed. The LEF accused the FDA of perpetrating a \"Holocaust\" and \"seeking gestapo-like power\" through its regulation of drugs and marketing claims.\n\nIn 2003, Doubleday published \"The Immortal Cell: One Scientist's Quest to Solve the Mystery of Human Aging,\" by Michael D. West. West emphasised the potential role of embryonic stem cells in life extension.\n\nOther modern life extensionists include writer Gennady Stolyarov, who insists that death is \"the enemy of us all, to be fought with medicine, science, and technology\"; transhumanist philosopher Zoltan Istvan, who proposes that the \"transhumanist must safeguard one's own existence above all else\"; futurist George Dvorsky, who considers aging to be a problem that desperately needs to be solved; and recording artist Steve Aoki, who has been called \"one of the most prolific campaigners for life extension\".\n\nIn 1991, the American Academy of Anti-Aging Medicine (A4M) was formed. The American Board of Medical Specialties recognizes neither anti-aging medicine nor the A4M's professional standing.\n\nIn 2003, Aubrey de Grey and David Gobel formed the Methuselah Foundation, which gives financial grants to anti-aging research projects. In 2009, de Grey and several others founded the SENS Research Foundation, a California-based scientific research organization which conducts research into aging and funds other anti-aging research projects at various universities. In 2013, Google announced Calico, a new company based in San Francisco that will harness new technologies to increase scientific understanding of the biology of aging. It is led by Arthur D. Levinson, and its research team includes scientists such as Hal V. Barron, David Botstein, and Cynthia Kenyon. In 2014, biologist Craig Venter founded Human Longevity Inc., a company dedicated to scientific research to end aging through genomics and cell therapy. They received funding with the goal of compiling a comprehensive human genotype, microbiome, and phenotype database.\n\nAside from private initiatives, aging research is being conducted in university laboratories, and includes universities such as Harvard and UCLA. University researchers have made a number of breakthroughs in extending the lives of mice and insects by reversing certain aspects of aging.\n\nPolitics relevant to the substances of life extension pertain mostly to communications and availability.\n\nIn the United States, product claims on food and drug labels are strictly regulated. The First Amendment (freedom of speech) protects third-party publishers' rights to distribute fact, opinion and speculation on life extension practices. Manufacturers and suppliers also provide informational publications, but because they market the substances, they are subject to monitoring and enforcement by the Federal Trade Commission (FTC), which polices claims by marketers. What constitutes the difference between truthful and false claims is hotly debated and is a central controversy in this arena.\n\nSome critics dispute the portrayal of aging as a disease. For example, Leonard Hayflick, who determined that fibroblasts are limited to around 50 cell divisions, reasons that aging is an unavoidable consequence of entropy. Hayflick and fellow biogerontologists Jay Olshansky and Bruce Carnes have strongly criticized the anti-aging industry in response to what they see as unscrupulous profiteering from the sale of unproven anti-aging supplements.\n\nResearch by Sobh and Martin (2011) suggests that people buy anti-aging products to obtain a hoped-for self (e.g., keeping a youthful skin) or to avoid a feared-self (e.g., looking old). The research shows that when consumers pursue a hoped-for self, it is expectations of success that most strongly drive their motivation to use the product. The research also shows why doing badly when trying to avoid a feared self is more motivating than doing well. When product use is seen to fail it is more motivating than success when consumers seek to avoid a feared-self.\n\nThough many scientists state that life extension and radical life extension are possible, there are still no international or national programs focused on radical life extension. There are political forces staying for and against life extension. By 2012, in Russia, the United States, Israel, and the Netherlands, the Longevity political parties started. They aimed to provide political support to radical life extension research and technologies, and ensure the fastest possible and at the same time soft transition of society to the next step – life without aging and with radical life extension, and to provide access to such technologies to most currently living people.\n\nSome tech innovators and Silicon Valley entrepreneurs have invested heavily into anti-aging research. This includes Larry Ellison (founder of Oracle), Peter Thiel (former Paypal CEO), Larry Page (co-founder of Google), and Peter Diamandis.\n\nLeon Kass (chairman of the US President's Council on Bioethics from 2001 to 2005) has questioned whether potential exacerbation of overpopulation problems would make life extension unethical. He states his opposition to life extension with the words:\nJohn Harris, former editor-in-chief of the Journal of Medical Ethics, argues that as long as life is worth living, according to the person himself, we have a powerful moral imperative to save the life and thus to develop and offer life extension therapies to those who want them.\n\nTranshumanist philosopher Nick Bostrom has argued that any technological advances in life extension must be equitably distributed and not restricted to a privileged few. In an extended metaphor entitled \"The Fable of the Dragon-Tyrant\", Bostrom envisions death as a monstrous dragon who demands human sacrifices. In the fable, after a lengthy debate between those who believe the dragon is a fact of life and those who believe the dragon can and should be destroyed, the dragon is finally killed. Bostrom argues that political inaction allowed many preventable human deaths to occur.\n\nControversy about life extension is due to fear of overpopulation and possible effects on society. Biogerontologist Aubrey De Grey counters the overpopulation critique by pointing out that the therapy could postpone or eliminate menopause, allowing women to space out their pregnancies over more years and thus \"decreasing\" the yearly population growth rate. Moreover, the philosopher and futurist Max More argues that, given the fact the worldwide population growth rate is slowing down and is projected to eventually stabilize and begin falling, superlongevity would be unlikely to contribute to overpopulation.\n\nA Spring 2013 Pew Research poll in the United States found that 38% of Americans would want life extension treatments, and 56% would reject it. However, it also found that 68% believed most people would want it and that only 4% consider an \"ideal lifespan\" to be more than 120 years. The median \"ideal lifespan\" was 91 years of age and the majority of the public (63%) viewed medical advances aimed at prolonging life as generally good. 41% of Americans believed that radical life extension (RLE) would be good for society, while 51% said they believed it would be bad for society. One possibility for why 56% of Americans claim they would reject life extension treatments may be due to the cultural perception that living longer would result in a longer period of decrepitude, and that the elderly in our current society are unhealthy.\n\nReligious people are no more likely to oppose life extension than the unaffiliated, though some variation exists between religious denominations.\n\nMainstream medical organizations and practitioners do not consider aging to be a disease. David Sinclair says: \"I don't see aging as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body\". The two main arguments used are that aging is both inevitable and universal while diseases are not. However, not everyone agrees. Harry R. Moody, director of academic affairs for AARP, notes that what is normal and what is disease strongly depend on a historical context. David Gems, assistant director of the Institute of Healthy Ageing, argues that aging should be viewed as a disease. In response to the universality of aging, David Gems notes that it is as misleading as arguing that Basenji are not dogs because they do not bark. Because of the universality of aging he calls it a \"special sort of disease\". Robert M. Perlman, coined the terms \"aging syndrome\" and \"disease complex\" in 1954 to describe aging.\n\nThe discussion whether aging should be viewed as a disease or not has important implications. One view is, this would stimulate pharmaceutical companies to develop life extension therapies and in the United States of America, it would also increase the regulation of the anti-aging market by the FDA. Anti-aging now falls under the regulations for cosmetic medicine which are less tight than those for drugs.\n\nTheoretically, extension of maximum lifespan in humans could be achieved by reducing the rate of aging damage by periodic replacement of damaged tissues, molecular repair or rejuvenation of deteriorated cells and tissues, reversal of harmful epigenetic changes, or the enhancement of enzyme telomerase activity.\n\nResearch geared towards life extension strategies in various organisms is currently under way at a number of academic and private institutions. Since 2009, investigators have found ways to increase the lifespan of nematode worms and yeast by 10-fold; the record in nematodes was achieved through genetic engineering and the extension in yeast by a combination of genetic engineering and caloric restriction. A 2009 review of longevity research noted: \"Extrapolation from worms to mammals is risky at best, and it cannot be assumed that interventions will result in comparable life extension factors. Longevity gains from dietary restriction, or from mutations studied previously, yield smaller benefits to Drosophila than to nematodes, and smaller still to mammals. This is not unexpected, since mammals have evolved to live many times the worm's lifespan, and humans live nearly twice as long as the next longest-lived primate. From an evolutionary perspective, mammals and their ancestors have already undergone several hundred million years of natural selection favoring traits that could directly or indirectly favor increased longevity, and may thus have already settled on gene sequences that promote lifespan. Moreover, the very notion of a \"life-extension factor\" that could apply across taxa presumes a linear response rarely seen in biology.\"\n\nThere are a number of chemicals intended to slow the aging process currently being studied in animal models. One type of research is related to the observed effects of a calorie restriction (CR) diet, which has been shown to extend lifespan in some animals. Based on that research, there have been attempts to develop drugs that will have the same effect on the aging process as a caloric restriction diet, which are known as Caloric restriction mimetic drugs. Some drugs that are already approved for other uses have been studied for possible longevity effects on laboratory animals because of a possible CR-mimic effect; they include rapamycin, metformin and other geroprotectors. MitoQ, resveratrol and pterostilbene are dietary supplements that have also been studied in this context.\n\nOther attempts to create anti-aging drugs have taken different research paths. One notable direction of research has been research into the possibility of using the enzyme telomerase in order to counter the process of telomere shortening. However, there are potential dangers in this, since some research has also linked telomerase to cancer and to tumor growth and formation.\n\nFuture advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular computers, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical nanomachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSome life extensionists suggest that therapeutic cloning and stem cell research could one day provide a way to generate cells, body parts, or even entire bodies (generally referred to as reproductive cloning) that would be genetically identical to a prospective patient. Recently, the US Department of Defense initiated a program to research the possibility of growing human body parts on mice. Complex biological structures, such as mammalian joints and limbs, have not yet been replicated. Dog and primate brain transplantation experiments were conducted in the mid-20th century but failed due to rejection and the inability to restore nerve connections. As of 2006, the implantation of bio-engineered bladders grown from patients' own cells has proven to be a viable treatment for bladder disease. Proponents of body part replacement and cloning contend that the required biotechnologies are likely to appear earlier than other life-extension technologies.\n\nThe use of human stem cells, particularly embryonic stem cells, is controversial. Opponents' objections generally are based on interpretations of religious teachings or ethical considerations. Proponents of stem cell research point out that cells are routinely formed and destroyed in a variety of contexts. Use of stem cells taken from the umbilical cord or parts of the adult body may not provoke controversy.\n\nThe controversies over cloning are similar, except general public opinion in most countries stands in opposition to reproductive cloning. Some proponents of therapeutic cloning predict the production of whole bodies, lacking consciousness, for eventual brain transplantation.\n\nReplacement of biological (susceptible to diseases) organs with mechanical ones could extend life. This is the goal of the 2045 Initiative.\n\nFor cryonicists (advocates of cryopreservation), storing the body at low temperatures after death may provide an \"ambulance\" into a future in which advanced medical technologies may allow resuscitation and repair. They speculate cryogenic temperatures will minimize changes in biological tissue for many years, giving the medical community ample time to cure all disease, rejuvenate the aged and repair any damage that is caused by the cryopreservation process.\n\nMany cryonicists do not believe that legal death is \"real death\" because stoppage of heartbeat and breathing—the usual medical criteria for legal death—occur before biological death of cells and tissues of the body. Even at room temperature, cells may take hours to die and days to decompose. Although neurological damage occurs within 4–6 minutes of cardiac arrest, the irreversible neurodegenerative processes do not manifest for hours. Cryonicists state that rapid cooling and cardio-pulmonary support applied immediately after certification of death can preserve cells and tissues for long-term preservation at cryogenic temperatures. People, particularly children, have survived up to an hour without heartbeat after submersion in ice water. In one case, full recovery was reported after 45 minutes underwater. To facilitate rapid preservation of cells and tissue, cryonics \"standby teams\" are available to wait by the bedside of patients who are to be cryopreserved to apply cooling and cardio-pulmonary support as soon as possible after declaration of death.\n\nNo mammal has been successfully cryopreserved and brought back to life, with the exception of frozen human embryos. Resuscitation of a postembryonic human from cryonics is not possible with current science. Some scientists still support the idea based on their expectations of the capabilities of future science.\n\nAnother proposed life extension technology would combine existing and predicted future biochemical and genetic techniques. SENS proposes that rejuvenation may be obtained by removing aging damage via the use of stem cells and tissue engineering, telomere-lengthening machinery, allotopic expression of mitochondrial proteins, targeted ablation of cells, immunotherapeutic clearance, and novel lysosomal hydrolases.\n\nWhile many biogerontologists find these ideas \"worthy of discussion\" and SENS conferences feature important research in the field, some contend that the alleged benefits are too speculative given the current state of technology, referring to it as \"fantasy rather than science\".\n\nGenome editing, in which nucleic acid polymers are delivered as a drug and are either expressed as proteins, interfere with the expression of proteins, or correct genetic mutations, has been proposed as a future strategy to prevent aging.\n\nA large array of genetic modifications have been found to increase lifespan in model organisms such as yeast, nematode worms, fruit flies, and mice. As of 2013, the longest extension of life caused by a single gene manipulation was roughly 50% in mice and 10-fold in nematode worms.\n\nIn \"The Selfish Gene\", Richard Dawkins describes an approach to life-extension that involves \"fooling genes\" into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a statistical certainty that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, we should be able to prevent these genes from switching on, and we should be able to do so by \"identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body\".\n\nOne hypothetical future strategy that, as some suggest, \"eliminates\" the complications related to a physical body, involves the copying or transferring (e.g. by progressively replacing neurons with transistors) of a conscious mind from a biological brain to a non-biological computer system or computational device. The basic idea is to scan the structure of a particular brain in detail, and then construct a software model of it that is so faithful to the original that, when run on appropriate hardware, it will behave in essentially the same way as the original brain. Whether or not an exact copy of one's mind constitutes actual life extension is matter of debate.\n\nSome scientists believe that the dead may one day be \"resurrected\" through simulation technology.\n\nSome clinics currently offer injection of blood products from young donors. The alleged benefits of the treatment, none of which have been demonstrated in a proper study, include a longer life, darker hair, better memory, better sleep, curing heart diseases, diabetes and Alzheimer. The approach is based on parabiosis studies such as Irina Conboy do on mice, but Conboy says young blood does not reverse aging (even in mice) and that those who offer those treatments have misunderstood her research. Neuroscientist Tony Wyss-Coray, who also studied blood exchanges on mice as recently as 2014, said people offering those treatments are \"basically abusing people's trust\" and that young blood treatments are \"the scientific equivalent of fake news\". The treatment appeared in HBO's Silicon Valley fiction series.\n\nTwo clinics in California, run by Jesse Karmazin and David C. Wright, offer $8,000 injections of plasma extracted from the blood of young people. Karmazin has not published in any peer-reviewed journal and his current study does not use a control group.\n\n\n"}
{"id": "55409829", "url": "https://en.wikipedia.org/wiki?curid=55409829", "title": "List of Lieutenants of Hampton Court Chase, Rangers of Bushy Park", "text": "List of Lieutenants of Hampton Court Chase, Rangers of Bushy Park\n\nThis is a list of Lieutenants and Keepers of Hampton Court Chase and ex officio (by virtue of that office) Rangers of Bushy Park.\n\nIn 1539 Hampton Court Chase was created an 'Honour' by an Act of Parliament instigated by Henry VIII and passed as a public act by approval of Parliament and the King. The title holder increasingly over time referred to as Ranger or Keeper of Bushey Park held as his domain the right to build and rebuild a grand house in Bushy Park coupled with a degree of local power and exercise likewise to the monarch in hunting rights. Henry thereby created the first forest since the New Forest of William the Conqueror and obtained the rights to a share of cattle and game in the extent to append to his \"close park\" to Hampton Court Palace taken from Wolsey. The area's other local landowners included principally manorial tenants of the confiscated Chertsey and Westminster Abbeys – and made all non-mete parts an overall chase for royal and official hunting parties for boar, game birds, hare, coneys and deer. Holding all persons to account for animal grazing and wood-taking financially were added benefits. The statute set up a new forest or chase for the king, to be called 'Hampton Court Chase' but which was smaller than the many square miles the King wished:\n\nThe bounds of the chase apart from Bushy Park took in the manors of Hampton, Hanworth, Kempton, Walton-on-Thames. Parliament, through the courts, was deemed to have strongly curtailed its royal forest rights so as not to impinge on freeholds nor main customary tenants lands in Molesey, Weybridge, Cobham and part of Esher, its furthest reach due to Henry's legal deed (indenture) of 1537.\n\nThe Ranger or Keeper in early years held the position of Housekeeper of the Palace. The rangership of the land in the bend of the river east of the Palace, the 'House' or 'Home' Park was usually separate. Kit Villiers, an ennobled courtier by James I, inherited the honour from his brother and took as his official seat Ashley House (and Ashley Park) in Walton on Thames. Reinstatement in wealth and titles to the Villiers (Earls of Jersey and Anglesey) family after the English Civil War failed to reunite the more southerly lands associated with the honour which were taken and sold in the course of the 1649-1660 Commonwealth of England. Some holders of the title lived in Bushy Park seasonally or as their main home. The monarch tended to grant the title to the main heir of the previous holder. Parliament in the English Interregnum granted the title to General Monck, who remained in favour in the English Restoration.\n\n\n\n"}
{"id": "3958869", "url": "https://en.wikipedia.org/wiki?curid=3958869", "title": "List of conservation organisations", "text": "List of conservation organisations\n\nThis is a list of conservation organisations, which are organisations that primarily deal with the conservation of various ecosystems.\nCave Conservancies are land trusts specialized in caves and karst features.\n\n"}
{"id": "39127306", "url": "https://en.wikipedia.org/wiki?curid=39127306", "title": "Mechanism (philosophy)", "text": "Mechanism (philosophy)\n\nMechanism is the belief that natural wholes (principally living things) are like complicated machines or artifacts, composed of parts lacking any intrinsic relationship to each other. Thus, the source of an apparent thing's activities is not the whole itself, but its parts or an external influence on the parts.\n\nThe doctrine of mechanism in philosophy comes in two different flavors. They are both doctrines of metaphysics, but they are different in scope and ambitions: the first is a global doctrine about nature; the second is a local doctrine about humans and their minds, which is hotly contested. For clarity, we might distinguish these two doctrines as universal mechanism and anthropic mechanism.\n\nThe older doctrine, here called universal mechanism, is the ancient philosophies closely linked with materialism and reductionism, especially that of the atomists and to a large extent, stoic physics. They held that the universe is reducible to completely mechanical principles—that is, the motion and collision of matter. Later mechanists believed the achievements of the scientific revolution had shown that all phenomena could eventually be explained in terms of 'mechanical' laws, natural laws governing the motion and collision of matter that implied a thorough going determinism: if \"all\" phenomena could be explained \"entirely\" through the motion of matter under the laws of classical physics, then even more surely than the gears of a clock determine that it must strike 2:00 an hour after striking 1:00, \"all\" phenomena must be completely determined: whether past, present or future. (One of the philosophical implications of modern quantum mechanics is that this view of determinism is not defensible.)\n\nThe French mechanist and determinist Pierre Simon de Laplace formulated the sweeping implications of this thesis by saying:\n\nOne of the first and most famous expositions of universal mechanism is found in the opening passages of \"Leviathan\" by Thomas Hobbes (1651). What is less frequently appreciated is that René Descartes was a staunch mechanist, though today, in the philosophy of mind, he is remembered for introducing the mind–body problem in terms of dualism and physicalism.\n\nDescartes was a substance dualist, and argued that reality was composed of two radically different types of substance: extended matter, on the one hand, and immaterial mind, on the other. Descartes argued that one cannot explain the conscious mind in terms of the spatial dynamics of mechanistic bits of matter cannoning off each other. Nevertheless, his understanding of biology was thoroughly mechanistic in nature:\n\nHis scientific work was based on the traditional mechanistic understanding that animals and humans are completely mechanistic automata. Descartes' dualism was motivated by the seeming impossibility that mechanical dynamics could yield mental experiences.\n\nIsaac Newton ushered in a much weaker acceptation of mechanism that tolerated the antithetical, and as yet inexplicable, action at a distance of gravity. However, his work seemed to successfully predict the motion of both celestial and terrestrial bodies according to that principle, and the generation of philosophers who were inspired by Newton's example carried the mechanist banner nonetheless. Chief among them were French philosophers such as Julien Offray de La Mettrie and Denis Diderot (see also: French materialism).\n\nThe thesis in anthropic mechanism is not that everything can be completely explained in mechanical terms (although some anthropic mechanists may \"also\" believe that), but rather that everything \"about human beings\" can be completely explained in mechanical terms, as surely as can everything about clocks or the internal combustion engine.\n\nOne of the chief obstacles that all mechanistic theories have faced is providing a mechanistic explanation of the human mind; Descartes, for one, endorsed dualism in spite of endorsing a completely mechanistic conception of the material world because he argued that mechanism and the notion of a mind were logically incompatible. Hobbes, on the other hand, conceived of the mind and the will as purely mechanistic, completely explicable in terms of the effects of perception and the pursuit of desire, which in turn he held to be completely explicable in terms of the materialistic operations of the nervous system. Following Hobbes, other mechanists argued for a thoroughly mechanistic explanation of the mind, with one of the most influential and controversial expositions of the doctrine being offered by Julien Offray de La Mettrie in his \"Man a Machine\" (1748).\n\nToday, as in the past, the main points of debate between anthropic mechanists and anti-mechanists are mainly occupied with two topics: the mind — and consciousness, in particular — and free will. Anti-mechanists argue that anthropic mechanism is incompatible with our commonsense intuitions: in philosophy of mind they argue that unconscious matter cannot completely explain the phenomenon of consciousness, and in metaphysics they argue that anthropic mechanism implies determinism about human action, which (they argue) is incompatible with our understanding of ourselves as creatures with free will. Contemporary philosophers who have argued for this position include Norman Malcolm and\nDavid Chalmers.\n\nAnthropic mechanists typically respond in one of two ways. In the first, they agree with anti-mechanists that mechanism conflicts with some of our commonsense intuitions, but go on to argue that our commonsense intuitions are simply mistaken and need to be revised. Down this path lies eliminative materialism in philosophy of mind, and hard determinism on the question of free will. This option is accepted by the eliminative materialist philosopher Paul Churchland. Some have questioned how eliminative materialism is compatible with the freedom of will apparently required for anyone (including its adherents) to make truth claims. The second option, common amongst philosophers who adopt anthropic mechanism, is to argue that the arguments given for incompatibility are specious: whatever it is we mean by \"consciousness\" and \"free will,\" they urge, it is fully compatible with a mechanistic understanding of the human mind and will. As a result, they tend to argue for one or another non-eliminativist physicalist theories of mind, and for compatibilism on the question of free will. Contemporary philosophers who have argued for this sort of account include J. J. C. Smart and Daniel Dennett.\n\nSome scholars have debated over what, if anything, Gödel's incompleteness theorems imply about anthropic mechanism. Much of the debate centers on whether the human mind is equivalent to a Turing machine, or by the Church-Turing thesis, any finite machine at all. If it is, and if the machine is consistent, then Gödel's incompleteness theorems would apply to it.\n\nGödelian arguments claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent and powerful enough to recognize its own consistency. Since this is impossible for a Turing machine, the Gödelian concludes that human reasoning must be non-mechanical.\n\nHowever, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" \"H\" of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of \"H\" (otherwise \"H\" is provably inconsistent); and that Gödel's theorems do not lead to any valid argument against mechanism. This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in \"Artificial Intelligence\": \"\"any\" attempt to utilize (Gödel's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"\n\nOne of the earliest attempts to use incompleteness to reason about human intelligence was by Gödel himself in his 1951 Gibbs Lecture entitled \"Some basic theorems on the foundations of mathematics and their philosophical implications\". In this lecture, Gödel uses the incompleteness theorem to arrive at the following disjunction: (a) the human mind is not a consistent finite machine, or (b) there exist Diophantine equations for which it cannot decide whether solutions exist. Gödel finds (b) implausible, and thus seems to have believed the human mind was not equivalent to a finite machine, i.e., its power exceeded that of any finite machine. He recognized that this was only a conjecture, since one could never disprove (b). Yet he considered the disjunctive conclusion to be a \"certain fact\".\n\nIn subsequent years, more direct anti-mechanist lines of reasoning were apparently floating around the intellectual atmosphere. In 1960, Hilary Putnam published a paper entitled \"Minds and Machines,\" in which he points out the flaws of a typical anti-mechanist argument. Informally, this is the argument that the (alleged) difference between \"what can be mechanically proven\" and \"what can be seen to be true by humans\" shows that human intelligence is not mechanical in nature. Or, as Putnam puts it:\n\nLet T be a Turing machine which \"represents\" me in the sense that T can prove just the mathematical statements I prove. Then using Gödel's technique I can discover a proposition that T cannot prove, and moreover I can prove this proposition. This refutes the assumption that T \"represents\" me, hence I am not a Turing machine.\n\nHilary Putnam objects that this argument ignores the issue of consistency. Gödel's technique can only be applied to consistent systems. It is conceivable, argues Putnam, that the human mind is inconsistent. If one is to use Gödel's technique to prove the proposition that T cannot prove, one must first prove (the mathematical statement representing) the consistency of T, a daunting and perhaps impossible task. Later Putnam suggested that while Gödel's theorems cannot be applied to humans, since they make mistakes and are therefore inconsistent, it may be applied to the human faculty of science or mathematics in general. If we are to believe that it is consistent, then either we cannot prove its consistency, or it cannot be represented by a Turing machine.\n\nJ. R. Lucas in \"Minds, Machines and Gödel\" (1961), and later in his book \"The Freedom of the Will\" (1970), lays out an anti-mechanist argument closely following the one described by Putnam, including reasons for why the human mind can be considered consistent. Lucas admits that, by Gödel's second theorem, a human mind cannot formally prove its own consistency, and even says (perhaps facetiously) that women and politicians are inconsistent. Nevertheless, he sets out arguments for why a male non-politician can be considered consistent. These arguments are philosophical in nature and are the subject of much debate; Lucas provides references to responses on his own website.\n\nAnother work was done by Judson Webb in his 1968 paper \"Metamathematics and the Philosophy of Mind\". Webb claims that previous attempts have glossed over whether one truly can see that the Gödelian statement \"p\" pertaining to oneself, is true. Using a different formulation of Gödel's theorems, namely, that of Raymond Smullyan and Emil Post, Webb shows one can derive convincing arguments for oneself of both the truth and falsity of \"p\". He furthermore argues that all arguments about the philosophical implications of Gödel's theorems are really arguments about whether the Church-Turing thesis is true.\n\nLater, Roger Penrose entered the fray, providing somewhat novel anti-mechanist arguments in his books, \"The Emperor's New Mind\" (1989) [ENM] and \"Shadows of the Mind\" (1994) [SM]. These books have proved highly controversial. Martin Davis responded to ENM in his paper \"Is Mathematical Insight Algorithmic?\" (ps), where he argues that Penrose ignores the issue of consistency. Solomon Feferman gives a critical examination of SM in his paper \"Penrose's Gödelian argument\" (pdf). The response of the scientific community to Penrose's arguments has been negative, with one group of scholars calling Penrose's repeated attempts to form a persuasive Gödelian argument \"a kind of intellectual shell game, in which a precisely defined notion to which a mathematical result applies... is switched for a vaguer notion\".\n\nA Gödel-based anti-mechanism argument can be found in Douglas Hofstadter's book \"\", though Hofstadter is widely viewed as a known skeptic of such arguments:\nLooked at this way, Gödel's proof suggests – though by no means does it prove! – that there could be some high-level way of viewing the mind/brain, involving concepts which do not appear on lower levels, and that this level might have explanatory power that does not exist – not even in principle – on lower levels. It would mean that some facts could be explained on the high level quite easily, but not on lower levels at all. No matter how long and cumbersome a low-level statement were made, it would not explain the phenomena in question.\nIt is analogous to the fact that, if you make derivation after derivation in Peano arithmetic, no matter how long and cumbersome you make them, you will never come up with one for G – despite the fact that on a higher level, you can see that the Gödel sentence is true.\n\nWhat might such high-level concepts be? It has been proposed for eons, by various holistically or \"soulistically\" inclined scientists and humanists that consciousness is a phenomenon that escapes explanation in terms of brain components; so here is a candidate at least. There is also the ever-puzzling notion of free will. So perhaps these qualities could be \"emergent\" in the sense of requiring explanations which cannot be furnished by the physiology alone\n\n\n"}
{"id": "315426", "url": "https://en.wikipedia.org/wiki?curid=315426", "title": "Mediocrity principle", "text": "Mediocrity principle\n\nThe mediocrity principle is the philosophical notion that \"if an item is drawn at random from one of several sets or categories, it's likelier to come from the most numerous category than from any one of the less numerous categories\". The principle has been taken to suggest that there is nothing very unusual about the evolution of the Solar System, Earth's history, the evolution of biological complexity, human evolution, or any one nation. It is a heuristic in the vein of the Copernican principle, and is sometimes used as a philosophical statement about the place of humanity. The idea is to assume mediocrity, rather than starting with the assumption that a phenomenon is special, privileged, exceptional, or even superior.\n\nThe mediocrity principle suggests, given the existence of life on Earth, that life typically exists on Earth-like planets throughout the universe.\n\nThe mediocrity principle is in contrast with the anthropic principle, which asserts that the presence of an intelligent observer (humans) limits the circumstances to bounds under which intelligent life can be observed to exist, no matter how improbable. Both stand in contrast to the fine-tuning hypothesis, which asserts that the natural conditions for intelligent life are implausibly rare.\n\nThe mediocrity principle implies that Earth-like environments are necessarily common, based in part on the evidence of any happening at all, whereas the anthropic principle suggests that no assertion can be made about the probability of intelligent life based on a sample set of one (self-described) example, who are necessarily capable of making such an assertion about themselves.\n\nIt is also possible to handle the Mediocrity Principle as a statistical problem, a case of a single Data point statistics, also present in the German tank problem.\n\nDavid Deutsch argues that the mediocrity principle is incorrect from a physical point of view, in reference to either humanity's part of the universe or to its species. Deutsch refers to Stephen Hawking's quote: \"The human race is just a chemical scum on a moderate-sized planet, orbiting around a very average star in the outer suburb of one among a hundred billion galaxies\". Deutsch wrote that Earth's neighborhood in the universe is not typical (80% of the universe's matter is dark matter) and that a concentration of mass such as the Solar System is an \"isolated, uncommon phenomenon\". He also disagrees with Richard Dawkins, who considers that humans, because of natural evolution, are limited to the capabilities of their species. Deutsch responds that even though evolution did not give humans the ability to detect neutrinos, scientists can currently detect them, which significantly expands their capabilities beyond what is available as a result of evolution.\n\n"}
{"id": "27132425", "url": "https://en.wikipedia.org/wiki?curid=27132425", "title": "New Earth (Christianity)", "text": "New Earth (Christianity)\n\nThe New Earth is an expression used in the Book of Isaiah (Is 65:17 & 66:22), 2 Peter (2 Peter 3:13), and the Book of Revelation (Rev 21:1) in the Bible to describe the final state of redeemed humanity. It is one of the central doctrines of Christian eschatology and is referred to in the Nicene Creed as the world to come.\n\nThe twenty-first chapter of the Book of Revelation introduces the final state of perfection where, according to one commentator, \"cosmic time has been turned into eternity.\" In symbolic and visual language, God allows John to see the glory and beauty of the inheritance of His people. The first thing the reader notices about this vision is that it includes a \"new heavens and a new earth\" (21:1). To understand what the Bible teaches about eternity, the reader of the Apocalypse must understand the New Testament doctrine of the \"New Heavens and the New Earth.\" \n\nThe basic difference with the promises of the Old Testament is that in Revelation they also have an ontological value (: \"Then I saw 'a new heaven and a new earth,' for the first heaven and the first earth had passed away, and there was no longer any sea...'He will wipe every tear from their eyes. There will be no more death' or mourning or crying or pain, for the old order of things has passed away\") and no longer just gnosiological (: \"See, I will create/new heavens and a new earth./The former things will not be remembered,/nor will they come to mind\").\n\nBut, in accordance with his promise, we wait for new heavens and a new earth, where righteousness is at home ().\n\nIn Koine Greek, there were two words that are translated as \"new\" in the English Bible; \"neos\" and \"kainos\". One Greek resource states:\n\nThat \"kainos\" should not be taken as something totally new can be seen in a passage like the following:\nHere the Apostle Paul uses \"kainos\" in the expression \"new creation.\" Paul did not intend to convey the idea that this is a completely different individual. There is continuity between the old person and the new person to such an extent that it remains the same person, but renovated. The person is the same, but the quality of that person has been transformed.\n\nIn the same way, the biblical concept of the New Earth is one of renovation and restoration. Either on this current earth or on rebuilt new planet. This conclusion is supported by Peter's words in his public speech in the temple at Jerusalem. \n\nThis earth, however, will be either cleansed or destroyed by fire for the purpose of restoration as expressed in the following passage:\n\n\n"}
{"id": "49516442", "url": "https://en.wikipedia.org/wiki?curid=49516442", "title": "Phylogenetic inertia", "text": "Phylogenetic inertia\n\nPhylogenetic inertia or phylogenetic constraint refers to the limitations on the future evolutionary pathways that have been imposed by previous adaptations.\n\nCharles Darwin first recognized this phenomenon, though the term was later coined by Huber in 1939. Darwin explained the idea of phylogenetic inertia based on his observations; he spoke about it when explaining the \"Law of Conditions of Existence\". Darwin also suggested that, after speciation, the organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa. This is the main concept of phylogenetic inertia.\n\nRichard Dawkins also explained these constraints by likening natural selection to a river in his 1982 book \"The Extended Phenotype\".\n\n\nBirds are the only speciose group of vertebrates that are exclusively oviparous, or egg laying. It has been suggested that birds are phylogenetically constrained, as being derived from reptiles, and likely have not overcome this constraint or diverged far enough away to develop viviparity, or live birth.\n\n\n\nThere have been several studies that have been able to effectively test for phylogenetic inertia when looking into shared traits; predominantly with a comparative methods approach. Some have used comparative methods and found evidence for certain traits attributed to adaptation, and some to phylogeny; there were also numerous traits that could be attributed to both. Another study developed a new method of comparative examination that showed to be a powerful predictor of phylogenetic inertia in a variety of situations. It was called Phylogenetic Eigenvector Regression (PVR), which runs principal component analyses between species on a pairwise phylogenetic distance matrix. In another, different study, the authors described methods for measuring phylogenetic inertia, looked at effectiveness of various comparative methods, and found that different methods can reveal different aspects of drivers. Autoregression and PVR showed good results with morphological traits.\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "5764764", "url": "https://en.wikipedia.org/wiki?curid=5764764", "title": "Radiation trapping", "text": "Radiation trapping\n\nRadiation trapping, imprisonment of resonance radiation, radiative transfer of spectral lines, line transfer or radiation diffusion is a phenomenon in physics whereby radiation may be \"trapped\" in a system as it is emitted by one atom and absorbed by another.\n"}
{"id": "513965", "url": "https://en.wikipedia.org/wiki?curid=513965", "title": "Rammed earth", "text": "Rammed earth\n\nRammed earth, also known as taipa in Portuguese, tapial or tapia in Spanish, pisé (de terre) in French, and hangtu (), is a technique for constructing foundations, floors, and walls using natural raw materials such as earth, chalk, lime, or gravel. It is an ancient method that has been revived recently as a sustainable building material used in a technique of natural building.\n\nRammed earth is simple to manufacture, non-combustible, thermally massive, strong, and durable. However, structures such as walls can be laborious to construct of rammed earth without machinery, e. g., powered tampers, and they are susceptible to water damage if inadequately protected or maintained.\n\nEdifices formed of rammed earth are on every continent except Antarctica, in a range of environments including temperate, wet, semiarid desert, montane, and tropical regions. The availability of suitable soil and a building design appropriate for local climatic conditions are the factors that favour its use.\n\nManufacturing rammed earth involves compressing a damp mixture of earth that has suitable proportions of sand, gravel, clay, and/or an added stabilizer into an externally supported frame or mold, forming either a solid wall or individual blocks. Historically, additives such as lime or animal blood were used to stabilize it, while modern construction adds lime, cement, or asphalt emulsions. To add variety, some modern builders also add coloured oxides or other materials, e.g. bottles, tires, or pieces of timber.\n\nThe construction of an entire wall begins with a temporary frame, denominated the \"formwork\", which is usually made of wood or plywood, as a mold for the desired shape and dimensions of each section of wall. The form must be durable and well braced, and the two opposing faces must be clamped together to prevent bulging or deformation caused by the large compressing forces. Damp material is poured into the formwork to a depth of and then compacted to approximately 50% of its original height. The material is compressed iteratively, in batches or courses, so as to gradually erect the wall up to the top of the formwork. Tamping was historically manual with a long ramming pole, and was very laborious, but modern construction can be made less so by employing pneumatically powered tampers.\n\nAfter a wall is complete, it is sufficiently strong to immediately remove the formwork. This is necessary if a surface texture is to be applied, e.g., by wire brushing, carving, or mold impression, because the walls become too hard to work after approximately one hour. Construction is optimally done in warm weather so that the walls can dry and harden. The compression strength of the rammed earth increases as it cures; some time is necessary for it to dry and as long as two years can be necessary for complete curing. Exposed walls must be sealed to prevent water damage.\n\nIn modern variations of the technique, rammed-earth walls are constructed on top of conventional footings or a reinforced concrete slab base.\n\nWhere blocks made of rammed earth are used, they are generally stacked like regular blocks and are bonded together with a thin mud slurry instead of cement. Special machines, usually powered by small engines and often portable, are used to compress the material into blocks.\n\nPresently more than 30% of the world's population uses earth as a building material. Rammed earth has been used globally in a wide range of climatic conditions. Rammed-earth housing may resolve homelessness caused by otherwise expensive construction techniques.\n\nThe compressive strength of rammed earth is a maximum of . This is less than that of concrete but more than sufficiently strong for domestic edifices. Indeed, properly constructed rammed earth endures for thousands of years, as many ancient structures that are still standing around the world demonstrate. Rammed earth reinforced with rebar, wood, or bamboo can prevent collapse caused by earthquakes or heavy storms, because unreinforced edifices of rammed earth resist earthquake damage extremely poorly. See 1960 Agadir earthquake for an example of the total destruction which may be inflicted on such structures by an earthquake. Adding cement to soil mixtures poor in clay can also increase the load-bearing capacity of rammed-earth edifices. The United States Department of Agriculture observed in 1925 that rammed-earth structures endure indefinitely and can be constructed for less than two-thirds of the cost of standard frame houses.\n\nSoil is a widely available, inexpensive, and sustainable resource. Therefore, construction with rammed earth is very viable. Unskilled labour can do most of the necessary work. While the cost of rammed earth is low, rammed-earth construction without mechanical tools is very time-consuming and laborious; however, with a mechanical tamper and prefabricated formwork it can require only two or three days to construct the walls of a house.\n\nOne significant benefit of rammed earth is its high thermal mass: like brick or concrete, it can absorb heat during daytime and nocturnally release it. This action moderates daily temperature variations and reduces the need for air conditioning and heating. In colder climates, rammed-earth walls can be insulated with Styrofoam or a similar insert. It must also be protected from heavy rain and insulated with vapour barriers.\n\nRammed earth can effectively regulate humidity if unclad walls containing clay are exposed to an internal space. Humidity is regulated between 40% and 60%, which is the ideal range for asthma sufferers and for the storage of susceptible objects such as books. The material mass and clay content of rammed earth allows an edifice to breathe more than concrete edifices, which avoids problems of condensation but prevents significant loss of heat.\n\nUntouched, rammed-earth walls have the colour and texture of natural earth. Moisture-impermeable finishes, such as cement render, are avoided because they impair the ability of a wall to desorb moisture, which quality is necessary to preserve its strength. Well-cured walls accept nails and screws easily, and can be effectively patched with the same material used to build them. Blemishes can be repaired using the soil mixture as a plaster and sanded smooth.\n\nThe thickness, typically , and density of rammed-earth walls make them suitable for soundproofing. They are also inherently fireproof, resistant to termite damage, and non-toxic.\n\nEdifices of rammed earth are thought to be more sustainable and environmentally friendly than popular techniques of construction. Because rammed-earth edifices use locally available materials, they usually have low embodied energy and generate very little waste. The soils used are typically subsoils low in clay, between 5% and 15%, which conserve the topsoil for agriculture. When the soil excavated in preparation for a foundation can be used, the cost and energy consumption of transportation are minimal. Rammed earth is probably the least environmentally detrimental construction material and technique that is readily and commercially available today to construct solid masonry edifices. Rammed earth has potentially low manufacturing impact, contingent on the amount of cement and the amount that is locally sourced; it is often quarried aggregates rather than \"earth\".\n\nFormwork is removable and can be reused, reducing the need for lumber.\nMixing cement with the soil can counteract sustainable benefits such as low embodied energy and humidity regulation because manufacture of the cement itself adds to the global carbon dioxide burden at a rate of 1.25 tonnes per tonne of cement produced. Partial substitution of cement with alternatives such as ground granulated blast furnace slag has not been demonstrated to be effective, and implicates other questions of sustainability.\n\nRammed earth can contribute to the overall energy efficiency of edifices: the density, thickness, and thermal conductivity of rammed earth render it an especially suitable material for passive solar heating. Warmth requires almost 12 hours to be conducted through a wall thick.\n\nRammed-earth construction may also reduce the ecological impacts of deforestation and the toxicity of artificial materials associated with conventional construction techniques.\n\nAlthough it has low greenhouse emissions in theory, transportation and the production of cement can add significantly to the overall emissions of modern rammed earth construction. The most basic kind of traditional rammed earth has very low greenhouse gas emissions but the more engineered and processed variant of rammed earth has the potential for significant emissions.\n\nEvidence of ancient use of rammed earth has been found in Neolithic archaeological sites of the Yangshao and Longshan cultures along the Yellow River in China, dating to 5000 BCE. By 2000 BCE, rammed-earth architectural techniques (夯土 \"Hāng tǔ\") were commonly used for walls and foundations in China.\n\nIn the 1800s, rammed earth was popularized in the United States by the book \"Rural Economy\" by S. W. Johnson. The technique was used to construct the Borough House Plantation and the Church of the Holy Cross in Stateburg, South Carolina, both being National Historic Landmarks.\nAn outstanding example of a rammed-earth edifice in Canada is St. Thomas Anglican Church in Shanty Bay, Ontario, erected between 1838 and 1841.\n\nFrom the 1920s through the 1940s rammed-earth construction in the US was studied. South Dakota State College extensively researched and constructed almost one hundred weathering walls of rammed earth. For over 30 years the college investigated the use of paints and plasters in relation to colloids in soil. In 1945, Clemson Agricultural College of South Carolina published the results of their research of rammed earth in a pamphlet titled \"Rammed Earth Building Construction\". In 1936, on a homestead near Gardendale, Alabama, the United States Department of Agriculture constructed an experimental community of rammed-earth edifices with architect Thomas Hibben. The houses were inexpensively constructed and were sold to the public along with sufficient land for gardens and small plots for livestock. The project successfully provided valuable homes to low-income families.\n\nThe US Agency for International Development is working with undeveloped countries to improve the engineering of rammed-earth houses. It also financed the authorship of the \"Handbook of Rammed Earth\" by Texas A&M University and the Texas Transportation Institute. The \"Handbook\" was unavailable for purchase by the public until the Rammed Earth Institute International gained permission to reprint it.\n\nInterest in rammed earth declined after World War II when the cost of modern construction materials decreased. Rammed earth was considered substandard, and still is opposed by many contractors, engineers, and tradesmen who are unfamiliar with earthen construction techniques. The prevailing perception that such materials and techniques perform poorly in regions prone to earthquakes has prevented their use in much of the world. In Chile, for example, rammed earth edifices normally cannot be conventionally insured against damage or even be approved by the government.\n\nA notable example of 21st century use of rammed earth is the façade of the Nk'Mip Desert Cultural Centre in southern British Columbia, Canada. As of 2014 it is the largest rammed earth wall in North America.\n\n\nRammed earth wall construction at Central Arizona College\n"}
{"id": "827792", "url": "https://en.wikipedia.org/wiki?curid=827792", "title": "Rare Earth hypothesis", "text": "Rare Earth hypothesis\n\nIn planetary astronomy and astrobiology, the Rare Earth hypothesis argues that the origin of life and the evolution of biological complexity such as sexually reproducing, multicellular organisms on Earth (and, subsequently, human intelligence) required an improbable combination of astrophysical and geological events and circumstances.\n\nAccording to the hypothesis, complex extraterrestrial life is an improbable phenomenon and likely to be rare. The term \"Rare Earth\" originates from \"Rare Earth: Why Complex Life Is Uncommon in the Universe\" (2000), a book by Peter Ward, a geologist and paleontologist, and Donald E. Brownlee, an astronomer and astrobiologist, both faculty members at the University of Washington.\n\nA contrary view was argued in the 1970s and 1980s by Carl Sagan and Frank Drake, among others. It holds that Earth is a typical rocky planet in a typical planetary system, located in a non-exceptional region of a common barred-spiral galaxy. Given the principle of mediocrity (in the same vein as the Copernican principle), it is probable that we are typical, and the universe teems with complex life. However, Ward and Brownlee argue that planets, planetary systems, and galactic regions that are as friendly to complex life as the Earth, the Solar System, and our galactic region are rare.\n\nThe Rare Earth hypothesis argues that the evolution of biological complexity requires a host of fortuitous circumstances, such as a galactic habitable zone, a central star and planetary system having the requisite character, the circumstellar habitable zone, a right-sized terrestrial planet, the advantage of a gas giant guardian like Jupiter and a large natural satellite, conditions needed to ensure the planet has a magnetosphere and plate tectonics, the chemistry of the lithosphere, atmosphere, and oceans, the role of \"evolutionary pumps\" such as massive glaciation and rare bolide impacts, and whatever led to the appearance of the eukaryote cell, sexual reproduction and the Cambrian explosion of animal, plant, and fungi phyla. The evolution of human intelligence may have required yet further events, which are extremely unlikely to have happened were it not for the Cretaceous–Paleogene extinction event 66 million years ago removing dinosaurs as the dominant terrestrial vertebrates.\n\nIn order for a small rocky planet to support complex life, Ward and Brownlee argue, the values of several variables must fall within narrow ranges. The universe is so vast that it could contain many Earth-like planets. But if such planets exist, they are likely to be separated from each other by many thousands of light years. Such distances may preclude communication among any intelligent species evolving on such planets, which would solve the Fermi paradox: \"If extraterrestrial aliens are common, why aren't they obvious?\"\n\n\"Rare Earth\" suggests that much of the known universe, including large parts of our galaxy, are \"dead zones\" unable to support complex life. Those parts of a galaxy where complex life is possible make up the galactic habitable zone, primarily characterized by distance from the Galactic Center. As that distance increases:\nItem #1 rules out the outer reaches of a galaxy; #2 and #3 rule out galactic inner regions. Hence a galaxy's habitable zone may be a ring sandwiched between its uninhabitable center and outer reaches.\n\nAlso, a habitable planetary system must maintain its favorable location long enough for complex life to evolve. A star with an eccentric (elliptic or hyperbolic) galactic orbit will pass through some spiral arms, unfavorable regions of high star density; thus a life-bearing star must have a galactic orbit that is nearly circular, with a close synchronization between the orbital velocity of the star and of the spiral arms. This further restricts the galactic habitable zone within a fairly narrow range of distances from the Galactic Center. Lineweaver et al. calculate this zone to be a ring 7 to 9 kiloparsecs in radius, including no more than 10% of the stars in the Milky Way, about 20 to 40 billion stars. Gonzalez, et al. would halve these numbers; they estimate that at most 5% of stars in the Milky Way fall in the galactic habitable zone.\n\nApproximately 77% of observed galaxies are spiral, two-thirds of all spiral galaxies are barred, and more than half, like the Milky Way, exhibit multiple arms. According to Rare Earth, our own galaxy is unusually quiet and dim (see below), representing just 7% of its kind. Even so, this would still represent more than 200 billion galaxies in the known universe.\n\nOur galaxy also appears unusually favorable in suffering fewer collisions with other galaxies over the last 10 billion years, which can cause more supernovae and other disturbances. Also, the Milky Way's central black hole seems to have neither too much nor too little activity.\n\nThe orbit of the Sun around the center of the Milky Way is indeed almost perfectly circular, with a period of 226 Ma (million years), closely matching the rotational period of the galaxy. However, the majority of stars in barred spiral galaxies populate the spiral arms rather than the halo and tend to move in gravitationally aligned orbits, so there is little that is unusual about the Sun's orbit. While the Rare Earth hypothesis predicts that the Sun should rarely, if ever, have passed through a spiral arm since its formation, astronomer Karen Masters has calculated that the orbit of the Sun takes it through a major spiral arm approximately every 100 million years. Some researchers have suggested that several mass extinctions do correspond with previous crossings of the spiral arms.\n\nThe terrestrial example suggests that complex life requires liquid water, requiring an orbital distance neither too close nor too far from the central star, another scale of habitable zone or Goldilocks Principle: \nThe habitable zone varies with the star's type and age.\n\nFor advanced life, the star must also be highly stable, which is typical of middle star life, about 4.6 billion years old. Proper metallicity and size are also important to stability. The Sun has a low 0.1% luminosity variation. To date no solar twin star twin, with an exact match of the sun's luminosity variation, has been found, though some come close. The star must have no stellar companions, as in binary systems, which would disrupt the orbits of planets. Estimates suggest 50% or more of all star systems are binary. The habitable zone for a main sequence star very gradually moves out over its lifespan until it becomes a white dwarf and the habitable zone vanishes.\n\nThe liquid water and other gases available in the habitable zone bring the benefit of greenhouse warming. Even though the Earth's atmosphere contains a water vapor concentration from 0% (in arid regions) to 4% (in rain forest and ocean regions) and – as of February 2018 – only 408.05 parts per million of , these small amounts suffice to raise the average surface temperature by about 40 °C, with the dominant contribution being due to water vapor, which together with clouds makes up between 66% and 85% of Earth's greenhouse effect, with contributing between 9% and 26% of the effect.\n\nRocky planets must orbit within the habitable zone for life to form. Although the habitable zone of such hot stars as Sirius or Vega is wide, hot stars also emit much more ultraviolet radiation that ionizes any planetary atmosphere. They may become red giants before advanced life evolves on their planets.\nThese considerations rule out the massive and powerful stars of type F6 to O (see stellar classification) as homes to evolved metazoan life.\n\nSmall red dwarf stars conversely have small habitable zones wherein planets are in tidal lock, with one very hot side always facing the star and another very cold side; and they are also at increased risk of solar flares (see Aurelia). Life therefore cannot arise in such systems. Rare Earth proponents claim that only stars from F7 to K1 types are hospitable. Such stars are rare: G type stars such as the Sun (between the hotter F and cooler K) comprise only 9% of the hydrogen-burning stars in the Milky Way.\n\nSuch aged stars as red giants and white dwarfs are also unlikely to support life. Red giants are common in globular clusters and elliptical galaxies. White dwarfs are mostly dying stars that have already completed their red giant phase. Stars that become red giants expand into or overheat the habitable zones of their youth and middle age (though theoretically planets at a much greater distance may become habitable).\n\nAn energy output that varies with the lifetime of the star will likely prevent life (e.g., as Cepheid variables). A sudden decrease, even if brief, may freeze the water of orbiting planets, and a significant increase may evaporate it and cause a greenhouse effect that prevents the oceans from reforming.\n\nAll known life requires the complex chemistry of metallic elements. The absorption spectrum of a star reveals the presence of metals within, and studies of stellar spectra reveal that many, perhaps most, stars are poor in metals. Because heavy metals originate in supernova explosions, metallicity increases in the universe over time. Low metallicity characterizes the early universe: globular clusters and other stars that formed when the universe was young, stars in most galaxies other than large spirals, and stars in the outer regions of all galaxies. Metal-rich central stars capable of supporting complex life are therefore believed to be most common in the quiet suburbs of the larger spiral galaxies—where radiation also happens to be weak.\n\nRare Earth proponents argue that a planetary system capable of sustaining complex life must be structured more or less like the Solar System, with small and rocky inner planets and outer gas giants. Without the protection of 'celestial vacuum cleaner' planets with strong gravitational pull, a planet would be subject to more catastrophic asteroid collisions.\n\nObservations of exo-planets have shown that arrangements of planets similar to our Solar System are rare. Most planetary systems have super Earths, several times larger than Earth, close to their star, whereas our Solar System's inner region has only a few small rocky planets and none inside Mercury's orbit. Only 10% of stars have giant planets similar to Jupiter and Saturn, and those few rarely have stable nearly circular orbits distant from their star. Konstantin Batygin and colleagues argue that these features can be explained if, early in the history of the Solar System, Jupiter and Saturn drifted towards the Sun, sending showers of planetesimals towards the super-Earths which sent them spiralling into the Sun, and ferrying icy building blocks into the terrestrial region of the Solar System which provided the building blocks for the rocky planets. The two giant planets then drifted out again to their present position. However, in the view of Batygin and his colleagues: \"The concatenation of chance events required for this delicate choreography suggest that small, Earth-like rocky planets – and perhaps life itself – could be rare throughout the cosmos.\"\n\nRare Earth argues that a gas giant must not be too close to a body where life is developing. Close placement of gas giant(s) could disrupt the orbit of a potential life-bearing planet, either directly or by drifting into the habitable zone.\n\nNewtonian dynamics can produce chaotic planetary orbits, especially in a system having large planets at high orbital eccentricity.\n\nThe need for stable orbits rules out stars with systems of planets that contain large planets with orbits close to the host star (called \"hot Jupiters\"). It is believed that hot Jupiters have migrated inwards to their current orbits. In the process, they would have catastrophically disrupted the orbits of any planets in the habitable zone. To exacerbate matters, hot Jupiters are much more common orbiting F and G class stars.\n\nIt is argued that life requires terrestrial planets like Earth and as gas giants lack such a surface, that complex life cannot arise there.\n\nA planet that is too small cannot hold much atmosphere, making surface temperature low and variable and oceans impossible. A small planet will also tend to have a rough surface, with large mountains and deep canyons. The core will cool faster, and plate tectonics may be brief or entirely absent. A planet that is too large will retain too dense an atmosphere like Venus. Although Venus is similar in size and mass to Earth, its surface atmospheric pressure is 92 times that of Earth, and surface temperature of 735 K (462 °C; 863 °F). Earth had a similar early atmosphere to Venus, but may have lost it in the giant impact event.\n\nRare Earth proponents argue that plate tectonics and a strong magnetic field are essential for biodiversity, global temperature regulation, and the carbon cycle.\nThe lack of mountain chains elsewhere in the Solar System is direct evidence that Earth is the only body with plate tectonics, and thus the only nearby body capable of supporting life.\n\nPlate tectonics depend on the right chemical composition and a long-lasting source of heat from radioactive decay. Continents must be made of less dense felsic rocks that \"float\" on underlying denser mafic rock. Taylor emphasizes that tectonic subduction zones require the lubrication of oceans of water. Plate tectonics also provides a means of biochemical cycling.\n\nPlate tectonics and as a result continental drift and the creation of separate land masses would create diversified ecosystems and biodiversity, one of the strongest defences against extinction. An example of species diversification and later competition on Earth's continents is the Great American Interchange. North and Middle America drifted into South America at around 3.5 to 3 Ma. The fauna of South America evolved separately for about 30 million years, since Antarctica separated. Many species were subsequently wiped out in mainly South America by competing Northern American animals.\n\nThe Moon is unusual because the other rocky planets in the Solar System either have no satellites (Mercury and Venus), or only tiny satellites which are probably captured asteroids (Mars).\n\nThe Giant-impact theory hypothesizes that the Moon resulted from the impact of a Mars-sized body, dubbed Theia, with the young Earth. This giant impact also gave the Earth its axial tilt (inclination) and velocity of rotation. Rapid rotation reduces the daily variation in temperature and makes photosynthesis viable. The \"Rare Earth\" hypothesis further argues that the axial tilt cannot be too large or too small (relative to the orbital plane). A planet with a large tilt will experience extreme seasonal variations in climate. A planet with little or no tilt will lack the stimulus to evolution that climate variation provides. In this view, the Earth's tilt is \"just right\". The gravity of a large satellite also stabilizes the planet's tilt; without this effect the variation in tilt would be chaotic, probably making complex life forms on land impossible.\n\nIf the Earth had no Moon, the ocean tides resulting solely from the Sun's gravity would be only half that of the lunar tides. A large satellite gives rise to tidal pools, which may be essential for the formation of complex life, though this is far from certain.\n\nA large satellite also increases the likelihood of plate tectonics through the effect of tidal forces on the planet's crust. The impact that formed the Moon may also have initiated plate tectonics, without which the continental crust would cover the entire planet, leaving no room for oceanic crust. It is possible that the large scale mantle convection needed to drive plate tectonics could not have emerged in the absence of crustal inhomogeneity. A further theory indicates that such a large moon may also contribute to maintaining a planet's magnetic shield by continually acting upon a metallic planetary core as dynamo, thus protecting the surface of the planet from charged particles and cosmic rays, and helping to ensure the atmosphere is not stripped over time by solar winds.\n\nA terrestrial planet of the right size is needed to retain an atmosphere, like Earth and Venus. On Earth, once the giant impact of Theia thinned Earth's atmosphere, other events were needed to make the atmosphere capable of sustaining life. The Late Heavy Bombardment reseeded Earth with water lost after the impact of Theia. The development of an ozone layer formed protection from ultraviolet (UV) sunlight. Nitrogen and carbon dioxide are needed in a correct ratio for life to form. Lightning is needed for nitrogen fixation. The carbon dioxide gas needed for life comes from sources such as volcanoes and geysers. Carbon dioxide is only needed at low levels (currently at 400 ppm); at high levels it is poisonous. Precipitation is needed to have a stable water cycle. A proper atmosphere must reduce diurnal temperature variation.\n\nRegardless of whether planets with similar physical attributes to the Earth are rare or not, some argue that life usually remains simple bacteria. Biochemist Nick Lane argues that simple cells (prokaryotes) emerged soon after Earth's formation, but since almost half the planet's life had passed before they evolved into complex ones (eukaryotes) all of whom share a common ancestor, this event can only have happened once. In some views, prokaryotes lack the cellular architecture to evolve into eukaryotes because a bacterium expanded up to eukaryotic proportions would have tens of thousands of times less energy available; two billion years ago, one simple cell incorporated itself into another, multiplied, and evolved into mitochondria that supplied the vast increase in available energy that enabled the evolution of complex life. If this incorporation occurred only once in four billion years or is otherwise unlikely, then life on most planets remains simple. An alternative view is that mitochondria evolution was environmentally triggered, and that mitochondria-containing organisms appeared soon after the first traces of atmospheric oxygen.\n\nThe evolution and persistence of sexual reproduction is another mystery in biology. The purpose of sexual reproduction is unclear, as in many organisms it has a 50% cost (fitness disadvantage) in relation to asexual reproduction. Mating types (types of gametes, according to their compatibility) may have arisen as a result of anisogamy (gamete dimorphism), or the male and female genders may have evolved before anisogamy. It is also unknown why most sexual organisms use a binary mating system, and why some organisms have gamete dimorphism. Charles Darwin was the first to suggest that sexual selection drives speciation; without it, complex life would probably not have evolved.\n\nWhile life on Earth is regarded to have spawned relatively early in the planet's history, the evolution from multicellular to intelligent organisms took around 800 million years. Civilizations on Earth have existed for about 12,000 years and radio communication reaching space has existed for less than 100 years. Relative to the age of the Solar System (~4.57 Ga) this is a short time, in which extreme climatic variations, super volcanoes, and large meteorite impacts were absent. These events would severely harm intelligent life, as well as life in general. For example, the Permian-Triassic mass extinction, caused by widespread and continuous volcanic eruptions in an area the size of Western Europe, led to the extinction of 95% of known species around 251.2 Ma ago. About 65 million years ago, the Chicxulub impact at the Cretaceous–Paleogene boundary (~65.5 Ma) on the Yucatán peninsula in Mexico led to a mass extinction of the most advanced species at that time.\n\nIf there were intelligent extraterrestrial civilizations able to make contact with distant Earth, they would have to live in the same 12Ka period of the 800Ma evolution of life.\n\nThe following discussion is adapted from Cramer. The Rare Earth equation is Ward and Brownlee's riposte to the Drake equation. It calculates formula_1, the number of Earth-like planets in the Milky Way having complex life forms, as:\n\nwhere:\nWe assume formula_5. The Rare Earth hypothesis can then be viewed as asserting that the product of the other nine Rare Earth equation factors listed below, which are all fractions, is no greater than 10 and could plausibly be as small as 10. In the latter case, formula_1 could be as small as 0 or 1. Ward and Brownlee do not actually calculate the value of formula_1, because the numerical values of quite a few of the factors below can only be conjectured. They cannot be estimated simply because we have but one data point: the Earth, a rocky planet orbiting a G2 star in a quiet suburb of a large barred spiral galaxy, and the home of the only intelligent species we know, namely ourselves.\n\nThe Rare Earth equation, unlike the Drake equation, does not factor the probability that complex life evolves into intelligent life that discovers technology (Ward and Brownlee are not evolutionary biologists). Barrow and Tipler review the consensus among such biologists that the evolutionary path from primitive Cambrian chordates, e.g., \"Pikaia\" to \"Homo sapiens\", was a highly improbable event. For example, the large brains of humans have marked adaptive disadvantages, requiring as they do an expensive metabolism, a long gestation period, and a childhood lasting more than 25% of the average total life span. Other improbable features of humans include:\n\nWriters who support the Rare Earth hypothesis:\n\nCases against the Rare Earth Hypothesis take various forms.\n\nThe hypothesis concludes, more or less, that complex life is rare because it can evolve only on the surface of an Earth-like planet or on a suitable satellite of a planet. Some biologists, such as Jack Cohen, believe this assumption too restrictive and unimaginative; they see it as a form of circular reasoning.\n\nAccording to David Darling, the Rare Earth hypothesis is neither hypothesis nor prediction, but merely a description of how life arose on Earth. In his view Ward and Brownlee have done nothing more than select the factors that best suit their case.\n\nWhat matters is not whether there's anything unusual about the Earth; there's going to be something idiosyncratic about every planet in space. What matters is whether any of Earth's circumstances are not only unusual but also essential for complex life. So far we've seen nothing to suggest there is.\n\nCritics also argue that there is a link between the Rare Earth Hypothesis and the creationist ideas of intelligent design.\n\nAn increasing number of extrasolar planet discoveries are being made with planets in planetary systems known as of . Rare Earth proponents argue life cannot arise outside Sun-like systems. However, some exobiologists have suggested that stars outside this range may give rise to life under the right circumstances; this possibility is a central point of contention to the theory because these late-K and M category stars make up about 82% of all hydrogen-burning stars.\n\nCurrent technology limits the testing of important Rare Earth criteria: surface water, tectonic plates, a large moon and biosignatures are currently undetectable. Though planets the size of Earth are difficult to detect and classify, scientists now think that rocky planets are common around Sun-like stars. The Earth Similarity Index (ESI) of mass, radius and temperature provides a means of measurement, but falls short of the full Rare Earth criteria.\n\nSome argue that Rare Earth's estimates of rocky planets in habitable zones (formula_3 in the Rare Earth equation) are too restrictive. James Kasting cites the Titius-Bode law to contend that it is a misnomer to describe habitable zones as narrow when there is a 50% chance of at least one planet orbiting within one. In 2013 a study that was published in the journal Proceedings of the National Academy of Sciences calculated that about \"one in five\" of all sun-like stars are expected to have earthlike planets \"within the habitable zones of their stars\"; 8.8 billion of them therefore exist in the Milky Way galaxy alone. On 4 November 2013, astronomers reported, based on \"Kepler\" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars.\n\nThe requirement for a system to have a Jovian planet as protector (Rare Earth equation factor formula_15) has been challenged, affecting the number of proposed extinction events (Rare Earth equation factor formula_16). Kasting's 2001 review of Rare Earth questions whether a Jupiter protector has any bearing on the incidence of complex life. Computer modelling including the 2005 Nice model and 2007 Nice 2 model yield inconclusive results in relation to Jupiter's gravitational influence and impacts on the inner planets. A study by Horner and Jones (2008) using computer simulation found that while the total effect on all orbital bodies within the Solar System is unclear, Jupiter has caused more impacts on Earth than it has prevented. Lexell's Comet, a 1770 near miss that passed closer to Earth than any other comet in recorded history, was known to be caused by the gravitational influence of Jupiter. Grazier (2017) claims that the idea of Jupiter as a shield is a misinterpretation of a 1996 study by George Wetherill, and using computer models Grazier was able to demonstrate that Saturn protects Earth from more asteroids and comets than does Jupiter.\n\nWard and Brownlee argue that for complex life to evolve (Rare Earth equation factor formula_12), tectonics must be present to generate biogeochemical cycles, and predicted that such geological features would not be found outside of Earth, pointing to a lack of observable mountain ranges and subduction. There is, however, no scientific consensus on the evolution of plate tectonics on Earth. Though it is believed that tectonic motion first began around three billion years ago, by this time photosynthesis and oxygenation had already begun. Furthermore, recent studies point to plate tectonics as an episodic planetary phenomenon, and that life may evolve during periods of \"stagnant-lid\" rather than plate tectonic states.\n\nRecent evidence also points to similar activity either having occurred or continuing to occur elsewhere. The geology of Pluto, for example, described by Ward and Brownlee as \"without mountains or volcanoes ... devoid of volcanic activity\", has since been found to be quite the contrary, with a geologically active surface possessing organic molecules and mountain ranges like Tenzing Montes and Hillary Montes comparable in relative size to those of Earth, and observations suggest the involvement of endogenic processes. Plate tectonics has been suggested as a hypothesis for the Martian dichotomy, and in 2012 geologist An Yin put forward evidence for active plate tectonics on Mars. Europa has long been suspected to have plate tectonics and in 2014 NASA announced evidence of active subduction. In 2017, scientists studying the geology of Charon confirmed that icy plate tectonics also operated on Pluto's largest moon.\n\nKasting suggests that there is nothing unusual about the occurrence of plate tectonics in large rocky planets and liquid water on the surface as most should generate internal heat even without the assistance of radioactive elements. Studies by Valencia and Cowan suggest that plate tectonics may be inevitable for terrestrial planets Earth sized or larger, that is, Super-Earths, which are now known to be more common in planetary systems.\n\nThe hypothesis that molecular oxygen, necessary for animal life, is rare and that a Great Oxygenation Event (Rare Earth equation factor formula_12) could only have been triggered and sustained by tectonics, appears to have been invalidated by more recent discoveries.\n\nWard and Brownlee ask \"whether oxygenation, and hence the rise of animals, would ever have occurred on a world where there were no continents to erode\". Extraterrestrial free oxygen has recently been detected around other solid objects, including Mercury, Venus, Mars Jupiter's four Galilean moons, Saturn's moons Enceladus, Dione and Rhea and even the atmosphere of a comet. This has led scientists to speculate whether processes other than photosynthesis could be capable of generating an environment rich in free oxygen. Wordsworth (2014) concludes that oxygen generated other than through photodissociation may be likely on Earth-like exoplanets, and could actually lead to false positive detections of life. Narita (2015) suggests photocatalysis by titanium dioxide as a geochemical mechanism for producing oxygen atmospheres.\n\nSince Ward & Brownlee's assertion that \"there is irrefutable evidence that oxygen is a necessary ingredient for animal life\", anaerobic metazoa have been found that indeed do metabolise without oxygen. Spinoloricus nov. sp., for example, a species discovered in the hypersaline anoxic L'Atalante basin at the bottom of the Mediterranean Sea in 2010, appears to metabolise with hydrogen, lacking mitochondria and instead using hydrogenosomes. Stevenson (2015) has proposed other membrane alternatives for complex life in worlds without oxygen. In 2017, scientists from the NASA Astrobiology Institute discovered the necessary chemical preconditions for the formation of azotosomes on Saturn's moon Titan, a world that lacks atmospheric oxygen. Independent studies by Schirrmeister and by Mills concluded that Earth's multicellular life existed prior to the Great Oxygenation Event, not as a consequence of it.\n\nNASA scientists Hartman and McKay argue that plate tectonics may in fact slow the rise of oxygenation (and thus stymie complex life rather than promote it). Computer modelling by Tilman Spohn in 2014 found that plate tectonics on Earth may have arisen from the effects of complex life's emergence, rather than the other way around as the Rare Earth might suggest. The action of lichens on rock may have contributed to the formation of subduction zones in the presence of water. Kasting argues that if oxygenation caused the Cambrian explosion then any planet with oxygen producing photosynthesis should have complex life.\n\nThe importance of Earth's magnetic field to the development of complex life has been disputed. Kasting argues that the atmosphere provides sufficient protection against cosmic rays even during times of magnetic pole reversal and atmosphere loss by sputtering. Kasting also dismisses the role of the magnetic field in the evolution of eukaryotes, citing the age of the oldest known magnetofossils.\n\nThe requirement of a large moon (Rare Earth equation factor formula_14) has also been challenged. Even if it were required, such an occurrence may not be as unique as predicted by the Rare Earth Hypothesis. Recent work by Edward Belbruno and J. Richard Gott of Princeton University suggests that giant impactors such as those that may have formed the Moon can indeed form in planetary trojan points ( or Lagrangian point) which means that similar circumstances may occur in other planetary systems.\n\nRare Earth's assertion that the Moon's stabilization of Earth's obliquity and spin is a requirement for complex life has been questioned. Kasting argues that a moonless Earth would still possess habitats with climates suitable for complex life and questions whether the spin rate of a moonless Earth can be predicted. Although the giant impact theory posits that the impact forming the Moon increased Earth's rotational speed to make a day about 5 hours long, the Moon has slowly \"stolen\" much of this speed to reduce Earth's solar day since then to about 24 hours and continues to do so: in 100 million years Earth's solar day will be roughly 24 hours 38 minutes (the same as Mars's solar day); in 1 billion years, 30 hours 23 minutes. Larger secondary bodies would exert proportionally larger tidal forces that would in turn decelerate their primaries faster and potentially increase the solar day of a planet in all other respects like Earth to over 120 hours within a few billion years. This long solar day would make effective heat dissipation for organisms in the tropics and subtropics extremely difficult in a similar manner to tidal locking to a red dwarf star. Short days (high rotation speed) causes high wind speeds at ground level. Long days (slow rotation speed) cause the day and night temperatures to be too extreme.\n\nMany Rare Earth proponents argue that the Earth's plate tectonics would probably not exist if not for the tidal forces of the Moon. The hypothesis that the Moon's tidal influence initiated or sustained Earth's plate tectonics remains unproven, though at least one study implies a temporal correlation to the formation of the Moon. Evidence for the past existence of plate tectonics on planets like Mars which may never have had a large moon would counter this argument. Kasting argues that a large moon is not required to initiate plate tectonics.\n\nRare Earth proponents argue that simple life may be common, though complex life requires specific environmental conditions to arise. Critics consider life could arise on a moon of a gas giant, though this is less likely if life requires volcanicity. The moon must have stresses to induce tidal heating, but not so dramatic as seen on Jupiter's Io. However, the moon is within the gas giant's intense radiation belts, sterilizing any biodiversity before it can get established. Dirk Schulze-Makuch disputes this, hypothesizing alternative biochemistries for alien life. While Rare Earth proponents argue that only microbial extremophiles could exist in subsurface habitats beyond Earth, some argue that complex life can also arise in these environments. Examples of extremophile animals such as the \"Hesiocaeca methanicola\", an animal that inhabits ocean floor methane clathrates substances more commonly found in the outer Solar System, the tardigrades which can survive in the vacuum of space or \"Halicephalobus mephisto\" which exists in crushing pressure, scorching temperatures and extremely low oxygen levels 3.6 kilometres deep in the Earth's crust, are sometimes cited by critics as complex life capable of thriving in \"alien\" environments. Jill Tarter counters the classic counterargument that these species adapted to these environments rather than arose in them, by suggesting that we cannot assume conditions for life to emerge which are not actually known. There are suggestions that complex life could arise in sub-surface conditions which may be similar to those where life may have arisen on Earth, such as the tidally heated subsurfaces of Europa or Enceladus. Ancient circumvental ecosystems such as these support complex life on Earth such as Riftia pachyptila that exist completely independent of the surface biosphere.\n\n\n"}
{"id": "43530", "url": "https://en.wikipedia.org/wiki?curid=43530", "title": "Schist", "text": "Schist\n\nSchist (pronounced ) is a medium-grade metamorphic rock. Schist has medium to large, flat, sheet-like grains in a preferred orientation (nearby grains are roughly parallel). It is defined by having more than 50% platy and elongated minerals (such as micas or talc), often finely interleaved with quartz and feldspar. These lamellar (flat, planar) minerals include micas, chlorite, talc, hornblende, graphite, and others. Quartz often occurs in drawn-out grains to such an extent that a particular form called quartz schist is produced. Schist is often garnetiferous. Schist forms at a higher temperature and has larger grains than phyllite. Geological foliation (metamorphic arrangement in layers) with medium to large grained flakes in a preferred sheetlike orientation is called \"schistosity\".\n\nThe names of various schists are derived from their mineral constituents. For example, schists primarily composed of biotite and muscovite are called mica schists. Most schists are mica schists, but graphite and chlorite schists are also common. Schists are also named for their prominent or perhaps unusual mineral constituents, as in the case of garnet schist, tourmaline schist, and glaucophane schist.\n\nThe individual mineral grains in schist, drawn out into flaky scales by heat and pressure, can be seen with the naked eye. Schist is characteristically \"foliated\", meaning that the individual mineral grains split off easily into flakes or slabs. The word schist is derived ultimately from the Greek word \"σχίζειν\" (\"schízein\") meaning \"to split\", which is a reference to the ease with which schists can be split along the plane in which the platy minerals lie.\n\nMost schists are derived from clays and muds that have passed through a series of metamorphic processes involving the production of shales, slates and phyllites as intermediate steps. Certain schists are derived from fine-grained igneous rocks such as basalts and tuffs.\n\nBefore the mid-18th century, the terms slate, shale and schist were not sharply differentiated by those involved with mining. In the context of underground coal mining, shale was frequently referred to as slate well into the 20th century.\n\nDuring metamorphism, rocks which were originally sedimentary, igneous or metamorphic are converted into schists and gneisses. If the composition of the rocks was originally similar, they may be very difficult to distinguish from one another if the metamorphism has been great. A quartz-porphyry, for example, and a fine grained feldspathic sandstone, may both be converted into a grey or pink mica-schist. Usually, however, it is possible to distinguish between sedimentary and igneous schists and gneisses. If, for example, the whole district occupied by these rocks has traces of bedding, clastic structure, or unconformability, then it may be a sign that the original rock was sedimentary. In other cases intrusive junctions, chilled edges, contact alteration or porphyritic structure may prove that in its original condition a metamorphic gneiss was an igneous rock. The last appeal is often to the chemistry, for there are certain rock types which occur only as sediments, while others are found only among igneous masses, and however advanced the metamorphism may be, it rarely modifies the chemical composition of the mass very greatly. Such rocks as limestones, dolomites, quartzites and aluminous shales have very definite chemical characteristics which distinguish them even when completely recrystallized.\n\nThe schists are classified principally according to the minerals they consist of and on their chemical composition. For example, many metamorphic limestones, marbles, and calc-schists, with crystalline dolomites, contain silicate minerals such as mica, tremolite, diopside, scapolite, quartz and feldspar. They are derived from calcareous sediments of different degrees of purity. Another group is rich in quartz (quartzites, quartz schists and quartzose gneisses), with variable amounts of white and black mica, garnet, feldspar, zoisite and hornblende. These were once sandstones and arenaceous rocks. The graphitic schists may readily be believed to represent sediments once containing coal or plant remains; there are also schistose ironstones (hematite-schists), but metamorphic beds of salt or gypsum are exceedingly uncommon. Among schists of igneous origin there are the silky calc-schists, the foliated serpentines (once ultramafic masses rich in olivine), and the white mica-schists, porphyroids and banded halleflintas, which have been derived from rhyolites, quartz-porphyries and felsic tuffs. The majority of mica-schists, however, are altered claystones and shales, and pass into the normal sedimentary rocks through various types of phyllite and mica-slates. They are among the most common metamorphic rocks; some of them are graphitic and others calcareous. The diversity in appearance and composition is very great, but they form a well-defined group not difficult to recognize, from the abundance of black and white micas and their thin, foliated, schistose character. A subgroup is the andalusite-, staurolite-, kyanite- and sillimanite-schists which usually make their appearance in the vicinity of gneissose granites, and have presumably been affected by contact metamorphism.\n\nIn geotechnical engineering a schistosity plane often forms a discontinuity that may have a large influence on the mechanical behavior (strength, deformation, etc.) of rock masses in, for example, tunnel, foundation, or slope construction.\n\n\n"}
{"id": "4968799", "url": "https://en.wikipedia.org/wiki?curid=4968799", "title": "Sky brightness", "text": "Sky brightness\n\nSky brightness refers to the visual perception of the sky and how it scatters and diffuses light. The fact that the sky is not completely dark at night is easily visible. If light sources (e.g. the Moon and light pollution) were removed from the night sky, it would appear absolutely dark. Silhouettes of objects against the sky itself would not be visible.\n\nThe sky's brightness varies greatly over the day, and the primary cause differs as well. During daytime, when the Sun is above the horizon, the direct scattering of sunlight is the overwhelmingly dominant source of light. During twilight (the duration after sunset or before sunrise until or since, respectively, the full darkness of night), the situation is more complicated, and a further differentiation is required.\n\nTwilight (both dusk and dawn) is divided into three 6° segments that mark the Sun's position below the horizon. At civil twilight, the center of the Sun's disk appears to be between 1/4° and 6° below the horizon. At nautical twilight, the Sun's altitude is between –6° and –12°. At astronomical twilight, the Sun is between –12° and –18°. When the Sun's depth is more than 18°, the sky generally attains its maximum darkness.\n\nSources of the night sky's intrinsic brightness include airglow, indirect scattering of sunlight, scattering of starlight, and light pollution.\n\nWhen physicist Anders Ångström examined the spectrum of the aurora borealis, he discovered that even on nights when the aurora was absent, its characteristic green line was still present. It was not until the 1920s that scientists were beginning to identify and understand the emission lines in aurorae and of the sky itself, and what was causing them. The green line Angstrom observed is in fact an emission line with a wavelength of 557.7 nm, caused by the recombination of oxygen in the upper atmosphere.\n\nAirglow is the collective name of the various processes in the upper atmosphere that result in the emission of photons, with the driving force being primarily UV radiation from the Sun. Several emission lines are dominant: a green line from oxygen at 557.7 nm, a yellow doublet from sodium at 589.0 and 589.6 nm, and red lines from oxygen at 630.0 and 636.4 nm.\n\nThe sodium emissions come from a thin sodium layer approximately 10 km thick at an altitude of 90–100 km, above the mesopause and in the D-layer of the ionosphere. The red oxygen lines originate at altitudes of about 300 km, in the F-layer. The green oxygen emissions are more spatially distributed. How sodium gets to mesospheric heights is not yet well understood, but it is believed to be a combination of upward transport of sea salt and meteoritic dust.\n\nIn daytime, sodium and red oxygen emissions are dominant and roughly 1,000 times as bright as nighttime emissions because in daytime, the upper atmosphere is fully exposed to solar UV radiation. The effect is however not noticeable to the human eye, since the glare of directly scattered sunlight outshines and obscures it.\n\nIndirectly scattered sunlight comes from two directions. From the atmosphere itself, and from outer space. In the first case, the sun has just set but still illuminates the upper atmosphere directly. Because the amount of scattered sunlight is proportional to the number of scatterers (i.e. air molecules) in the line of sight, the intensity of this light decreases rapidly as the sun drops further below the horizon and illuminates less of the atmosphere.\n\nWhen the sun's altitude is < -6° 99% of the atmosphere in zenith is in the Earth's shadow and second order scattering takes over. At the horizon, however, 35% of the atmosphere along the line of sight is still directly illuminated, and continues to be until the sun reaches -12°. From -12° to -18° only the uppermost parts of the atmosphere along the horizon, directly above the spot where the sun is, is still illuminated. After that, all direct illumination ceases and astronomical darkness sets in.\n\nA second source sunlight is the zodiacal light, which is caused by reflection and scattering of sunlight on interplanetary dust. Zodiacal light varies quite a lot in intensity depending on the position of the earth, location of the observer, time of year, and composition and distribution of the reflecting dust.\n\nNot only sunlight is scattered by the molecules in the air. Starlight and the diffuse light of the milky way are also scattered by the air, and it is found that stars up to V magnitude 16 contribute to the diffuse scattered starlight.\n\nOther sources such as galaxies and nebulae don't contribute significantly.\n\nThe total brightness of all the stars was first measured by Burns in 1899, with a calculated result that the total brightness reaching earth was equivalent to that of 2,000 first-magnitude stars with subsequent measurements by others.\n\nLight pollution is an ever-increasing source of sky brightness in urbanized areas. In densely populated areas that do not have stringent light pollution control, the entire night sky is regularly 5 to 50 times brighter than it would be if all lights were switched off, and very often the influence of light pollution is far greater than natural sources (including moonlight). With urbanization and light pollution, one third of humanity, and the majority of those in developed countries, cannot see the Milky Way.\n\nWhen the sun has just set, the brightness of the sky decreases rapidly, thereby enabling us to see the airglow that is caused from such high altitudes that they are still fully sunlit until the sun drops more than about 12° below the horizon. During this time, yellow emissions from the sodium layer and red emissions from the 630 nm oxygen lines are dominant, and contribute to the purplish color sometimes seen during civil and nautical twilight.\n\nAfter the sun has also set for these altitudes at the end of nautical twilight, the intensity of light emanating from earlier mentioned lines decreases, until the oxygen-green remains as the dominant source.\n\nWhen astronomical darkness has set in, the green 557.7 nm oxygen line is dominant, and atmospheric scattering of starlight occurs.\n\nDifferential refraction causes different parts of the spectrum to dominate, producing a golden hour and a blue hour.\n\nThe following table gives the relative and absolute contributions to night sky brightness at zenith on a perfectly dark night at middle latitudes without moonlight and in the absence of any light pollution.\n\nThe total sky brightness in zenith is therefore ~220 S or 21.9 mag/arcsec² in the V-band. Note that the contributions from Airglow and Zodiacal light vary with the time of year, the solar cycle, and the observer's latitude roughly as follows:\n\nwhere \"S\" is the solar 10.7 cm flux in MJy, and various sinusoidally between 0.8 and 2.0 with the 11-year solar cycle, yielding an upper contribution of ~270 S at solar maximum.\n\nThe intensity of zodiacal light depends on the ecliptic latitude and longitude of the point in the sky being observed relative to that of the sun. At ecliptic longitudes differing from the sun's by > 90 degrees, the relation is \nwhere \"β\" is the ecliptic latitude and is smaller than 60°, when larger than 60 degrees the contribution is that given in the table. Along the ecliptic plane there are enhancements in the zodiacal light where it is much brighter near the sun and with a secondary maximum opposite the sun at 180 degrees longitude (the gegenschein).\n\nIn extreme cases natural zenith sky brightness can be as high as ~21.0 mag/arcsec², roughly twice as bright as nominal conditions.\n\n"}
{"id": "233636", "url": "https://en.wikipedia.org/wiki?curid=233636", "title": "Spherical Earth", "text": "Spherical Earth\n\nThe earliest reliably documented mention of the spherical Earth concept dates from around the 6th century BC when it appeared in ancient Greek philosophy but remained a matter of speculation until the 3rd century BC, when Hellenistic astronomy established the spherical shape of the Earth as a physical given and calculated Earth's circumference. The paradigm was gradually adopted throughout the Old World during Late Antiquity and the Middle Ages. A practical demonstration of Earth's sphericity was achieved by Ferdinand Magellan and Juan Sebastián Elcano's expedition's circumnavigation (1519–1522).\n\nThe concept of a spherical Earth displaced earlier beliefs in a flat Earth: In early Mesopotamian mythology, the world was portrayed as a flat disk floating in the ocean with a hemispherical sky-dome above, and this forms the premise for early world maps like those of Anaximander and Hecataeus of Miletus. Other speculations on the shape of Earth include a seven-layered ziggurat or cosmic mountain, alluded to in the Avesta and ancient Persian writings (see seven climes).\n\nThe realization that the figure of the Earth is more accurately described as an ellipsoid dates to the 17th century, as described by Isaac Newton in \"Principia\". In the early 19th century, the flattening of the earth ellipsoid was determined to be of the order of 1/300 (Delambre, Everest). The modern value as determined by the US DoD World Geodetic System since the 1960s is close to 1/298.25.\n\nThe Earth is massive enough that gravity maintains it as a roughly spherical shape. Its formation into a sphere was made easy by its primordial hot, liquid phase.\n\nThe Solar System formed from a dust cloud that was at least partially the remnant of one or more supernovas that created heavy elements by nucleosynthesis. Grains of matter accreted through electrostatic interaction. As they grew in mass, gravity took over in gathering yet more mass, releasing the potential energy of their collisions and in-falling as heat. The protoplanetary disk also had a greater proportion of radioactive elements than the Earth today because, over time, those elements decayed. Their decay heated the early Earth even further, and continue to contribute to Earth's internal heat budget. The early Earth was thus mostly liquid.\n\nA sphere is the only stable shape for a non-rotating, gravitationally self-attracting liquid. The outward acceleration caused by the Earth's rotation is greater at the equator than at the poles (where is it zero), so the sphere gets deformed into an ellipsoid, which represents the shape having the lowest potential energy for a rotating, fluid body. This ellipsoid is slightly fatter around the equator than a perfect sphere would be. Earth's shape is also slightly lumpy because it is composed of different materials of different densities that exert slightly different amounts of gravitational force per volume.\n\nThe liquidity of a hot, newly formed planet allows heavier elements to sink down to the middle and forces lighter elements closer to the surface, a process known as planetary differentiation. This event is known as the iron catastrophe; the most abundant heavier elements were iron and nickel, which now form the Earth's core.\n\nThough the surface rocks of the Earth have cooled enough to solidify, the outer core of the planet is still hot enough to remain liquid. Energy is still being released; volcanic and tectonic activity has pushed rocks into hills and mountains and blown them out of calderas. Meteors also create impact craters and surrounding ridges. However, if the energy release ceases from these processes, then they tend to erode away over time and return toward the lowest potential-energy curve of the ellipsoid. Weather powered by solar energy can also move water, rock, and soil to make the Earth slightly out of round.\n\nEarth undulates as the shape of its lowest potential energy changes daily due to the gravity of the Sun and Moon as they move around with respect to the Earth. This is what causes tides in the oceans' water, which can flow freely along the changing potential.\n\nThe IAU definitions of planet and dwarf planet require that a Sun-orbiting body has undergone the rounding process to reach a roughly spherical shape, an achievement known as hydrostatic equilibrium. The same spheroidal shape can be seen from smaller rocky planets like Mars to gas giants like Jupiter.\n\nAny natural Sun-orbiting body that has not reached hydrostatic equilibrium is classified by the IAU as a small Solar System body (SSB). These come in many non-spherical shapes which are lumpy masses accreted haphazardly by in-falling dust and rock; not enough mass falls in to generate the heat needed to complete the rounding. Some SSSBs are just collections of relatively small rocks that are weakly held next to each other by gravity but are not actually fused into a single big bedrock. Some larger SSSBs are nearly round but have not reached hydrostatic equilibrium. The small Solar System body 4 Vesta is large enough to have undergone at least partial planetary differentiation.\n\nStars like the Sun are also spheroidal due to gravity's effects on their plasma, which is a free-flowing fluid. Ongoing stellar fusion is a much greater source of heat for stars compared to the initial heat released during formation.\n\nThe roughly spherical shape of the Earth can be confirmed by many different types of observation from ground level, aircraft, and spacecraft. The shape causes a number of phenomena that a flat Earth would not. Some of these phenomena and observations would be possible on other shapes, such as a curved disc or torus, but no other shape would explain all of them.\n\nMany pictures have been taken of the entire Earth by satellites launched by a variety of governments and private organizations. From high orbits, where half the planet can be seen at once, it is plainly spherical. The only way to piece together all the pictures taken of the ground from lower orbits so that all the surface features line up seamlessly and without distortion is to put them on an approximately spherical surface.\n\nAstronauts in low Earth orbit can personally see the curvature of the planet, and travel all the way around several times a day.\n\nThe astronauts who travelled to the Moon have seen the entire Moon-facing half at once, and can watch the sphere rotate once a day (approximately; the Moon is also moving with respect to the Earth).\n\nPeople in high-flying aircraft or skydiving from high-altitude balloons can plainly see the curvature of the Earth. Commercial aircraft do not necessarily fly high enough to make this obvious. Trying to measure the curvature of the horizon by taking a picture is complicated by the fact that camera lenses can produce distorted images depending on the angle used. An extreme version of this effect can be seen in the fisheye lens. Scientific measurements would require a carefully calibrated lens.\n\nThe fastest way for an airplane to travel between two distant cities is a great circle route, which deviates significantly from what would be the fastest straight-line travel path on a flat Earth.\n\nPhotos of the ground taken from airplanes over a large enough area also do not fit seamlessly together on a flat surface, but do fit on a roughly spherical surface. Aerial photographs of large areas must be corrected to account for curvature.\n\nOn a completely flat Earth with no visual interference (such as trees, hills, or atmospheric haze) the ground itself would never obscure distant objects; one would be able to see all the way to the edge of the surface. A spherical surface has a horizon which is closer when viewed from a lower altitude. In theory, a person standing on the surface with eyes above the ground can see the ground up to about away, but a person at the top of the Eiffel Tower at can see the ground up to about away.\n\nThis phenomenon would seem to present a method to verify that the Earth's surface is locally convex. If the degree of curvature was determined to be the same everywhere on the Earth's surface, and that surface was determined to be large enough, it would show that the Earth is spherical.\n\nIn practice, this turns out to be an unreliable method of measurement, due to variations in atmospheric refraction. This additional effect can give the impression that the earth's surface is flat, curved more convexly than it is, or even that it is concave, by bending light travelling near the surface of the earth (as happened in various trials of the famous Bedford Level experiment).\n\nThe phenomenon of variable atmospheric bending can be empirically confirmed by noting that sometimes the refractive layers of air can cause the image of a distant object to be broken into pieces or even turned upside down. This is commonly seen at sunset, when the sun's shape is distorted, but has also been photographed happening for ships, and has caused the city of Chicago to appear normally, upside down, and broken into pieces from across Lake Michigan (from where it is normally below the horizon). Because of their longer wavelengths, radio waves are even more susceptible to atmospheric refraction and reflection, which can cause radio and television signals to be received from towers thousands of miles away which cannot be seen with visible light.\n\nWhen the atmosphere is relatively well-mixed, the visual effects generally expected of a spherical Earth can be observed. For example, ships travelling on large bodies of water (such as the ocean) disappear over the horizon progressively, such that the highest part of the ship can still be seen even when lower parts cannot, proportional to distance from the observer. The same is true of the coastline or mountain when viewed from a ship or from across a large lake or flat terrain.\n\nThe shadow of the Earth on the Moon during a lunar eclipse is always a dark circle that moves from one side of the moon to the other (partially grazing it during a partial eclipse). This could be produced by a flat disc that always faces the Moon head-on during the eclipse, but this is inconsistent with the fact that the Moon is only rarely directly overhead during an eclipse. For each eclipse, the local surface of the Earth is pointed in a somewhat different direction. The shadow of a circular disc held at an angle is an oval, not a circle as is seen during the eclipse. The idea of the Earth being a flat disc is also inconsistent with the fact that a given lunar eclipse is only visible from half of the Earth at a time.\n\nThe only shape that casts a round shadow no matter which direction it is pointed is a sphere, and the ancient Greeks deduced that this must mean the Earth is spherical.\n\nOn a perfectly spherical Earth, flat terrain or ocean, when viewed from the surface, blocks exactly half the sky - a hemisphere of 180°. Moving away from the surface of the Earth means that the ground blocks less and less of the sky. For example, when viewed from the Moon, the Earth blocks only a small portion of the sky, because it is so distant. This phenomenon of geometry means that when viewed from a high mountain, flat ground or ocean blocks less than 180° of the sky. The rate of change in the angle blocked by the sky as altitude increases is different for a disc than a sphere, and values observed show that the Earth is locally convex. (The angles blocked would also be different for a mountain close to the edge of a flat Earth compared to a mountain in the middle of a flat Earth, and this is not observed.) In theory, measurements of this type from all around the Earth would confirm that it is a complete sphere (as opposed to some other shape with convex areas) though actually taking all those measurements would be very expensive.\n\nUsing other evidence to hypothesize a spherical shape, the medieval Iranian scholar Al-Biruni used this phenomenon to calculate the Earth's circumference to within of the correct value.\n\nThe fixed stars can be demonstrated to be very far away, by diurnal parallax measurements (a technique known at least as early as Ancient Greece). Unlike the Sun, Moon, and planets, they do not change position with respect to one another (at least not perceptibly over the span of a human lifetime); the shapes of the constellations are always the same. This makes them a convenient reference background for determining the shape of the Earth. Adding distance measurements on the ground allows calculation of the Earth's size.\n\nThe fact that different stars are visible from different locations on the Earth was noticed in ancient times. Aristotle wrote that some stars are visible from Egypt which are not visible from Europe. This would not be possible if the Earth was flat.\n\nAt the North Pole it is continuously nighttime for six months of the year and the same hemisphere of stars (a 180° view) are always visible making one counterclockwise rotation every 24 hours. The star Polaris (the \"North Star\") is almost at the center of this rotation (which is directly overhead). Some of the 88 modern constellations visible are Ursa Major (including the Big Dipper), Cassiopeia, and Andromeda. The other six months of the year, it is continuously daytime and the light from the Sun mostly blots out the stars. (The location of the poles can be defined by these phenomena, which only occur there; more than 24 hours of continuous daylight can occur north of the Arctic Circle and south of the Antarctic Circle.)\n\nAt the South Pole, a completely non-overlapping set of constellations are visible during the six months of continuous nighttime, including Orion, Crux, and Centaurus. This 180° hemisphere of stars rotate clockwise once every 24 hours, around a point directly overhead (where there do not happen to be any particularly bright stars).\n\nThe fact that the stars visible from the north and south poles do not overlap must mean that the two observation spots are on opposite sides of the Earth, which is not possible if the Earth is a single-sided disc, but is possible for other shapes (like a sphere, but also any other convex shape like a donut or dumbbell).\n\nFrom any point on the equator, 360° of stars are visible over the course of the night, as the sky rotates around a line drawn from due north to due south (which could be defined as \"the directions to walk to get to the poles in the shortest amount of time\"). When facing east, the stars visible from the north pole are on the left, and the stars visible from the south pole are on the right. This means the equator must be facing at a 90° angle from the poles.\n\nThe direction any intermediate spot on the Earth is facing can also be calculated by measuring the angles of the fixed stars and determining how much of the sky is visible. For example, New York City is about 40° north of the equator. The apparent motion of the Sun blots out slightly different parts of the sky from day to day, but over the course of the entire year it sees a dome of 280° (360° - 80°). So for example, both Orion and the Big Dipper are visible during at least part of the year.\n\nMaking stellar observations from a representative set of points across the Earth, combined with knowing the shortest on-the-ground distance between any two given points makes an approximate sphere the only possible shape for the Earth.\n\nKnowing the difference in angle between two points on the Earth's surface and the surface distance between them allows a calculation of the Earth's size. Using observations at Rhodes (in Greece) and Alexandria (in Egypt) and the distance between them, the Ancient Greek philosopher Posidonius actually did use this technique to calculate the circumference of the planet to within perhaps 4% of the correct value (though modern equivalents of his units of measure are not precisely known).\n\nSince the 1500s, many people have sailed or flown completely around the world in all directions, and none have discovered an edge or impenetrable barrier. (See Circumnavigation, Arctic exploration, and History of Antarctica.)\n\nSome flat Earth theories that propose the world is a north-pole-centered disc, conceive of Antarctica as an impenetrable ice wall that encircles the planet and hides any edges. This disc model explains east-west circumnavigation as simply moving around the disc in a circle. (East-west paths form a circle in both disc and spherical geometry.) It is possible in this model to traverse the North Pole, but it is not possible to perform a circumnavigation that includes the South Pole (which it posits does not exist).\n\nExplorers, government researchers, commercial pilots, and tourists have been to Antarctica and found that it is not a large ring that encircles the entire world, but actually a roughly disc-shaped continent smaller than South America but larger than Australia, with an interior that can in fact be traversed in order to take a shorter path from e.g. the tip of South America to Australia than would be possible on a disc.\n\nThe first land crossing of the entirety of Antarctica was the Commonwealth Trans-Antarctic Expedition in 1955-58, and many exploratory airplanes have since passed over the continent in various directions.\n\nOn a flat Earth, an omnidirectional Sun (emitting light in all directions, as it does) would illuminate the entire surface at the same time, and all places would experience sunrise and sunset at the horizon at the same time (with some small variations due to mountains and valleys). With a spherical Earth, half the planet is in daylight at any given time (the hemisphere facing the Sun) and the other half is experiencing nighttime. When a given location on the spherical Earth is in sunlight, its antipode - the location exactly on the opposite side of the Earth - is always experiencing nighttime. The spherical shape of the Earth causes the Sun to rise and set at different times in different places, and different locations get different amounts of sunlight each day. \n\nIn order to explain day and night, time zones, and the seasons, some flat Earth theorists propose that the Sun does not emit light in all directions, but acts more like a spotlight, only illuminating part of the flat Earth at a time. This theory is not consistent with observation; at sunrise and sunset, a spotlight Sun would be up in the sky at least a little bit, rather than at the horizon where it is always actually observed. A spotlight Sun would also appear at different angles in the sky with respect to a flat ground than it does with respect to a curved ground. Assuming light travels in straight lines, actual measurements of the Sun's angle in the sky from locations very distant from each other are only consistent with a geometry where the Sun is very far away and is being seen from a hemispherical surface (the daylight half of the Earth). These two phenomena are related: a low-altitude spotlight Sun would spent most of the day near the horizon for most locations on Earth (which is not observed), but rise and set fairly close to the horizon. A high-altitude Sun would spend more of the day away from the horizon, but rise and set fairly far from the horizon (which is not observed).\n\nAncient timekeeping reckoned \"noon\" as the time of day when the sun is highest in the sky, with the rest of the hours in the day measured against that. During the day, the apparent solar time can be measured directly with a sundial. In ancient Egypt, the first known sundials divided the day into 12 hours, though because the length of the day changed with the season, the length of the hours also changed. Sundials that defined hours as always being the same duration appeared in the Renaissance. In Western Europe, clock towers and striking clocks were used in the Middle Ages to keep people nearby appraised of the local time, though compared to modern times this was less important in a largely agrarian society.\n\nBecause the Sun reaches its highest point at different times for different longitudes (about four minutes of time for every degree of longitude difference east or west), the local solar noon in each city is different except for those directly north or south of each other. This means that the clocks in different cities could be offset from each other by minutes or hours. As clocks became more precise and industrialization made timekeeping more important, cities switched to mean solar time, which ignores minor variations in the timing of local solar noon over the year, due to the elliptical nature of the Earth's orbit, and its tilt.\n\nThe differences in clock time between cities was not generally a problem until the advent of railroad travel in the 1800s, which both made travel between distant cities much faster than by walking or horse, and also required passengers to show up at specific times to meet their desired trains. In the United Kingdom, railroads gradually switched to Greenwich Mean Time (set from local time at the Greenwich observatory in London), followed by public clocks across the country generally, forming a single time zone. In the United States, railroads published schedules based on local time, then later based on standard time for that railroad (typically the local time at the railroad's headquarters), and then finally based on four standard time zones shared across all railroads, where neighboring zones differed by exactly one hour. At first railroad time was synchronized by portable chronometers, and then later by telegraph and radio signals.\n\nSan Francisco is at 122.41°W longitude and Richmond, Virginia is at 77.46°W longitude. They are both at about 37.6°N latitude (±.2°). The approximately 45° of longitude difference translates into about 180 minutes, or 3 hours, of time between sunsets in the two cities, for example. San Francisco is in the Pacific Time zone, and Richmond is in the Eastern Time zone, which are three hours apart, so the local clocks in each city show that the sun sets at about the same time when using the local time zone. But a phone call from Richmond to San Francisco at sunset will reveal that there are still three hours of daylight left in California.\n\nOn a flat Earth with an omnidirectional Sun, all places would experience the same amount of daylight every day, and all places would get daylight at the same time. Actual day length varies considerably, with places closer to the poles getting very long days in the summer and very short days in the winter, with northerly summer happening at the same time as southerly winter. Places north of the Arctic Circle and south of the Antarctic Circle get no sunlight for at least one day a year, and get 24-hour sunlight for at least one day a year. Both the poles experience sunlight for 6 months and darkness for 6 months, at opposite times.\n\nThe movement of daylight between the northern and southern hemispheres happens because of the axial tilt of the Earth. The imaginary line around which the Earth spins, which goes between the North Pole and South Pole, is tilted about 23° from the oval that describes its orbit around the Sun. The Earth always points in the same direction as it moves around the Sun, so for half the year (summer in the Northern Hemisphere), the North Pole is pointed slightly toward the Sun, keeping it in daylight all the time because the Sun lights up the half of the Earth that is facing it (and the North Pole is always in that half due to the tilt). For the other half of the orbit, the South Pole is tilted slightly toward the Sun, and it is winter in the Northern Hemisphere. This means that at the equator, the Sun is not directly overhead at noon, except around the autumnal equinox and vernal equinox, when one spot on the equator is pointed directly at the Sun.\n\nThe length of the day varies because as the Earth rotates some places (near the poles) pass through only a short curve near the top or bottom of the sunlight half; other places (near the equator) travel along much longer curves through the middle.\n\nThe length of twilight would be very different on a flat Earth. On a round Earth, the atmosphere above the ground is lit for a while before sunrise and after sunset are observed at ground level, because the Sun is still visible from higher altitudes. Longer twilights are observed at higher latitudes (near the poles) due to a shallower angle of the Sun's apparent movement compared to the horizon. On a flat Earth, the Sun's shadow would reach the upper atmosphere very quickly, except near the closest edge of the Earth, and would always set at the same angle to the ground (which is not what is observed). The \"spotlight Sun\" theory is also not consistent with this observation, since the air cannot be lit without the ground below it also being lit (except for shadows of mountains and other surface obstacles).\n\nOn a given day, if many different cities measure the angle of the Sun at local noon, the resulting data, when combined with the known distances between cities, shows that the Earth has 180 degrees of north-south curvature. (A full range of angles will be observed if the north and south poles are included, and the day chosen is either the autumnal or spring equinox.) This is consistent with many rounded shapes, including a sphere, and is inconsistent with a flat shape.\n\nSome claim that this experiment assumes a very distant Sun, such that the incoming rays are essentially parallel, and if a flat Earth is assumed, that the measured angles can allow one to calculate the distance to the Sun, which must be small enough that its incoming rays are not very parallel. However, if more than two relatively well-separated cities are included in the experiment, the calculation will make clear whether the Sun is distant or nearby. For example, on the equinox, the 0 degree angle from the North Pole and the 90 degree angle from the equator predict a Sun which would have to be located essentially next to the surface of a flat Earth, but the difference in angle between the equator and New York City would predict a Sun much further away if the Earth is flat. Because these results are contradictory, the surface of the Earth cannot be flat; the data \"is\" consistent with a nearly spherical Earth and a Sun which is very far away compared with the diameter of the Earth.\n\nUsing the knowledge that the Sun is very far away, the ancient Greek geographer Eratosthenes performed an experiment using the differences in the observed angle of the Sun from two different locations to calculate the circumference of the Earth. Though modern telecommunications and timekeeping were not available, he was able to make sure the measurements happened at the same time by having them taken when the Sun was highest in the sky (local noon) at both locations. Using slightly inaccurate assumptions about the locations of two cities, he came to a result within 15% of the correct value.\n\nOn level ground, the difference in the distance to the horizon between lying down and standing up is large enough to watch the Sun set twice by quickly standing up immediately after seeing it set for the first time while lying down. This also can be done with a cherry picker or a tall building with a fast elevator. On a flat Earth, one would not be able to see the Sun again (unless standing near the edge closest to the Sun) due to a much faster-moving Sun shadow. \n\nWhen the supersonic Concorde took off not long after sunset from London and flew westward to New York faster than the sunset was advancing westward on the ground, passengers observed a sunrise in the west. After landing in New York, passengers saw a second sunset in the west.\n\nBecause the speed of the Sun's shadow is slower in polar regions (due to the steeper angle), even a subsonic aircraft can overtake the sunset when flying at high latitudes. One photographer used a roughly circular route around the North Pole to take pictures of 24 sunsets in the same 24-hour period, pausing westward progress in each time zone to let the shadow of the Sun catch up. The surface of the Earth rotates at at 80° north or south, and at the equator.\n\nBecause the Earth is spherical, long-distance travel sometimes requires heading in different directions than one would head on a flat Earth.\n\nFor example, consider an airplane that travels in a straight line, takes a 90-degree right turn, travels another , takes another 90-degree right turn, and travels a third time. On a flat Earth, the aircraft would have travelled along three sides of a square, and arrive at a spot about from where it started. But because the Earth is spherical, in reality it will have travelled along three sides of a triangle, and arrive back very close to its starting point. If the starting point is the North Pole, it would have travelled due south from the North Pole to the equator, then west for a quarter of the way around the Earth, and then due north back to the North Pole.\n\nIn spherical geometry, the sum of angles inside a triangle is greater than 180° (in this example 270°, having arrived back at the north pole a 90° angle to the departure path) unlike on a flat surface, where it is always exactly 180°.\n\nA meridian of longitude is a line where local solar noon occurs at the same time each day. These lines define \"north\" and \"south\". These are perpendicular to lines of latitude that define \"east\" and \"west\", where the Sun is at the same angle at local noon on the same day. If the Sun were travelling from east to west over a flat Earth, meridian lines would always be the same distance apart - they would form a square grid when combined with lines of latitude. In reality, meridian lines get farther apart as one travels toward the equator, which is only possible on a round Earth. In places where land is plotted on a grid system, this causes discontinuities in the grid. For example, in areas of the Midwestern United States that use the Public Land Survey System, the northernmost and westernmost sections of a township deviate from what would otherwise be an exact square mile. The resulting discontinuities are sometimes reflected directly in local roads, which have kinks where the grid cannot follow completely straight lines.\n\nLow-pressure weather systems with inward winds (such as a hurricane) spin counterclockwise north of the equator, but clockwise south of the equator. This is due to the Coriolis force, and requires that (assuming they are attached to each other and rotating in the same direction) the north and southern halves of the Earth are angled in opposite directions (e.g. the north is facing toward Polaris and the south is facing away from it).\n\nThe laws of gravity, chemistry, and physics that explain the formation and rounding of the Earth are well-tested through experiment, and applied successfully to many engineering tasks.\n\nFrom these laws, we know the amount of mass the Earth contains, and that a non-spherical planet the size of the Earth would not be able to support itself against its own gravity. A flat disc the size of the Earth, for example, would likely crack, heat up, liquefy, and re-form into a roughly spherical shape. On a disc strong enough to maintain its shape, gravity would not pull downward with respect to the surface, but would pull toward the center of the disc, contrary to what is observed on level terrain (and which would create major problems with oceans flowing toward the center of the disk).\n\nIgnoring the other concerns, some flat Earth theorists explain the observed surface \"gravity\" by proposing that the flat Earth is constantly accelerating upwards. Such a theory would also leave open for explanation the tides seen in Earth's oceans, which are conventionally explained by the gravity exerted by the Sun and Moon.\n\nObservation of Foucault pendulums, popular in science museums around the world, demonstrate both that the world is spherical and that it rotates (not that the stars are rotating around it).\n\nThe mathematics of navigation by GPS assume that satellites are moving in known orbits around an approximately spherical surface. The accuracy of GPS navigation in determining latitude and longitude and the way these numbers map onto locations on the ground show that these assumptions are correct. The same is true for the operational GLONASS system run by Russia, and the in-development European Galileo, Chinese BeiDou, and Indian IRNSS.\n\nSatellites, including communications satellites used for television, telephone, and Internet connections, would not stay in orbit unless the modern theory of gravitation were correct. The details of which satellites are visible from which places on the ground at which times prove an approximately spherical shape of the Earth. (Undersea cables are also used for intercontinental communications.)\n\nRadio transmitters are mounted on tall towers because they generally rely on line-of-sight propagation. The distance to the horizon is further at higher altitude, so mounting them higher significantly increases the area they can serve. Some signals can be transmitted at much longer distances, but only if they are at frequencies where they can use groundwave propagation, tropospheric propagation, tropospheric scatter, or ionospheric propagation to reflect or refract signals around the curve of the Earth.\n\nThe design of some large structures needs to take the shape of the Earth into account. For example, the towers of the Humber Bridge, although both vertical with respect to gravity, are farther apart at the top than the bottom due to the local curvature.\n\nThe Hebrew Bible imagined a three-part world, with the heavens (\"shamayim\") above, earth (\"eres\") in the middle, and the underworld (\"sheol\") below. After the 4th century BCE this was gradually replaced by a Greek scientific cosmology of a spherical earth surrounded by multiple concentric heavens.\n\nThough the earliest written mention of a spherical Earth comes from ancient Greek sources, there is no account of how the sphericity of the Earth was discovered. A plausible explanation is that it was \"the experience of travellers that suggested such an explanation for the variation in the observable altitude of the pole and the change in the area of circumpolar stars, a change that was quite drastic between Greek settlements\" around the eastern Mediterranean Sea, particularly those between the Nile Delta and Crimea.\n\nIn \"The Histories\", written 431–425 BC, Herodotus cast doubt on a report of the Sun observed shining from the north. He stated that the phenomenon was observed during a circumnavigation of Africa undertaken by Phoenician explorers employed by Egyptian pharaoh Necho II c. 610–595 BC (, 4.42) who claimed to have had the Sun on their right when circumnavigating in a clockwise direction. To modern historians, these details confirm the truth of the Phoenicians' report and even open the possibility that the Phoenicians knew about the spherical model. However, nothing certain about their knowledge of geography and navigation has survived.\n\nEarly Greek philosophers alluded to a spherical Earth, though with some ambiguity. Pythagoras (6th century BC) was among those said to have originated the idea, but this might reflect the ancient Greek practice of ascribing every discovery to one or another of their ancient wise men. Some idea of the sphericity of the Earth seems to have been known to both Parmenides and Empedocles in the 5th century BC, and although the idea cannot reliably be ascribed to Pythagoras, it might nevertheless have been formulated in the Pythagorean school in the 5th century BC although some disagree. After the 5th century BC, no Greek writer of repute thought the world was anything but round.\n\nPlato (427–347 BC) travelled to southern Italy to study Pythagorean mathematics. When he returned to Athens and established his school, Plato also taught his students that Earth was a sphere, though he offered no justifications. \"My conviction is that the Earth is a round body in the centre of the heavens, and therefore has no need of air or of any similar force to be a support\". If man could soar high above the clouds, Earth would resemble \"one of those balls which have leather coverings in twelve pieces, and is decked with various colours, of which the colours used by painters on Earth are in a manner samples.\"\nIn Timaeus, his one work that was available throughout the Middle Ages in Latin, we read that the Creator \"made the world in the form of a globe, round as from a lathe, having its extremes in every direction equidistant from the centre, the most perfect and the most like itself of all figures\", though the word \"world\" here refers to the heavens.\n\nAristotle (384–322 BC) was Plato's prize student and \"the mind of the school\". Aristotle observed \"there are stars seen in Egypt and [...] Cyprus which are not seen in the northerly regions.\" Since this could only happen on a curved surface, he too believed Earth was a sphere \"of no great size, for otherwise the effect of so slight a change of place would not be quickly apparent.\" (\"De caelo\", 298a2–10)\n\nAristotle provided physical and observational arguments supporting the idea of a spherical Earth:\n\n\nThe concepts of symmetry, equilibrium and cyclic repetition permeated Aristotle's work. In his \"Meteorology\" he divided the world into five climatic zones: two temperate areas separated by a torrid zone near the equator, and two cold inhospitable regions, \"one near our upper or northern pole and the other near the ... southern pole,\" both impenetrable and girdled with ice (\"Meteorologica\", 362a31–35). Although no humans could survive in the frigid zones, inhabitants in the southern temperate regions could exist.\n\nAristotle's theory of natural place relied on a spherical Earth to explain why heavy things go down (toward what Aristotle believed was the center of the Universe), and things like air and fire go up. In this geocentric model, the structure of the universe was believed to be a series of perfect spheres. The Sun, Moon, planets and fixed stars were believed to move on celestial spheres around a stationary Earth.\n\nThough Aristotle's theory of physics survived in the Christian world for many centuries, the heliocentric model was eventually shown to be a more correct explanation of the Solar System than the geocentric model, and atomic theory was shown to be a more correct explanation of the nature of matter than classical elements like earth, water, air, fire, and aether.\n\nIn proposition 2 of the First Book of his treatise \"On floating bodies,\" Archimedes demonstrates that \"The surface of any fluid at rest is the surface of a sphere whose centre is the same as that of the earth,\". Subsequently, in propositions 8 and 9 of the same work, he assumes the result of proposition 2 that the Earth is a sphere and that the surface of a fluid on it is a sphere centered on the center of the Earth.\n\nEratosthenes, a Greek astronomer from Hellenistic Cyrenaica (276–194 BC), estimated Earth's circumference around 240 BC. He had heard that in Syene the Sun was directly overhead at the summer solstice whereas in Alexandria it still cast a shadow. Using the differing angles the shadows made as the basis of his trigonometric calculations he estimated a circumference of around 250,000 \"stades\". The length of a 'stade' is not precisely known, but Eratosthenes's figure only has an error of around five to fifteen percent. Eratosthenes used rough estimates and round numbers, but depending on the length of the stadion, his result is within a margin of between 2% and 20% of the actual meridional circumference, . Note that Eratosthenes could only measure the circumference of the Earth by assuming that the distance to the Sun is so great that the rays of sunlight are practically parallel.\n\nSeventeen hundred years after Eratosthenes, Christopher Columbus studied Eratosthenes's findings before sailing west for the Indies. However, ultimately he rejected Eratosthenes in favour of other maps and arguments that interpreted Earth's circumference to be a third smaller than reality. If, instead, Columbus had accepted Eratosthenes findings, then he may have never gone west, since he didn't have the supplies or funding needed for the much longer voyage.\n\nSeleucus of Seleucia (c. 190 BC), who lived in the city of Seleucia in Mesopotamia, wrote that the Earth is spherical (and actually orbits the Sun, influenced by the heliocentric theory of Aristarchus of Samos).\n\nPosidonius (c. 135 – 51 BC) put faith in Eratosthenes's method, though by observing the star Canopus, rather than the sun in establishing the Earth's circumference. In Ptolemy's \"Geographia\", his result was favoured over that of Eratosthenes. Posidonius furthermore expressed the distance of the sun in earth radii.\n\nFrom its Greek origins, the idea of a spherical earth, along with much of Greek astronomical thought, slowly spread across the globe and ultimately became the adopted view in all major astronomical traditions.\n\nIn the West, the idea came to the Romans through the lengthy process of cross-fertilization with Hellenistic civilization. Many Roman authors such as Cicero and Pliny refer in their works to the rotundity of the earth as a matter of course.\n\nIt has been suggested that seafarers probably provided the first observational evidence that the Earth was not flat, based on observations of the horizon. This argument was put forward by the geographer Strabo (c. 64 BC – 24 AD), who suggested that the spherical shape of the Earth was probably known to seafarers around the Mediterranean Sea since at least the time of Homer, citing a line from the \"Odyssey\" as indicating that the poet Homer knew of this as early as the 7th or 8th century BC. Strabo cited various phenomena observed at sea as suggesting that the Earth was spherical. He observed that elevated lights or areas of land were visible to sailors at greater distances than those less elevated, and stated that the curvature of the sea was obviously responsible for this.\n\nClaudius Ptolemy (90–168 AD) lived in Alexandria, the centre of scholarship in the 2nd century. In the \"Almagest,\" which remained the standard work of astronomy for 1,400 years, he advanced many arguments for the spherical nature of the Earth. Among them was the observation that when a ship is sailing towards mountains, observers note these seem to rise from the sea, indicating that they were hidden by the curved surface of the sea. He also gives separate arguments that the Earth is curved north-south and that it is curved east-west.\n\nHe compiled an eight-volume \"Geographia\" covering what was known about the earth. The first part of the \"Geographia\" is a discussion of the data and of the methods he used. As with the model of the Solar System in the \"Almagest\", Ptolemy put all this information into a grand scheme. He assigned coordinates to all the places and geographic features he knew, in a grid that spanned the globe (although most of this has been lost). Latitude was measured from the equator, as it is today, but Ptolemy preferred to express it as the length of the longest day rather than degrees of arc (the length of the midsummer day increases from 12h to 24h as you go from the equator to the polar circle). He put the meridian of 0 longitude at the most western land he knew, the Canary Islands.\n\n\"Geographia\" indicated the countries of \"Serica\" and \"Sinae\" (China) at the extreme right, beyond the island of \"Taprobane\" (Sri Lanka, oversized) and the \"Aurea Chersonesus\" (Southeast Asian peninsula).\n\nPtolemy also devised and provided instructions on how to create maps both of the whole inhabited world (\"oikoumenè\") and of the Roman provinces. In the second part of the \"Geographia,\" he provided the necessary topographic lists, and captions for the maps. His \"oikoumenè\" spanned 180 degrees of longitude from the Canary Islands in the Atlantic Ocean to China, and about 81 degrees of latitude from the Arctic to the East Indies and deep into Africa. Ptolemy was well aware that he knew about only a quarter of the globe.\n\nKnowledge of the spherical shape of the Earth was received in scholarship of Late Antiquity as a matter of course, in both Neoplatonism and Early Christianity. Calcidius's fourth-century Latin commentary on and translation of Plato's \"Timaeus\", which was one of the few examples of Greek scientific thought that was known in the Early Middle Ages in Western Europe, discussed Hipparchus's use of the geometrical circumstances of eclipses to compute the relative diameters of the Sun, Earth, and Moon.\n\nTheological doubt informed by the flat Earth model implied in the Hebrew Bible inspired some early Christian scholars such as Lactantius, John Chrysostom and Athanasius of Alexandria, but this remained an eccentric current. Learned Christian authors such as Basil of Caesarea, Ambrose and Augustine of Hippo were clearly aware of the sphericity of the Earth. \"Flat Earthism\" lingered longest in Syriac Christianity, which tradition laid greater importance on a literalist interpretation of the Old Testament. Authors from that tradition, such as Cosmas Indicopleustes, presented the Earth as flat as late as in the 6th century. This last remnant of the ancient model of the cosmos disappeared during the 7th century. From the 8th century and the beginning medieval period, \"no cosmographer worthy of note has called into question the sphericity of the Earth.\"\n\nGreek ethnographer Megasthenes, c. 300 BC, has been interpreted as stating that the contemporary Brahmans believed in a spherical earth as the center of the universe. With the spread of Greek culture in the east, Hellenistic astronomy filtered eastwards to ancient India where its profound influence became apparent in the early centuries AD. The Greek concept of an Earth surrounded by the spheres of the planets and that of the fixed stars, vehemently supported by astronomers like Varahamihir and Brahmagupta, strengthened the astronomical principles. Some ideas were found possible to preserve, although in altered form.\n\nThe works of the classical Indian astronomer and mathematician, Aryabhatta (476–550 AD), deal with the sphericity of the Earth and the motion of the planets. The final two parts of his Sanskrit magnum opus, the \"Aryabhatiya\", which were named the \"Kalakriya\" (\"reckoning of time\") and the \"Gol\" (\"sphere\"), state that the Earth is spherical and that its circumference is 4,967 yojanas. In modern units this is , close to the current equatorial value of .\n\nKnowledge of the sphericity of the Earth survived into the medieval corpus of knowledge by direct transmission of the texts of Greek antiquity (Aristotle), and via authors such as Isidore of Seville and Beda Venerabilis.\nIt became increasingly traceable with the rise of scholasticism and medieval learning.\nSpread of this knowledge beyond the immediate sphere of Greco-Roman scholarship was necessarily gradual, associated with the pace of Christianisation of Europe. For example, the first evidence of knowledge of the spherical shape of the Earth in Scandinavia is a 12th-century Old Icelandic translation of \"Elucidarius\".\n\nA non-exhaustive list of more than a hundred Latin and vernacular writers from Late Antiquity and the Middle Ages who were aware that the earth was spherical has been compiled by Reinhard Krüger, professor for Romance literature at the University of Stuttgart.\nAmpelius, Chalcidius, Macrobius, Martianus Capella,\nBasil of Caesarea, Ambrose of Milan, Aurelius Augustinus, Paulus Orosius, Jordanes, Cassiodorus, Boethius, Visigoth king Sisebut.\n\nIsidore of Seville, Beda Venerabilis, Theodulf of Orléans, Vergilius of Salzburg,\nIrish monk Dicuil, Rabanus Maurus, King Alfred of England, Remigius of Auxerre, Johannes Scotus Eriugena, , Gerbert d’Aurillac (Pope Sylvester II).\n\nNotker the German of Sankt-Gallen, Hermann of Reichenau, Hildegard von Bingen, Petrus Abaelardus, Honorius Augustodunensis, Gautier de Metz, Adam of Bremen, Albertus Magnus, Thomas Aquinas, Berthold of Regensburg, Guillaume de Conches, , Abu-Idrisi, Bernardus Silvestris, Petrus Comestor, Thierry de Chartres, Gautier de Châtillon, Alexander Neckam, Alain de Lille, Averroes, Snorri Sturluson, Moshe ben Maimon, Lambert of Saint-Omer, Gervasius of Tilbury, Robert Grosseteste, Johannes de Sacrobosco, Thomas de Cantimpré, Peire de Corbian, Vincent de Beauvais, Robertus Anglicus, , Ristoro d'Arezzo, Roger Bacon, Jean de Meung, Brunetto Latini, Alfonso X of Castile.\n\nMarco Polo, Dante Alighieri, Meister Eckhart, Enea Silvio Piccolomini (Pope Pius II), Perot de Garbalei (\"divisiones mundi\"), Cecco d'Ascoli, , Levi ben Gershon, Konrad of Megenberg, Nicole Oresme, Petrus Aliacensis, , Toscanelli, , Jean de Mandeville, Christine de Pizan, Geoffrey Chaucer, William Caxton, Martin Behaim, Christopher Columbus.\n\nBishop Isidore of Seville (560–636) taught in his widely read encyclopedia, \"The Etymologies,\" that the Earth was \"round\". The bishop's confusing exposition and choice of imprecise Latin terms have divided scholarly opinion on whether he meant a sphere or a disk or even whether he meant anything specific. Notable recent scholars claim that he taught a spherical earth. Isidore did not admit the possibility of people dwelling at the antipodes, considering them as legendary and noting that there was no evidence for their existence.\n\n\nThe monk Bede (c. 672–735) wrote in his influential treatise on computus, \"The Reckoning of Time\", that the Earth was round. He explained the unequal length of daylight from \"the roundness of the Earth, for not without reason is it called 'the orb of the world' on the pages of Holy Scripture and of ordinary literature. It is, in fact, set like a sphere in the middle of the whole universe.\" (De temporum ratione, 32). The large number of surviving manuscripts of \"The Reckoning of Time,\" copied to meet the Carolingian requirement that all priests should study the computus, indicates that many, if not most, priests were exposed to the idea of the sphericity of the Earth. Ælfric of Eynsham paraphrased Bede into Old English, saying, \"Now the Earth's roundness and the Sun's orbit constitute the obstacle to the day's being equally long in every land.\"\n\nBede was lucid about earth's sphericity, writing \"We call the earth a globe, not as if the shape of a sphere were expressed in the diversity of plains and mountains, but because, if all things are included in the outline, the earth's circumference will represent the figure of a perfect globe... For truly it is an orb placed in the centre of the universe; in its width it is like a circle, and not circular like a shield but rather like a ball, and it extends from its centre with perfect roundness on all sides.\"\n\nThe 7th-century Armenian scholar Anania Shirakatsi described the world as \"being like an egg with a spherical yolk (the globe) surrounded by a layer of white (the atmosphere) and covered with a hard shell (the sky).\"\n\nIslamic astronomy was developed on the basis of a spherical earth inherited from Hellenistic astronomy. The Islamic theoretical framework largely relied on the fundamental contributions of Aristotle (\"De caelo\") and Ptolemy (\"Almagest\"), both of whom worked from the premise that the earth was spherical and at the centre of the universe (geocentric model).\n\nEarly Islamic scholars recognized Earth's sphericity, leading Muslim mathematicians to develop spherical trigonometry in order to further mensuration and to calculate the distance and direction from any given point on the Earth to Mecca. This determined the \"Qibla,\" or Muslim direction of prayer.\n\nAround 830 AD, Caliph al-Ma'mun commissioned a group of Muslim astronomers and geographers to measure the distance from Tadmur (Palmyra) to Raqqa in modern Syria. They found the cities to be separated by one degree of latitude and the meridian arc distance between them to be 66 miles and thus calculated the Earth's circumference to be 24,000 miles.\n\nAnother estimate given by his astronomers was 56 Arabic miles (111.8 km) per degree, which corresponds to a circumference of 40,248 km, very close to the currently modern values of 111.3 km per degree and 40,068 km circumference, respectively.\n\nAndalusian polymath Ibn Hazm stated that the proof of the Earth's sphericity \"is that the Sun is always vertical to a particular spot on Earth\".\n\nAl-Farghānī (Latinized as Alfraganus) was a Persian astronomer of the 9th century involved in measuring the diameter of the Earth, and commissioned by Al-Ma'mun. His estimate given above for a degree (56 Arabic miles) was much more accurate than the 60 Roman miles (89.7 km) given by Ptolemy. Christopher Columbus uncritically used Alfraganus's figure as if it were in Roman miles instead of in Arabic miles, in order to prove a smaller size of the Earth than that propounded by Ptolemy.\n\n\nAbu Rayhan Biruni (973–1048) used a new method to accurately compute the Earth's circumference, by which he arrived at a value that was close to modern values for the Earth's circumference. His estimate of 6,339.6 km for the Earth radius was only 31.4 km less than the modern mean value of 6,371.0 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, Biruni developed a new method of using trigonometric calculations based on the angle between a plain and mountain top. This yielded more accurate measurements of the Earth's circumference and made it possible for a single person to measure it from a single location.\nBiruni's method was intended to avoid \"walking across hot, dusty deserts,\" and the idea came to him when he was on top of a tall mountain in India. From the top of the mountain, he sighted the angle to the horizon which, along with the mountain's height (which he calculated beforehand), allowed him to calculate the curvature of the Earth.\nHe also made use of algebra to formulate trigonometric equations and used the astrolabe to measure angles.\n\nAccording to John J. O'Connor and Edmund F. Robertson,\n\nMuslim scholars who held to the round Earth theory used it for a quintessentially Islamic purpose: to calculate the distance and direction from any given point on the Earth to Mecca. This determined the Qibla, or Muslim direction of prayer.\n\nA terrestrial globe (Kura-i-ard) was among the presents sent by the Persian Muslim astronomer Jamal-al-Din to Kublai Khan's Chinese court in 1267. It was made of wood on which \"seven parts of water are represented in green, three parts of land in white, with rivers, lakes etc.\" Ho Peng Yoke remarks that \"it did not seem to have any general appeal to the Chinese in those days\".\n\nDuring the High Middle Ages, the astronomical knowledge in Christian Europe was extended beyond what was transmitted directly from ancient authors by transmission of learning from Medieval Islamic astronomy. An early student of such learning was Gerbert d'Aurillac, the later Pope Sylvester II.\n\nSaint Hildegard (Hildegard von Bingen, 1098–1179), depicted the spherical earth several times in her work \"Liber Divinorum Operum\".\n\nJohannes de Sacrobosco (c. 1195 – c. 1256 AD) wrote a famous work on Astronomy called \"Tractatus de Sphaera,\" based on Ptolemy, which primarily considers the sphere of the sky. However, it contains clear proofs of the earth's sphericity in the first chapter.\n\nMany scholastic commentators on Aristotle's \"On the Heavens\" and Sacrobosco's \"Treatise on the Sphere\" unanimously agreed that the earth is spherical or round. Grant observes that no author who had studied at a medieval university thought that the earth was flat.\n\nThe \"Elucidarium\" of Honorius Augustodunensis (c. 1120), an important manual for the instruction of lesser clergy, which was translated into Middle English, Old French, Middle High German, Old Russian, Middle Dutch, Old Norse, Icelandic, Spanish, and several Italian dialects, explicitly refers to a spherical Earth. Likewise, the fact that Bertold von Regensburg (mid-13th century) used the spherical Earth as an illustration in a sermon shows that he could assume this knowledge among his congregation. The sermon was preached in the vernacular German, and thus was not intended for a learned audience.\n\nDante's \"Divine Comedy,\" written in Italian in the early 14th century, portrays Earth as a sphere, discussing implications such as the different stars visible in the southern hemisphere, the altered position of the sun, and the various timezones of the Earth.\n\nThe Portuguese exploration of Africa and Asia, Columbus's voyage to the Americas (1492) and, finally, Ferdinand Magellan's circumnavigation of the earth (1519–21) provided practical evidence of the global shape of the earth.\n\nThe first direct demonstration of Earth's sphericity came in the form of the first circumnavigation in history, an expedition captained by Portuguese explorer Ferdinand Magellan. The expedition was financed by the Spanish Crown. On August 10, 1519, the five ships under Magellan's command departed from Seville. They crossed the Atlantic Ocean, passed through what is now called the Strait of Magellan, crossed the Pacific, and arrived in Cebu, where Magellan was killed by Philippine natives in a battle. His second in command, the Spaniard Juan Sebastián Elcano, continued the expedition and, on September 6, 1522, arrived at Seville, completing the circumnavigation. Charles I of Spain, in recognition of his feat, gave Elcano a coat of arms with the motto \"Primus circumdedisti me\" (in Latin, \"You went around me first\").\n\nA circumnavigation alone does not prove that the earth is spherical. It could be cylindric or irregularly globular or one of many other shapes. Still, combined with trigonometric evidence of the form used by Eratosthenes 1,700 years prior, the Magellan expedition removed any reasonable doubt in educated circles in Europe. The Transglobe Expedition (1979–1982) was the first expedition to make a circumpolar circumnavigation, traveling the world \"vertically\" traversing both of the poles of rotation using only surface transport.\n\nIn the 17th century, the idea of a spherical Earth, now considerably advanced by Western astronomy, ultimately spread to Ming China, when Jesuit missionaries, who held high positions as astronomers at the imperial court, successfully challenged the Chinese belief that the Earth was flat and square.\n\nThe \"Ge zhi cao\" (格致草) treatise of Xiong Mingyu (熊明遇) published in 1648 showed a printed picture of the Earth as a spherical globe, with the text stating that \"the round Earth certainly has no square corners\". The text also pointed out that sailing ships could return to their port of origin after circumnavigating the waters of the Earth.\n\nThe influence of the map is distinctly Western, as traditional maps of Chinese cartography held the graduation of the sphere at 365.25 degrees, while the Western graduation was of 360 degrees. Also of interest to note is on one side of the world, there is seen towering Chinese pagodas, while on the opposite side (upside-down) there were European cathedrals. The adoption of European astronomy, facilitated by the failure of indigenous astronomy to make progress, was accompanied by a sinocentric reinterpretation that declared the imported ideas Chinese in origin:\n\nEuropean astronomy was so much judged worth consideration that numerous Chinese authors developed the idea that the Chinese of antiquity had anticipated most of the novelties presented by the missionaries as European discoveries, for example, the rotundity of the Earth and the \"heavenly spherical star carrier model.\" Making skillful use of philology, these authors cleverly reinterpreted the greatest technical and literary works of Chinese antiquity. From this sprang a new science wholly dedicated to the demonstration of the Chinese origin of astronomy and more generally of all European science and technology.\n\nAlthough mainstream Chinese science until the 17th century held the view that the earth was flat, square, and enveloped by the celestial sphere, this idea was criticized by the Jin-dynasty scholar Yu Xi (fl. 307–345), who suggested that the Earth could be either square or round, in accordance with the shape of the heavens. The Yuan-dynasty mathematician Li Ye (c. 1192–1279) firmly argued that the Earth was spherical, just like the shape of the heavens only smaller, since a square Earth would hinder the movement of the heavens and celestial bodies in his estimation. The 17th-century \"Ge zhi cao\" treatise also used the same terminology to describe the shape of the Earth that the Eastern-Han scholar Zhang Heng (78–139 AD) had used to describe the shape of the sun and moon (i.e. that the former was as round as a crossbow bullet, and the latter was the shape of a ball).\n\nGeodesy, also called geodetics, is the scientific discipline that deals with the measurement and representation of the Earth, its gravitational field and geodynamic phenomena (polar motion, Earth tides, and crustal motion) in three-dimensional time-varying space.\n\nGeodesy is primarily concerned with positioning and the gravity field and geometrical aspects of their temporal variations, although it can also include the study of Earth's magnetic field. Especially in the German speaking world, geodesy is divided into geomensuration (\"Erdmessung\" or \"höhere Geodäsie\"), which is concerned with measuring the Earth on a global scale, and surveying (\"Ingenieurgeodäsie\"), which is concerned with measuring parts of the surface.\n\nThe Earth's shape can be thought of in at least two ways;\n\nAs the science of geodesy measured Earth more accurately, the shape of the geoid was first found not to be a perfect sphere but to approximate an oblate spheroid, a specific type of ellipsoid. More recent measurements have measured the geoid to unprecedented accuracy, revealing mass concentrations beneath Earth's surface.\n\n\n\n"}
{"id": "3331283", "url": "https://en.wikipedia.org/wiki?curid=3331283", "title": "Suction cup", "text": "Suction cup\n\nA suction cup, also known as a sucker, is a device or object that uses the negative fluid pressure of air or water to adhere to nonporous surfaces, creating a partial vacuum.\n\nSuction cups are peripherial traits of some animals such as octopuses and squids, and have been reproduced artificially for numerous purposes.\n\nThe working face of the suction cup is made of elastic, flexible material and has a curved surface. When the center of the suction cup is pressed against a flat, non-porous surface, the volume of the space between the suction cup and the flat surface is reduced, which causes the air or water between the cup and the surface to be expelled past the rim of the circular cup. The cavity which develops between the cup and the flat surface has little to no air or water in it because most of the fluid has already been forced out of the inside of the cup, causing a lack of pressure. The pressure difference between the atmosphere on the outside of the cup and the low-pressure cavity on the inside of the cup keeps the cup adhered to the surface.\n\nWhen the user ceases to apply physical pressure to the outside of the cup, the elastic substance of which the cup is made tends to resume its original, curved shape. The length of time for which the suction effect can be maintained depends mainly on how long it takes for air or water to leak back into the cavity between the cup and the surface, equalizing the pressure with the surrounding atmosphere. This depends on the porosity and flatness of the surface and the properties of the cup's rim.\n\nThe force required to detach an ideal suction cup by pulling it directly away from the surface is given by the formula:\nwhere:\n\nThis is derived from the definition of pressure, which is:\nFor example, a suction cup of radius 2.0 cm has an area of formula_3(0.020 m) = 0.0013 square meters. Using the force formula (\"F\" = \"AP\"), the result is\n\"F\" = (0.0013 m)(100,000 Pa) = about 130 newtons.\n\nThe above formula relies on several assumptions:\n\nArtificial suction cups are believed to have first been used in the third century, B.C., and were made out of gourds. They were used to suction \"bad blood\" from internal organs to the surface. Hippocrates is believed to have invented this procedure.\n\nThe first modern suction cup patents were issued by the United States Patent and Trademark Office during the 1860s. TC Roche was awarded U.S. Patent No. 52,748 in 1866 for a \"Photographic Developer Dipping Stick\"; the patent discloses a primitive suction cup means for handling photographic plates during developing procedures. In 1868, Orwell Needham patented a more refined suction cup design, U.S. Patent No. 82,629, calling his invention an \"Atmospheric Knob\" purposed for general use as a handle and drawer opening means.\n\nSuction cups have a number of commercial and industrial applications:\n\nOn May 25, 1981, Dan Goodwin, a.k.a. SpiderDan, scaled Sears Tower, the former world's tallest building, with a pair of suction cups. He went on to scale the Renaissance Center in Dallas, the Bonaventure Hotel in Los Angeles, the World Trade Center in New York City, Parque Central Tower in Caracas, the Nippon TV station in Tokyo, and the Millennium Tower in San Francisco.\n\n"}
{"id": "49021319", "url": "https://en.wikipedia.org/wiki?curid=49021319", "title": "TALE-likes", "text": "TALE-likes\n\nTranscription Activator Like Effector Likes (TALE-likes) are a group of bacterial DNA binding proteins named for the first and still best studied group, the TALEs of \"Xanthomonas\" bacteria. TALEs are important factors in the plant diseases caused by \"Xanthomonas\" bacteria, but are known primarily for their role in biotechnology as programmable DNA binding proteins, particularly in the context of TALE nucleases. TALE-likes have additionally been found in many strains of the \"Ralstonia solanacearum\" bacterial species complex, in \"Burkholderia rhizoxinica\" strain HKI 454, and in two unknown marine bacteria. Whether or not all these proteins from a single phylogenetic grouping is as yet unclear.\n\nThe unifying feature of the TALE-likes are their tandem arrays of DNA binding repeats. These repeats are, with few exceptions, 33-35 amino acids in length, and composed of two alpha-helices on either side of a flexible loop containing the DNA base binding residues and with neighbouring repeats joined by flexible linker loops. Evidence for this common structure comes in part from solved crystal structures of TALEs and a \"Burkholderia\" TALE-like, but also from the conservation of the code that all TALE-likes use to recognise DNA-sequences.\n\nTALEs are the first identified, best-studied and largest group within the TALE-likes. TALEs are found throughout the bacterial genus \"Xanthomonas\", comprising mostly plant pathogens. Those TALEs which have been studied have all been shown to be secreted as part of the Type III secretion system into host plant cells. Once inside the host cell they translocate to the nucleus, bind specific DNA sequences within host promoters and turn on downstream genes. Every part of this process is thought to be conserved across all TALEs. The single meaningful difference between individual TALEs, based on current understanding, is the specific DNA sequence that each TALE binds. TALEs from even closely related strains differ in the composition of repeats that make up their DNA binding domain. Repeat composition determines DNA binding preference. In particular position 13 of each repeat confers the DNA base preference of each repeat. During early research it was noted that almost all the differences between repeats of a single TALE repeat array are found in positions 12 and 13 and this finding led to the hypothesis that these residues determine base preference. In fact repeat positions 12 and 13, referred to jointly as the Repeat Variable Diresidue (RVD) are commonly said to confer base specificity despite clear evidence that position 13 is the base determining residue. In addition to the repeat domain TALEs also possess a number of conserved features in the domains flanking the repeats. These include domains for type-III-secretion, nuclear localization and transcriptional activation. This allows TALEs to carry out their biological role as effector proteins secreted into host plant cells to activate expression of specific host genes.\n\nDiversity and evolution\n\nWhilst the RVD positions are commonly the only variable positions within a single TALE repeat array it should be noted that there are more differences when comparing repeat arrays of different TALEs. The diversity of TALEs across the Xanthomonas genus is considerable, but a particularly striking finding is that the evolutionary history one arrives at by comparing repeat compositions differs from that found when comparing non-repeat sequences. Repeat arrays of TALEs are thought to evolve rapidly, with a number of recombinatorial processes suggested to shape repeat array evolution. Recombination of TALE repeat arrays has been demonstrated in a forced-selection experiment. This evolutionary dynamism is though to be made possible by the very high sequence identity of TALE repeats, which is a unique feature of TALEs as opposed to other TALE-likes.\n\nT-zero\n\nAnother unique feature of TALEs is a set of four repeat structures at the N-terminal flank of the core repeat array. These structures, termed non-canonical or degenerate repeats have been shown to be vital for DNA binding, though all but one do not contact DNA bases and thus make no contribution to sequence preference. The one exception is repeat -1, which encodes a fixed T-zero preference to all TALEs. This means that the target sequences of TALEs are always preceded by a thymine base. This is thought to be common to all TALEs, with the possible exception of TalC from \"Xanthomonas oryzae pv. oryzae\" strain AXO1947.\n\nDiscovery and molecular properties\n\nIt was noted in the 2002 publication of the genome of reference strain \"Ralstonia solanacearum\" GMI1000 that its genome encodes a protein similar to \"Xanthomonas\" TALEs. Based on similar domain structure and repeat sequences it was presumed that this gene and homologs in other \"Ralstonia\" strains would encode proteins with the same molecular properties as TALEs, including sequence-specific DNA binding. In 2013 this was confirmed by two studies. These genes and the proteins they encode are referred to as RipTALs (Ralstonia injected protein TALE-like) in line with the standard nomenclature of Ralstonia effectors. Whilst the DNA binding code of the core repeats is conserved with TALEs, RipTALs do not share the T-zero preference, instead they have a strict G-zero requirement. In addition repeats within a single RipTAL repeat array have multiple sequence differences beyond the RVD positions, unlike the near-identical repeats of TALEs.\n\nBiological role\n\nSeveral lines of evidence support the idea that RipTALs function as effector proteins, promoting bacterial growth or disease by manipulating the expression of plant genes. They are secreted into plant cells by the Type III secretion system, which is the main delivery system for effector proteins. They are able to function as sequence-specific transcription factors in plant cells. In addition a strain lacking its RipTAL was shown to grow slower inside eggplant leaf tissue than the wild type. Furthermore, a study based on DNA polymorphisms in \"ripTAL\" repeat domain sequences and host plants found a statistically significant connection between host plant and repeat domain variants. This is expected if the RipTALs of different strains are adapted to target genes in specific host plants. Despite this to date no target genes have been identified for any RipTAL.\n\nDiscovery\n\nThe publication of the genome of bacterial strain \"Bukrholderia rhizoxinica\" HKI 454, in 2011 led to the discovery of a set of TALE-like genes that differed considerably in nature from the TALEs and RipTALS. The proteins encoded by these genes were studied for their DNA binding properties by two groups independently and named the Bats (Burkholderia TALE-likes ) or BurrH. This research showed that the repeat units of the \"Burkholderia\" TALE-likes bind DNA with the same code as TALEs, governed by position 13 of each repeat. There are, however, a number of differences.\n\nBiological role\n\n\"Burkholderia\" TALE-likes are composed almost entirely of repeats, lacking the large non-repetitive domains found flanking the repeats in TALEs and RpTALs. Those domains are key to the functions of TALEs and RipTALs allowing them to infiltrate the plant nucleus and turn on gene expression. It is therefore currently unclear what the biological roles of \"Burkholderia\" TALE-likes are. What is clear is that they are not effector proteins secreted into plant cells to act as transcription factors, the biological role of TALEs and RipTALs. It is not unexpected that they may differ in biological roles from TALEs and RipTALs since the life style of the bacterium they derive from is very unlike that of TALE and RipTAL bearing bacteria. \"B. rhizoxinica\" is an endosymbiont, living inside a fungus, \"Rhizopus microsporus\", a plant pathogen. The same fungus is also an opportunistic human pathogen in immuno-compromised patients, but whereas \"B. rhizoxinica\" is necessary for pathogenicity on plant hosts it is irrelevant to human infection. It is unclear whether the \"Burkholderia\" TALE-likes are ever secreted either into the fungus, let alone into host plants.\nUses in Biotechnology\n\nAs noted in the publications on \"Burkholderia\" TALE-likes there may be some advantages to using these proteins as a scaffold for programmable DNA-binding proteins to function as transcription factors or designer-nucleases, compared to TALEs. These advantages are a shorter repeat size, more compact domain structure (no large non-repeat domains), greater repeat sequence diversity enabling the use of PCR on the genes encoding them and making them less vulnerable to recombinatorial repeat loss. In addition Burkholderia TALE-likes have no T-zero requirement relaxing the constraints on DNA target selection. However, to uses of Burkholderia TALE-likes as programmable DNA binding proteins have been published, outside of the original characterization publications.\n\nDiscovery\n\nIn 2007 the results of a sweep of the world's oceans by the J. Craig Venter Institute were made publicly available. The paper in 2014 on \"Burkholderia\" TALE-likes was also the first to report that two entries from that database resembled TALE-likes, based on sequence similarity. These were further characterized and assessed for their DNA-binding potential in 2015. The repeat units encoded by these sequences were found to mediate DNA binding with base preference matching the TALE code, and judged likely to form structures nearly identical to Bat1 repeats based on molecular dynamics simulations. The proteins encoded by these DNA sequences were therefore designated Marine Organism TALE-likes (MOrTLs) 1 and 2.\n\nEvolutionary relationship to other TALE-likes\n\nWhilst repeats of MOrTL1 and 2 both conform structurally and functionally to the TALE-like norm, they differ considerably at the sequence level both from all other TALE-likes and from one another. It is not known whether they are truly homologous to the other TALE-likes, and thus constitute together with the TALEs, RipTALs and Bats a true protein-family. Alternatively they may have evolved independently. It is particularly difficult to judge the relationship to the other TALE-likes because almost nothing is known of the organisms that MOrTL1 and MOrTL2 come from. It is known only that they were found in two separate sea-water samples from the Gulf of Mexico and are likely to be bacteria based on size-exclusion before DNA sequencing.\n"}
{"id": "712222", "url": "https://en.wikipedia.org/wiki?curid=712222", "title": "Transit of Earth from Mars", "text": "Transit of Earth from Mars\n\nA transit of Earth across the Sun as seen from Mars takes place when the planet Earth passes directly between the Sun and Mars, obscuring a small part of the Sun's disc for an observer on Mars. During a transit, Earth would be visible from Mars as a small black disc moving across the face of the Sun. They occur every 26, 79 and 100 years, and every 1,000 years or so there is an extra 53rd-year transit.\n\nTransits of Earth from Mars usually occur in pairs, with one following the other after 79 years; rarely, there are three in the series. The transits also follow a 284-year cycle, occurring at intervals of 100.5, 79, 25.5, and 79 years; a transit falling on a particular date is usually followed by another transit 284 years later. Transits occurring when Mars is at its ascending node are in May, those at descending node happen in November. This cycle corresponds fairly closely to 151 Mars orbits, 284 Earth orbits, and 133 synodic periods, and is analogous to the cycle of transits of Venus from Earth, which follow a cycle of 243 years (121.5, 8, 105.5, 8). There are currently four such active series, containing from 8 to 25 transits. A new one is set to begin in 2394. The last series ending was in 1211.\n\nNo one has ever seen a transit of Earth from Mars, but the next transit will take place on November 10, 2084. The last such transit took place on May 11, 1984.\n\nDuring the event, the Moon could almost always also be seen in transit, although due to the distance between Earth and Moon, sometimes one body completes the transit before the other begins (this last occurred in the 1800 transit, and will happen again in 2394).\n\nA transit of Earth from Mars corresponds to Mars being perfectly uniformly illuminated at opposition from Earth, its phase being 180.0° without any defect of illumination. During the 1879 event, this permitted Charles Augustus Young to attempt a careful measurement of the oblateness (polar compression) of Mars. He obtained the value 1/219, or 0.0046. This is close to the modern value of 1/154 (many sources will cite somewhat different values, such as 1/193, because even a difference of only a couple of kilometers in the values of Mars' polar and equatorial radii gives a considerably different result).\n\nMuch more recently, better measurements of the oblateness of Mars have been made by using radar from the Earth. Also, better measurements have been made by using artificial satellites that have been put into orbit around Mars, including \"Mariner 9\", \"Viking 1\", \"Viking 2\", and Soviet orbiters, and the more recent orbiters that have been sent from the Earth to Mars.\n\nA science fiction short story published in 1971 by Arthur C. Clarke, called \"Transit of Earth\", depicts a doomed astronaut on Mars observing the transit in 1984. This short story was first published in the January 1971 issue of \"Playboy\" magazine.\n\nSometimes Earth only grazes the Sun during a transit. In this case it is possible that in some areas of Mars a full transit can be seen while in other regions there is only a partial transit (no second or third contact). The last transit of this type was on 30 April 1211, and the next such transit will occur on 27 November 4356. It is also possible that a transit of Earth can be seen in some parts of Mars as a partial transit, while in others Earth misses the Sun. Such a transit last occurred on 26 October 664, and the next transit of this type will occur on 14 December 5934.\n\nThe simultaneous occurrence of a transit of Venus and a transit of Earth is extremely rare, and will next occur in the year 571,471.\n\n\n\n"}
{"id": "44055899", "url": "https://en.wikipedia.org/wiki?curid=44055899", "title": "Triploid block", "text": "Triploid block\n\nTriploid block is a phenomenon describing the formation of nonviable progeny after hybridization of flowering plants that differ in ploidy. The barrier is established in the endosperm, a nutritive tissue supporting embryo growth. This phenomenon usually happens when autopolyploidy occurs in diploid plants. Triploid blocks lead to reproductive isolation. The triploid block effects have been explained as possibly due to genomic imprinting in the endosperm.\n"}
{"id": "308158", "url": "https://en.wikipedia.org/wiki?curid=308158", "title": "Vacuum energy", "text": "Vacuum energy\n\nVacuum energy is an underlying background energy that exists in space throughout the entire Universe. This behavior is codified in Heisenberg's energy–time uncertainty principle. Still, the exact effect of such fleeting bits of energy is difficult to quantify. The vacuum energy is a special case of zero-point energy that relates to the quantum vacuum.\nThe effects of vacuum energy can be experimentally observed in various phenomena such as spontaneous emission, the Casimir effect and the Lamb shift, and are thought to influence the behavior of the Universe on cosmological scales. Using the upper limit of the cosmological constant, the vacuum energy of free space has been estimated to be 10 joules (10 ergs) per cubic meter. However, in both quantum electrodynamics (QED) and stochastic electrodynamics (SED), consistency with the principle of Lorentz covariance and with the magnitude of the Planck constant suggest a much larger value of 10 joules per cubic meter. This huge discrepancy is known as the cosmological constant problem.\n\nQuantum field theory states that all fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. A field in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field is like the displacement of a ball from its rest position. The theory requires \"vibrations\" in, or more accurately changes in the strength of, such a field to propagate as per the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball–spring combination be quantized, that is, that the strength of the field be quantized at each point in space. Canonically, if the field at each point in space is a simple harmonic oscillator, its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. Thus, according to the theory, even the vacuum has a vastly complex structure and all calculations of quantum field theory must be made in relation to this model of the vacuum.\n\nThe theory considers vacuum to implicitly have the same properties as a particle, such as spin or polarization in the case of light, energy, and so on. According to the theory, most of these properties cancel out on average leaving the vacuum empty in the literal sense of the word. One important exception, however, is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator requires the lowest possible energy, or zero-point energy of such an oscillator to be:\n\nformula_1\n\nSumming over all possible oscillators at all points in space gives an infinite quantity. To remove this infinity, one may argue that only differences in energy are physically measurable, much as the concept of potential energy has been treated in classical mechanics for centuries. This argument is the underpinning of the theory of renormalization. In all practical calculations, this is how the infinity is handled.\n\nVacuum energy can also be thought of in terms of virtual particles (also known as vacuum fluctuations) which are created and destroyed out of the vacuum. These particles are always created out of the vacuum in particle–antiparticle pairs, which in most cases shortly annihilate each other and disappear. However, these particles and antiparticles may interact with others before disappearing, a process which can be mapped using Feynman diagrams. Note that this method of computing vacuum energy is mathematically equivalent to having a quantum harmonic oscillator at each point and, therefore, suffers the same renormalization problems.\n\nAdditional contributions to the vacuum energy come from spontaneous symmetry breaking in quantum field theory.\n\nVacuum energy has a number of consequences. In 1948, Dutch physicists Hendrik B. G. Casimir and Dirk Polder predicted the existence of a tiny attractive force between closely placed metal plates due to resonances in the vacuum energy in the space between them. This is now known as the Casimir effect and has since been extensively experimentally verified. It is therefore believed that the vacuum energy is \"real\" in the same sense that more familiar conceptual objects such as electrons, magnetic fields, etc., are real. However, alternative explanations for the Casimir effect have since been proposed.\n\nOther predictions are harder to verify. Vacuum fluctuations are always created as particle–antiparticle pairs. The creation of these virtual particles near the event horizon of a black hole has been hypothesized by physicist Stephen Hawking to be a mechanism for the eventual \"evaporation\" of black holes. If one of the pair is pulled into the black hole before this, then the other particle becomes \"real\" and energy/mass is essentially radiated into space from the black hole. This loss is cumulative and could result in the black hole's disappearance over time. The time required is dependent on the mass of the black hole (the equations indicate that the smaller the black hole, the more rapidly it evaporates) but could be on the order of 10 years for large solar-mass black holes.\n\nThe vacuum energy also has important consequences for physical cosmology. General relativity predicts that energy is equivalent to mass, and therefore, if the vacuum energy is \"really there\", it should exert a gravitational force. Essentially, a non-zero vacuum energy is expected to contribute to the cosmological constant, which affects the expansion of the universe. In the special case of vacuum energy, general relativity stipulates that the gravitational field is proportional to (where \"ρ\" is the mass–energy density, and \"p\" is the pressure). Quantum theory of the vacuum further stipulates that the pressure of the zero-state vacuum energy is always negative and equal in magnitude to \"ρ\". Thus, the total is , a negative value. If indeed the vacuum ground state has non-zero energy, the calculation implies a repulsive gravitational field, giving rise to acceleration of the expansion of the universe. However, the vacuum energy is mathematically infinite without renormalization, which is based on the assumption that we can only measure energy in a relative sense, which is not true if we can observe it indirectly via the cosmological constant.\n\nThe existence of vacuum energy is also sometimes used as theoretical justification for the possibility of free-energy machines. It has been argued that due to the broken symmetry (in QED), free energy does not violate conservation of energy, since the laws of thermodynamics only apply to equilibrium systems. However, consensus amongst physicists is that this is unknown as the nature of vacuum energy remains an unsolved problem. In particular, the second law of thermodynamics is unaffected by the existence of vacuum energy. However, in Stochastic Electrodynamics, the energy density is taken to be a classical random noise wave field which consists of real electromagnetic noise waves propagating isotropically in all directions. The energy in such a wave field would seem to be accessible, e.g., with nothing more complicated than a directional coupler. The most obvious difficulty appears to be the spectral distribution of the energy, which compatibility with Lorentz invariance requires to take the form \"Kf\", where \"K\" is a constant and \"f\" denotes frequency. It follows that the energy and momentum flux in this wave field only becomes significant at extremely short wavelengths where directional coupler technology is currently lacking.\n\nIn 1934, Georges Lemaître used an unusual perfect-fluid equation of state to interpret the cosmological constant as due to vacuum energy. In 1948, the Casimir effect provided an experimental method for a verification of the existence of vacuum energy; in 1955, however, Evgeny Lifshitz offered a different origin for the Casimir effect. In 1957, Lee and Yang proved the concepts of broken symmetry and parity violation, for which they won the Nobel prize. In 1973, Edward Tryon proposed the zero-energy universe hypothesis: that the Universe may be a large-scale quantum-mechanical vacuum fluctuation where positive mass–energy is balanced by negative gravitational potential energy. During the 1980s, there were many attempts to relate the fields that generate the vacuum energy to specific fields that were predicted by attempts at a Grand unification theory and to use observations of the Universe to confirm one or another version. However, the exact nature of the particles (or fields) that generate vacuum energy, with a density such as that required by inflation theory, remains a mystery.\n\n\n"}
{"id": "26830333", "url": "https://en.wikipedia.org/wiki?curid=26830333", "title": "Water-energy nexus", "text": "Water-energy nexus\n\nThere is no formal definition for the water-energy nexus - the concept refers to the relationship between the water used for energy production, including both electricity and sources of fuel such as oil and natural gas, and the energy consumed to extract, purify, deliver, heat/cool, treat and dispose of water (and wastewater) sometimes referred to as the energy intensity (EI). The relationship is not truly a closed loop as the water used for energy production need not be the same water that is processed using that energy, but all forms of energy production require some input of water making the relationship inextricable.\n\nAmong the first studies to evaluate the water and energy relationship was a life-cycle analysis conducted by Peter Gleick in 1994 that highlighted the interdependence and initiated the joint study of water and energy. In 2014 the US Department of Energy (DOE) released their report on the water-energy nexus citing the need for joint water-energy policies and better understanding of the nexus and its susceptibility to climate change as a matter of national security. The hybrid Sankey diagram in the DOE's 2014 water-energy nexus report summarizes water and energy flows in the US by sector, demonstrating interdependence as well as singling out thermoelectric power as the single largest user of water, used mainly for cooling. \n\nAll types of energy generation consume water either to process the raw materials used in the facility, constructing and maintaining the plant, or to just generate the electricity itself. Renewable power sources such as photovoltaic solar and wind power, which require little water to produce energy, require water in processing the raw materials to build. Water can either be \"used\" or \"consumed,\" and can be categorised as fresh, ground, surface, blue, grey or green among others. Water is considered used if it does not reduce the supply of water to downstream users, i.e. water that is taken and returned to the same source (instream use), such as in thermoelectric plants that use water for cooling and are by far the largest users of water. While used water is returned to the system for downstream uses, it has usually been degraded in some way, mainly due to thermal or chemical pollution, and the natural flow has been altered which does not factor into an assessment if only the quantity of water is considered. Water is consumed when it is removed completely from the system, such as by evaporation or consumption by crops or humans. When assessing water use all these factors must be considered as well as spatiotemporal considerations making precise determination of water use very difficult.\n\nSpang et al. (2014) conducted a study looking at the water consumption for energy production (WCEP) internationally that both showed the variation in energy types produced across countries as well as the vast differences in efficiency of energy production per unit of water use (figure 1). Operations of water distribution systems and power distribution systems under emergency conditions of limited power and water availability is an important consideration for improving the overall resilience of the water - energy nexus. Khatavkar and Mays (2017) present a methodology for control of water distribution and power distribution systems under emergency conditions of drought and limited power availability to ascertain at least minimal supply of cooling water to the power plants. Khatavkar and Mays (2017 b) applied an optimization model for water - energy nexus system for a hypothetical regional level system which showed an improved resilience for several contingency scenarios.\n\nIn 2001 operating water systems in the US consumed approximately 3% of the total annual electricity (~75 TWh). The California's State Water Project (SWP) and Central Valley Project (CVP) are together the largest water system in the world with the highest water lift, over 2000 ft. across the Tehachapi mountains, delivering water from the wetter and relatively rural north of the state, to the agriculturally intensive central valley, and finally to the arid and heavily populated south. Consequently, the SWP and CVP are the single largest consumers of electricity in California consuming approximately 5 TWh of electricity each per year. In 2001, 19% of the state’s total electricity use (~48 TWh/year) was used in processing water including end uses, with the urban sector accounting for 65% of this. In addition to electricity, 30% of California’s natural gas consumption was due to water-related processes, mainly residential water heating, and 88 million gallons of diesel was consumed by groundwater pumps for agriculture. The residential sector alone accounted for 48% of the total combined electricity and natural gas consumed for water-related processes in the state.\n\nAccording to the California Public Utilities Commission (CPUC) Energy Division’s Embedded Energy in Water Studies report:\"“'Energy Intensity' refers to the average amount of energy needed to transport or treat water or wastewater on a per unit basis\"\"Energy Intensity is sometimes used synonymously with embedded or embodied energy. In 2005, water deliveries to Southern California were assessed to have an average EI of 12.7 MWh/MG, nearly two-thirds of which was due to transportation. Following the findings that a fifth of California’s electricity is consumed in water-related processes including end-use, the CPUC responded by authorising a statewide study into the relationship between energy and water that was conducted by the California Institute for Energy and Environment (CIEE), and developed programs to save energy through water conservation.\n\nHydroelectricity is a special case of water used for energy production mainly because hydroelectric power generation is regarded as being clean and renewable, and dams (the main source of hydroelectric production) serve multiple purposes besides energy generation, including flood prevention, storage, control and recreation which make justifiable allocation analyses difficult. Furthermore, the impacts of hydroelectric power generation can be hard to quantify both in terms of evaporative consumptive losses and altered quality of water, since damming results in flows that are much colder than for flowing streams. In some cases the moderation of flows can be seen as a rivalry of water use in time may also need to accounted for in impact analysis.\n\n\n"}
{"id": "427992", "url": "https://en.wikipedia.org/wiki?curid=427992", "title": "Water hammer", "text": "Water hammer\n\nWater hammer (or, more generally, fluid hammer, also called hydraulic shock) is a pressure surge or wave caused when a fluid, usually a liquid but sometimes also a gas, in motion is forced to stop or change direction suddenly, a momentum change. A water hammer commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.\n\nThis pressure wave can cause major problems, from noise and vibration to pipe collapse. It is possible to reduce the effects of the water hammer pulses with accumulators, expansion tanks, surge tanks, blowoff valves, and other features.\n\nRough calculations can be made either using the Zhukovsky (Joukowsky) equation, or more accurate ones using the method of characteristics.\n\nIn the 1st century B.C., Marcus Vitruvius Pollio described the effect of water hammer in lead pipes and stone tubes of the Roman public water supply. Water hammer was exploited before there was even a word for it; in 1772, Englishman John Whitehurst built a hydraulic ram for a home in Cheshire, England. In 1796, French inventor Joseph Michel Montgolfier (1740–1810) built a hydraulic ram for his paper mill in Voiron. In French and Italian, the terms for \"water hammer\" come from the hydraulic ram: \"coup de bélier\" (French) and \"colpo d'ariete\" (Italian) both mean \"blow of the ram\". As the 19th century witnessed the installation of municipal water supplies, water hammer became a concern to civil engineers. Water hammer also interested physiologists who were studying the circulatory system.\n\nAlthough it was prefigured in work by Thomas Young, the theory of water hammer is generally considered to have begun in 1883 with the work of German physiologist Johannes von Kries (1853–1928), who was investigating the pulse in blood vessels. However, his findings went unnoticed by civil engineers. Kries's findings were subsequently derived independently in 1898 by the Russian fluid dynamicist Nikolay Yegorovich Zhukovsky (1847–1921), in 1898 by the American civil engineer Joseph Palmer Frizell (1832–1910), and in 1902 by the Italian engineer Lorenzo Allievi (1856–1941).\n\nWhen a pipe is suddenly closed at the outlet (downstream), the mass of water before the closure is still moving, thereby building up high pressure and a resulting shock wave. In domestic plumbing this is experienced as a loud banging resembling a hammering noise. Water hammer can cause pipelines to break if the pressure is high enough. Air traps or stand pipes (open at the top) are sometimes added as to water systems to absorb the potentially damaging forces caused by the moving water.\n\nIn hydroelectric generating stations, the water traveling along the tunnel or pipeline may be prevented from entering a turbine by closing a valve. For example, if there is 14 km of tunnel of 7.7 m diameter full of water travelling at 3.75 m/s, that represents approximately 8000 megajoules of kinetic energy that must be arrested. This arresting is frequently achieved by a surge shaft open at the top, into which the water flows. As the water rises up the shaft its kinetic energy is converted into potential energy, which causes the water in the tunnel to decelerate. At some HEP stations, such as the Saxon Falls Hydro Power Plant In Michigan, what looks like a water tower is actually one of these devices, known in these cases as a surge drum.\n\nIn the home, a water hammer may occur when a dishwasher, washing machine or toilet shuts off water flow. The result may be heard as a loud bang, repetitive banging (as the shock wave travels back and forth in the plumbing system), or as some shuddering.\n\nOn the other hand, when an upstream valve in a pipe closes, water downstream of the valve attempts to continue flowing creating a vacuum that may cause the pipe to collapse or implode. This problem can be particularly acute if the pipe is on a downhill slope. To prevent this, air and vacuum relief valves or air vents are installed just downstream of the valve to allow air to enter the line to prevent this vacuum from occurring.\n\nOther causes of water hammer are pump failure and check valve slam (due to sudden deceleration, a check valve may slam shut rapidly, depending on the dynamic characteristic of the check valve and the mass of the water between a check valve and tank). To alleviate this situation, it is recommended to install non-slam check valves as they do not rely on gravity or fluid flow for their closure. For vertical pipes, other suggestions include installing new piping that can be designed to include air chambers to alleviate the possible shockwave of water due to excess water flow.\n\nSteam distribution systems may also be vulnerable to a situation similar to water hammer, known as \"steam hammer\". In a steam system, a water hammer most often occurs when some of the steam condenses into water in a horizontal section of the piping. Steam picks up the water, forming a \"slug\", and hurls this at high velocity into a pipe fitting, creating a loud hammering noise and greatly stressing the pipe. This condition is usually caused by a poor condensate drainage strategy.\n\nWhere air filled traps are used, these eventually become depleted of their trapped air over a long period through absorption into the water. This can be cured by shutting off the supply, opening taps at the highest and lowest locations to drain the system (thereby restoring air to the traps), and then closing the taps and re-opening the supply.\n\nOn turbocharged internal combustion engines, a fluid hammer can take place when the throttle is closed while the turbocharger is forcing air into the engine. A pressure relief valve placed before the throttle prevents the air from surging against the throttle body by diverting it elsewhere, thus protecting the turbocharger from pressure damage. This valve can either recirculate the air into the turbocharger's intake (recirculation valve), or it can blow the air into the atmosphere and produce the distinctive hiss-flutter of an aftermarket turbocharger (blowoff valve).\n\nIf a stream of high pressure water impinges on a surface, water hammer can quickly erode and destroy it. In the 2009 Sayano–Shushenskaya hydroelectric power station accident, the lid to a 640 MW turbine was ejected upwards, hitting the ceiling above. During the accident, the rotor was seen flying through the air, still spinning, about 3 meters above the floor. Unrestrained, per second of water began to spray all over the generator hall. The geyser caused the structural failure of steel ceiling joists, precipitating a roof collapse around the failed turbine.\n\nWhen an explosion happens in an enclosed space, water hammer can cause the walls of the container to deform. However, it can also impart momentum to the enclosure if it is free to move. An underwater explosion in the SL-1 nuclear reactor vessel caused the water to accelerate upwards through of air before it struck the vessel head at with a pressure of . This pressure wave caused the steel vessel to jump 9 feet 1 inch (2.77 m) into the air before it dropped into its prior location. It is imperative to perform ongoing preventative maintenance to avoid water hammer as the results of these powerful explosions have resulted in fatalities.\n\nWater hammer has caused accidents and fatalities, but usually damage is limited to breakage of pipes or appendages. An engineer should always assess the risk of a pipeline burst. Pipelines transporting hazardous liquids or gases warrant special care in design, construction, and operation. Hydroelectric power plants especially must be carefully designed and maintained because the water hammer can cause water pipes to fail catastrophically.\n\nThe following characteristics may reduce or eliminate water hammer:\n\nOne of the first to successfully investigate the water hammer problem was the Italian engineer Lorenzo Allievi.\n\nWater hammer can be analyzed by two different approaches—\"rigid column theory\", which ignores compressibility of the fluid and elasticity of the walls of the pipe, or by a full analysis that includes elasticity. When the time it takes a valve to close is long compared to the propagation time for a pressure wave to travel the length of the pipe, then rigid column theory is appropriate; otherwise considering elasticity may be necessary.\nBelow are two approximations for the peak pressure, one that considers elasticity, but assumes the valve closes instantaneously, and a second that neglects elasticity but includes a finite time for the valve to close.\n\nThe pressure profile of the water hammer pulse can be calculated from the Joukowsky equation\n\nSo for a valve closing instantaneously, the maximum magnitude of the water hammer pulse is:\n\nwhere Δ\"P\" is the magnitude of the pressure wave (Pa), \"ρ\" is the density of the fluid (kg m), \"a\" is the speed of sound in the fluid (ms), and Δ\"v\" is the change in the fluid's velocity (ms). The pulse comes about due to Newton's laws of motion and the continuity equation applied to the deceleration of a fluid element.\n\nAs the speed of sound in a fluid is formula_3, the peak pressure depends on the fluid compressibility if the valve is closed abruptly.\n\nwhere\n\nWhen the valve is closed slowly compared to the transit time for a pressure wave to travel the length of the pipe, the elasticity can be neglected, and the phenomenon can be described in terms of inertance or rigid column theory:\n\nAssuming constant deceleration of the water column (\"dv\"/\"dt\" = \"v\"/\"t\"), gives:\n\nwhere:\n\nThe above formula becomes, for water and with imperial unit: P = 0.0135 V L/t.\nFor practical application, a safety factor of about 5 is recommended:\n\nwhere \"P\" is the inlet pressure in psi, \"V\" is the flow velocity in ft/sec, \"t\" is the valve closing time in seconds and \"L\" is the upstream pipe length in feet.\n\nWhen a valve with a volumetric flow rate Q is closed, an excess pressure ΔP is created upstream of the valve, whose value is given by the Joukowsky equation:\n\nIn this expression:\nThe hydraulic impedance \"Z\" of the pipeline determines the magnitude of the water hammer pulse. It is itself defined by:\n\nwith:\n\nThe latter follows from a series of hydraulic concepts: \n\nThus, the equivalent elasticity is the sum of the original elasticities:\n\nAs a result, we see that we can reduce the water hammer by:\n\nThe water hammer effect can be simulated by solving the following partial differential equations.\n\nwhere \"V\" is the fluid velocity inside pipe, \"formula_15\" is the fluid density and \"B\" is the equivalent bulk modulus, \"f\" is the Darcy-Weisbach friction factor.\n\nColumn separation is a phenomenon that can occur during a water-hammer event. If the pressure in a pipeline drops below the vapor pressure of the liquid, cavitation will occur (some of the liquid vaporizes, forming a bubble in the pipeline, keeping the pressure close to the vapor pressure). This is most likely to occur at specific locations such as closed ends, high points or knees (changes in pipe slope). When subcooled liquid flows into the space previously occupied by vapor the area of contact between the vapor and the liquid increases. This causes the vapor to condense into the liquid reducing the pressure in the vapor space. The liquid on either side of the vapor space is then accelerated into this space by the pressure difference. The collision of the two columns of liquid (or of one liquid column if at a closed end) causes a large and nearly instantaneous rise in pressure. This pressure rise can damage hydraulic machinery, individual pipes and supporting structures. Many repetitions of cavity formation and collapse may occur in a single water-hammer event.\n\nMost water hammer software packages use the method of characteristics to solve the differential equations involved. This method works well if the wave speed does not vary in time due to either air or gas entrainment in a pipeline. The Wave Method (WM) is also used in various software packages. WM lets operators analyze large networks efficiently. Many commercial and non commercial packages are available.\n\nSoftware packages vary in complexity, dependent on the processes modeled. The more sophisticated packages may have any of the following features:\n\n\n\n"}
{"id": "11677633", "url": "https://en.wikipedia.org/wiki?curid=11677633", "title": "Withy", "text": "Withy\n\nA withy or withe is a strong flexible willow stem, typically used in thatching and for gardening.\n\nSeveral species and hybrid cultivars of willows (often known as osiers) are grown for withy production; typical species include \"Salix acutifolia\", \"Salix daphnoides\", \"Salix × mollissima\", \"Salix purpurea\", \"Salix triandra\", and \"Salix viminalis\".\n\nThe term is also sometimes used to describe any type of flexible rod used in rural crafts such as hazel or ash.\n\nWithies traditionally serve to mark minor tidal channels in UK harbours and estuaries. In many places they remain in use and are often marked on navigation charts. At high tide the tops of a line of withies stuck in the mud on one or both sides of a channel will show above water to indicate where the deeper water lies. Note the images of international navigation-chart symbols for withies (port and starboard).\n\nPlaces such as Wythenshawe and Withy Grove (both in Manchester) take their names from the willow woods and groves that grew there in earlier times. The Somerset Levels remain the only area in the UK growing basket willow commercially.\n\n\n"}
{"id": "38714", "url": "https://en.wikipedia.org/wiki?curid=38714", "title": "World", "text": "World\n\nThe world is the planet Earth and all life upon it, including human civilization. In a philosophical context, the \"world\" is the whole of the physical Universe, or an ontological world (the \"world\" of an individual). In a theological context, the \"world\" is the material or the profane sphere, as opposed to the celestial, spiritual, transcendent or sacred spheres. \"End of the world\" scenarios refer to the end of human history, often in religious contexts.\n\nThe history of the world is commonly understood as spanning the major geopolitical developments of about five millennia, from the first civilizations to the present. In terms such as world religion, world language, world government, and world war, the term \"world\" suggests an international or intercontinental scope without necessarily implying participation of every part of the world.\n\nThe world population is the sum of all human populations at any time; similarly, the world economy is the sum of the economies of all societies or countries, especially in the context of globalization. Terms such as \"world championship\", \"gross world product\", and \"world flags\" imply the sum or combination of all sovereign states.\n\nThe English word \"world\" comes from the Old English \"weorold (-uld), weorld, worold (-uld, -eld)\", a compound of \"wer\" \"man\" and \"eld\" \"age,\" which thus means roughly \"Age of Man.\"\nThe Old English is a reflex of the Common Germanic \"*wira-alđiz\", also reflected in Old Saxon \"werold\", Old Dutch \"werilt\", Old High German \"weralt\", Old Frisian \"warld\" and Old Norse \"verǫld\" (whence the Icelandic \"veröld\").\n\nThe corresponding word in Latin is \"mundus\", literally \"clean, elegant\", itself a loan translation of Greek \"cosmos\" \"orderly arrangement.\" While the Germanic word thus reflects a mythological notion of a \"domain of Man\" (compare Midgard), presumably as opposed to the divine sphere on the one hand and the chthonic sphere of the underworld on the other, the Greco-Latin term expresses a notion of creation as an act of establishing order out of chaos.\n\n\"World\" distinguishes the entire planet or population from any particular country or region: \"world affairs\" pertain not just to one place but to the whole world, and \"world history\" is a field of history that examines events from a global (rather than a national or a regional) perspective. \"Earth\", on the other hand, refers to the planet as a physical entity, and distinguishes it from other planets and physical objects.\n\n\"World\" was also classically used to mean the material universe, or the cosmos: \"The worlde is an apte frame of heauen and earthe, and all other naturall thinges contained in them.\" The earth was often described as \"the center of the world\".\n\nThe term can also be used attributively, to mean \"global\", or \"relating to the whole world\", forming usages such as world community or world canonical texts.\n\nBy extension, a \"world\" may refer to any planet or heavenly body, especially when it is thought of as inhabited, especially in the context of science fiction or futurology.\n\n\"World\", in its original sense, when qualified, can also refer to a particular domain of human experience.\n\n\nIn philosophy, the term world has several possible meanings. In some contexts, it refers to everything that makes up reality or the physical universe. In others, it can mean have a specific ontological sense (see world disclosure). While clarifying the concept of world has arguably always been among the basic tasks of Western philosophy, this theme appears to have been raised explicitly only at the start of the twentieth century and has been the subject of continuous debate. The question of what the world is has by no means been settled.\n\nThe traditional interpretation of Parmenides' work is that he argued that the everyday perception of reality of the physical world (as described in \"doxa\") is mistaken, and that the reality of the world is 'One Being' (as described in aletheia): an unchanging, ungenerated, indestructible whole.\n\nIn his Allegory of the Cave, Plato distinguishes between forms and ideas and imagines two distinct worlds: the sensible world and the intelligible world.\n\nIn Georg Wilhelm Friedrich Hegel's philosophy of history, the expression \"Weltgeschichte ist Weltgericht\" (World History is a tribunal that judges the World) is used to assert the view that History is what judges men, their actions and their opinions. Science is born from the desire to transform the World in relation to Man; its final end is technical application.\n\n\"The World as Will and Representation\" is the central work of Arthur Schopenhauer.\nSchopenhauer saw the human will as our one window to the world behind the representation; the Kantian thing-in-itself. He believed, therefore, that we could gain knowledge about the thing-in-itself, something Kant said was impossible, since the rest of the relationship between representation and thing-in-itself could be understood by analogy to the relationship between human will and human body.\n\nTwo definitions that were both put forward in the 1920s, however, suggest the range of available opinion. \"The world is everything that is the case,\" wrote Ludwig Wittgenstein in his influential \"Tractatus Logico-Philosophicus\", first published in 1921. This definition would serve as the basis of logical positivism, with its assumption that there is exactly one world, consisting of the totality of facts, regardless of the interpretations that individual people may make of them.\n\nMartin Heidegger, meanwhile, argued that \"the surrounding world is different for each of us, and notwithstanding that we move about in a common world\". The world, for Heidegger, was that into which we are always already \"thrown\" and with which we, as beings-in-the-world, must come to terms. His conception of \"world disclosure\" was most notably elaborated in his 1927 work \"Being and Time\".\n\nIn response, Sigmund Freud proposed that we do not move about in a common world, but a common thought process. He believed that all the actions of a person are motivated by one thing: lust. This led to numerous theories about reactionary consciousness.\n\nSome philosophers, often inspired by David Lewis, argue that metaphysical concepts such as possibility, probability, and necessity are best analyzed by comparing \"the\" world to a range of possible worlds; a view commonly known as modal realism.\n\nMythological cosmologies often depict the world as centered on an \"axis mundi\" and delimited by a boundary such as a world ocean, a world serpent or similar. In some religions, worldliness (also called carnality) is that which relates to this world as opposed to other worlds or realms.\n\nIn Buddhism, the world means society, as distinct from the monastery. It refers to the material world, and to worldly gain such as wealth, reputation, jobs, and war. The spiritual world would be the path to enlightenment, and changes would be sought in what we could call the psychological realm.\n\nIn Christianity, the term often connotes the concept of the fallen and corrupt world order of human society, in contrast to the World to Come. The world is frequently cited alongside \"the flesh\" and \"the Devil\" as a source of temptation that Christians should flee. Monks speak of striving to be \"\"in\" this world, but not \"of\" this world\"—as Jesus said—and the term \"worldhood\" has been distinguished from \"monkhood\", the former being the status of merchants, princes, and others who deal with \"worldly\" things.\n\nThis view is clearly expressed by king Alfred the Great of England (d. 899) in his famous Preface to the \"Cura Pastoralis\":\nAlthough Hebrew and Greek words meaning \"world\" are used in Scripture with the normal variety of senses, many examples of its use in this particular sense can be found in the teachings of Jesus according to the Gospel of John, e.g. 7:7, 8:23, 12:25, 14:17, 15:18-19, 17:6-25, 18:36. For contrast, a relatively newer concept is Catholic imagination.\n\n\"Contemptus mundi\" is the name given to the recognition that the world, in all its vanity, is nothing more than a futile attempt to hide from God by stifling our desire for the good and the holy. This view has been criticized as a \"pastoral of fear\" by modern historian Jean Delumeau.\n\nDuring the Second Vatican Council, there was a novel attempt to develop a positive theological view of the World, which is illustrated by the pastoral optimism of the constitutions \"Gaudium et spes\", \"Lumen gentium\", \"Unitatis redintegratio\" and \"Dignitatis humanae\".\n\nIn Eastern Christian monasticism or asceticism, the world of mankind is driven by passions. Therefore, the passions of the World are simply called \"the world\". Each of these passions are a link to the world of mankind or order of human society. Each of these passions must be overcome in order for a person to receive salvation (theosis). The process of theosis is a personal relationship with God. This understanding is taught within the works of ascetics like Evagrius Ponticus, and the most seminal ascetic works read most widely by Eastern Christians, the Philokalia and the Ladder of Divine Ascent (the works of Evagrius and John Climacus are also contained within the Philokalia). At the highest level of world transcendence is hesychasm which culminates into the Vision of God.\n\n\"Orbis Catholicus\" is a Latin phrase meaning \"Catholic world\", per the expression Urbi et Orbi, and refers to that area of Christendom under papal supremacy. It is somewhat similar to the phrases secular world, Jewish world and Islamic world.\n\n\"Dunya\" derives from the root word \"dana\" that means to bring near. In that sense, \"dunya\" is \"what is brought near\".\n\nHinduism is an Indian religion and \"dharma\", or a way of life, widely practised in the Indian subcontinent. It includes a number of Indian religious traditions with a loose sense of interconnection, as different from Jainism and Buddhism, and (since medieaval and modern times) Islam and Christianity. Hinduism has been called the oldest religion in the world,\n"}
