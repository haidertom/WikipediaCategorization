{"id": "2249310", "url": "https://en.wikipedia.org/wiki?curid=2249310", "title": "Aether theories", "text": "Aether theories\n\nAether theories (also known as ether theories) in physics propose the existence of a medium, the aether (also spelled \"ether\", from the Greek word (), meaning \"upper air\" or \"pure, fresh air\"), a space-filling substance or field, thought to be necessary as a transmission medium for the propagation of electromagnetic or gravitational forces. The assorted \"aether theories\" embody the various conceptions of this \"medium\" and \"substance\"\". This early modern aether has little in common with the aether of classical elements from which the name was borrowed. Since the development of special relativity, theories using a substantial aether fell out of use in modern physics, and were replaced by more abstract models.\n\nIsaac Newton suggests the existence of an aether in the Third Book of \"Opticks\" (1718): \"Doth not this aethereal medium in passing out of water, glass, crystal, and other compact and dense bodies in empty spaces, grow denser and denser by degrees, and by that means refract the rays of light not in a point, but by bending them gradually in curve lines? ...Is not this medium much rarer within the dense bodies of the Sun, stars, planets and comets, than in the empty celestial space between them? And in passing from them to great distances, doth it not grow denser and denser perpetually, and thereby cause the gravity of those great bodies towards one another, and of their parts towards the bodies; every body endeavouring to go from the denser parts of the medium towards the rarer?\"\n\nIn the 19th century, luminiferous aether (or ether), meaning light-bearing aether, was a theorized medium for the propagation of light (electromagnetic radiation). However, a series of increasingly complex experiments had been carried out in the late 1800s like the Michelson-Morley experiment in an attempt to detect the motion of Earth through the aether, and had failed to do so. A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. Joseph Larmor discussed the aether in terms of a moving magnetic field caused by the acceleration of electrons.\n\nJames Clerk Maxwell said of the aether, \"In several parts of this treatise an attempt has been made to explain electromagnetic phenomena by means of mechanical action transmitted from one body to another by means of a medium occupying the space between them. The undulatory theory of light also assumes the existence of a medium. We have now to show that the properties of the electromagnetic medium are identical with those of the luminiferous medium.\"\n\nHendrik Lorentz and George Francis FitzGerald offered within the framework of Lorentz ether theory a more elegant solution to how the motion of an absolute aether could be undetectable (length contraction), but if their equations were correct, Albert Einstein's 1905 special theory of relativity could generate the same mathematics without referring to an aether at all. This led most physicists to conclude that this early modern notion of a luminiferous aether was not a useful concept. Einstein however stated that this consideration was too radical and too anticipatory and that his theory of relativity still needed the presence of a medium with certain properties.\n\nFrom the 16th until the late 19th century, gravitational phenomena had also been modelled utilizing an aether. The most well-known formulation is Le Sage's theory of gravitation, although other models were proposed by Isaac Newton, Bernhard Riemann, and Lord Kelvin. None of those concepts are considered to be viable by the scientific community today.\n\nEinstein sometimes used the word \"aether\" for the gravitational field within general relativity, but this terminology never gained widespread support.\n\nQuantum mechanics can be used to describe spacetime as being non-empty at extremely small scales, fluctuating and generating particle pairs that appear and disappear incredibly quickly. It has been suggested by some such as Paul Dirac that this quantum vacuum may be the equivalent in modern physics of a particulate aether. However, Dirac's aether hypothesis was motivated by his dissatisfaction with quantum electrodynamics, and it never gained support from the mainstream scientific community.\n\nRobert B. Laughlin, Nobel Laureate in Physics, endowed chair in physics, Stanford University, had this to say about ether in contemporary theoretical physics:\n\nLouis de Broglie stated, \"Any particle, ever isolated, has to be imagined as in continuous \"energetic contact\" with a hidden medium.\"\n\nAccording to the philosophical point of view of Einstein, Dirac, Bell, Polyakov, ’t Hooft, Laughlin, de Broglie, Maxwell, Newton and other theorists, there might be a medium with physical properties filling 'empty' space, an aether, enabling the observed physical processes.\n\nAlbert Einstein in 1894 or 1895: \"The velocity of a wave is proportional to the square root of the elastic forces which cause [its] propagation, and inversely proportional to the mass of the aether moved by these forces.\"\n\nAlbert Einstein in 1920: \"We may say that according to the general theory of relativity space is endowed with physical qualities; in this sense, therefore, there exists an Aether. According to the general theory of relativity space without Aether is unthinkable; for in such space there not only would be no propagation of light, but also no possibility of existence for standards of space and time (measuring-rods and clocks), nor therefore any space-time intervals in the physical sense. But this Aether may not be thought of as endowed with the quality characteristic of ponderable media, as consisting of parts which may be tracked through time. The idea of motion may not be applied to it.\"\n\nPaul Dirac wrote in 1951: \"Physical knowledge has advanced much since 1905, notably by the arrival of quantum mechanics, and the situation [about the scientific plausibility of Aether] has again changed. If one examines the question in the light of present-day knowledge, one finds that the Aether is no longer ruled out by relativity, and good reasons can now be advanced for postulating an Aether ... We have now the velocity at all points of space-time, playing a fundamental part in electrodynamics. It is natural to regard it as the velocity of some real physical thing. Thus with the new theory of electrodynamics [vacuum filled with virtual particles] we are rather forced to have an Aether\".\n\nJohn Bell in 1986, interviewed by Paul Davies in \"The Ghost in the Atom\" has suggested that an Aether theory might help resolve the EPR paradox by allowing a reference frame in which signals go faster than light. He suggests Lorentz contraction is perfectly coherent, not inconsistent with relativity, and could produce an aether theory perfectly consistent with the Michelson-Morley experiment. Bell suggests the aether was wrongly rejected on purely philosophical grounds: \"what is unobservable does not exist\" [p. 49]. Einstein found the non-aether theory simpler and more elegant, but Bell suggests that doesn't rule it out. Besides the arguments based on his interpretation of quantum mechanics, Bell also suggests resurrecting the aether because it is a useful pedagogical device. That is, many problems are solved more easily by imagining the existence of an aether.\n\nEinstein remarked \"God does not play dice with the Universe\". And those agreeing with him are looking for a classical, deterministic aether theory that would imply quantum-mechanical predictions as a statistical approximation, a hidden variable theory. In particular, Gerard 't Hooft conjectured that: \"We should not forget that quantum mechanics does not really describe what kind of dynamical phenomena are actually going on, but rather gives us probabilistic results. To me, it seems extremely plausible that any reasonable theory for the dynamics at the Planck scale would lead to processes that are so complicated to describe, that one should expect apparently stochastic fluctuations in any approximation theory describing the effects of all of this at much larger scales. It seems quite reasonable first to try a classical, deterministic theory for the Planck domain. One might speculate then that what we call quantum mechanics today, may be nothing else than an ingenious technique to handle this dynamics statistically.\" In their paper Blasone, Jizba and Kleinert \"have attempted to substantiate the recent proposal of G. ’t Hooft in which quantum theory is viewed as not a complete field theory, but is in fact an emergent phenomenon arising from a deeper level of dynamics. The underlying dynamics are taken to be classical mechanics with singular Lagrangians supplied with an appropriate information loss condition. With plausible assumptions about the actual nature of the constraint dynamics, quantum theory is shown to emerge when the classical Dirac-Bergmann algorithm for constrained dynamics is applied to the classical path integral [...].\"\n\nLouis de Broglie, \"If a hidden sub-quantum medium is assumed, knowledge of its nature would seem desirable. It certainly is of quite complex character. It could not serve as a universal reference medium, as this would be contrary to relativity theory.\"\n\nIn 1982, Ioan-Iovitz Popescu, a Romanian physicist, wrote that the aether is \"a form of existence of the matter, but it differs qualitatively from the common (atomic and molecular) substance or radiation (photons)\". The \"fluid aether\" is \"governed by the principle of inertia and its presence produces a modification of the space-time geometry\". Built upon Le Sage's \"ultra-mundane corpuscles\", Popescu's theory posits a finite Universe \"filled with some particles of exceedingly small mass, traveling chaotically at speed of light\" and material bodies \"made up of such particles called \"etherons\"\".\n\nSid Deutsch, a professor of electrical engineering and bioengineerig, conjectures that a \"spherical, spinning\" \"aether particle\" must exist in order \"to carry electromagnetic waves\" and derives its diameter and mass using the density of dark matter.\n\nA degenerate Fermi fluid model, \"composed primarily of \"electrons and positrons\"\" that has the consequence of a speed of light decreasing \"with time on the scale of the age of the universe\" was proposed by Allen Rothwarf. In a cosmological extension the model was \"extended to predict a \"decelerating expansion of the universe\"\".\n\n\n"}
{"id": "277664", "url": "https://en.wikipedia.org/wiki?curid=277664", "title": "Aharonov–Bohm effect", "text": "Aharonov–Bohm effect\n\nThe Aharonov–Bohm effect, sometimes called the Ehrenberg–Siday–Aharonov–Bohm effect, is a quantum mechanical phenomenon in which an electrically charged particle is affected by an electromagnetic potential (V, A), despite being confined to a region in which both the magnetic field B and electric field E are zero. The underlying mechanism is the coupling of the electromagnetic potential with the complex phase of a charged particle's wave function, and the Aharonov–Bohm effect is accordingly illustrated by interference experiments.\n\nThe most commonly described case, sometimes called the Aharonov–Bohm solenoid effect, takes place when the wave function of a charged particle passing around a long solenoid experiences a phase shift as a result of the enclosed magnetic field, despite the magnetic field being negligible in the region through which the particle passes and the particle's wavefunction being negligible inside the solenoid. This phase shift has been observed experimentally. There are also magnetic Aharonov–Bohm effects on bound energies and scattering cross sections, but these cases have not been experimentally tested. An electric Aharonov–Bohm phenomenon was also predicted, in which a charged particle is affected by regions with different electrical potentials but zero electric field, but this has no experimental confirmation yet. A separate \"molecular\" Aharonov–Bohm effect was proposed for nuclear motion in multiply connected regions, but this has been argued to be a different kind of geometric phase as it is \"neither nonlocal nor topological\", depending only on local quantities along the nuclear path.\n\nWerner Ehrenberg (1901–1975) and Raymond E. Siday first predicted the effect in 1949. Yakir Aharonov and David Bohm published their analysis in 1959. After publication of the 1959 paper, Bohm was informed of Ehrenberg and Siday's work, which was acknowledged and credited in Bohm and Aharonov's subsequent 1961 paper. \nThe effect was confirmed experimentally, with a very large error, while Bohm was still alive. By the time the error was down to a respectable value, Bohm had died.\n\nIn the 18th and 19th centuries, physics was dominated by Newtonian dynamics, with its emphasis on forces. Electromagnetic phenomena were elucidated by a series of experiments involving the measurement of forces between charges, currents and magnets in various configurations. Eventually, a description arose according to which charges, currents and magnets acted as local sources of propagating force fields, which then acted on other charges and currents locally through the Lorentz force law. In this framework, because one of the observed properties of the electric field was that it was irrotational, and one of the observed properties of the magnetic field was that it was divergenceless, it was possible to express an electrostatic field as the gradient of a scalar potential (e.g. Coulomb's electrostatic potential, which is mathematically analogous to the classical gravitational potential) and a stationary magnetic field as the curl of a vector potential (then a new concept – the idea of a scalar potential was already well accepted by analogy with gravitational potential). The language of potentials generalised seamlessly to the fully dynamic case but, since all physical effects were describable in terms of the fields which were the derivatives of the potentials, potentials (unlike fields) were not uniquely determined by physical effects: potentials were only defined up to an arbitrary additive constant electrostatic potential and an irrotational stationary magnetic vector potential.\n\nThe Aharonov–Bohm effect is important conceptually because it bears on three issues apparent in the recasting of (Maxwell's) classical electromagnetic theory as a gauge theory, which before the advent of quantum mechanics could be argued to be a mathematical reformulation with no physical consequences. The Aharonov–Bohm thought experiments and their experimental realization imply that the issues were not just philosophical.\n\nThe three issues are: \n\nBecause of reasons like these, the Aharonov–Bohm effect was chosen by the \"New Scientist\" magazine as one of the \"seven wonders of the quantum world\".\n\nIt is generally argued that Aharonov–Bohm effect illustrates the physicality of electromagnetic potentials, \"Φ\" and A, in quantum mechanics. Classically it was possible to argue that only the electromagnetic fields are physical, while the electromagnetic potentials are purely mathematical constructs, that due to gauge freedom aren't even unique for a given electromagnetic field.\n\nHowever, Vaidman has challenged this interpretation by showing that the AB effect can be explained without the use of potentials so long as one gives a full quantum mechanical treatment to the source charges that produce the electromagnetic field. According to this view, the potential in quantum mechanics is just as physical (or non-physical) as it was classically. Aharonov, Cohen, and Rohrlich responded that the effect may be due to a local gauge potential or due to non-local gauge-invariant fields.\n\nTwo papers published in the journal in 2017 \"Physical Review A\" have demonstrated a quantum mechanical solution for the system. Their analysis shows that the phase shift can be viewed as generated by solenoid's vector potential acting on the electron or the electron's vector potential acting on the solenoid or the electron and solenoid currents acting on the quantized vector potential.\n\nSimilarly, the Aharonov–Bohm effect illustrates that the Lagrangian approach to dynamics, based on energies, is not just a computational aid to the Newtonian approach, based on forces. Thus the Aharonov–Bohm effect validates the view that forces are an incomplete way to formulate physics, and potential energies must be used instead. In fact Richard Feynman complained that he had been taught electromagnetism from the perspective of electromagnetic fields, and he wished later in life he had been taught to think in terms of the electromagnetic potential instead, as this would be more fundamental. In Feynman's path-integral view of dynamics, the potential field directly changes the phase of an electron wave function, and it is these changes in phase that lead to measurable quantities.\n\nThe Aharonov–Bohm effect shows that the local E and B fields do not contain full information about the electromagnetic field, and the electromagnetic four-potential, (\"Φ\", A), must be used instead. By Stokes' theorem, the magnitude of the Aharonov–Bohm effect can be calculated using the electromagnetic fields alone, \"or\" using the four-potential alone. But when using just the electromagnetic fields, the effect depends on the field values in a region from which the test particle is excluded. In contrast, when using just the electromagnetic four-potential, the effect only depends on the potential in the region where the test particle is allowed. Therefore, one must either abandon the principle of locality, which most physicists are reluctant to do, or accept that the electromagnetic four-potential offers a more complete description of electromagnetism than the electric and magnetic fields can. On the other hand, the AB effect is crucially quantum mechanical; quantum mechanics is well-known to feature non-local effects (albeit still disallowing superluminal communication), and Vaidman has argued that this is just a non-local quantum effect in a different form.\n\nIn classical electromagnetism the two descriptions were equivalent. With the addition of quantum theory, though, the electromagnetic potentials \"Φ\" and A are seen as being more fundamental.  \nDespite this, all observable effects end up being expressible in terms of the electromagnetic fields, E and B. This is interesting because, while you can calculate the electromagnetic field from the four-potential, due to gauge freedom the reverse is not true.\n\nThe magnetic Aharonov–Bohm effect can be seen as a result of the requirement that quantum physics be invariant with respect to the gauge choice for the electromagnetic potential, of which the magnetic vector potential formula_1 forms part.\n\nElectromagnetic theory implies that a particle with electric charge formula_2 travelling along some path formula_3 in a region with zero magnetic field formula_4, but non-zero formula_1 (by formula_6), acquires a phase shift formula_7, given in SI units by\n\nTherefore, particles, with the same start and end points, but travelling along two different routes will acquire a phase difference formula_9 determined by the magnetic flux formula_10 through the area between the paths (via Stokes' theorem and formula_11), and given by:\n\nIn quantum mechanics the same particle can travel between two points by a variety of paths. Therefore, this phase difference can be observed by placing a solenoid between the slits of a double-slit experiment (or equivalent). An ideal solenoid (i.e. infinitely long and with a perfectly uniform current distribution) encloses a magnetic field formula_4, but does not produce any magnetic field outside of its cylinder, and thus the charged particle (e.g. an electron) passing outside experiences no magnetic field formula_4. However, there is a (curl-free) vector potential formula_1 outside the solenoid with an enclosed flux, and so the relative phase of particles passing through one slit or the other is altered by whether the solenoid current is turned on or off. This corresponds to an observable shift of the interference fringes on the observation plane.\n\nThe same phase effect is responsible for the quantized-flux requirement in superconducting loops. This quantization occurs because the superconducting wave function must be single valued: its phase difference formula_9 around a closed loop must be an integer multiple of formula_17 (with the charge formula_18 for the electron Cooper pairs), and thus the flux must be a multiple of formula_19. The superconducting flux quantum was actually predicted prior to Aharonov and Bohm, by F. London in 1948 using a phenomenological model.\n\nThe first claimed experimental confirmation was by Robert G. Chambers in 1960, in an electron interferometer with a magnetic field produced by a thin iron whisker, and other early work is summarized in Olariu and Popèscu (1984). However, subsequent authors questioned the validity of several of these early results because the electrons may not have been completely shielded from the magnetic fields. An early experiment in which an unambiguous Aharonov–Bohm effect was observed by completely excluding the magnetic field from the electron path (with the help of a superconducting film) was performed by Tonomura et al. in 1986. The effect's scope and application continues to expand. Webb \"et al.\" (1985) demonstrated Aharonov–Bohm oscillations in ordinary, non-superconducting metallic rings; for a discussion, see Schwarzschild (1986) and Imry & Webb (1989). Bachtold \"et al.\" (1999) detected the effect in carbon nanotubes; for a discussion, see Kong \"et al.\" (2004).\n\nThe magnetic Aharonov–Bohm effect is also closely related to Dirac's argument that the existence of a magnetic monopole can be accommodated by the existing magnetic source-free Maxwell's equations if both electric and magnetic charges are quantized.\n\nA magnetic monopole implies a mathematical singularity in the vector potential, which can be expressed as a Dirac string of infinitesimal diameter that contains the equivalent of all of the 4π\"g\" flux from a monopole \"charge\" \"g\". The Dirac string starts from, and terminates on, a magnetic monopole. Thus, assuming the absence of an infinite-range scattering effect by this arbitrary choice of singularity, the requirement of single-valued wave functions (as above) necessitates charge-quantization. That is, formula_20 must be an integer (in cgs units) for any electric charge \"q\" and magnetic charge \"q\".\n\nLike the electromagnetic potential A the Dirac string is not gauge invariant (it moves around with fixed endpoints under a gauge transformation) and so is also not directly measurable.\n\nJust as the phase of the wave function depends upon the magnetic vector potential, it also depends upon the scalar electric potential. By constructing a situation in which the electrostatic potential varies for two paths of a particle, through regions of zero electric field, an observable Aharonov–Bohm interference phenomenon from the phase shift has been predicted; again, the absence of an electric field means that, classically, there would be no effect.\n\nFrom the Schrödinger equation, the phase of an eigenfunction with energy \"E\" goes as formula_21. The energy, however, will depend upon the electrostatic potential \"V\" for a particle with charge \"q\". In particular, for a region with constant potential \"V\" (zero field), the electric potential energy \"qV\" is simply added to \"E\", resulting in a phase shift:\n\nwhere \"t\" is the time spent in the potential.\n\nThe initial theoretical proposal for this effect suggested an experiment where charges pass through conducting cylinders along two paths, which shield the particles from external electric fields in the regions where they travel, but still allow a varying potential to be applied by charging the cylinders. This proved difficult to realize, however. Instead, a different experiment was proposed involving a ring geometry interrupted by tunnel barriers, with a bias voltage \"V\" relating the potentials of the two halves of the ring. This situation results in an Aharonov–Bohm phase shift as above, and was observed experimentally in 1998.\n\nNano rings were created by accident while intending to make quantum dots. They have interesting optical properties associated with excitons and the Aharonov–Bohm effect. Application of these rings used as light capacitors or buffers includes photonic computing and communications technology. Analysis and measurement of geometric phases in mesoscopic rings is ongoing. It is even suggested they could be used to make a form of slow glass.\n\nSeveral experiments, including some reported in 2012, show Aharonov-Bohm oscillations in charge density wave (CDW) current versus magnetic flux, of dominant period \"h\"/2\"e\" through CDW rings up to 85 µm in circumference above 77 K. This behavior is similar to that of the superconducting quantum interference devices (see SQUID).\n\nThe Aharonov–Bohm effect can be understood from the fact that one can only measure absolute values of the wave function. While this allows for measurement of phase differences through quantum interference experiments, there is no way to specify a wavefunction with constant absolute phase. In the absence of an electromagnetic field one can come close by declaring the eigenfunction of the momentum operator with zero momentum to be the function \"1\" (ignoring normalization problems) and specifying wave functions relative to this eigenfunction \"1\". In this representation the i-momentum operator is (up to a factor formula_23) the differential operator formula_24. However, by gauge invariance, it is equally valid to declare the zero momentum eigenfunction to be formula_25 at the cost of representing the i-momentum operator (up to a factor) as formula_26 i.e. with a pure gauge vector potential formula_27. There is no real asymmetry because representing the former in terms of the latter is just as messy as representing the latter in terms of the former. This means that it is physically more natural to describe wave \"functions\", in the language of differential geometry, as sections in a complex line bundle with a hermitian metric and a U(1)-connection formula_28. The curvature form of the connection, formula_29, is, up to the factor i, the Faraday tensor of the electromagnetic field strength. The Aharonov–Bohm effect is then a manifestation of the fact that a connection with zero curvature (i.e. flat), need not be trivial since it can have monodromy along a topologically nontrivial path fully contained in the zero curvature (i.e. field free) region. By definition this means that sections that are parallelly translated along a topologically non trivial path pick up a phase, so that covariant constant sections cannot be defined over the whole field free region.\n\nGiven a trivialization of the line-bundle, a non-vanishing section, the U(1)-connection is given by the 1-form corresponding to the electromagnetic four-potential \"A\" as formula_30 where \"d\" means exterior derivation on the Minkowski space. The monodromy is the holonomy of the flat connection. The holonomy of a connection, flat or non flat, around a closed loop formula_31 is formula_32 (one can show this does not depend on the trivialization but only on the connection). For a flat connection one can find a gauge transformation in any simply connected field free region(acting on wave functions and connections) that gauges away the vector potential. However, if the monodromy is nontrivial, there is no such gauge transformation for the whole outside region. In fact as a consequence of Stokes' theorem, the holonomy is determined by the magnetic flux through a surface formula_33 bounding the loop formula_31, but such a surface may exist only if formula_33 passes through a region of non trivial field:\n\nThe monodromy of the flat connection only depends on the topological type of the loop in the field free region (in fact on the loops homology class). The holonomy description is general, however, and works inside as well as outside the superconductor. Outside of the conducting tube containing the magnetic field, the field strength formula_37. In other words, outside the tube the connection is flat, and the monodromy of the loop contained in the field-free region depends only on the winding number around the tube. The monodromy of the connection for a loop going round once (winding number 1) is the phase difference of a particle interfering by propagating left and right of the superconducting tube containing the magnetic field. \nIf one wants to ignore the physics inside the superconductor and only describe the physics in the outside region, it becomes natural and mathematically convenient to describe the quantum electron by a section in a complex line bundle with an \"external\" flat connection formula_28 with monodromy\n\nrather than an external EM field formula_41. The Schrödinger equation readily generalizes to this situation by using the Laplacian of the connection for the (free) Hamiltonian\n\nEquivalently, one can work in two simply connected regions with cuts that pass from the tube towards or away from the detection screen. In each of these regions the ordinary free Schrödinger equations would have to be solved, but in passing from one region to the other, in only one of the two connected components of the intersection (effectively in only one of the slits) a monodromy factor formula_43 is picked up, which results in the shift in the interference pattern as one changes the flux.\n\nEffects with similar mathematical interpretation can be found in other fields. For example, in classical statistical physics, quantization of a molecular motor motion in a stochastic environment can be interpreted as an Aharonov–Bohm effect induced by a gauge field acting in the space of control parameters.\n\n\n"}
{"id": "25287133", "url": "https://en.wikipedia.org/wiki?curid=25287133", "title": "Anywhere on Earth", "text": "Anywhere on Earth\n\nAnywhere on Earth (AoE) is a calendar designation which indicates that a period expires when the date passes everywhere on Earth. The last place on Earth where any date exists is on Howland and Baker islands, in the time zone (the West side of the International Date Line), and so is the last spot on the globe for any day to exist. Therefore, the day ends AoE when it ends on Howland Island.\n\nThe convention originated in IEEE 802.16 balloting procedures. At this point, many IEEE 802 ballot deadlines are established as the end of day using \"AoE\", for \"Anywhere on Earth\" as a designation. This means that the deadline has not passed if, anywhere on Earth, the deadline date has not yet passed.\n\nNote that the day's end AoE occurs at noon Coordinated Universal Time (UTC) of the following day, Howland and Baker islands being halfway around the world from the prime meridian that is the base reference longitude for UTC.\n\nThus, in standard notation this is:\n\n"}
{"id": "30744522", "url": "https://en.wikipedia.org/wiki?curid=30744522", "title": "Applications of evolution", "text": "Applications of evolution\n\nEvolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications. Creationists often claim that the theory of evolution lacks any practical applications; however, this claim has been refuted by scientists.\n\nThe evolutionary approach is key to much current research in biology that does not set out to study evolution per se, especially in organismal biology and ecology. For example, evolutionary thinking is key to life history theory. Annotation of genes and their function relies heavily on comparative, that is evolutionary, approaches. The field of evolutionary developmental biology investigates how developmental processes work by using the comparative method to determine how they evolved.\n\nA major technological application of evolution is artificial selection, which is the intentional selection of certain traits in a population of organisms. Humans have used artificial selection for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA in molecular biology. It is also possible to use repeated rounds of mutation and selection to evolve proteins with particular properties, such as modified enzymes or new antibodies, in a process called directed evolution.\n\nAntibiotic resistance can be a result of point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure; those bacteria which have a mutation allowing them to survive will live on to reproduce. They will then pass this trait to their offspring, which will result in a fully resistant colony.\n\nUnderstanding the changes that have occurred during organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation, such as crystallins and the melanocortin 1 receptor. Similarly, comparing the genome of the Antarctic icefish, which lacks red blood cells, to close relatives such as the Antarctic rockcod revealed genes needed to make these blood cells.\n\nAs evolution can produce highly optimised processes and networks, it has many applications in computer science. Here, simulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n"}
{"id": "10038314", "url": "https://en.wikipedia.org/wiki?curid=10038314", "title": "Arizona flagstone", "text": "Arizona flagstone\n\nArizona flagstone is composed of rounded grains of quartz which are cemented by silica. Other minerals are present, mostly as thin seams of clay, mica, secondary calcite, and gypsum. Arizona flagstone is mainly quarried from the Coconino and Prescott National Forests.\n\nAlthough flagstone and dimension stone are quarried from all over the state of Arizona, the town of Ash Fork, Arizona, is well known as the center of production and has proclaimed itself \"The Flagstone Capital of the World\". Extensive outcrops of Arizona flagstone are also found in Mohave, Coconino, Yavapai, Navajo, Apache, and Gila counties.\n\n"}
{"id": "30938173", "url": "https://en.wikipedia.org/wiki?curid=30938173", "title": "Ate-u-tiv", "text": "Ate-u-tiv\n\nAte-u-Tiv (sometimes written as \"Ate u Tiv\" and less popularly known as \"Tsun\") is a traditional architectural masterpiece of the Tiv People of the Middle-belt Region of Nigeria in West Africa.\n\nThe word \"Atē\" stands for the round, open hut; while \"Átē-ŭ-Tiv\" attributes it to the Tiv people. The Ate-u-Tiv serves as a relaxation and reception point for \"vanya\" (guests) and allows \"mbamaren, ônov man angbianev\" (family members) to \"tema imiôngo\" (chat), sharing ideas and telling stories. The \"Orya\" (family head) receives guests and attend to family issues (discussions) from the \"Ate\".\n\nTo construct a traditional Ate-u-Tiv, you will require a minimum of six poles called \"mtôm\" which are y-shaped at the top; these serve as the pillars. The total number of poles required will however depend on the diameter of the Ate to be constructed. These are erected upright into the ground in a circle with equal spacing between poles. Next is \"ukyaver\" which comprise stems of slim climbing plants. These are used in constructing a sort of lintel to hold the roof.\n\nThe roof of an \"Ate-u-Tiv\" comprise \"ihyange\" (paulins) and \"ihila\" (grass). The ihyange are woven together in a cone-shape with ukyaver holding together the rafters. The completed structure is then hoisted unto the pillars/lintel with the coned-top upright. Ihila, having been woven together, is then used to provide a thick-layered roofing. This roof filters incoming air making it cool and clean and at the same time stopping the rains.\n\nThe Tiv people are well known for their hospitality and the \"Ate-u-Tiv\" is an important component of this hospitality. In order to readily receive visitors, each compound builds an \"Ate\" which is furnished with chairs made from wood, canes, etc. In modern days, the components of the \"Ate\" may vary. Some roofs are now a combination of iron roofing sheets covered by grass that may not necessarily be \"ihila\"; paulins are regularly made of plywood, etc. The \"Ate\" design now adorns public places such as hotel gardens, public amusement parks, zoos, museums, etc. New usage of the term \"Ate-u-Tiv\" may refer to a meeting place, social network or forum.\n\nLike Tiv music, modernization has not changed Tiv architecture, particularly the design and usage of the \"Ate-u-Tiv\". Ate-u-Tiv has remained a symbol of Tiv hospitality.\n\n\n"}
{"id": "60784", "url": "https://en.wikipedia.org/wiki?curid=60784", "title": "Boulder", "text": "Boulder\n\nIn geology, a boulder is a rock fragment with size greater than in diameter. Smaller pieces are called cobbles and pebbles. While a boulder may be small enough to move or roll manually, others are extremely massive. \nIn common usage, a boulder is too large for a person to move. Smaller boulders are usually just called rocks or stones. The word \"boulder\" is short for \"boulder stone\", from Middle English \"bulderston\" or Swedish \"bullersten\".\n\nIn places covered by ice sheets during Ice Ages, such as Scandinavia, northern North America, and Siberia, glacial erratics are common. Erratics are boulders picked up by ice sheets during their advance, and deposited when they melt. They are called \"erratic\" because they typically are of a different rock type than the bedrock on which they are deposited. One of them is used as the pedestal of the Bronze Horseman in Saint Petersburg, Russia.\n\nSome noted rock formations involve giant boulders exposed by erosion, such as the Devil's Marbles in Australia's Northern Territory, the Horeke basalts in New Zealand, where an entire valley contains only boulders, and The Baths on the island of Virgin Gorda in the British Virgin Islands.\n\nBoulder-sized clasts are found in some sedimentary rocks, such as coarse conglomerate and boulder clay.\n\nThe climbing of large boulders is called bouldering.\n\n"}
{"id": "19349161", "url": "https://en.wikipedia.org/wiki?curid=19349161", "title": "Cambrian explosion", "text": "Cambrian explosion\n\nThe Cambrian explosion or Cambrian radiation was an event approximately in the Cambrian period when most major animal phyla appeared in the fossil record. It lasted for about 20–25 million years. It resulted in the divergence of most modern metazoan phyla. The event was accompanied by major diversification of other organisms.\n\nBefore the Cambrian explosion, most organisms were simple, composed of individual cells occasionally organized into colonies. Over the following 70 to 80 million years, the rate of diversification accelerated, and the variety of life began to resemble that of today. Almost all present animal phyla appeared during this period.\n\nThe Cambrian explosion has generated extensive scientific debate.\n\nThe seemingly rapid appearance of fossils in the \"Primordial Strata\" was noted by William Buckland in the 1840s, and in his 1859 book \"On the Origin of Species\", Charles Darwin discussed the then inexplicable lack of earlier fossils as one of the main difficulties for his theory of descent with slow modification through natural selection. The long-running puzzlement about the appearance of the Cambrian fauna, seemingly abruptly, without precursor, centers on three key points: whether there really was a mass diversification of complex organisms over a relatively short period of time during the early Cambrian; what might have caused such rapid change; and what it would imply about the origin of animal life. Interpretation is difficult due to a limited supply of evidence, based mainly on an incomplete fossil record and chemical signatures remaining in Cambrian rocks.\n\nThe first discovered Cambrian fossils were trilobites, described by Edward Lhuyd, the curator of Oxford Museum, in 1698. Although their evolutionary importance was not known, on the basis of their old age, William Buckland (1784–1856) realised that a dramatic step-change in the fossil record had occurred around the base of what we now call the Cambrian. Nineteenth-century geologists such as Adam Sedgwick and Roderick Murchison used the fossils for dating rock strata, specifically for establishing the Cambrian and Silurian periods. By 1859, leading geologists including Roderick Murchison, were convinced that what was then called the lowest Silurian stratum showed the origin of life on Earth, though others, including Charles Lyell, differed. In \"On the Origin of Species\", Charles Darwin considered this sudden appearance of a solitary group of trilobites, with no apparent antecedents, and absence of other fossils, to be \"undoubtedly of the gravest nature\" among the difficulties in his theory of natural selection. He reasoned that earlier seas had swarmed with living creatures, but that their fossils had not been found due to the imperfections of the fossil record. In the sixth edition of his book, he stressed his problem further as:\n\nAmerican paleontologist Charles Walcott, who studied the Burgess Shale fauna, proposed that an interval of time, the \"Lipalian\", was not represented in the fossil record or did not preserve fossils, and that the ancestors of the Cambrian animals evolved during this time.\n\nEarlier fossil evidence has since been found. The earliest claim is that the history of life on earth goes back : Rocks of that age at Warrawoona, Australia, were claimed to contain fossil stromatolites, stubby pillars formed by colonies of microorganisms. Fossils (\"Grypania\") of more complex eukaryotic cells, from which all animals, plants, and fungi are built, have been found in rocks from , in China and Montana. Rocks dating from contain fossils of the Ediacara biota, organisms so large that they are likely multicelled, but very unlike any modern organism. In 1948, Preston Cloud argued that a period of \"eruptive\" evolution occurred in the Early Cambrian, but as recently as the 1970s, no sign was seen of how the 'relatively' modern-looking organisms of the Middle and Late Cambrian arose.\n\nThe intense modern interest in this \"Cambrian explosion\" was sparked by the work of Harry B. Whittington and colleagues, who, in the 1970s, reanalysed many fossils from the Burgess Shale and concluded that several were as complex as, but different from, any living animals. The most common organism, \"Marrella\", was clearly an arthropod, but not a member of any known arthropod class. Organisms such as the five-eyed \"Opabinia\" and spiny slug-like \"Wiwaxia\" were so different from anything else known that Whittington's team assumed they must represent different phyla, seemingly unrelated to anything known today. Stephen Jay Gould's popular 1989 account of this work, \"Wonderful Life\", brought the matter into the public eye and raised questions about what the explosion represented. While differing significantly in details, both Whittington and Gould proposed that all modern animal phyla had appeared almost simultaneously in a rather short span of geological period. This view led to the modernization of Darwin's tree of life and the theory of punctuated equilibrium, which Eldredge and Gould developed in the early 1970s and which views evolution as long intervals of near-stasis \"punctuated\" by short periods of rapid change.\n\nOther analyses, some more recent and some dating back to the 1970s, argue that complex animals similar to modern types evolved well before the start of the Cambrian.\n\nRadiometric dates for much of the Cambrian, obtained by analysis of radioactive elements contained within rocks, have only recently become available, and for only a few regions.\n\nRelative dating (\"A\" was before \"B\") is often assumed sufficient for studying processes of evolution, but this, too, has been difficult, because of the problems involved in matching up rocks of the same age across different continents.\n\nTherefore, dates or descriptions of sequences of events should be regarded with some caution until better data become available.\n\nFossils of organisms' bodies are usually the most informative type of evidence. Fossilization is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence, the fossil record is very incomplete, increasingly so as earlier times are considered. Despite this, they are often adequate to illustrate the broader patterns of life's history. Also, biases exist in the fossil record: different environments are more favourable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although 30-plus phyla of living animals are known, two-thirds have never been found as fossils.\nThe Cambrian fossil record includes an unusually high number of lagerstätten, which preserve soft tissues. These allow paleontologists to examine the internal anatomy of animals, which in other sediments are only represented by shells, spines, claws, etc. – if they are preserved at all. The most significant Cambrian lagerstätten are the early Cambrian Maotianshan shale beds of Chengjiang (Yunnan, China) and Sirius Passet (Greenland); the middle Cambrian Burgess Shale (British Columbia, Canada); and the late Cambrian Orsten (Sweden) fossil beds.\n\nWhile lagerstätten preserve far more than the conventional fossil record, they are far from complete. Because lagerstätten are restricted to a narrow range of environments (where soft-bodied organisms can be preserved very quickly, e.g. by mudslides), most animals are probably not represented; further, the exceptional conditions that create lagerstätten probably do not represent normal living conditions. In addition, the known Cambrian lagerstätten are rare and difficult to date, while Precambrian lagerstätten have yet to be studied in detail.\n\nThe sparseness of the fossil record means that organisms usually exist long before they are found in the fossil record – this is known as the Signor–Lipps effect.\n\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and reflects organisms' behaviour. Also, many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. While exact assignment of trace fossils to their makers is generally impossible, traces may, for example, provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\n\nSeveral chemical markers indicate a drastic change in the environment around the start of the Cambrian. The markers are consistent with a mass extinction, or with a massive warming resulting from the release of methane ice.\nSuch changes may reflect a cause of the Cambrian explosion, although they may also have resulted from an increased level of biological activity – a possible result of the explosion. Despite these uncertainties, the geochemical evidence helps by making scientists focus on theories that are consistent with at least one of the likely environmental changes.\n\nCladistics is a technique for working out the \"family tree\" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characteristics that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or protein. The result of a successful analysis is a hierarchy of clades – groups whose members are believed to share a common ancestor. The cladistic technique is sometimes problematic, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.\n\nFrom the relationships, it may be possible to constrain the date that lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved more than X million years ago.\n\nIt is also possible to estimate how long ago two living clades diverged – i.e. about how long ago their last common ancestor must have lived  – by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only a very approximate timing: they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques vary by a factor of two. However, the clocks can give an indication of branching rate, and when combined with the constraints of the fossil record, recent clocks suggest a sustained period of diversification through the Ediacaran and Cambrian.\n\nA phylum is the highest level in the Linnaean system for classifying organisms. Phyla can be thought of as groupings of animals based on general body plan. Despite the seemingly different external appearances of organisms, they are classified into phyla based on their internal and developmental organizations. For example, despite their obvious differences, spiders and barnacles both belong to the phylum Arthropoda, but earthworms and tapeworms, although similar in shape, belong to different phyla. As chemical and genetic testing becomes more accurate, previously hypothesised phyla are often entirely reworked.\n\nA phylum is not a fundamental division of nature, such as the difference between electrons and protons. It is simply a very high-level grouping in a classification system created to describe all currently living organisms. This system is imperfect, even for modern animals: different books quote different numbers of phyla, mainly because they disagree about the classification of a huge number of worm-like species. As it is based on living organisms, it accommodates extinct organisms poorly, if at all.\n\nThe concept of stem groups was introduced to cover evolutionary \"aunts\" and \"cousins\" of living groups, and have been hypothesized based on this scientific theory. A crown group is a group of closely related living animals plus their last common ancestor plus all its descendants. A stem group is a set of offshoots from the lineage at a point earlier than the last common ancestor of the crown group; it is a relative concept, for example tardigrades are living animals that form a crown group in their own right, but Budd (1996) regarded them as also being a stem group relative to the arthropods.\n\nThe term \"Triploblastic\" means consisting of three layers, which are formed in the embryo, quite early in the animal's development from a single-celled egg to a larva or juvenile form. The innermost layer forms the digestive tract (gut); the outermost forms skin; and the middle one forms muscles and all the internal organs except the digestive system. Most types of living animal are triploblastic – the best-known exceptions are Porifera (sponges) and Cnidaria (jellyfish, sea anemones, etc.).\n\nThe bilaterians are animals that have right and left sides at some point in their life histories. This implies that they have top and bottom surfaces and, importantly, distinct front and back ends. All known bilaterian animals are triploblastic, and all known triploblastic animals are bilaterian. Living echinoderms (sea stars, sea urchins, sea cucumbers, etc.) 'look' radially symmetrical (like wheels) rather than bilaterian, but their larvae exhibit bilateral symmetry and some of the earliest echinoderms may have been bilaterally symmetrical. Porifera and Cnidaria are radially symmetrical, not bilaterian, and not triploblastic.\n\nThe term \"Coelomate\" means having a body cavity (coelom) containing the internal organs. Most of the phyla featured in the debate about the Cambrian explosion are coelomates: arthropods, annelid worms, molluscs, echinoderms, and chordates – the noncoelomate priapulids are an important exception. All known coelomate animals are triploblastic bilaterians, but some triploblastic bilaterian animals do not have a coelom – for example flatworms, whose organs are surrounded by unspecialized tissues.\n\nUnderstanding of the Cambrian explosion relies upon knowing what was there beforehand – did the event herald the sudden appearance of a wide range of animals and behaviours, or did such things exist beforehand?\n\nPhylogenetic analysis has been used to support the view that during the Cambrian explosion, metazoans (multi-celled animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.\n\nChanges in the abundance and diversity of some types of fossil have been interpreted as evidence for \"attacks\" by animals or other organisms. Stromatolites, stubby pillars built by colonies of microorganisms, are a major constituent of the fossil record from about , but their abundance and diversity declined steeply after about . This decline has been attributed to disruption by grazing and burrowing animals.\n\nPrecambrian marine diversity was dominated by small fossils known as acritarchs. This term describes almost any small organic walled fossil – from the egg cases of small metazoans to resting cysts of many different kinds of green algae. After appearing around , acritarchs underwent a boom around , increasing in abundance, diversity, size, complexity of shape, and especially size and number of spines. Their increasingly spiny forms in the last 1 billion years may indicate an increased need for defence against predation. Other groups of small organisms from the Neoproterozoic era also show signs of antipredator defenses. A consideration of taxon longevity appears to support an increase in predation pressure around this time.\nIn general, the fossil record shows a very slow appearance of these lifeforms in the Precambrian, with many cyanobacterial species making up much of the underlying sediment.\n\nThe layers of the Doushantuo formation from around \nharbour microscopic fossils that may represent early bilaterians. Some have been described as animal embryos and eggs, although some may represent the remains of giant bacteria.\nAnother fossil, \"Vernanimalcula\", has been interpreted as a coelomate bilaterian,\nbut may simply be an infilled bubble.\n\nThese fossils form the earliest hard-and-fast evidence of animals, as opposed to other predators.\n\nThe traces of organisms moving on and directly underneath the microbial mats that covered the Ediacaran sea floor are preserved from the Ediacaran period, about . They were probably made by organisms resembling earthworms in shape, size, and how they moved. The burrow-makers have never been found preserved, but, because they would need a head and a tail, the burrowers probably had bilateral symmetry – which would in all probability make them bilaterian animals. They fed above the sediment surface, but were forced to burrow to avoid predators.\n\nAround the start of the Cambrian (about ), many new types of traces first appear, including well-known vertical burrows such as \"Diplocraterion\" and \"Skolithos\", and traces normally attributed to arthropods, such as \"Cruziana\" and \"Rusophycus\". The vertical burrows indicate that worm-like animals acquired new behaviours, and possibly new physical capabilities. Some Cambrian trace fossils indicate that their makers possessed hard exoskeletons, although they were not necessarily mineralised.\n\nBurrows provide firm evidence of complex organisms; they are also much more readily preserved than body fossils, to the extent that the absence of trace fossils has been used to imply the genuine absence of large, motile, bottom-dwelling organisms. They provide a further line of evidence to show that the Cambrian explosion represents a real diversification, and is not a preservational artefact.\n\nThis new habit changed the seafloor's geochemistry, and led to decreased oxygen in the ocean and increased CO2-levels in the seas and the atmosphere, resulting in global warming for tens of millions years, and could be responsible for mass extinctions. But as burrowing became established, it allowed an explosion of its own, for as burrowers disturbed the sea floor, they aerated it, mixing oxygen into the toxic muds. This made the bottom sediments more hospitable, and allowed a wider range of organisms to inhabit them – creating new niches and the scope for higher diversity.\n\nAt the start of the Ediacaran period, much of the acritarch fauna, which had remained relatively unchanged for hundreds of millions of years, became extinct, to be replaced with a range of new, larger species, which would prove far more ephemeral. This radiation, the first in the fossil record, is followed soon after by an array of unfamiliar, large, fossils dubbed the Ediacara biota, which flourished for 40 million years until the start of the Cambrian. Most of this \"Ediacara biota\" were at least a few centimeters long, significantly larger than any earlier fossils. The organisms form three distinct assemblages, increasing in size and complexity as time progressed.\n\nMany of these organisms were quite unlike anything that appeared before or since, resembling discs, mud-filled bags, or quilted mattresses – one palæontologist proposed that the strangest organisms should be classified as a separate kingdom, Vendozoa.\n\nAt least some may have been early forms of the phyla at the heart of the \"Cambrian explosion\" debate, having been interpreted as early molluscs (\"Kimberella\"), echinoderms (\"Arkarua\"); and arthropods (\"Spriggina\", \"Parvancorina\"). Still, debate exists about the classification of these specimens, mainly because the diagnostic features that allow taxonomists to classify more recent organisms, such as similarities to living organisms, are generally absent in the ediacarans. However, there seems little doubt that \"Kimberella\" was at least a triploblastic bilaterian animal. These organisms are central to the debate about how abrupt the Cambrian explosion was. If some were early members of the animal phyla seen today, the \"explosion\" looks a lot less sudden than if all these organisms represent an unrelated \"experiment\", and were replaced by the animal kingdom fairly soon thereafter (40M years is \"soon\" by evolutionary and geological standards).\n\nPaul Knauth, a geologist at Arizona State University, maintains that photosynthesizing organisms such as algae may have grown over a 750- to 800-million-year-old formation in Death Valley known as the Beck Spring Dolomite. In the early 1990s, samples from this 1,000-foot thick layer of dolomite revealed that the region housed flourishing mats of photosynthesizing, unicellular life forms which antedated the Cambrian explosion.\n\nMicrofossils have been unearthed from holes riddling the otherwise barren surface of the dolomite. These geochemical and microfossil findings support the idea that during the Precambrian period, complex life evolved both in the oceans and on land. Knauth contends that animals may well have had their origins in freshwater lakes and streams, and not in the oceans.\n\nSome 30 years later, a number of studies have documented an abundance of geochemical and microfossil evidence showing that life covered the continents as far back as 2.2 billion years ago. Many paleobiologists now accept the idea that simple life forms existed on land during the Precambrian, but are opposed to the more radical idea that multicellular life thrived on land more than 600 million years ago.\n\nThe first Ediacaran and lowest Cambrian (Nemakit-Daldynian) skeletal fossils represent tubes and problematic sponge spicules. The oldest sponge spicules are monaxon siliceous, aged around , known from the Doushantou Formation in China and from deposits of the same age in Mongolia, although the interpretation of these fossils as spicules has been challenged. In the late Ediacaran-lowest Cambrian, numerous tube dwellings of enigmatic organisms appeared. It was organic-walled tubes (e.g. \"Saarina\") and chitinous tubes of the sabelliditids (e.g. \"Sokoloviina\", \"Sabellidites\", \"Paleolina\") that prospered up to the beginning of the Tommotian. The mineralized tubes of \"Cloudina\", \"Namacalathus\", \"Sinotubulites\", and a dozen more of the other organisms from carbonate rocks formed near the end of the Ediacaran period from , as well as the triradially symmetrical mineralized tubes of anabaritids (e.g. \"Anabarites\", \"Cambrotubulus\") from uppermost Ediacaran and lower Cambrian. Ediacaran mineralized tubes are often found in carbonates of the stromatolite reefs and thrombolites, i.e. they could live in an environment adverse to the majority of animals.\n\nAlthough they are as hard to classify as most other Ediacaran organisms, they are important in two other ways. First, they are the earliest known calcifying organisms (organisms that built shells from calcium carbonate). Secondly, these tubes are a device to rise over a substrate and competitors for effective feeding and, to a lesser degree, they serve as armor for protection against predators and adverse conditions of environment. Some \"Cloudina\" fossils show small holes in shells. The holes possibly are evidence of boring by predators sufficiently advanced to penetrate shells. A possible \"evolutionary arms race\" between predators and prey is one of the hypotheses that attempt to explain the Cambrian explosion.\n\nIn the lowest Cambrian, the stromatolites were decimated. This allowed animals to begin colonization of warm-water pools with carbonate sedimentation. At first, it was anabaritids and \"Protohertzina\" (the fossilized grasping spines of chaetognaths) fossils. Such mineral skeletons as shells, sclerites, thorns, and plates appeared in uppermost Nemakit-Daldynian; they were the earliest species of halkierids, gastropods, hyoliths and other rare organisms. The beginning of the Tommotian has historically been understood to mark an explosive increase of the number and variety of fossils of molluscs, hyoliths, and sponges, along with a rich complex of skeletal elements of unknown animals, the first archaeocyathids, brachiopods, tommotiids, and others. Also soft-bodied extant phyla such as comb jellies, scalidophorans, entoproctans, horseshoe worms and lobopodians had armored forms. This sudden increase is partially an artefact of missing strata at the Tommotian type section, and most of this fauna in fact began to diversify in a series of pulses through the Nemakit-Daldynian and into the Tommotian.\n\nSome animals may already have had sclerites, thorns, and plates in the Ediacaran (e.g. \"Kimberella\" had hard sclerites, probably of carbonate), but thin carbonate skeletons cannot be fossilized in siliciclastic deposits. Older (~750 Ma) fossils indicate that mineralization long preceded the Cambrian, probably defending small photosynthetic algae from single-celled eukaryotic predators.\n\nTrace fossils (burrows, etc.) are a reliable indicator of what life was around, and indicate a diversification of life around the start of the Cambrian, with the freshwater realm colonized by animals almost as quickly as the oceans.\n\nFossils known as \"small shelly fauna\" have been found in many parts on the world, and date from just before the Cambrian to about 10 million years after the start of the Cambrian (the Nemakit-Daldynian and Tommotian ages; see timeline). These are a very mixed collection of fossils: spines, sclerites (armor plates), tubes, archeocyathids (sponge-like animals), and small shells very like those of brachiopods and snail-like molluscs – but all tiny, mostly 1 to 2 mm long.\n\nWhile small, these fossils are far more common than complete fossils of the organisms that produced them; crucially, they cover the window from the start of the Cambrian to the first lagerstätten: a period of time otherwise lacking in fossils. Hence, they supplement the conventional fossil record and allow the fossil ranges of many groups to be extended.\n\nThe earliest trilobite fossils are about 530 million years old, but the class was already quite diverse and worldwide, suggesting they had been around for quite some time.\nThe fossil record of trilobites began with the appearance of trilobites with mineral exoskeletons – not from the time of their origin.\n\nThe earliest generally accepted echinoderm fossils appeared a little bit later, in the Late Atdabanian; unlike modern echinoderms, these early Cambrian echinoderms were not all radially symmetrical.\n\nThese provide firm data points for the \"end\" of the explosion, or at least indications that the crown groups of modern phyla were represented.\n\nThe Burgess Shale and similar lagerstätten preserve the soft parts of organisms, which provide a wealth of data to aid in the classification of enigmatic fossils. It often preserved complete specimens of organisms only otherwise known from dispersed parts, such as loose scales or isolated mouthparts. Further, the majority of organisms and taxa in these horizons are entirely soft-bodied, hence absent from the rest of the fossil record. Since a large part of the ecosystem is preserved, the ecology of the community can also be tentatively reconstructed.\nHowever, the assemblages may represent a \"museum\": a deep-water ecosystem that is evolutionarily \"behind\" the rapidly diversifying fauna of shallower waters.\n\nBecause the lagerstätten provide a mode and quality of preservation that is virtually absent outside of the Cambrian, many organisms appear completely different from anything known from the conventional fossil record. This led early workers in the field to attempt to shoehorn the organisms into extant phyla; the shortcomings of this approach led later workers to erect a multitude of new phyla to accommodate all the oddballs. It has since been realised that most oddballs diverged from lineages before they established the phyla known today – slightly different designs, which were fated to perish rather than flourish into phyla, as their cousin lineages did.\n\nThe preservational mode is rare in the preceding Ediacaran period, but those assemblages known show no trace of animal life – perhaps implying a genuine absence of macroscopic metazoans.\n\nCrustaceans, one of the four great modern groups of arthropods, are very rare throughout the Cambrian. Convincing crustaceans were once thought to be common in Burgess Shale-type biotas, but none of these individuals can be shown to fall into the crown group of \"true crustaceans\". The Cambrian record of crown-group crustaceans comes from microfossils. The Swedish Orsten horizons contain later Cambrian crustaceans, but only organisms smaller than 2 mm are preserved. This restricts the data set to juveniles and miniaturised adults.\n\nA more informative data source is the organic microfossils of the Mount Cap formation, Mackenzie Mountains, Canada. This late Early Cambrian assemblage () consists of microscopic fragments of arthropods' cuticle, which is left behind when the rock is dissolved with hydrofluoric acid. The diversity of this assemblage is similar to that of modern crustacean faunas. Analysis of fragments of feeding machinery found in the formation shows that it was adapted to feed in a very precise and refined fashion. This contrasts with most other early Cambrian arthropods, which fed messily by shovelling anything they could get their feeding appendages on into their mouths. This sophisticated and specialised feeding machinery belonged to a large (about 30 cm) organism, and would have provided great potential for diversification; specialised feeding apparatus allows a number of different approaches to feeding and development, and creates a number of different approaches to avoid being eaten.\n\nAfter an extinction at the Cambrian–Ordovician boundary, another radiation occurred, which established the taxa that would dominate the Palaeozoic.\n\nDuring this radiation, the total number of orders doubled, and families tripled, increasing marine diversity to levels typical of the Palaeozoic, and disparity to levels approximately equivalent to today's.\n\nThe event lasted for about the next 20–25 million years. Different authors break the explosion down into stages in different ways.\n\nEd Landing recognizes three stages: Stage 1, spanning the Ediacaran-Cambrian boundary, corresponds to a diversification of biomineralizing animals and of deep and complex burrows; Stage 2, corresponding to the radiation of molluscs and stem-group Brachiopods (hyoliths and tommotiids), which apparently arose in intertidal waters; and Stage 3, seeing the Atdabanian diversification of trilobites in deeper waters, but little change in the intertidal realm.\n\nGraham Budd synthesises various schemes to produce a compatible view of the SSF record of the Cambrian explosion, divided slightly differently into four intervals: a \"Tube world\", lasting from , spanning the Ediacaran-Cambrian boundary, dominated by Cloudina, Namacalathus and pseudoconodont-type elements; a \"Sclerite world\", seeing the rise of halkieriids, tommotiids, and hyoliths, lasting to the end of the Fortunian (c. 525 Ma); a brachiopod world, perhaps corresponding to the as yet unratified Cambrian Stage 2; and Trilobite World, kicking off in Stage 3.\n\nComplementary to the shelly fossil record, trace fossils can be divided into five subdivisions: \"Flat world\" (late Ediacaran), with traces restricted to the sediment surface; Protreozoic III (after Jensen), with increasing complexity; \"pedum\" world, initiated at the base of the Cambrian with the base of the \"T.pedum\" zone (see discussion at Cambrian#Dating the Cambrian); \"Rusophycus\" world, spanning and thus corresponding exactly to the periods of Sclerite World and Brachiopod World under the SSF paradigm; and \"Cruziana\" world, with an obvious correspondence to Trilobite World.\n\nThere is strong evidence for species of Cnidaria and Porifera existing in the Ediacaran and possible members of Porifera even before that during the Cryogenian. Bryozoans don't appear in the fossil record until after the Cambrian, in the Lower Ordovician.\n\nThe fossil record as Darwin knew it seemed to suggest that the major metazoan groups appeared in a few million years of the early to mid-Cambrian, and even in the 1980s, this still appeared to be the case.\n\nHowever, evidence of Precambrian Metazoa is gradually accumulating. If the Ediacaran \"Kimberella\" was a mollusc-like protostome (one of the two main groups of coelomates), the protostome and deuterostome lineages must have split significantly before (deuterostomes are the other main group of coelomates). Even if it is not a protostome, it is widely accepted as a bilaterian. Since fossils of rather modern-looking cnidarians (jellyfish-like organisms) have been found in the Doushantuo lagerstätte, the cnidarian and bilaterian lineages must have diverged well over .\n\nTrace fossils and predatory borings in \"Cloudina\" shells provide further evidence of Ediacaran animals. Some fossils from the Doushantuo formation have been interpreted as embryos and one (\"Vernanimalcula\") as a bilaterian coelomate, although these interpretations are not universally accepted. Earlier still, predatory pressure has acted on stromatolites and acritarchs since around .\n\nSome say that the evolutionary change was accelerated by an order of magnitude, but the presence of Precambrian animals somewhat dampens the \"bang\" of the explosion; not only was the appearance of animals gradual, but their evolutionary radiation (\"diversification\") may also not have been as rapid as once thought. Indeed, statistical analysis shows that the Cambrian explosion was no faster than any of the other radiations in animals' history. However, it does seem that some innovations linked to the explosion – such as resistant armour – only evolved once in the animal lineage; this makes a lengthy Precambrian animal lineage harder to defend. Further, the conventional view that all the phyla arose in the Cambrian is flawed; while the phyla may have diversified in this time period, representatives of the crown groups of many phyla do not appear until much later in the Phanerozoic. Further, the mineralised phyla that form the basis of the fossil record may not be representative of other phyla, since most mineralised phyla originated in a benthic setting. The fossil record is consistent with a Cambrian explosion that was limited to the benthos, with pelagic phyla evolving much later.\n\nEcological complexity among marine animals increased in the Cambrian, as well later in the Ordovician. However, recent research has overthrown the once-popular idea that disparity was exceptionally high throughout the Cambrian, before subsequently decreasing. In fact, disparity remains relatively low throughout the Cambrian, with modern levels of disparity only attained after the early Ordovician radiation.\n\nThe diversity of many Cambrian assemblages is similar to today's, and at a high (class/phylum) level, diversity is thought by some to have risen relatively smoothly through the Cambrian, stabilizing somewhat in the Ordovician. This interpretation, however, glosses over the astonishing and fundamental pattern of basal polytomy and phylogenetic telescoping at or near the Cambrian boundary, as seen in most major animal lineages. Thus Harry Blackmore Whittington's questions regarding the abrupt nature of the Cambrian explosion remain, and have yet to be satisfactorily answered.\n\nBudd and Mann suggested that the Cambrian explosion was the result of a type of survivorship bias called the \"Push of the past\". As groups at their origin tend to go extinct, it follows that any long-lived group would have experienced an unusually rapid rate of diversification early on, creating the illusion of a general speed-up in diversification rates. However, rates of diversification could remain at background levels and still generate this sort of effect in the surviving lineages.\n\nDespite the evidence that moderately complex animals (triploblastic bilaterians) existed before and possibly long before the start of the Cambrian, it seems that the pace of evolution was exceptionally fast in the early Cambrian. Possible explanations for this fall into three broad categories: environmental, developmental, and ecological changes. Any explanation must explain both the timing and magnitude of the explosion.\n\nEarth's earliest atmosphere contained no free oxygen (O); the oxygen that animals breathe today, both in the air and dissolved in water, is the product of billions of years of photosynthesis. Cyanobacteria were the first organisms to evolve the ability to photosynthesize, introducing a steady supply of oxygen into the environment. Initially, oxygen levels did not increase substantially in the atmosphere. The oxygen quickly reacted with iron and other minerals in the surrounding rock and ocean water. Once a saturation point was reached for the reactions in rock and water, oxygen was able to exist as a gas in its diatomic form. Oxygen levels in the atmosphere increased substantially afterward. As a general trend, the concentration of oxygen in the atmosphere has risen gradually over about the last 2.5 billion years.\n\nOxygen levels seem to have a positive correlation with diversity in eukaryotes well before the Cambrian period. The last common ancestor of all extant eukaryotes is thought to have lived around 1.8 billion years ago. Around 800 million years ago, there was a notable increase in the complexity and number of eukaryotes species in the fossil record. Before the spike in diversity, eukaryotes are thought to have lived in highly sulfuric environments. Sulfide interferes with mitochondrial function in aerobic organisms, limiting the amount of oxygen that could be used to drive metabolism. Oceanic sulfide levels decreased around 800 million years ago, which supports the importance of oxygen in eukaryotic diversity.\n\nThe shortage of oxygen might well have prevented the rise of large, complex animals. The amount of oxygen an animal can absorb is largely determined by the area of its oxygen-absorbing surfaces (lungs and gills in the most complex animals; the skin in less complex ones); but, the amount needed is determined by its volume, which grows faster than the oxygen-absorbing area if an animal's size increases equally in all directions. An increase in the concentration of oxygen in air or water would increase the size to which an organism could grow without its tissues becoming starved of oxygen. However, members of the Ediacara biota reached metres in length tens of millions of years before the Cambrian explosion. Other metabolic functions may have been inhibited by lack of oxygen, for example the construction of tissue such as collagen, required for the construction of complex structures, or to form molecules for the construction of a hard exoskeleton. However, animals are not affected when similar oceanographic conditions occur in the Phanerozoic; there is no convincing correlation between oxygen levels and evolution, so oxygen may have been no more a prerequisite to complex life than liquid water or primary productivity.\n\nThe amount of ozone (O) required to shield Earth from biologically lethal UV radiation, wavelengths from 200 to 300 nanometers (nm), is believed to have been in existence around the Cambrian explosion. The presence of the ozone layer may have enabled the development of complex life and life on land, as opposed to life being restricted to the water.\n\nIn the late Neoproterozoic (extending into the early Ediacaran period), the Earth suffered massive glaciations in which most of its surface was covered by ice. This may have caused a mass extinction, creating a genetic bottleneck; the resulting diversification may have given rise to the Ediacara biota, which appears soon after the last \"Snowball Earth\" episode.\nHowever, the snowball episodes occurred a long time before the start of the Cambrian, and it is hard to see how so much diversity could have been caused by even a series of bottlenecks; the cold periods may even have \"delayed\" the evolution of large size organisms.\n\nNewer research suggests that volcanically active midocean ridges caused a massive and sudden surge of the calcium concentration in the oceans, making it possible for marine organisms to build skeletons and hard body parts.\nAlternatively a high influx of ions could have been provided by the widespread erosion that produced Powell's Great Unconformity.\n\nAn increase of calcium may also have been caused by erosion of the Transgondwanan Supermountain that existed at the time the explosion. The roots of the mountain are preserved in present-day East Africa as an orogen.\n\nA range of theories are based on the concept that minor modifications to animals' development as they grow from embryo to adult may have been able to cause very large changes in the final adult form. The Hox genes, for example, control which organs individual regions of an embryo will develop into. For instance, if a certain \"Hox\" gene is expressed, a region will develop into a limb; if a different Hox gene is expressed in that region (a minor change), it could develop into an eye instead (a phenotypically major change).\n\nSuch a system allows a large range of disparity to appear from a limited set of genes, but such theories linking this with the explosion struggle to explain why the origin of such a development system should by itself lead to increased diversity or disparity. Evidence of Precambrian metazoans combines with molecular data to show that much of the genetic architecture that could feasibly have played a role in the explosion was already well established by the Cambrian.\n\nThis apparent paradox is addressed in a theory that focuses on the physics of development. It is proposed that the emergence of simple multicellular forms provided a changed context and spatial scale in which novel physical processes and effects were mobilized by the products of genes that had previously evolved to serve unicellular functions. Morphological complexity (layers, segments, lumens, appendages) arose, in this view, by self-organization.\n\nHorizontal gene transfer has also been identified as a possible factor in the rapid acquisition of the biochemical capability of biomineralization among organisms during this period, based on evidence that the gene for a critical protein in the process was originally transferred from a bacterium into sponges.\n\nThese focus on the interactions between different types of organism. Some of these hypotheses deal with changes in the food chain; some suggest arms races between predators and prey, and others focus on the more general mechanisms of coevolution. Such theories are well suited to explaining why there was a rapid increase in both disparity and diversity, but they must explain why the \"explosion\" happened when it did.\n\nEvidence for such an extinction includes the disappearance from the fossil record of the Ediacara biota and shelly fossils such as \"Cloudina\", and the accompanying perturbation in the record.\n\nMass extinctions are often followed by adaptive radiations as existing clades expand to occupy the ecospace emptied by the extinction. However, once the dust had settled, overall disparity and diversity returned to the pre-extinction level in each of the Phanerozoic extinctions.\n\nAndrew Parker has proposed that predator-prey relationships changed dramatically after eyesight evolved. Prior to that time, hunting and evading were both close-range affairs – smell, vibration, and touch were the only senses used. When predators could see their prey from a distance, new defensive strategies were needed. Armor, spines, and similar defenses may also have evolved in response to vision. He further observed that, where animals lose vision in unlighted environments such as caves, diversity of animal forms tends to decrease. Nevertheless, many scientists doubt that vision could have caused the explosion. Eyes may well have evolved long before the start of the Cambrian. It is also difficult to understand why the evolution of eyesight would have caused an explosion, since other senses, such as smell and pressure detection, can detect things at a greater distance in the sea than sight can; but the appearance of these other senses apparently did not cause an evolutionary explosion.\n\nThe ability to avoid or recover from predation often makes the difference between life and death, and is therefore one of the strongest components of natural selection. The pressure to adapt is stronger on the prey than on the predator: if the predator fails to win a contest, it loses a meal; if the prey is the loser, it loses its life.\n\nBut, there is evidence that predation was rife long before the start of the Cambrian, for example in the increasingly spiny forms of acritarchs, the holes drilled in \"Cloudina\" shells, and traces of burrowing to avoid predators. Hence, it is unlikely that the \"appearance\" of predation was the trigger for the Cambrian \"explosion\", although it may well have exhibited a strong influence on the body forms that the \"explosion\" produced. However, the intensity of predation does appear to have increased dramatically during the Cambrian as new predatory \"tactics\" (such as shell-crushing) emerged. This rise of predation during the Cambrian was confirmed by the temporal pattern of the median predator ratio at the scale of genus, in fossil communities covering the Cambrian and Ordovician periods, but this pattern is not correlated to diversification rate. This lack of correlation between predator ratio and diversification over the Cambrian and Ordovician suggests that predators did not trigger the large evolutionary radiation of animals during this interval. Thus the role of predators as triggerers of diversification may have been limited to the very beginning of the \"Cambrian explosion\".\n\nGeochemical evidence strongly indicates that the total mass of plankton has been similar to modern levels since early in the Proterozoic. Before the start of the Cambrian, their corpses and droppings were too small to fall quickly towards the seabed, since their drag was about the same as their weight. This meant they were destroyed by scavengers or by chemical processes before they reached the sea floor.\n\nMesozooplankton are plankton of a larger size. Early Cambrian specimens filtered microscopic plankton from the seawater. These larger organisms would have produced droppings and corpses that were large enough to fall fairly quickly. This provided a new supply of energy and nutrients to the mid-levels and bottoms of the seas, which opened up a huge range of new possible ways of life. If any of these remains sank uneaten to the sea floor they could be buried; this would have taken some carbon out of circulation, resulting in an increase in the concentration of breathable oxygen in the seas (carbon readily combines with oxygen).\n\nThe initial herbivorous mesozooplankton were probably larvae of benthic (seafloor) animals. A larval stage was probably an evolutionary innovation driven by the increasing level of predation at the seafloor during the Ediacaran period.\n\nMetazoans have an amazing ability to increase diversity through coevolution. This means that an organism's traits can lead to traits evolving in other organisms; a number of responses are possible, and a different species can potentially emerge from each one. As a simple example, the evolution of predation may have caused one organism to develop a defence, while another developed motion to flee. This would cause the predator lineage to split into two species: one that was good at chasing prey, and another that was good at breaking through defences. Actual coevolution is somewhat more subtle, but, in this fashion, great diversity can arise: three quarters of living species are animals, and most of the rest have formed by coevolution with animals.\n\nEvolving organisms inevitably change the environment they evolve in. The Devonian colonization of land had planet-wide consequences for sediment cycling and ocean nutrients, and was likely linked to the Devonian mass extinction. A similar process may have occurred on smaller scales in the oceans, with, for example, the sponges filtering particles from the water and depositing them in the mud in a more digestible form; or burrowing organisms making previously unavailable resources available for other organisms.\n\nThe explosion may not have been a significant evolutionary event. It may represent a threshold being crossed: for example a threshold in genetic complexity that allowed a vast range of morphological forms to be employed. This genetic threshold may have a correlation to the amount of oxygen available to organisms. Using oxygen for metabolism produces much more energy than anaerobic processes. Organisms that use more oxygen have the opportunity to produce more complex proteins, providing a template for further evolution. These proteins translate into larger, more complex structures that allow organisms better to adapt to their environments. With the help of oxygen, genes that code for these proteins could contribute to the expression of complex traits more efficiently. Access to a wider range of structures and functions would allow organisms to evolve in different directions, increasing the number of niches that could be inhabited. Furthermore, organisms had the opportunity to become more specialized in their own niches.\n\nThe \"Cambrian explosion\" can be viewed as two waves of metazoan expansion into empty niches: first, a coevolutionary rise in diversity as animals explored niches on the Ediacaran sea floor, followed by a second expansion in the early Cambrian as they became established in the water column. The rate of diversification seen in the Cambrian phase of the explosion is unparalleled among marine animals: it affected all metazoan clades of which Cambrian fossils have been found. Later radiations, such as those of fish in the Silurian and Devonian periods, involved fewer taxa, mainly with very similar body plans. Although the recovery from the Permian-Triassic extinction started with about as few animal species as the Cambrian explosion, the recovery produced far fewer significantly new types of animals.\n\nWhatever triggered the early Cambrian diversification opened up an exceptionally wide range of previously unavailable ecological niches. When these were all occupied, limited space existed for such wide-ranging diversifications to occur again, because strong competition existed in all niches and incumbents usually had the advantage. If a wide range of empty niches had continued, clades would be able to continue diversifying and become disparate enough for us to recognise them as different phyla; when niches are filled, lineages will continue to resemble one another long after they diverge, as limited opportunity exists for them to change their life-styles and forms.\n\nThere were two similar explosions in the evolution of land plants: after a cryptic history beginning about , land plants underwent a uniquely rapid adaptive radiation during the Devonian period, about . Furthermore, Angiosperms (flowering plants) originated and rapidly diversified during the Cretaceous period.\n\n\nTimeline References:\n\n"}
{"id": "5129726", "url": "https://en.wikipedia.org/wiki?curid=5129726", "title": "Casimir pressure", "text": "Casimir pressure\n\nCasimir pressure is created by the Casimir force of virtual particles.\n\nAccording to experiments, the Casimir force formula_1 between two closely spaced neutral parallel plate conductors is directly proportional to their surface area formula_2:\n\nformula_3\n\nTherefore, dividing the magnitude of Casimir force by the area of each conductor, Casimir pressure formula_4 can be found. Because the Casimir force between conductors is attractive, the Casimir pressure in space between the conductors is negative.\n\nBecause virtual particles are physical representations of the zero point energy of physical vacuum, the Casimir pressure is the difference in the density of the zero point energy of empty space inside and outside of cavity made by conductive plates.\n\nSome scientists believe that zero point energy is the dominant energy of the Universe and that the Casimir pressure of this energy is the main cause of the observed accelerated expansion of the Universe. In other words, virtual particles drive the accelerated expansion of the Universe.\n\n"}
{"id": "2533237", "url": "https://en.wikipedia.org/wiki?curid=2533237", "title": "Catalytic triad", "text": "Catalytic triad\n\nA catalytic triad is a set of three coordinated amino acids that can be found in the active site of some enzymes. Catalytic triads are most commonly found in hydrolase and transferase enzymes (e.g. proteases, amidases, esterases, acylases, lipases and β-lactamases). An Acid-Base-Nucleophile triad is a common motif for generating a nucleophilic residue for covalent catalysis. The residues form a charge-relay network to polarise and activate the nucleophile, which attacks the substrate, forming a covalent intermediate which is then hydrolysed to release the product and regenerate free enzyme. The nucleophile is most commonly a serine or cysteine amino acid, but occasionally threonine or even selenocysteine. The 3D structure of the enzyme brings together the triad residues in a precise orientation, even though they may be far apart in the sequence (primary structure).\n\nAs well as divergent evolution of function (and even the triad's nucleophile), catalytic triads show some of the best examples of convergent evolution. Chemical constraints on catalysis have led to the same catalytic solution independently evolving in at least 23 separate superfamilies. Their mechanism of action is consequently one of the best studied in biochemistry.\n\nThe enzymes trypsin and chymotrypsin were first purified in the 1930s. A serine in each of trypsin and chymotrypsin was identified as the catalytic nucleophile (by diisopropyl fluorophosphate modification) in the 1950s. The structure of chymotrypsin was solved by X-ray crystallography in the 1960s, showing the orientation of the catalytic triad in the active site. Other proteases were sequenced and aligned to reveal a family of related proteases, now called the S1 family. Simultaneously, the structures of the evolutionarily unrelated papain and subtilisin proteases were found to contain analogous triads. The 'charge-relay' mechanism for the activation of the nucleophile by the other triad members was proposed in the late 1960s. As more protease structures were solved by X-ray crystallography in the 1970s and 80s, homologous (such as TEV protease) and analogous (such as papain) triads were found. The MEROPS classification system in the 1990s and 2000s began classing proteases into structurally related enzyme superfamilies and so acts as a database of the convergent evolution of triads in over 20 superfamilies. Understanding how chemical constraints on evolution led to the convergence of so many enzyme families on the same triad geometries has developed in the 2010s.\n\nSince their initial discovery, there have been increasingly detailed investigations of their exact catalytic mechanism. Of particular contention in the 1990s and 2000s was whether low-barrier hydrogen bonding contributed to catalysis, or whether ordinary hydrogen bonding is sufficient to explain the mechanism. The massive body of work on the charge-relay, covalent catalysis used by catalytic triads has led to the mechanism being the best characterised in all of biochemistry.\n\nEnzymes that contain a catalytic triad use it for one of two reaction types: either to split a substrate (hydrolases) or to transfer one portion of a substrate over to a second substrate (transferases). Triads are an inter-dependent set of residues in the active site of an enzyme and act in concert with other residues (e.g. binding site and oxyanion hole) to achieve nucleophilic catalysis. These triad residues act together to make the nucleophile member highly reactive, generating a covalent intermediate with the substrate that is then resolved to complete catalysis.\n\nCatalytic triads perform covalent catalysis using a residue as a nucleophile. The reactivity of the nucleophilic residue is increased by the functional groups of the other triad members. The nucleophile is polarised and oriented by the base, which is itself bound and stabilised by the acid.\n\nCatalysis is performed in two stages. First, the activated nucleophile attacks the carbonyl carbon and forces the carbonyl oxygen to accept an electron, leading to a tetrahedral intermediate. The build-up of negative charge on this intermediate is typically stabilized by an oxyanion hole within the active site. The intermediate then collapses back to a carbonyl, ejecting the first half of the substrate, but leaving the second half still covalently bound to the enzyme as an acyl-enzyme intermediate. The ejection of this first leaving group is often aided by donation of a proton by the base.\n\nThe second stage of catalysis is the resolution of the acyl-enzyme intermediate by the attack of a second substrate. If this substrate is water then the result is hydrolysis; if it is an organic molecule then the result is transfer of that molecule onto the first substrate. Attack by this second substrate forms a new tetrahedral intermediate, which resolves by ejecting the enzyme's nucleophile, releasing the second product and regenerating free enzyme.\n\nThe side-chain of the nucleophilic residue performs covalent catalysis on the substrate. The lone pair of electrons present on the oxygen or sulphur attacks the electropositive carbonyl carbon. The 20 naturally occurring biological amino acids do not contain any sufficiently nucleophilic functional groups for many difficult catalytic reactions. Embedding the nucleophile in a triad increases its reactivity for efficient catalysis. The most commonly used nucleophiles are the hydroxyl (OH) of serine and the thiol/thiolate ion (SH/S) of cysteine. Alternatively, threonine proteases use the secondary hydroxyl of threonine, however due to steric hindrance of the side chain's extra methyl group such proteases use their \"N\"-terminal amide as the base, rather than a separate amino acid.\n\nUse of oxygen or sulphur as the nucleophilic atom causes minor differences in catalysis. Compared to oxygen, sulphur’s extra d orbital makes it larger (by 0.4 Å) and softer, allows it to form longer bonds (d and d by 1.3-fold), and gives it a lower p\"K\" (by 5 units). Serine is therefore more dependent than cysteine on optimal orientation of the acid-base triad members to reduce its p\"K\" in order to achieve concerted deprotonation with catalysis. The low p\"K\" of cysteine works to its disadvantage in the resolution of the first tetrahedral intermediate as unproductive reversal of the original nucleophilic attack is the more favourable breakdown product. The triad base is therefore preferentially oriented to protonate the leaving group amide to ensure that it is ejected to leave the enzyme sulphur covalently bound to the substrate N-terminus. Finally, resolution of the acyl-enzyme (to release the substrate C-terminus) requires serine to be re-protonated whereas cysteine can leave as S. Sterically, the sulphur of cysteine also forms longer bonds and has a bulkier van der Waals radius and if mutated to serine can be trapped in unproductive orientations in the active site.\n\nVery rarely, the selenium atom of the uncommon amino acid selenocysteine is used as a nucleophile. The deprotonated Se state is strongly favoured when in a catalytic triad.\n\nSince no natural amino acids are strongly nucleophilic, the base in a catalytic triad polarises and deprotonates the nucleophile to increase its reactivity. Additionally, it protonates the first product to aid leaving group departure.\n\nThe base is most commonly histidine since its p\"K\" allows for effective base catalysis, hydrogen bonding to the acid residue, and deprotonation of the nucleophile residue. β-lactamases such as TEM-1 use a lysine residue as the base. Because lysine's p\"K\" is so high (p\"K\"=11), a glutamate and several other residues act as the acid to stabilise its deprotonated state during the catalytic cycle. Threonine proteases use their \"N\"-terminal amide as the base, since steric crowding by the catalytic threonine's methyl prevents other residues from being close enough.\n\nThe acidic triad member forms a hydrogen bond with the basic residue. This aligns the basic residue by restricting its side-chain rotation, and polarises it by stabilising its positive charge. Two amino acids have acidic side chains at physiological pH (aspartate or glutamate) and so are the most commonly used for this triad member. Cytomegalovirus protease uses a pair of histidines, one as the base, as usual, and one as the acid. The second histidine is not as effective an acid as the more common aspartate or glutamate, leading to a lower catalytic efficiency. In some enzymes, the acid member of the triad is less necessary and some act only as a dyad. For example, papain uses asparagine as its third triad member which orients the histidine base but does not act as an acid. Similarly, hepatitis A virus protease contains an ordered water in the position where an acid residue should be.\n\nThe Serine-Histidine-Aspartate motif is one of the most thoroughly characterised catalytic motifs in biochemistry. The triad is exemplified by chymotrypsin, a model serine protease from the PA superfamily which uses its triad to hydrolyse protein backbones. The aspartate is hydrogen bonded to the histidine, increasing the p\"K\" of its imidazole nitrogen from 7 to around 12. This allows the histidine to act as a powerful general base and to activate the serine nucleophile. It also has an oxyanion hole consisting of several backbone amides which stabilises charge build-up on intermediates. The histidine base aids the first leaving group by donating a proton, and also activates the hydrolytic water substrate by abstracting a proton as the remaining OH attacks the acyl-enzyme intermediate.\n\nThe same triad has also convergently evolved in α/β hydrolases such as some lipases and esterases, however orientation of the triad members is reversed. Additionally, brain acetyl hydrolase (which has the same fold as a small G-protein) has also been found to have this triad. The equivalent Ser-His-\"Glu \"triad is used in acetylcholinesterase.\n\nThe second most studied triad is the Cysteine-Histidine-Aspartate motif. Several families of cysteine proteases use this triad set, for example TEV protease and papain. The triad acts similarly to serine protease triads, with a few notable differences. Due to cysteine's low p\"K\", the importance of the Asp to catalysis varies and several cysteine proteases are effectively Cys-His dyads (e.g. hepatitis A virus protease), whilst in others the cysteine is already deprotonated before catalysis begins (e.g. papain). This triad is also used by some amidases, such as \"N\"-glycanase to hydrolyse non-peptide C-N bonds.\n\nThe triad of cytomegalovirus protease uses histidine as both the acid and base triad members. Removing the acid histidine results in only a 10-fold activity loss (compared to >10,000-fold when aspartate is removed from chymotrypsin). This triad has been interpreted as a possible way of generating a less active enzyme to control cleavage rate.\n\nAn unusual triad is found in seldolisin proteases. The low p\"K\" of the glutamate carboxylate group means that it only acts as a base in the triad at very low pH. The triad is hypothesised to be an adaptation to specific environments like acidic hot springs (e.g. kumamolysin) or cell lysosome (e.g. tripeptidyl peptidase).\n\nThe endothelial protease vasohibin uses a cysteine as the nucleophile, but a serine to coordinate the histidine base. Despite the serine being a poor acid, it is still effective in orienting the histidine in the catalytic triad. Some homologues alternatively have a threonine instead of serine at the acid location.\n\nThreonine proteases, such as the proteasome protease subunit and ornithine acyltransferases use the secondary hydroxyl of threonine in a manner analogous to the use of the serine primary hydroxyl. However, due to the steric interference of the extra methyl group of threonine, the base member of the triad is the \"N\"-terminal amide which polarises an ordered water which, in turn, deprotonates the catalytic hydroxyl to increase its reactivity. Similarly, there exist equivalent 'serine only' and 'cysteine only' configurations such as penicillin acylase G and penicillin acylase V which are evolutionarily related to the proteasome proteases. Again, these use their \"N\"-terminal amide as a base.\n\nThis unusual triad occurs only in one superfamily of amidases. In this case, the lysine acts to polarise the middle serine. The middle serine then forms two strong hydrogen bonds to the nucleophilic serine to activate it (one with the side chain hydroxyl and the other with the backbone amide). The middle serine is held in an unusual \"cis\" orientation to facilitate precise contacts with the other two triad residues. The triad is further unusual in that the lysine and \"cis\"-serine both act as the base in activating the catalytic serine, but the same lysine also performs the role of the acid member as well as making key structural contacts.\n\nThe rare, but naturally occurring amino acid selenocysteine (Sec), can also be found as the nucleophile in some catalytic triads. Selenocysteine is similar to cysteine, but contains a selenium atom instead of a sulphur. An example is in the active site of thioredoxin reductase, which uses the selenium for reduction of disulphide in thioredoxin.\n\nIn addition to naturally occurring types of catalytic triads, protein engineering has been used to create enzyme variants with non-native amino acids, or entirely synthetic amino acids. Catalytic triads have also been inserted into otherwise non-catalytic proteins, or protein mimics.\n\nSubtilisin (a serine protease) has had its oxygen nucleophile replaced with each of sulphur, selenium, or tellurium. Cysteine and selenocysteine were inserted by mutagenesis, whereas the non-natural amino acid, tellurocysteine, was inserted using auxotrophic cells fed with synthetic tellurocysteine. These elements are all in the 16th periodic table column (chalcogens), so have similar properties. In each case, changing the nucleophile reduced the enzyme's protease activity, but increased a different activity. A sulphur nucleophile improved the enzymes transferase activity (sometimes called subtiligase). Selenium and tellurium nucleophiles converted the enzyme into a oxidoreductase. When the nucleophile of TEV protease was converted from cysteine to serine, it protease activity was strongly reduced, but was able to be restored by directed evolution.\n\nNon-catalytic proteins have been used as scaffolds, having catalytic triads inserted into them which were then improved by directed evolution. The Ser-His-Asp triad has been inserted into an antibody, as well as a range of other proteins. Similarly, catalytic triad mimics have been created in small organic molecules like diaryl diselenide, and displayed on larger polymers like Merrifield resins, and self-assembling short peptide nanostructures.\n\nThe sophistication of the active site network causes residues involved in catalysis (and residues in contact with these) to be highly evolutionarily conserved. However, there are examples of divergent evolution in catalytic triads, both in the reaction catalysed, and the residues used in catalysis. The triad remains the core of the active site, but it is evolutionarily adapted to serve different functions. Some proteins, called pseudoenzymes, have non-catalytic functions (e.g. regulation by inhibitory binding) and have accumulated mutations that inactivate their catalytic triad.\n\nCatalytic triads perform covalent catalysis via an acyl-enzyme intermediate. If this intermediate is resolved by water, the result is hydrolysis of the substrate. However, if the intermediate is resolved by attack by a second substrate, then the enzyme acts as a transferase. For example, attack by an acyl group results in an acyltransferase reaction. Several families of transferase enzymes have evolved from hydrolases by adaptation to exclude water and favour attack of a second substrate. In different members of the α/β-hydrolase superfamily, the Ser-His-Asp triad is tuned by surrounding residues to perform at least 17 different reactions. Some of these reactions are also achieved with mechanisms that have altered formation, or resolution of the acyl-enzyme intermediate, or that don't proceed via an acyl-enzyme intermediate.\n\nAdditionally, an alternative transferase mechanism has been evolved by amidophosphoribosyltransferases, which has two active sites. In the first active site, a cysteine triad hydrolyses a glutamine substrate to release free ammonia. The ammonia then diffuses though an internal tunnel in the enzyme to the second active site, where it is transferred to a second substrate.\n\nDivergent evolution of active site residues is slow, due to strong chemical constraints. Nevertheless, some protease superfamilies have evolved from one nucleophile to another. This can be inferred when a superfamily (with the same fold) contains families that use different nucleophiles. Such nucleophile switches have occurred several times during evolutionary history, however the mechanisms by which this happen are still unclear.\n\nWithin protease superfamilies that contain a mixture of nucleophiles (e.g. the PA clan), families are designated by their catalytic nucleophile (C=cysteine proteases, S=serine proteases).\nThe enzymology of proteases provides some of the clearest known examples of convergent evolution. The same geometric arrangement of triad residues occurs in over 20 separate enzyme superfamilies. Each of these superfamilies is the result of convergent evolution for the same triad arrangement within a different structural fold. This is because there are limited productive ways to arrange three triad residues, the enzyme backbone and the substrate. These examples reflect the intrinsic chemical and physical constraints on enzymes, leading evolution to repeatedly and independently converge on equivalent solutions.\n\nThe same triad geometries been converged upon by serine proteases such as the chymotrypsin and subtilisin superfamilies. Similar convergent evolution has occurred with cysteine proteases such as viral C3 protease and papain superfamilies. These triads have converged to almost the same arrangement due to the mechanistic similarities in cysteine and serine proteolysis mechanisms.\n\nFamilies of Cysteine proteases\nFamilies of Serine proteases\nThreonine proteases use the amino acid threonine as their catalytic nucleophile. Unlike cysteine and serine, threonine is a secondary hydroxyl (i.e. has a methyl group). This methyl group greatly restricts the possible orientations of triad and substrate as the methyl clashes with either the enzyme backbone or histidine base. When the nucleophile of a serine protease was mutated to threonine, the methyl occupied a mixture of positions, most of which prevented substrate binding. Consequently, the catalytic residue of a threonine protease is located at it \"N\"-terminus.\n\nTwo evolutionarily independent enzyme superfamilies with different protein folds are known to use the \"N\"-terminal residue as a nucleophile: Superfamily PB (proteasomes using the Ntn fold) and Superfamily PE (acetyltransferases using the DOM fold) This commonality of active site structure in completely different protein folds indicates that the active site evolved convergently in those superfamilies.\n\nFamilies of threonine proteases\n"}
{"id": "1846827", "url": "https://en.wikipedia.org/wiki?curid=1846827", "title": "Catastrophe modeling", "text": "Catastrophe modeling\n\nCatastrophe modeling (also known as cat modeling) is the process of using computer-assisted calculations to estimate the losses that could be sustained due to a catastrophic event such as a hurricane or earthquake. Cat modeling is especially applicable to analyzing risks in the insurance industry and is at the confluence of actuarial science, engineering, meteorology, and seismology.\n\nNatural catastrophes (sometimes referred to as \"nat cat\") include: \nHuman catastrophes include: \n\n\nThe input into a typical cat modeling software package is information on the exposures being analyzed that are vulnerable to catastrophe risk. The exposure data can be categorized into three basic groups:\n\n\nThe output is an estimate of the losses that the model predicts would be associated with a particular event or set of events. When running a \"probabilistic\" model, the output is either a probabilistic loss distribution or a set of events that could be used to create a loss distribution; probable maximum losses (PMLs) and average annual losses (AALs) are calculated from the loss distribution. When running a \"deterministic\" model, losses caused by a specific event are calculated; for example, Hurricane Katrina or \"a magnitude 8.0 earthquake in downtown San Francisco\" could be analyzed against the portfolio of exposures.\n\n\nRecently, an effort to create and disseminate open multi-hazard cat risk modeling tools was initiated by the Alliance for Global Open Risk Analysis (AGORA).\n\nAlso, Oasis Loss Modelling Framework (LMF) http://www.oasislmf.org has been created. It has been founded as a not for profit organisation funded and owned by the Insurance Industry to promote open access to models and to promote transparency. Open source code for the framework is available at http://github.com/oasislmf .\n\nAdditionally, the insurance industry is currently working with the Association for Cooperative Operations Research and Development (ACORD) to develop an industry standard for collecting and sharing exposure data. To date, the industry has been operating on closed, proprietary data formats.\n\n\n"}
{"id": "36980", "url": "https://en.wikipedia.org/wiki?curid=36980", "title": "Clay", "text": "Clay\n\nClay is a finely-grained natural rock or soil material that combines one or more clay minerals with possible traces of quartz (SiO), metal oxides (AlO , MgO etc.) and organic matter. Geologic clay deposits are mostly composed of phyllosilicate minerals containing variable amounts of water trapped in the mineral structure. Clays are plastic due to particle size and geometry as well as water content, and become hard, brittle and non–plastic upon drying or firing. Depending on the soil's content in which it is found, clay can appear in various colours from white to dull grey or brown to deep orange-red.\n\nAlthough many naturally occurring deposits include both silts and clay, clays are distinguished from other fine-grained soils by differences in size and mineralogy. Silts, which are fine-grained soils that do not include clay minerals, tend to have larger particle sizes than clays. There is, however, some overlap in particle size and other physical properties. The distinction between silt and clay varies by discipline. Geologists and soil scientists usually consider the separation to occur at a particle size of 2 µm (clays being finer than silts), sedimentologists often use 4–5 μm, and colloid chemists use 1 μm. Geotechnical engineers distinguish between silts and clays based on the plasticity properties of the soil, as measured by the soils' Atterberg limits. ISO 14688 grades clay particles as being smaller than 2 μm and silt particles as being larger.\n\nMixtures of sand, silt and less than 40% clay are called loam. Loam makes good soil and is used as a building material.\n\nClay minerals typically form over long periods of time as a result of the gradual chemical weathering of rocks, usually silicate-bearing, by low concentrations of carbonic acid and other diluted solvents. These solvents, usually acidic, migrate through the weathering rock after leaching through upper weathered layers. In addition to the weathering process, some clay minerals are formed through hydrothermal activity. There are two types of clay deposits: primary and secondary. Primary clays form as residual deposits in soil and remain at the site of formation. Secondary clays are clays that have been transported from their original location by water erosion and deposited in a new sedimentary deposit. Clay deposits are typically associated with very low energy depositional environments such as large lakes and marine basins.\n\nDepending on the academic source, there are three or four main groups of clays: kaolinite, montmorillonite-smectite, illite, and chlorite. Chlorites are not always considered to be a clay, sometimes being classified as a separate group within the phyllosilicates. There are approximately 30 different types of \"pure\" clays in these categories, but most \"natural\" clay deposits are mixtures of these different types, along with other weathered minerals.\n\nVarve (or \"varved clay\") is clay with visible annual layers, which are formed by seasonal deposition of those layers and are marked by differences in erosion and organic content. This type of deposit is common in former glacial lakes. When fine sediments are delivered into the calm waters of these glacial lake basins away from the shoreline, they settle to the lake bed. The resulting seasonal layering is preserved in an even distribution of clay sediment banding.\n\nQuick clay is a unique type of marine clay indigenous to the glaciated terrains of Norway, Canada, Northern Ireland, and Sweden. It is a highly sensitive clay, prone to liquefaction, which has been involved in several deadly landslides.\n\nPowder X-ray diffraction can be used to identify clays.\n\nThe physical and reactive chemical properties can be used to help elucidate the composition of clays.\n\nClays exhibit plasticity when mixed with water in certain proportions. However, when dry, clay becomes firm and when fired in a kiln, permanent physical and chemical changes occur. These changes convert the clay into a ceramic material. Because of these properties, clay is used for making pottery, both utilitarian and decorative, and construction products, such as bricks, wall and floor tiles. Different types of clay, when used with different minerals and firing conditions, are used to produce earthenware, stoneware, and porcelain. Prehistoric humans discovered the useful properties of clay. Some of the earliest pottery shards recovered are from central Honshu, Japan. They are associated with the Jōmon culture and deposits they were recovered from have been dated to around 14,000 BC.\n\nClay tablets were the first known writing medium. Scribes wrote by inscribing them with cuneiform script using a blunt reed called a stylus. Purpose-made clay balls were also used as sling ammunition.\n\nClays sintered in fire were the first form of ceramic. Bricks, cooking pots, art objects, dishware, smoking pipes, and even musical instruments such as the ocarina can all be shaped from clay before being fired. Clay is also used in many industrial processes, such as paper making, cement production, and chemical filtering. Until the late 20th century, bentonite clay was widely used as a mold binder in the manufacture of sand castings.\n\nClay, being relatively impermeable to water, is also used where natural seals are needed, such as in the cores of dams, or as a barrier in landfills against toxic seepage (lining the landfill, preferably in combination with geotextiles). (See puddling.)\n\nStudies in the early 21st century have investigated clay's absorption capacities in various applications, such as the removal of heavy metals from waste water and air purification.\n\nTraditional uses of clay as medicine goes back to prehistoric times. An example is Armenian bole, which is used to soothe an upset stomach. Some animals such as parrots and pigs ingest clay for similar reasons. Kaolin clay and attapulgite have been used as anti-diarrheal medicines.\n\nClay as the defining ingredient of loam is one of the oldest building materials on Earth, among other ancient, naturally-occurring geologic materials such as stone and organic materials like wood. Between one-half and two-thirds of the world's population, in both traditional societies as well as developed countries, still live or work in buildings made with clay, often baked into brick, as an essential part of its load-bearing structure. Also a primary ingredient in many natural building techniques, clay is used to create adobe, cob, cordwood, and rammed earth structures and building elements such as wattle and daub, clay plaster, clay render case, clay floors and clay paints and ceramic building material. Clay was used as a mortar in brick chimneys and stone walls where protected from water.\n\n\n\n"}
{"id": "12850901", "url": "https://en.wikipedia.org/wiki?curid=12850901", "title": "Climate risk", "text": "Climate risk\n\nClimate risk means a risk resulting from climate change and affecting natural and human systems and regions.\n\nIn the course of increasing global temperature and extreme weather phenomena\nthe Intergovernmental Panel on Climate Change (IPCC) has been founded by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO) for a better understanding of climate change and meeting concerns of these observations. Its main aim is evaluating climate risks and exploring strategies for the prevention of these risks.\n\nAs per current projections of IPCC the following future effects have to be expected:\n\nWhile affecting all economic sectors, the effect on single continents will differ. Beside these direct physical climate risks there are also indirect derived ones:\n\nDirect risks of climate change are expected especially for branches, which strongly depend on natural resources like agriculture, fishing, forestry, health care, real estate and tourism. For example, storms and flooding damage buildings and infrastructure, whereas hot summers with less precipitation cause crop failure.\nThe governmental endeavours to reduce climate costs have direct effects on economy. For example, the targets regarding emissions within the Kyoto-Protocol shall be realised by implementing emissions trading. By this instrument the value of emissions can be quantified monetarily, approximating the value of avoiding hazardous substances. This value shall be internalized by companies and considered in investment decisions. By considering emission costs the prices for i.e. energy and transport can increase and therefore change consumer demand. The insecurity of legislation leads to indefinite adjournation of projects and investments.\n\nSimilar to the tobacco industry, industries producing excessive greenhouse gases are exposed to the risk of an increasing number of lawsuits, if damages can be traced back to emissions, i.e. for floodings, crop failure, etc.\n\nIf companies do not take measures to reduce climate risks they are competitively disadvantaged. This might lead to increasing production costs caused by obsolete technologies and therefore to decreasing profits.\n\nProduction shortfalls can result from direct or indirect climate risks. I.e. hurricanes damaging oil production facilities can lead to a scarcity of oil and increasing prices. Also the price for energy will rise, because heatwaves cause water scarcity and therefore the supply for cooling water of power plants becomes short.\n\nCompanies who are publicly criticized for their environmental policy or high emission rates, might lose customers, because of negative reputation. This risk is currently subordinate.\n\nBesides climate risks also opportunities can derive from climate change for some branches and innovative companies, i.e. for the automobile and renewable energy sectors. Especially energy-intensive sectors can reduce energy costs by using more efficient technologies, which necessarily have to be developed in near future.\n\n\n\n"}
{"id": "44262036", "url": "https://en.wikipedia.org/wiki?curid=44262036", "title": "Coleridge's theory of life", "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n"}
{"id": "53643626", "url": "https://en.wikipedia.org/wiki?curid=53643626", "title": "Conservation and restoration of insect specimens", "text": "Conservation and restoration of insect specimens\n\nThe conservation and restoration of insect specimens is the process of caring for and preserving insects as a part of a collection. Conservation concerns begin at collection and continue through preparation, storage, examination, documentation, research and treatment when restoration is needed.\n\nInsect collecting can be done in many different ways depending on the kind of insects being collected and from which habitats. Both hobbyists and professional entomologist have found particular ways to collect with minimal damage to their specimens. Following established techniques helps begin the conservation of insect specimens from the beginning by eliminating as much potential damage as possible. It must be done delicately to ensure that neither the collector nor the live insect itself will cause harm to the distinctive features such as wings, legs and antennae that give purpose to the collection. Special collection nets, traps and techniques must be utilized in consideration of how easily breakage can happen. A kill jar is often used to immediately immobilize the insect before it can damage itself.\n\nThe way an insect specimen is prepared is often the first step in their conservation. They have to be carefully prepared with the appropriate methods depending on their size, anatomy, and potentially delicate features to ensure they will not break before they begin their role as a specimen available for study, research and display. Some specimens must be prepared using a dry method and others with liquid to preserve. The choice made will preserve key features necessary for identification and represent the insect's living form as closely as possible.\n\nThe process of pinning insect specimens is a dry method to preserve and display collections and requires special entomological equipment to accomplish effectively. It is used primarily for hard-bodied, medium to large specimens and is beneficial for easier study and color preservation. Flies and butterflies, though they are partially soft-bodied, are also best preserved through pinning because when preserved in fluid their hairs and/or scales will either clot or fall off. Some smaller specimens may still be pinned use minuten pins, which are much thinner, to avoid breakage. Insects are pinned on foam block or specialized pinning blocks that provide support for the limbs while drying and may be moved to another specialized, protected display case after they have dried completely, at which point they will be more brittle. The pin is most often driven through the thorax of the insect just to the right of the mid-line to preserve the appearance of at least one side should any damage occur from pinning. The exception is butterflies, dragonflies and damselflies, which are pinned through the middle of the thorax. Enough of the pin must be left both above and below the specimen to allow for labeling below and handling.\nCarding is used when pinning would destroy too much of the specimen because its size is quite small. A triangular point is cut from acid free card to ensure best conservation practice because it comes in direct contact with the specimen. A pin is then driven through the broad side of the point for mounting. A soluble glue that can be removed with solvents when necessary is used to adhere the right side of the thorax of the specimen to the point opposite the pinned side. The point is sometimes bent to allow the specimen to present in the same position as normally pinned specimens.\n\nA wet specimen is a specimen preserved in fluid, often 70% alcohol. Specimens that would receive this preservation technique are usually soft-bodied, such as caterpillars, larva, and spiders because of their soft abdomens. This is done to minimize shriveling allowing the identifying characteristics to be preserved as true to life as possible. Hard-bodied insects may also be preserved temporarily in alcohol before pinning.\n\nSlides of very small insects are also kept as part of insect collections.\n\nThe American Institute for Conservation (AIC) describes in their Code of Ethics the aspects of conservation to include: preventive conservation, examination, documentation, treatment, research and education. Each of these areas also apply to the conservation and restoration of insect specimens.\n\nInsect collections may suffer multiple types of degradation including fading colors from light exposure, mold growth from improper humidity and temperature levels, and infestations from pests that feed on dried insect, but much of this is avoidable when proper preventive conservation practices are followed.\n\nIn addition to maintaining a clean storage environment for specimens, it is sometimes necessary to clean the specimens themselves. Cleaning incredibly delicate and brittle dry insect specimens is done carefully and methodically. The conservator chooses the appropriate method based on the kind of insect that needs cleaning and how robust it is. Cleaning tools vary widely, but generally clean watercolor brushes are used to gently dust the specimens, sometimes with a stereo microscope for very small specimens, warm water and/or alcohol baths used with or without an ultrasonic cleaner, and lens blowers to gently blow away dust, or dry the specimen after a cleansing bath.\n\nA common way to prevent damage from pests intent to feed on insect specimens and dust accumulation in a collection is to keep them in sealed boxes or cabinets. When properly sealed, they can also aid in preventing damages cause by relative humidity (RH) and temperature fluctuations. Wet specimens are kept in separate vials or jars and in a secure cabinet, tray or shelf. Fluid levels are regularly monitored to ensure specimens are completely immersed in fluid, though a well-sealed jar or vial will prevent excessive evaporation.\n\nProper handling for insect specimens prevents the excessive breaking of legs, antennae and other body parts that could then easily be lost. Curved forceps may be used to allow more precision and less chance of the brittle specimen coming in contact with the handler. The handler picks up the specimen by the pin, which is placed with enough space below the specimen for the handler to put in the pinning block and enough space above to grip without touching the specimen.\nIntegrated pest management (IPM) is a specialized modern pest control used in museums. All IPM systems begin with regular sanitation and monitoring of collections to detect castings from various pests, and checking insect traps laid out to capture and identify which pests are present. Some pests, such as carpet beetles and flour beetles, feed on dried insects. When an infestation is present, treatment may be necessary. Freezing is commonly used to rid insect collections of pests. Alternatively, inert gases may be used for an anoxic fumigation - depriving the pests of oxygen to exterminate, and in extreme cases chemical fumigation proven to be safe for collections and people may be used.\n\nAssessing the condition of an insect collection is done regularly and the results are recorded accordingly. The conservator observes the specimens in high detail remarking all areas of damage, or altered states of the specimen. Tools used during this process may include a strong light source, magnifying glass and handling tools that allow the conservator to pick up the specimen from the pin without touching it. The observations made during the examination process result in the conclusions drawn for a treatment plan if necessary. The conservator is knowledgeable of the kinds of deterioration to look for specific to insect specimens.\n\n\nThe documentation of insect specimens is carried out in many different aspects of a specimen's existence. Documented information begins with the capture of an insect. The collector records information about capture method, place and date of capture and any relevant habitat information in field notes. This information is then transferred to labels and collection records. The documentation path then continues with every recorded observation or treatment the specimen receives. Killing agents, preservation agents, rehydrating agents, and fumigants are all important to record. This then informs any future decisions for conservation actions.\n\nAs a minimum, labels contain the place and date of collection, and the identification of genus and species. On pinned insects, the labels are likewise pinned with the space left under the specimen on the same pin. There are various ways to write the information on labels, but an ink that will not fade or come off in liquid is generally used. The paper is ideally 100% cotton or linen rag to avoid yellowing or embrittlement of the paper as it ages.\nWith improvements in digital photography and web resources, many natural history museums have begun a new kind of documentation through digitization, bringing high quality images and associated information to anyone with access to the Internet. Large databases can hold vast amounts of information improving research efforts.\n\nScientific illustration of insects is an older technique used to preserve information about collected insects. It visually documents insects, and unlike photography, can add intellectual ideas about anatomy and behavior of the insect through artists' renditions. Scientifically informed observation of specimens combined with technical and aesthetic skill yields the highly detailed illustrations necessary for the documentation of each species that is illustrated.\n\nThis area of insect specimen documentation comes after the close examination of specimens by conservators. The conservator records all of the visual information about the specimen that can be gleaned from detailed inspection. Conclusions are drawn from inspection and potential treatments are also documented to inform researchers and future conservators. \nResearching the collections of insects provides important information that develops knowledge about ecology, human health and crops. Well-kept records aid the researcher in identifying whether there are differences in an observed specimen because of damages, treatments or deterioration. Research of the insect collections in museums can lead to new discoveries of species, and provide an important historical resource.\n\nOnce a close observation reveals what issues are present in insect collections, the conservator makes a decision on actions to be taken. It is highly preferable that any treatment applied be reversible or done with little risk to the specimen. For example, broken limbs may be glued back on, which has traditionally been done with white glue. The advantage to white glue being that it is removable in warm water. Another common problem is pest infestation. When dried insect collections have suffered an infestation, the affected specimens can be frozen or sealed with inert gases to kill the pests without harming the specimens. Other treatments might include simply refilling wet specimens' jars with alcohol to ensure the specimens are completely submerged, cleaning specimens of dust and debris, or repositioning specimens for display or research. In the case that a specimen needs to be repositioned, the conservator will \"relax\" the specimen in a jar with a rehydrating substrate to move the limbs without breaking them. The technique used will vary among conservators. Some use a relaxing jar that the specimen is left in for days with the substrate of choice, others may choose to use a warm water bath with a drop of detergent. Whatever treatments are used are diligently documented.\n\nConservation of insect specimens is done in large part to preserve the information for the public. The display of collections in museums and their interpretation offer one avenue that accomplishes this effort. However, websites offer a unique opportunity to disseminate information to a broad audience with layers of information to give general information or to provide depth where desired. These websites are often also provided by museums and their collections. Below is a list of some major educational endeavors with interests in insect specimens.\n\n"}
{"id": "27233680", "url": "https://en.wikipedia.org/wiki?curid=27233680", "title": "Decomposed granite", "text": "Decomposed granite\n\nDecomposed granite is classification of rock that is derived from granite via its weathering to the point that the parent material readily fractures into smaller pieces of weaker rock. Further weathering yields material that easily crumbles into a mixtures of gravel-sized particles known as grus, that in turn may break down to produce a mixture of clay and silica sand or silt particles. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite. It has practical uses that include its incorporation into paving and driveway materials, residential gardening materials in arid environments, as well as various types of walkways and heavy-use paths in parks. Different colors of decomposed granite are available, deriving from the natural range of granite colors from different quarry sources, and admixture of other natural and synthetic materials can extend the range of decomposed granite properties.\n\nDecomposed granite is rock of granitic origin that has weathered to the point that it readily fractures into smaller pieces of weak rock. Further weathering produces rock that easily crumbles into mixtures of gravel-sized particles, sand, and silt-sized particles with some clay. Eventually, the gravel may break down to produce a mixture of silica sand, silt particles, and clay. Different specific granite types have differing propensities to weather, and so differing likelihoods of producing decomposed granite.\n\nThe parent granite material is a common type of igneous rock that is granular, with its grains large enough to be distinguished with the unaided eye (i.e., it is phaneritic in texture); it is composed of plagioclase feldspar, orthoclase feldspar, quartz, mica, and possibly other minerals. The chemical transformation of feldspar, one of the primary constituents of granite, into the clay mineral kaolin is one of the important weathering processes. The presence of clay allows water to seep in and further weaken the rock allowing it to fracture or crumble into smaller particles, where, ultimately, the grains of silica produced from the granite are relatively resistant to weathering, and may remain almost unaltered.\n\nDecomposed granite, as a crushed stone form, is used as a pavement building material. It is used on driveways, garden walkways, bocce courts and pétanque terrains, and urban, regional, and national park walkways and heavy-use paths. DG can be installed and compacted to meet handicapped accessibility specifications and criteria, such as the ADA standards in the U.S. Different colors are available based on the various natural ranges available from different quarry sources, and polymeric stabilizers and other additives can be included to change the properties of the natural material. Decomposed granite is also sometimes used as a component of soil mixtures for cultivating bonsai.\n\n"}
{"id": "48719972", "url": "https://en.wikipedia.org/wiki?curid=48719972", "title": "Diversification rates", "text": "Diversification rates\n\nDiversification rates are the rates at which new species form (the Speciation rate, λ) and living species go extinct (the extinction rate, μ). Diversification rates can be estimated from fossils, data on the species diversity of clades and their ages, or phylogenetic trees. Diversification rates are typically reported on a per-lineage basis (e.g. speciation rate per lineage per unit of time), and refer to the diversification dynamics expected under a birth-death process.\n\nA broad range of studies have demonstrated that diversification rates can vary tremendously both through time and across the tree of life. Current research efforts are focused on predicting diversification rates based on aspects of species or their environment. Diversification rates are also subject to various survivorship biases such as the \"Push of the past\"\n\nDiversification rates can be estimated time-series data on fossil occurrences. With perfect data, this would be an easy task; one could just count the number of speciation and extinction events in a given time interval, and then use these data to calculate per-lineage rates of speciation and extinction per unit time. However, the incomplete nature of the fossil record means that our calculations need to include the possibility that some fossil lineages were not sampled, and that we do not have precise estimates for the times of speciation and extinction of the taxa that are sampled. More sophisticated methods account for the probability of sampling any lineage, which might also depend on some properties of the lineage itself (e.g. whether it has any hard body parts that tend to fossilize) as well as the environment in which it lives.\n\nMany estimates of diversification rates for fossil lineages are for higher-level taxonomic groups like genera or families. Such rates are informative about general patterns and trends of diversification through time and across clades but can be difficult to compare directly to rates of speciation and extinction of individual species.\n\nDiversification rates can be estimated from data on the ages and diversities of monophyletic clades in the tree of life. For example, if a clade is 100 million years old and includes 1000 species, we can estimate the net diversification rate of that clade by using a formula derived from a birth-death model of diversification:\n\nEquations are also available for estimating speciation and extinction rates separately when one has ages and diversities for multiple clades.\n\nDiversification rates can be estimated using the information available in phylogenetic trees. To calculate diversification rates, such phylogenetic trees have to include branch lengths. Various methods are available to estimate speciation and extinction rates from phylogenetic trees using both maximum likelihood and Bayesian statistical approaches. One can also use phylogenetic trees to test for changing rates of speciation and/or extinction, both through time and across clades, and to associate rates of evolution with potential explanatory factors.\n\n"}
{"id": "2262154", "url": "https://en.wikipedia.org/wiki?curid=2262154", "title": "Doppler broadening", "text": "Doppler broadening\n\nIn atomic physics, Doppler broadening is the broadening of spectral lines due to the Doppler effect caused by a distribution of velocities of atoms or molecules. Different velocities of the emitting particles result in different Doppler shifts, the cumulative effect of which is the line broadening.\nThis resulting line profile is known as a Doppler profile. A particular case is the thermal Doppler broadening due to the thermal motion of the particles. Then, the broadening depends only on the frequency of the spectral line, the mass of the emitting particles, and their temperature, and therefore can be used for inferring the temperature of an emitting body.\n\nSaturated absorption spectroscopy, also known as Doppler-free spectroscopy, can be used to find the true frequency of an atomic transition without cooling a sample down to temperatures at which the Doppler broadening is minimal.\n\nWhen thermal motion causes a particle to move towards the observer, the emitted radiation will be shifted to a higher frequency. Likewise, when the emitter moves away, the frequency will be lowered. For non-relativistic thermal velocities, the Doppler shift in frequency will be:\n\nwhere formula_2 is the observed frequency, formula_3 is the rest frequency, formula_4 is the velocity of the emitter towards the observer, and formula_5 is the speed of light.\n\nSince there is a distribution of speeds both toward and away from the observer in any volume element of the radiating body, the net effect will be to broaden the observed line. If formula_6 is the fraction of particles with velocity component formula_7 to formula_8 along a line of sight, then the corresponding distribution of the frequencies is\n\nwhere formula_10 is the velocity towards the observer corresponding to the shift of the rest frequency formula_11 to formula_12. Therefore,\n\nWe can also express the broadening in terms of the wavelength formula_13. Recalling that in the non-relativistic limit formula_14, we obtain\n\nIn the case of the thermal Doppler broadening, the velocity distribution is given by the Maxwell distribution\n\nwhere formula_16 is the mass of the emitting particle, formula_17 is the temperature and formula_18 is the Boltzmann constant. \n\nThen,\n\nWe can simplify this expression as\n\nwhich we immediately recognize as a Gaussian profile with the standard deviation\n\nand full width at half maximum (FWHM)\n\nIn astronomy and plasma physics, the thermal Doppler broadening is one of the explanations for the broadening of spectral lines, and as such gives an indication for the temperature of observed material. Other causes of velocity distributions may exist, though, for example due to turbulent motion. For a fully developed turbulence, the resulting line profile is generally very difficult to distinguish from the thermal one.\nAnother cause could be a large range of \"macroscopic\" velocities resulting, e.g., from the receding and approaching portions of a rapidly spinning accretion disk. Finally, there are many other factors which can also broaden the lines. For example, a sufficiently high particle number density may lead to significant Stark broadening.\n\nDoppler broadening can also be used to determine the velocity distribution of a gas given its absorption spectrum. In particular, this has been used to determine the velocity distribution of interstellar gas clouds. \n\nDoppler broadening has also been used as a design consideration in high temperature nuclear reactors. In principle, as the reactor fuel heats up, the neutron absorption spectrum will broaden due to the relative thermal motion of the fuel atoms with respect to the neutrons. Given the shape of the neutron absorption spectrum, this has the result of reducing neutron absorption cross section, reducing the likelihood of absorption and fission. The end result is that reactors designed to take advantage of doppler broadening will decrease their reactivity as temperature increases, creating a passive safety measure. This tends to be more relevant to gas cooled reactors as other mechanisms are dominant in water cooled reactors.\n\n"}
{"id": "55742877", "url": "https://en.wikipedia.org/wiki?curid=55742877", "title": "Droplet cluster", "text": "Droplet cluster\n\nDroplet cluster is a self-assembled levitating monolayer of microdroplets usually arranged into a hexagonally ordered structure over a locally heated thin (about 1 mm) layer of water. The droplet cluster is typologically similar to colloidal crystals. The phenomenon was observed for the first time in 2004, and it has been extensively studied after that.\n\nGrowing condensing droplets with a typical diameter of 0.01 mm – 0.2 mm levitate at an equilibrium height, where their weight is equilibrated by the drag force of the ascending air-vapor jet rising over the heated spot. At the same time, the droplets are dragged towards the center of the heated spot; however, they do not merge forming an ordered hexagonal (densest packed) pattern due to an aerodynamic repulsive pressure force from gas flow between the droplets. The spot is usually heated by a laser beam or another source of heat to 60 °C – 95 °C, although the phenomenon was observed also at temperatures slightly above 20 °C. The height of levitation and the distance between the droplets are of the same order as their diameters.\nDue to complex nature of aerodynamic forces between the microdroplets in an ascending jet, the droplets do not coalesce but form a closed packed hexagonal structure showing similarity with various classical and newly discovered objects, where self-organization is prominent, including water breath figures, colloid and dust crystals, foams, Rayleigh–Bénard cells, and to some extent, ice crystals. The droplets pack near the center of heated area where the temperature and the intensity of the ascending vapor jets are the highest. At the same time, there are repulsion forces of aerodynamic nature between the droplets. Consequently, the cluster packs itself in the densest packing shape (a hexagonal honeycomb structure) with a certain distance between the droplets dependent on the repulsion forces.\n\nBy controlling the temperature and temperature gradient one can control the number of droplets and their density and size. Using infrared irradiation, it is possible to suppress droplet growth and stabilize them for extended periods of time.\n\nIt has been suggested that the phenomenon, when combined with a spectrographic study of droplets content, can be used for rapid biochemical in situ analysis. Recent studies have shown that the cluster can exist at lower temperatures of about 20 °C, which makes it suitable for biochemical analysis of living objects.\n\nClusters with an arbitrary small number of droplets can be created. Unlike the clusters with a large number of droplets, small clusters cannot always form a hexagonally symmetric structure. Instead, they produce various more or less symmetric configurations depending on the number of droplets. Tracing individual droplets in small clusters is crucial for potential applications. The symmetry, orderliness, and stability of these configurations can be studied with such a measure of self-organization as the Voronoi entropy.\n\nThe phenomenon of the droplet cluster is different from the Leidenfrost effect because the latter occurs at much higher temperatures over a solid surface, while the droplet cluster forms at lower temperatures over a liquid surface. The phenomenon has also been observed with liquids other than water.\n\n\n"}
{"id": "13566263", "url": "https://en.wikipedia.org/wiki?curid=13566263", "title": "Dukhin number", "text": "Dukhin number\n\nThe Dukhin number () is a dimensionless quantity that characterizes the contribution of the surface conductivity to various electrokinetic and electroacoustic effects, as well as to electrical conductivity and permittivity of fluid heterogeneous systems. \n\nIt was introduced by Lyklema in “Fundamentals of Interface and Colloid Science”. A recent IUPAC Technical Report used this term explicitly and detailed several means of measurement in physical systems.\n\nThe Dukhin number is a ratio of the surface conductivity formula_1 to the fluid bulk electrical conductivity K multiplied by particle size \"a\":\n"}
{"id": "1800265", "url": "https://en.wikipedia.org/wiki?curid=1800265", "title": "Emergy", "text": "Emergy\n\nEmergy is the amount of energy that was consumed in direct and indirect transformations to make a product or service. Emergy is a measure of quality differences between different forms of energy. Emergy is an expression of all the energy used in the work processes that generate a product or service in units of one type of energy. Emergy is measured in units of \"emjoule\"s, a unit referring to the available energy consumed in transformations. Emergy accounts for different forms of energy and resources (e.g. sunlight, water, fossil fuels, minerals, etc.) Each form is generated by transformation processes in nature and each has a different ability to support work in natural and in human systems. The recognition of these quality differences is a key concept.\n\nThe theoretical and conceptual basis for the emergy methodology is grounded in thermodynamics, general system theory and systems ecology. Evolution of the theory by Howard T. Odum over the first thirty years is reviewed in \"Environmental Accounting\" and in the volume edited by C. A. S. Hall titled \"Maximum Power\".\n\nBeginning in the 1950s, Odum analyzed energy flow in ecosystems (\"e.g.\" Silver Springs, Florida; Enewetak atoll in the south Pacific; Galveston Bay, Texas and Puerto Rican rainforests, amongst others) where energies in various forms at various scales were observed. His analysis of energy flow in ecosystems, and the differences in the potential energy of sunlight, fresh water currents, wind and ocean currents led him to make the suggestion that when two or more different energy sources drive a system, they cannot be added without first converting them to a common measure that accounts for their differences in energy quality. This led him to introduce the concept of \"energy of one kind\" as a common denominator with the name \"energy cost\". He then expanded the analysis to model food production in the 1960s, and in the 1970s to fossil fuels.\n\nOdum's first formal statement of what would later be termed emergy was in 1973:\nEnergy is measured by calories, btu's, kilowatthours, and other intraconvertable units, but energy has a scale of quality which is not indicated by these measures. The ability to do work for man depends on the energy quality and quantity and this is measurable by the amount of energy of a lower quality grade required to develop the higher grade. The scale of energy goes from dilute sunlight up to plant matter, to coal, from coal to oil, to electricity and up to the high quality efforts of computer and human information processing.\n\nIn 1975, he introduced a table of \"Energy Quality Factors\", kilocalories of sunlight energy required to make a kilocalorie of a higher quality energy, the first mention of the energy hierarchy principle which states that \"energy quality is measured by the energy used in the transformations\" from one type of energy to the next.\n\nThese energy quality factors, were placed on a fossil-fuel basis and called \"Fossil Fuel Work Equivalents\" (FFWE), and the quality of energies were measured based on a fossil fuel standard with rough equivalents of 1 kilocalorie of fossil fuel equal to 2000 kilocalories of sunlight. \"Energy quality ratios\" were computed by evaluating the quantity of energy in a transformation process to make a new form and were then used to convert different forms of energy to a common form, in this case fossil fuel equivalents. FFWE's were replaced with coal equivalents (CE) and by 1977, the system of evaluating quality was placed on a solar basis and termed solar equivalents (SE).\n\nThe term \"embodied energy\" was used for a time in the early 1980s to refer to energy quality differences in terms of their costs of generation, and a ratio called a \"quality factor\" for the calories (or joules) of one kind of energy required to make those of another. However, since the term embodied energy was used by other groups who were evaluating the fossil fuel energy required to generate products and were not including all energies or using the concept to imply quality, embodied energy was dropped in favor of \"embodied solar calories\", and the quality factors became known as \"transformation ratios\".\n\nUse of the term \"embodied energy\" for this concept was modified in 1986 when David Scienceman, a visiting scholar at the University of Florida from Australia, suggested the term \"emergy\" and \"emjoule\" or \"emcalorie\" as the unit of measure to distinguish emergy units from units of available energy. The term transformation ratio was shortened to transformity in about the same time. It is important to note that throughout this twenty years the baseline or the basis for evaluating forms of energy and resources shifted from organic matter, to fossil fuels and finally to solar energy.\n\nAfter 1986, the emergy methodology continued to develop as the community of scientists expanded and as new applied research into combined systems of humans and nature presented new conceptual and theoretical questions. The maturing of the emergy methodology resulted in more rigorous definitions of terms and nomenclature and refinement of the methods of calculating transformities. The International Society for the Advancement of Emergy Research and a biennial International Conference at the University of Florida support this research.\n\nEmergy— amount of energy of one form that is used in transformations directly and indirectly to make a product or service. The unit of emergy is the emjoule or emergy joule. Using emergy, sunlight, fuel, electricity, and human service can be put on a common basis by expressing each of them in the emjoules of solar energy that is required to produce them. If solar emergy is the baseline, then the results are solar emjoules (abbreviated seJ). Although other baselines have been used, such as coal emjoules or electrical emjoules, in most cases emergy data are given in solar emjoules.\n\nUnit Emergy Values (UEVs) — the emergy required to generate one unit of output. Types of UEVs:\n\nEmergy accounting converts the thermodynamic basis of all forms of energy, resources and human services into equivalents of a single form of energy, usually solar. To evaluate a system, a system diagram organizes the evaluation and account for energy inputs and outflows. A table of the flows of resources, labor and energy is constructed from the diagram and all flows are evaluated. The final step involves interpreting the results.\n\nIn some cases, an evaluation is done to determine the fit of a development proposal within its environment. It also allows comparison of alternatives. Another purpose is to seek the best use of resources to maximize economic vitality.\n\nSystem diagrams show the inputs that are evaluated and summed to obtain the emergy of a flow. A diagram of a city and its regional support area is shown in Figure 1.\n\nA table (see example below) of resource flows, labor and energy is constructed from the diagram. Raw data on inflows that cross the boundary are converted into emergy units, and then summed to obtain total emergy supporting the system. Energy flows per unit time (usually per year) are presented in the table as separate line items.\n\nAll tables are followed by footnotes that show citations for data and calculations.\n\nThe table allows a unit emergy value to be calculated. The final, output row (row “O” in the example table above) is evaluated first in units of energy or mass. Then the input emergy is summed and the unit emergy value is calculated by dividing the emergy by the units of the output.\n\nFigure 2 shows non-renewable environmental contributions (N) as an emergy storage of materials, renewable environmental inputs (R), and inputs from the economy as purchased (F) goods and services. Purchased inputs are needed for the process to take place and include human service and purchased non-renewable energy and material brought in from elsewhere (fuels, minerals, electricity, machinery, fertilizer, etc.). Several ratios, or indices are given in Figure 2 that assess the global performance of a process.\n\nOther ratios are useful depending on the type and scale of the system under evaluation.\n\nThe recognition of the relevance of energy to the growth and dynamics of complex systems has resulted in increased emphasis on environmental evaluation methods that can account for and interpret the effects of matter and energy flows at all scales in systems of humanity and nature. The following table lists some general areas in which the emergy methodology has been employed.\n\nThe concept of emergy has been controversial within academe including ecology, thermodynamics and economy. Emergy theory has been criticized for allegedly offering an energy theory of value to replace other theories of value. The stated goal of emergy evaluations is to provide an \"ecocentric\" valuation of systems, processes. Thus it does not purport to replace economic values but to provide additional information, from a different point of view.\n\nThe idea that a calorie of sunlight is not equivalent to a calorie of fossil fuel or electricity strikes many as absurd, based on the 1st Law definition of energy units as measures of heat (i.e. Joule's mechanical equivalent of heat). Others have rejected the concept as impractical since from their perspective it is impossible to objectively quantify the amount of sunlight that is required to produce a quantity of oil. In combining systems of humanity and nature and evaluating environmental input to economies, mainstream economists criticize the emergy methodology for disregarding market values.\n\n"}
{"id": "46488727", "url": "https://en.wikipedia.org/wiki?curid=46488727", "title": "Energoland", "text": "Energoland\n\nEnergoland is an information centre for energy and electricity generation which was opened by Slovenské elektrárne on 14 October 2014 at Mochovce, Slovakia, at the site of the nuclear power plant. It is situated between Levice and Nitra. The centre mainly serves the schools and public. The entry is free of charge. The exposition was awarded as an excellent communication project at the PIME conference and evaluated as a five-star training centre by Laura Elizabeth Kennedy, governor of the United States of America in the Board of Governors of the International Atomic Energy Agency in Vienna and chargé d'affaires of permanent representation of the USA in international organisation in Vienna.\n\nIn Energoland, there are more than thirty objects, applications, and interactive expositions. Edutainment (blending education and entertainment) at Mochovce offers information on energy, electricity generation, global warming, and carbon burden. In addition, visitors learn about the energy mix, electricity grid dispatching, or about the radiation around us. The exposition also focuses on power industry, not only with respect to safety and radioactive waste but also dealing with the fuel cycle, nanowolrd of atoms, and Cherenkov radiation. Some of the specialities include the 3D cinema with its own movie Energy Odyssey, a model of emission-free motorcycle, interactive LED floor or thermal mirror, and mobile application using augmented reality, \"Energoland\", downloadable from Google Play or AppStore. Through the Energy Map, various facts and statistics can be searched which are directly shown in the map of the world with the highlighted countries.\nEnergoland also wants its visitors to make their own opinion of the electricity generation and its sources. Therefore, it does not provide information only about nuclear energy but it also brings knowledge about other sources: water, sun, wind, geothermal energy, or fossil fuels.\n\nThe design of the building is the work of Ing. arch. Viktor Šabík (BARAK Architekti) who, together with the QEX Company and Italian agency Piter Design, designed the interior of Energoland, as well.[2] The total area of the building, including the training rooms and offices is 2,000 square metres; the info centre itself takes up the area of 600 m2.\n\nEnergoland is not only a visitor and information centre; it also serves as a training centre for the employees of Slovenské elektrárne and provides office areas. There are many events for the employees of Slovenské elektrárne or public taking place here, such as the 90 reactor-years anniversary, programme within the national round of Olympics in Physics, Science Talks during the Science and Technology Week, Sustainable Development Conference in May 2015, etc.\n\n"}
{"id": "514231", "url": "https://en.wikipedia.org/wiki?curid=514231", "title": "Frequency-dependent selection", "text": "Frequency-dependent selection\n\nFrequency-dependent selection is an evolutionary process by which the fitness of a phenotype depends on its frequency relative to other phenotypes in a given population.\n\n\nFrequency-dependent selection is usually the result of interactions between species (predation, parasitism, or competition), or between genotypes within species (usually competitive or symbiotic), and has been especially frequently discussed with relation to anti-predator adaptations. Frequency-dependent selection can lead to polymorphic equilibria, which result from interactions among genotypes within species, in the same way that multi-species equilibria require interactions between species in competition (e.g. where \"α\" parameters in Lotka-Volterra competition equations are non-zero).\n\nThe first explicit statement of frequency-dependent selection appears to have been by Edward Bagnall Poulton in 1884, on the way that predators could maintain color polymorphisms in their prey.\n\nPerhaps the best known early modern statement of the principle is Bryan Clarke's 1962 paper on apostatic selection (a synonym of negative frequency-dependent selection). Clarke discussed predator attacks on polymorphic British snails, citing Luuk Tinbergen's classic work on searching images as support that predators such as birds tended to specialize in common forms of palatable species. Clarke later argued that frequency-dependent balancing selection could explain molecular polymorphisms (often in the absence of heterosis) in opposition to the neutral theory of molecular evolution.\n\nAnother example is plant self-incompatibility alleles. When two plants share the same incompatibility allele, they are unable to mate. Thus, a plant with a new (and therefore, rare) allele has more success at mating, and its allele spreads quickly through the population .\n\nIn human pathogens, such as the flu virus, once a particular strain has become common, most individuals have developed an immune response to that strain. But a rare, novel strain of the flu virus is able to spread quickly to almost any individual, causing continual evolution of viral strains.\n\nThe major histocompatibility complex (MHC) is involved in the recognition of foreign antigens and cells. Frequency-dependent selection may explain the high degree of polymorphism in the MHC.\n\nIn behavioral ecology, negative frequency-dependent selection often maintains multiple behavioral strategies within a species. A classic example is the Hawk-Dove model of interactions among individuals in a population. In a population with two traits A and B, being one form is better when most members are the other form. As another example, male common side-blotched lizards have three morphs, which either defend large territories and maintain large harems of females, defend smaller territories and keep one female, or mimic females in order to sneak matings from the other two morphs. These three morphs participate in a rock paper scissors sort of interaction such that no one morph completely outcompetes the other two. Another example occurs in the scaly-breasted munia, where certain individuals become scroungers and others become producers.\n\nPositive frequency-dependent selection gives an advantage to common phenotypes. A good example is warning coloration in aposematic species. Predators are more likely to remember a common color pattern that they have already encountered frequently than one that is rare. This means that new mutants or migrants that have color patterns other than the common type are eliminated from the population by differential predation. Positive frequency-dependent selection provides the basis for Müllerian mimicry, as described by Fritz Müller , because all species involved are aposematic and share the benefit of a common, honest signal to potential predators.\n\nAnother, rather complicated example occurs in the Batesian mimicry complex between a harmless mimic, the scarlet kingsnake (\"Lampropeltis elapsoides\"), and the model, the eastern coral snake (\"Micrurus fulvius\"), in locations where the model and mimic were in deep sympatry, the phenotype of the scarlet kingsnake was quite variable due to relaxed selection. But where the pattern was rare, the predator population was not 'educated', so the pattern brought no benefit. The scarlet kingsnake was much less variable on the allopatry/sympatry border of the model and mimic, most probably due to increased selection since the eastern coral snake is rare, but present, on this border. Therefore, the coloration is only advantageous once it has become common.\n\n\n"}
{"id": "8782181", "url": "https://en.wikipedia.org/wiki?curid=8782181", "title": "Geographical zone", "text": "Geographical zone\n\nThe five main latitude regions of the Earth's surface comprise geographical zones, divided by the major circles of latitude. The differences between them relate to climate. They are as follows: \n\n\nOn the basis of latitudinal extent, the globe is divided into three broad heat zones.\n\nThe Torrid is also known as the Tropics. The zone is bounded on the north by the Tropic of Cancer and on the south by the Tropic of Capricorn; these latitudes mark the northern and southern extremes in which the sun seasonally passes directly overhead. This happens annually, but in the region between, the sun passes overhead twice a year.\n\nIn the Northern Hemisphere, in the sun's apparent northward migration after the March equinox, it passes overhead once, then after the June solstice, at which time it reaches the Tropic of Cancer, it passes over again on its apparent southward journey. After the September equinox the sun passes into the Southern Hemisphere. It then passes similarly over the southern tropical regions until it reaches the Tropic of Capricorn at the December solstice, and back again as it returns northwards to the Equator.\n\nIn the two Temperate Zones also known as tropical zone not, consisting of the tepid latitudes, the Sun is never directly overhead, and the climate is mild, generally ranging from warm to cool. The four annual seasons, spring, summer, autumn and winter, occur in these areas. The North Temperate Zone includes Europe, Northern Asia, and North and Central America. The South Temperate Zone includes Southern Australasia, southern South America, and Southern Africa.\n\nThe two Frigid Zones, or polar regions, experience the midnight sun and the polar night for part of the year - at the edge of the zone there is one day at the winter solstice when the Sun is invisible, and one day at the summer solstice when the sun remains above the horizon for 24 hours. In the center of the zone (the pole) the day is one year long with six months of daylight and six months of night. The Frigid Zones are the coldest regions of Earth and are generally covered in ice and snow.It receives slanting rays of the sun as this region lies farthest from the equator. Summer season in this region lies lasts for about 2 to 3 months and there is almost 24 hour sunlight during summer.\n\nThe concept of a geographical zone was first hypothesized by the ancient Greek scholar Parmenides and lastingly modified by Aristotle. Both philosophers theorized the Earth divided into three types of climatic zones based on their distance from the equator.\n\nLike Parmeneides, thinking that the area near the equator was too hot for habitation, Aristotle dubbed the region around the equator (from 23.5° N to 23.5° S) the \"Torrid Zone.\" Both philosophers reasoned the region from the Arctic Circle to the pole to be permanently frozen. This region, thought uninhabitable, was called the \"Frigid Zone.\" The only area believed to be habitable was the northern \"Temperate Zone\" (the southern one not having been discovered), lying between the \"Frigid Zones\" and the \"Torrid Zone\". However, humans have inhabited almost all climates on Earth, including inside the Arctic Circle.\n\nAs knowledge of the Earth's geography improved, a second \"Temperate Zone\" was discovered south of the equator, and a second \"Frigid Zone\" was discovered around the Antarctic. Although Aristotle's map was oversimplified, the general idea was correct. Today, the most commonly used climate map is the Köppen climate classification, developed by Russian climatologist of German descent and amateur botanist Wladimir Köppen (1846–1940), which divides the world into five major climate regions, based on average annual precipitation, average monthly precipitation, and average monthly temperature.\n\n"}
{"id": "4387132", "url": "https://en.wikipedia.org/wiki?curid=4387132", "title": "Gravity of Earth", "text": "Gravity of Earth\n\nThe gravity of Earth, denoted by , is the net acceleration that is imparted to objects due to the combined effect of gravitation (from distribution of mass within Earth) and the centrifugal force (from the Earth's rotation).\n\nIn SI units this acceleration is measured in metres per second squared (in symbols, m/s or m·s) or equivalently in newtons per kilogram (N/kg or N·kg). Near Earth's surface, gravitational acceleration is approximately 9.8 m/s, which means that, ignoring the effects of air resistance, the speed of an object falling freely will increase by about 9.8 metres per second every second. This quantity is sometimes referred to informally as \"little \" (in contrast, the gravitational constant is referred to as \"big \").\n\nThe precise strength of Earth's gravity varies depending on location. The nominal \"average\" value at Earth's surface, known as is, by definition, 9.80665 m/s. This quantity is denoted variously as , (though this sometimes means the normal equatorial value on Earth, 9.78033 m/s), , gee, or simply (which is also used for the variable local value). \n\nThe weight of an object on Earth's surface is the downwards force on that object, given by Newton's second law of motion, or (). Gravitational acceleration contributes to the total gravity acceleration, but other factors, such as the rotation of Earth, also contribute, and, therefore, affect the weight of the object.\nGravity does not normally include the gravitational pull of the Moon and Sun, which are accounted for in terms of tidal effects.\nIt is a vector (physics) quantity, whose direction coincides with a plumb bob.\n\nA non-rotating perfect sphere of uniform mass density, or whose density varies solely with distance from the centre (spherical symmetry), would produce a gravitational field of uniform magnitude at all points on its surface. The Earth is rotating and is also not spherically symmetric; rather, it is slightly flatter at the poles while bulging at the Equator: an oblate spheroid. There are consequently slight deviations in the magnitude of gravity across its surface.\n\nGravity on the Earth's surface varies by around 0.7%, from 9.7639 m/s on the Nevado Huascarán mountain in Peru to 9.8337 m/s at the surface of the Arctic Ocean. In large cities, it ranges from 9.7760 in Kuala Lumpur, Mexico City, and Singapore to 9.825 in Oslo and Helsinki.\n\nIn 1901 the third General Conference on Weights and Measures defined a standard gravitational acceleration for the surface of the Earth: \"g\" = 9.80665 m/s. It was based on measurements done at the Pavillon de Breteuil near Paris in 1888, with a theoretical correction applied in order to convert to a latitude of 45° at sea level. This definition is thus not a value of any particular place or carefully worked out average, but an agreement for a value to use if a better actual local value is not known or not important. It is also used to define the units kilogram force and pound force.\n\nThe surface of the Earth is rotating, so it is not an inertial frame of reference. At latitudes nearer the Equator, the outward centrifugal force produced by Earth's rotation is larger than at polar latitudes. This counteracts the Earth's gravity to a small degree – up to a maximum of 0.3% at the Equator – and reduces the apparent downward acceleration of falling objects.\n\nThe second major reason for the difference in gravity at different latitudes is that the Earth's equatorial bulge (itself also caused by centrifugal force from rotation) causes objects at the Equator to be farther from the planet's centre than objects at the poles. Because the force due to gravitational attraction between two bodies (the Earth and the object being weighed) varies inversely with the square of the distance between them, an object at the Equator experiences a weaker gravitational pull than an object at the poles.\n\nIn combination, the equatorial bulge and the effects of the surface centrifugal force due to rotation mean that sea-level gravity increases from about 9.780 m/s at the Equator to about 9.832 m/s at the poles, so an object will weigh about 0.5% more at the poles than at the Equator.\n\nGravity decreases with altitude as one rises above the Earth's surface because greater altitude means greater distance from the Earth's centre. All other things being equal, an increase in altitude from sea level to causes a weight decrease of about 0.29%. (An additional factor affecting apparent weight is the decrease in air density at altitude, which lessens an object's buoyancy. This would increase a person's apparent weight at an altitude of 9,000 metres by about 0.08%)\n\nIt is a common misconception that astronauts in orbit are weightless because they have flown high enough to escape the Earth's gravity. In fact, at an altitude of , equivalent to a typical orbit of the ISS, gravity is still nearly 90% as strong as at the Earth's surface. Weightlessness actually occurs because orbiting objects are in free-fall.\n\nThe effect of ground elevation depends on the density of the ground (see Slab correction section). A person flying at 30 000 ft above sea level over mountains will feel more gravity than someone at the same elevation but over the sea. However, a person standing on the earth's surface feels less gravity when the elevation is higher.\n\nThe following formula approximates the Earth's gravity variation with altitude:\n\nWhere\n\nThe formula treats the Earth as a perfect sphere with a radially symmetric distribution of mass; a more accurate mathematical treatment is discussed below.\n\nAn approximate value for gravity at a distance from the center of the Earth can be obtained by assuming that the Earth's density is spherically symmetric. The gravity depends only on the mass inside the sphere of radius . All the contributions from outside cancel out as a consequence of the inverse-square law of gravitation. Another consequence is that the gravity is the same as if all the mass were concentrated at the center. Thus, the gravitational acceleration at this radius is\nwhere is the gravitational constant and is the total mass enclosed within radius . If the Earth had a constant density , the mass would be and the dependence of gravity on depth would be\nIf the density decreased linearly with increasing radius from a density at the center to at the surface, then , and the dependence would be\n\nThe actual depth dependence of density and gravity, inferred from seismic travel times (see Adams–Williamson equation), is shown in the graphs below.\n\nLocal differences in topography (such as the presence of mountains), geology (such as the density of rocks in the vicinity), and deeper tectonic structure cause local and regional differences in the Earth's gravitational field, known as gravitational anomalies. Some of these anomalies can be very extensive, resulting in bulges in sea level, and throwing pendulum clocks out of synchronisation.\n\nThe study of these anomalies forms the basis of gravitational geophysics. The fluctuations are measured with highly sensitive gravimeters, the effect of topography and other known factors is subtracted, and from the resulting data conclusions are drawn. Such techniques are now used by prospectors to find oil and mineral deposits. Denser rocks (often containing mineral ores) cause higher than normal local gravitational fields on the Earth's surface. Less dense sedimentary rocks cause the opposite.\n\nIn air, objects experience a supporting buoyancy force which reduces the apparent strength of gravity (as measured by an object's weight). The magnitude of the effect depends on air density (and hence air pressure); see Apparent weight for details.\n\nThe gravitational effects of the Moon and the Sun (also the cause of the tides) have a very small effect on the apparent strength of Earth's gravity, depending on their relative positions; typical variations are 2 µm/s (0.2 mGal) over the course of a day.\n\nGravity acceleration is a vector quantity. In a spherically symmetric Earth, gravity would point directly towards the sphere's centre. As the Earth is slightly flatter, there are consequently slight deviations in the direction of gravity.\n\nTools exist for calculating the strength of gravity at various cities around the world. The effect of latitude can be clearly seen with gravity in high-latitude cities: Anchorage (9.826 m/s), Helsinki (9.825 m/s), being about 0.5% greater than that in cities near the equator: Kuala Lumpur (9.776 m/s), Manila (9.780 m/s). The effect of altitude can be seen in Mexico City (9.776 m/s; altitude ), and by comparing Denver (9.798 m/s; ) with Washington, D.C. (9.801 m/s; ), both of which are near 39° N. Measured values can be obtained from Physical and Mathematical Tables by T.M. Yarwood and F. Castle, Macmillan, revised edition 1970.\n\nIf the terrain is at sea level, we can estimate formula_5, the acceleration at latitude formula_6:\n\nThis is the International Gravity Formula 1967, the 1967 Geodetic Reference System Formula, Helmert's equation or Clairaut's formula.\n\nAn alternative formula for \"g\" as a function of latitude is the WGS (World Geodetic System) 84 Ellipsoidal Gravity Formula:\nwhere,\n\n\nthen, where formula_13,\n\nThe difference between the WGS-84 formula and Helmert's equation is less than 0.68 μm·s.\n\nThe first correction to be applied to the model is the free air correction (FAC) that accounts for heights above sea level. Near the surface of the Earth (sea level), gravity decreases with height such that linear extrapolation would give zero gravity at a height of one half of the earth's radius - (9.8 m·s per 3,200 km.)\n\nUsing the mass and radius of the Earth:\n\nThe FAC correction factor (Δ\"g\") can be derived from the definition of the acceleration due to gravity in terms of G, the Gravitational Constant (see Estimating \"g\" from the law of universal gravitation, below):\n\nwhere:\n\nAt a height \"h\" above the nominal surface of the earth \"g\" is given by:\n\nSo the FAC for a height \"h\" above the nominal earth radius can be expressed:\n\nThis expression can be readily used for programming or inclusion in a spreadsheet. Collecting terms, simplifying and neglecting small terms (\"h\"«\"r\"), however yields the good approximation:\n\nUsing the numerical values above and for a height \"h\" in metres:\n\nGrouping the latitude and FAC altitude factors the expression most commonly found in the literature is:\nwhere formula_24 = acceleration in m·s at latitude formula_25 and altitude \"h\" in metres.\n\nFor flat terrain above sea level a second term is added for the gravity due to the extra mass; for this purpose the extra mass can be approximated by an infinite horizontal slab, and we get 2π\"G\" times the mass per unit area, i.e. 4.2 m·s·kg (0.042 μGal·kg·m) (the Bouguer correction). For a mean rock density of 2.67 g·cm this gives 1.1 s (0.11 mGal·m). Combined with the free-air correction this means a reduction of gravity at the surface of ca. 2 µm·s (0.20 mGal) for every metre of elevation of the terrain. (The two effects would cancel at a surface rock density of 4/3 times the average density of the whole earth. The density of the whole earth is 5.515 g·cm, so standing on a slab of something like iron whose density is over 7.35 g·cm would increase one's weight.)\n\nFor the gravity below the surface we have to apply the free-air correction as well as a double Bouguer correction. With the infinite slab model this is because moving the point of observation below the slab changes the gravity due to it to its opposite. Alternatively, we can consider a spherically symmetrical Earth and subtract from the mass of the Earth that of the shell outside the point of observation, because that does not cause gravity inside. This gives the same result.\n\nFrom the law of universal gravitation, the force on a body acted upon by Earth's gravity is given by\n\nwhere \"r\" is the distance between the centre of the Earth and the body (see below), and here we take \"m\" to be the mass of the Earth and \"m\" to be the mass of the body.\n\nAdditionally, Newton's second law, \"F\" = \"ma\", where \"m\" is mass and \"a\" is acceleration, here tells us that\n\nComparing the two formulas it is seen that:\n\nSo, to find the acceleration due to gravity at sea level, substitute the values of the gravitational constant, \"G\", the Earth's mass (in kilograms), \"m\", and the Earth's radius (in metres), \"r\", to obtain the value of \"g\":\n\nNote that this formula only works because of the mathematical fact that the gravity of a uniform spherical body, as measured on or above its surface, is the same as if all its mass were concentrated at a point at its centre. This is what allows us to use the Earth's radius for \"r\".\n\nThe value obtained agrees approximately with the measured value of \"g\". The difference may be attributed to several factors, mentioned above under \"Variations\":\nThere are significant uncertainties in the values of \"r\" and \"m\" as used in this calculation, and the value of \"G\" is also rather difficult to measure precisely.\n\nIf \"G\", \"g\" and \"r\" are known then a reverse calculation will give an estimate of the mass of the Earth. This method was used by Henry Cavendish.\n\n\n"}
{"id": "4205386", "url": "https://en.wikipedia.org/wiki?curid=4205386", "title": "Legal naturalism", "text": "Legal naturalism\n\nLegal naturalism is a term coined by Olufemi Taiwo to describe a current in the social philosophy of Karl Marx which can be interpreted as one of Natural Law. Taiwo considered it the manifestation of Natural Law in a dialectical materialist context.\n\n\n"}
{"id": "3958869", "url": "https://en.wikipedia.org/wiki?curid=3958869", "title": "List of conservation organisations", "text": "List of conservation organisations\n\nThis is a list of conservation organisations, which are organisations that primarily deal with the conservation of various ecosystems.\nCave Conservancies are land trusts specialized in caves and karst features.\n\n"}
{"id": "18027464", "url": "https://en.wikipedia.org/wiki?curid=18027464", "title": "List of herbaria in North America", "text": "List of herbaria in North America\n\nThis is a list of herbaria in North America, organized first by country or region where the herbarium is located, then within each region by size of the collection. For other continents, see List of herbaria.\n\nThe table below lists herbaria located in Central America and the Caribbean.\n\nAdditional Collection Resources:\n\nListed alphabetically by Herbarium Code. Note that this list includes herbaria that are inactive, meaning that the institutions are not currently adding new materials to their collections. This list also includes herbaria that have been incorporated into other herbaria.\n\nSee also: List of herbaria.\n"}
{"id": "23708860", "url": "https://en.wikipedia.org/wiki?curid=23708860", "title": "Magmatic water", "text": "Magmatic water\n\nMagmatic water or juvenile water is water that exists within, and in equilibrium with, a magma or water-rich volatile fluids that are derived from a magma. This magmatic water is released to the atmosphere during a volcanic eruption. Magmatic water may also be released as hydrothermal fluids during the late stages of magmatic crystallization or solidification within the Earth's crust. The crystallization of hydroxyl bearing amphibole and mica minerals acts to contain part of the magmatic water within a solidified igneous rock. Ultimate sources of this magmatic water includes water and hydrous minerals in rocks melted during subduction as well as primordial water brought up from the deep mantle.\n\nWater has limited solubility in silicate melts ranging from almost 0% at surface pressure to 10% at 1100 °C and 5 kbar of pressure for a granitic melt. Solubility is lower for more mafic magmas. As the temperature and pressure drop during emplacement and cooling of the magma a separate aqueous phase will exsolve. This aqueous phase will be enriched in other volatile and silicate incompatible species such as the metals: copper, lead, zinc, silver and gold; alkalis and alkaline earths and others, including: lithium, beryllium, boron, rubidium; and volatiles: fluorine, chlorine and carbon dioxide.\n\nWater in silicate melts at the high temperature and pressure conditions within the crust exists as a supercritical fluid rather than in a gaseous state (the critical point for water is at 374 °C and 218 bar).\n\nStable isotope studies of oxygen and hydrogen in igneous rocks indicate that the oxygen-18, δO, content is approximately 6 - 8 ‰ higher than standard mean ocean water (SMOW) while the deuterium, δH, content is 40 to 80 ‰ lower than SMOW. Water in equilibrium with igneous melts should bear the same isotopic signature for oxygen-18 and deuterium. Isotope data on hydrothermal solutions spatially associated with igneous intrusions should reflect this isotopic signature. However, isotopic studies of hydrothermal waters indicate that most bear the isotopic signature of meteoric water. Any magmatic waters in these hydrothermal solutions must have been swamped by the circulating meteoric groundwaters of the environment.\n\nFluid inclusions are microscopic bubbles of aqueous solutions which were trapped within crystals during crystallization and are considered as relic samples of the mineralizing waters. Analyses of the isotopic content of these trapped bubbles show a wide range of δO and δH content. All examined show an enrichment in O and depletion in H relative to SMOW and meteoric waters. Fluid inclusion data from a number of ore deposits plot directly on the magmatic water \"region\" of an δO vs δH plot.\n\n"}
{"id": "52555162", "url": "https://en.wikipedia.org/wiki?curid=52555162", "title": "Metal vapor synthesis", "text": "Metal vapor synthesis\n\nIn chemistry, metal vapor synthesis (MVS) is a method for preparing metal complexes by combining freshly produced metal atoms or small particles with ligands. In contrast to the high reactivity of such freshly produced metal atoms, bulk metals typically are unreactive toward neutral ligands. The method has been used to prepare compounds that cannot be prepared by traditional synthetic methods, e.g. Ti(η-toluene). The technique relies on a reactor that evaporates the metal, allowing the vapor to impinge on a cold reactor wall that is coated with the organic ligand. The metal evaporates upon being heated resistively or irradiated with an electron beam. The apparatus operates under high vacuum. In a common implementation, the metal vapor and the organic ligand are co-condensed at liquid nitrogen temperatures.\n\nIn several case where compounds are prepared by MVS, related preparations employ conventional routes. Thus, tris(butadiene)molybdenum was first prepared by co-condensation of butadiene and Mo vapor, but yields are higher for the reduction of molybdenum(V) chloride in the presence of the diene.\n"}
{"id": "19053", "url": "https://en.wikipedia.org/wiki?curid=19053", "title": "Mineral", "text": "Mineral\n\nA mineral is a naturally occurring chemical compound, usually of crystalline form and not produced by life processes. A mineral has one specific chemical composition, whereas a rock can be an aggregate of different minerals or mineraloids. The study of minerals is called mineralogy.\n\nMinerals are classified by variety, species, series and group, in order of increasing generality. As of November 2018, there are more than 5,500 known mineral \"species\"; 5,389 of these have been approved by the International Mineralogical Association (IMA). \n\nMinerals are distinguished by various chemical and physical properties. Differences in chemical composition and crystal structure distinguish the various species, which were determined by the mineral's geological environment when formed. Changes in the temperature, pressure, or bulk composition of a rock mass cause changes in its minerals. Within a mineral species there may be variation in physical properties or minor amounts of impurities that are recognized by mineralogists or wider society as a mineral \"variety\", for example amethyst, a purple variety of the mineral species quartz.\n\nMinerals can be described by their various physical properties, which are related to their chemical structure and composition. Common distinguishing characteristics include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, tenacity, cleavage, fracture, parting, specific gravity, magnetism, taste or smell, radioactivity, and reaction to acid.\n\nMinerals are classified by key chemical constituents; the two dominant systems are the Dana classification and the Strunz classification. Silicon and oxygen constitute approximately 75% of the Earth's crust, which translates directly into the predominance of silicate minerals. The silicate minerals compose over 90% of the Earth's crust. The silicate class of minerals is subdivided into six subclasses by the degree of polymerization in the chemical structure. All silicate minerals have a base unit of a [SiO] silica tetrahedron—that is, a silicon cation coordinated by four oxygen anions, which gives the shape of a tetrahedron. These tetrahedra can be polymerized to give the subclasses: orthosilicates (no polymerization, thus single tetrahedra), disilicates (two tetrahedra bonded together), cyclosilicates (rings of tetrahedra), inosilicates (chains of tetrahedra), phyllosilicates (sheets of tetrahedra), and tectosilicates (three-dimensional network of tetrahedra). Other important mineral groups include the native elements, sulfides, oxides, halides, carbonates, sulfates, and phosphates.\n\nOne definition of a mineral encompasses the following criteria:\n\n\nThe first three general characteristics are less debated than the last two.\n\nMineral classification schemes and their definitions are evolving to match recent advances in mineral science. Recent changes have included the addition of an organic class, in both the new Dana and the Strunz classification schemes. The organic class includes a very rare group of minerals with hydrocarbons. The IMA Commission on New Minerals and Mineral Names adopted in 2009 a hierarchical scheme for the naming and classification of mineral groups and group names and established seven commissions and four working groups to review and classify minerals into an official listing of their published names. According to these new rules, \"mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification.\"\n\nThe Nickel (1995) exclusion of biogenic substances was not universally adhered to. For example, Lowenstam (1981) stated that \"organisms are capable of forming a diverse array of minerals, some of which cannot be formed inorganically in the biosphere.\" The distinction is a matter of classification and less to do with the constituents of the minerals themselves. Skinner (2005) views all solids as potential minerals and includes biominerals in the mineral kingdom, which are those that are created by the metabolic activities of organisms. Skinner expanded the previous definition of a mineral to classify \"element or compound, amorphous or crystalline, formed through \"biogeochemical \" processes,\" as a mineral.\n\nRecent advances in high-resolution genetics and X-ray absorption spectroscopy are providing revelations on the biogeochemical relations between microorganisms and minerals that may make Nickel's (1995) biogenic mineral exclusion obsolete and Skinner's (2005) biogenic mineral inclusion a necessity. For example, the IMA-commissioned \"Working Group on Environmental Mineralogy and Geochemistry \" deals with minerals in the hydrosphere, atmosphere, and biosphere. The group's scope includes mineral-forming microorganisms, which exist on nearly every rock, soil, and particle surface spanning the globe to depths of at least 1600 metres below the sea floor and 70 kilometres into the stratosphere (possibly entering the mesosphere). Biogeochemical cycles have contributed to the formation of minerals for billions of years. Microorganisms can precipitate metals from solution, contributing to the formation of ore deposits. They can also catalyze the dissolution of minerals.\n\nPrior to the International Mineralogical Association's listing, over 60 biominerals had been discovered, named, and published. These minerals (a sub-set tabulated in Lowenstam (1981)) are considered minerals proper according to the Skinner (2005) definition. These biominerals are not listed in the International Mineral Association official list of mineral names, however, many of these biomineral representatives are distributed amongst the 78 mineral classes listed in the Dana classification scheme. Another rare class of minerals (primarily biological in origin) include the mineral liquid crystals that have properties of both liquids and crystals. To date, over 80,000 liquid crystalline compounds have been identified.\n\nThe Skinner (2005) definition of a mineral takes this matter into account by stating that a mineral can be crystalline or amorphous, the latter group including liquid crystals. Although biominerals and liquid mineral crystals, are not the most common form of minerals, they help to define the limits of what constitutes a mineral proper. The formal Nickel (1995) definition explicitly mentioned crystallinity as a key to defining a substance as a mineral. A 2011 article defined icosahedrite, an aluminium-iron-copper alloy as mineral; named for its unique natural icosahedral symmetry, it is a quasicrystal. Unlike a true crystal, quasicrystals are ordered but not periodic.\n\nMinerals are not equivalent to rocks. A rock is an aggregate of one or more minerals or mineraloids. Some rocks, such as limestone or quartzite, are composed primarily of one mineral—calcite or aragonite in the case of limestone, and quartz in the latter case. Other rocks can be defined by relative abundances of key (essential) minerals; a granite is defined by proportions of quartz, alkali feldspar, and plagioclase feldspar. The other minerals in the rock are termed accessory, and do not greatly affect the bulk composition of the rock. Rocks can also be composed entirely of non-mineral material; coal is a sedimentary rock composed primarily of organically derived carbon.\n\nIn rocks, some mineral species and groups are much more abundant than others; these are termed the rock-forming minerals. The major examples of these are quartz, the feldspars, the micas, the amphiboles, the pyroxenes, the olivines, and calcite; except for the last one, all of these minerals are silicates. Overall, around 150 minerals are considered particularly important, whether in terms of their abundance or aesthetic value in terms of collecting.\n\nCommercially valuable minerals and rocks are referred to as industrial minerals. For example, muscovite, a white mica, can be used for windows (sometimes referred to as isinglass), as a filler, or as an insulator. Ores are minerals that have a high concentration of a certain element, typically a metal. Examples are cinnabar (HgS), an ore of mercury, sphalerite (ZnS), an ore of zinc, or cassiterite (SnO), an ore of tin. Gems are minerals with an ornamental value, and are distinguished from non-gems by their beauty, durability, and usually, rarity. There are about 20 mineral species that qualify as gem minerals, which constitute about 35 of the most common gemstones. Gem minerals are often present in several varieties, and so one mineral can account for several different gemstones; for example, ruby and sapphire are both corundum, AlO.\n\nMinerals are classified by variety, species, series and group, in order of increasing generality. The basic level of definition is that of mineral species, each of which is distinguished from the others by unique chemical and physical properties. For example, quartz is defined by its formula, SiO, and a specific crystalline structure that distinguishes it from other minerals with the same chemical formula (termed polymorphs). When there exists a range of composition between two minerals species, a mineral series is defined. For example, the biotite series is represented by variable amounts of the endmembers phlogopite, siderophyllite, annite, and eastonite. In contrast, a mineral group is a grouping of mineral species with some common chemical properties that share a crystal structure. The pyroxene group has a common formula of XY(Si,Al)O, where X and Y are both cations, with X typically bigger than Y; the pyroxenes are single-chain silicates that crystallize in either the orthorhombic or monoclinic crystal systems. Finally, a mineral variety is a specific type of mineral species that differs by some physical characteristic, such as colour or crystal habit. An example is amethyst, which is a purple variety of quartz.\n\nTwo common classifications, Dana and Strunz, are used for minerals; both rely on composition, specifically with regards to important chemical groups, and structure. James Dwight Dana, a leading geologist of his time, first published his \"System of Mineralogy\" in 1837; as of 1997, it is in its eighth edition. The Dana classification assigns a four-part number to a mineral species. Its class number is based on important compositional groups; the type gives the ratio of cations to anions in the mineral, and the last two numbers group minerals by structural similarity within a given type or class. The less commonly used Strunz classification, named for German mineralogist Karl Hugo Strunz, is based on the Dana system, but combines both chemical and structural criteria, the latter with regards to distribution of chemical bonds.\n\n, 5,389 mineral species are approved by the IMA. They are most commonly named after a person (45%), followed by discovery location (23%); names based on chemical composition (14%) and physical properties (8%) are the two other major groups of mineral name etymologies.\n\nThe word \"species\" (from the Latin \"species\", \"a particular sort, kind, or type with distinct look, or appearance\") comes from the classification scheme in \"Systema Naturae\" by Carl Linnaeus. He divided the natural world into three kingdoms – plants, animals, and minerals – and classified each with the same hierarchy. In descending order, these were Phylum, Class, Order, Family, Tribe, Genus, and Species.\n\nThe abundance and diversity of minerals is controlled directly by their chemistry, in turn dependent on elemental abundances in the Earth. The majority of minerals observed are derived from the Earth's crust. Eight elements account for most of the key components of minerals, due to their abundance in the crust. These eight elements, summing to over 98% of the crust by weight, are, in order of decreasing abundance: oxygen, silicon, aluminium, iron, magnesium, calcium, sodium and potassium. Oxygen and silicon are by far the two most important – oxygen composes 47% of the crust by weight, and silicon accounts for 28%.\n\nThe minerals that form are directly controlled by the bulk chemistry of the parent body. For example, a magma rich in iron and magnesium will form mafic minerals, such as olivine and the pyroxenes; in contrast, a more silica-rich magma will crystallize to form minerals that incorporate more SiO, such as the feldspars and quartz. In a limestone, calcite or aragonite (both CaCO) form because the rock is rich in calcium and carbonate. A corollary is that a mineral will not be found in a rock whose bulk chemistry does not resemble the bulk chemistry of a given mineral with the exception of trace minerals. For example, kyanite, AlSiO forms from the metamorphism of aluminium-rich shales; it would not likely occur in aluminium-poor rock, such as quartzite.\n\nThe chemical composition may vary between end member species of a solid solution series. For example, the plagioclase feldspars comprise a continuous series from sodium-rich end member albite (NaAlSiO) to calcium-rich anorthite (CaAlSiO) with four recognized intermediate varieties between them (given in order from sodium- to calcium-rich): oligoclase, andesine, labradorite, and bytownite. Other examples of series include the olivine series of magnesium-rich forsterite and iron-rich fayalite, and the wolframite series of manganese-rich hübnerite and iron-rich ferberite.\n\nChemical substitution and coordination polyhedra explain this common feature of minerals. In nature, minerals are not pure substances, and are contaminated by whatever other elements are present in the given chemical system. As a result, it is possible for one element to be substituted for another. Chemical substitution will occur between ions of a similar size and charge; for example, K will not substitute for Si because of chemical and structural incompatibilities caused by a big difference in size and charge. A common example of chemical substitution is that of Si by Al, which are close in charge, size, and abundance in the crust. In the example of plagioclase, there are three cases of substitution. Feldspars are all framework silicates, which have a silicon-oxygen ratio of 2:1, and the space for other elements is given by the substitution of Si by Al to give a base unit of [AlSiO]; without the substitution, the formula would be charge-balanced as SiO, giving quartz. The significance of this structural property will be explained further by coordination polyhedra. The second substitution occurs between Na and Ca; however, the difference in charge has to accounted for by making a second substitution of Si by Al.\n\nCoordination polyhedra are geometric representations of how a cation is surrounded by an anion. In mineralogy, coordination polyhedra are usually considered in terms of oxygen, due its abundance in the crust. The base unit of silicate minerals is the silica tetrahedron – one Si surrounded by four O. An alternate way of describing the coordination of the silicate is by a number: in the case of the silica tetrahedron, the silicon is said to have a coordination number of 4. Various cations have a specific range of possible coordination numbers; for silicon, it is almost always 4, except for very high-pressure minerals where the compound is compressed such that silicon is in six-fold (octahedral) coordination with oxygen. Bigger cations have a bigger coordination numbers because of the increase in relative size as compared to oxygen (the last orbital subshell of heavier atoms is different too). Changes in coordination numbers leads to physical and mineralogical differences; for example, at high pressure, such as in the mantle, many minerals, especially silicates such as olivine and garnet, will change to a perovskite structure, where silicon is in octahedral coordination. Other examples are the aluminosilicates kyanite, andalusite, and sillimanite (polymorphs, since they share the formula AlSiO), which differ by the coordination number of the Al; these minerals transition from one another as a response to changes in pressure and temperature. In the case of silicate materials, the substitution of Si by Al allows for a variety of minerals because of the need to balance charges.\n\nChanges in temperature and pressure and composition alter the mineralogy of a rock sample. Changes in composition can be caused by processes such as weathering or metasomatism (hydrothermal alteration). Changes in temperature and pressure occur when the host rock undergoes tectonic or magmatic movement into differing physical regimes. Changes in thermodynamic conditions make it favourable for mineral assemblages to react with each other to produce new minerals; as such, it is possible for two rocks to have an identical or a very similar bulk rock chemistry without having a similar mineralogy. This process of mineralogical alteration is related to the rock cycle. An example of a series of mineral reactions is illustrated as follows.\n\nOrthoclase feldspar (KAlSiO) is a mineral commonly found in granite, a plutonic igneous rock. When exposed to weathering, it reacts to form kaolinite (AlSiO(OH), a sedimentary mineral, and silicic acid):\n\nUnder low-grade metamorphic conditions, kaolinite reacts with quartz to form pyrophyllite (AlSiO(OH)):\n\nAs metamorphic grade increases, the pyrophyllite reacts to form kyanite and quartz:\n\nAlternatively, a mineral may change its crystal structure as a consequence of changes in temperature and pressure without reacting. For example, quartz will change into a variety of its SiO polymorphs, such as tridymite and cristobalite at high temperatures, and coesite at high pressures.\n\nClassifying minerals ranges from simple to difficult. A mineral can be identified by several physical properties, some of them being sufficient for full identification without equivocation. In other cases, minerals can only be classified by more complex optical, chemical or X-ray diffraction analysis; these methods, however, can be costly and time-consuming. Physical properties applied for classification include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, cleavage and fracture, and specific gravity. Other less general tests include fluorescence, phosphorescence, magnetism, radioactivity, tenacity (response to mechanical induced changes of shape or form), piezoelectricity and reactivity to dilute acids.\n\nCrystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction. Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.\n\nThese families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and α, β, γ represent the angle opposite the respective crystallographic axis (e.g. α is the angle opposite the a-axis, viz. the angle between the b and c axes):\n\nThe hexagonal crystal family is also split into two crystal \"systems\" – the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry.\n\nChemistry and crystal structure together define a mineral. With a restriction to 32 point groups, minerals of different chemistry may have identical crystal structure. For example, halite (NaCl), galena (PbS), and periclase (MgO) all belong to the hexaoctahedral point group (isometric family), as they have a similar stoichiometry between their different constituent elements. In contrast, polymorphs are groupings of minerals that share a chemical formula but have a different structure. For example, pyrite and marcasite, both iron sulfides, have the formula FeS; however, the former is isometric while the latter is orthorhombic. This polymorphism extends to other sulfides with the generic AX formula; these two groups are collectively known as the pyrite and marcasite groups.\n\nPolymorphism can extend beyond pure symmetry content. The aluminosilicates are a group of three minerals – kyanite, andalusite, and sillimanite – which share the chemical formula AlSiO. Kyanite is triclinic, while andalusite and sillimanite are both orthorhombic and belong to the dipyramidal point group. These differences arise corresponding to how aluminium is coordinated within the crystal structure. In all minerals, one aluminium ion is always in six-fold coordination with oxygen. Silicon, as a general rule, is in four-fold coordination in all minerals; an exception is a case like stishovite (SiO, an ultra-high pressure quartz polymorph with rutile structure). In kyanite, the second aluminium is in six-fold coordination; its chemical formula can be expressed as AlAlSiO, to reflect its crystal structure. Andalusite has the second aluminium in five-fold coordination (AlAlSiO) and sillimanite has it in four-fold coordination (AlAlSiO).\n\nDifferences in crystal structure and chemistry greatly influence other physical properties of the mineral. The carbon allotropes diamond and graphite have vastly different properties; diamond is the hardest natural substance, has an adamantine lustre, and belongs to the isometric crystal family, whereas graphite is very soft, has a greasy lustre, and crystallises in the hexagonal family. This difference is accounted for by differences in bonding. In diamond, the carbons are in sp hybrid orbitals, which means they form a framework where each carbon is covalently bonded to four neighbours in a tetrahedral fashion; on the other hand, graphite is composed of sheets of carbons in sp hybrid orbitals, where each carbon is bonded covalently to only three others. These sheets are held together by much weaker van der Waals forces, and this discrepancy translates to large macroscopic differences.\n\nTwinning is the intergrowth of two or more crystals of a single mineral species. The geometry of the twinning is controlled by the mineral's symmetry. As a result, there are several types of twins, including contact twins, reticulated twins, geniculated twins, penetration twins, cyclic twins, and polysynthetic twins. Contact, or simple twins, consist of two crystals joined at a plane; this type of twinning is common in spinel. Reticulated twins, common in rutile, are interlocking crystals resembling netting. Geniculated twins have a bend in the middle that is caused by start of the twin. Penetration twins consist of two single crystals that have grown into each other; examples of this twinning include cross-shaped staurolite twins and Carlsbad twinning in orthoclase. Cyclic twins are caused by repeated twinning around a rotation axis. This type of twinning occurs around three, four, five, six, or eight-fold axes, and the corresponding patterns are called threelings, fourlings, fivelings, sixlings, and eightlings. Sixlings are common in aragonite. Polysynthetic twins are similar to cyclic twins through the presence of repetitive twinning; however, instead of occurring around a rotational axis, polysynthetic twinning occurs along parallel planes, usually on a microscopic scale.\n\nCrystal habit refers to the overall shape of crystal. Several terms are used to describe this property. Common habits include acicular, which describes needlelike crystals as in natrolite, bladed, dendritic (tree-pattern, common in native copper), equant, which is typical of garnet, prismatic (elongated in one direction), and tabular, which differs from bladed habit in that the former is platy whereas the latter has a defined elongation. Related to crystal form, the quality of crystal faces is diagnostic of some minerals, especially with a petrographic microscope. Euhedral crystals have a defined external shape, while anhedral crystals do not; those intermediate forms are termed subhedral.\n\nThe hardness of a mineral defines how much it can resist scratching. This physical property is controlled by the chemical composition and crystalline structure of a mineral. A mineral's hardness is not necessarily constant for all sides, which is a function of its structure; crystallographic weakness renders some directions softer than others. An example of this property exists in kyanite, which has a Mohs hardness of 5½ parallel to [001] but 7 parallel to [100].\n\nThe most common scale of measurement is the ordinal Mohs hardness scale. Defined by ten indicators, a mineral with a higher index scratches those below it. The scale ranges from talc, a phyllosilicate, to diamond, a carbon polymorph that is the hardest natural material. The scale is provided below:\n\nLustre indicates how light reflects from the mineral's surface, with regards to its quality and intensity. There are numerous qualitative terms used to describe this property, which are split into metallic and non-metallic categories. Metallic and sub-metallic minerals have high reflectivity like metal; examples of minerals with this lustre are galena and pyrite. Non-metallic lustres include: adamantine, such as in diamond; vitreous, which is a glassy lustre very common in silicate minerals; pearly, such as in talc and apophyllite; resinous, such as members of the garnet group; silky which is common in fibrous minerals such as asbestiform chrysotile.\n\nThe diaphaneity of a mineral describes the ability of light to pass through it. Transparent minerals do not diminish the intensity of light passing through them. An example of a transparent mineral is muscovite (potassium mica); some varieties are sufficiently clear to have been used for windows. Translucent minerals allow some light to pass, but less than those that are transparent. Jadeite and nephrite (mineral forms of jade are examples of minerals with this property). Minerals that do not allow light to pass are called opaque.\n\nThe diaphaneity of a mineral depends on the thickness of the sample. When a mineral is sufficiently thin (e.g., in a thin section for petrography), it may become transparent even if that property is not seen in a hand sample. In contrast, some minerals, such as hematite or pyrite, are opaque even in thin-section.\n\nColour is the most obvious property of a mineral, but it is often non-diagnostic. It is caused by electromagnetic radiation interacting with electrons (except in the case of incandescence, which does not apply to minerals). Two broad classes of elements (idiochromatic and allochromatic) are defined with regards to their contribution to a mineral's colour: Idiochromatic elements are essential to a mineral's composition; their contribution to a mineral's colour is diagnostic. Examples of such minerals are malachite (green) and azurite (blue). In contrast, allochromatic elements in minerals are present in trace amounts as impurities. An example of such a mineral would be the ruby and sapphire varieties of the mineral corundum.\nThe colours of pseudochromatic minerals are the result of interference of light waves. Examples include labradorite and bornite.\n\nIn addition to simple body colour, minerals can have various other distinctive optical properties, such as play of colours, asterism, chatoyancy, iridescence, tarnish, and pleochroism. Several of these properties involve variability in colour. Play of colour, such as in opal, results in the sample reflecting different colours as it is turned, while pleochroism describes the change in colour as light passes through a mineral in a different orientation. Iridescence is a variety of the play of colours where light scatters off a coating on the surface of crystal, cleavage planes, or off layers having minor gradations in chemistry. In contrast, the play of colours in opal is caused by light refracting from ordered microscopic silica spheres within its physical structure. Chatoyancy (\"cat's eye\") is the wavy banding of colour that is observed as the sample is rotated; asterism, a variety of chatoyancy, gives the appearance of a star on the mineral grain. The latter property is particularly common in gem-quality corundum.\nThe streak of a mineral refers to the colour of a mineral in powdered form, which may or may not be identical to its body colour. The most common way of testing this property is done with a streak plate, which is made out of porcelain and coloured either white or black. The streak of a mineral is independent of trace elements or any weathering surface. A common example of this property is illustrated with hematite, which is coloured black, silver, or red in hand sample, but has a cherry-red to reddish-brown streak. Streak is more often distinctive for metallic minerals, in contrast to non-metallic minerals whose body colour is created by allochromatic elements. Streak testing is constrained by the hardness of the mineral, as those harder than 7 powder the \"streak plate\" instead.\n\nBy definition, minerals have a characteristic atomic arrangement. Weakness in this crystalline structure causes planes of weakness, and the breakage of a mineral along such planes is termed cleavage. The quality of cleavage can be described based on how cleanly and easily the mineral breaks; common descriptors, in order of decreasing quality, are \"perfect\", \"good\", \"distinct\", and \"poor\". In particularly transparent minerals, or in thin-section, cleavage can be seen as a series of parallel lines marking the planar surfaces when viewed from the side. Cleavage is not a universal property among minerals; for example, quartz, consisting of extensively interconnected silica tetrahedra, does not have a crystallographic weakness which would allow it to cleave. In contrast, micas, which have perfect basal cleavage, consist of sheets of silica tetrahedra which are very weakly held together.\n\nAs cleavage is a function of crystallography, there are a variety of cleavage types. Cleavage occurs typically in either one, two, three, four, or six directions. Basal cleavage in one direction is a distinctive property of the micas. Two-directional cleavage is described as prismatic, and occurs in minerals such as the amphiboles and pyroxenes. Minerals such as galena or halite have cubic (or isometric) cleavage in three directions, at 90°; when three directions of cleavage are present, but not at 90°, such as in calcite or rhodochrosite, it is termed rhombohedral cleavage. Octahedral cleavage (four directions) is present in fluorite and diamond, and sphalerite has six-directional dodecahedral cleavage.\n\nMinerals with many cleavages might not break equally well in all of the directions; for example, calcite has good cleavage in three directions, but gypsum has perfect cleavage in one direction, and poor cleavage in two other directions. Angles between cleavage planes vary between minerals. For example, as the amphiboles are double-chain silicates and the pyroxenes are single-chain silicates, the angle between their cleavage planes is different. The pyroxenes cleave in two directions at approximately 90°, whereas the amphiboles distinctively cleave in two directions separated by approximately 120° and 60°. The cleavage angles can be measured with a contact goniometer, which is similar to a protractor.\n\nParting, sometimes called \"false cleavage\", is similar in appearance to cleavage but is instead produced by structural defects in the mineral, as opposed to systematic weakness. Parting varies from crystal to crystal of a mineral, whereas all crystals of a given mineral will cleave if the atomic structure allows for that property. In general, parting is caused by some stress applied to a crystal. The sources of the stresses include deformation (e.g. an increase in pressure), exsolution, or twinning. Minerals that often display parting include the pyroxenes, hematite, magnetite, and corundum.\n\nWhen a mineral is broken in a direction that does not correspond to a plane of cleavage, it is termed to have been fractured. There are several types of uneven fracture. The classic example is conchoidal fracture, like that of quartz; rounded surfaces are created, which are marked by smooth curved lines. This type of fracture occurs only in very homogeneous minerals. Other types of fracture are fibrous, splintery, and hackly. The latter describes a break along a rough, jagged surface; an example of this property is found in native copper.\n\nTenacity is related to both cleavage and fracture. Whereas fracture and cleavage describes the surfaces that are created when a mineral is broken, tenacity describes how resistant a mineral is to such breaking. Minerals can be described as brittle, ductile, malleable, sectile, flexible, or elastic.\n\nSpecific gravity numerically describes the density of a mineral. The dimensions of density are mass divided by volume with units: kg/m or g/cm. Specific gravity measures how much water a mineral sample displaces. Defined as the quotient of the mass of the sample and difference between the weight of the sample in air and its corresponding weight in water, specific gravity is a unitless ratio. Among most minerals, this property is not diagnostic. Rock forming minerals – typically silicates or occasionally carbonates – have a specific gravity of 2.5–3.5.\n\nHigh specific gravity is a diagnostic property of a mineral. A variation in chemistry (and consequently, mineral class) correlates to a change in specific gravity. Among more common minerals, oxides and sulfides tend to have a higher specific gravity as they include elements with higher atomic mass. A generalization is that minerals with metallic or adamantine lustre tend to have higher specific gravities than those having a non-metallic to dull lustre. For example, hematite, FeO, has a specific gravity of 5.26 while galena, PbS, has a specific gravity of 7.2–7.6, which is a result of their high iron and lead content, respectively. A very high specific gravity becomes very pronounced in native metals; kamacite, an iron-nickel alloy common in iron meteorites has a specific gravity of 7.9, and gold has an observed specific gravity between 15 and 19.3.\n\nOther properties can be used to diagnose minerals. These are less general, and apply to specific minerals.\n\nDropping dilute acid (often 10% HCl) onto a mineral aids in distinguishing carbonates from other mineral classes. The acid reacts with the carbonate ([CO]) group, which causes the affected area to effervesce, giving off carbon dioxide gas. This test can be further expanded to test the mineral in its original crystal form or powdered form. An example of this test is done when distinguishing calcite from dolomite, especially within rocks (limestone and dolostone respectively). Calcite immediately effervesces in acid, whereas acid must be applied to powdered dolomite (often to a scratched surface in a rock), for it to effervesce. Zeolite minerals will not effervesce in acid; instead, they become frosted after 5–10 minutes, and if left in acid for a day, they dissolve or become a silica gel.\n\nWhen tested, magnetism is a very conspicuous property of minerals. Among common minerals, magnetite exhibits this property strongly, and magnetism is also present, albeit not as strongly, in pyrrhotite and ilmenite. Some minerals exhibit electrical properties – for example, quartz is piezoelectric – but electrical properties are rarely used as diagnostic criteria for minerals because of incomplete data and natural variation.\n\nMinerals can also be tested for taste or smell. Halite, NaCl, is table salt; its potassium-bearing counterpart, sylvite, has a pronounced bitter taste. Sulfides have a characteristic smell, especially as samples are fractured, reacting, or powdered.\n\nRadioactivity is a rare property; minerals may be composed of radioactive elements. They could be a defining constituent, such as uranium in uraninite, autunite, and carnotite, or as trace impurities. In the latter case, the decay of a radioactive element damages the mineral crystal; the result, termed a \"radioactive halo\" or \"pleochroic halo\", is observable with various techniques, such as thin-section petrography.\n\nAs the composition of the Earth's crust is dominated by silicon and oxygen, silicate elements are by far the most important class of minerals in terms of rock formation and diversity. However, non-silicate minerals are of great economic importance, especially as ores.\n\nNon-silicate minerals are subdivided into several other classes by their dominant chemistry, which includes native elements, sulfides, halides, oxides and hydroxides, carbonates and nitrates, borates, sulfates, phosphates, and organic compounds. Most non-silicate mineral species are rare (constituting in total 8% of the Earth's crust), although some are relatively common, such as calcite, pyrite, magnetite, and hematite. There are two major structural styles observed in non-silicates: close-packing and silicate-like linked tetrahedra. close-packed structures is a way to densely pack atoms while minimizing interstitial space. Hexagonal close-packing involves stacking layers where every other layer is the same (\"ababab\"), whereas cubic close-packing involves stacking groups of three layers (\"abcabcabc\"). Analogues to linked silica tetrahedra include SO (sulfate), PO (phosphate), AsO (arsenate), and VO (vanadate). The non-silicates have great economic importance, as they concentrate elements more than the silicate minerals do.\n\nThe largest grouping of minerals by far are the silicates; most rocks are composed of greater than 95% silicate minerals, and over 90% of the Earth's crust is composed of these minerals. The two main constituents of silicates are silicon and oxygen, which are the two most abundant elements in the Earth's crust. Other common elements in silicate minerals correspond to other common elements in the Earth's crust, such as aluminium, magnesium, iron, calcium, sodium, and potassium. Some important rock-forming silicates include the feldspars, quartz, olivines, pyroxenes, amphiboles, garnets, and micas.\n\nThe base unit of a silicate mineral is the [SiO] tetrahedron. In the vast majority of cases, silicon is in four-fold or tetrahedral coordination with oxygen. In very high-pressure situations, silicon will be in six-fold or octahedral coordination, such as in the perovskite structure or the quartz polymorph stishovite (SiO). In the latter case, the mineral no longer has a silicate structure, but that of rutile (TiO), and its associated group, which are simple oxides. These silica tetrahedra are then polymerized to some degree to create various structures, such as one-dimensional chains, two-dimensional sheets, and three-dimensional frameworks. The basic silicate mineral where no polymerization of the tetrahedra has occurred requires other elements to balance out the base 4- charge. In other silicate structures, different combinations of elements are required to balance out the resultant negative charge. It is common for the Si to be substituted by Al because of similarity in ionic radius and charge; in those cases, the [AlO] tetrahedra form the same structures as do the unsubstituted tetrahedra, but their charge-balancing requirements are different.\n\nThe degree of polymerization can be described by both the structure formed and how many tetrahedral corners (or coordinating oxygens) are shared (for aluminium and silicon in tetrahedral sites). Orthosilicates (or nesosilicates) have no linking of polyhedra, thus tetrahedra share no corners. Disilicates (or sorosilicates) have two tetrahedra sharing one oxygen atom. Inosilicates are chain silicates; single-chain silicates have two shared corners, whereas double-chain silicates have two or three shared corners. In phyllosilicates, a sheet structure is formed which requires three shared oxygens; in the case of double-chain silicates, some tetrahedra must share two corners instead of three as otherwise a sheet structure would result. Framework silicates, or tectosilicates, have tetrahedra that share all four corners. The ring silicates, or cyclosilicates, only need tetrahedra to share two corners to form the cyclical structure.\n\nThe silicate subclasses are described below in order of decreasing polymerization.\n\nTectosilicates, also known as framework silicates, have the highest degree of polymerization. With all corners of a tetrahedra shared, the silicon:oxygen ratio becomes 1:2. Examples are quartz, the feldspars, feldspathoids, and the zeolites. Framework silicates tend to be particularly chemically stable as a result of strong covalent bonds.\n\nForming 12% of the Earth's crust, quartz (SiO) is the most abundant mineral species. It is characterized by its high chemical and physical resistivity. Quartz has several polymorphs, including tridymite and cristobalite at high temperatures, high-pressure coesite, and ultra-high pressure stishovite. The latter mineral can only be formed on Earth by meteorite impacts, and its structure has been composed so much that it had changed from a silicate structure to that of rutile (TiO). The silica polymorph that is most stable at the Earth's surface is α-quartz. Its counterpart, β-quartz, is present only at high temperatures and pressures (changes to α-quartz below 573 °C at 1 bar). These two polymorphs differ by a \"kinking\" of bonds; this change in structure gives β-quartz greater symmetry than α-quartz, and they are thus also called high quartz (β) and low quartz (α).\n\nFeldspars are the most abundant group in the Earth's crust, at about 50%. In the feldspars, Al substitutes for Si, which creates a charge imbalance that must be accounted for by the addition of cations. The base structure becomes either [AlSiO] or [AlSiO] There are 22 mineral species of feldspars, subdivided into two major subgroups – alkali and plagioclase – and two less common groups – celsian and banalsite. The alkali feldspars are most commonly in a series between potassium-rich orthoclase and sodium-rich albite; in the case of plagioclase, the most common series ranges from albite to calcium-rich anorthite. Crystal twinning is common in feldspars, especially polysynthetic twins in plagioclase and Carlsbad twins in alkali feldspars. If the latter subgroup cools slowly from a melt, it forms exsolution lamellae because the two components – orthoclase and albite – are unstable in solid solution. Exsolution can be on a scale from microscopic to readily observable in hand-sample; perthitic texture forms when Na-rich feldspar exsolve in a K-rich host. The opposite texture (antiperthitic), where K-rich feldspar exsolves in a Na-rich host, is very rare.\n\nFeldspathoids are structurally similar to feldspar, but differ in that they form in Si-deficient conditions, which allows for further substitution by Al. As a result, feldspathoids cannot be associated with quartz. A common example of a feldspathoid is nepheline ((Na, K)AlSiO); compared to alkali feldspar, nepheline has an AlO:SiO ratio of 1:2, as opposed to 1:6 in the feldspar. Zeolites often have distinctive crystal habits, occurring in needles, plates, or blocky masses. They form in the presence of water at low temperatures and pressures, and have channels and voids in their structure. Zeolites have several industrial applications, especially in waste water treatment.\n\nPhyllosilicates consist of sheets of polymerized tetrahedra. They are bound at three oxygen sites, which gives a characteristic silicon:oxygen ratio of 2:5. Important examples include the mica, chlorite, and the kaolinite-serpentine groups. The sheets are weakly bound by van der Waals forces or hydrogen bonds, which causes a crystallographic weakness, in turn leading to a prominent basal cleavage among the phyllosilicates. In addition to the tetrahedra, phyllosilicates have a sheet of octahedra (elements in six-fold coordination by oxygen) that balance out the basic tetrahedra, which have a negative charge (e.g. [SiO]) These tetrahedra (T) and octahedra (O) sheets are stacked in a variety of combinations to create phyllosilicate groups. Within an octahedral sheet, there are three octahedral sites in a unit structure; however, not all of the sites may be occupied. In that case, the mineral is termed dioctahedral, whereas in other case it is termed trioctahedral.\n\nThe kaolinite-serpentine group consists of T-O stacks (the 1:1 clay minerals); their hardness ranges from 2 to 4, as the sheets are held by hydrogen bonds. The 2:1 clay minerals (pyrophyllite-talc) consist of T-O-T stacks, but they are softer (hardness from 1 to 2), as they are instead held together by van der Waals forces. These two groups of minerals are subgrouped by octahedral occupation; specifically, kaolinite and pyrophyllite are dioctahedral whereas serpentine and talc trioctahedral.\n\nMicas are also T-O-T-stacked phyllosilicates, but differ from the other T-O-T and T-O-stacked subclass members in that they incorporate aluminium into the tetrahedral sheets (clay minerals have Al in octahedral sites). Common examples of micas are muscovite, and the biotite series. The chlorite group is related to mica group, but a brucite-like (Mg(OH)) layer between the T-O-T stacks.\n\nBecause of their chemical structure, phyllosilicates typically have flexible, elastic, transparent layers that are electrical insulators and can be split into very thin flakes. Micas can be used in electronics as insulators, in construction, as optical filler, or even cosmetics. Chrysotile, a species of serpentine, is the most common mineral species in industrial asbestos, as it is less dangerous in terms of health than the amphibole asbestos.\n\nInosilicates consist of tetrahedra repeatedly bonded in chains. These chains can be single, where a tetrahedron is bound to two others to form a continuous chain; alternatively, two chains can be merged to create double-chain silicates. Single-chain silicates have a silicon:oxygen ratio of 1:3 (e.g. [SiO]), whereas the double-chain variety has a ratio of 4:11, e.g. [SiO]. Inosilicates contain two important rock-forming mineral groups; single-chain silicates are most commonly pyroxenes, while double-chain silicates are often amphiboles. Higher-order chains exist (e.g. three-member, four-member, five-member chains, etc.) but they are rare.\n\nThe pyroxene group consists of 21 mineral species. Pyroxenes have a general structure formula of XY(SiO), where X is an octahedral site, while Y can vary in coordination number from six to eight. Most varieties of pyroxene consist of permutations of Ca, Fe and Mg to balance the negative charge on the backbone. Pyroxenes are common in the Earth's crust (about 10%) and are a key constituent of mafic igneous rocks.\n\nAmphiboles have great variability in chemistry, described variously as a \"mineralogical garbage can\" or a \"mineralogical shark swimming a sea of elements\". The backbone of the amphiboles is the [SiO]; it is balanced by cations in three possible positions, although the third position is not always used, and one element can occupy both remaining ones. Finally, the amphiboles are usually hydrated, that is, they have a hydroxyl group ([OH]), although it can be replaced by a fluoride, a chloride, or an oxide ion. Because of the variable chemistry, there are over 80 species of amphibole, although variations, as in the pyroxenes, most commonly involve mixtures of Ca, Fe and Mg. Several amphibole mineral species can have an asbestiform crystal habit. These asbestos minerals form long, thin, flexible, and strong fibres, which are electrical insulators, chemically inert and heat-resistant; as such, they have several applications, especially in construction materials. However, asbestos are known carcinogens, and cause various other illnesses, such as asbestosis; amphibole asbestos (anthophyllite, tremolite, actinolite, grunerite, and riebeckite) are considered more dangerous than chrysotile serpentine asbestos.\n\nCyclosilicates, or ring silicates, have a ratio of silicon to oxygen of 1:3. Six-member rings are most common, with a base structure of [SiO]; examples include the tourmaline group and beryl. Other ring structures exist, with 3, 4, 8, 9, 12 having been described. Cyclosilicates tend to be strong, with elongated, striated crystals.\n\nTourmalines have a very complex chemistry that can be described by a general formula XYZ(BO)TOVW. The TO is the basic ring structure, where T is usually Si, but substitutable by Al or B. Tourmalines can be subgrouped by the occupancy of the X site, and from there further subdivided by the chemistry of the W site. The Y and Z sites can accommodate a variety of cations, especially various transition metals; this variability in structural transition metal content gives the tourmaline group greater variability in colour. Other cyclosilicates include beryl, AlBeSiO, whose varieties include the gemstones emerald (green) and aquamarine (bluish). Cordierite is structurally similar to beryl, and is a common metamorphic mineral.\n\nSorosilicates, also termed disilicates, have tetrahedron-tetrahedron bonding at one oxygen, which results in a 2:7 ratio of silicon to oxygen. The resultant common structural element is the [SiO] group. The most common disilicates by far are members of the epidote group. Epidotes are found in variety of geologic settings, ranging from mid-ocean ridge to granites to metapelites. Epidotes are built around the structure [(SiO)(SiO)] structure; for example, the mineral \"species\" epidote has calcium, aluminium, and ferric iron to charge balance: CaAl(Fe, Al)(SiO)(SiO)O(OH). The presence of iron as Fe and Fe helps understand oxygen fugacity, which in turn is a significant factor in petrogenesis.\n\nOther examples of sorosilicates include lawsonite, a metamorphic mineral forming in the blueschist facies (subduction zone setting with low temperature and high pressure), vesuvianite, which takes up a significant amount of calcium in its chemical structure.\n\nOrthosilicates consist of isolated tetrahedra that are charge-balanced by other cations. Also termed nesosilicates, this type of silicate has a silicon:oxygen ratio of 1:4 (e.g. SiO). Typical orthosilicates tend to form blocky equant crystals, and are fairly hard. Several rock-forming minerals are part of this subclass, such as the aluminosilicates, the olivine group, and the garnet group.\n\nThe aluminosilicates –bkyanite, andalusite, and sillimanite, all AlSiO – are structurally composed of one [SiO] tetrahedron, and one Al in octahedral coordination. The remaining Al can be in six-fold coordination (kyanite), five-fold (andalusite) or four-fold (sillimanite); which mineral forms in a given environment is depend on pressure and temperature conditions. In the olivine structure, the main olivine series of (Mg, Fe)SiO consist of magnesium-rich forsterite and iron-rich fayalite. Both iron and magnesium are in octahedral by oxygen. Other mineral species having this structure exist, such as tephroite, MnSiO. The garnet group has a general formula of XY(SiO), where X is a large eight-fold coordinated cation, and Y is a smaller six-fold coordinated cation. There are six ideal endmembers of garnet, split into two group. The pyralspite garnets have Al in the Y position: pyrope (MgAl(SiO)), almandine (FeAl(SiO)), and spessartine (MnAl(SiO)). The ugrandite garnets have Ca in the X position: uvarovite (CaCr(SiO)), grossular (CaAl(SiO)) and andradite (CaFe(SiO)). While there are two subgroups of garnet, solid solutions exist between all six end-members.\n\nOther orthosilicates include zircon, staurolite, and topaz. Zircon (ZrSiO) is useful in geochronology as the Zr can be substituted by U; furthermore, because of its very resistant structure, it is difficult to reset it as a chronometer. Staurolite is a common metamorphic intermediate-grade index mineral. It has a particularly complicated crystal structure that was only fully described in 1986. Topaz (AlSiO(F, OH), often found in granitic pegmatites associated with tourmaline, is a common gemstone mineral.\n\nNative elements are those that are not chemically bonded to other elements. This mineral group includes native metals, semi-metals, and non-metals, and various alloys and solid solutions. The metals are held together by metallic bonding, which confers distinctive physical properties such as their shiny metallic lustre, ductility and malleability, and electrical conductivity. Native elements are subdivided into groups by their structure or chemical attributes.\n\nThe gold group, with a cubic close-packed structure, includes metals such as gold, silver, and copper. The platinum group is similar in structure to the gold group. The iron-nickel group is characterized by several iron-nickel alloy species. Two examples are kamacite and taenite, which are found in iron meteorites; these species differ by the amount of Ni in the alloy; kamacite has less than 5–7% nickel and is a variety of native iron, whereas the nickel content of taenite ranges from 7–37%. Arsenic group minerals consist of semi-metals, which have only some metallic traits; for example, they lack the malleability of metals. Native carbon occurs in two allotropes, graphite and diamond; the latter forms at very high pressure in the mantle, which gives it a much stronger structure than graphite.\n\nThe sulfide minerals are chemical compounds of one or more metals or semimetals with a sulfur; tellurium, arsenic, or selenium can substitute for the sulfur. Sulfides tend to be soft, brittle minerals with a high specific gravity. Many powdered sulfides, such as pyrite, have a sulfurous smell when powdered. Sulfides are susceptible to weathering, and many readily dissolve in water; these dissolved minerals can be later redeposited, which creates enriched secondary ore deposits. Sulfides are classified by the ratio of the metal or semimetal to the sulfur, such as M:S equal to 2:1, or 1:1. Many sulfide minerals are economically important as metal ores; examples include sphalerite (ZnS), an ore of zinc, galena (PbS), an ore of lead, cinnabar (HgS), an ore of mercury, and molybdenite (MoS, an ore of molybdenum. Pyrite (FeS), is the most commonly occurring sulfide, and can be found in most geological environments. It is not, however, an ore of iron, but can be instead oxidized to produce sulfuric acid. Related to the sulfides are the rare sulfosalts, in which a metallic element is bonded to sulfur and a semimetal such as antimony, arsenic, or bismuth. Like the sulfides, sulfosalts are typically soft, heavy, and brittle minerals.\n\nOxide minerals are divided into three categories: simple oxides, hydroxides, and multiple oxides. Simple oxides are characterized by O as the main anion and primarily ionic bonding. They can be further subdivided by the ratio of oxygen to the cations. The periclase group consists of minerals with a 1:1 ratio. Oxides with a 2:1 ratio include cuprite (CuO) and water ice. Corundum group minerals have a 2:3 ratio, and includes minerals such as corundum (AlO), and hematite (FeO). Rutile group minerals have a ratio of 1:2; the eponymous species, rutile (TiO) is the chief ore of titanium; other examples include cassiterite (SnO; ore of tin), and pyrolusite (MnO; ore of manganese). In hydroxides, the dominant anion is the hydroxyl ion, OH. Bauxites are the chief aluminium ore, and are a heterogeneous mixture of the hydroxide minerals diaspore, gibbsite, and bohmite; they form in areas with a very high rate of chemical weathering (mainly tropical conditions). Finally, multiple oxides are compounds of two metals with oxygen. A major group within this class are the spinels, with a general formula of XYO. Examples of species include spinel (MgAlO), chromite (FeCrO), and magnetite (FeO). The latter is readily distinguishable by its strong magnetism, which occurs as it has iron in two oxidation states (FeFeO), which makes it a multiple oxide instead of a single oxide.\n\nThe halide minerals are compounds in which a halogen (fluorine, chlorine, iodine, or bromine) is the main anion. These minerals tend to be soft, weak, brittle, and water-soluble. Common examples of halides include halite (NaCl, table salt), sylvite (KCl), fluorite (CaF). Halite and sylvite commonly form as evaporites, and can be dominant minerals in chemical sedimentary rocks. Cryolite, NaAlF, is a key mineral in the extraction of aluminium from bauxites; however, as the only significant occurrence at Ivittuut, Greenland, in a granitic pegmatite, was depleted, synthetic cryolite can be made from fluorite.\n\nThe carbonate minerals are those in which the main anionic group is carbonate, [CO]. Carbonates tend to be brittle, many have rhombohedral cleavage, and all react with acid. Due to the last characteristic, field geologists often carry dilute hydrochloric acid to distinguish carbonates from non-carbonates. The reaction of acid with carbonates, most commonly found as the polymorph calcite and aragonite (CaCO), relates to the dissolution and precipitation of the mineral, which is a key in the formation of limestone caves, features within them such as stalactite and stalagmites, and karst landforms. Carbonates are most often formed as biogenic or chemical sediments in marine environments. The carbonate group is structurally a triangle, where a central C cation is surrounded by three O anions; different groups of minerals form from different arrangements of these triangles. The most common carbonate mineral is calcite, which is the primary constituent of sedimentary limestone and metamorphic marble. Calcite, CaCO, can have a high magnesium impurity. Under high-Mg conditions, its polymorph aragonite will form instead; the marine geochemistry in this regard can be described as an aragonite or calcite sea, depending on which mineral preferentially forms. Dolomite is a double carbonate, with the formula CaMg(CO). Secondary dolomitization of limestone is common, in which calcite or aragonite are converted to dolomite; this reaction increases pore space (the unit cell volume of dolomite is 88% that of calcite), which can create a reservoir for oil and gas. These two mineral species are members of eponymous mineral groups: the calcite group includes carbonates with the general formula XCO, and the dolomite group constitutes minerals with the general formula XY(CO).\n\nThe sulfate minerals all contain the sulfate anion, [SO]. They tend to be transparent to translucent, soft, and many are fragile. Sulfate minerals commonly form as evaporites, where they precipitate out of evaporating saline waters. Sulfates can also be found in hydrothermal vein systems associated with sulfides, or as oxidation products of sulfides. Sulfates can be subdivided into anhydrous and hydrous minerals. The most common hydrous sulfate by far is gypsum, CaSO⋅2HO. It forms as an evaporite, and is associated with other evaporites such as calcite and halite; if it incorporates sand grains as it crystallizes, gypsum can form desert roses. Gypsum has very low thermal conductivity and maintains a low temperature when heated as it loses that heat by dehydrating; as such, gypsum is used as an insulator in materials such as plaster and drywall. The anhydrous equivalent of gypsum is anhydrite; it can form directly from seawater in highly arid conditions. The barite group has the general formula XSO, where the X is a large 12-coordinated cation. Examples include barite (BaSO), celestine (SrSO), and anglesite (PbSO); anhydrite is not part of the barite group, as the smaller Ca is only in eight-fold coordination.\n\nThe phosphate minerals are characterized by the tetrahedral [PO] unit, although the structure can be generalized, and phosphorus is replaced by antimony, arsenic, or vanadium. The most common phosphate is the apatite group; common species within this group are fluorapatite (Ca(PO)F), chlorapatite (Ca(PO)Cl) and hydroxylapatite (Ca(PO)(OH)). Minerals in this group are the main crystalline constituents of teeth and bones in vertebrates. The relatively abundant monazite group has a general structure of ATO, where T is phosphorus or arsenic, and A is often a rare-earth element (REE). Monazite is important in two ways: first, as a REE \"sink\", it can sufficiently concentrate these elements to become an ore; secondly, monazite group elements can incorporate relatively large amounts of uranium and thorium, which can be used in monazite geochronology to date the rock based on the decay of the U and Th to lead.\n\nThe Strunz classification includes a class for . These rare compounds contain organic carbon, but can be formed by a geologic process. For example, whewellite, CaCO⋅HO is an oxalate that can be deposited in hydrothermal ore veins. While hydrated calcium oxalate can be found in coal seams and other sedimentary deposits involving organic matter, the hydrothermal occurrence is not considered to be related to biological activity.\n\nIt has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on the planet Mars. Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.\n\nOn January 24, 2014, NASA reported that current studies by the \"Curiosity\" and \"Opportunity\" rovers on Mars will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.\n\n\n\n"}
{"id": "35659147", "url": "https://en.wikipedia.org/wiki?curid=35659147", "title": "Patterns in nature", "text": "Patterns in nature\n\nPatterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.\n\nIn the 19th century, Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, British mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. Hungarian biologist Aristid Lindenmayer and French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.\n\nMathematics, physics and chemistry can explain patterns in nature at different levels. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.\n\nEarly Greek philosophers attempted to explain order in nature, anticipating modern concepts. Plato (c. 427 – c. 347 BC) — looking only at his work on natural patterns — argued for the existence of universals. He considered these to consist of ideal forms ( \"eidos\": \"form\") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect mathematical circle. Pythagoras explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles to an extent anticipated Darwin's evolutionary explanation for the structures of organisms.\n\nIn 1202, Leonardo Fibonacci (c. 1170 – c. 1250) introduced the Fibonacci number sequence to the western world with his book \"Liber Abaci\". Fibonacci gave an (unrealistic) biological example, on the growth in numbers of a theoretical rabbit population.\n\nIn 1658, the English physician and philosopher Sir Thomas Browne discussed \"how Nature Geometrizeth\" in \"The Garden of Cyrus\", citing Pythagorean numerology involving the number 5, and the Platonic form of the quincunx pattern. The discourse's central chapter features examples and observations of the quincunx in botany.\n\nIn 1917, D'Arcy Wentworth Thompson (1860–1948) published his book \"On Growth and Form\". His description of phyllotaxis and the Fibonacci sequence, the mathematical relationships in the spiral growth patterns of plants, is classic. He showed that simple equations could describe all the apparently complex spiral growth patterns of animal horns and mollusc shells.\n\nThe Belgian physicist Joseph Plateau (1801–1883) formulated the mathematical problem of the existence of a minimal surface with a given boundary, which is now named after him. He studied soap films intensively, formulating Plateau's laws which describe the structures formed by films in foams.\n\nThe German psychologist Adolf Zeising (1810–1876) claimed that the golden ratio was expressed in the arrangement of plant parts, in the skeletons of animals and the branching patterns of their veins and nerves, as well as in the geometry of crystals.\n\nErnst Haeckel (1834–1919) painted beautiful illustrations of marine organisms, in particular Radiolaria, emphasising their symmetry to support his faux-Darwinian theories of evolution.\n\nThe American photographer Wilson Bentley (1865–1931) took the first micrograph of a snowflake in 1885.\nIn 1952, Alan Turing (1912–1954), better known for his work on computing and codebreaking, wrote \"The Chemical Basis of Morphogenesis\", an analysis of the mechanisms that would be needed to create patterns in living organisms, in the process called morphogenesis. He predicted oscillating chemical reactions, in particular the Belousov–Zhabotinsky reaction. These activator-inhibitor mechanisms can, Turing suggested, generate patterns (dubbed \"Turing patterns\") of stripes and spots in animals, and contribute to the spiral patterns seen in plant phyllotaxis.\n\nIn 1968, the Hungarian theoretical biologist Aristid Lindenmayer (1925–1989) developed the L-system, a formal grammar which can be used to model plant growth patterns in the style of fractals. L-systems have an alphabet of symbols that can be combined using production rules to build larger strings of symbols, and a mechanism for translating the generated strings into geometric structures. In 1975, after centuries of slow development of the mathematics of patterns by Gottfried Leibniz, Georg Cantor, Helge von Koch, Wacław Sierpiński and others, Benoît Mandelbrot wrote a famous paper, \"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\", crystallising mathematical thought into the concept of the fractal.\n\nLiving things like orchids, hummingbirds, and the peacock's tail have abstract designs with a beauty of form, pattern and colour that artists struggle to match. The beauty that people perceive in nature has causes at different levels, notably in the mathematics that governs what patterns can physically form, and among living things in the effects of natural selection, that govern how patterns evolve.\n\nMathematics seeks to discover and explain abstract patterns or regularities of all kinds.\nVisual patterns in nature find explanations in chaos theory, fractals, logarithmic spirals, topology and other mathematical patterns. For example, L-systems form convincing models of different patterns of tree growth.\nThe laws of physics apply the abstractions of mathematics to the real world, often as if it were perfect. For example, a crystal is perfect when it has no structural defects such as dislocations and is fully symmetric. Exact mathematical perfection can only approximate real objects. Visible patterns in nature are governed by physical laws; for example, meanders can be explained using fluid dynamics.\n\nIn biology, natural selection can cause the development of patterns in living things for several reasons, including camouflage, sexual selection, and different kinds of signalling, including mimicry and cleaning symbiosis. In plants, the shapes, colours, and patterns of insect-pollinated flowers like the lily have evolved to attract insects such as bees. Radial patterns of colours and stripes, some visible only in ultraviolet light serve as nectar guides that can be seen at a distance.\n\nSymmetry is pervasive in living things. Animals mainly have bilateral or mirror symmetry, as do the leaves of plants and some flowers such as orchids. Plants often have radial or rotational symmetry, as do many flowers and some groups of animals such as sea anemones. Fivefold symmetry is found in the echinoderms, the group that includes starfish, sea urchins, and sea lilies.\n\nAmong non-living things, snowflakes have striking sixfold symmetry; each flake's structure forms a record of the varying conditions during its crystallization, with nearly the same pattern of growth on each of its six arms. Crystals in general have a variety of symmetries and crystal habits; they can be cubic or octahedral, but true crystals cannot have fivefold symmetry (unlike quasicrystals). Rotational symmetry is found at different scales among non-living things, including the crown-shaped splash pattern formed when a drop falls into a pond, and both the spheroidal shape and rings of a planet like Saturn.\n\nSymmetry has a variety of causes. Radial symmetry suits organisms like sea anemones whose adults do not move: food and threats may arrive from any direction. But animals that move in one direction necessarily have upper and lower sides, head and tail ends, and therefore a left and a right. The head becomes specialised with a mouth and sense organs (cephalisation), and the body becomes bilaterally symmetric (though internal organs need not be). More puzzling is the reason for the fivefold (pentaradiate) symmetry of the echinoderms. Early echinoderms were bilaterally symmetrical, as their larvae still are. Sumrall and Wray argue that the loss of the old symmetry had both developmental and ecological causes.\n\nFractals are infinitely self-similar, iterated mathematical constructs having fractal dimension. Infinite iteration is not possible in nature so all 'fractal' patterns are only approximate. For example, the leaves of ferns and umbellifers (Apiaceae) are only self-similar (pinnate) to 2, 3 or 4 levels. Fern-like growth patterns occur in plants and in animals including bryozoa, corals, hydrozoa like the air fern, \"Sertularia argentea\", and in non-living things, notably electrical discharges. Lindenmayer system fractals can model different patterns of tree growth by varying a small number of parameters including branching angle, distance between nodes or branch points (internode length), and number of branches per branch point.\n\nFractal-like patterns occur widely in nature, in phenomena as diverse as clouds, river networks, geologic fault lines, mountains, coastlines, animal coloration, snow flakes, crystals, blood vessel branching, actin cytoskeleton, and ocean waves.\n\nSpirals are common in plants and in some animals, notably molluscs. For example, in the nautilus, a cephalopod mollusc, each chamber of its shell is an approximate copy of the next one, scaled by a constant factor and arranged in a logarithmic spiral. Given a modern understanding of fractals, a growth spiral can be seen as a special case of self-similarity.\n\nPlant spirals can be seen in phyllotaxis, the arrangement of leaves on a stem, and in the arrangement (parastichy) of other parts as in composite flower heads and seed heads like the sunflower or fruit structures like the pineapple and snake fruit, as well as in the pattern of scales in pine cones, where multiple spirals run both clockwise and anticlockwise. These arrangements have explanations at different levels – mathematics, physics, chemistry, biology – each individually correct, but all necessary together. Phyllotaxis spirals can be generated mathematically from Fibonacci ratios: the Fibonacci sequence runs 1, 1, 2, 3, 5, 8, 13... (each subsequent number being the sum of the two preceding ones). For example, when leaves alternate up a stem, one rotation of the spiral touches two leaves, so the pattern or ratio is 1/2. In hazel the ratio is 1/3; in apricot it is 2/5; in pear it is 3/8; in almond it is 5/13. In disc phyllotaxis as in the sunflower and daisy, the florets are arranged in Fermat's spiral with Fibonacci numbering, at least when the flowerhead is mature so all the elements are the same size. Fibonacci ratios approximate the golden angle, 137.508°, which governs the curvature of Fermat's spiral.\n\nFrom the point of view of physics, spirals are lowest-energy configurations which emerge spontaneously through self-organizing processes in dynamic systems. From the point of view of chemistry, a spiral can be generated by a reaction-diffusion process, involving both activation and inhibition. Phyllotaxis is controlled by proteins that manipulate the concentration of the plant hormone auxin, which activates meristem growth, alongside other mechanisms to control the relative angle of buds around the stem. From a biological perspective, arranging leaves as far apart as possible in any given space is favoured by natural selection as it maximises access to resources, especially sunlight for photosynthesis.\n\nIn mathematics, a dynamical system is chaotic if it is (highly) sensitive to initial conditions (the so-called \"butterfly effect\"), which requires the mathematical properties of topological mixing and dense periodic orbits.\n\nAlongside fractals, chaos theory ranks as an essentially universal influence on patterns in nature. There is a relationship between chaos and fractals—the \"strange attractors\" in chaotic systems have a fractal dimension. Some cellular automata, simple sets of mathematical rules that generate patterns, have chaotic behaviour, notably Stephen Wolfram's Rule 30.\n\nVortex streets are zigzagging patterns of whirling vortices created by the unsteady separation of flow of a fluid, most often air or water, over obstructing objects. Smooth (laminar) flow starts to break up when the size of the obstruction or the velocity of the flow become large enough compared to the viscosity of the fluid.\n\nMeanders are sinuous bends in rivers or other channels, which form as a fluid, most often water, flows around bends. As soon as the path is slightly curved, the size and curvature of each loop increases as helical flow drags material like sand and gravel across the river to the inside of the bend. The outside of the loop is left clean and unprotected, so erosion accelerates, further increasing the meandering in a powerful positive feedback loop.\n\nWaves are disturbances that carry energy as they move. Mechanical waves propagate through a medium – air or water, making it oscillate as they pass by. Wind waves are sea surface waves that create the characteristic chaotic pattern of any large body of water, though their statistical behaviour can be predicted with wind wave models. As waves in water or wind pass over sand, they create patterns of ripples. When winds blow over large bodies of sand, they create dunes, sometimes in extensive dune fields as in the Taklamakan desert. Dunes may form a range of patterns including crescents, very long straight lines, stars, domes, parabolas, and longitudinal or seif ('sword') shapes.\n\nBarchans or crescent dunes are produced by wind acting on desert sand; the two horns of the crescent and the slip face point downwind. Sand blows over the upwind face, which stands at about 15 degrees from the horizontal, and falls onto the slip face, where it accumulates up to the angle of repose of the sand, which is about 35 degrees. When the slip face exceeds the angle of repose, the sand avalanches, which is a nonlinear behaviour: the addition of many small amounts of sand causes nothing much to happen, but then the addition of a further small amount suddenly causes a large amount to avalanche. Apart from this nonlinearity, barchans behave rather like solitary waves.\n\nA soap bubble forms a sphere, a surface with minimal area — the smallest possible surface area for the volume enclosed. Two bubbles together form a more complex shape: the outer surfaces of both bubbles are spherical; these surfaces are joined by a third spherical surface as the smaller bubble bulges slightly into the larger one.\n\nA foam is a mass of bubbles; foams of different materials occur in nature. Foams composed of soap films obey Plateau's laws, which require three soap films to meet at each edge at 120° and four soap edges to meet at each vertex at the tetrahedral angle of about 109.5°. Plateau's laws further require films to be smooth and continuous, and to have a constant average curvature at every point. For example, a film may remain nearly flat on average by being curved up in one direction (say, left to right) while being curved downwards in another direction (say, front to back). Structures with minimal surfaces can be used as tents. Lord Kelvin identified the problem of the most efficient way to pack cells of equal volume as a foam in 1887; his solution uses just one solid, the bitruncated cubic honeycomb with very slightly curved faces to meet Plateau's laws. No better solution was found until 1993 when Denis Weaire and Robert Phelan proposed the Weaire–Phelan structure; the Beijing National Aquatics Center adapted the structure for their outer wall in the 2008 Summer Olympics.\n\nAt the scale of living cells, foam patterns are common; radiolarians, sponge spicules, silicoflagellate exoskeletons and the calcite skeleton of a sea urchin, \"Cidaris rugosa\", all resemble mineral casts of Plateau foam boundaries. The skeleton of the Radiolarian, \"Aulonia hexagona\", a beautiful marine form drawn by Ernst Haeckel, looks as if it is a sphere composed wholly of hexagons, but this is mathematically impossible. The Euler characteristic states that for any convex polyhedron, the number of faces plus the number of vertices (corners) equals the number of edges plus two. A result of this formula is that any closed polyhedron of hexagons has to include exactly 12 pentagons, like a soccer ball, Buckminster Fuller geodesic dome, or fullerene molecule. This can be visualised by noting that a mesh of hexagons is flat like a sheet of chicken wire, but each pentagon that is added forces the mesh to bend (there are fewer corners, so the mesh is pulled in).\n\nTessellations are patterns formed by repeating tiles all over a flat surface. There are 17 wallpaper groups of tilings. While common in art and design, exactly repeating tilings are less easy to find in living things. The cells in the paper nests of social wasps, and the wax cells in honeycomb built by honey bees are well-known examples. Among animals, bony fish, reptiles or the pangolin, or fruits like the salak are protected by overlapping scales or osteoderms, these form more-or-less exactly repeating units, though often the scales in fact vary continuously in size. Among flowers, the snake's head fritillary, \"Fritillaria meleagris\", have a tessellated chequerboard pattern on their petals. The structures of minerals provide good examples of regularly repeating three-dimensional arrays. Despite the hundreds of thousands of known minerals, there are rather few possible types of arrangement of atoms in a crystal, defined by crystal structure, crystal system, and point group; for example, there are exactly 14 Bravais lattices for the 7 lattice systems in three-dimensional space.\n\nCracks are linear openings that form in materials to relieve stress. When an elastic material stretches or shrinks uniformly, it eventually reaches its breaking strength and then fails suddenly in all directions, creating cracks with 120 degree joints, so three cracks meet at a node. Conversely, when an inelastic material fails, straight cracks form to relieve the stress. Further stress in the same direction would then simply open the existing cracks; stress at right angles can create new cracks, at 90 degrees to the old ones. Thus the pattern of cracks indicates whether the material is elastic or not. In a tough fibrous material like oak tree bark, cracks form to relieve stress as usual, but they do not grow long as their growth is interrupted by bundles of strong elastic fibres. Since each species of tree has its own structure at the levels of cell and of molecules, each has its own pattern of splitting in its bark.\n\nLeopards and ladybirds are spotted; angelfish and zebras are striped. These patterns have an evolutionary explanation: they have functions which increase the chances that the offspring of the patterned animal will survive to reproduce. One function of animal patterns is camouflage; for instance, a leopard that is harder to see catches more prey. Another function is signalling — for instance, a ladybird is less likely to be attacked by predatory birds that hunt by sight, if it has bold warning colours, and is also distastefully bitter or poisonous, or mimics other distasteful insects. A young bird may see a warning patterned insect like a ladybird and try to eat it, but it will only do this once; very soon it will spit out the bitter insect; the other ladybirds in the area will remain undisturbed. The young leopards and ladybirds, inheriting genes that somehow create spottedness, survive. But while these evolutionary and functional arguments explain why these animals need their patterns, they do not explain how the patterns are formed.\n\nAlan Turing, and later the mathematical biologist James Murray, described a mechanism that spontaneously creates spotted or striped patterns: a reaction-diffusion system. The cells of a young organism have genes that can be switched on by a chemical signal, a morphogen, resulting in the growth of a certain type of structure, say a darkly pigmented patch of skin. If the morphogen is present everywhere, the result is an even pigmentation, as in a black leopard. But if it is unevenly distributed, spots or stripes can result. Turing suggested that there could be feedback control of the production of the morphogen itself. This could cause continuous fluctuations in the amount of morphogen as it diffused around the body. A second mechanism is needed to create standing wave patterns (to result in spots or stripes): an inhibitor chemical that switches off production of the morphogen, and that itself diffuses through the body more quickly than the morphogen, resulting in an activator-inhibitor scheme. The Belousov–Zhabotinsky reaction is a non-biological example of this kind of scheme, a chemical oscillator.\n\nLater research has managed to create convincing models of patterns as diverse as zebra stripes, giraffe blotches, jaguar spots (medium-dark patches surrounded by dark broken rings) and ladybird shell patterns (different geometrical layouts of spots and stripes, see illustrations). Richard Prum's activation-inhibition models, developed from Turing's work, use six variables to account for the observed range of nine basic within-feather pigmentation patterns, from the simplest, a central pigment patch, via concentric patches, bars, chevrons, eye spot, pair of central spots, rows of paired spots and an array of dots. More elaborate models simulate complex feather patterns in the guineafowl \"Numida meleagris\" in which the individual feathers feature transitions from bars at the base to an array of dots at the far (distal) end. These require an oscillation created by two inhibiting signals, with interactions in both space and time.\n\nPatterns can form for other reasons in the vegetated landscape of tiger bush and fir waves. Tiger bush stripes occur on arid slopes where plant growth is limited by rainfall. Each roughly horizontal stripe of vegetation effectively collects the rainwater from the bare zone immediately above it. Fir waves occur in forests on mountain slopes after wind disturbance, during regeneration. When trees fall, the trees that they had sheltered become exposed and are in turn more likely to be damaged, so gaps tend to expand downwind. Meanwhile, on the windward side, young trees grow, protected by the wind shadow of the remaining tall trees. Natural patterns are sometimes formed by animals, as in the Mima mounds of the Northwestern United States and some other areas, which appear to be created over many years by the burrowing activities of pocket gophers, while the so-called fairy circles of Namibia appear to be created by the interaction of competing groups of sand termites, along with competition for water among the desert plants.\n\nIn permafrost soils with an active upper layer subject to annual freeze and thaw, patterned ground can form, creating circles, nets, ice wedge polygons, steps, and stripes. Thermal contraction causes shrinkage cracks to form; in a thaw, water fills the cracks, expanding to form ice when next frozen, and widening the cracks into wedges. These cracks may join up to form polygons and other shapes.\n\nThe fissured pattern that develops on vertebrate brains are caused by a physical process of constrained expansion dependent on two geometric parameters: relative tangential cortical expansion and relative thickness of the cortex. Similar patterns of gyri (peaks) and sulci (troughs) have been demonstrated in models of the brain starting from smooth, layered gels, with the patterns caused by compressive mechanical forces resulting from the expansion of the outer layer (representing the cortex) after the addition of a solvent. Numerical models in computer simulations support natural and experimental observations that the surface folding patterns increase in larger brains.\n\n\n\n\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "5308780", "url": "https://en.wikipedia.org/wiki?curid=5308780", "title": "Philo (journal)", "text": "Philo (journal)\n\nPhilo was a peer-reviewed academic journal published by the Society of Humanist Philosophers from 1998 to 2014. It is published at the Center for Inquiry with assistance from Purdue University. It focused on the discussion of philosophical issues from an explicitly naturalist perspective. The journal published articles, critical discussions, review essays, and book reviews in all fields of philosophy, and particularly invited work on the philosophical credentials of both naturalism and various supernaturalist alternatives to naturalism. Electronic access to the journal is provided by the Philosophy Documentation Center.\n\n\n"}
{"id": "55968776", "url": "https://en.wikipedia.org/wiki?curid=55968776", "title": "Reciprocal causation", "text": "Reciprocal causation\n\nIn biology, reciprocal causation arises when developing organisms are both \"products\" of evolution as well as \"causes\" of evolution. Formally, reciprocal causation exists when process A is a cause of process B and, subsequently, process B is a cause of process A, with this feedback potentially repeated. Some researchers, particularly advocates of the extended evolutionary synthesis, promote the view that causation in biological systems is inherently reciprocal.\n\nHarvard evolutionary biologist Ernst Mayr (1961) suggested that there are two fundamentally different types of causation in biology, ‘ultimate’ and ‘proximate’. Ultimate causes (e.g. natural selection) were seen as (i) providing historical accounts for the existence of an organism’s features, and (ii) explaining the function or ‘goal-directedness’ of living beings. In contrast, proximate causes (e.g. physiology) were seen as explaining how biological systems work. According to Mayr, the evolutionary sciences study ultimate causes and the rest of biology studies proximate causes. In some of his works, Mayr considered these domains autonomous:\n“The clarification of the biochemical mechanism by which the genetic program is translated into the phenotype tells us absolutely nothing about the steps by which natural selection has built up the particular genetic program.”\nMayr, 1980\nThere has been widespread acceptance of the proximate-ultimate dichotomy within the evolutionary sciences. However, many biologists, psychologists and philosophers have taken issue with Mayr’s corollary that the proximate-ultimate distinction implies that development is irrelevant to evolution. For instance, evolutionary biologist Mary Jane West-Eberhard writes:\n“The proximate-ultimate distinction has given rise to a new confusion, namely, a belief that proximate causes of phenotypic variation have nothing to do with ultimate, evolutionary explanation.”\nWest-Eberhard, 2003\nMayr’s position implied a unidirectional or linear conception of causation for both development and evolution: genotypes cause phenotypes (proximate causation), whilst through natural selection, changes in environments cause changes in organisms (ultimate causation). Reciprocal causation was proposed as an alternative to this linear characterization. (see also ) It emphasizes how causation cycles through biological systems recursively, allowing proximate causes to feed back and thereby feature in ultimate explanations. \n\nReciprocal causation features in several explanations within contemporary evolutionary biology, including sexual selection theory, coevolution, habitat selection, and frequency-dependent selection. In these examples, the source of selection on a trait coevolves with the trait itself, therefore causation is reciprocal and developmental processes potentially become relevant to evolutionary accounts. For instance, a peacock’s tail evolves through mating preferences in peahens, and those preferences coevolve with the male trait. The ‘ultimate explanation’ for the male trait is the prior existence of female preferences, proximately manifest in differential peahen mate choice decisions, whilst the ‘ultimate explanation’ for the peahens’ mating preferences is the prior existence of variation in the peacock’s tail associated with fitness. This example illustrates how reciprocal causation is not a rejection of the proximate-ultimate distinction itself, but instead a rejection of the implication that developmental processes should not feature in evolutionary explanations.\n\nReciprocal causation also applies in other domains of evolutionary biology. The extended evolutionary synthesis emphasizes how developmental events, including both the causal effects of environments on organisms (for instance, arising through developmental plasticity, or epigenetic inheritance) and the causal effects of organisms on environments (e.g. niche construction), can direct the course of evolution. Developmental plasticity, niche construction, extra-genetic forms of inheritance and developmental bias are recognized as playing evolutionary roles that cannot be reduced to natural selection of genetically encoded characters or strategies. Proximate causes are not autonomous from natural selection, but rather feed back to influence the rate and direction of adaptive evolution. This goes beyond the recognition that ontogenetic processes can impose constraints on the action of selection, or that proximate and ultimate processes interact. Rather, developmental processes are also seen as a source of evolutionary novelty, initiators of evolutionary episodes, and co-directors of patterns of evolutionary change.\n\nAcceptance or rejection of Mayr’s proximate-ultimate distinction may lie at the centre of several major debates within contemporary biology, concerning evo devo (evolutionary developmental biology), niche construction, cultural evolution, human cooperation, and the evolution of language. According to some biologists and philosophers, these disputes share a common pattern. On one side are researchers who consider that interaction and feedback processes traditionally characterized as ‘proximate’ have explanatory value for ‘ultimate’ evolutionary questions. Their concern is that the proximate-ultimate distinction has discouraged consideration of the manner in which developmental processes can set the evolutionary agenda, for instance, by introducing innovations, channeling phenotypic variation, or initiating evolutionary episodes through modifying selection pressures. One the other side are researchers who largely adopt Mayr’s stance with a clean separation of proximate and ultimate causation. For the latter, a failure to respect Mayr’s dichotomy is considered a sign of confusing an evolutionary explanation with a mechanistic explanation.\n"}
{"id": "44120129", "url": "https://en.wikipedia.org/wiki?curid=44120129", "title": "Revolving rivers", "text": "Revolving rivers\n\nRevolving rivers are a surprising, uncommon way of sand pile growth that can be found in a few sands around the world, but has been studied in detail only for one Cuban sand from a place called Santa Teresa (Pinar del Rio province).\n\nWhen pouring \"revolving\" sand on a flat surface from a fixed position, the growth of a conical pile does not occur by the common avalanche mechanism, where sand slides down the pile in a more or less random fashion. What happens in that a relatively thin \"river\" of flowing sand travels from the pouring point at the apex of the pile to its base, while the rest of the sand at the surface is static. In addition, the river \"revolves\" around the pile either in clockwise or counter-clockwise directions (looking from top) depending on the initial conditions of the experiment. Actually the river constitutes the \"cutting edge\" of a layer of sand that deposits as a helix on the conical pile, and makes it grow.\nFor small sandpiles, rivers are continuous, but they become intermittent\nfor larger piles.\n\nThe phenomenon was observed first by E. Altshuler at the University of Havana in 1995, but at the time he assumed that it was well known, and temporarily forgot about it. In 2000, being at the University of Houston, he told K. E. Bassler, who showed a vivid interest in the matter. Embarrassingly enough, Altshuler was unable to demonstrate it before Bassler using a random sand from Houston, so he had to send him a video from Cuba after his return to the island.\n\nOnce the existence of the strange phenomenon was confirmed for everyone, E. Altshuler and a number of collaborators performed a systematic study in Havana, which was then jointly published with Bassler.\nFurther work has been done to understand in more detail the\nphenomenon, and it has been found in other sands from different parts of the world. \nHowever, the connection between the physical, chemical (and possibly biological) properties of the grains in a specific sand, the nature of the inter-grain interactions, and the emergence of the revolving rivers is still an open question.\n\nSand from Santa Teresa is made of almost pure silicon dioxide grains with an average grain size of 0.2 mm approximately and no visible special features regarding grain shape. But in spite of its apparent simplicity, many puzzles still remain. For example, after many experiments one batch of sand may stop showing revolving rivers (just as singing sand eventually stops singing), which suggests that the decay is connected to certain properties of the surface of the grains that degrade by continued friction.\n\nVideos of the effect are available on YouTube.\n"}
{"id": "55695147", "url": "https://en.wikipedia.org/wiki?curid=55695147", "title": "Simmonsite", "text": "Simmonsite\n\nSimmonsite is a halide mineral, being a tertiary light metal fluoride, with formula NaLiAlF. It was first discovered in nature in Mineral County in the Gillis Range of Nevada, U.S.A. The mineral is found intergrown with cryolite, cryolithionite and trace elpasolite. The mineral has a monoclinic structure of P2 or P2/m. The ideal chemical formula for simmonsite is NaLiAlF. The mineral has a no visible cleavage, Mohs hardness of 2.53, a pale white color with a white streak and feels somewhat greasy. Simmonsite was named for the Professor of Mineralogy and Petrology at the University of New Orleans, William B. Simmons.\n\nSimmonsite, cryolite and cryolithionite form together as part of the alumino-fluoride aassemblage in a late stage breccia pipe structure that cross the Zapot amazonite-topaz-zinnwaldite pegmatite located in the described location above in Nevada. This assemblage occurs with around a third of each of theses phases. The three phases form an intergrowth of anhedral to subhedral grains that can be micrometers to approximately two or three hundred micrometers in size. The main anhydrous alumino-fluorides results in size almost 20cm or more across and weight more than 1kg. Another assemblage of alumino-fluorides exist, some that contain water or hydroxyl, pachnolite, weberite, thomsenollite, prosopite, ralstonite, as well as a second generation of cryolithionite.\n\nSimmonsite contains no observable euhedral crystals but it does have complex polysynthetic twinning. Its color is pale buff to cream in the hadn specimens accompanied by a white streak and has a greasy appearance. It exhibits a hardness of 2.5-3 on the Mohs hardness scale. There is no apparent cleavage, a subconchoidal fracture, no parting, and is not brittle. The measured density is 3.05(2) g/cm.\n\nThe optical properties of this mineral were discovered by using the standard optical spindle stage procedure in 589 nm light. It was determined to have birefringence with a Berek compensator. \n\nSimmonsite is a fairly simple mineral chemically as the only elements present in major or minor amounts are Na, Al, Li, and F. The empirical chemical formula for simmonsite is NaLiAlF. The four primary alumino-fluorides behave distinctly different under an electron beam with elpasolite and cryolithionite showing no beam damage with time, even though simmonsite did have some and cryolite having the greatest extent of damage.\n\nNatural simmonsite is monoclinic with P2 or P2/m. The unit cell dimensions are a=7.5006(6) Å, b= 7.474(1) Å, c= 7.503(1) Å, β = 90.847(9)°, V= 420.6(1) Å3, Z= 4. These dimensions are almost identical to those of babingtonite. Simmonsite has had a long history of it being difficult to understand as there have been previous studies trying to understand the crystallography. Errors were discovered in previous work by Holm and Holm(1970) that suggested it was monoclinic, B-centered, a=7.54 Å, b= 7.52 Å, c= 7.53 Å, β = 90.81°.\n\nList of Minerals\n"}
{"id": "1576445", "url": "https://en.wikipedia.org/wiki?curid=1576445", "title": "Sociological naturalism", "text": "Sociological naturalism\n\nSociological naturalism is a theory that states that the natural world and social world are roughly identical and governed by similar principles. Sociological naturalism, in sociological texts simply referred to as naturalism, can be traced back to the philosophical thinking of Auguste Comte in the 19th century, closely connected to positivism, which advocates use of the scientific method of the natural sciences in studying social sciences. It should not be identified too closely with Positivism, however, since whilst the latter advocates the use of controlled situations like experiments as sources of scientific information, naturalism insists that social processes should only be studied in their \"natural\" setting. A similar form of naturalism was applied to the scientific study of art and literature by Hippolyte Taine (see Race, milieu, and moment).\n\nContemporary sociologists do not generally dispute that social phenomena take place within the natural universe and, as such, are subject to natural constraints, such as the laws of physics. Up for debate is the nature of the distinctiveness of social phenomena as a subset of natural phenomena. Broad support exists for the antipositivist claim that crucial qualitative differences mean that one cannot explain social phenomena effectively using investigative tools or even standards of validity derived from other natural sciences. From this point of view, naturalism does not imply scientism. \n\nHowever, a classically positivist conflation of naturalism with scientism has not disappeared; this view is still dominant in some old and prestigious schools, such as the sociology departments at the University of Chicago in the United States, and McGill University in Montréal, Canada.\n\nMore recently, actor-network theory has analyzed the social construction of the nature/society distinction itself.\n\n"}
{"id": "7917758", "url": "https://en.wikipedia.org/wiki?curid=7917758", "title": "Software evolution", "text": "Software evolution\n\nSoftware evolution is the term used in software engineering (specifically software maintenance) to refer to the process of developing software initially, then repeatedly updating it for various reasons. \n\nFred Brooks, in his key book \"The Mythical Man-Month\", states that over 90% of the costs of a typical system arise in the maintenance phase, and that any successful piece of software will inevitably be maintained.\n\nIn fact, Agile methods stem from maintenance-like activities in and around web based technologies, where the bulk of the capability comes from frameworks and standards.\n\nSoftware maintenance address bug fixes and minor enhancements and software evolution focus on adaptation and migration.\nSoftware technologies will continue to develop. These changes will require new laws and theories to be created and justified. Some models as well would require additional aspects in developing future programs. Innovations and improvements do increase unexpected form of software development. The maintenance issues also would probably change as to adapt to the evolution of the future software. Software processes are themselves evolving, after going through learning and refinements, it is always improve their efficiency and effectiveness.\n\nThe need for software evolution comes from the fact that no one is able to predict how user requirements will evolve \"a priori\" . \nIn other words, the existing systems are never complete and continue to evolve. As they evolve, the complexity of the systems will grow unless there is a better solution available to solve these issues. The main objectives of software evolution are ensuring functional relevance, reliability and flexibility of the system. Software evolution can be fully manual (based on changes by software engineers), partially automated (e.g. using refactoring tools) or fully automated (with autonomous configuration or evolution). \n\nSoftware evolution has been greatly impacted by the Internet:\n\n\n\nE.B. Swanson initially identified the \nthree categories of maintenance: corrective, adaptive, and perfective. Four categories of software were then catalogued by Lientz and Swanson (1980).\nThese have since been updated and normalized internationally in the ISO/IEC 14764:2006:\n\n\nAll of the preceding take place when there is a known requirement for change.\n\nAlthough these categories were supplemented by many authors like Warren et al. (1999) and Chapin (2001), the ISO/IEC 14764:2006 international standard has kept the basic four categories.\n\nMore recently the description of software maintenance and evolution has been done using ontologies (Kitchenham et al. (1999), Deridder (2002), Vizcaíno (2003), Dias (2003), and Ruiz (2004)), which enrich the description of the many evolution activities.\n\nCurrent trends and practices are projected forward using a new model of software evolution called the staged model [1]. Staged model was introduced to replace conventional analysis which is less suitable for modern software development is rapid changing due to its difficulties of hard to contribute in software evolution. There are five distinct stages contribute in simple staged model (Initial development, Evolution, Servicing, Phase-out, and Close-down). \n\nProf. Meir M. Lehman, who worked at Imperial College London from 1972 to 2002, and his colleagues have identified a set of behaviours in the evolution of proprietary software. These behaviours (or observations) are known as Lehman's Laws, and there are eight of them:\n\n\nIt is worth mentioning that the applicability of all of these laws for all types of software systems has been studied by several researchers. For example, see a presentation by Nanjangud C Narendra where he describes a case study of an enterprise Agile project in the light of Lehman’s laws of software evolution. Some empirical observations coming from the study of open source software development appear to challenge some of the laws .\n\nThe laws predict that the need for functional change in a software system is inevitable, and not a consequence of incomplete or incorrect analysis of requirements or bad programming. They state that there are limits to what a software development team can achieve in terms of safely implementing changes and new functionality.\n\nMaturity Models specific to software evolution have been developed to improve processes, and help to ensure continuous rejuvenation of the software as it evolves iteratively.\n\nThe \"global process\" that is made by the many stakeholders (e.g. developers, users, their managers) has many feedback loops. The evolution speed is a function of the feedback loop structure and other characteristics of the global system. Process simulation techniques, such as system dynamics can be useful in understanding and managing such global process.\n\nSoftware evolution is not likely to be Darwinian, Lamarckian or Baldwinian, but an important phenomenon on its own. Given the increasing dependence on software at all levels of society and economy, the successful evolution of software is becoming increasingly critical. This is an important topic of research that hasn't received much attention.\n\nThe evolution of software, because of its rapid path in comparison to other man-made entities, was seen by Lehman as the \"fruit fly\" of the study of the evolution of artificial systems.\n\n\n\n"}
{"id": "37738", "url": "https://en.wikipedia.org/wiki?curid=37738", "title": "Soil", "text": "Soil\n\nSoil is a mixture of organic matter, minerals, gases, liquids, and organisms that together support life. Earth's body of soil is the pedosphere, which has four important functions: it is a medium for plant growth; it is a means of water storage, supply and purification; it is a modifier of Earth's atmosphere; it is a habitat for organisms; all of which, in turn, modify the soil.\n\nThe pedosphere interfaces with the lithosphere, the hydrosphere, the atmosphere, and the biosphere. The term \"pedolith\", used commonly to refer to the soil, translates to \"ground stone\". Soil consists of a solid phase of minerals and organic matter (the soil matrix), as well as a porous phase that holds gases (the soil atmosphere) and water (the soil solution). Accordingly, soils are often treated as a three-state system of solids, liquids, and gases.\n\nSoil is a product of the influence of climate, relief (elevation, orientation, and slope of terrain), organisms, and its parent materials (original minerals) interacting over time. It continually undergoes development by way of numerous physical, chemical and biological processes, which include weathering with associated erosion. Given its complexity and strong internal connectedness, it is considered an ecosystem by soil ecologists.\n\nMost soils have a dry bulk density (density of soil taking into account voids when dry) between 1.1 and 1.6 g/cm, while the soil particle density is much higher, in the range of 2.6 to 2.7 g/cm. Little of the soil of planet Earth is older than the Pleistocene and none is older than the Cenozoic, although fossilized soils are preserved from as far back as the Archean.\n\nSoil science has two basic branches of study: edaphology and pedology. Edaphology is concerned with the influence of soils on living things. Pedology is focused on the formation, description (morphology), and classification of soils in their natural environment. In engineering terms, soil is included in the broader concept of regolith, which also includes other loose material that lies above the bedrock, as can be found on the Moon and other celestial objects, as well. Soil is also commonly referred to as earth or dirt; some scientific definitions distinguish \"dirt\" from \"soil\" by restricting the former term specifically to the displaced soil.\n\nSoil is a major component of the Earth's ecosystem. The world's ecosystems are impacted in far-reaching ways by the processes carried out in the soil, from ozone depletion and global warming to rainforest destruction and water pollution. With respect to Earth's carbon cycle, soil is an important carbon reservoir, and it is potentially one of the most reactive to human disturbance and climate change. As the planet warms, it has been predicted that soils will add carbon dioxide to the atmosphere due to increased biological activity at higher temperatures, a positive feedback (amplification). This prediction has, however, been questioned on consideration of more recent knowledge on soil carbon turnover.\n\nSoil acts as an engineering medium, a habitat for soil organisms, a recycling system for nutrients and organic wastes, a regulator of water quality, a modifier of atmospheric composition, and a medium for plant growth, making it a critically important provider of ecosystem services. Since soil has a tremendous range of available niches and habitats, it contains most of the Earth's genetic diversity. A gram of soil can contain billions of organisms, belonging to thousands of species, mostly microbial and in the main still unexplored. Soil has a mean prokaryotic density of roughly 10 organisms per gram, whereas the ocean has no more than 10 procaryotic organisms per milliliter (gram) of seawater. Organic carbon held in soil is eventually returned to the atmosphere through the process of respiration carried out by heterotrophic organisms, but a substantial part is retained in the soil in the form of soil organic matter; tillage usually increases the rate of soil respiration, leading to the depletion of soil organic matter. Since plant roots need oxygen, ventilation is an important characteristic of soil. This ventilation can be accomplished via networks of interconnected soil pores, which also absorb and hold rainwater making it readily available for uptake by plants. Since plants require a nearly continuous supply of water, but most regions receive sporadic rainfall, the water-holding capacity of soils is vital for plant survival.\n\nSoils can effectively remove impurities, kill disease agents, and degrade contaminants, this latter property being called natural attenuation. Typically, soils maintain a net absorption of oxygen and methane and undergo a net release of carbon dioxide and nitrous oxide. Soils offer plants physical support, air, water, temperature moderation, nutrients, and protection from toxins. Soils provide readily available nutrients to plants and animals by converting dead organic matter into various nutrient forms.\n\nA typical soil is about 50% solids (45% mineral and 5% organic matter), and 50% voids (or pores) of which half is occupied by water and half by gas. The percent soil mineral and organic content can be treated as a constant (in the short term), while the percent soil water and gas content is considered highly variable whereby a rise in one is simultaneously balanced by a reduction in the other. The pore space allows for the infiltration and movement of air and water, both of which are critical for life existing in soil. Compaction, a common problem with soils, reduces this space, preventing air and water from reaching plant roots and soil organisms.\n\nGiven sufficient time, an undifferentiated soil will evolve a soil profile which consists of two or more layers, referred to as soil horizons, that differ in one or more properties such as in their texture, structure, density, porosity, consistency, temperature, color, and reactivity. The horizons differ greatly in thickness and generally lack sharp boundaries; their development is dependent on the type of parent material, the processes that modify those parent materials, and the soil-forming factors that influence those processes. The biological influences on soil properties are strongest near the surface, while the geochemical influences on soil properties increase with depth. Mature soil profiles typically include three basic master horizons: A, B, and C. The solum normally includes the A and B horizons. The living component of the soil is largely confined to the solum, and is generally more prominent in the A horizon.\n\nThe soil texture is determined by the relative proportions of the individual particles of sand, silt, and clay that make up the soil. The interaction of the individual mineral particles with organic matter, water, gases via biotic and abiotic processes causes those particles to flocculate (stick together) to form aggregates or peds. Where these aggregates can be identified, a soil can be said to be developed, and can be described further in terms of color, porosity, consistency, reaction (acidity), etc.\n\nWater is a critical agent in soil development due to its involvement in the dissolution, precipitation, erosion, transport, and deposition of the materials of which a soil is composed. The mixture of water and dissolved or suspended materials that occupy the soil pore space is called the soil solution. Since soil water is never pure water, but contains hundreds of dissolved organic and mineral substances, it may be more accurately called the soil solution. Water is central to the dissolution, precipitation and leaching of minerals from the soil profile. Finally, water affects the type of vegetation that grows in a soil, which in turn affects the development of the soil, a complex feedback which is exemplified in the dynamics of banded vegetation patterns in semi-arid regions.\n\nSoils supply plants with nutrients, most of which are held in place by particles of clay and organic matter (colloids) The nutrients may be adsorbed on clay mineral surfaces, bound within clay minerals (absorbed), or bound within organic compounds as part of the living organisms or dead soil organic matter. These bound nutrients interact with soil water to buffer the soil solution composition (attenuate changes in the soil solution) as soils wet up or dry out, as plants take up nutrients, as salts are leached, or as acids or alkalis are added.\n\nPlant nutrient availability is affected by soil pH, which is a measure of the hydrogen ion activity in the soil solution. Soil pH is a function of many soil forming factors, and is generally lower (more acid) where weathering is more advanced.\n\nMost plant nutrients, with the exception of nitrogen, originate from the minerals that make up the soil parent material. Some nitrogen originates from rain as dilute nitric acid and ammonia, but most of the nitrogen is available in soils as a result of nitrogen fixation by bacteria. Once in the soil-plant system, most nutrients are recycled through living organisms, plant and microbial residues (soil organic matter), mineral-bound forms, and the soil solution. Both living microorganisms and soil organic matter are of critical importance to this recycling, and thereby to soil formation and soil fertility. Microbial activity in soils may release nutrients from minerals or organic matter for use by plants and other microorganisms, sequester (incorporate) them into living cells, or cause their loss from the soil by volatilisation (loss to the atmosphere as gases) or leaching.\n\nThe history of the study of soil is intimately tied to humans' urgent need to provide food for themselves and forage for our animals. Throughout history, civilizations have prospered or declined as a function of the availability and productivity of their soils.\n\nThe Greek historian Xenophon (450–355 BCE) is credited with being the first to expound upon the merits of green-manuring crops: \"But then whatever weeds are upon the ground, being turned into earth, enrich the soil as much as dung.\"\n\nColumella's \"Husbandry,\" circa 60 CE, advocated the use of lime and that clover and alfalfa (green manure) should be turned under, and was used by 15 generations (450 years) under the Roman Empire until its collapse. From the fall of Rome to the French Revolution, knowledge of soil and agriculture was passed on from parent to child and as a result, crop yields were low. During the European Dark Ages, Yahya Ibn al-'Awwam's handbook, with its emphasis on irrigation, guided the people of North Africa, Spain and the Middle East; a translation of this work was finally carried to the southwest of the United States when under Spanish influence. Olivier de Serres, considered as the father of French agronomy, was the first to suggest the abandonment of fallowing and its replacement by hay meadows within crop rotations, and he highlighted the importance of soil (the French terroir) in the management of vineyards. His famous book \"Le Théâtre d’Agriculture et mesnage des champs\" contributed to the rise of modern, sustainable agriculture and to the collapse of old agricultural practices such as the lifting of forest litter for the amendment of crops (the French \"soutrage\") and assarting, which ruined the soils of western Europe during Middle Ages and even later on according to regions.\n\nExperiments into what made plants grow first led to the idea that the ash left behind when plant matter was burned was the essential element but overlooked the role of nitrogen, which is not left on the ground after combustion, a belief which prevailed until the 19th century. In about 1635, the Flemish chemist Jan Baptist van Helmont thought he had proved water to be the essential element from his famous five years' experiment with a willow tree grown with only the addition of rainwater. His conclusion came from the fact that the increase in the plant's weight had apparently been produced only by the addition of water, with no reduction in the soil's weight. John Woodward (d. 1728) experimented with various types of water ranging from clean to muddy and found muddy water the best, and so he concluded that earthy matter was the essential element. Others concluded it was humus in the soil that passed some essence to the growing plant. Still others held that the vital growth principal was something passed from dead plants or animals to the new plants. At the start of the 18th century, Jethro Tull demonstrated that it was beneficial to cultivate (stir) the soil, but his opinion that the stirring made the fine parts of soil available for plant absorption was erroneous.\n\nAs chemistry developed, it was applied to the investigation of soil fertility. The French chemist Antoine Lavoisier showed in about 1778 that plants and animals must [combust] oxygen internally to live and was able to deduce that most of the 165-pound weight of van Helmont's willow tree derived from air. It was the French agriculturalist Jean-Baptiste Boussingault who by means of experimentation obtained evidence showing that the main sources of carbon, hydrogen and oxygen for plants were air and water, while nitrogen was taken from soil. Justus von Liebig in his book \"Organic chemistry in its applications to agriculture and physiology\" (published 1840), asserted that the chemicals in plants must have come from the soil and air and that to maintain soil fertility, the used minerals must be replaced. Liebig nevertheless believed the nitrogen was supplied from the air. The enrichment of soil with guano by the Incas was rediscovered in 1802, by Alexander von Humboldt. This led to its mining and that of Chilean nitrate and to its application to soil in the United States and Europe after 1840.\n\nThe work of Liebig was a revolution for agriculture, and so other investigators started experimentation based on it. In England John Bennet Lawes and Joseph Henry Gilbert worked in the Rothamsted Experimental Station, founded by the former, and (re)discovered that plants took nitrogen from the soil, and that salts needed to be in an available state to be absorbed by plants. Their investigations also produced the \"superphosphate\", consisting in the acid treatment of phosphate rock. This led to the invention and use of salts of potassium (K) and nitrogen (N) as fertilizers. Ammonia generated by the production of coke was recovered and used as fertiliser. Finally, the chemical basis of nutrients delivered to the soil in manure was understood and in the mid-19th century chemical fertilisers were applied. However, the dynamic interaction of soil and its life forms still awaited discovery.\n\nIn 1856 J. Thomas Way discovered that ammonia contained in fertilisers was transformed into nitrates, and twenty years later Robert Warington proved that this transformation was done by living organisms. In 1890 Sergei Winogradsky announced he had found the bacteria responsible for this transformation.\n\nIt was known that certain legumes could take up nitrogen from the air and fix it to the soil but it took the development of bacteriology towards the end of the 19th century to lead to an understanding of the role played in nitrogen fixation by bacteria. The symbiosis of bacteria and leguminous roots, and the fixation of nitrogen by the bacteria, were simultaneously discovered by the German agronomist Hermann Hellriegel and the Dutch microbiologist Martinus Beijerinck.\n\nCrop rotation, mechanisation, chemical and natural fertilisers led to a doubling of wheat yields in western Europe between 1800 and 1900.\n\nThe scientists who studied the soil in connection with agricultural practices had considered it mainly as a static substrate. However, soil is the result of evolution from more ancient geological materials, under the action of biotic and abiotic (not associated with life) processes. After studies of the improvement of the soil commenced, others began to study soil genesis and as a result also soil types and classifications.\n\nIn 1860, in Mississippi, Eugene W. Hilgard studied the relationship among rock material, climate, and vegetation, and the type of soils that were developed. He realised that the soils were dynamic, and considered soil types classification. Unfortunately his work was not continued. At the same time Vasily Dokuchaev (about 1870) was leading a team of soil scientists in Russia who conducted an extensive survey of soils, finding that similar basic rocks, climate and vegetation types lead to similar soil layering and types, and established the concepts for soil classifications. Due to language barriers, the work of this team was not communicated to western Europe until 1914 through a publication in German by Konstantin Dmitrievich Glinka, a member of the Russian team.\n\nCurtis F. Marbut was influenced by the work of the Russian team, translated Glinka's publication into English, and as he was placed in charge of the U. S. National Cooperative Soil Survey, applied it to a national soil classification system.\n\nSoil formation, or pedogenesis, is the combined effect of physical, chemical, biological and anthropogenic processes working on soil parent material. Soil is said to be formed when organic matter has accumulated and colloids are washed downward, leaving deposits of clay, humus, iron oxide, carbonate, and gypsum, producing a distinct layer called the B horizon. This is a somewhat arbitrary definition as mixtures of sand, silt, clay and humus will support biological and agricultural activity before that time. These constituents are moved from one level to another by water and animal activity. As a result, layers (horizons) form in the soil profile. The alteration and movement of materials within a soil causes the formation of distinctive soil horizons. However, more recent definitions of soil embrace soils without any organic matter, such as those regoliths that formed on Mars and analogous conditions in planet Earth deserts.\n\nAn example of the development of a soil would begin with the weathering of lava flow bedrock, which would produce the purely mineral-based parent material from which the soil texture forms. Soil development would proceed most rapidly from bare rock of recent flows in a warm climate, under heavy and frequent rainfall. Under such conditions, plants (in a first stage nitrogen-fixing lichens and cyanobacteria then epilithic higher plants) become established very quickly on basaltic lava, even though there is very little organic material. The plants are supported by the porous rock as it is filled with nutrient-bearing water that carries minerals dissolved from the rocks. Crevasses and pockets, local topography of the rocks, would hold fine materials and harbour plant roots. The developing plant roots are associated with mineral-weathering mycorrhizal fungi that assist in breaking up the porous lava, and by these means organic matter and a finer mineral soil accumulate with time. Such initial stages of soil development have been described on volcanoes, inselbergs, and glacial moraines.\n\nHow soil formation proceeds is influenced by at least five classic factors that are intertwined in the evolution of a soil. They are: parent material, climate, topography (relief), organisms, and time. When reordered to climate, relief, organisms, parent material, and time, they form the acronym CROPT.\n\nThe mineral material from which a soil forms is called parent material. Rock, whether its origin is igneous, sedimentary, or metamorphic, is the source of all soil mineral materials and the origin of all plant nutrients with the exceptions of nitrogen, hydrogen and carbon. As the parent material is chemically and physically weathered, transported, deposited and precipitated, it is transformed into a soil.\n\nTypical soil parent mineral materials are:\nParent materials are classified according to how they came to be deposited. Residual materials are mineral materials that have weathered in place from primary bedrock. Transported materials are those that have been deposited by water, wind, ice or gravity. Cumulose material is organic matter that has grown and accumulates in place.\n\nResidual soils are soils that develop from their underlying parent rocks and have the same general chemistry as those rocks. The soils found on mesas, plateaux, and plains are residual soils. In the United States as little as three percent of the soils are residual.\n\nMost soils derive from transported materials that have been moved many miles by wind, water, ice and gravity.\n\nCumulose parent material is not moved but originates from deposited organic material. This includes peat and muck soils and results from preservation of plant residues by the low oxygen content of a high water table. While peat may form sterile soils, muck soils may be very fertile.\n\nThe weathering of parent material takes the form of physical weathering (disintegration), chemical weathering (decomposition) and chemical transformation. Generally, minerals that are formed under high temperatures and pressures at great depths within the Earth's mantle are less resistant to weathering, while minerals formed at low temperature and pressure environment of the surface are more resistant to weathering. Weathering is usually confined to the top few meters of geologic material, because physical, chemical, and biological stresses and fluctuations generally decrease with depth. Physical disintegration begins as rocks that have solidified deep in the Earth are exposed to lower pressure near the surface and swell and become mechanically unstable. Chemical decomposition is a function of mineral solubility, the rate of which doubles with each 10 °C rise in temperature, but is strongly dependent on water to effect chemical changes. Rocks that will decompose in a few years in tropical climates will remain unaltered for millennia in deserts. Structural changes are the result of hydration, oxidation, and reduction. Chemical weathering mainly results from the excretion of organic acids and chelating compounds by bacteria and fungi, thought to increase under present-day greenhouse effect.\n\n\nOf the above, hydrolysis and carbonation are the most effective, in particular in regions of high rainfall, temperature and physical erosion. Chemical weathering becomes more effective as the surface area of the rock increases, thus is favoured by physical disintegration. This stems in latitudinal and altitudinal climate gradients in regolith formation.\n\nSaprolite is a particular example of a residual soil formed from the transformation of granite, metamorphic and other types of bedrock into clay minerals. Often called [weathered granite], saprolite is the result of weathering processes that include: hydrolysis, chelation from organic compounds, hydration (the solution of minerals in water with resulting cation and anion pairs) and physical processes that include freezing and thawing. The mineralogical and chemical composition of the primary bedrock material, its physical features, including grain size and degree of consolidation, and the rate and type of weathering transforms the parent material into a different mineral. The texture, pH and mineral constituents of saprolite are inherited from its parent material. This process is also called \"arenization\", resulting in the formation of sandy soils (granitic arenas), thanks to the much higher resistance of quartz compared to other mineral components of granite (micas, amphiboles, feldspars).\n\nThe principal climatic variables influencing soil formation are effective precipitation (i.e., precipitation minus evapotranspiration) and temperature, both of which affect the rates of chemical, physical, and biological processes. Temperature and moisture both influence the organic matter content of soil through their effects on the balance between primary production and decomposition: the colder or drier the climate the lesser atmospheric carbon is fixed as organic matter while the lesser organic matter is decomposed.\n\nClimate is the dominant factor in soil formation, and soils show the distinctive characteristics of the climate zones in which they form, with a feedback to climate through transfer of carbon stocked in soil horizons back to the atmosphere. If warm temperatures and abundant water are present in the profile at the same time, the processes of weathering, leaching, and plant growth will be maximized. According to the climatic determination of biomes, humid climates favor the growth of trees. In contrast, grasses are the dominant native vegetation in subhumid and semiarid regions, while shrubs and brush of various kinds dominate in arid areas.\n\nWater is essential for all the major chemical weathering reactions. To be effective in soil formation, water must penetrate the regolith. The seasonal rainfall distribution, evaporative losses, site topography, and soil permeability interact to determine how effectively precipitation can influence soil formation. The greater the depth of water penetration, the greater the depth of weathering of the soil and its development. Surplus water percolating through the soil profile transports soluble and suspended materials from the upper layers (eluviation) to the lower layers (illuviation), including clay particles and dissolved organic matter. It may also carry away soluble materials in the surface drainage waters. Thus, percolating water stimulates weathering reactions and helps differentiate soil horizons. Likewise, a deficiency of water is a major factor in determining the characteristics of soils of dry regions. Soluble salts are not leached from these soils, and in some cases they build up to levels that curtail plant and microbial growth. Soil profiles in arid and semi-arid regions are also apt to accumulate carbonates and certain types of expansive clays (calcrete or caliche horizons). In tropical soils, when the soil has been deprived of vegetation (e.g. by deforestation) and thereby is submitted to intense evaporation, the upward capillary movement of water, which has dissolved iron and aluminum salts, is responsible for the formation of a superficial hard pan of laterite or bauxite, respectively, which is improper for cutivation, a known case of irreversible soil degradation (lateritization, bauxitization).\n\nThe direct influences of climate include:\n\nClimate directly affects the rate of weathering and leaching. Wind moves sand and smaller particles (dust), especially in arid regions where there is little plant cover, depositing it close or far from the entrainment source. The type and amount of precipitation influence soil formation by affecting the movement of ions and particles through the soil, and aid in the development of different soil profiles. Soil profiles are more distinct in wet and cool climates, where organic materials may accumulate, than in wet and warm climates, where organic materials are rapidly consumed. The effectiveness of water in weathering parent rock material depends on seasonal and daily temperature fluctuations, which favour tensile stresses in rock minerals, and thus their mechanical disaggregation, a process called \"thermal fatigue\". By the same process freeze-thaw cycles are an effective mechanism which breaks up rocks and other consolidated materials.\n\nClimate also indirectly influences soil formation through the effects of vegetation cover and biological activity, which modify the rates of chemical reactions in the soil.\n\nThe topography, or relief, is characterized by the inclination (slope), elevation, and orientation of the terrain. Topography determines the rate of precipitation or runoff and rate of formation or erosion of the surface soil profile. The topographical setting may either hasten or retard the work of climatic forces.\n\nSteep slopes encourage rapid soil loss by erosion and allow less rainfall to enter the soil before running off and hence, little mineral deposition in lower profiles. In semiarid regions, the lower effective rainfall on steeper slopes also results in less complete vegetative cover, so there is less plant contribution to soil formation. For all of these reasons, steep slopes prevent the formation of soil from getting very far ahead of soil destruction. Therefore, soils on steep terrain tend to have rather shallow, poorly developed profiles in comparison to soils on nearby, more level sites.\n\nIn swales and depressions where runoff water tends to concentrate, the regolith is usually more deeply weathered and soil profile development is more advanced. However, in the lowest landscape positions, water may saturate the regolith to such a degree that drainage and aeration are restricted. Here, the weathering of some minerals and the decomposition of organic matter are retarded, while the loss of iron and manganese is accelerated. In such low-lying topography, special profile features characteristic of wetland soils may develop. Depressions allow the accumulation of water, minerals and organic matter and in the extreme, the resulting soils will be saline marshes or peat bogs. Intermediate topography affords the best conditions for the formation of an agriculturally productive soil.\n\nSoil is the most abundant ecosystem on Earth, but the vast majority of organisms in soil are microbes, a great many of which have not been described. There may be a population limit of around one billion cells per gram of soil, but estimates of the number of species vary widely from 50,000 per gram to over a million per gram of soil. The total number of organisms and species can vary widely according to soil type, location, and depth.\n\nPlants, animals, fungi, bacteria and humans affect soil formation (see soil biomantle and stonelayer). Soil animals, including soil macrofauna and soil mesofauna, mix soils as they form burrows and pores, allowing moisture and gases to move about, a process called bioturbation. In the same way, plant roots penetrate soil horizons and open channels upon decomposition. Plants with deep taproots can penetrate many metres through the different soil layers to bring up nutrients from deeper in the profile. Plants have fine roots that excrete organic compounds (sugars, organic acids, mucigel), slough off cells (in particular at their tip) and are easily decomposed, adding organic matter to soil, a process called \"rhizodeposition\". Micro-organisms, including fungi and bacteria, effect chemical exchanges between roots and soil and act as a reserve of nutrients in a soil biological \"hotspot\" called rhizosphere. The growth of roots through the soil stimulates microbial populations, stimulating in turn the activity of their predators (notably amoeba), thereby increasing the mineralization rate, and in last turn root growth, a positive feedback called the soil microbial loop. Out of root influence, in the bulk soil, most bacteria are in a quiescent stage, forming microaggregates, i.e. mucilaginous colonies to which clay particles are glued, offering them a protection against desiccation and predation by soil microfauna (bacteriophagous protozoa and nematodes). Microaggregates (20-250 µm) are ingested by soil mesofauna and macrofauna, and bacterial bodies are partly or totally digested in their guts.\n\nHumans impact soil formation by removing vegetation cover with erosion, waterlogging, lateritization or podzolization (according to climate and topography) as the result. Their tillage also mixes the different soil layers, restarting the soil formation process as less weathered material is mixed with the more developed upper layers, resulting in net increased rate of mineral weathering.\n\nEarthworms, ants, termites, moles, gophers, as well as some millipedes and tenebrionid beetles mix the soil as they burrow, significantly affecting soil formation. Earthworms ingest soil particles and organic residues, enhancing the availability of plant nutrients in the material that passes through their bodies. They aerate and stir the soil and create stable soil aggregates, after having disrupted links between soil particles during the intestinal transit of ingested soil, thereby assuring ready infiltration of water. In addition, as ants and termites build mounds, they transport soil materials from one horizon to another. Other important functions are fulfilled by earthworms in the soil ecosystem, in particular their intense mucus production, both within the intestine and as a lining in their galleries, exert a priming effect on soil microflora, giving them the status of ecosystem engineers, which they share with ants and termites.\n\nIn general, the mixing of the soil by the activities of animals, sometimes called pedoturbation, tends to undo or counteract the tendency of other soil-forming processes that create distinct horizons. Termites and ants may also retard soil profile development by denuding large areas of soil around their nests, leading to increased loss of soil by erosion. Large animals such as gophers, moles, and prairie dogs bore into the lower soil horizons, bringing materials to the surface. Their tunnels are often open to the surface, encouraging the movement of water and air into the subsurface layers. In localized areas, they enhance mixing of the lower and upper horizons by creating, and later refilling, underground tunnels. Old animal burrows in the lower horizons often become filled with soil material from the overlying A horizon, creating profile features known as crotovinas.\n\nVegetation impacts soils in numerous ways. It can prevent erosion caused by excessive rain that might result from surface runoff. Plants shade soils, keeping them cooler and slow evaporation of soil moisture, or conversely, by way of transpiration, plants can cause soils to lose moisture, resulting in complex and highly variable relationships between leaf area index (measuring light interception) and moisture loss: more generally plants prevent soil from desiccation during driest months while they dry it during moister months, thereby acting as a buffer against strong moisture variation. Plants can form new chemicals that can break down minerals, both directly and indirectly through mycorrhizal fungi and rhizosphere bacteria, and improve the soil structure. The type and amount of vegetation depends on climate, topography, soil characteristics and biological factors, mediated or not by human activities. Soil factors such as density, depth, chemistry, pH, temperature and moisture greatly affect the type of plants that can grow in a given location. Dead plants and fallen leaves and stems begin their decomposition on the surface. There, organisms feed on them and mix the organic material with the upper soil layers; these added organic compounds become part of the soil formation process.\n\nHuman activities widely influence soil formation. For example, it is believed that Native Americans regularly set fires to maintain several large areas of prairie grasslands in Indiana and Michigan, although climate and mammalian grazers (e.g. bisons) are also advocated to explain the maintenance of the Great Plains of North America. In more recent times, human destruction of natural vegetation and subsequent tillage of the soil for crop production has abruptly modified soil formation. Likewise, irrigating soil in an arid region drastically influences soil-forming factors, as does adding fertilizer and lime to soils of low fertility.\n\nTime is a factor in the interactions of all the above. While a mixture of sand, silt and clay constitute the texture of a soil and the aggregation of those components produces peds, the development of a distinct B horizon marks the development of a soil or pedogenesis. With time, soils will evolve features that depend on the interplay of the prior listed soil-forming factors. It takes decades to several thousand years for a soil to develop a profile, although the notion of soil development has been criticized, soil being in a constant state-of-change under the influence of fluctuating soil-forming factors. That time period depends strongly on climate, parent material, relief, and biotic activity. For example, recently deposited material from a flood exhibits no soil development as there has not been enough time for the material to form a structure that further defines soil. The original soil surface is buried, and the formation process must begin anew for this deposit. Over time the soil will develop a profile that depends on the intensities of biota and climate. While a soil can achieve relative stability of its properties for extended periods, the soil life cycle ultimately ends in soil conditions that leave it vulnerable to erosion. Despite the inevitability of soil retrogression and degradation, most soil cycles are long.\n\nSoil-forming factors continue to affect soils during their existence, even on \"stable\" landscapes that are long-enduring, some for millions of years. Materials are deposited on top or are blown or washed from the surface. With additions, removals and alterations, soils are always subject to new conditions. Whether these are slow or rapid changes depends on climate, topography and biological activity.\n\nThe physical properties of soils, in order of decreasing importance for ecosystem services such as crop production, are texture, structure, bulk density, porosity, consistency, temperature, colour and resistivity. Soil texture is determined by the relative proportion of the three kinds of soil mineral particles, called soil separates: sand, silt, and clay. At the next larger scale, soil structures called peds or more commonly \"soil aggregates\" are created from the soil separates when iron oxides, carbonates, clay, silica and humus, coat particles and cause them to adhere into larger, relatively stable secondary structures. Soil bulk density, when determined at standardized moisture conditions, is an estimate of soil compaction. Soil porosity consists of the void part of the soil volume and is occupied by gases or water. Soil consistency is the ability of soil materials to stick together. Soil temperature and colour are self-defining. Resistivity refers to the resistance to conduction of electric currents and affects the rate of corrosion of metal and concrete structures which are buried in soil. These properties vary through the depth of a soil profile, i.e. through soil horizons. Most of these properties determine the aeration of the soil and the ability of water to infiltrate and to be held within the soil.\n\nThe mineral components of soil are sand, silt and clay, and their relative proportions determine a soil's texture. Properties that are influenced by soil texture include porosity, permeability, infiltration, shrink-swell rate, water-holding capacity, and susceptibility to erosion. In the illustrated USDA textural classification triangle, the only soil in which neither sand, silt nor clay predominates is called loam. While even pure sand, silt or clay may be considered a soil, from the perspective of conventional agriculture a loam soil with a small amount of organic material is considered \"ideal\", inasmuch as fertilizers or manure are currently used to mitigate nutrient losses due to crop yields in the long term. The mineral constituents of a loam soil might be 40% sand, 40% silt and the balance 20% clay by weight. Soil texture affects soil behaviour, in particular, its retention capacity for nutrients (e.g., cation exchange capacity) and water.\n\nSand and silt are the products of physical and chemical weathering of the parent rock; clay, on the other hand, is most often the product of the precipitation of the dissolved parent rock as a secondary mineral, except when derived from the weathering of mica. It is the surface area to volume ratio (specific surface area) of soil particles and the unbalanced ionic electric charges within those that determine their role in the fertility of soil, as measured by its cation exchange capacity. Sand is least active, having the least specific surface area, followed by silt; clay is the most active. Sand's greatest benefit to soil is that it resists compaction and increases soil porosity, although this property stands only for pure sand, not for sand mixed with smaller minerals which fill the voids among sand grains. Silt is mineralogically like sand but with its higher specific surface area it is more chemically and physically active than sand. But it is the clay content of soil, with its very high specific surface area and generally large number of negative charges, that gives a soil its high retention capacity for water and nutrients. Clay soils also resist wind and water erosion better than silty and sandy soils, as the particles bond tightly to each other,\nand that with a strong mitigation effect of organic matter.\n\nSand is the most stable of the mineral components of soil; it consists of rock fragments, primarily quartz particles, ranging in size from in diameter. Silt ranges in size from . Clay cannot be resolved by optical microscopes as its particles are or less in diameter and a thickness of only 10 angstroms (10 m). In medium-textured soils, clay is often washed downward through the soil profile (a process called eluviation) and accumulates in the subsoil (a process called illuviation). There is no clear relationship between the size of soil mineral components and their mineralogical nature: sand and silt particles can be calcareous as well as siliceous, while textural clay () can be made of very fine quartz particles as well as of multi-layered secondary minerals. Soil mineral components belonging to a given textural class may thus share properties linked to their specific surface area (e.g. moisture retention) but not those linked to their chemical composition (e.g. cation exchange capacity).\n\nSoil components larger than are classed as rock and gravel and are removed before determining the percentages of the remaining components and the textural class of the soil, but are included in the name. For example, a sandy loam soil with 20% gravel would be called gravelly sandy loam.\n\nWhen the organic component of a soil is substantial, the soil is called organic soil rather than mineral soil. A soil is called organic if:\n\n\nThe clumping of the soil textural components of sand, silt and clay causes aggregates to form and the further association of those aggregates into larger units creates soil structures called peds (a contraction of the word pedolith). The adhesion of the soil textural components by organic substances, iron oxides, carbonates, clays, and silica, the breakage of those aggregates from expansion-contraction caused by freezing-thawing and wetting-drying cycles, and the build-up of aggregates by soil animals, microbial colonies and root tips shape soil into distinct geometric forms. The peds evolve into units which have various shapes, sizes and degrees of development. A soil clod, however, is not a ped but rather a mass of soil that results from mechanical disturbance of the soil such as cultivation. Soil structure affects aeration, water movement, conduction of heat, plant root growth and resistance to erosion. Water, in turn, has a strong effect on soil structure, directly via the dissolution and precipitation of minerals, the mechanical destruction of aggregates (slaking) and indirectly by promoting plant, animal and microbial growth.\n\nSoil structure often gives clues to its texture, organic matter content, biological activity, past soil evolution, human use, and the chemical and mineralogical conditions under which the soil formed. While texture is defined by the mineral component of a soil and is an innate property of the soil that does not change with agricultural activities, soil structure can be improved or destroyed by the choice and timing of farming practices.\n\nSoil structural classes:\n\n\nAt the largest scale, the forces that shape a soil's structure result from swelling and shrinkage that initially tend to act horizontally, causing vertically oriented prismatic peds. This mechanical process is mainly exemplified in the development of vertisols. Clayey soil, due to its differential drying rate with respect to the surface, will induce horizontal cracks, reducing columns to blocky peds. Roots, rodents, worms, and freezing-thawing cycles further break the peds into smaller peds of a more or less spherical shape.\n\nAt a smaller scale, plant roots extend into voids (macropores) and remove water causing macroporosity to increase and microporosity to decrease, thereby decreasing aggregate size. At the same time, root hairs and fungal hyphae create microscopic tunnels that break up peds.\n\nAt an even smaller scale, soil aggregation continues as bacteria and fungi exude sticky polysaccharides which bind soil into smaller peds. The addition of the raw organic matter that bacteria and fungi feed upon encourages the formation of this desirable soil structure.\n\nAt the lowest scale, the soil chemistry affects the aggregation or dispersal of soil particles. The clay particles contain polyvalent cations which give the faces of clay layers localized negative charges. At the same time, the edges of the clay plates have a slight positive charge, thereby allowing the edges to adhere to the negative charges on the faces of other clay particles or to flocculate (form clumps). On the other hand, when monovalent ions, such as sodium, invade and displace the polyvalent cations, they weaken the positive charges on the edges, while the negative surface charges are relatively strengthened. This leaves negative charge on the clay faces that repel other clay, causing the particles to push apart, and by doing so deflocculate clay suspensions. As a result, the clay disperses and settles into voids between peds, causing those to close. In this way the open structure of the soil is destroyed and the soil is made impenetrable to air and water. Such sodic soil (also called haline soil) tends to form columnar peds near the surface.\n\nSoil particle density is typically 2.60 to 2.75 grams per cm and is usually unchanging for a given soil. Soil particle density is lower for soils with high organic matter content, and is higher for soils with high iron-oxides content. Soil bulk density is equal to the dry mass of the soil divided by the volume of the soil; i.e., it includes air space and organic materials of the soil volume. Thereby soil bulk density is always less than soil particle density and is a good indicator of soil compaction. The soil bulk density of cultivated loam is about 1.1 to 1.4 g/cm (for comparison water is 1.0 g/cm). Contrary to particle density, soil bulk density is highly variable for a given soil, with a strong causal relationship with soil biological activity and management strategies. However, it has been shown that, depending on species and the size of their aggregates (faeces), earthworms may either increase or decrease soil bulk density. A lower bulk density by itself does not indicate suitability for plant growth due to the confounding influence of soil texture and structure. A high bulk density is indicative of either soil compaction or a mixture of soil textural classes in which small particles fill the voids among coarser particles. Hence the positive correlation between the fractal dimension of soil, considered as a porous medium, and its bulk density, that explains the poor hydraulic conductivity of silty clay loam in the absence of a faunal structure.\n\nPore space is that part of the bulk volume of soil that is not occupied by either mineral or organic matter but is open space occupied by either gases or water. In a productive, medium-textured soil the total pore space is typically about 50% of the soil volume. Pore size varies considerably; the smallest pores (cryptopores; <0.1 µm) hold water too tightly for use by plant roots; plant-available water is held in ultramicropores, micropores and mesopores (0.1–75 µm); and macropores (>75 µm) are generally air-filled when the soil is at field capacity.\n\nSoil texture determines total volume of the smallest pores; clay soils have smaller pores, but more total pore space than sands, despite of a much lower permeability. Soil structure has a strong influence on the larger pores that affect soil aeration, water infiltration and drainage. Tillage has the short-term benefit of temporarily increasing the number of pores of largest size, but these can be rapidly degraded by the destruction of soil aggregation.\n\nThe pore size distribution affects the ability of plants and other organisms to access water and oxygen; large, continuous pores allow rapid transmission of air, water and dissolved nutrients through soil, and small pores store water between rainfall or irrigation events. Pore size variation also compartmentalizes the soil pore space such that many microbial and faunal organisms are not in direct competition with one another, which may explain not only the large number of species present, but the fact that functionally redundant organisms (organisms with the same ecological niche) can co-exist within the same soil.\n\nConsistency is the ability of soil to stick to itself or to other objects (cohesion and adhesion, respectively) and its ability to resist deformation and rupture. It is of approximate use in predicting cultivation problems and the engineering of foundations. Consistency is measured at three moisture conditions: air-dry, moist, and wet. In those conditions the consistency quality depends upon the clay content. In the wet state, the two qualities of stickiness and plasticity are assessed. A soil's resistance to fragmentation and crumbling is assessed in the dry state by rubbing the sample. Its resistance to shearing forces is assessed in the moist state by thumb and finger pressure. Additionally, the cemented consistency depends on cementation by substances other than clay, such as calcium carbonate, silica, oxides and salts; moisture content has little effect on its assessment. The measures of consistency border on subjective compared to other measures such as pH, since they employ the apparent feel of the soil in those states.\n\nThe terms used to describe the soil consistency in three moisture states and a last not affected by the amount of moisture are as follows:\n\n\nSoil consistency is useful in estimating the ability of soil to support buildings and roads. More precise measures of soil strength are often made prior to construction.\n\nSoil temperature depends on the ratio of the energy absorbed to that lost. Soil has a temperature range between -20 to 60 °C, with a mean annual temperature from -10 to 26 °C according to biomes. Soil temperature regulates seed germination, breaking of seed dormancy, plant and root growth and the availability of nutrients. Soil temperature has important seasonal, monthly and daily variations, fluctuations in soil temperature being much lower with increasing soil depth. Heavy mulching (a type of soil cover) can slow the warming of soil in summer, and, at the same time, reduce fluctuations in surface temperature.\n\nMost often, agricultural activities must adapt to soil temperatures by:\n\n\nSoil temperatures can be raised by drying soils or the use of clear plastic mulches. Organic mulches slow the warming of the soil.\n\nThere are various factors that affect soil temperature, such as water content, soil color, and relief (slope, orientation, and elevation), and soil cover (shading and insulation), in addition to air temperature. The color of the ground cover and its insulating properties have a strong influence on soil temperature. Whiter soil tends to have a higher albedo than blacker soil cover, which encourages whiter soils to have lower soil temperatures. The specific heat of soil is the energy required to raise the temperature of soil by 1 °C. The specific heat of soil increases as water content increases, since the heat capacity of water is greater than that of dry soil. The specific heat of pure water is ~ 1 calorie per gram, the specific heat of dry soil is ~ 0.2 calories per gram, hence, the specific heat of wet soil is ~ 0.2 to 1 calories per gram (0.8 to 4.2 kJ per kilogram). Also, a tremendous energy (~540 cal/g or 2260 kJ/kg) is required to evaporate water (known as the heat of vaporization). As such, wet soil usually warms more slowly than dry soil – wet surface soil is typically 3 to 6 °C colder than dry surface soil.\n\nSoil heat flux refers to the rate at which heat energy moves through the soil in response to a temperature difference between two points in the soil. The heat flux density is the amount of energy that flows through soil per unit area per unit time and has both magnitude and direction. For the simple case of conduction into or out of the soil in the vertical direction, which is most often applicable the heat flux density is:\n\nIn SI units\n\nHeat flux is in the direction opposite the temperature gradient, hence the minus sign. That is to say, if the temperature of the surface is higher than at depth x the negative sign will result in a positive value for the heat flux q, and which is interpreted as the heat being conducted into the soil.\n\nSoil temperature is important for the survival and early growth of seedlings. Soil temperatures affect the anatomical and morphological character of root systems. All physical, chemical, and biological processes in soil and roots are affected in particular because of the increased viscosities of water and protoplasm at low temperatures. In general, climates that do not preclude survival and growth of white spruce above ground are sufficiently benign to provide soil temperatures able to maintain white spruce root systems. In some northwestern parts of the range, white spruce occurs on permafrost sites and although young unlignified roots of conifers may have little resistance to freezing, the root system of containerized white spruce was not affected by exposure to a temperature of less than 30 °C.\n\nOptimum temperatures for tree root growth range between 10 °C and 25 °C in general and for spruce in particular. In 2-week-old white spruce seedlings that were then grown for 6 weeks in soil at temperatures of 15 °C, 19 °C, 23 °C, 27 °C, and 31 °C; shoot height, shoot dry weight, stem diameter, root penetration, root volume, and root dry weight all reached maxima at 19 °C.\n\nHowever, whereas strong positive relationships between soil temperature (5 °C to 25 °C) and growth have been found in trembling aspen and balsam poplar, white and other spruce species have shown little or no changes in growth with increasing soil temperature. Such insensitivity to soil low temperature may be common among a number of western and boreal conifers.\n\nSoil temperatures are increasing worldwide under the influence of present-day global climate warming, with opposing views about expected effects on carbon capture and storage and feedback loops to climate change Most threats are about permafrost thawing and attended effects on carbon destocking and ecosystem collapse.\n\nSoil colour is often the first impression one has when viewing soil. Striking colours and contrasting patterns are especially noticeable. The Red River of the South carries sediment eroded from extensive reddish soils like Port Silt Loam in Oklahoma. The Yellow River in China carries yellow sediment from eroding loess soils. Mollisols in the Great Plains of North America are darkened and enriched by organic matter. Podsols in boreal forests have highly contrasting layers due to acidity and leaching.\n\nIn general, color is determined by the organic matter content, drainage conditions, and degree of oxidation. Soil color, while easily discerned, has little use in predicting soil characteristics. It is of use in distinguishing boundaries of horizons within a soil profile, determining the origin of a soil's parent material, as an indication of wetness and waterlogged conditions, and as a qualitative means of measuring organic, iron oxide and clay contents of soils. Color is recorded in the Munsell color system as for instance 10YR3/4 \"Dusky Red\", with 10YR as \"hue\", 3 as \"value\" and 4 as \"chroma\". Munsell color dimensions (hue, value and chroma) can be averaged among samples and treated as quantitative parameters, displaying significant correlations with various soil and vegetation properties.\n\nSoil color is primarily influenced by soil mineralogy. Many soil colours are due to various iron minerals. The development and distribution of colour in a soil profile result from chemical and biological weathering, especially redox reactions. As the primary minerals in soil parent material weather, the elements combine into new and colourful compounds. Iron forms secondary minerals of a yellow or red colour, organic matter decomposes into black and brown humic compounds, and manganese and sulfur can form black mineral deposits. These pigments can produce various colour patterns within a soil. Aerobic conditions produce uniform or gradual colour changes, while reducing environments (anaerobic) result in rapid colour flow with complex, mottled patterns and points of colour concentration.\n\nSoil resistivity is a measure of a soil's ability to retard the conduction of an electric current. The electrical resistivity of soil can affect the rate of galvanic corrosion of metallic structures in contact with the soil. Higher moisture content or increased electrolyte concentration can lower resistivity and increase conductivity, thereby increasing the rate of corrosion. Soil resistivity values typically range from about 1 to 100000 Ω·m, extreme values being for saline soils and dry soils overlaying cristalline rocks, respectively.\n\nWater that enters a field is removed from a field by runoff, drainage, evaporation or transpiration. Runoff is the water that flows on the surface to the edge of the field; drainage is the water that flows through the soil downward or toward the edge of the field underground; evaporative water loss from a field is that part of the water that evaporates into the atmosphere directly from the field's surface; transpiration is the loss of water from the field by its evaporation from the plant itself.\n\nWater affects soil formation, structure, stability and erosion but is of primary concern with respect to plant growth. Water is essential to plants for four reasons:\n\n\nIn addition, water alters the soil profile by dissolving and re-depositing minerals, often at lower levels, and possibly leaving the soil sterile in the case of extreme rainfall and drainage. In a loam soil, solids constitute half the volume, gas one-quarter of the volume, and water one-quarter of the volume of which only half will be available to most plants, with a strong variation according to matric potential.\n\nA flooded field will drain the gravitational water under the influence of gravity until water's adhesive and cohesive forces resist further drainage at which point it is said to have reached field capacity. At that point, plants must apply suction to draw water from a soil. The water that plants may draw from the soil is called the available water. Once the available water is used up the remaining moisture is called unavailable water as the plant cannot produce sufficient suction to draw that water in. A plant must produce suction that increases from zero for a flooded field to 1/3 bar at field dry condition (one bar is a little less than one atmosphere pressure). At 15 bar suction, wilting point, seeds will not germinate, plants begin to wilt and then die. Water moves in soil under the influence of gravity, osmosis and capillarity. When water enters the soil, it displaces air from interconnected macropores by buoyancy, and breaks aggregates into which air is entrapped, a process called slaking.\n\nThe rate at which a soil can absorb water depends on the soil and its other conditions. As a plant grows, its roots remove water from the largest pores (macropores) first. Soon the larger pores hold only air, and the remaining water is found only in the intermediate- and smallest-sized pores (micropores). The water in the smallest pores is so strongly held to particle surfaces that plant roots cannot pull it away. Consequently, not all soil water is available to plants, with a strong dependence on texture. When saturated, the soil may lose nutrients as the water drains. Water moves in a draining field under the influence of pressure where the soil is locally saturated and by capillarity pull to drier parts of the soil. Most plant water needs are supplied from the suction caused by evaporation from plant leaves (transpiration) and a lower fraction is supplied by suction created by osmotic pressure differences between the plant interior and the soil solution. Plant roots must seek out water and grow preferentially in moister soil microsites, but some parts of the root system are also able to remoisten dry parts of the soil. Insufficient water will damage the yield of a crop. Most of the available water is used in transpiration to pull nutrients into the plant.\n\nWater is retained in a soil when the adhesive force of attraction that water's hydrogen atoms have for the oxygen of soil particles is stronger than the cohesive forces that water's hydrogen feels for other water oxygen atoms. When a field is flooded, the soil pore space is completely filled by water. The field will drain under the force of gravity until it reaches what is called field capacity, at which point the smallest pores are filled with water and the largest with water and gases. The total amount of water held when field capacity is reached is a function of the specific surface area of the soil particles. As a result, high clay and high organic soils have higher field capacities. The total force required to pull or push water out of soil is termed suction and usually expressed in units of bars (10 pascal) which is just a little less than one-atmosphere pressure. Alternatively, the terms \"soil moisture tension\" or water potential may be used.\n\nThe forces with which water is held in soils determine its availability to plants. Forces of adhesion hold water strongly to mineral and humus surfaces and less strongly to itself by cohesive forces. A plant's root may penetrate a very small volume of water that is adhering to soil and be initially able to draw in water that is only lightly held by the cohesive forces. But as the droplet is drawn down, the forces of adhesion of the water for the soil particles produce increasingly higher suction, finally up to 15 bar. At 15 bar suction, the soil water amount is called wilting point. At that suction the plant cannot sustain its water needs as water is still being lost from the plant by transpiration, the plant's turgidity is lost, and it wilts, although stomatal closure may decrease transpiration and thus may retard wilting below the wilting point, in particular under adaptation or acclimatization to drought. The next level, called air-dry, occurs at 1000 bar suction. Finally the oven dry condition is reached at 10,000 bar suction. All water below wilting percentage is called unavailable water.\n\nWhen the soil moisture content is optimal for plant growth, the water in the large and intermediate size pores can move about in the soil and be easily used by plants. The amount of water remaining in a soil drained to field capacity and the amount that is available are functions of the soil type. Sandy soil will retain very little water, while clay will hold the maximum amount. The time required to drain a field from flooded condition for a clay loam that begins at 43% water by weight to a field capacity of 22% is six days, whereas a sand loam that is flooded to its maximum of 22% water will take two days to reach field capacity of 11% water. The available water for the clay loam might be 11% whereas for the sand loam it might be only 8% by weight.\n\nThe above are average values for the soil textures as the percentages of sand, silt and clay vary.\n\nWater moves through soil due to the force of gravity, osmosis and capillarity. At zero to one-third bar suction, water is pushed through soil from the point of its application under the force of gravity and the pressure gradient created by the pressure of the water; this is called saturated flow. At higher suction, water movement is pulled by capillarity from wetter toward drier soil. This is caused by water's adhesion to soil solids, and is called unsaturated flow.\n\nWater infiltration and movement in soil is controlled by six factors:\n\n\nWater infiltration rates range from per hour for high clay soils to per hour for sand and well stabilised and aggregated soil structures. Water flows through the ground unevenly, in the form of so-called \"gravity fingers\", because of the surface tension between water particles.\n\nTree roots, whether living or dead, create preferential channels for rainwater flow through soil, magnifying infiltration rates of water up to 27 times.\n\nFlooding temporarily increases soil permeability in river beds, helping to recharge aquifers.\n\nWater applied to a soil is pushed by pressure gradients from the point of its application where it is saturated locally, to less saturated areas, such as the vadose zone. Once soil is completely wetted, any more water will move downward, or percolate out of the range of plant roots, carrying with it clay, humus, nutrients, primarily cations, and various contaminants, including pesticides, pollutants, viruses and bacteria, potentially causing groundwater contamination. In order of decreasing solubility, the leached nutrients are:\n\nIn the United States percolation water due to rainfall ranges from zero inches just east of the Rocky Mountains to twenty or more inches in the Appalachian Mountains and the north coast of the Gulf of Mexico.\n\nSoil physics (Darcy-type model) predicts that at suctions less than one-third bar, water moves theoretically in all directions via unsaturated flow at a rate that is dependent on the square of the diameter of the water-filled pores, but there is still not an adequate physical theory linking all types of waterflow in soil. Preferential flow occurs along interconnected macropores, crevices, root and worm channels, which drain water under gravity. Water is also pulled by capillary action due to the adhesion force of water to the soil solids, producing a suction gradient from wet towards drier soil and from macropores to micropores. Water flow (also called hydraulic conductivity) is primarily from coarse-textured soil into fine-textured soil horizons and is slowest in fine-textured soils such as clay.\n\nOf equal importance to the storage and movement of water in soil is the means by which plants acquire it and their nutrients. Most soil water is taken up by plants as passive absorption caused by the pulling force of water evaporating (transpiring) from the long column of water (xylem sap flow) that leads from the plant's roots to its leaves, according to the cohesion-tension theory. The upward movement of water and solutes (hydraulic lift) is regulated in the roots by the endodermis and in the plant foliage by stomatal conductance, and can be interrupted in root and shoot xylem vessels by cavitation, also called \"xylem embolism\". In addition, the high concentration of salts within plant roots creates an osmotic pressure gradient that pushes soil water into the roots. Osmotic absorption becomes more important during times of low water transpiration caused by lower temperatures (for example at night) or high humidity, and the reverse occurs under high temperature or low humidity. It is these process that cause guttation and wilting, respectively.\n\nRoot extension is vital for plant survival. A study of a single winter rye plant grown for four months in one cubic foot of loam soil showed that the plant developed 13,800,000 roots, a total of 385 miles in length with 2,550 square feet in surface area; and 14 billion hair roots of 6,600 miles total length and 4,320 square feet total area; for a total surface area of 6,870 square feet (83 ft squared). The total surface area of the loam soil was estimated to be 560,000 square feet. In other words, the roots were in contact with only 1.2% of the soil. However, root extension should be viewed as a dynamic process, allowing new roots to explore a new volume of soil each day, increasing dramatically the total volume of soil explored over a given growth period, and thus the volume of water taken up by the root system over this period. Root architecture, i.e. the spatial configuration of the root system, plays a prominent role in the adaptation of plants to soil water and nutrient availabiity, and thus in plant productivity.\n\nRoots must seek out water as the unsaturated flow of water in soil can move only at a rate of up to 2.5 cm (one inch) per day; as a result they are constantly dying and growing as they seek out high concentrations of soil moisture. Insufficient soil moisture, to the point of causing wilting, will cause permanent damage and crop yields will suffer. When grain sorghum was exposed to soil suction as low as 13.0 bar during the seed head emergence through bloom and seed set stages of growth, its production was reduced by 34%.\n\nOnly a small fraction (0.1% to 1%) of the water used by a plant is held within the plant. The majority is ultimately lost via transpiration, while evaporation from the soil surface is also substantial, the transpiration:evaporation ratio varying according to vegetation type and climate, peaking in tropical rainforests and dipping in steppes and deserts. Transpiration plus evaporative soil moisture loss is called evapotranspiration. Evapotranspiration plus water held in the plant totals to consumptive use, which is nearly identical to evapotranspiration.\n\nThe total water used in an agricultural field includes surface runoff, drainage and consumptive use. The use of loose mulches will reduce evaporative losses for a period after a field is irrigated, but in the end the total evaporative loss (plant plus soil) will approach that of an uncovered soil, while more water is immediately available for plant growth. Water use efficiency is measured by the transpiration ratio, which is the ratio of the total water transpired by a plant to the dry weight of the harvested plant. Transpiration ratios for crops range from 300 to 700. For example, alfalfa may have a transpiration ratio of 500 and as a result 500 kilograms of water will produce one kilogram of dry alfalfa.\n\nThe atmosphere of soil, or soil gas, is radically different from the atmosphere above. The consumption of oxygen by microbes and plant roots, and their release of carbon dioxide, decrease oxygen and increase carbon dioxide concentration. Atmospheric CO concentration is 0.04%, but in the soil pore space it may range from 10 to 100 times that level, thus potentially contributing to the inhibition of root respiration. Calcareous soils regulate CO concentration thanks to carbonate buffering, contrary to acid soils in which all CO respired accumulates in the soil pore system. At extreme levels CO is toxic. This suggests a possible negative feedback control of soil CO concentration through its inhibitory effects on root and microbial respiration (also called 'soil respiration'). In addition, the soil voids are saturated with water vapour, at least until the point of maximal hygroscopicity, beyond which a vapour-pressure deficit occurs in the soil pore space. Adequate porosity is necessary, not just to allow the penetration of water, but also to allow gases to diffuse in and out. Movement of gases is by diffusion from high concentrations to lower, the diffusion coefficient decreasing with soil compaction. Oxygen from above atmosphere diffuses in the soil where it is consumed and levels of carbon dioxide in excess of above atmosphere diffuse out with other gases (including greenhouse gases) as well as water. Soil texture and structure strongly affect soil porosity and gas diffusion. It is the total pore space (porosity) of soil, not the pore size, and the degree of pore interconnection (or conversely pore sealing), together with water content, air turbulence and temperature, that determine the rate of diffusion of gases into and out of soil. Platy soil structure and soil compaction (low porosity) impede gas flow, and a deficiency of oxygen may encourage anaerobic bacteria to reduce (strip oxygen) from nitrate NO to the gases N, NO, and NO, which are then lost to the atmosphere, thereby depleting the soil of nitrogen. Aerated soil is also a net sink of methane CH but a net producer of methane (a strong heat-absorbing greenhouse gas) when soils are depleted of oxygen and subject to elevated temperatures.\n\nSoil atmosphere is also the seat of emissions of volatiles other than carbon and nitrogen oxides from various soil organisms, e.g. roots, bacteria, fungi, animals. These volatiles are used as chemical cues, making soil atmosphere the seat of interaction networks playing a decisive role in the stability, dynamics and evolution of soil ecosystems. Biogenic soil volatile organic compounds are exchanged with the aboveground atmosphere, in which they are just 1–2 orders of magnitude lower than those from aboveground vegetation.\n\nWe humans can get some idea of the soil atmosphere through the well-known 'after-the-rain' scent, when infiltering rainwater flushes out the whole soil atmosphere after a drought period, or when soil is excavated, a bulk property attributed in a reductionist manner to particular biochemical compounds such as petrichor or geosmin.\n\nSoil particles can be classified by their chemical composition (mineralogy) as well as their size. The particle size distribution of a soil, its texture, determines many of the properties of that soil, in particular hydraulic conductivity and water potential but the mineralogy of those particles can strongly modify those properties. The mineralogy of the finest soil particles, clay, is especially important.\n\nGravel, sand and silt are the larger soil particles, and their mineralogy is often inherited from the parent material of the soil, but may include products of weathering (such as concretions of calcium carbonate or iron oxide), or residues of plant and animal life (such as silica phytoliths). Quartz is the most common mineral in the sand or silt fraction as it is resistant to chemical weathering, except under hot climate; other common minerals are feldspars, micas and ferromagnesian minerals such as pyroxenes, amphiboles and olivines, which are dissolved or transformed in clay under the combined influence of physico-chemical and biological processes.\n\nDue to its high specific surface area and its unbalanced negative electric charges, clay is the most active mineral component of soil. It is a colloidal and most often a crystalline material. In soils, clay is a soil textural class and is defined in a physical sense as any mineral particle less than in effective diameter. Many soil minerals, such as gypsum, carbonates, or quartz, are small enough to be classified as clay based on their physical size, but chemically they do not afford the same utility as do mineralogically-defined clay minerals. Chemically, clay minerals are a range of phyllosilicate minerals with certain reactive properties.\n\nBefore the advent of X-ray diffraction clay was thought to be very small particles of quartz, feldspar, mica, hornblende or augite, but it is now known to be (with the exception of mica-based clays) a precipitate with a mineralogical composition that is dependent on but different from its parent materials and is classed as a secondary mineral. The type of clay that is formed is a function of the parent material and the composition of the minerals in solution. Clay minerals continue to be formed as long as the soil exists. Mica-based clays result from a modification of the primary mica mineral in such a way that it behaves and is classed as a clay. Most clays are crystalline, but some clays or some parts of clay minerals are amorphous. The clays of a soil are a mixture of the various types of clay, but one type predominates.\n\nTypically there are four main groups of clay minerals: kaolinite, montmorillonite-smectite, illite, and chlorite. Most clays are crystalline and most are made up of three or four planes of oxygen held together by planes of aluminium and silicon by way of ionic bonds that together form a single layer of clay. The spatial arrangement of the oxygen atoms determines clay's structure. Half of the weight of clay is oxygen, but on a volume basis oxygen is ninety percent. The layers of clay are sometimes held together through hydrogen bonds, sodium or potassium bridges and as a result will swell less in the presence of water. Clays such as montmorillonite have layers that are loosely attached and will swell greatly when water intervenes between the layers.\n\nIn a wider sense clays can be classified as:\n\n\nAlumino-silica clays or aluminosilicate clays are characterised by their regular crystalline or quasi-crystalline structure. Oxygen in ionic bonds with silicon forms a tetrahedral coordination (silicon at the center) which in turn forms sheets of silica. Two sheets of silica are bonded together by a plane of aluminium which forms an octahedral coordination, called alumina, with the oxygens of the silica sheet above and that below it. Hydroxyl ions (OH) sometimes substitute for oxygen. During the clay formation process, Al may substitute for Si in the silica layer, and as much as one fourth of the aluminium Al may be substituted by Zn, Mg or Fe in the alumina layer. The substitution of lower-valence cations for higher-valence cations (isomorphous substitution) gives clay a local negative charge on an oxygen atom that attracts and holds water and positively charged soil cations, some of which are of value for plant growth. Isomorphous substitution occurs during the clay's formation and does not change with time.\n\nThe carbonate and sulfate minerals are much more soluble and hence are found primarily in desert soils where leaching is less active.\n\nAmorphous clays are young, and commonly found in volcanic ash. They are mixtures of alumina and silica which have not formed the ordered crystal shape of alumino-silica clays which time would provide. The majority of their negative charges originates from hydroxyl ions, which can gain or lose a hydrogen ion (H) in response to soil pH, in such way was as to buffer the soil pH. They may have either a negative charge provided by the attached hydroxyl ion (OH), which can attract a cation, or lose the hydrogen of the hydroxyl to solution and display a positive charge which can attract anions. As a result, they may display either high CEC in an acid soil solution, or high anion exchange capacity in a basic soil solution.\n\nSesquioxide clays are a product of heavy rainfall that has leached most of the silica from alumino-silica clay, leaving the less soluble oxides iron hematite (FeO), iron hydroxide (Fe(OH)), aluminium hydroxide gibbsite (Al(OH)), hydrated manganese birnessite (MnO). It takes hundreds of thousands of years of leaching to create sesquioxide clays. \"Sesqui\" is Latin for \"one and one-half\": there are three parts oxygen to two parts iron or aluminium; hence the ratio is one and one-half (not true for all). They are hydrated and act as either amorphous or crystalline. They are not sticky and do not swell, and soils high in them behave much like sand and can rapidly pass water. They are able to hold large quantities of phosphates. Sesquioxides have low CEC but are able to hold anions as well as cations. Such soils range from yellow to red in colour. Such clays tend to hold phosphorus so tightly that it is unavailable for absorption by plants.\n\nHumus is the final state of decomposition of organic matter. While it may linger for a thousand years, on the larger scale of the age of the mineral soil components, it is temporary. It is composed of the very stable lignins (30%) and complex sugars (polyuronides, 30%), proteins (30%), waxes, and fats that are resistant to breakdown by microbes. Its chemical assay is 60% carbon, 5% nitrogen, some oxygen and the remainder hydrogen, sulfur, and phosphorus. On a dry weight basis, the CEC of humus is many times greater than that of clay.\n\nIn the extreme environment of high temperatures and the leaching caused by the heavy rain of tropical rain forests, the clay and organic colloids are largely destroyed. The heavy rains wash the alumino-silicate clays from the soil leaving only sesquioxide clays of low CEC. The high temperatures and humidity allow bacteria and fungi to virtually dissolve any organic matter on the rain-forest floor overnight and much of the nutrients are volatilized or leached from the soil and lost. However, carbon in the form of charcoal is far more stable than soil colloids and is capable of performing many of the functions of the soil colloids of sub-tropical soils. Soil containing substantial quantities of charcoal, of an anthropogenic origin, is called terra preta. Research into terra preta is still young but is promising. Fallow periods \"on the Amazonian Dark Earths can be as short as 6 months, whereas fallow periods on oxisols are usually 8 to 10 years long\"\n\nThe chemistry of a soil determines its ability to supply available plant nutrients and affects its physical properties and the health of its microbial population. In addition, a soil's chemistry also determines its corrosivity, stability, and ability to absorb pollutants and to filter water. It is the surface chemistry of mineral and organic colloids that determines soil's chemical properties. \"A colloid is a small, insoluble, nondiffusible particle larger than a molecule but small enough to remain suspended in a fluid medium without settling. Most soils contain organic colloidal particles called humus as well as the inorganic colloidal particles of clays.\" The very high specific surface area of colloids and their net charges, gives soil its ability to hold and release ions. Negatively charged sites on colloids attract and release cations in what is referred to as cation exchange. Cation-exchange capacity (CEC) is the amount of exchangeable cations per unit weight of dry soil and is expressed in terms of milliequivalents of positively charged ions per 100 grams of soil (or centimoles of positive charge per kilogram of soil; cmol/kg). Similarly, positively charged sites on colloids can attract and release anions in the soil giving the soil anion exchange capacity (AEC).\n\nThe cation exchange, that takes place between colloids and soil water, buffers (moderates) soil pH, alters soil structure, and purifies percolating water by adsorbing cations of all types, both useful and harmful.\n\nThe negative or positive charges on colloid particles make them able to hold cations or anions, respectively, to their surfaces. The charges result from four sources.\n\n\nCations held to the negatively charged colloids resist being washed downward by water and out of reach of plants' roots, thereby preserving the fertility of soils in areas of moderate rainfall and low temperatures.\n\nThere is a hierarchy in the process of cation exchange on colloids, as they differ in the strength of adsorption by the colloid and hence their ability to replace one another. If present in equal amounts in the soil water solution:\n\nAl replaces H replaces Ca replaces Mg replaces K same as NH replaces Na\n\nIf one cation is added in large amounts, it may replace the others by the sheer force of its numbers. This is called mass action. This is largely what occurs with the addition of fertiliser.\n\nAs the soil solution becomes more acidic (low pH, and an abundance of H), the other cations more weakly bound to colloids are pushed into solution as hydrogen ions occupy those sites. A low pH may cause hydrogen of hydroxyl groups to be pulled into solution, leaving charged sites on the colloid available to be occupied by other cations. This ionisation of hydroxyl groups on the surface of soil colloids creates what is described as pH-dependent charges. Unlike permanent charges developed by isomorphous substitution, pH-dependent charges are variable and increase with increasing pH. Freed cations can be made available to plants but are also prone to be leached from the soil, possibly making the soil less fertile. Plants are able to excrete H into the soil and by that means, change the pH of the soil near the root and push cations off the colloids, thus making those available to the plant.\n\nCation exchange capacity should be thought of as the soil's ability to remove cations from the soil water solution and sequester those to be exchanged later as the plant roots release hydrogen ions to the solution. CEC is the amount of exchangeable hydrogen cation (H) that will combine with 100 grams dry weight of soil and whose measure is one milliequivalents per 100 grams of soil (1 meq/100 g). Hydrogen ions have a single charge and one-thousandth of a gram of hydrogen ions per 100 grams dry soil gives a measure of one milliequivalent of hydrogen ion. Calcium, with an atomic weight 40 times that of hydrogen and with a valence of two, converts to (40/2) x 1 milliequivalent = 20 milliequivalents of hydrogen ion per 100 grams of dry soil or 20 meq/100 g. The modern measure of CEC is expressed as centimoles of positive charge per kilogram (cmol/kg) of oven-dry soil.\n\nMost of the soil's CEC occurs on clay and humus colloids, and the lack of those in hot, humid, wet climates, due to leaching and decomposition respectively, explains the relative sterility of tropical soils. Live plant roots also have some CEC.\n\nAnion exchange capacity should be thought of as the soil's ability to remove anions from the soil water solution and sequester those for later exchange as the plant roots release carbonate anions to the soil water solution. Those colloids which have low CEC tend to have some AEC. Amorphous and sesquioxide clays have the highest AEC, followed by the iron oxides. Levels of AEC are much lower than for CEC. Phosphates tend to be held at anion exchange sites.\n\nIron and aluminum hydroxide clays are able to exchange their hydroxide anions (OH) for other anions. The order reflecting the strength of anion adhesion is as follows:\n\nThe amount of exchangeable anions is of a magnitude of tenths to a few milliequivalents per 100 g dry soil. As pH rises, there are relatively more hydroxyls, which will displace anions from the colloids and force them into solution and out of storage; hence AEC decreases with increasing pH (alkalinity).\n\nSoil reactivity is expressed in terms of pH and is a measure of the acidity or alkalinity of the soil. More precisely, it is a measure of hydrogen ion concentration in an aqueous solution and ranges in values from 0 to 14 (acidic to basic) but practically speaking for soils, pH ranges from 3.5 to 9.5, as pH values beyond those extremes are toxic to life forms.\n\nAt 25 °C an aqueous solution that has a pH of 3.5 has 10 moles H (hydrogen ions) per litre of solution (and also 10 mole/litre OH). A pH of 7, defined as neutral, has 10 moles hydrogen ions per litre of solution and also 10 moles of OH per litre; since the two concentrations are equal, they are said to neutralise each other. A pH of 9.5 has 10 moles hydrogen ions per litre of solution (and also 10 mole per litre OH). A pH of 3.5 has one million times more hydrogen ions per litre than a solution with pH of 9.5 (9.5 - 3.5 = 6 or 10) and is more acidic.\n\nThe effect of pH on a soil is to remove from the soil or to make available certain ions. Soils with high acidity tend to have toxic amounts of aluminium and manganese. Plants which need calcium need moderate alkalinity, but most minerals are more soluble in acid soils. Soil organisms are hindered by high acidity, and most agricultural crops do best with mineral soils of pH 6.5 and organic soils of pH 5.5.\n\nIn high rainfall areas, soils tend to acidity as the basic cations are forced off the soil colloids by the mass action of hydrogen ions from the rain as those attach to the colloids. High rainfall rates can then wash the nutrients out, leaving the soil sterile. Once the colloids are saturated with H, the addition of any more hydrogen ions or aluminum hydroxyl cations drives the pH even lower (more acidic) as the soil has been left with no buffering capacity. In areas of extreme rainfall and high temperatures, the clay and humus may be washed out, further reducing the buffering capacity of the soil. In low rainfall areas, unleached calcium pushes pH to 8.5 and with the addition of exchangeable sodium, soils may reach pH 10. Beyond a pH of 9, plant growth is reduced. High pH results in low micro-nutrient mobility, but water-soluble chelates of those nutrients can correct the deficit. Sodium can be reduced by the addition of gypsum (calcium sulphate) as calcium adheres to clay more tightly than does sodium causing sodium to be pushed into the soil water solution where it can be washed out by an abundance of water.\n\nThere are acid-forming cations (hydrogen and aluminium) and there are base-forming cations. The fraction of the base-forming cations that occupy positions on the soil colloids is called the base saturation percentage. If a soil has a CEC of 20 meq and 5 meq are aluminium and hydrogen cations (acid-forming), the remainder of positions on the colloids (20-5 = 15 meq) are assumed occupied by base-forming cations, so that the percentage base saturation is 15/20 x 100% = 75% (the compliment 25% is assumed acid-forming cations). When the soil pH is 7 (neutral), base saturation is 100 percent and there are no hydrogen ions stored on the colloids. Base saturation is almost in direct proportion to pH (increases with increasing pH). It is of use in calculating the amount of lime needed to neutralise an acid soil. The amount of lime needed to neutralize a soil must take account of the amount of acid forming ions on the colloids not just those in the soil water solution. The addition of enough lime to neutralize the soil water solution will be insufficient to change the pH, as the acid forming cations stored on the soil colloids will tend to restore the original pH condition as they are pushed off those colloids by the calcium of the added lime.\n\nThe resistance of soil to change in pH, as a result of the addition of acid or basic material, is a measure of the buffering capacity of a soil and (for a particular soil type) increases as the CEC increases. Hence, pure sand has almost no buffering ability, while soils high in colloids have high buffering capacity. Buffering occurs by cation exchange and neutralisation.\n\nThe addition of a small amount highly basic aqueous ammonia to a soil will cause the ammonium to displace hydrogen ions from the colloids, and the end product is water and colloidally fixed ammonium, but little permanent change overall in soil pH.\n\nThe addition of a small amount of lime, Ca(OH), will displace hydrogen ions from the soil colloids, causing the fixation of calcium to colloids and the evolution of CO and water, with little permanent change in soil pH.\n\nThe above are examples of the buffering of soil pH. The general principal is that an increase in a particular cation in the soil water solution will cause that cation to be fixed to colloids (buffered) and a decrease in solution of that cation will cause it to be withdrawn from the colloid and moved into solution (buffered). The degree of buffering is often related to the CEC of the soil; the greater the CEC, the greater the buffering capacity of the soil.\n\nSixteen elements or nutrients are essential for plant growth and reproduction. They are carbon C, hydrogen H, oxygen O, nitrogen N, phosphorus P, potassium K, sulfur S, calcium Ca, magnesium Mg, iron Fe, boron B, manganese Mn, copper Cu, zinc Zn, molybdenum Mo, nickel Ni and chlorine Cl. Nutrients required for plants to complete their life cycle are considered essential nutrients. Nutrients that enhance the growth of plants but are not necessary to complete the plant's life cycle are considered non-essential. With the exception of carbon, hydrogen and oxygen, which are supplied by carbon dioxide and water, and nitrogen, provided through nitrogen fixation, the nutrients derive originally from the mineral component of the soil.\n\nPlant uptake of nutrients can only proceed when they are present in a plant-available form. In most situations, nutrients are absorbed in an ionic form from (or together with) soil water. Although minerals are the origin of most nutrients, and the bulk of most nutrient elements in the soil is held in crystalline form within primary and secondary minerals, they weather too slowly to support rapid plant growth. For example, The application of finely ground minerals, feldspar and apatite, to soil seldom provides the necessary amounts of potassium and phosphorus at a rate sufficient for good plant growth, as most of the nutrients remain bound in the crystals of those minerals.\n\nThe nutrients adsorbed onto the surfaces of clay colloids and soil organic matter provide a more accessible reservoir of many plant nutrients (e.g. K, Ca, Mg, P, Zn). As plants absorb the nutrients from the soil water, the soluble pool is replenished from the surface-bound pool. The decomposition of soil organic matter by microorganisms is another mechanism whereby the soluble pool of nutrients is replenished – this is important for the supply of plant-available N, S, P, and B from soil.\n\nGram for gram, the capacity of humus to hold nutrients and water is far greater than that of clay minerals. All in all, small amounts of humus may remarkably increase the soil's capacity to promote plant growth.\n\nNutrients in the soil are taken up by the plant through its roots. To be taken up by a plant, a nutrient element must be located near the root surface; however, the supply of nutrients in contact with the root is rapidly depleted. There are three basic mechanisms whereby nutrient ions dissolved in the soil solution are brought into contact with plant roots:\n\n\nAll three mechanisms operate simultaneously, but one mechanism or another may be most important for a particular nutrient. For example, in the case of calcium, which is generally plentiful in the soil solution, mass flow alone can usually bring sufficient amounts to the root surface. However, in the case of phosphorus, diffusion is needed to supplement mass flow. For the most part, nutrient ions must travel some distance in the soil solution to reach the root surface. This movement can take place by mass flow, as when dissolved nutrients are carried along with the soil water flowing toward a root that is actively drawing water from the soil. In this type of movement, the nutrient ions are somewhat analogous to leaves floating down a stream. In addition, nutrient ions continually move by diffusion from areas of greater concentration toward the nutrient-depleted areas of lower concentration around the root surface. That process is due to random motion of molecules. By this means, plants can continue to take up nutrients even at night, when water is only slowly absorbed into the roots as transpiration has almost stopped. Finally, root interception comes into play as roots continually grow into new, undepleted soil.\n\nIn the above table, phosphorus and potassium nutrients move more by diffusion than they do by mass flow in the soil water solution, as they are rapidly taken up by the roots creating a concentration of almost zero near the roots (the plants cannot transpire enough water to draw more of those nutrients near the roots). The very steep concentration gradient is of greater influence in the movement of those ions than is the movement of those by mass flow. The movement by mass flow requires the transpiration of water from the plant causing water and solution ions to also move toward the roots. Movement by root interception is slowest as the plants must extend their roots.\n\nPlants move ions out of their roots in an effort to move nutrients in from the soil. Hydrogen H is exchanged for other cations, and carbonate (HCO) and hydroxide (OH) anions are exchanged for nutrient anions. As plant roots remove nutrients from the soil water solution, they are replenished as other ions move off of clay and humus (by ion exchange or desorption), are added from the weathering of soil minerals, and are released by the decomposition of soil organic matter. Plants derive a large proportion of their anion nutrients from decomposing organic matter, which typically holds about 95 percent of the soil nitrogen, 5 to 60 percent of the soil phosphorus and about 80 percent of the soil sulfur. Where crops are produced, the replenishment of nutrients in the soil must usually be augmented by the addition of fertilizer or organic matter.\n\nBecause nutrient uptake is an active metabolic process, conditions that inhibit root metabolism may also inhibit nutrient uptake. Examples of such conditions include waterlogging or soil compaction resulting in poor soil aeration, excessively high or low soil temperatures, and above-ground conditions that result in low translocation of sugars to plant roots.\n\nPlants obtain their carbon from atmospheric carbon dioxide. About 45% of a plant's dry mass is carbon; plant residues typically have a carbon to nitrogen ratio (C/N) of between 13:1 and 100:1. As the soil organic material is digested by arthropods and micro-organisms, the C/N decreases as the carbonaceous material is metabolized and carbon dioxide (CO) is released as a byproduct which then finds its way out of the soil and into the atmosphere. The nitrogen is sequestered in the bodies of the living matter of those decomposing organisms and so it builds up in the soil. Normal CO concentration in the atmosphere is 0.03%, this can be the factor limiting plant growth. In a field of maize on a still day during high light conditions in the growing season, the CO concentration drops very low, but under such conditions the crop could use up to 20 times the normal concentration. The respiration of CO by soil micro-organisms decomposing soil organic matter contributes an important amount of CO to the photosynthesising plants. Within the soil, CO concentration is 10 to 100 times that of atmospheric levels but may rise to toxic levels if the soil porosity is low or if diffusion is impeded by flooding.\n\nNitrogen is the most critical element obtained by plants from the soil and nitrogen deficiency often limits plant growth. Plants can use the nitrogen as either the ammonium cation (NH) or the anion nitrate (NO). Usually, most of the nitrogen in soil is bound within organic compounds that make up the soil organic matter, and must be mineralized to the ammonium or nitrate form before it can be taken up by most plants. The total nitrogen content depends largely on the soil organic matter content, which in turn depends on the climate, vegetation, topography, age and soil management. Soil nitrogen typically decreases by 0.2 to 0.3% for every temperature increase by 10 °C. Usually, grassland soils contain more soil nitrogen than forest soils. Cultivation decreases soil nitrogen by exposing soil organic matter to decomposition by microorganisms, and soils under no-tillage maintain more soil nitrogen than tilled soils.\n\nSome micro-organisms are able to metabolise organic matter and release ammonium in a process called \"mineralisation\". Others take free ammonium and oxidise it to nitrate. Nitrogen-fixing bacteria are capable of metabolising N into the form of ammonia in a process called nitrogen fixation. Both ammonium and nitrate can be \"immobilized\" by their incorporation into the microbes' living cells, where it is temporarily sequestered in the form of amino acids and protein. Nitrate may also be lost from the soil when bacteria metabolise it to the gases N and NO. The loss of gaseous forms of nitrogen to the atmosphere due to microbial action is called \"denitrification\". Nitrogen may also be \"leached\" from the soil if it is in the form of nitrate or lost to the atmosphere as ammonia due to a chemical reaction of ammonium with alkaline soil by way of a process called \"volatilisation\". Ammonium may also be sequestered in clay by \"fixation\". A small amount of nitrogen is added to soil by rainfall.\n\nIn the process of mineralisation, microbes feed on organic matter, releasing ammonia (NH), ammonium (NH) and other nutrients. As long as the carbon to nitrogen ratio (C/N) of fresh residues in the soil is above 30:1, nitrogen will be in short supply and other bacteria will feed on the ammonium and incorporate its nitrogen into their cells in the immobilization process. In that form the nitrogen is said to be \"immobilised\". Later, when such bacteria die, they too are \"mineralised\" and some of the nitrogen is released as ammonium and nitrate. If the C/N is less than 15, ammonia is freed to the soil, where it may be used by bacteria which oxidise it to nitrate (nitrification). Bacteria may on average add nitrogen per acre, and in an unfertilised field, this is the most important source of usable nitrogen. In a soil with 5% organic matter perhaps 2 to 5% of that is released to the soil by such decomposition. It occurs fastest in warm, moist, well aerated soil. The mineralisation of 3% of the organic material of a soil that is 4% organic matter overall, would release of nitrogen as ammonium per acre.\n\nIn nitrogen fixation, rhizobium bacteria convert N to ammonia (NH). Rhizobia share a symbiotic relationship with host plants, since rhizobia supply the host with nitrogen and the host provides rhizobia with nutrients and a safe environment. It is estimated that such symbiotic bacteria in the root nodules of legumes add 45 to 250 pounds of nitrogen per acre per year, which may be sufficient for the crop. Other, free-living nitrogen-fixing bacteria and blue-green algae live independently in the soil and release nitrate when their dead bodies are converted by way of mineralisation.\n\nSome amount of usable nitrogen is fixed by lightning as nitric oxide (NO) and nitrogen dioxide (NO). Nitrogen dioxide is soluble in water to form nitric acid (HNO) solution of H and NO. Ammonia, NH, previously released from the soil or from combustion, may fall with precipitation as nitric acid at a rate of about five pounds nitrogen per acre per year.\n\nWhen bacteria feed on soluble forms of nitrogen (ammonium and nitrate), they temporarily sequester that nitrogen in their bodies in a process called \"immobilisation\". At a later time when those bacteria die, their nitrogen may be released as ammonium by the processes of mineralisation.\n\nProtein material is easily broken down, but the rate of its decomposition is slowed by its attachment to the crystalline structure of clay and when trapped between the clay layers. The layers are small enough that bacteria cannot enter. Some organisms can exude extracellular enzymes that can act on the sequestered proteins. However, those enzymes too may be trapped on the clay crystals.\n\nAmmonium fixation occurs when ammonium pushes potassium ions from between the layers of clay such as illite or montmorillonite. Only a small fraction of soil nitrogen is held this way.\n\nUsable nitrogen may be lost from soils when it is in the form of nitrate, as it is easily leached. Further losses of nitrogen occur by denitrification, the process whereby soil bacteria convert nitrate (NO) to nitrogen gas, N or NO. This occurs when poor soil aeration limits free oxygen, forcing bacteria to use the oxygen in nitrate for their respiratory process. Denitrification increases when oxidisable organic material is available and when soils are warm and slightly acidic. Denitrification may vary throughout a soil as the aeration varies from place to place. Denitrification may cause the loss of 10 to 20 percent of the available nitrates within a day and when conditions are favourable to that process, losses of up to 60 percent of nitrate applied as fertiliser may occur.\n\n\"Ammonium volatilisation\" occurs when ammonium reacts chemically with an alkaline soil, converting NH to NH. The application of ammonium fertiliser to such a field can result in volatilisation losses of as much as 30 percent.\n\nAfter nitrogen, phosphorus is probably the element most likely to be deficient in soils. The soil mineral apatite is the most common mineral source of phosphorus. While there is on average 1000 lb of phosphorus per acre in the soil, it is generally in the form of phosphates with low solubility. Total phosphorus is about 0.1 percent by weight of the soil, but only one percent of that is available. Of the part available, more than half comes from the mineralisation of organic matter. Agricultural fields may need to be fertilised to make up for the phosphorus that has been removed in the crop.\n\nWhen phosphorus does form solubilised ions of HPO, they rapidly form insoluble phosphates of calcium or hydrous oxides of iron and aluminum. Phosphorus is largely immobile in the soil and is not leached but actually builds up in the surface layer if not cropped. The application of soluble fertilisers to soils may result in zinc deficiencies as zinc phosphates form. Conversely, the application of zinc to soils may immobilise phosphorus again as zinc phosphate. Lack of phosphorus may interfere with the normal opening of the plant leaf stomata, resulting in plant temperatures 10 percent higher than normal. Phosphorus is most available when soil pH is 6.5 in mineral soils and 5.5 in organic soils.\n\nThe amount of potassium in a soil may be as much as 80,000 lb per acre-foot, of which only 150 lb is available for plant growth. Common mineral sources of potassium are the mica biotite and potassium feldspar, KAlSiO. When solubilised, half will be held as exchangeable cations on clay while the other half is in the soil water solution. Potassium fixation often occurs when soils dry and the potassium is bonded between layers of illite clay. Under certain conditions, dependent on the soil texture, intensity of drying, and initial amount of exchangeable potassium, the fixed percentage may be as much as 90 percent within ten minutes. Potassium may be leached from soils low in clay.\n\nCalcium is one percent by weight of soils and is generally available but may be low as it is soluble and can be leached. It is thus low in sandy and heavily leached soil or strongly acidic mineral soil. Calcium is supplied to the plant in the form of exchangeable ions and moderately soluble minerals. Calcium is more available on the soil colloids than is potassium because the common mineral calcite, CaCO, is more soluble than potassium-bearing minerals.\n\nMagnesium is one of the dominant exchangeable cations in most soils (as are calcium and potassium). Primary minerals that weather to release magnesium include hornblende, biotite and vermiculite. Soil magnesium concentrations are generally sufficient for optimal plant growth, but highly weathered and sandy soils may be magnesium deficient due to leaching by heavy precipitation.\n\nMost sulfur is made available to plants, like phosphorus, by its release from decomposing organic matter. Deficiencies may exist in some soils (especially sandy soils) and if cropped, sulfur needs to be added. The application of large quantities of nitrogen to fields that have marginal amounts of sulfur may cause sulfur deficiency in the rapidly growing plants by the plant's growth outpacing the supply of sulfur. A 15-ton crop of onions uses up to 19 lb of sulfur and 4 tons of alfalfa uses 15 lb per acre. Sulfur abundance varies with depth. In a sample of soils in Ohio, United States, the sulfur abundance varied with depths, 0-6 inches, 6-12 inches, 12-18 inches, 18-24 inches in the amounts: 1056, 830, 686, 528 lb per acre respectively.\n\nThe micronutrients essential in plant life, in their order of importance, include iron, manganese, zinc, copper, boron, chlorine and molybdenum. The term refers to plants' needs, not to their abundance in soil. They are required in very small amounts but are essential to plant health in that most are required parts of some enzyme system which speeds up plants' metabolisms. They are generally available in the mineral component of the soil, but the heavy application of phosphates can cause a deficiency in zinc and iron by the formation of insoluble zinc and iron phosphates. Iron deficiency may also result from excessive amounts of heavy metals or calcium minerals (lime) in the soil. Excess amounts of soluble boron, molybdenum and chloride are toxic.\n\nNutrients which enhance the health but whose deficiency does not stop the life cycle of plants include: cobalt, strontium, vanadium, silicon and nickel. As their importance are evaluated they may be added to the list of essential plant nutrients.\n\nSoil organic matter is made up of organic compounds and includes plant, animal and microbial material, both living and dead. A typical soil has a biomass composition of 70% microorganisms, 22% macrofauna, and 8% roots. The living component of an acre of soil may include 900 lb of earthworms, 2400 lb of fungi, 1500 lb of bacteria, 133 lb of protozoa and 890 lb of arthropods and algae.\n\nA small part of the organic matter consists of the living cells such as bacteria, molds, and actinomycetes that work to break down the dead organic matter. Were it not for the action of these micro-organisms, the entire carbon dioxide part of the atmosphere would be sequestered as organic matter in the soil.\n\nChemically, organic matter is classed as follows:\n\n\nMost living things in soils, including plants, insects, bacteria, and fungi, are dependent on organic matter for nutrients and/or energy. Soils have organic compounds in varying degrees of decomposition which rate is dependent on the temperature, soil moisture, and aeration. Bacteria and fungi feed on the raw organic matter, which are fed upon by amoebas, which in turn are fed upon by nematodes and arthropods. Organic matter holds soils open, allowing the infiltration of air and water, and may hold as much as twice its weight in water. Many soils, including desert and rocky-gravel soils, have little or no organic matter. Soils that are all organic matter, such as peat (histosols), are infertile. In its earliest stage of decomposition, the original organic material is often called raw organic matter. The final stage of decomposition is called humus.\n\nIn grassland, much of the organic matter added to the soil is from the deep, fibrous, grass root systems. By contrast, tree leaves falling on the forest floor are the principal source of soil organic matter in the forest. Another difference is the frequent occurrence in the grasslands of fires that destroy large amounts of aboveground material but stimulate even greater contributions from roots. Also, the much greater acidity under any forests inhibits the action of certain soil organisms that otherwise would mix much of the surface litter into the mineral soil. As a result, the soils under grasslands generally develop a thicker A horizon with a deeper distribution of organic matter than in comparable soils under forests, which characteristically store most of their organic matter in the forest floor (O horizon) and thin A horizon.\n\nHumus refers to organic matter that has been decomposed by soil flora and fauna to the point where it is resistant to further breakdown. Humus usually constitutes only five percent of the soil or less by volume, but it is an essential source of nutrients and adds important textural qualities crucial to soil health and plant growth. Humus also hold bits of undecomposed organic matter which feed arthropods and worms which further improve the soil. The end product, humus, is soluble in water and forms a weak acid that can attack silicate minerals. Humus is a colloid with a high cation and anion exchange capacity that on a dry weight basis is many times greater than that of clay colloids. It also acts as a buffer, like clay, against changes in pH and soil moisture.\n\nHumic acids and fulvic acids, which begin as raw organic matter, are important constituents of humus. After the death of plants and animals, microbes begin to feed on the residues, resulting finally in the formation of humus. With decomposition, there is a reduction of water-soluble constituents, cellulose and hemicellulose, and nutrients such as nitrogen, phosphorus, and sulfur. As the residues break down, only stable molecules made of aromatic carbon rings, oxygen and hydrogen remain in the form of humin, lignin and lignin complexes collectively called humus. While the structure of humus has few nutrients, it is able to attract and hold cation and anion nutrients by weak bonds that can be released into the soil solution in response to changes in soil pH.\n\nLignin is resistant to breakdown and accumulates within the soil. It also reacts with amino acids, which further increases its resistance to decomposition, including enzymatic decomposition by microbes. Fats and waxes from plant matter have some resistance to decomposition and persist in soils for a while. Clay soils often have higher organic contents that persist longer than soils without clay as the organic molecules adhere to and are stabilised by the clay. Proteins normally decompose readily, but when bound to clay particles, they become more resistant to decomposition. Clay particles also absorb the enzymes exuded by microbes which would normally break down proteins. The addition of organic matter to clay soils can render that organic matter and any added nutrients inaccessible to plants and microbes for many years. High soil tannin (polyphenol) content can cause nitrogen to be sequestered in proteins or cause nitrogen immobilisation.\n\nHumus formation is a process dependent on the amount of plant material added each year and the type of base soil. Both are affected by climate and the type of organisms present. Soils with humus can vary in nitrogen content but typically have 3 to 6 percent nitrogen. Raw organic matter, as a reserve of nitrogen and phosphorus, is a vital component affecting soil fertility. Humus also absorbs water, and expands and shrinks between dry and wet states, increasing soil porosity. Humus is less stable than the soil's mineral constituents, as it is reduced by microbial decomposition, and over time its concentration diminshes without the addition of new organic matter. However, humus may persist over centuries if not millennia.\n\nThe production, accumulation and degradation of organic matter are greatly dependent on climate. Temperature, soil moisture and topography are the major factors affecting the accumulation of organic matter in soils. Organic matter tends to accumulate under wet or cold conditions where decomposer activity is impeded by low temperature or excess moisture which results in anaerobic conditions. Conversely, excessive rain and high temperatures of tropical climates enables rapid decomposition of organic matter and leaching of plant nutrients; forest ecosystems on these soils rely on efficient recycling of nutrients and plant matter to maintain their productivity. Excessive slope may encourage the erosion of the top layer of soil which holds most of the raw organic material that would otherwise eventually become humus.\n\nCellulose and hemicellulose undergo fast decomposition by fungi and bacteria, with a half-life of 12–18 days in a temperate climate. Brown rot fungi can decompose the cellulose and hemicellulose, leaving the lignin and phenolic compounds behind. Starch, which is an energy storage system for plants, undergoes fast decomposition by bacteria and fungi. Lignin consists of polymers composed of 500 to 600 units with a highly branched, amorphous structure. Lignin undergoes very slow decomposition, mainly by white rot fungi and actinomycetes; its half-life under temperate conditions is about six months.\n\nA horizontal layer of the soil, whose physical features, composition and age are distinct from those above and beneath, is referred to as a soil horizon. The naming of a horizon is based on the type of material of which it is composed. Those materials reflect the duration of specific processes of soil formation. They are labelled using a shorthand notation of letters and numbers which describe the horizon in terms of its colour, size, texture, structure, consistency, root quantity, pH, voids, boundary characteristics and presence of nodules or concretions. No soil profile has all the major horizons. Some may have only one horizon.\n\nThe exposure of parent material to favourable conditions produces mineral soils that are marginally suitable for plant growth. That growth often results in the accumulation of organic residues. The accumulated organic layer called the O horizon produces a more active soil due to the effect of the organisms that live within it. Organisms colonise and break down organic materials, making available nutrients upon which other plants and animals can live. After sufficient time, humus moves downward and is deposited in a distinctive organic surface layer called the A horizon.\n\nSoil is classified into categories in order to understand relationships between different soils and to determine the suitability of a soil for a particular use. One of the first classification systems was developed by Russian scientist Dokuchaev around 1880. It was modified a number of times by American and European researchers, and developed into the system commonly used until the 1960s. It was based on the idea that soils have a particular morphology based on the materials and factors that form them. In the 1960s, a different classification system began to emerge which focused on soil morphology instead of parental materials and soil-forming factors. Since then it has undergone further modifications. The World Reference Base for Soil Resources (WRB) aims to establish an international reference base for soil classification.\n\nThere are fourteen soil orders at the top level of the Australian Soil Classification. They are: Anthroposols, Organosols, Podosols, Vertosols, Hydrosols, Kurosols, Sodosols, Chromosols, Calcarosols, Ferrosols, Dermosols, Kandosols, Rudosols and Tenosols.\n\nThe EU's soil taxonomy is based on a new standard soil classification in the World Reference Base for Soil Resources produced by the UN's Food and Agriculture Organization. According to this, the major soils in the European Union are:\n\nA taxonomy is an arrangement in a systematic manner; the USDA soil taxonomy has six levels of classification. They are, from most general to specific: order, suborder, great group, subgroup, family and series. Soil properties that can be measured quantitatively are used in this classification system – they include: depth, moisture, temperature, texture, structure, cation exchange capacity, base saturation, clay mineralogy, organic matter content and salt content. There are 12 soil orders (the top hierarchical level) in soil taxonomy. The names of the orders end with the suffix \"-sol\". The criteria for the different soil orders include properties that reflect major differences in the genesis of soils. The orders are:\n\nThe percentages listed above are for land area free of ice. \"Soils of Mountains\", which constitute the balance (11.6%), have a mixture of those listed above, or are classified as \"Rugged Mountains\" which have no soil.\n\nThe above soil orders in sequence of increasing degree of development are Entisols, Inceptisols, Aridisols, Mollisols, Alfisols, Spodosols, Ultisols, and Oxisols. Histosols and Vertisols may appear in any of the above at any time during their development.\n\nThe soil suborders within an order are differentiated on the basis of soil properties and horizons which depend on soil moisture and temperature. Forty-seven suborders are recognized in the United States.\n\nThe soil great group category is a subdivision of a suborder in which the kind and sequence of soil horizons distinguish one soil from another. About 185 great groups are recognized in the United States. Horizons marked by clay, iron, humus and hard pans and soil features such as the expansion-contraction of clays (that produce self-mixing provided by clay), temperature, and marked quantities of various salts are used as distinguishing features.\n\nThe great group categories are divided into three kinds of soil subgroups: typic, intergrade and extragrade. A typic subgroup represents the basic or 'typical' concept of the great group to which the described subgroup belongs. An intergrade subgroup describes the properties that suggest how it grades towards (is similar to) soils of other soil great groups, suborders or orders. These properties are not developed or expressed well enough to cause the soil to be included within the great group towards which they grade, but suggest similarities. Extragrade features are aberrant properties which prevent that soil from being included in another soil classification. About 1,000 soil subgroups are defined in the United States.\n\nA soil family category is a group of soils within a subgroup and describes the physical and chemical properties which affect the response of soil to agricultural management and engineering applications. The principal characteristics used to differentiate soil families include texture, mineralogy, pH, permeability, structure, consistency, the locale's precipitation pattern, and soil temperature. For some soils the criteria also specify the percentage of silt, sand and coarse fragments such as gravel, cobbles and rocks. About 4,500 soil families are recognised in the United States.\n\nA family may contain several soil series which describe the physical location using the name of a prominent physical feature such as a river or town near where the soil sample was taken. An example would be Merrimac for the Merrimack River in New Hampshire. More than 14,000 soil series are recognised in the United States. This permits very specific descriptions of soils.\n\nA soil phase of series, originally called 'soil type' describes the soil surface texture, slope, stoniness, saltiness, erosion, and other conditions.\n\nSoil is used in agriculture, where it serves as the anchor and primary nutrient base for plants; however, as demonstrated by hydroponics, it is not essential to plant growth if the soil-contained nutrients can be dissolved in a solution. The types of soil and available moisture determine the species of plants that can be cultivated.\n\nSoil material is also a critical component in the mining, construction and landscape development industries. Soil serves as a foundation for most construction projects. The movement of massive volumes of soil can be involved in surface mining, road building and dam construction. Earth sheltering is the architectural practice of using soil for external thermal mass against building walls. Many building materials are soil based.\n\nSoil resources are critical to the environment, as well as to food and fibre production. Soil provides minerals and water to plants. Soil absorbs rainwater and releases it later, thus preventing floods and drought. Soil cleans water as it percolates through it. Soil is the habitat for many organisms: the major part of known and unknown biodiversity is in the soil, in the form of invertebrates (earthworms, woodlice, millipedes, centipedes, snails, slugs, mites, springtails, enchytraeids, nematodes, protists), bacteria, archaea, fungi and algae; and most organisms living above ground have part of them (plants) or spend part of their life cycle (insects) below-ground. Above-ground and below-ground biodiversities are tightly interconnected, making soil protection of paramount importance for any restoration or conservation plan.\n\nThe biological component of soil is an extremely important carbon sink since about 57% of the biotic content is carbon. Even on desert crusts, cyanobacteria, lichens and mosses capture and sequester a significant amount of carbon by photosynthesis. Poor farming and grazing methods have degraded soils and released much of this sequestered carbon to the atmosphere. Restoring the world's soils could offset the effect of increases in greenhouse gas emissions and slow global warming, while improving crop yields and reducing water needs.\n\nWaste management often has a soil component. Septic drain fields treat septic tank effluent using aerobic soil processes. Landfills use soil for daily cover. Land application of waste water relies on soil biology to aerobically treat BOD.\n\nOrganic soils, especially peat, serve as a significant fuel resource; but wide areas of peat production, such as sphagnum bogs, are now protected because of patrimonial interest.\n\nGeophagy is the practice of eating soil-like substances. Both animals and human cultures occasionally consume soil for medicinal, recreational, or religious purposes. It has been shown that some monkeys consume soil, together with their preferred food (tree foliage and fruits), in order to alleviate tannin toxicity.\n\nSoils filter and purify water and affect its chemistry. Rain water and pooled water from ponds, lakes and rivers percolate through the soil horizons and the upper rock strata, thus becoming groundwater. Pests (viruses) and pollutants, such as persistent organic pollutants (chlorinated pesticides, polychlorinated biphenyls), oils (hydrocarbons), heavy metals (lead, zinc, cadmium), and excess nutrients (nitrates, sulfates, phosphates) are filtered out by the soil. Soil organisms metabolise them or immobilise them in their biomass and necromass, thereby incorporating them into stable humus. The physical integrity of soil is also a prerequisite for avoiding landslides in rugged landscapes.\n\nLand degradation refers to a human-induced or natural process which impairs the capacity of land to function. Soils degradation involves the acidification, contamination, desertification, erosion or salination.\n\nSoil acidification is beneficial in the case of alkaline soils, but it degrades land when it lowers crop productivity and increases soil vulnerability to contamination and erosion. Soils are often initially acid because their parent materials were acid and initially low in the basic cations (calcium, magnesium, potassium and sodium). Acidification occurs when these elements are leached from the soil profile by rainfall or by the harvesting of forest or agricultural crops. Soil acidification is accelerated by the use of acid-forming nitrogenous fertilizers and by the effects of acid precipitation.\n\nSoil contamination at low levels is often within a soil's capacity to treat and assimilate waste material. Soil biota can treat waste by transforming it; soil colloids can adsorb the waste material. Many waste treatment processes rely on this treatment capacity. Exceeding treatment capacity can damage soil biota and limit soil function. Derelict soils occur where industrial contamination or other development activity damages the soil to such a degree that the land cannot be used safely or productively. Remediation of derelict soil uses principles of geology, physics, chemistry and biology to degrade, attenuate, isolate or remove soil contaminants to restore soil functions and values. Techniques include leaching, air sparging, chemical amendments, phytoremediation, bioremediation and natural degradation.\n\nDesertification is an environmental process of ecosystem degradation in arid and semi-arid regions, often caused by human activity. It is a common misconception that droughts cause desertification. Droughts are common in arid and semiarid lands. Well-managed lands can recover from drought when the rains return. Soil management tools include maintaining soil nutrient and organic matter levels, reduced tillage and increased cover. These practices help to control erosion and maintain productivity during periods when moisture is available. Continued land abuse during droughts, however, increases land degradation. Increased population and livestock pressure on marginal lands accelerates desertification.\n\nErosion of soil is caused by water, wind, ice, and movement in response to gravity. More than one kind of erosion can occur simultaneously. Erosion is distinguished from weathering, since erosion also transports eroded soil away from its place of origin (soil in transit may be described as sediment). Erosion is an intrinsic natural process, but in many places it is greatly increased by human activity, especially poor land use practices. These include agricultural activities which leave the soil bare during times of heavy rain or strong winds, overgrazing, deforestation, and improper construction activity. Improved management can limit erosion. Soil conservation techniques which are employed include changes of land use (such as replacing erosion-prone crops with grass or other soil-binding plants), changes to the timing or type of agricultural operations, terrace building, use of erosion-suppressing cover materials (including cover crops and other plants), limiting disturbance during construction, and avoiding construction during erosion-prone periods.\n\nA serious and long-running water erosion problem occurs in China, on the middle reaches of the Yellow River and the upper reaches of the Yangtze River. From the Yellow River, over 1.6 billion tons of sediment flow each year into the ocean. The sediment originates primarily from water erosion (gully erosion) in the Loess Plateau region of northwest China.\n\nSoil piping is a particular form of soil erosion that occurs below the soil surface. It causes levee and dam failure, as well as sink hole formation. Turbulent flow removes soil starting at the mouth of the seep flow and the subsoil erosion advances up-gradient. The term sand boil is used to describe the appearance of the discharging end of an active soil pipe.\n\nSoil salination is the accumulation of free salts to such an extent that it leads to degradation of the agricultural value of soils and vegetation. Consequences include corrosion damage, reduced plant growth, erosion due to loss of plant cover and soil structure, and water quality problems due to sedimentation. Salination occurs due to a combination of natural and human-caused processes. Arid conditions favour salt accumulation. This is especially apparent when soil parent material is saline. Irrigation of arid lands is especially problematic. All irrigation water has some level of salinity. Irrigation, especially when it involves leakage from canals and overirrigation in the field, often raises the underlying water table. Rapid salination occurs when the land surface is within the capillary fringe of saline groundwater. Soil salinity control involves watertable control and flushing with higher levels of applied water in combination with tile drainage or another form of subsurface drainage.\n\nSoils which contain high levels of particular clays, such as smectites, are often very fertile. For example, the smectite-rich clays of Thailand's Central Plains are among the most productive in the world.\n\nMany farmers in tropical areas, however, struggle to retain organic matter in the soils they work. In recent years, for example, productivity has declined in the low-clay soils of northern Thailand. Farmers initially responded by adding organic matter from termite mounds, but this was unsustainable in the long-term. Scientists experimented with adding bentonite, one of the smectite family of clays, to the soil. In field trials, conducted by scientists from the International Water Management Institute in cooperation with Khon Kaen University and local farmers, this had the effect of helping retain water and nutrients. Supplementing the farmer's usual practice with a single application of 200 kg bentonite per rai (6.26 rai = 1 hectare) resulted in an average yield increase of 73%. More work showed that applying bentonite to degraded sandy soils reduced the risk of crop failure during drought years.\n\nIn 2008, three years after the initial trials, IWMI scientists conducted a survey among 250 farmers in northeast Thailand, half of whom had applied bentonite to their fields. The average improvement for those using the clay addition was 18% higher than for non-clay users. Using the clay had enabled some farmers to switch to growing vegetables, which need more fertile soil. This helped to increase their income. The researchers estimated that 200 farmers in northeast Thailand and 400 in Cambodia had adopted the use of clays, and that a further 20,000 farmers were introduced to the new technique.\n\nIf the soil is too high in clay, adding gypsum, washed river sand and organic matter will balance the composition. Adding organic matter (like ramial chipped wood for instance) to soil which is depleted in nutrients and too high in sand will boost its quality.\n\n"}
{"id": "614763", "url": "https://en.wikipedia.org/wiki?curid=614763", "title": "Stark effect", "text": "Stark effect\n\nThe Stark effect is the shifting and splitting of spectral lines of atoms and molecules due to the presence of an external electric field. It is the electric-field analogue of the Zeeman effect, where a spectral line is split into several components due to the presence of the magnetic field. Although initially coined for the static case, it is also used in the wider context to describe effect of time-dependent electric fields. In particular, the Stark effect is responsible for the pressure broadening (Stark broadening) of spectral lines by charged particles in plasmas. For majority of spectral lines, the Stark effect is either linear (proportional to the applied electric field) or quadratic with a high accuracy.\n\nThe Stark effect can be observed both for emission and absorption lines. The latter is sometimes called the inverse Stark effect, but this term is no longer used in the modern literature.\n\nThe effect is named after the German physicist Johannes Stark, who discovered it in 1913. It was independently discovered in the same year by the Italian physicist Antonino Lo Surdo, and in Italy it is thus sometimes called the Stark–Lo Surdo effect. The discovery of this effect contributed importantly to the development of quantum theory and was rewarded with the Nobel Prize in Physics for Johannes Stark in the year 1919.\n\nInspired by the magnetic Zeeman effect, and especially by Lorentz's explanation of it, Woldemar Voigt performed classical mechanical calculations of quasi-elastically bound electrons in an electric field. By using experimental indices of refraction he gave an estimate of the Stark splittings. This estimate was a few orders of magnitude too low. Not deterred by this prediction, Stark undertook measurements on excited states of the hydrogen atom and succeeded in observing splittings.\n\nBy the use of the Bohr–Sommerfeld (\"old\") quantum theory, Paul Epstein and Karl Schwarzschild were independently able to derive equations for the linear and quadratic Stark effect in hydrogen. Four years later, Hendrik Kramers derived formulas for intensities of spectral transitions. Kramers also included the effect of fine structure, which includes corrections for relativistic kinetic energy and coupling between electron spin and orbital motion. The first quantum mechanical treatment (in the framework of Heisenberg's matrix mechanics) was by Wolfgang Pauli. Erwin Schrödinger discussed at length the Stark effect in his third paper on quantum theory (in which he introduced his perturbation theory), once in the manner of the 1916 work of Epstein (but generalized from the old to the new quantum theory) and once by his (first-order) perturbation approach.\nFinally, Epstein reconsidered the linear and quadratic Stark effect from the point of view of the new quantum theory. He derived equations for the line intensities which were a decided improvement over Kramers' results obtained by the old quantum theory.\n\nWhile first-order perturbation effects for the Stark effect in hydrogen are in agreement for the Bohr–Sommerfeld model and the quantum-mechanical theory of the atom, higher-order effects are not. Measurements of the Stark effect under high field strengths confirmed the correctness of the quantum theory over the Bohr model.\n\nAn electric field pointing from left to right, for example, tends to pull nuclei to the right and electrons to the left. In another way of viewing it, if an electronic state has its electron disproportionately to the left, its energy is lowered, while if it has the electron disproportionately to the right, its energy is raised.\n\nOther things being equal, the effect of the electric field is greater for outer electron shells, because the electron is more distant from the nucleus, so it travels farther left and farther right.\n\nThe Stark effect can lead to splitting of degenerate energy levels. For example, in the Bohr model, an electron has the same energy whether it is in the 2s state or any of the 2p states. However, in an electric field, there will be hybrid orbitals (also called quantum superpositions) of the 2s and 2p states where the electron tends to be to the left, which will acquire a lower energy, and other hybrid orbitals where the electron tends to be to the right, which will acquire a higher energy. Therefore, the formerly degenerate energy levels will split into slightly lower and slightly higher energy levels.\n\nThe Stark effect originates from the interaction between a charge distribution (atom or molecule) and an external electric field. Before turning to quantum mechanics we describe the interaction\nclassically and consider a continuous charge distribution ρ(r).\nIf this charge distribution is non-polarizable its interaction energy with an external electrostatic potential \"V\"(r) is \nIf the electric field is of macroscopic origin and the charge distribution is microscopic, it is reasonable to assume that the electric field is uniform over the charge distribution. That is, \"V\" is given by a two-term Taylor expansion,\nwhere we took the origin 0 somewhere within ρ.\nSetting \"V\"(0) as the zero energy, the interaction becomes\nHere we have introduced the dipole moment μ of ρ as an integral over the charge distribution. In case ρ consists of \"N\" point charges \"q\" this definition becomes a sum\n\nElectric-field perturbation applied to a classical hydrogen atom produces a distortion of the electron orbit in a direction perpendicular to the applied field. This effect can be shown without perturbation theory using the relation between the angular momentum and the Laplace–Runge–Lenz vector. Using the Laplace-Runge-Lenz approach, one can see both the transverse distortion and the usual Stark effect. The transverse distortion is not mentioned in most textbooks. This approach can also lead to an \"exactly solvable\" approximate model Hamiltonian for an atom in a strong oscillatory field. “There are few exactly-solvable problems in quantum mechanics, and even fewer with a time-dependent Hamiltonian.”\n\nTurning now to quantum mechanics an atom or a molecule can be thought of as a collection of point charges (electrons and nuclei), so that the second definition of the dipole applies. The interaction of atom or molecule with a uniform external field is described by the operator\nThis operator is used as a perturbation in first- and second-order perturbation theory to account for the first- and second-order Stark effect.\n\nLet the unperturbed atom or molecule be in a \"g\"-fold degenerate state with orthonormal zeroth-order state functions formula_7. (Non-degeneracy is the special case \"g\" = 1). According to perturbation theory the first-order energies are the eigenvalues of the \"g\" x \"g\" matrix with general element \nIf \"g\" = 1 (as is often the case for electronic states of molecules) the first-order energy becomes proportional to the expectation (average) value of the dipole operator formula_9,\n\nBecause a dipole moment is a polar vector, the diagonal elements of the perturbation matrix V vanish for systems with an inversion center (such as atoms). Molecules with an inversion center in a non-degenerate electronic state do not have a (permanent) dipole and hence do not show a linear Stark effect.\n\nIn order to obtain a non-zero matrix V for systems with an inversion center it is necessary that some of the unperturbed functions formula_11 have opposite parity (obtain plus and minus under inversion), because only functions of opposite parity give non-vanishing matrix elements. Degenerate zeroth-order states of opposite parity occur for excited hydrogen-like (one-electron) atoms or Rydberg states. Neglecting fine-structure effects, such a state with the principal quantum number \"n\" is \"n\"-fold degenerate and\nwhere formula_13 is the azimuthal (angular momentum) quantum number. For instance, the excited \"n\" = 4 state contains the following formula_14 states,\nThe one-electron states with even formula_14 are even under parity, while those with odd formula_14 are odd under parity. Hence hydrogen-like atoms with \"n\">1 show first-order Stark effect.\n\nThe first-order Stark effect occurs in rotational transitions of symmetric top molecules (but not for linear and asymmetric molecules). In first approximation a molecule may be seen as a rigid rotor. A symmetric top rigid rotor has the unperturbed eigenstates\nwith 2(2\"J\"+1)-fold degenerate energy for |K| > 0 and (2\"J\"+1)-fold degenerate energy for K=0.\nHere \"D\" is an element of the Wigner D-matrix. The first-order perturbation matrix on basis of the unperturbed rigid rotor function is non-zero and can be diagonalized. This gives shifts and splittings\nin the rotational spectrum. Quantitative analysis of these Stark shift yields the permanent electric dipole moment of the symmetric top molecule.\n\nAs stated, the quadratic Stark effect is described by second-order perturbation theory.\nThe zeroth-order eigenproblem\nis assumed to be solved. The perturbation theory gives\nwith the components of the polarizability tensor α defined by\nThe energy \"E\" gives the quadratic Stark effect.\n\nNeglecting the hyperfine structure (which is often justified — unless extremely weak electric fields are considered), the polarizability tensor of atoms is isotropic,\nFor some molecules this expression is a reasonable approximation, too.\n\nIt is important to note that for the ground state formula_23 is \"always\" positive, i.e., the quadratic Stark shift is always negative.\n\nThe perturbative treatment of the Stark effect has some problems. In the presence of an electric field, states of atoms and molecules that were previously bound (square-integrable), become formally (non-square-integrable) resonances of finite width.\nThese resonances may decay in finite time via field ionization. For low lying states and not too strong fields the decay times are so long, however, that for all practical purposes the system can be regarded as bound. For highly excited states and/or very strong fields ionization may have to be accounted for. (See also the article on the Rydberg atom).\n\nIn a semiconductor heterostructure, where a small bandgap material is sandwiched between two layers of a larger bandgap material, the Stark effect can be dramatically enhanced by bound excitons. This is because the electron and hole which form the exciton are pulled in opposite directions by the applied electric field, but they remain confined in the smaller bandgap material, so the exciton is not merely pulled apart by the field. The quantum-confined Stark effect is widely used for semiconductor-based optical modulators, particularly for optical fiber communications.\n\n\n\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "54612063", "url": "https://en.wikipedia.org/wiki?curid=54612063", "title": "The Genius of Birds", "text": "The Genius of Birds\n\nThe Genius of Birds is a 2016 book by nature writer Jennifer Ackerman.\n\n\"The Genius of Birds\" highlights new findings and discoveries in the field of bird intelligence. The book explores birds as thinkers (contrary to the cliché \"bird brain\") in the context of observed behavior in the wild and brings to it the scientific findings from lab and field research.\n\nNew research suggests that some birds, such as those in the family corvidae, can rival primates and even humans in forms of intelligence. Much like humans, birds have enormous brains relative to the rest of their bodies.\n\nAckerman highlights the complex social structures of avian society. They are capable of abstract thinking, problem solving, recognizing faces, gift giving, sharing, grieving, and meaningful communication with humans. Ackerman goes in depth to highlight scientific studies that uncover behavior such as tool usage, speaking in regional accents, navigation, and theory of mind.\n\nThe book is a New York Times Bestseller, and was named one of the 10 best nonfiction books of 2016 by The Wall Street Journal.\n"}
{"id": "31950254", "url": "https://en.wikipedia.org/wiki?curid=31950254", "title": "Thin-film thickness monitor", "text": "Thin-film thickness monitor\n\nThin-film thickness monitors, deposition rate controllers, and so on, are a family of instruments used in high and ultra-high vacuum systems. They can measure the thickness of a thin film, not only after it has been made, but while it is still being deposited, and some can control either the final thickness of the film, the rate at which it is deposited, or both. Not surprisingly, the devices which control some aspect of the process tend to be called controllers, and those that simply monitor the process tend to be called monitors.\n\nMost such instruments use a quartz crystal microbalance as the sensor. Optical measurements are sometimes used; this may be especially appropriate if the film being deposited is part of a thin film optical device.\n\nA thickness monitor measures how much material deposited on its sensor. Most deposition processes are at least somewhat directional. The sensor and the sample generally cannot be in the same direction from the deposition source (if they were, the one closer to the source would shadow the other), and may not even be at the same distance from it. Therefore, the rate at which the material is deposited on the sensor may not equal the rate at which it is deposited on the sample. The ratio of the two rates is sometimes called the \"tooling factor\". For careful work, the tooling factor should be checked by measuring the amount of material deposited on some samples after the fact and comparing it to what the thickness monitor measured. Fizeau interferometers are often used to do this. Many other techniques might be used, depending on the thickness and characteristics of the thin film, including surface profilers, ellipsometry, dual polarisation interferometry and scanning electron microscopy of cross-sections of the sample. Many thickness monitors and controllers allow tooling factors to be entered into the device before deposition begins.\n\nThe correct tooling factor can be calculated as follows:\n\nformula_1\n\nwhere F is the initial tooling factor, T is the film thickness indicated by the instrument, and T is the actual, independently measured thickness of the deposited film. If no tooling factor has been preset or used before, F equals 1.\n"}
{"id": "3672150", "url": "https://en.wikipedia.org/wiki?curid=3672150", "title": "Vacuum evaporation", "text": "Vacuum evaporation\n\nVacuum evaporation is the process of causing the pressure in a liquid-filled container to be reduced below the vapor pressure of the liquid, causing the liquid to evaporate at a lower temperature than normal. Although the process can be applied to any type of liquid at any vapor pressure, it is generally used to describe the boiling of water by lowering the container's internal pressure below standard atmospheric pressure and causing the water to boil at room temperature.\n\nThe vacuum evaporation treatment process consists of reducing the interior pressure of the evaporation chamber below atmospheric pressure. This reduces the boiling point of the liquid to be evaporated, thereby reducing or eliminating the need for heat in both the boiling and condensation processes. In addition, there are other technical advantages such as the ability to distill other liquids with high boiling points and avoiding the decomposition of substances that are sensitive to temperature, etc.\n\nWhen the process is applied to food and the water is evaporated and removed, the food can be stored for long periods of time without spoiling. It is also used when boiling a substance at normal temperatures would chemically change the consistency of the product, such as egg whites coagulating when attempting to dehydrate the albumen into a powder.\n\nThis process was invented by Henri Nestlé in 1866, of Nestlé Chocolate fame, although the Shakers were already using a vacuum pan earlier than that (see condensed milk).\n\nThis process is used industrially to make such food products as evaporated milk for milk chocolate, and tomato paste for ketchup.\nIn the sugar industry vacuum evaporation is used in the crystallization of sucrose solutions. Traditionally, this process was performed in batch mode, but nowadays continuous vacuum pans are available.\n\nVacuum evaporators are used in a wide range of industrial sectors to treat industrial wastewater. It represents a clean, safe and very versatile technology having low management costs, which in most cases serves as a zero-discharge treatment system.\n\nVacuum evaporation is also a form of physical vapor deposition used in the semiconductor, microelectronics, and optical industries and in this context is a process of depositing thin films of material onto surfaces. Such a technique consists of pumping a vacuum chamber to pressures of less than 10 torr and heating a material to produce a flux of vapor in order to deposit the material onto a surface. The material to be vaporized is typically heated until its vapor pressure is high enough to produce a flux of several Angstroms per second by using an electrically resistive heater or bombardment by a high voltage beam.\n\n"}
{"id": "701188", "url": "https://en.wikipedia.org/wiki?curid=701188", "title": "Vacuum state", "text": "Vacuum state\n\nIn quantum field theory, the quantum vacuum state (also called the quantum vacuum or vacuum state) is the quantum state with the lowest possible energy. Generally, it contains no physical particles. Zero-point field is sometimes used as a synonym for the vacuum state of an individual quantized field.\n\nAccording to present-day understanding of what is called the vacuum state or the quantum vacuum, it is \"by no means a simple empty space\". According to quantum mechanics, the vacuum state is not truly empty but instead contains fleeting electromagnetic waves and particles that pop into and out of existence.\n\nThe QED vacuum of quantum electrodynamics (or QED) was the first vacuum of quantum field theory to be developed. QED originated in the 1930s, and in the late 1940s and early 1950s it was reformulated by Feynman, Tomonaga and Schwinger, who jointly received the Nobel prize for this work in 1965. Today the electromagnetic interactions and the weak interactions are unified in the theory of the electroweak interaction.\n\nThe Standard Model is a generalization of the QED work to include all the known elementary particles and their interactions (except gravity). Quantum chromodynamics is the portion of the Standard Model that deals with strong interactions, and QCD vacuum is the vacuum of quantum chromodynamics. It is the object of study in the Large Hadron Collider and the Relativistic Heavy Ion Collider, and is related to the so-called \"vacuum structure of strong interactions\".\n\nIf the quantum field theory can be accurately described through perturbation theory, then the properties of the vacuum are analogous to the properties of the ground state of a quantum mechanical harmonic oscillator, or more accurately, the ground state of a measurement problem. In this case the vacuum expectation value (VEV) of any field operator vanishes. For quantum field theories in which perturbation theory breaks down at low energies (for example, Quantum chromodynamics or the BCS theory of superconductivity) field operators may have non-vanishing vacuum expectation values called condensates. In the Standard Model, the non-zero vacuum expectation value of the Higgs field, arising from spontaneous symmetry breaking, is the mechanism by which the other fields in the theory acquire mass.\n\nIn many situations, the vacuum state can be defined to have zero energy, although the actual situation is considerably more subtle. The vacuum state is associated with a zero-point energy, and this zero-point energy has measurable effects. In the laboratory, it may be detected as the Casimir effect. In physical cosmology, the energy of the cosmological vacuum appears as the cosmological constant. In fact, the energy of a cubic centimeter of empty space has been calculated figuratively to be one trillionth of an erg (or 0.6 eV). An outstanding requirement imposed on a potential Theory of Everything is that the energy of the quantum vacuum state must explain the physically observed cosmological constant.\n\nFor a relativistic field theory, the vacuum is Poincaré invariant, which follows from\nWightman axioms but can be also proved directly without these axioms. Poincaré invariance implies that only scalar combinations of field operators have non-vanishing VEV's. The VEV may break some of the internal symmetries of the Lagrangian of the field theory. In this case the vacuum has less symmetry than the theory allows, and one says that \"spontaneous symmetry breaking\" has occurred. See Higgs mechanism, standard model.\n\nIn principle, quantum corrections to Maxwell's equations can cause the experimental electrical permittivity ε of the vacuum state to deviate from the defined scalar value ε of the electric constant. These theoretical developments are described, for example, in Dittrich and Gies.\nIn particular, the theory of quantum electrodynamics predicts that the QED vacuum should exhibit nonlinear effects that will make it behave like a birefringent material with ε slightly greater than ε for extremely strong electric fields. Explanations for dichroism from particle physics, outside quantum electrodynamics, also have been proposed. Active attempts to measure such effects have yielded negative results so far.\n\nThe presence of virtual particles can be rigorously based upon the non-commutation of the quantized electromagnetic fields. Non-commutation means that although the average values of the fields vanish in a quantum vacuum, their variances do not. The term \"vacuum fluctuations\" refers to the variance of the field strength in the minimal energy state, and is described picturesquely as evidence of \"virtual particles\". It is sometimes attempted to provide an intuitive picture of virtual particles, or variances, based upon the Heisenberg energy-time uncertainty principle:\n(with \"ΔE\" and \"Δt\" being the energy and time variations respectively; \"ΔE\" is the accuracy in the measurement of energy and \"Δt\" is the time taken in the measurement, and is the Reduced Planck constant) arguing along the lines that the short lifetime of virtual particles allows the \"borrowing\" of large energies from the vacuum and thus permits particle generation for short times. Although the phenomenon of virtual particles is accepted, this interpretation of the energy-time uncertainty relation is not universal. One issue is the use of an uncertainty relation limiting measurement accuracy as though a time uncertainty \"Δt\" determines a \"budget\" for borrowing energy \"ΔE\". Another issue is the meaning of \"time\" in this relation, because energy and time (unlike position and momentum , for example) do not satisfy a canonical commutation relation (such as ). Various schemes have been advanced to construct an observable that has some kind of time interpretation, and yet does satisfy a canonical commutation relation with energy. The very many approaches to the energy-time uncertainty principle are a long and continuing subject.\n\nAccording to Astrid Lambrecht (2002): \"When one empties out a space of all matter and lowers the temperature to absolute zero, one produces in a \"Gedankenexperiment\" [mental experiment] the quantum vacuum state.\" According to Fowler & Guggenheim (1939/1965), the third law of thermodynamics may be precisely enunciated as follows:\n\"It is impossible by any procedure, no matter how idealized, to reduce any assembly to the absolute zero in a finite number of operations.\" (See also.)\n\nPhoton-photon interaction can occur only through interaction with the vacuum state of some other field, for example through the Dirac electron-positron vacuum field; this is associated with the concept of vacuum polarization. According to Milonni (1994): \"... \"all quantum fields have zero-point energies and vacuum fluctuations.\"\" This means that there is a component of the quantum vacuum respectively for each component field (considered in the conceptual absence of the other fields), such as the electromagnetic field, the Dirac electron-positron field, and so on. According to Milonni (1994), some of the effects attributed to the vacuum electromagnetic field can have several physical interpretations, some more conventional than others. The Casimir attraction between uncharged conductive plates is often proposed as an example of an effect of the vacuum electromagnetic field. Schwinger, DeRaad, and Milton (1978) are cited by Milonni (1994) as validly, though unconventionally, explaining the Casimir effect with a model in which \"the vacuum is regarded as truly a state with all physical properties equal to zero.\" In this model, the observed phenomena are explained as the effects of the electron motions on the electromagnetic field, called the source field effect. Milonni writes: \n\nThe basic idea here will be that the Casimir force may be derived from the source fields alone even in completely conventional QED, ... Milonni provides detailed argument that the measurable physical effects usually attributed to the vacuum electromagnetic field cannot be explained by that field alone, but require in addition a contribution from the self-energy of the electrons, or their radiation reaction. He writes: \"The radiation reaction and the vacuum fields are two aspects of the same thing when it comes to physical interpretations of various QED processes including the Lamb shift, van der Waals forces, and Casimir effects. \n\nThis point of view is also stated by Jaffe (2005): \"The Casimir force can be calculated without reference to vacuum fluctuations, and like all other observable effects in QED, it vanishes as the fine structure constant, , goes to zero.\"\n\nThe vacuum state is written as formula_2 or formula_3. The vacuum expectation value (see also Expectation value) of any field formula_4 should be written as formula_5.\n\n\n\n\n"}
{"id": "1945275", "url": "https://en.wikipedia.org/wiki?curid=1945275", "title": "Wigner effect", "text": "Wigner effect\n\nThe Wigner effect (named for its discoverer, Eugene Wigner), also known as the discomposition effect or Wigner's Disease, is the dislocation of atoms in a solid caused by neutron radiation. \n\nAny solid can display the Wigner effect. The effect is of most concern in neutron moderators, such as graphite, intended to reduce the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235.\n\nTo create the Wigner effect, neutrons that collide with the atoms in a crystal structure must have enough energy to displace them from the lattice. This amount (threshold displacement energy) is approximately 25 eV. A neutron's energy can vary widely, but it is not uncommon to have energies up to and exceeding 10 MeV (10,000,000 eV) in the centre of a nuclear reactor. A neutron with a significant amount of energy will create a displacement cascade in a matrix via elastic collisions. For example, a 1 MeV neutron striking graphite will create 900 displacements; not all displacements will create defects, because some of the struck atoms will find and fill the vacancies that were either small pre-existing voids or vacancies newly formed by the other struck atoms.\n\nThe atoms that do not find a vacancy come to rest in non-ideal locations; that is, not along the symmetrical lines of the lattice. These atoms are referred to as interstitial atoms, or simply interstitials. An interstitial atom and its associated vacancy are known as a Frenkel defect. Because these atoms are not in the ideal location, they have an energy associated with them, much as a ball at the top of a hill has gravitational potential energy. This energy is referred to as Wigner energy. When a large number of interstitials have accumulated, they pose a risk of releasing all of their energy suddenly, creating a rapid, very great increase in temperature. Sudden, unplanned increases in temperature can present a large risk for certain types of nuclear reactors with low operating temperatures; one such was the indirect cause of the Windscale fire. Accumulation of energy in irradiated graphite has been recorded as high as 2.7 kJ/g, but is typically much lower than this. Graphite, having a heat capacity of 0.720 J/g°C, could see a sudden increase in temperature of about 3750 °C (6780 °F).\n\nDespite some reports, Wigner energy buildup had nothing to do with the cause of the Chernobyl disaster: this reactor, like all contemporary power reactors, operated at a high enough temperature to allow the displaced graphite structure to realign itself before any potential energy could be stored. Wigner energy may have played some part following the prompt critical neutron spike, when the accident entered the graphite fire phase of events.\n\nA buildup of Wigner energy can be relieved by heating the material. This process is known as annealing. In graphite this occurs at 250 °C.\n\nIn 2003, it was postulated that Wigner energy can be stored by the formation of metastable defect structures in graphite. Notably, the large energy release observed at 200–250 °C has been described in terms of a metastable interstitial-vacancy pair. The interstitial atom becomes trapped on the lip of the vacancy, and there is a barrier for it to recombine to give perfect graphite.\n\n"}
{"id": "56106", "url": "https://en.wikipedia.org/wiki?curid=56106", "title": "Wildfire", "text": "Wildfire\n\nA wildfire or wildland fire is a fire in an area of combustible vegetation occurring in rural areas. Depending on the type of vegetation present, a wildfire can also be classified more specifically as a brush fire, bushfire, desert fire, forest fire, grass fire, hill fire, peat fire, vegetation fire, and veld fire.\n\nFossil charcoal indicates that wildfires began soon after the appearance of terrestrial plants 420 million years ago. Wildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Earth is an intrinsically flammable planet owing to its cover of carbon-rich vegetation, seasonally dry climates, atmospheric oxygen, and widespread lightning and volcanic ignitions.\n\nWildfires can be characterized in terms of the cause of ignition, their physical properties, the combustible material present, and the effect of weather on the fire. Wildfires can cause damage to property and human life, though naturally occurring wildfires may have beneficial effects on native vegetation, animals, and ecosystems that have evolved with fire. High-severity wildfire creates complex early seral forest habitat (also called \"snag forest habitat\"), which often has higher species richness and diversity than unburned old forest. Many plant species depend on the effects of fire for growth and reproduction. Wildfires in ecosystems where wildfire is uncommon or where non-native vegetation has encroached may have strongly negative ecological effects. Wildfire behavior and severity result from the combination of factors such as available fuels, physical setting, and weather. Analyses of historical meteorological data and national fire records in western North America show the primacy of climate in driving large regional fires via wet periods that create substantial fuels or drought and warming that extend conducive fire weather.\n\nStrategies for wildfire prevention, detection, and suppression have varied over the years. One common and inexpensive technique is controlled burning, intentionally igniting smaller fires to minimize the amount of flammable material available for a potential wildfire. Vegetation may be burned periodically to maintain high species diversity and limit the accumulation of plants and other debris that may serve as fuel. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior when under extreme weather conditions. Wildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan Van Wagtendonk, a biologist at the Yellowstone Field Station. Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure.\n\nThree major natural causes of wildfire ignitions exist: \n\nThe most common direct human causes of wildfire ignition include arson, discarded cigarettes, power-line arcs (as detected by arc mapping), and sparks from equipment. Ignition of wildland fires via contact with hot rifle-bullet fragments is also possible under the right conditions. Wildfires can also be started in communities experiencing shifting cultivation, where land is cleared quickly and farmed until the soil loses fertility, and slash and burn clearing. Forested areas cleared by logging encourage the dominance of flammable grasses, and abandoned logging roads overgrown by vegetation may act as fire corridors. Annual grassland fires in southern Vietnam stem in part from the destruction of forested areas by US military herbicides, explosives, and mechanical land-clearing and -burning operations during the Vietnam War.\n\nThe most common cause of wildfires varies throughout the world. In Canada and northwest China, lightning operates as the major source of ignition. In other parts of the world, human involvement is a major contributor. In Africa, Central America, Fiji, Mexico, New Zealand, South America, and Southeast Asia, wildfires can be attributed to human activities such as agriculture, animal husbandry, and land-conversion burning. In China and in the Mediterranean Basin, human carelessness is a major cause of wildfires. In the United States and Australia, the source of wildfires can be traced both to lightning strikes and to human activities (such as machinery sparks, cast-away cigarette butts, or arson). Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.\n\nThe spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows: \n\nWildfires occur when all of the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation, that is subjected to sufficient heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are required to evaporate any water within the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, plants dry out and are therefore more flammable, often a consequence of droughts.\n\nA wildfire \"front\" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of . Next, the pyrolysis of wood at releases flammable gases. Finally, wood can smoulder at or, when heated sufficiently, ignite at . Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to , which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or \"torching\": the drying of tree canopies and their subsequent ignition from below.\n\nWildfires have a rapid \"forward rate of spread\" (FROS) when burning through dense, uninterrupted fuels. They can move as fast as in forests and in grasslands. Wildfires can advance tangential to the main front to form a \"flanking\" front, or burn in the opposite direction of the main front by \"backing\". They may also spread by \"jumping\" or \"spotting\" as winds and vertical convection columns carry \"firebrands\" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels that surround a wildfire are especially vulnerable to ignition from firebrands. Spotting can create \"spot fires\" as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as from the fire front.\n\nEspecially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than . Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.\n\nThe thermal heat from wildfire can cause significant weathering of rocks and boulders, heat can rapidly expand a boulder and thermal shock can occur, which may cause an object's structure to fail.\n\nHeat waves, droughts, cyclical climate changes such as El Niño, and regional weather patterns such as high-pressure ridges can increase the risk and alter the behavior of wildfires dramatically. Years of precipitation followed by warm periods can encourage more widespread fires and longer fire seasons. Since the mid-1980s, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season in the Western United States. Global warming may increase the intensity and frequency of droughts in many areas, creating more intense and frequent wildfires. A 2015 study indicates that the increase in fire risk in California may be attributable to human-induced climate change. A study of alluvial sediment deposits going back over 8,000 years found warmer climate periods experienced severe droughts and stand-replacing fires and concluded climate was such a powerful influence on wildfire that trying to recreate presettlement forest structure is likely impossible in a warmer future.\n\nIntensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour \"fire day\" that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.\n\nWildfire's occurrence throughout the history of terrestrial life invites conjecture that fire must have had pronounced evolutionary effects on most ecosystems' flora and fauna. Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.\n\nHigh-severity wildfire creates complex early seral forest habitat (also called “snag forest habitat”), which often has higher species richness and diversity than unburned old forest. Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to soil, the heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife. Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests. Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding.\n\nAlthough some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds. Invasive species, such as \"Lygodium microphyllum\" and \"Bromus tectorum\", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.\n\nIn the Amazon Rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by the year 2030. Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding away other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors burned off of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating back to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.\n\nIn tundra there is a natural pattern of accumulation of fuel and wildfire which varies depending on the nature of vegetation and terrain. Research in Alaska has shown fire-event return intervals, (FRIs) that typically vary from 150 to 200 years with dryer lowland areas burning more frequently than wetter upland areas.\n\nPlants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition. For example, plants of the genus \"Eucalyptus\" contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called \"serotiny\". Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.\n\nGrasslands in Western Sabah, Malaysian pine forests, and Indonesian \"Casuarina\" forests are believed to have resulted from previous periods of fire. Chamise deadwood litter is low in water content and flammable, and the shrub quickly sprouts after a fire. Cape lilies lie dormant until flames brush away the covering and then blossom almost overnight. Sequoia rely on periodic fires to reduce competition, release seeds from their cones, and clear the soil and canopy for new growth. Caribbean Pine in Bahamian pineyards have adapted to and rely on low-intensity, surface fires for survival and growth. An optimum fire frequency for growth is every 3 to 10 years. Too frequent fires favor herbaceous plants, and infrequent fires favor species typical of Bahamian dry forests.\n\nMost of the Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about . The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding . Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.\n\nWildfires can affect local atmospheric pollution, and release carbon in the form of carbon dioxide. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentration beyond safe levels. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.\n\nIn the Welsh Borders, the first evidence of wildfire is rhyniophytoid plant fossils preserved as charcoal, dating to the Silurian period (about ). Smoldering surface fires started to occur sometime before the Early Devonian period . Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30-31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.\n\nWildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera \"Eucalyptus\", \"Pinus\" and \"Sequoia\", which have thick bark to withstand fires and employ serotiny.\n\nThe human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered the preexisting landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices. In the mid-19th century, explorers from observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in the lands protected by Kakadu National Park to encourage biodiversity.\n\nWildfires typically occurred during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).\n\nAccording to a paper published in Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.\n\nIncreases of certain native tree species (i.e. conifers) in favor of others (i.e. leaf trees) also increases wildfire risk, especially if these trees are also planted in monocultures\n\nSome invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California and gamba grass in Australia.\n\nWildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread. Prevention techniques aim to manage air quality, maintain ecological balances, protect resources, and to affect future fires. North American firefighting policies permit naturally caused fires to burn to maintain their ecological role, so long as the risks of escape into high-value areas are mitigated. However, prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.\n\nIn 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie \"Bambi\", and the official mascot of the U.S. Forest Service, Smokey Bear. Reducing human-caused ignitions may be the most effective means of reducing unwanted wildfire. Alteration of fuels is commonly undertaken when attempting to affect future fire risk and behavior. Wildfire prevention programs around the world may employ techniques such as \"wildland fire use\" and \"prescribed or controlled burns\". \"Wildland fire use\" refers to any fire of natural causes that is monitored but allowed to burn. \"Controlled burns\" are fires ignited by government agencies under less dangerous weather conditions.\n\nVegetation may be burned periodically to maintain high species diversity and frequent burning of surface fuels limits fuel accumulation. Wildland fire use is the cheapest and most ecologically appropriate policy for many forests. Fuels may also be removed by logging, but fuels treatments and thinning have no effect on severe fire behavior Wildfire models are often used to predict and compare the benefits of different fuel treatments on future wildfire spread, but their accuracy is low.\n\nWildfire itself is reportedly \"the most effective treatment for reducing a fire's rate of spread, fireline intensity, flame length, and heat per unit of area\" according to Jan van Wagtendonk, a biologist at the Yellowstone Field Station.\n\nBuilding codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.\n\nFast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.\n\nCurrently, public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. A government report on a recent trial of three automated camera fire detection systems in Australia did, however, conclude \"...detection by the camera systems was slower and less reliable than by a trained human observer\". These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.\n\nA small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or \"tree-rechargeable\": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.\n\nSatellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than . The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from for MODIS and AVHRR data and up to for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution and may also limit the effectiveness of satellite imagery.\n\nin 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions. The active fire detection product using data from Suomi NPP's Visible Infrared Imaging Radiometer Suite (VIIRS) increases the resolution of fire observations to 1,230 feet (375 meters). Previous NASA satellite data products available since the early 2000s observed fires at 3,280 foot (1 kilometer) resolution. The data is one of the intelligence tools used by the USFS and Department of Interior agencies across the United States to guide resource allocation and strategic fire management decisions. The enhanced VIIRS fire product enables detection every 12 hours or less of much smaller fires and provides more detail and consistent tracking of fire lines during long duration wildfires – capabilities critical for early warning systems and support of routine mapping of fire progression. Active fire locations are available to users within minutes from the satellite overpass through data processing facilities at the USFS Remote Sensing Applications Center, which uses technologies developed by the NASA Goddard Space Flight Center Direct Readout Laboratory in Greenbelt, Maryland. The model uses data on weather conditions and the land surrounding an active fire to predict 12–18 hours in advance whether a blaze will shift direction. The state of Colorado decided to incorporate the weather-fire model in its firefighting efforts beginning with the 2016 fire season.\n\nIn 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375m fire product, put it to use during several large wildfires in Kruger.\n\nThe demand for timely, high-quality fire information has increased in recent years. Wildfires in the United States burn an average of 7 million acres of land each year. For the last 10 years, the USFS and Department of Interior have spent a combined average of about $2–4 billion annually on wildfire suppression.\n\nWildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of per year.\n\nAbove all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, USA, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.\n\nIn California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.\n\nWildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis. Between 2000–2016, more than 350 wildland firefighters died on-duty.\n\nEspecially in hot weather condition, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.\n\nSmoke, ash, and debris can also pose serious respiratory hazards to wildland fire fighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.\n\nFirefighters are also at risk of cardiac events including strokes and heart attacks. Fire fighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland fire fighters face include slips, trips and falls, burns, scrapes and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.\n\nFire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents. The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.\n\nTypical fire retardants contain the same agents as fertilizers. Fire retardant may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.\n\nCurrent USDA procedure maintains that the aerial application of fire retardant in the United States must clear waterways by a minimum of 300 feet in order to safeguard effects of retardant runoff. Aerial uses of fire retardant are required to avoid application near waterways and endangered species (plant and animal habitats). After any incident of fire retardant misapplication, the U.S. Forest Service requires reporting and assessment impacts be made in order to determine mitigation, remediation, and/or restrictions on future retardant uses in that area.\n\nWildfire modeling is concerned with numerical simulation of wildfires in order to comprehend and predict fire behavior. Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including windspeed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by large wildfires than by small fires.\n\nWildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human induced geographical and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.\n\nThe most noticeable adverse effect of wildfires is the destruction of property. However, the release of hazardous chemicals from the burning of wildland fuels also significantly impacts health in humans.\n\nWildfire smoke is composed primarily of carbon dioxide and water vapor. Other common smoke components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small particulates suspended in air which come in solid form or in liquid droplets are also present in smoke. 80 -90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.\n\nDespite carbon dioxide's high concentration in smoke, it poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.\n\nThe degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract though inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.\n\nThe U.S. Environmental Protection Agency (EPA) developed the Air Quality Index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use this index as a tool to determine their exposure to hazardous air pollutants based on visibility range.\n\nAfter a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits.\n\nFirefighters are at the greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at a close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed. The study obtained exposure concentrations for one wildland firefighter over a 10-hour shift spent holding down a fireline. The firefighter was exposed to a wide range of carbon monoxide and respiratory irritant (combination of particulate matter 3.5 µm and smaller, acrolein, and formaldehype) levels. Carbon monoxide levels reached up to 160ppm and the TLV irritant index value reached a high of 10. In contrast, the OSHA PEL for carbon monoxide is 30ppm and for the TLV respiratory irritant index, the calculated threshold limit value is 1; any value above 1 exceeds exposure limits.\n\nBetween 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis.\n\nResidents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods.\n\nAdditionally, there is evidence of an increase in maternal stress, as documented by researchers M.H. O'Donnell and A.M. Behie, thus affecting birth outcomes. In Australia, studies show that male infants born with drastically higher average birth weights were born in mostly severely fire-affected areas. This is attributed to the fact that maternal signals directly affect fetal growth patterns.\n\nAsthma is one of the most common chronic disease among children in the United States affecting estimated 6.2 million children. A recent area of research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved are in this. In human's considerable airway development occurs during the 2 and 3 trimester and continue until 3 years of age. It is hypothesized that exposure to these toxins during this period could have consequential effects as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma. Recent Meta-Analyses have found significant association between PM, NO and development of asthma during childhood despite heterogeneity among studies. Furthermore, maternal exposure to chronic stressor, which are most like to be present in distressed communities, which is also a relevant co relate of childhood asthma which may further help explain the early childhood exposure to air pollution, neighborhood poverty and childhood risk. Living in distressed neighborhood is not only linked to pollutant source location and exposure but can also be associated with degree of magnitude of chronic individual stress which can in turn alter the allostatic load of the maternal immune system leading to adverse outcomes in children, including increased susceptibility to air pollution and other hazards.\n\nWildfire smoke contains particulate matter that may have adverse effects upon the human respiratory system. Evidence of the health effects of wildfire smoke should be relayed to the public so that exposure may be limited. Evidence of health effects can also be used to influence policy to promote positive health outcomes.\n\nInhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide.\n\nParticulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on the diameter of the particle: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer.  Each size can enter the body through inhalation, but the PM impact on the body varies by size. Coarse particles are filtered by the upper airways and these particles can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing. Coarse PM is often composed of materials that are heavier and more toxic that lead to short-term effects with stronger impact.\n\nSmaller particulate moves further into the respiratory system creating issues deep into the lungs and the bloodstream. In asthma patients, PM causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes cause the cells to be damaged and impacts the cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised. The third PM type is ultra-fine PM (UFP). UFP can enter the bloodstream like PM however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe. PM is of the largest concern in regards to wildfire. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly with exposure to fine particle from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.\n\nSmoke from wildfires can cause health problems, especially for children and those who already have respiratory problems. Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma. \n\nAn observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled. Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children. Particulate Matter (PM) triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.\n\nAlthough some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma. In investigating the association of medication use for obstructive lung disease and wildfire exposure, researchers found increases both in the usage of inhalers and initiation of long-term control as in oral steroids. More specifically, some people with asthma reported higher use of quick-relief medications (inhalers). After two major wildfires in California, researchers found an increase in physician prescriptions for quick-relief medications in the years following the wildfires than compared to the year before each occurrence. \n\nThere is consistent evidence between wildfire smoke and the exacerbation of asthma.\n\nCarbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. For this reason, carbon monoxide inhalation is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headache, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma and even death. However, even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990–2006 found that 21.9% of the deaths occurred from heart attacks.\n\nAnother important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from countries ranging from the United States and Canada to Greece and Australia who were directly and indirectly affected by wildfires were found by researchers to demonstrate several different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.\n\nIn a new twist to wildfire health effects, former uranium mining sites were burned over in the summer of 2012 near North Fork, Idaho. This prompted concern from area residents and Idaho State Department of Environmental Quality officials over the potential spread of radiation in the resultant smoke, since those sites had never been completely cleaned up from radioactive remains.\n\nThe western US has seen an increase in both frequency and intensity of wildfires over the last several decades. This increase has been attributed to the arid climate of the western US and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western United States. Evidence has demonstrated that wildfire smoke can increase levels of particulate matter in the atmosphere.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.\n\nThe EPA has defined acceptable concentrations of particulate matter in the air. The National Ambient Air Quality Standards are part of the Clean Air Act and provide mandated guidelines for pollutant levels and the monitoring of ambient air quality. In addition to these monitoring programs, the increased incidence of wildfires near populated areas have precipitated several epidemiological studies. Such studies have demonstrated an association between negative human health effects and an increase in fine particulate matter due to wildfire smoke. The size of the particulate matter is significant as smaller particulate matter (fine) is easily inhaled into the human respiratory tract. Often, small particulate matter can be inhaled into deep lung tissue causing respiratory distress, illness, or disease. \n\nAn increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD. Looking at the wildfires in Southern California in October 2003 in a similar manner, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke. Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.\n\nChildren participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Recently, it was demonstrated that mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed to wildfire during birth. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide it is estimated that 339,000 people die due to the effects of wildfire smoke each year.\n\nWhile the size of particulate matter is an important consideration for health effects, the chemical composition of particulate matter (PM) from wildfire smoke should also be considered. Antecedent studies have demonstrated that the chemical composition of PM from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke. Health outcomes for people exposed to wildfire smoke may differ from those exposed to smoke from alternative sources such as solid fuels. \n\n"}
{"id": "572303", "url": "https://en.wikipedia.org/wiki?curid=572303", "title": "World Pantheist Movement", "text": "World Pantheist Movement\n\nThe World Pantheist Movement (WPM) is the world's largest organization of people associated with pantheism, a philosophy which asserts that spirituality should be centered on nature. The WPM promotes naturalistic pantheism\n\nThe WPM grew out of a mailing list started by Paul Harrison in 1997, arising around his Scientific Pantheism website. An initial group of 15 volunteers worked on a joint statement of agreed beliefs (the Pantheist Credo). The WPM officially opened for membership in December 1999.\n\nThe official views of the World Pantheist Movement are listed in the nine points of the Belief Statement (see external links).\nThese are summarized as follows:\n\nThe specific Statement is as follows:\n\n\nThe WPM encourages wonder and awe at the beauty and mystery of the Universe and fosters the full range of positive emotional responses to life. It promotes ethical principles such as respect for the rights of humans and other living creatures, non-discrimination, justice and peace.\n\nIt respects the scientific method as humanity's most accurate approach for deepening its understanding of nature, while accepting that science is a never-ending quest and that some technologies have created massive social and environmental problems. It accepts that there are some questions that science may never answer - such as why anything exists, rather than nothing at all. It does not give any credence to ideas such as a separate soul distinct from the body, or of the consciousness' survival after death, but believes that people achieve a form of immortality through the ongoing effects of their actions, the things they create, others' memories, the legacy of their genes, and the recycling of their elements in nature.\n\nIt does not prescribe any particular set of religious practices, instead leaving the matter up to individuals. Pantheistic activity is viewed not as a ritual that must be upheld in order to placate gods and spirits, or to follow rules prescribed in scripture, but rather as an individual expression of one's deep feelings towards Nature and the wider Universe. Among members and friends of the WPM, the most common practices are meditation and close, daily observation of nature. Both of these are sometimes accompanied by the use of natural objects such as pebbles, shells, bark etc. About a quarter of WPM members report using some form of pagan celebration, but this is done for self-expression and fun, rather than out of literal belief in pagan theologies.\n\nMembers and friends may meet in small groups, the format of which varies. Groups may discuss general ideas; watch and discuss relevant (often nature-related) books, or films; share personal experiences; go on nature outings; or participate in nature conservation projects or other forms of community service.\n\nViewing the recycling of human elements in soil, water, and the atmosphere as a major element of human \"immortality,\" the WPM supports the \"natural death\" approach. In this context, it seeks to return the elements to the natural cycle in the fastest and most responsible way possible, such as burial in a biodegradable container in nature-reserve type grounds.\n\nThe WPM is governed by a board of 13 directors. Its main forms of activity consist of:\n\nThe World Pantheist Movement symbol is the spiral as seen in the curves of the nautilus shell, or in the spiral arms of a galaxy, showing the link between the cosmic physical (thousands of light years) and the biological. Sometimes the Nautilus spiral alone is used; it embodies the Fibonacci series and the golden ratio.\n\nThe WPM has a policy of accepting a diversity of language and methods of celebration among its members, although - following large surveys of its website visitors' preferences - it generally avoids overtly theistic or religious language in its official literature. Some members view themselves as atheists, while others hold agnostic stances. Some members use theistic vocabulary, however they do not believe in a thinking creator god, and simply use the word \"God\" to describe their feelings of reverence towards Nature and the wider Universe. Some members, while accepting the basic naturalistic beliefs of the WPM, like to combine these with symbols and ceremonies from other traditions, most commonly paganism, philosophical Taoism, and Buddhism.\n\nThe WPM does not interfere with or promote any specific personal choices regarding sexuality or the use of recreational or psychotropic drugs. Similarly, members have a diversity of views on vegetarianism, hunting, non-violence, and many other political, social, and technological issues. All these issues are left to members' understanding of Naturalistic Pantheistic morality and the Belief Statement.\n\nPaul Harrison, the World Pantheist Movement founder, was vice president of the Universal Pantheist Society (UPS) in the mid-1990s, but resigned after becoming skeptical of the possibility of promoting \"generic\" pantheism given the very wide variety of beliefs held by different types of pantheist (specifically, by those with naturalistic beliefs and those with dualist or idealist beliefs). The WPM has since gained a membership considerably larger than the UPS, along with a wider circle of non-members who participate in its online forum and Facebook pages.\n\n\n"}
